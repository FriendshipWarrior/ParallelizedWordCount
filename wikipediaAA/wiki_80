<doc id="26501" url="http://en.wikipedia.org/wiki?curid=26501" title="Boeing RC-135">
Boeing RC-135

The Boeing RC-135 is a family of large reconnaissance aircraft built by Boeing and modified by a number of companies, including General Dynamics, Lockheed, LTV, E-Systems, and L3 Communications, and used by the United States Air Force and Royal Air Force to support theater and national level intelligence consumers with near real-time on-scene collection, analysis and dissemination capabilities. Based on the C-135 Stratolifter airframe, various types of RC-135s have been in service since 1961. Unlike the C-135 and KC-135 which are recognized by Boeing as the Model 717, the RC-135 is internally designated as the Model 739 by the company. Many variants have been modified numerous times, resulting in a large variety of designations, configurations, and program names.
Design and development.
The first RC-135 variant, the RC-135A, were ordered in 1962 by the United States Air Force to replace the Boeing RB-50 Superfortress. Originally nine were ordered but this was later reduced to four. Boeing allocated the variant the designation "Boeing 739-700" but they were modified variant of the KC-135A then in production. They used the same J57-P engines as the tanker variant did, but carried cameras in a bay just aft of the nose wheel bay where the forward fuel tank was normally located. They had no refueling system fitted and they were to be used for photographic and surveying tasks.
The next variant ordered was the RC-135B to be used as an electronic intelligence aircraft to replace the Boeing RB-47H Stratojet on ELINT duties. Similar to the earlier variants, the RC-135Bs were fitted with TF-33 turbofans rather than the older J57s. These ten aircraft were delivered directly into storage in 1965 while they awaited installation of an improved electronics suite. By 1967, they emerged as RC-135Cs and were all delivered that year. The refueling boom was not fitted and the boom operator station was used as a camera bay for a KA-59 camera. Externally, the aircraft were fitted with sideways looking airborne radar (SLAR) antenna on the lower forward fuselage.
The RC-135Bs were the last of the new aircraft built. All the RC variants that followed were modified aircraft, either from earlier RC variants or from tankers.
In 2005, the RC-135 fleet completed a series of significant airframe, navigation and powerplant upgrades which include re-engining from the Pratt & Whitney TF33 to the CFM International CFM-56 (F108) engines used on the KC-135R and T Stratotanker and upgrade of the flight deck instrumentation and navigation systems to the AMP standard. The AMP standard includes conversion from analog readouts to a digital "glass cockpit" configuration.
Operational history.
The current RC-135 fleet is the latest iteration of modifications to this pool of aircraft dating back to the early 1960s. Initially employed by Strategic Air Command for reconnaissance, the RC-135 fleet has participated in every armed conflict involving U.S. forces during its tenure. RC-135s supported operations in Vietnam War, the Mediterranean for Operation El Dorado Canyon, Grenada for Operation Urgent Fury, Panama for Operation Just Cause, the Balkans for Operations Deliberate Force and Allied Force, and Southwest Asia for Operations Desert Shield, Desert Storm, Enduring Freedom and Iraqi Freedom. RC-135s have maintained a constant presence in Southwest Asia since the early 1990s. They were stalwarts of Cold War operations, with missions flown around the periphery of the USSR and its client states in Europe and around the world.
Originally, all RC-135s were operated by Strategic Air Command. Since 1992 they have been assigned to Air Combat Command. The RC-135 fleet is permanently based at Offutt Air Force Base, Nebraska and operated by the 55th Wing, using forward operating locations worldwide. The 55th Wing operates 22 platforms in three variants: three RC-135S Cobra Ball, two RC-135U Combat Sent, and 17 RC-135V/W Rivet Joint.
On August 9, 2010, the Rivet Joint recognized its 20th anniversary of continued service in Central Command, dating back to the beginning of Desert Storm. This represents the longest unbroken presence of any aircraft in the Air Force inventory. During this time it has flown over 8,000 combat missions supporting air and ground forces of Operations Desert Storm, Desert Shield, Northern Watch, Southern Watch, Iraqi Freedom and Enduring Freedom, which continues to this day.
On 22 March 2010 the British Ministry of Defence announced that it had reached agreement with the US Government to purchase three RC-135W Rivet Joint aircraft along with associated ground systems to replace the Nimrod R1, which was subsequently retired in June 2011. The aircraft are scheduled to be delivered between 2014 and 2018. The RAF received the first RC-135W in September 2013, and first deployed one in July 2014 to potentially assist efforts to combat Islamic State of Iraq and the Levant militants in Iraq.
Variants.
KC-135A Reconnaissance Platforms.
At least four KC-135A tankers were converted into makeshift reconnaissance platforms with no change of Mission Design Series (MDS) designation. KC-135As 55-3121, 55-3127, 59-1465, and 59-1514 were modified beginning in 1961. That year the Soviet Union announced its intention to detonate a 100 Megaton thermonuclear device on Novaya Zemlya, the so-called Tsar Bomba. A testbed KC-135A (55-3127) was modified under the Big Safari program to the SPEED LIGHT BRAVO configuration in order to obtain intelligence information on the test. The success of the mission prompted conversion of additional aircraft for intelligence gathering duties.
KC-135R Rivet Stand / Rivet Quick.
Not to be confused with the CFM F108-powered KC-135R tanker, the KC-135R MDS was applied in July 1967, to the three KC-135A reconnaissance aircraft under the Rivet Stand program name. The three aircraft were 55-3121, 59-1465, 59-1514, with KC-135A 58-0126 converted in 1969 to replace 59-1465 which had crashed at Offutt AFB, Nebraska in 1967. Externally the aircraft had varied configurations throughout their lives, but generally they were distinguished by five "towel bar" antennas along the spine of the upper fuselage and a radome below the forward fuselage. The first three aircraft retained the standard tanker nose radome, while 58-0126 was fitted with the 'hog nose' radome commonly associated with an RC-135. A trapeze-like structure in place of the refueling boom which was used to trail an aerodynamic shape housing a specialized receiver array (colloquially known as a "blivet") on a wire was installed. This was reported to be used for "Briar Patch" and "Combat Lion" missions. There were four small optically flat windows on each side of the forward fuselage. On some missions a small wing-like structure housing sensors was fitted to each side of the forward fuselage, with a diagonal brace below it. With the loss of 59-1465, KC-135A 58-0126 was modified to this standard under the Rivet Quick operational name. All four aircraft have now been lost or converted to KC-135R tanker configuration. They are among the few KC-135 tankers equipped with an aerial refueling receptacle above the cockpit, left over from their service as intelligence gathering platforms.
KC-135T Cobra Jaw.
KC-135R 55-3121 was modified in 1969 by Lockheed Air Services to the unique KC-135T configuration under the Cobra Jaw program name. Externally distinguished by the 'hog nose' radome, the aircraft also featured spinning "fang" receiver antennas below the nose radome, a large blade antenna above the forward fuselage, a single 'towel bar' antenna on the spine, teardrop antennas forward of the horizontal stabilizers on each side, and the trapeze-like structure in place of the refueling boom. The aircraft briefly carried nose art consisting of the Ford Cobra Jet cartoon cobra. It was later modified into an RC-135T Rivet Dandy.
RC-135A.
Four RC-135A (63-8058,63-8059, 63-8060, & 63-8061) were photo mapping platform utilized briefly by the Air Photographic & Charting Service, based at Turner AFB, Georgia and later at Forbes AFB, Kansas as part of the 1370th Photographic Mapping Wing . The mission was soon taken over by satellites, and the RC-135As were de-modified and used as staff transports, in the early 1980s they were converted to KC-135D's. Due to delays in reinstalling their original equipment, the RC-135As were the last of the entire C-135 series delivered to the USAF. The Boeing model number for the RC-135A is 739-700.
RC-135B.
The as-delivered version of the RC-135. The RC-135B was never used operationally, as it had no mission equipment installed by Boeing. The entire RC-135B production run of ten aircraft was delivered directly to Martin Aircraft in Baltimore, Maryland for modification and installation of mission equipment under the Big Safari program. Upon completion, the RC-135Bs were re-designated RC-135C. The Boeing model number for the RC-135B is 739-445B.
RC-135C Big Team.
Modified and re-designated RC-135B aircraft used for strategic reconnaissance duties, equipped with the AN/ASD-1 electronic intelligence (ELINT) system. This system was characterized by the large 'cheek' pods on the forward fuselage containing the Automated ELINT Emitter Locating System (AEELS – not Side Looking Airborne Radar – SLAR, as often quoted), as well as numerous other antennae and a camera position in the refuelling pod area of the aft fuselage. The aircraft was crewed by two pilots, two navigators, numerous intelligence gathering specialists, and airborne linguists. When the RC-135C was fully deployed, SAC was able to retire its fleet of RB-47H Stratojets from active reconnaissance duties. All ten continue in active service as either RC-135V Rivet Joint or RC-135U Combat Sent platforms.
RC-135D Office Boy / Rivet Brass.
The RC-135Ds, originally designated KC-135A-II, were the first reconnaissance configured C-135's given the 'R' MDS designation, although they were not the first reconnaissance-tasked members of the C-135 family. They were delivered to Eielson Air Force Base, Alaska in 1962 as part of the Office Boy Project. Serial numbers were 60-356, 60-357, and 60-362. The aircraft began operational missions in 1963. These three aircraft were ordered as KC-135A tankers, but delivered without refueling booms, and known as "falsie C-135As" pending the delivery of the first actual C-135A cargo aircraft in 1961. The primary Rivet Brass mission flew along the northern border of the Soviet Union, often as a shuttle mission between Eielson and RAF Upper Heyford, Oxfordshire, and later RAF Mildenhall, Suffolk, UK. The RC-135D was also used in Southeast Asia during periods when the RC-135M (see below) was unavailable. In the late 1970s, with the expansion of the RC-135 fleet powered by TF33 turbofan engines, the RC-135Ds were converted into tankers, and are currently in the fleet as KC-135Rs.
RC-135E Lisa Ann / Rivet Amber.
Originally designated C-135B-II, project name Lisa Ann, the RC-135E Rivet Amber was a one-of-a-kind aircraft equipped with a large 7 MW Hughes Aircraft phased-array radar system. Originally delivered as a C-135B, 62-4137 operated from Shemya Air Force Station, Alaska from 1966-1969. Its operations were performed in concert with the RC-135S Rivet Ball aircraft (see below). The radar system alone weighed over 35,000 pounds and cost over US$35 million (1960 dollars), making Rivet Amber both the heaviest C-135-derivative aircraft flying and the most expensive Air Force aircraft for its time. The radiation generated by the radar was sufficient to be a health hazard to the crew, and both ends of the radar compartment were shielded by thick lead bulkheads. This prevented the forward and aft crew areas from having direct contact after boarding the aircraft. The system could track an object the size of a soccer ball from a distance of 300 mi, and its mission was to monitor Soviet ballistic missile testing in the reentry phase. The power requirement for the phased array radar was enormous, necessitating an additional power supply. This took the form of a podded Lycoming T55-L5 turboshaft engine in a pod under the left inboard wing section, driving a 350kVA generator dedicated to powering mission equipment. On the opposite wing in the same location was a podded heat exchanger to permit cooling of the massive electronic components on board the aircraft. This configuration has led to the mistaken impression that the aircraft had six engines. On June 5, 1969, Rivet Amber was lost at sea on a ferry flight from Shemya to Eielson AFB for maintenance, and no trace of the aircraft or its crew was ever found.
RC-135M Rivet Card.
The RC-135M was an interim type with more limited ELINT capability than the RC-135C but with extensive additional COMINT capability. They were operated by the 82d Reconnaissance Squadron during the Vietnam War from Kadena AB, gathering signals intelligence over the Gulf of Tonkin and Laos with the program name Combat Apple (originally Burning Candy). There were six RC-135M aircraft, 62-4131, 62-4132, 62-4134, 62-4135, 62-4138 and 62-4139, all of which were later modified to and continue in active service as RC-135W Rivet Joints by the early 1980s.
RC-135S Nancy Rae / Wanda Belle / Rivet Ball.
Rivet Ball was the predecessor program to Cobra Ball and was initiated with a single RC-135S (serial 59-1491, formerly a JKC-135A) on December 31, 1961. The aircraft first operated under the Nancy Rae project name as an asset of Air Force Systems Command and later as an RC-135S reconnaissance platform with Strategic Air Command under the project name Wanda Belle. The name Rivet Ball was assigned in January 1967. The aircraft operated from Shemya AFB, Alaska. Along with most other RC-135 variants, the RC-135S had an elongated nose radome housing an S band receiving antenna. The aircraft was characterized by ten large optically flat quartz windows on the right side of the fuselage used for tracking cameras. Unlike any other RC-135S, Rivet Ball also had a pleixiglass dome mounted top center on its fuselage for the Manual Tracker position. It holds the distinction of obtaining the very first photographic documentation of Soviet Multiple Reentry vehicle (MRV) testing on October 4, 1968. On January 13, 1969 Rivet Ball was destroyed in a landing accident at Shemya when it hydroplaned off the end of runway 28 with no fatalities.
RC-135S Cobra Ball.
The RC-135S Cobra Ball is a measurement and signature intelligence MASINT collector equipped with special electro-optical instruments designed to observe ballistic missile flights at long range. The Cobra Ball monitors missile-associated signals and tracks missiles during boost and re-entry phases to provide reconnaissance for treaty verification and theater ballistic missile proliferation. The aircraft are extensively modified C-135Bs. The right wing and engines are traditionally painted black on Cobra Ball aircraft.
There are three aircraft in service and they are part of the 55th Wing, 45th Reconnaissance Squadron based at Offutt Air Force Base, Nebraska. Cobra Ball aircraft were originally assigned to Shemya and used to observe ballistic missile tests on the Kamchatka peninsula in conjunction with Cobra Dane and Cobra Judy. Two aircraft were converted for Cobra Ball in 1969 and following the loss of an aircraft in 1981 another aircraft was converted in 1983. The sole RC-135X was also converted into an RC-135S in the late 1990s to supplement the other aircraft.
Following the loss of one RC-135T aircraft, an EC-135B was modified in 1985 as a TC-135S for use as a training aircraft for the RC-135S crews to enable them to train with the different aerodynamic effects from standard aircraft. It did not carry any mission equipment.
RC-135T Rivet Dandy.
KC-135T 55-3121 was modified to RC-135T Rivet Dandy configuration in 1971. It was used to supplement the RC-135C/D/M fleet, then in short supply due to ongoing upgrades requiring airframes to be out of service. It operated under the Burning Candy operational order. In 1973 the aircraft's SIGINT gear was removed and transferred to KC-135E 58-0126, resulting in 55-3121 assuming the role of trainer, a role which it fulfilled for the remainder of its life. Externally it retained the 'hog nose' radome and some other external modifications, but the trapeze below the tail was removed, and no refueling boom was fitted and the aircraft had no operational reconnaissance role. In this configuration it operated variously with the 376th Strategic Wing at Kadena AB, Okinawa, the 305th AREFW at Grissom AFB, Indiana, and the 6th Strategic Wing at Eielson AFB, Alaska. In 1982 the aircraft was modified with Pratt & Whitney TF33-PW102 engines and other modifications common to the KC-135E tanker program, and returned to Eielson AFB. It crashed while on approach to Valdez Airport, Alaska on 25 February 1985 with the loss of three crew members. The wreckage was not found until August 1985, six months after the accident.
RC-135U Combat Sent.
The RC-135U Combat Sent is designed to collect technical intelligence on adversary radar emitter systems. Combat Sent data is collected to develop new or upgraded radar warning receivers, radar jammers, decoys, anti-radiation missiles, and training simulators.
Distinctly identified by the antennae arrays on the nose, tail, and wing tips, three RC-135C aircraft were converted to RC-135U (63-9792, 64-14847, & 64-14849) in the early 1970s and 63-9792 was converted to Rivet Joint, late 1978, and all aircraft are based at Offutt Air Force Base, Nebraska. Minimum crew requirements are 2 pilots, 2 navigators, 3 systems engineers, 10 electronic warfare officers, and 6 area specialists.
RC-135V/W Rivet Joint.
The RC-135V/W is the USAF's standard airborne SIGINT platform. Its sensor suite allows the mission crew to detect, identify and geolocate signals throughout the electromagnetic spectrum. The mission crew can then forward gathered information in a variety of formats to a wide range of consumers via Rivet Joint's extensive communications suite. The crew consists of the cockpit crew, electronic warfare officers, intelligence operators, and airborne systems maintenance personnel. All Rivet Joint airframe and mission systems modifications are performed by L-3 Communications in Greenville, Texas, under the oversight of the Air Force Materiel Command.
All RC-135s are assigned to Air Combat Command. The RC-135 is permanently based at Offutt Air Force Base, Nebraska., and operated by the 55th Wing, using various forward deployment locations worldwide.
Under the "BIG SAFARI" program name, RC-135Vs were upgraded from the RC-135C "Big Team" configuration, itself a mission modified RC-135B (the first version delivered). RC-135Ws were originally delivered as C-135B transports, and most were modified from RC-135Ms. For many years, the RC-135V/W could be identified by the four large disc-capped MUCELS antennas forward, four somewhat smaller blade antennae aft and myriad of smaller underside antennas. Baseline 8 Rivet Joints (in the 2000s) introduced the first major change to the external RC-135V/W configuration replacing the MUCELS antennas with plain blade antennas. The configuration of smaller underside antennas was also changed significantly.
RC-135X Cobra Eye.
The sole RC-135X Cobra Eye was converted during the mid-to-late-1980s from a C-135B Telemetry/Range Instrumented Aircraft, serial number 62-4128, with the mission of tracking ICBM reentry vehicles. In 1993, it was converted into an additional RC-135S Cobra Ball.
RC-135W Rivet Joint (Project Airseeker).
The United Kingdom is buying three KC-135R aircraft for conversion to RC-135W Rivet Joint standard under the Airseeker project. Acquisition of the three aircraft is budgeted at £634m (~US$1,000m), with entry into service planned for October 2014. The aircraft will form No. 51 Squadron RAF, based at RAF Waddington along with the RAF's other ISTAR assets. They are expected to remain in British service until 2045.
Previously, the Royal Air Force had gathered signals intelligence with three Nimrod R1, converted in the 1970s from the Nimrod MR1 maritime patrol aircraft. When the time came to upgrade the maritime Nimrods to MRA4 standard, Project Helix was launched in August 2003 to study options for extending the life of the R1 out to 2025. The option of switching to Rivet Joint was added to Helix in 2008, and the retirement of the R1 became inevitable when the MRA4 was cancelled under the UK's 2010 budget cuts. The R1's involvement over Libya in Operation Ellamy delayed its retirement until June 2011. 
Helix became Project Airseeker, under which three KC-135R airframes are being converted to RC-135W standard by L-3 Communications. L-3 will also provide ongoing maintenance and upgrades under a long-term agreement. The three airframes are former United States Air Force KC-135R, all of which first flew in 1964 but will be modified to the latest RC-135W standard before delivery. The three airframes on offer to the UK are the youngest KC-135s in the USAF fleet. As of September 2010 the aircraft had approximately 23,200 flying hours, 22,200 hours and 23,200 hours.
51 Sqn personnel began training at Offutt in January 2011 for conversion to the RC-135. The first RC135W (ZZ664) was delivered ahead of schedule to the Royal Air Force on 12 November 2013, for final approval and testing by the Defence Support and Equipment team prior to its release to service from the UK MAA.
Specifications (RC-135).
General characteristics* Crew: 27: 3 pilots, 2 navigators, 22 rear-crew members
Performance

</doc>
<doc id="26517" url="http://en.wikipedia.org/wiki?curid=26517" title="Richard Hell">
Richard Hell

Richard Hell (born Richard Lester Meyers) is an American singer, songwriter, bass guitarist, and writer.
Richard Hell was an innovator of punk music and fashion. He was one of the first to spike his hair and wear torn, cut and drawn-on shirts, often held together with safety pins. Malcolm McLaren, manager of the Sex Pistols, has credited Hell as a source of inspiration for the Sex Pistols' look and attitude, as well as the safety-pin and graphics accessorized clothing that McLaren sold in his London shop, Sex.
Hell was in several important, early punk bands, including Neon Boys, Television, and The Heartbreakers, after which he formed Richard Hell & The Voidoids. Their 1977 album "Blank Generation" influenced many other punk bands. Its title song was named "One of the 500 Songs That Shaped Rock" by music writers in the Rock and Roll Hall of Fame listing and is ranked as one of the all-time Top 10 punk songs by a 2006 poll of original British punk figures, as reported in the "Rough Guide to Punk".
Since the late 1980s, Hell has devoted himself primarily to writing, publishing two novels and several other books. He was the film critic for "BlackBook" magazine from 2004 to 2006.
Biography.
Early life and career.
Richard Hell grew up in Lexington, Kentucky, in the 1950s. His father, a secular Jew, was an experimental psychologist, researching animal behavior. He died when Hell was 7 years old. Hell was then raised by his mother, who came from Methodists of Welsh and English ancestry. After her husband's death, she returned to school and eventually became a professor.
Hell attended the Sanford School in Delaware for one year, where he became friends with Tom Miller, who later changed his name to Tom Verlaine. They ran away from school together and were arrested in Alabama for arson and vandalism a short time later.
Hell never finished high school, instead moving to New York City to make his way as a poet. In New York he met fellow young poet David Giannini, and moved to Santa Fe, New Mexico for several months, where Giannini and Meyers co-founded "Genesis:Grasp". They used an AM VariTyper with changeable fonts to publish the magazine. They began publishing books and magazines, but decided to go their separate ways in 1971, after which Hell created and published Dot Books. Before he was 21, his own poems were published in numerous periodicals, ranging from "Rolling Stone" to the New Directions "Annual"s. In 1971, along with Verlaine, Hell also published under the pseudonym Theresa Stern, a fictional poet whose photo was actually a combination of both his and Verlaine's faces in drag, superimposed over one another to create a new identity.
The Neon Boys, Television, and the Heartbreakers.
In 1972, Verlaine joined Hell in New York and formed the Neon Boys. In 1974, the band added a second guitarist, Richard Lloyd, and changed their name to Television.
Television's performances at CBGB helped kick-start the first wave of punk bands, inspiring a number of different artists including Patti Smith, who wrote the first press review of Television for the "Soho Weekly News" in June 1974. She formed a highly successful band of her own, The Patti Smith Group. Television was one of the early bands to play at CBGB, and persuaded owner Hilly Kristal to book rock bands there on a regular basis. They also built the club's first stage.
Hell started playing his punk rock anthem "Blank Generation" during his time in Television. In early 1975, Hell parted ways with Television after a dispute over creative control. Hell claimed that he and Verlaine had originally divided the songwriting evenly but that later Verlaine sometimes refused to play Hell's songs. Verlaine remained silent on the subject.
Hell left Television the same week that Jerry Nolan and Johnny Thunders quit the New York Dolls. In May 1975, the three of them formed The Heartbreakers; not to be confused with Tom Petty's band, which adopted the same name the following year. After one show, Walter Lure joined The Heartbreakers as a second guitarist.
Richard Hell and the Voidoids.
In December 1975, Hell quit The Heartbreakers and started Richard Hell and the Voidoids with Robert Quine, Ivan Julian and Marc Bell. The band released two albums, though the second, "Destiny Street", retained only Quine from the original group, with Naux (Juan Maciel) on guitar and Fred Maher on drums, and suffered from Hell's distractions, narcotics especially, during recording. Hell's best known songs with the Voidoids included "Blank Generation", "Love Comes in Spurts", "The Kid With the Replaceable Head" and "Time". In 2009, the guitar tracks on "Destiny Street" were re-recorded and released as "Destiny Street Repaired", with guitarists Julian, Marc Ribot and Bill Frisell playing with the original rhythm tracks.
Also in 2009, Hell gave his blessing to the public access program Pancake Mountain to create an animated music video for "The Kid with the Replaceable Head". It was the Voidoids first and only official music video. The cut used for the animation appears on Hell's 2005 retrospective album, "Spurts, The Richard Hell Story".
Dim Stars and Hell's books, further life.
Hell's only other album release was as part of the band Dim Stars, for which he came out of retirement for a month in the early 1990s. Dim Stars featured guitarist Thurston Moore and drummer Steve Shelley from Sonic Youth, Gumball's guitarist Don Fleming, and Quine. They formed only to record the one album and one EP, both titled "Dim Stars", and played one show in public, a WFMU benefit at the The Ritz in Manhattan. Hell played bass, sang lead vocals and wrote the lyrics for the album.
Hell also co-wrote and sang lead vocals on the song "Never Mind" by The Heads, a 1996 collaborative effort between three former members of Talking Heads.
In 1996, Hell wrote a novel, "Go Now", drawn largely from his own experiences. He released a collection of short pieces (poems, essays and drawings) called "Hot and Cold" in 2001. His second novel, "Godlike", was published in 2005 by Akashic Books as part of Dennis Cooper's Little House on the Bowery Series. All three books were highly praised. Also published in 2005 was "Rabbit Duck", a book of 13 poems written in collaboration with David Shapiro. Hell's nonfiction has been widely anthologized as well, including a number of appearances in "best music writing" collections.
Hell's archive of his manuscripts, tapes, correspondence (written and email), journals, and other documents of his life was purchased for $50,000 by New York University's Fales Library in 2003.
Hell has appeared in several low-budget films, most notably Susan Seidelman's "Smithereens". (Other acting appearances include Uli Lommell's "Blank Generation", Nick Zedd's "Geek Maggot Bingo", Rachel Amadeo's "What About Me?" and Rachid Kerdouche's "Final Reward". Hell had a non-speaking cameo role as Madonna's murdered boyfriend in Susan Seidelman's 1985 "Desperately Seeking Susan".) 
Hell was married to Scandal's Patty Smyth for two years during 1985–86, and they had a daughter, Ruby. Hell married Sheelagh Bevan in 2002.

</doc>
<doc id="26524" url="http://en.wikipedia.org/wiki?curid=26524" title="Rogue">
Rogue

Rogue may refer to:

</doc>
<doc id="26535" url="http://en.wikipedia.org/wiki?curid=26535" title="Richard of Saint Victor">
Richard of Saint Victor

Richard of Saint Victor, C.R.S.A., (died 1173) is known today as one of the most influential religious thinkers of his time. He was a prominent mystical theologian, and was prior of the famous Augustinian Abbey of Saint Victor in Paris from 1162 until his death in 1173.
Life.
Very little is known about the origins and upbringing of Richard of Saint Victor. John of Toulouse wrote a short "Vita" of Richard in the seventeenth century. He said that Richard came from Scotland. John added that Richard was received into the Abbey of St Victor by Abbot Gilduin (1114–1155) and was a student under Hugh of St Victor, the most influential of all Victorine teachers (implying that Richard entered the community before Hugh's death in 1141). This account of Richard's early life is not accepted by all modern scholars, however, and some have suggested that Richard entered the abbey after Hugh's death in 1141.
All scholarship agrees, however, that Richard was a magister during the 1150s, and was then promoted to subprior in 1159 (as stated by a document found at the abbey). He served under Achard of St. Victor's elected successor Ernisius, who was unworthy of the position. Richard's life was then burdened by the frustrations of working under a man who was ill-suited for his responsibilities. Ernisius wasted the abbey's resources on overly ambitious building projects and persecuted those who attempted to resist him. Richard was allowed to keep his office but his influence was restricted. Things became so unbearable that an appeal was made to the Pope, who then visited Saint Victor in 1162. Through a multitude of transactions, Ernisius was eventually removed from his position and the Pope commended Richard for his continued involvement in the matter. Letters from England written to Richard show that he was in constant touch with English affairs and give evidence of the international character of intellectual life at this time.
He was then promoted to prior in 1162, a position he held until his death on 10 March 1173.
Writings.
Richard wrote extensively (Migne's "Patrologia Latina" contains 34 works attributed to him, and this is not Richard's full corpus). There are some problems with establishing the chronology of Richard’s works. The earliest ones come before 1153, and the latest were written one or two years before his death. His earlier works are similar to the general teaching and writing of the period. His writing develops from basic exegesis, theology and philosophy to more of a study of purely spiritual questions. In his early writings he relies on the moral interpretations of previous theologians such as Augustine of Hippo, Bede, Pope Gregory I and Hugh. He later became more independent and strayed from Hugh's influence. There is some debate between historians about which of Richard's texts are the most influential and important. Because Richard's work covers many spheres of thought it is somewhat difficult to categorise his work.
"The Book of the Twelve Patriarchs", or "Benjamin Minor".
"The Book of the Twelve Patriarchs", sometimes titled "Benjamin Minor", is one of Richard of Saint Victor's great works on contemplation. It is not exactly known when it was written, but it would seem to date before 1162. Richard specifies that this work is not a treatise on contemplation but rather prepares the mind for contemplation. 
"The Mystical Ark", or "Benjamin Major".
"The Mystical Ark", sometimes called "Benjamin Major" or "The Grace of Contemplation" completes this with the study of the mind in relation to prayer. However, in the last chapters of "Benjamin Major", written later than the Minor, Richard almost abandons his topic and the discussion of the teaching of mystical theology takes up a good portion of every remaining chapter. He is still attempting to instruct his followers on a text but he has also engaged himself in creating a system of mystical theology.
"De Trinitate".
One of Richard’s greatest works was the "De Trinitate" which was probably written while Richard was prior, between 1162 and 1173. This is known because it incorporates pieces of theological text which editors are now finding in earlier works. "De Trinitate" is Richard's most independent and original study on dogmatic theology. It stems from the desire to show that dogmatic truths of Christian revelation are ultimately not against reason. Richard's theological approach stems from a profoundly mystical life of prayer, which in the Spirit seeks to involve the mind, in continuation with the Augustinian and Anselmian tradition.
Owing to the fact that until recently this masterpiece has not been available in any English translation, its diffusion has been limited and its influence has seldom gone beyond 'Book III', condemning serious enquiry to an understanding of Richard's argument that is only partial. Finally, in 2011, through the efforts of Ruben Angelici's scholarship, the first, full translation of Richard's 'De Trinitate' has been released for publication in English and now this scholastic masterpiece is readily available to a wider audience to be appreciated in its entirety.
Other Treatises and Works.
Richard wrote a massive handbook of biblical education entitled "Liber Exceptionum" ("Book of Selections/Book of Notes"), important scriptural commentaries, and many treatises. 
"The Four Degrees of Violent Charity", composed about 1170, with its description of how vehement love leads to union with God and more perfect service of neighbour, has been of interest to writers interested in Christian mysticism.
Richard’s other treatises are a number of short works which mainly deal with textual difficulties and theological issues. Many of them can be grouped together with larger works. Some of them are correspondence between Richard and his students while others seem to have been written at the request of friends. Although short, they are often interesting because they allow the modern reader to see the mentality of the students and the discussions and issues of the time.
Richard of Saint Victor’s "Commentary on Ezekiel" is of special interest in the field of art history because they explanations laid out by the author are accompanied by illustrations. A number of copies have come down to us, none of which are dated, but they are written in a style attributable to the second half of the twelfth century.
Historiographical contributions.
What makes Richard of Saint-Victor stand out from other theologians of his time is that he approaches theological problems as more of a psychologist, contributing to 'a careful analysis of contemplative experiences.' 

</doc>
<doc id="26538" url="http://en.wikipedia.org/wiki?curid=26538" title="Roman Curia">
Roman Curia

The Roman Curia is the administrative apparatus of the Holy See and the central body through which the Roman Pontiff conducts the affairs of the universal Catholic Church. It acts in his name and with his authority for the good and for the service of the particular Churches and provides the necessary central organization for the correct functioning of the Church and the achievement of its goals.
 In exercising supreme, full, and immediate power in the universal Church, the Roman pontiff makes use of the departments of the Roman Curia which, therefore, perform their duties in his name and with his authority for the good of the churches and in the service of the sacred pastors
 — Decree concerning the Pastoral Office of Bishops in the Church, "Christus Dominus"
The structure and organization of responsibilities within the Curia is at present regulated by the Apostolic Constitution Pastor Bonus issued by Pope John Paul II on 28 June 1988, which Pope Francis has decided to revise.
Historical background.
"Curia" in medieval and later Latin usage means "court" in the sense of "royal court" rather than "court of law". The Roman Curia, then, sometimes anglicized as the Court of Rome, as in the 1534 Act of Parliament that forbade appeals to it from England, is the Papal Court, and assists the Pope in carrying out his functions. The Roman Curia can be loosely compared to cabinets in governments of countries with a Western form of governance, but only the Second Section of the Secretariat of State, known also as the Section for Relations with States, the Pontifical Commission for Vatican City State (established in 1939 by Pius XII) and the Congregation for Catholic Education, can be directly compared with specific ministries of a civil government.
It is normal for every Latin Catholic diocese to have its own "curia" for its administration. For the Diocese of Rome, these functions are "not" handled by the Roman Curia, but by the Vicariate General of His Holiness for the City of Rome, as provided by the Apostolic Constitution "Ecclesia in Urbe". The Vicar General of Rome, traditionally a cardinal, and his deputy the vicegerent, who holds the personal title of archbishop, supervise the governance of the diocese by reference to the Pope himself, but with no more dependence on the Roman Curia, as such, than other Catholic dioceses throughout the world. A distinct office, the Vicar General for Vatican City, administers the portion of the Diocese of Rome in Vatican City.
Until recently, there still existed hereditary officers of the Roman Curia, holding titles denominating functions that had ceased to be a reality when the Papal States were lost to the papacy. A reorganization, ordered by Pope Pius X, was incorporated into the 1917 Code of Canon Law. Further steps toward reorganization were begun by Pope Paul VI in the 1960s. Among the goals of this curial reform were the modernization of procedures and the internationalization of the curial staff. These reforms are reflected in the 1983 Code of Canon Law. The offices of the Vatican City State are not part of the Roman Curia, which is composed only of offices of the Holy See. The following organs or charges, according to the official website of the Holy See, comprise the Curia. It should be noted that all members of the Curia except the Cardinal Camerlengo and the Major Penitentiary resign their office immediately after a papal death or resignation. "See sede vacante"
The secretariats.
Secretariat of State.
The Secretariat of State is the oldest dicastery in the Roman Curia, the government of the Roman Catholic Church. It is headed by the Secretary of State, since 15 October 2013 Cardinal Pietro Parolin, who is responsible for all the political and diplomatic functions of the Holy See. The Secretariat is divided into two sections, the Section for General Affairs and the Section for Relations with States, known as the First Section and Second Section, respectively. The Secretariat of State was created in the 15th century and is now the dicastery most involved in coordinating the Holy See's activities.
Secretariat for the Economy.
The Secretariat for the Economy was established by Pope Francis in 2014.
The congregations.
The Roman Congregations are a type of dicastery (department with a jurisdiction) of the Roman Curia, the central administrative organism of the Catholic Church. Each Congregation is led by a prefect, who is a cardinal. Until recently, a non-cardinal appointed to head a congregation was styled as pro-prefect until he was made cardinal in a consistory. This practice has recently been abandoned.
The Congregation for the Doctrine of the Faith.
The Congregation for the Doctrine of the Faith (CDF) ("Congregatio pro Doctrina Fidei"), previously known as the Supreme Sacred Congregation of the Roman and Universal Inquisition, and sometimes simply called the Holy Office, is the oldest of the nine congregations of the Roman Curia. Among the most active of these major Curial departments, it oversees Catholic doctrine. The CDF is the modern name for what used to be the Holy Office of the Inquisition. The current prefect is Cardinal Gerhard Ludwig Müller.
The Congregation for the Oriental Churches.
The Congregation for the Oriental Churches ("Congregatio pro Ecclesiis Orientalibus") is the congregation of the Roman Curia responsible for contact with the Eastern Catholic Churches for the sake of assisting their development, protecting their rights and also maintaining whole and entire in the one Catholic Church, alongside the liturgical, disciplinary and spiritual patrimony of the Latin Church, the heritage of the various Oriental Christian traditions. It has exclusive authority over the following regions: Egypt and the Sinai Peninsula, Eritrea and northern Ethiopia, southern Albania and Bulgaria, Cyprus, Greece, Israel, Iran, Iraq, Lebanon, the Palestinian territories, Syria, Jordan, Turkey, and Ukraine.
The Congregation for the Oriental Churches has its origins in the "Congregatio de Propaganda Fide pro negotiis ritus orientalis" founded by Pope Pius IX on 6 January 1862. Included in the Congregation's membership are all Eastern Catholic patriarchs and major archbishops, as well as the President of the Pontifical Council for Promoting Christian Unity. It was formally set up by Pope Benedict XV on 1 May 1917. The title of Prefect was held by the Popes from 1917 until 1967, with the head of the Congregation titled as Secretary. Currently headed by its prefect, Cardinal Leonardo Sandri
The Congregation for Divine Worship and the Discipline of the Sacraments.
The Congregation for Divine Worship and the Discipline of the Sacraments ("Congregatio de Cultu Divino et Disciplina Sacramentorum") is the congregation of the Roman Curia that handles most affairs relating to liturgical practices of the Latin Catholic Church as distinct from the Eastern Catholic Churches and also some technical matters relating to the Sacraments. It is headed by Cardinal Robert Sarah as prefect. It is the direct successor of the "Sacred Congregation for the Discipline of the Sacraments" ("Sacra Congregatio de Disciplina Sacramentorum") (1908–1969).
The Congregation for the Causes of Saints.
The Congregation for the Causes of Saints ("Congregatio de Causis Sanctorum") is the congregation of the Roman Curia which oversees the complex process which leads to the canonization of saints, passing through the steps of a declaration of "heroic virtues" and beatification. After preparing a case, including the approval of miracles, the case is presented to the pope, who decides whether or not to proceed with beatification or canonization. The predecessor of the congregation was the Sacred Congregation for Rites, founded by Pope Sixtus V on 22 January 1588 in the Bull "Immensa Aeterni Dei". The congregation dealt both with regulating divine worship, and the causes of saints.
On 8 May 1969, Pope Paul VI issued the Apostolic Constitution "Sacra Rituum Congregatio", dividing it into two congregations, the "Congregation for the Divine Worship" and one for the causes of saints. The latter was given three offices, those of the judiciary, the Promoter General of the Faith and the historical-juridical. With the changes in the canonization process introduced by Pope John Paul II in 1983, a "College of Relators" was added to prepare the cases of those declared as Servants of God. This office is vacant due to the resignation of Benedict XVI.
The Congregation for the Evangelization of Peoples.
The Congregation for the Evangelization of Peoples ("Congregatio pro Gentium Evangelizatione") is the congregation of the Roman Curia responsible for missionary work and related activities. It is perhaps better known by its former title, the Sacred Congregation for the Propagation of the Faith ("Sacra Congregatio de Propaganda Fide"). Renamed by Pope John Paul II in 1982, its mission continues unbroken. Sr. Luzia Premoli, superior general of the Combonian Missionary Sisters, was appointed a member of the Congregation for the Evangelization of Peoples in 2014, thus becoming the first woman to be appointed a member of a Vatican congregation.
The Congregation for the Clergy.
The Congregation for the Clergy is the congregation of the Roman Curia responsible for overseeing matters regarding priests and deacons not belonging to institutes of consecrated life or societies of apostolic life, as well as for the seminaries (except those regulated by the Congregations for the Evangelization of Peoples and for the Oriental Churches), and houses of formation of religious and secular institutes. The Congregation for the Clergy handles requests for dispensation from active priestly ministry, as well as the legislation governing presbyteral councils and other organizations of priests around the world. The Congregation does not deal with clerical sexual abuse cases, as those are handled exclusively by the Congregation for the Doctrine of the Faith.
The Congregation for Institutes of Consecrated Life and Societies of Apostolic Life.
The Congregation for Institutes of Consecrated Life and Societies of Apostolic Life ("Congregatio pro Institutis Vitae Consecratae et Societatibus Vitae Apostolicae") is the congregation of the Roman Curia responsible for everything which concerns institutes of consecrated life (religious institutes and secular institutes) and societies of apostolic life, both of men and of women, regarding their government, discipline, studies, goods, rights, and privileges.
The Congregation for Catholic Education (in Seminaries and Institutes of Study).
The Congregation for Catholic Education (for Seminaries and Educational Institutions)("Congregatio de Institutione Catholica (de Seminariis atque Studiorum Institutis)") is the Pontifical congregation of the Roman Curia responsible for:
The Congregation for Bishops.
The Congregation for Bishops (Congregatio pro Episcopis) is the congregation of the Roman Curia which oversees the selection of new bishops that are not in mission territories or those areas that come under the jurisdiction of the Congregation for the Oriental Churches who deal with the Eastern Catholics, pending papal approval. It also schedules the papal audiences required quinquennially for bishops and arranges the creation of new dioceses. This office is headed by Cardinal Marc Ouellet, PSS.
The Tribunals.
There are three Tribunals of the Curia: the Sacred Apostolic Penitentiaria, the Sacred Roman Rota, and the Apostolic Signatura.
The Apostolic Penitentiary.
The Apostolic Penitentiary, more formally the Supreme Tribunal of the Apostolic Penitentiary, is one of the three tribunals of the Roman Curia. The Apostolic Penitentiary is chiefly a tribunal of mercy, responsible for issues relating to the forgiveness of sins in the Roman Catholic Church. The Apostolic Penitentiary has jurisdiction only over matters in the internal forum. Its work falls mainly into these categories:
The Supreme Tribunal of the Apostolic Signatura.
The Supreme Tribunal of the Apostolic Signatura is the highest judicial authority in the Catholic Church besides the Pope himself, who is the supreme ecclesiastical judge. In addition, it is an administrative office for matters pertaining to the judicial activity of the whole Church.
Appeals in standard judicial processes, if appealed to the Apostolic See, normally are not handled by the Signatura. Those go to the Roman Rota, which is the ordinary appellate tribunal of the Apostolic See. The Supreme Tribunal handles some of the more specialized kinds of cases, including the following:
Although a Rotal decision can be appealed, if not res judicata, to a different panel ("turnus") of the Rota, there is no right of appeal from a decision of the Signatura, although a complaint of nullity on formal grounds is possible. As an administrative office, it exercises jurisdiction (vigilance) over all the tribunals of the Catholic Church. It can also extend the jurisdiction of tribunals, grant dispensations for procedural laws, establish interdiocesan tribunals, and correct advocates.
The Tribunal of the Rota Romana.
The Tribunal of the Roman Rota is the highest appellate tribunal. While usually trying cases in appeal in third instance (as is normally the case in the Eastern Catholic Churches), or even in second instance if appeal is made to it directly from the sentence of a tribunal of first instance, it is also a court of first instance for cases specified in the law and for others committed to the Rota by the Roman Pontiff. It fosters the unity of jurisprudence and, through its own sentences, is a help to lower tribunals.
The greater Part of its decisions concern the nullity of marriage. In such cases its competence includes marriages between two Catholics, between a Catholic and non-Catholic, and between two non-Catholic parties whether one or both of the baptized parties belongs to the Latin or an Eastern Rite.
The court is named "Rota" (Latin for: wheel) because the judges, called "auditors", originally met in a round room to hear cases.
The Pontifical Councils.
The Pontifical Council for the Laity.
The Pontifical Council for the Laity has the responsibility of assisting the Pope in his dealings with the laity in lay ecclesial movements or individually, and their contributions to the Church. This office is vacant due to the resignation of Benedict XVI.
The Pontifical Council for Promoting Christian Unity.
The Pontifical Council for Promoting Christian Unity is a pontifical council of the Roman Curia dedicated chiefly to the promotion of dialogue and unity with other Christian churches and ecclesial communities, but also, through a closely linked specific commission, to advancing religious relations with Jews.
The Pontifical Council for the Family.
The Pontifical Council for the Family is part of the Curia of the Roman Catholic Church. It was established by Pope John Paul II on 9 May 1981 with the Motu Proprio "Familia a Deo Instituta" and substituted for the Committee for the Family of Pope Paul VI, which had been established in 1973. The Council "promotes the pastoral care of families, protects their rights and dignity in the Church and in civil society, so that they may ever be more able to fulfill their duties."
The Pontifical Council for Justice and Peace.
The Pontifical Council for Justice and Peace is a part of the Roman Curia dedicated to "action-oriented studies" for the international promotion of justice, peace, and human rights from the perspective of the Roman Catholic Church. To this end, it cooperates with various religious institutes and advocacy groups, as well as scholarly, ecumenical, and international organizations.
The Pontifical Council Cor Unum.
The Pontifical Council "Cor Unum" for Human and Christian Development is part of the Curia of the Catholic Church. It was established by Pope Paul VI on 15 July 1971 and is based in the Palazzo San Callisto, in Piazza San Callisto, Rome. Its mission is "the care of the Catholic Church for the needy, thereby encouraging human fellowship and making manifest the charity of Christ", and it undertakes this mission by carrying out humanitarian relief operations following disasters, fostering charity, and encouraging cooperation and coordination of other Catholic organizations.
The Pontifical Council for the Pastoral Care of Migrants and Itinerants.
The Pontifical Council for the Pastoral Care of Migrants and Itinerants (Pontificium Consilium de Spirituali Migrantium atque Itinerantium Cura) is a dicastery of the Roman Curia. The Council, established by Pope John Paul II on 28 June 1988, is dedicated to the spiritual welfare of migrants and itinerant people.
The Pontifical Council for the Pastoral Care of Health Care Workers.
The Pontifical Council for the Pastoral Care of Health Care Workers was set up by the Motu Proprio "Dolentium Hominum" of 11 February 1985, by Pope John Paul II who reformed the Pontifical Commission for the Pastoral Assistance to Health Care Workers into its present form in 1988. This office is vacant due to the resignation of Benedict XVI. The apostolic constitution "Pastor Bonus" describes the work of the council as
The Pontifical Council for Legislative Texts.
The Pontifical Council for Legislative Texts is part of the Roman Curia. Its work "consists mainly in interpreting the laws of the Church". ("Pastor Bonus", 154). This office is vacant due to the resignation of Benedict XVI.
The Pontifical Council for Interreligious Dialogue.
The Pontifical Council for Interreligious Dialogue is the central office of the Catholic Church for the promotion of interreligious dialogue in accordance with the spirit of the Second Vatican Council, in particular the declaration "Nostra aetate". It has the following responsibilities:
The Pontifical Council for Culture.
The Pontifical Council for Culture (Latin: "Pontificium Consilium de Cultura") is a Pontifical Council of the Roman Catholic Church with a mission to oversee the relationship of the Catholic Church with different cultures. The Pontifical Council for Dialogue with Non-Believers was merged with the Pontifical Council for Culture in 1993. On 30 July 2012, Pope Benedict XVI united the Council with the Pontifical Commission for the Cultural Goods of the Church.
The Pontifical Council for Social Communications.
The Pontifical Council for Social Communications ("Pontificium Consilium de Communicationibus Socialibus") is a dicastery of the Roman Curia. Established by Pope John Paul II on 28 June 1988, it is responsible for using the various forms of the media in spreading the Gospel.
The Pontifical Council for Promoting the New Evangelization.
The Pontifical Council for Promoting the New Evangelization is a pontifical council of the Roman Curia dedicated to catechetics and promoting the faith in parts of the world ("the West") where Christianity is well-established but is being affected by secularism.
The Synod of Bishops.
Although not theologically part of the Curia, the Synod of Bishops is a permanent institution established by Pope Paul VI on 15 September 1965 by the motu proprio "Apostolica Sollicitudo" in response to the desire of the Fathers of the Second Vatican Council - expressed in the Decree "Christus Dominus" - to embody the spirit of collegiality engendered by the conciliar experience. It is an advisory body of the Pope, whose members are, for the most part, elected by conferences of bishops around the world. The Pope presides over the synod personally or through others; determines its agenda; has power to convene, conclude, transfer, suspend or dissolve it; and ratifies the election of those who are, by the synod's special law, to be elected to it, and designates and appoints other members (can. 344). Members of the synod express their opinions on matters proposed to it but it is not its function to issue decrees unless the Pope gives it deliberative power in certain cases, in which event its decrees or resolutions must be approved and ratified by him (can. 343). The functioning of the Synod is regulated by the "Ordo Synodi Episcoporum" as amended in 1969, 1971 and 2006. The Synod of Bishops is automatically suspended when the Holy See is vacant.
The offices.
The Holy See's financial authorities are made up of three offices.
The Apostolic Camera.
The Apostolic Camera, or in Latin ("Reverenda") "Camera Apostolica" or "Apostolica Camera", is the central board of finance in the Papal administrative system, which at one time was of great importance in the government of the States of the Church, and in the administration of justice, led by the Camerlengo of the Holy Roman Church.
The Administration of the Patrimony of the Apostolic See.
The Administration of the Patrimony of the Apostolic See is part of the Roman Curia that deals with the "properties owned by the Holy See in order to provide the funds necessary for the Roman Curia to function". (Pastor Bonus, 172). It was established by Pope Paul VI on 15 August 1967. This office is vacant due to the resignation of Benedict XVI. Cardinals Attilio Nicora, Lorenzo Antonetti and Agostino Cacciavillan are former Presidents. It is composed of two sections. The Ordinary Section continues the work of the Administration of the Property of the Holy See, a commission to which Pope Leo XIII entrusted the administration of the property remaining to the Holy See after the complete loss of the Papal States in 1870. The Extraordinary Section administers the funds given by the Italian government to implement the Financial Convention attached to the Lateran Treaty of 1929. These funds were previously managed by the Special Administration of the Holy See.
The Prefecture for the Economic Affairs of the Holy See.
The Prefecture for the Economic Affairs of the Holy See, is an office of the Roman Curia, erected on 15 August 1967, and entrusted with overseeing all the offices of the Holy See that manage finances, regardless of their degree of autonomy. It does not manage finances itself, but instead audits the balance sheets and budgets of the offices that do. It then prepares and publishes annually a general financial report. It must be consulted on all projects of major importance undertaken by the offices in question.
The Pontifical commissions.
The Pontifical Commission for the Cultural Heritage of the Church.
The Pontifical Commission for the Cultural Heritage of the Church is an institution that guards the historical and artistic patrimony of the entire Church which includes works of art, historical documents, books, everything kept in museums as well as the libraries and archives. The commission was established in 1988 by Pope John Paul II.
The Pontifical Commission "Ecclesia Dei".
The Pontifical Commission "Ecclesia Dei" is a commission of the Roman Catholic Church established by Pope John Paul II's "motu proprio" "Ecclesia Dei" of 2 July 1988 for the care of those former followers of Archbishop Marcel Lefebvre who broke with him as a result of his consecration of four priests of his Society of St. Pius X as bishops on 30 June 1988, an act the Holy See deemed illicit and schismatic. On 2 July 2009 this commission was closely linked with the Congregation for the Doctrine of the Faith, whose Prefect is now ex officio President of the commission, which however maintains its separate identity.
The Pontifical Commission for Sacred Archaeology.
The Pontifical Commission for Sacred Archaeology was created by Pius IX (6 January 1852) "to take care of the ancient sacred cemeteries, look after their preventive preservation, further explorations, research and study, and also safeguard the oldest mementos of the early Christian centuries, the outstanding monuments and venerable Basilicas in Rome, in the Roman suburbs and soil, and in the other Dioceses in agreement with the respective Ordinaries". Pius XI made the Commission pontifical and expanded its powers.
The Pontifical Biblical Commission.
The Pontifical Biblical Commission, established 30 October 1902 by Pope Leo XIII, is a consultative body of scholars placed under the authority of the Congregation for the Doctrine of the Faith. The commission's duties include:
The International Theological Commission.
The International Theological Commission (ITC) is a dicastery of the Roman Curia consisting of 30 Catholic theologians from around the world. Its function is to advise the Congregation for the Doctrine of the Faith (CDF) of the Roman Catholic Church. The Prefect of the CDF is "ex officio" the president of the ITC, which is based in Rome.
Interdicasterial Commissions.
An example of a temporary commission set up to deal with a matter involving the work of several departments of the Roman Curia was the Interdicasterial Commission for the Catechism of the Catholic Church instituted in 1993 to prepare the definitive text in Latin of the Catechism of the Catholic Church. Its secretariat was in the building that houses the Congregation for the Doctrine of the Faith. It produced the Latin editio typica of the Catechism four years later, in 1997.
The Annuario Pontificio lists five interdicasterial commissions of longer duration. An example is the Standing Interdicasterial Commission for the Church in Eastern Europe, which replaced the earlier Pontifical Commission for Russia. Set up by Pope John Paul II by a motu proprio of 15 January 1993, it is presided over by the Cardinal Secretary of State and includes also the Secretary and the Undersecretary for Relations with States, and the Secretaries of the Congregations for the Eastern Churches, for the Clergy, and for Institutes of Consecrated Life and Societies of Apostolic Life, and of the Pontifical Council for Promoting Christian Unity.
The Pontifical Commission for Latin America.
The Pontifical Commission for Latin America is a dicastery of the Roman Curia. Established by Pope Pius XII on 19 April 1958, it is charged with providing assistance to and examining matters pertaining to the Church in Latin America. The Commission operates under the auspices of the Congregation for Bishops.
The Pontifical Commission for the Protection of Minors.
The Pontifical Commission for the Protection of Minors (Italian: "Pontificia Commissione per la Tutela dei Minori") was instituted by Pope Francis on 22 March 2014 for the safeguarding of minors. It is headed by Boston's Cardinal Archbishop, Sean P. O'Malley, O.F.M. Cap..
The Swiss Guard.
The "Corps of the Pontifical Swiss Guard" or "Swiss Guard" (Ger: "Schweizergarde", Ital. "Guardia Svizzera Pontificia", Lat. "Pontificia Cohors Helvetica", or "Cohors Pedestris Helvetiorum a Sacra Custodia Pontificis") is a small force responsible for the safety of the Pope since 1506, including the security of the Apostolic Palace and access to the entrances to the Vatican City. Its official language is Swiss German. It serves as the "de facto", if not "de jure", military of the Vatican City. As of 2003, it consists of 134 professional soldiers.
The Labour Office of the Apostolic See.
The Labour Office of the Apostolic See is responsible for labor relations of the Holy See with its employees. The office also settles labor issues which arise. It was instituted by Pope John Paul II on 1 January 1989 by an apostolic letter in the form of a motu proprio.
The pontifical academies.
A pontifical academy is an academic learned society established by or under the direction of the Holy See.
The Pontifical Academy Cultorum Martyrum.
Founded under the title Collegium Cultorum Martyrum, in 1879, the Pontifical Academy of Martyrs promotes devotion to them, enhances and deepens the exact history of the witnesses of the faith, and monuments related to them, from the first centuries of Christianity.
The Pontifical Ecclesiastical Academy.
The Pontifical Ecclesiastical Academy ("Pontificia Academia Ecclesiastica") is dedicated to training priests to serve in the diplomatic corps and the Secretariat of State of the Holy See. The diplomatic service of the Holy See can be traced back to the First Council of Nicaea when Pope Sylvester I sent legates to represent him during the discussions of the council. The present Academy was created as the Pontifical Academy of Ecclesiastical Nobles in 1701 by the abbot Pietro Garagni.
The Pontifical Academy for Life.
The Pontifical Academy for Life is an institution dedicated to promoting the Catholic Church's consistent life ethic. It also does related research on bioethics and Catholic moral theology.
The Pontifical Academy of Sciences.
The Pontifical Academy of Sciences was founded by the Roman Catholic Church in 1936 under its current name by Pope Pius XI and is placed under the protection of the reigning Pope. Its aim is to promote the progress of the mathematical, physical and natural sciences and the study of related epistemological problems. The Academy has its origins in the Accademia dei Lincei ("Academy of Lynxes") established in Rome in 1603, under Pope Clement VIII by the learned Roman prince, Federico Cesi (1585–1630), who was a young botanist and naturalist, and which claimed Galileo Galilei as its president. The current president is the microbiologist Werner Arber. The Academy is headquartered in the Casina Pio IV at the heart of the Vatican Gardens. The academy holds a membership roster of the most respected names in 20th century science, including Stephen Hawking and many Nobel laureates such as Charles Hard Townes.
The Pontifical Academy of Social Sciences.
The Pontifical Academy of Social Sciences was established by Pope John Paul II on 1 January 1994 (AAS 86 [1994], 213), with the aim of promoting the study and progress of the social sciences, primarily economics, sociology, law and political science. The Academy, through an appropriate dialogue, thus offers the Church the elements which she can use in the development of her social doctrine, and reflects on the application of that doctrine in contemporary society. The Academy, which is autonomous, maintains a close relationship with the Pontifical Council for Justice and Peace.

</doc>
<doc id="26569" url="http://en.wikipedia.org/wiki?curid=26569" title="Return to Castle Wolfenstein">
Return to Castle Wolfenstein

Return to Castle Wolfenstein is a first-person shooter video game published by Activision and originally released on November 19, 2001 for Microsoft Windows and subsequently for PlayStation 2, Xbox, Linux and Macintosh. The single player game was developed by Gray Matter Interactive and Nerve Software developed its multiplayer mode. id Software, the creators of "Wolfenstein 3D", oversaw the development and were credited as executive producers. The multiplayer side eventually became the most popular part of the game, and was influential in the genre. Splash Damage, an independently owned game developer in London, created some of the maps for the Game of the Year edition. Splash Damage also developed a downloadable multi-player only sequel called "", which is one of the most popular free downloadable games on the internet. A further sequel, titled "Wolfenstein", was released on August 18, 2009.
Synopsis.
"Return to Castle Wolfenstein" (RTCW) is a reboot of the early first-person shooter "Wolfenstein 3D". It includes a story-based single player campaign (which uses certain themes from the original game), as well as a team-based networked multiplayer mode.
In the campaign, Allied agents from the fictional "Office of Secret Actions" (OSA) are sent to investigate rumors surrounding one of Heinrich Himmler's personal projects, the SS Paranormal Division (also see Ahnenerbe). The agents are, however, captured before completing their mission and are imprisoned in Castle Wolfenstein. Taking the role of Blazkowicz, the player must escape the castle and continue investigating the activities of the SS Paranormal Division, which include research on resurrecting corpses, biotechnology, and secret weapons. During the game the player battles Waffen SS soldiers, elite "Fallschirmjäger" (paratroopers) known as Black Guards, undead creatures, and "Übersoldaten" (supersoldiers) formed from a blend of surgery and chemical engineering conducted by Wilhelm "Deathshead" Strasse. The end boss is an undead Saxon warrior-prince named Heinrich I.
The cable car in the castle is based on the 1968 movie "Where Eagles Dare", where a U.S. Army Brigadier General is captured and taken prisoner to the "Schloß Adler", a fortress high in the Alps above the town of Werfen, only reachable by cable car, and the headquarters of the German Secret Service in southern Bavaria. The supernatural element is based on the story of Castle Wewelsburg, a 17th-century castle occupied by the Germans under Heinrich Himmler's control, and used for occult rituals and practices.
In the German version of the game, it avoids making direct reference to Nazi Party and the "Third Reich", in order to comply with strict laws in Germany. The player is not battling Nazis but a secret sect called the "Wolves" led by Heinrich Höller, whose name is a pun of the original character Himmler (Himmler roughly translates as ""Heaven"er", Höller as ""Hell"er"). The Nazi swastika is also not present, the German forces use a Wolfenstein logo which is a combination of a stylized double-headed eagle prominent in most Nazi symbolism, a "W" (standing for Wolfenstein), and the "QIII" logo (the game engine and network code that RTCW is based upon). The "W" eagle logo is prominently seen on the cover art for the American version (above).
Music pieces such as Moonlight sonata and Für Elise are used in the single player campaign.
The team-based networked multiplayer features different character classes that must work together in order to win. There are four classes — lieutenant, medic, engineer, and soldier — the soldier can be one of several subclasses depending upon the special/heavy weapon that he selects. The multiplayer demo includes a beachhead assault map similar to Omaha Beach.
Story.
While investigating the activities of the SS Paranormal Division in Germany, B.J. Blazkowicz and Agent One are captured by the Nazis. Agent One dies during interrogation, but B.J. manages to escape Castle Wolfenstein's dungeon. He then fights his way out of the castle, using a tram car to leave the area and meet up with a member of the German resistance in a nearby village.
The SS Paranormal Division, under Oberführer Helga von Bulow, has been excavating the catacombs and crypts of an ancient church within the village. The Division's sloppy precautions have led to the awakening of hordes of undead creatures, including Saxon knights, and the entrance must be sealed off, leaving many soldiers trapped inside the catacombs. B.J. descends regardless and fights both Nazis and undead until he arrives at the ancient house of worship, the Defiled Church, where Nazi scientist Professor Zemph is conducting a 'life essence extraction' on the corpse of a Dark Knight. Shortly before B.J.'s arrival, Zemph tries to talk the impatient Helga von Bulow out of retrieving an ancient Thulian artifact, the "Dagger of Warding", but she shoots him and proceeds. This awakens a monster, Olaric, which kills her as well. Blazkowicz defeats Olaric, then is airlifted out with Zemph's notes and the dagger.
One of Germany's leading scientific researchers and Head of the SS Special Projects Division, Wilhelm "Deathshead" Strasse, is preparing to launch an attack on London. He intends to use a V-2 rocket fitted with an experimental germ warhead, launching it from his base near Katamarunde in the Baltics. Blazkowicz is parachuted some distance from the missile base and smuggles himself in on a supply truck. Once inside, Blazkowicz destroys the V-2 on its launchpad and fights his way out of the facility towards an airbase filled with experimental jet aircraft. There, he commandeers a "Kobra" rocket-plane and flies to safety in Malta.
Eager to know more about Deathshead and his secret projects, the Office of Secret Actions (OSA) sends Blazkowicz to the bombed city of Kugelstadt ('Bullet City'), where he is assisted by members of the German Kreisau Circle resistance group in breaking into a ruined factory and exfiltrating a defecting scientist. There he discovers the blueprints for the Reich's latest weapon, the Venom Gun, an electrically operated hand-held minigun. He also procures the weapon itself. Blazkowicz eventually breaks into Deathshead's underground research complex, the Secret Weapons Facility (SWF). There he encounters horrific creatures, malformed and twisted through surgery and mechanical implants. The creatures escape and go on a rampage. Blazkowicz sees Deathshead escape the SWF by U-Boat, and learns of his destination by interrogating a captured German officer.
Blazkowicz is then parachuted into Norway, close to Deathshead's mysterious X-Labs. After breaking into the facility, which has been overrun by the twisted creatures he encountered in Kugelstadt (dubbed 'Lopers'), Blazkowicz retrieves Deathshead's journal. He then confronts several prototype Übersoldaten, towering monstrosities coated in armor, powered by hydraulic legs and carrying powerful fixed weapons. Finally, he destroys one of Deathshead's completed super soldier, Übersoldaten, and kills the researchers who have developed it. Deathshead himself escapes in a Kobra rocket-plane and does not appear in the game again.
After studying the documents captured by Blazkowicz, the OSA has become aware of a scheme codenamed 'Operation: Resurrection', a plan to resurrect Heinrich I, a legendary and powerful Saxon warlock-king. Despite the skepticism of senior Allied commanders, the OSA parachutes Blazkowicz close to Castle Wolfenstein itself, at the Bramburg Dam, where he fights his way until he arrives at the town of Paderborn. After assassinating all the senior officers of the SS Paranormal Division present there for the resurrection, Blazkowicz fights his way through Chateau Schufstaffel and into the grounds beyond. After fighting two more Übersoldaten, Blazkowicz enters an excavation site near Castle Wolfenstein.
Inside the excavation site, Blazkowicz fights Nazi guards and prototype Übersoldaten, and makes his way to a boarded-up entrance to Castle Wolfenstein's crypts. There, he finds that the ruined part of the castle has become infested with undead creatures, which are attacking the castle's desperate garrison. After fighting his way through the castle, Blazkowicz arrives too late at the site of a dark ceremony to resurrect Heinrich I. At the ceremony, SS Psychic Marianna Blavatsky conjures up dark spirits, which transform three Übersoldaten into Dark Knights, Heinrich's lieutenants. She ultimately raises Heinrich I, who "thanks" her by turning her into his undead slave. In a climactic battle, Blazkowicz destroys the three Dark Knights, the undead Marianna Blavatsky, and Heinrich I. The SS chief Heinrich Himmler watches in horror, remarking afterwards "This American... he has ruined everything" as he leaves for Berlin.
Back in the OSA, Operation Resurrection is closed and Blazkowicz is off on some "R&R" — shooting Nazis.
Multiplayer.
Wolfenstein MultiPlayer (MP) is an objective game mode, in which players are split into two teams - Axis and Allies. Each team has a set of objectives to complete, the Allies usually being to destroy some sort of Axis advantage, and the Axis objectives being to defend their object(s). These objectives are split into two categories, primary and secondary. Primary objectives are ones which must be completed for victory, generally stealing secret documents or destroying a radar array; however secondary objectives are ones which are optional - they do not have to be completed, but if they are they may aid the appropriate team, such as blowing out a door to allow access into a tunnel which shortens travel time or allows less-noticeable infiltration of the enemy base.
Each team has access to a slightly different set of weapons, matching those used by each side in World War II. Players can choose from four different classes: Soldier, Medic, Lieutenant and Engineer.
Each class specializes in a certain aspect of the game, and an effective team will balance players out using all four classes, such as a soldier for blasting through enemy defences, a medic for supporting the team and keeping them alive (Soldier making up for the lack of firepower with medics, medics making up for the lack of health), a Lieutenant to resupply teammates with ammo (especially soldiers) and engineers to complete the objective, having their way cleared by the soldier which is then supported by the Lieutenant.
Gameplay modes.
There are three different modes of play in Wolf MP, each allowing for a different experience - general multiplayer deathmatch, stopwatch, and checkpoint. Stopwatch calls for the Allied side to complete a set of objectives within a predefined time limit. The opposing team then become the Allies and have to complete the objectives in a shorter time than the now Axis. Checkpoint gamemode is a mode in which teams capture flags. It may be more commonly known as Capture the Flag (CTF). Whichever team is first to control all the flags at once, wins.
Development.
The game is powered by a heavily modified version of the "Quake III: Team Arena" engine. The Return to Castle Wolfenstein engine was subsequently used as the foundation for "" (Splash Damage/Activision), "Trinity" (Gray Matter Interactive/Activision) (shown at E3 in 2004, but canceled shortly after) and "Call of Duty" (Infinity Ward/Activision).
There are many different releases of Return to Castle Wolfenstein. The original release, version 1.0, came in a game box featuring a book-like flap. A Collector's Edition, packaged in a metal case, was released at the same time. The contents of the Collector's Edition changed depending on when it was purchased and could include a poster and fabric patch, a poster and a bonus CD, or just the bonus CD. The Game of the Year Edition (2002 - v.1.33) came with the original Wolfenstein 3D, game demos, and seven new multiplayer maps (Trenchtoast, Tram Siege, Ice, Chateau, Keep, The Damned, and Rocket.) The Platinum Edition (2004 - v.1.41) included "", a stand-alone multiplayer expansion, and Wolfenstein 3D. Return to Castle Wolfenstein: Tides of War also came with the original Wolfenstein 3D as an unlockable after beating the campaign, and included some enhancements like surround sound.
Ports.
The game was released for the Linux and Macintosh platforms in 2002, with the Linux port done internally by Timothee Besset and the Mac port done by Aspyr Media. In 2003, the game was ported to the PlayStation 2 and Xbox video game consoles and subtitled as Operation Resurrection and Tides of War, respectively. Since the official source code release, the game has also been ported to Android under the name RTCW4A (the data files from the original game are required to play).
Console version differences.
Both console versions include an additional single player prequel mission, set in the fictional town of Ras El-Hadid in Egypt. The latter half of the level features an extensive underground burial site with many undead enemies, as does the original first mission. This prequel level is likely closer to the developers' true intentions for the story, as indicated by the distinctly Egyptian design of the burial site, including the presence of sand, traps, mummies and hieroglyphs on the walls in some areas (in the original storyline, this site is found in the middle of a German village during the second mission). The PS2 version has a bonus feature which allows you to purchase items at the end of each level by finding secrets. In the Xbox version a Secret Bonus is awarded after every level when all the secret areas for that level have been found. It also has several new equipable items and weapons as well as new enemies. The two player co-op mode is exclusive to Xbox and allows the second player to play as Agent One. The Xbox version has downloadable content, system link play and had online multiplayer via Xbox Live before Live play was disabled for original Xbox games. A Platinum Hits edition of the game was also released for the Xbox. The PlayStation 2 version does not support online multiplayer.
Sequels.
A multiplayer-only spinoff of the series, "Wolfenstein: Enemy Territory", was originally planned as a full-fledged expansion pack for "Return to Castle Wolfenstein" developed by Splash Damage. The single player component of the game was never completed and thus was removed entirely. The developers at that point decided the multiplayer part would be released as a free, downloadable standalone game. "Enemy Territory" is a team-based networked multiplayer game which involves completing objectives through teamwork using various character classes. As of late 2011, "Enemy Territory" remains a popular game.
This gameplay was also later reutilzied in a full-fledged commercial game "" set in id Software's "Quake" universe.
A sequel called "Wolfenstein" was developed by "Raven Software" and "id Software" and published by "Activision", and released on August 18, 2009.
Source code release.
The source code for "Return to Castle Wolfenstein" and "Enemy Territory" was released under the GNU General Public License (GPL) on August 12, 2010. The "ioquake3" developers at icculus.org announced the start of respective engine projects soon after.
Film.
A "Return to Castle Wolfenstein" film was announced in 2002 with Rob Cohen of xXx attached to direct. Little information has been available since, however, with the exception of a July 20, 2005 "IGN" interview. The interview discussed the "Return to Castle Wolfenstein" film with id employees. In the interview, Todd Hollenshead indicated that the movie was in the works, though still in the early stages.
On August 3, 2007, "Variety" confirmed "Return to Castle Wolfenstein", to be written and directed by Roger Avary and produced by Samuel Hadida. On November 2, 2012, Roger Avary has signed on to write and direct the film. The film is being described as a mix of "Inglourious Basterds" and "".
Reception.
"Return to Castle Wolfenstein" received favorable reviews from critics. At Metacritic, it scores 88/100 (based on 32 reviews), and on GameRankings it scores 86.75% (based on 50 reviews). "Eurogamer"'s Tom Bramwell called "Return to Castle Wolfenstein" "a worthy addition to the stable of id Software affiliated shoot 'em ups. The single player game is average to good and takes quite a while to finish, but the game really earns its salt by shipping with a first class multiplayer element."
Controversy.
In March 2008, the United States Department of State published a report to Congress, "Contemporary Global Anti-Semitism", that described "Return to Castle Wolfenstein" as an "anti-Semitic video game" with no qualifications. The report picked up on an article originally written in 2002 by Jonathan Kay of the New York Times regarding the recent introduction of "Nazi protagonists" in the online gaming market (referring specifically to "Day of Defeat" and Wolfenstein). The article was published just 19 days before "" was released which shares many similar features, and the Nazi protagonists in multiplayer.
Todd Hollenshead, chief executive of id Software at the time of the original article stated: "The trend you're seeing with new games is, to some extent, a reflection of what's going in the culture ... For instance, you've now got games with terrorists and counterterrorists. And World War II games such as "Return to Castle Wolfenstein" and "Day of Defeat" reflect what you see in popular movies... I don't doubt there are going to be people that go out and distort what the multiplayer gaming experience is and say, 'Oh, I can't believe you guys did this.' There are a lot of critics of the game industry, and they look for things to criticize."
Awards.
The game was nominated at 2002 Game Developers Choice Awards in the "Excellence in Programming" category (Sherman Archibald, John Carmack, and Ryan Feltrin).

</doc>
<doc id="26678" url="http://en.wikipedia.org/wiki?curid=26678" title="Star Wars">
Star Wars

Star Wars is an American epic space opera franchise centered on a film series created by George Lucas. The franchise depicts a galaxy described as "far, far away" in the distant past, and portrays Jedi as a representation of good, in conflict with the Sith, their evil counterpart. Their weapon of choice, the lightsaber, is commonly recognized in popular culture. The franchise's storylines contain many themes, with influences from philosophy and religion.
The first film in the series, "Star Wars", was released ""on May 25, 1977, by 20th Century Fox and became a worldwide pop culture phenomenon. It was followed by two sequels released at three-year intervals. Sixteen years later, the first in a new prequel trilogy of films was released; the final prequel was released on May 19, 2005. ""Reaction to the original trilogy was positive, while the prequel trilogy received a more mixed reaction. All six films were nominated for or won Academy Awards, and all were box office successes; the overall box office revenue generated totals $4.38 billion, making "Star Wars" the fifth-highest-grossing film series. The series has spawned an extensive media franchise "–" the Expanded Universe – including books, television series, computer and video games, and comic books, resulting in significant development of the series's fictional universe.
In 2012, The Walt Disney Company acquired Lucasfilm for $4.05 billion and announced three new "Star Wars" films, with the first film, "", planned for release in 2015. 20th Century Fox retains the physical distribution rights to the first two "Star Wars" trilogies, owning permanent rights for the original film "Episode IV: A New Hope" and holding the rights to "Episodes I"–"III", "V" and "VI" until May 2020. The Walt Disney Studios owns digital distribution rights to all the "Star Wars" films, excluding "A New Hope".
Setting.
The events depicted in "Star Wars" media take place in a fictional galaxy. Many species of alien creatures (often humanoid) are depicted. Robotic droids are also commonplace and are generally built to serve their owners. Space travel is common, and many planets in the galaxy are members of a Galactic Republic, later reorganized as the Galactic Empire.
One of the prominent elements of "Star Wars" is the "Force", an omnipresent energy that can be harnessed by those with that ability, known as Force-sensitives. It is described in the first produced film as "an energy field created by all living things [that] surrounds us, penetrates us, [and] binds the galaxy together." The Force allows users to perform various supernatural feats (such as telekinesis, clairvoyance, precognition, and mind control) and can amplify certain physical traits, such as speed and reflexes; these abilities vary between characters and can be improved through training. While the Force can be used for good, it has a dark side that, when pursued, imbues users with hatred, aggression, and malevolence.
The six films feature the Jedi, who use the Force for good, and the Sith, who use the dark side for evil in an attempt to take over the galaxy. In the "Star Wars" Expanded Universe, many dark side users are Dark Jedi rather than Sith, mainly because of the "" (see Sith Origin).
Theatrical films.
The film series began with "Star Wars", released on May 25, 1977. This was followed by two sequels: "The Empire Strikes Back", released on May 21, 1980, and "Return of the Jedi", released on May 25, 1983. The opening crawl of the sequels disclosed that they were numbered as "Episode V" and "Episode VI" respectively, though the films were generally advertised solely under their subtitles. Though the first film in the series was simply titled "Star Wars", with its 1981 re-release it had the subtitle "Episode IV: A New Hope" added to remain consistent with its sequel, and to establish it as the middle chapter of a continuing saga.
In 1997, to correspond with the 20th anniversary of "A New Hope", Lucas released a "Special Edition" of the "Star Wars" trilogy to theaters. The re-release featured alterations to the three films, primarily motivated by the improvement of CGI and other special effects technologies, which allowed visuals that were not possible to achieve at the time of the original filmmaking. Lucas continued to make changes to the films for subsequent releases, such as the first ever DVD release of the original trilogy on September 21, 2004 and the first ever Blu-ray release of all six films on September 16, 2011. Reception of the "Special Edition" was mixed, prompting petitions and fan edits to produce restored copies of the original trilogy.
More than two decades after the release of the original film, the series continued with a prequel trilogy; consisting of ', released on May 19, 1999; ', released on May 16, 2002; and ', released on May 19, 2005. On August 15, 2008, ' was released theatrically as a lead-in to the weekly . "" is scheduled for release on December 18, 2015. In 2013, it was announced the original "Star Wars" film will be the first Hollywood film to be dubbed into Navajo.
Plot overview.
The original trilogy begins with the Galactic Empire nearing completion of the Death Star space station, which will allow the Empire to crush the Rebel Alliance, an organized resistance formed to combat Emperor Palpatine's tyranny. Palpatine's Sith apprentice Darth Vader captures Princess Leia, a member of the rebellion who has stolen the plans to the Death Star and hidden them in the astromech droid R2-D2. R2, along with his protocol droid counterpart C-3PO, escapes to the desert planet Tatooine. There, the droids are purchased by farm boy Luke Skywalker and his step-uncle and aunt. While Luke is cleaning R2, he accidentally triggers a message put into the droid by Leia, who asks for assistance from the legendary Jedi Knight Obi-Wan Kenobi. Luke later assists the droids in finding the exiled Jedi, who is now passing as an old hermit under the alias Ben Kenobi. When Luke asks about his father, whom he has never met, Obi-Wan tells him that Anakin Skywalker was a great Jedi who was betrayed and murdered by Vader. Obi-Wan and Luke hire the smuggler Han Solo and his Wookiee co-pilot Chewbacca to take them to Alderaan, Leia's home world, which they eventually find has been destroyed by the Death Star. Once on board the space station, Luke and Han rescue Leia while Obi-Wan allows himself to be killed during a lightsaber duel with Vader; his sacrifice allows the group to escape with the plans that help the Rebels destroy the Death Star. Luke himself (guided by the power of the Force) fires the shot that destroys the deadly space station during the Battle of Yavin.
Three years later, Luke travels to find the Jedi Master Yoda, now living in exile on the swamp-infested world of Dagobah, to begin his Jedi training. However, Luke's training is interrupted when Vader lures him into a trap by capturing Han and his friends at Cloud City. During a fierce lightsaber duel, Vader reveals that he is Luke's father and attempts to turn him to the dark side of the Force. Luke escapes and, after rescuing Han from the gangster Jabba the Hutt, returns to Yoda to complete his training, only to find the 900-year-old Jedi Master on his deathbed. Before he dies, Yoda confirms that Vader is Luke's father. Moments later, Obi-Wan's spirit tells Luke that he must confront his father once again before he can become a Jedi, and that Leia is his twin sister.
As the Rebels attack the second Death Star, Luke engages Vader in another lightsaber duel as the Emperor watches; both Sith Lords intend to turn Luke to the dark side and take him as their apprentice. During the duel, Luke succumbs to his anger and brutally overpowers Vader, but controls himself at the last minute; realizing that he is about to suffer his father's fate, he spares Vader's life and proudly declares his allegiance to the Jedi. An enraged Palpatine then attempts to kill Luke with Force lightning, a sight that moves Vader to turn and kill the Emperor, suffering mortal wounds in the process. Redeemed, Anakin Skywalker dies in his son's arms. Luke becomes a full-fledged Jedi, and the Rebels destroy the second Death Star.
The prequel trilogy begins (32 years before the original film) with the corrupt Trade Federation setting up a blockade of battleships around the planet Naboo. The Sith Lord Darth Sidious had secretly planned the blockade to give his alter ego, Senator Palpatine, a pretense to overthrow and replace the Supreme Chancellor of the Galactic Republic. At the Chancellor's request, the Jedi Knight Qui-Gon Jinn and his apprentice, a younger Obi-Wan Kenobi, are sent to Naboo to negotiate with the Federation. However, the two Jedi are forced to instead help the Queen of Naboo, Padmé Amidala, escape from the blockade and plea her planet's crisis before the Republic Senate on Coruscant. When their starship is damaged during the escape, they land on Tatooine for repairs, where Qui-Gon discovers a nine-year-old Anakin Skywalker. Qui-Gon comes to believe that Anakin is the "Chosen One" foretold by Jedi prophecy to bring balance to the Force, and he helps liberate the boy from slavery. The Jedi Council, led by Yoda, reluctantly allows Obi-Wan to train Anakin after Qui-Gon is killed by Palpatine's first apprentice, Darth Maul, during the Battle of Naboo.
The remainder of the prequel trilogy chronicles Anakin's gradual descent to the dark side as he fights in the Clone Wars, which Palpatine secretly engineers to destroy the Jedi Order and lure Anakin into his service. Anakin and Padmé fall in love and secretly wed, and eventually Padmé becomes pregnant. Anakin has a prophetic vision of Padmé dying in childbirth, and Palpatine convinces him that the dark side of the Force holds the power to save her life. Desperate, Anakin submits to Palpatine's Sith teachings and is renamed Darth Vader.
While Palpatine re-organizes the Republic into the tyrannical Empire, Vader participates in the extermination of the Jedi Order, culminating in a lightsaber duel between himself and Obi-Wan on the volcanic planet Mustafar. Obi-Wan defeats his former apprentice and friend, severing his limbs and leaving him to burn to death on the shores of a lava flow. Palpatine arrives shortly afterward and saves Vader by placing him into a mechanical black mask and suit of armor that serves as a permanent life support system. At the same time, Padmé dies while giving birth to twins Luke and Leia. Obi-Wan and Yoda, now the only remaining Jedi alive, agree to separate the twins and keep them hidden from both Vader and the Emperor, until the time comes when Anakin's children can be used to help overthrow the Empire.
Themes.
Aside from its well known science fictional technology, "Star Wars" features elements such as knighthood, chivalry, and princesses that are related to archetypes of the fantasy genre. The "Star Wars" world, unlike fantasy and science-fiction films that featured sleek and futuristic settings, was portrayed as dirty and grimy. Lucas' vision of a "used future" was further popularized in the science fiction-horror films "Alien", which was set on a dirty space freighter; "Mad Max 2", which is set in a post-apocalyptic desert; and "Blade Runner", which is set in a crumbling, dirty city of the future. Lucas made a conscious effort to parallel scenes and dialogue between films, and especially to parallel the journeys of Luke Skywalker with that of his father Anakin when making the prequels.
"Star Wars" contains many themes of political science that mainly favor democracy over dictatorship. Political science has been an important element of "Star Wars" since the franchise first launched in 1977. The plot climax of "Star Wars" is modeled after the fall of the democratic Roman Republic and the formation of an empire. "Star Wars" also reflects on the events in America following the September 11 attacks. Some have drawn similarities between the rise in authoritarianism from around the beginning of "Clone Wars" until the end of the Old Republic and the United States government's actions after 9/11, specifically passage of the Patriot Act in 2001.
Technical information.
All six films of the "Star Wars" series were shot in an aspect ratio of 2.39:1. The original trilogy was shot with anamorphic lenses. Episodes IV and V were shot in Panavision, while Episode VI was shot in Joe Dunton Camera (JDC) scope. Episode I was shot with Hawk anamorphic lenses on Arriflex cameras, and Episodes II and III were shot with Sony's CineAlta high-definition digital cameras.
Lucas hired Ben Burtt to oversee the sound effects on "A New Hope". Burtt's accomplishment was such that the Academy of Motion Picture Arts and Sciences presented him with a Special Achievement Award because it had no award at the time for the work he had done. Lucasfilm developed the THX sound reproduction standard for "Return of the Jedi". John Williams composed the scores for all six films. Lucas' design for "Star Wars" involved a grand musical sound, with leitmotifs for different characters and important concepts. Williams' "Star Wars" title theme has become one of the most famous and well-known musical compositions in modern music history.
Lucas hired 'the Dean of Special Effects' John Stears, who created R2-D2, Luke Skywalker's Landspeeder, the Jedi Knights' lightsabers, and the Death Star. The technical lightsaber choreography for the original trilogy was developed by leading filmmaking sword-master Bob Anderson. Anderson trained actor Mark Hamill (Luke Skywalker) and performed all the sword stunts as Darth Vader during the lightsaber duels in "The Empire Strikes Back" and "Return of the Jedi", wearing Vader's costume. Anderson's role in the original "Star Wars" trilogy was highlighted in the film "Reclaiming the Blade", where he shares his experiences as the fight choreographer developing the lightsaber techniques for the movies.
Production history.
Original trilogy.
In 1971, Universal Studios agreed to make "American Graffiti" and "Star Wars" in a two-picture contract, although "Star Wars" was later rejected in its early concept stages. "American Graffiti" was completed in 1973 and, a few months later, Lucas wrote a short summary called "The Journal of the Whills", which told the tale of the training of apprentice CJ Thorpe as a "Jedi-Bendu" space commando by the legendary Mace Windy. Frustrated that his story was too difficult to understand, Lucas then began writing a 13-page treatment called "The Star Wars" on April 17, 1973, which had thematic parallels with Akira Kurosawa's "The Hidden Fortress". By 1974, he had expanded the treatment into a rough draft screenplay, adding elements such as the Sith, the Death Star, and a protagonist named Annikin Starkiller.
For the second draft, Lucas made heavy simplifications, and introduced the young hero on a farm as Luke Starkiller. Annikin became Luke's father, a wise Jedi knight. "The Force" was also introduced as a mystical energy field. The next draft removed the father character and replaced him with a substitute named Ben Kenobi, and in 1976 a fourth draft had been prepared for principal photography. The film was titled "Adventures of Luke Starkiller, as taken from the Journal of the Whills, Saga I: The Star Wars". During production, Lucas changed Luke's name to Skywalker and altered the title to simply "The Star Wars" and finally "Star Wars".
At that point, Lucas was not expecting the film to become part of a series. The fourth draft of the script underwent subtle changes that made it more satisfying as a self-contained film, ending with the destruction of the Empire itself by way of destroying the Death Star. However, Lucas "had" previously conceived of the film as the first in a series of adventures. Later, he realized the film would not in fact be the first in the sequence, but a film in the second trilogy in the saga. This is stated explicitly in George Lucas' preface to the 1994 reissue of "Splinter of the Mind's Eye":
It wasn't long after I began writing "Star Wars" that I realized the story was more than a single film could hold. As the saga of the Skywalkers and Jedi Knights unfolded, I began to see it as a tale that could take at least nine films to tell—three trilogies—and I realized, in making my way through the back story and after story, that I was really setting out to write the middle story.
The second draft contained a teaser for a never-made sequel about "The Princess of Ondos," and by the time of the third draft some months later Lucas had negotiated a contract that gave him rights to make two sequels. Not long after, Lucas met with author Alan Dean Foster, and hired him to write these two sequels as novels. The intention was that if "Star Wars" were successful, Lucas could adapt the novels into screenplays. He had also by that point developed an elaborate backstory to aid his writing process.
When "Star Wars" proved successful, Lucas decided to use the film as the basis for an elaborate serial, although at one point he considered walking away from the series altogether. However, Lucas wanted to create an independent filmmaking center—what would become Skywalker Ranch—and saw an opportunity to use the series as a financing agent. Alan Dean Foster had already begun writing the first sequel novel, but Lucas decided to abandon his plan to adapt Foster's work; the book was released as "Splinter of the Mind's Eye" the following year. At first Lucas envisioned a series of films with no set number of entries, like the James Bond series. In an interview with "Rolling Stone" in August 1977, he said that he wanted his friends to each take a turn at directing the films and giving unique interpretations on the series. He also said that the backstory in which Darth Vader turns to the dark side, kills Luke's father and fights Ben Kenobi on a volcano as the Galactic Republic falls would make an excellent sequel.
Later that year, Lucas hired science fiction author Leigh Brackett to write "Star Wars II" with him. They held story conferences and, by late November 1977, Lucas had produced a handwritten treatment called "The Empire Strikes Back". The treatment is similar to the final film, except that Darth Vader does not reveal he is Luke's father. In the first draft that Brackett would write from this, Luke's father appears as a ghost to instruct Luke.
Brackett finished her first draft in early 1978; Lucas has said he was disappointed with it, but before he could discuss it with her, she died of cancer. With no writer available, Lucas had to write his next draft himself. It was this draft in which Lucas first made use of the "Episode" numbering for the films; "Empire Strikes Back" was listed as "Episode II". As Michael Kaminski argues in "The Secret History of Star Wars", the disappointment with the first draft probably made Lucas consider different directions in which to take the story. He made use of a new plot twist: Darth Vader claims to be Luke's father. According to Lucas, he found this draft enjoyable to write, as opposed to the yearlong struggles writing the first film, and quickly wrote two more drafts, both in April 1978. He also took the script to a darker extreme by having Han Solo imprisoned in carbonite and left in limbo.
This new story point of Darth Vader being Luke's father had drastic effects on the series. Michael Kaminski argues in his book that it is unlikely that the plot point had ever seriously been considered or even conceived of before 1978, and that the first film was clearly operating under an alternate storyline where Vader was separate from Luke's father; there is not a single reference to this plot point before 1978. After writing the second and third drafts of "Empire Strikes Back" in which the point was introduced, Lucas reviewed the new backstory he had created: Anakin Skywalker was Ben Kenobi's brilliant student and had a child named Luke, but was swayed to the dark side by Emperor Palpatine (who became a Sith and not simply a politician). Anakin battled Ben Kenobi on the site of a volcano and was wounded, but then resurrected as Darth Vader. Meanwhile Kenobi hid Luke on Tatooine while the Republic became the Empire and Vader systematically hunted down and killed the Jedi.
With this new backstory in place, Lucas decided that the series would be a trilogy, changing "Empire Strikes Back" from "Episode II" to "Episode V" in the next draft. Lawrence Kasdan, who had just completed writing "Raiders of the Lost Ark", was then hired to write the next drafts, and was given additional input from director Irvin Kershner. Kasdan, Kershner, and producer Gary Kurtz saw the film as a more serious and adult film, which was helped by the new, darker storyline, and developed the series from the light adventure roots of the first film.
By the time he began writing "Episode VI" in 1981 (then titled "Revenge of the Jedi"), much had changed. Making "Empire Strikes Back" was stressful and costly, and Lucas' personal life was disintegrating. Burned out and not wanting to make any more "Star Wars" films, he vowed that he was done with the series in a May 1983 interview with "Time" magazine. Lucas' 1981 rough drafts had Darth Vader competing with the Emperor for possession of Luke—and in the second script, the "revised rough draft", Vader became a sympathetic character. Lawrence Kasdan was hired to take over once again and, in these final drafts, Vader was explicitly redeemed and finally unmasked. This change in character would provide a springboard to the "Tragedy of Darth Vader" storyline that underlies the prequels.
Prequel trilogy.
After losing much of his fortune in a divorce settlement in 1987, Lucas had no desire to return to "Star Wars", and had unofficially canceled his sequel trilogy by the time of "Return of the Jedi". Nevertheless, the prequels, which were only still a series of basic ideas partially pulled from his original drafts of "The Star Wars" continued to fascinate him with the possibilities of technical advances would make it possible to revisit his 20-year-old material. After "Star Wars" became popular once again, in the wake of Dark Horse's comic book line and Timothy Zahn's trilogy of novels, Lucas saw that there was still a large audience. His children were older, and with the explosion of CGI technology he was now considering returning to directing. By 1993 it was announced, in "Variety" among other sources, that he would be making the prequels. He began penning more to the story, now indicating the series would be a tragic one examining Anakin Skywalker's fall to the dark side. Lucas also began to change how the prequels would exist relative to the originals; at first they were supposed to be a "filling-in" of history tangential to the originals, but now he saw that they could form the beginning of one long story that started with Anakin's childhood and ended with his death. This was the final step towards turning the film series into a "Saga".
In 1994, Lucas finally had his first screenplay titled "Episode I: The Beginning". Following the release of that film, Lucas announced that he would also be directing the next two, and began working on "Episode II" at that time. The first draft of "Episode II" was completed just weeks before principal photography, and Lucas hired Jonathan Hales, a writer from "The Young Indiana Jones Chronicles", to polish it. Unsure of a title, Lucas had jokingly called the film "Jar Jar's Great Adventure." In writing "The Empire Strikes Back", Lucas initially decided that Lando Calrissian was a clone and came from a planet of clones which caused the "Clone Wars" mentioned by Princess Leia in "A New Hope"; he later came up with an alternate concept of an army of clone shocktroopers from a remote planet which attacked the Republic and were repelled by the Jedi. The basic elements of that backstory became the plot basis for "Episode II", with the new wrinkle added that Palpatine secretly orchestrated the crisis.
Lucas began working on "Episode III" before "Attack of the Clones" was released, offering concept artists that the film would open with a montage of seven Clone War battles. As he reviewed the storyline that summer, however, he says he radically re-organized the plot. Michael Kaminski, in "The Secret History of Star Wars", offers evidence that issues in Anakin's fall to the dark side prompted Lucas to make massive story changes, first revising the opening sequence to have Palpatine kidnapped and his apprentice, Count Dooku, murdered by Anakin as the first act in the latter's turn towards the dark side. After principal photography was complete in 2003, Lucas made even more massive changes in Anakin's character, re-writing his entire turn to the dark side; he would now turn primarily in a quest to save Padmé's life, rather than the previous version in which that reason was one of several, including that he genuinely believed that the Jedi were evil and plotting to take over the Republic. This fundamental re-write was accomplished both through editing the principal footage, and new and revised scenes filmed during pick-ups in 2004.
Lucas often exaggerated the amount of material he wrote for the series; much of it stemmed from the post‐1978 period when the series grew into a phenomenon. Michael Kaminski explained that these exaggerations were both a publicity and security measure. Kaminski rationalized that since the series' story radically changed throughout the years, it was always Lucas' intention to change the original story retroactively because audiences would only view the material from his perspective. When congratulating the producers of the TV series "Lost" in 2010, Lucas himself jokingly admitted, "when "Star Wars" first came out, I didn't know where it was going either. The trick is to pretend you've planned the whole thing out in advance. Throw in some father issues and references to other stories – let's call them homages – and you've got a series".
Sequel trilogy.
A sequel trilogy was reportedly planned (Episodes "VII", "VIII" and "IX") by Lucasfilm as a sequel to the original "Star Wars" trilogy (Episodes "IV", "V" and "VI"), released between 1977 and 1983. While the similarly discussed "Star Wars" prequel trilogy (Episodes "I", "II" and "III") was ultimately released between 1999 and 2005, Lucasfilm and George Lucas had for many years denied plans for a sequel trilogy, insisting that "Star Wars" is meant to be a six-part series. In  2008 (2008-), speaking about the upcoming "", Lucas maintained his status on the sequel trilogy: "I get asked all the time, 'What happens after "Return of the Jedi"?,' and there really is no answer for that. The movies were the story of Anakin Skywalker and Luke Skywalker, and when Luke saves the galaxy and redeems his father, that's where that story ends."
In January 2012, Lucas announced that he would step away from blockbuster films and instead produce smaller arthouse films. Asked whether the criticism he received following the prequel trilogy and the alterations to the original trilogy had influenced his decision to retire, Lucas said: "Why would I make any more when everybody yells at you all the time and says what a terrible person you are?"
Despite insisting that a sequel trilogy would never happen, George Lucas began working on story treatments for three new Star Wars films in 2011. In October 2012, The Walt Disney Company agreed to buy Lucasfilm and announced that "Star Wars Episode VII" would be released in 2015. Later, it was revealed that the three new upcoming films (Episodes VII-IX) would be based on story treatments that had been written by George Lucas prior to the sale of Lucasfilm. The co-chairman of Lucasfilm, Kathleen Kennedy became president of the company, reporting to Walt Disney Studios chairman Alan Horn. In addition, Kennedy will serve as executive producer on new "Star Wars" feature films, with franchise creator and Lucasfilm founder Lucas serving as creative consultant. The screenplay for "Episode VII" was originally set to be written by Michael Arndt, but in October 2013 it was announced that writing duties would be taken over by Lawrence Kasdan and J. J. Abrams. On January 25, 2013, The Walt Disney Studios and Lucasfilm officially announced J. J. Abrams as "Star Wars Episode VII"‍ '​s director and producer, along with Bryan Burk and Bad Robot Productions.
On November 20, 2012, "The Hollywood Reporter" reported that Lawrence Kasdan, writer of "The Empire Strikes Back" and "Return of the Jedi", and Simon Kinberg will write and produce "Episodes VIII" and "IX". Kasdan and Kinberg were later confirmed as creative consultants on those films, in addition to writing stand-alone films. In addition, John Williams, who wrote the music for the previous six episodes, has been hired to compose the music for Episodes "VII", "VIII" and "IX".
On June 21, 2014 it was reported that "Looper" director Rian Johnson would direct "Episode VIII" with Ram Bergman as a producer. Reports initially claimed he would direct "Episode IX" as well, but it was later revealed he would only write a story treatment for "Episode IX". When asked about "Episode VIII" in an August 2014 interview, Johnson said "it's boring to talk about, because the only thing I can really say is, I'm just happy. I don't have the terror I kind of expected I would, at least not yet. I'm sure I will at some point." It is scheduled to be released on May 26, 2017. J. J. Abrams will serve as executive producer.
Anthology series.
On February 5, 2013, Disney CEO Bob Iger confirmed the development of two stand-alone films, each individually written by Lawrence Kasdan and Simon Kinberg. On February 6, "Entertainment Weekly" reported that Disney is working on two films featuring Han Solo and Boba Fett. Disney CFO Jay Rasulo has described the stand-alone films as origin stories. Kathleen Kennedy explained that the stand-alone films will not crossover with the films of the sequel trilogy, stating, "George was so clear as to how that works. The canon that he created was the "Star Wars" saga. Right now, "Episode VII" falls within that canon. The spin-off movies, or we may come up with some other way to call those films, they exist within that vast universe that he created. There is no attempt being made to carry characters (from the stand-alone films) in and out of the saga episodes. Consequently, from the creative standpoint, it's a roadmap that George made pretty clear." In April 2015, Lucasfilm and Kathleen Kennedy announced that the stand-alone films will be referred to as the "Star Wars Anthology" series.
In May 2014, Lucasfilm announced that Gareth Edwards will direct the first anthology film, to be released on December 16, 2016, with Gary Whitta writing the first draft. On March 12, 2015, the film's title was revealed to be "Rogue One" with Chris Weitz rewriting the script, with Felicity Jones, Ben Mendelsohn and Diego Luna starring. On April 19, 2015, a teaser trailer was shown exclusively during the closing of the Star Wars Celebration. Lucasfilm also announced that filming would begin in the summer of 2015. The plot will revolve around a group of rebels on a mission to steal the Death Star plans; director Edwards stated, "It comes down to a group of individuals who don't have magical powers that have to somehow bring hope to the galaxy." Additionally, Kathleen Kennedy and Kiri Hart confirmed that the stand-alone films will be labeled as "anthology films". Edwards stated that the style of the film will be similar to that of a war film, stating, "It's the reality of war. Good guys are bad. Bad guys are good. It's complicated, layered; a very rich scenario in which to set a movie."
In June 2014, "Chronicle" director Josh Trank was announced as the director of the second anthology feature. On May 1, 2015, Lucasfilm and Trank jointly announced that Trank was no longer directing the film. On May 4, 2015, it was confirmed that the film would surround the origins of Boba Fett, a bounty hunter in the original trilogy.
3D releases.
At a ShoWest convention in 2005, Lucas demonstrated new technology and stated that he planned to release the six films in a new 3D film format, beginning with "A New Hope" in 2007. However, by January 2007, Lucasfilm stated on StarWars.com that "there are no definitive plans or dates for releasing the "Star Wars" saga in 3-D." At Celebration Europe in July 2007, Rick McCallum confirmed that Lucasfilm is "planning to take all six films and turn them into 3-D," but they are "waiting for the companies out there that are developing this technology to bring it down to a cost level that makes it worthwhile for everybody". In July 2008, Jeffrey Katzenberg, the CEO of DreamWorks Animation, revealed that Lucas plans to redo all six of the movies in 3D. In late September 2010, it was announced that "The Phantom Menace" would be theatrically re-released in 3-D on February 10, 2012. The plan was to re-release all six films in order, with the 3-D conversion process taking up to a year to complete for each film. However, the 3D re-releases of episodes "II" and "III" have been postponed to enable Lucasfilm to concentrate on "Episode VII".
Reception.
Academy Awards.
The six films together were nominated for 25 Academy Awards, of which they won ten. Three of these were Special Achievement Awards.
Expanded Universe.
The term "Expanded Universe" ("EU") is an umbrella term for officially licensed "Star Wars" material outside of the six feature films. The material expands the stories told in the films, taking place anywhere from 25,000 years before "The Phantom Menace" to 140 years after "Return of the Jedi". The first "Expanded Universe" story appeared in Marvel Comics' "Star Wars" #7 in January 1978 (the first six issues of the series having been an adaptation of the film), followed quickly by Alan Dean Foster's novel "Splinter of the Mind's Eye" the following month.
Despite Disney's acquisition of the product, George Lucas retains artistic control over the "Star Wars" universe. For example, the death of central characters and similar changes in the status quo requires his approval before authors were allowed to proceed. In addition, Lucasfilm Licensing and the new Lucasfilm Story Group devote efforts to ensure continuity between the works of various authors across companies. Elements of the Expanded Universe have been adopted by Lucas for use in the films, such as the name of capital planet Coruscant, which first appeared in Timothy Zahn's novel "Heir to the Empire" before being used in "The Phantom Menace". Additionally, Lucas so liked the character Aayla Secura, who was introduced in Dark Horse Comics' "Star Wars" series, that he included her as a character in "Attack of the Clones".
Lucas has played a large role in the production of various television projects, usually serving as storywriter or executive producer. "Star Wars" has had numerous radio adaptations. A radio adaptation of "A New Hope" was first broadcast on National Public Radio in 1981. The adaptation was written by science fiction author Brian Daley and directed by John Madden. It was followed by adaptations of "The Empire Strikes Back" in 1983 and "Return of the Jedi" in 1996. The adaptations included background material created by Lucas but not used in the films. Mark Hamill, Anthony Daniels, and Billy Dee Williams reprised their roles as Luke Skywalker, C-3PO, and Lando Calrissian, respectively, except in "Return of the Jedi" in which Luke was played by Joshua Fardon and Lando by Arye Gross. The series also used John Williams' original score from the films and Ben Burtt's original sound designs.
While Lucasfilm strived to maintain internal consistency between the films and television content with the expanded universe, only the films and the second "Clone Wars" television series are regarded as absolute canon, since Lucas worked on them directly. On April 25, 2014—anticipating future film installments—the company announced that they had devised a "story group" to oversee and co-ordinate all creative development. The first new on-screen canon to be produced will be the television series "Star Wars Rebels". Previous EU titles will be reprinted under the "Legends" banner.
Other films.
In addition to the two trilogies and "" film, several other authorized films have been produced:
Animated series.
Following the success of the "Star Wars" films and their subsequent merchandising, several animated television series have been created:
Literature.
"Star Wars"-based fiction predates the release of the first film, with the 1976 novelization of "Star Wars" (ghost-written by Alan Dean Foster and credited to Lucas). Foster's 1978 novel, "Splinter of the Mind's Eye", was the first Expanded Universe work to be released. In addition to filling in the time between "A New Hope" and "The Empire Strikes Back", this additional content greatly expanded the "Star Wars" timeline before and after the film series. "Star Wars" fiction flourished during the time of the original trilogy (1977–83) but slowed to a trickle afterwards. In 1992, however, Timothy Zahn's "Thrawn trilogy" debuted, sparking a new interest in the "Star Wars" universe. Since then, several hundred tie-in novels have been published by Bantam and Del Rey. A similar resurgence in the Expanded Universe occurred in 1996 with the Steve Perry novel "", set in between "The Empire Strikes Back" and "Return of the Jedi", and accompanying video game and comic book series.
LucasBooks radically changed the face of the "Star Wars" universe with the introduction of the "New Jedi Order" series, which takes place some 20 years after "Return of the Jedi" and stars a host of new characters alongside series originals. For younger audiences, three series have been introduced. The ' series follows the adventures of Obi-Wan Kenobi and his master Qui-Gon Jinn in the years before "The Phantom Menace". The "Jedi Quest" series follows the adventures of Obi-Wan and his apprentice Anakin Skywalker in between "The Phantom Menace" and "Attack of the Clones". ' series follows the adventures of Obi-Wan and another surviving Jedi almost immediately, set in between "Revenge of the Sith" and "A New Hope".
Following Disney's purchase of the franchise, Disney Publishing Worldwide also announced that Del Rey would publish a new line of canon Star Wars books under the Lucasfilm Story Group being released starting in September on a bi-monthly schedule. The Star Wars Legends banner would be used for those Extended Universe materials that are in print.
Marvel Comics published "Star Wars" comic book series and adaptations from 1977 to 1986. A wide variety of creators worked on this series, including Roy Thomas, Archie Goodwin, Howard Chaykin, Al Williamson, Carmine Infantino, Gene Day, Walt Simonson, Michael Golden, Chris Claremont, Whilce Portacio, Jo Duffy, and Ron Frenz. The Los Angeles Times Syndicate published a "Star Wars" newspaper strip by Russ Manning, Goodwin and Williamson with Goodwin writing under a pseudonym. In the late 1980s, Marvel announced it would publish a new "Star Wars" comic by Tom Veitch and Cam Kennedy. However, in December 1991, Dark Horse Comics acquired the "Star Wars" license and used it to launch a number of ambitious sequels to the original trilogy instead, including the popular "Dark Empire" stories. They have since gone on to publish a large number of original adventures set in the "Star Wars" universe. There have also been parody comics, including "Tag and Bink". On January 3, 2014, Marvel Comics—itself a Disney subsidiary—announced that it would once again publish "Star Wars" comic books and graphic novels, taking over from Dark Horse, with the first release arriving on January 14, 2015.
Games.
Since 1977, dozens of board, card, video, miniature, and tabletop role-playing games, among other types, have been published bearing the "Star Wars" name, beginning in 1977 with the board game "Star Wars: Escape from the Death Star" (not to be confused with another board game with the same title, published in 1990). "Star Wars" video games commercialization started in 1982 with "" published for the Atari 2600 by Parker Brothers. Since then, "Star Wars" has opened the way to a myriad of space-flight simulation games, first-person shooter games, role-playing video games, RTS games, and others. Three different official tabletop role-playing games have been developed for the "Star Wars" universe: in the 1980s and 1990s, one by Wizards of the Coast in the 2000s and one by Fantasy Flight Games in the 2010s.
The best-selling games so far are the ' and the ' series, with 12 million and 10 million units respectively while the most critically acclaimed is the first '. The most recently released games are ', ', ' and ', for the PS3, PSP, PS2, Xbox 360, Nintendo DS and Wii. While "The Complete Saga" focuses on all six episodes of the series, "The Force Unleashed", of the same name of the , takes place in the largely unexplored time period between "Revenge of the Sith" and "A New Hope" and casts players as Darth Vader's "secret apprentice" hunting down the remaining Jedi. The game features a new game engine, and was released on September 16, 2008 in the United States. There are three more titles based on the Clone Wars which were released for the Nintendo DS (') and Wii ("" and ").
"Star Wars" trading cards have been published since the first 'blue' series, by Topps, in 1977. Dozens of series have been produced, with Topps being the licensed creator in the United States. Some of the card series are of film stills, while others are original art. Many of the cards have become highly collectible with some very rare "promos", such as the 1993 Galaxy Series II "floating Yoda" P3 card often commanding US$ 1 000 or more. While most "base" or "common card" sets are plentiful, many "insert" or "chase cards" are very rare. From 1995 until 2001, Decipher, Inc. had the license for, created and produced a collectible card game based on "Star Wars"; the "Star Wars Collectible Card Game" (also known as SWCCG).
The board game "Risk" has been adapted to the series in two editions by Hasbro: and ' (2005) and ' (2006). From July 25 to August 15, 2013, Disney's online game Club Penguin hosted a Star Wars Takeover event based on the films.
On May 5, 2015, Disney announced a follow-up game through "Game Informer", "Disney Infinity 3.0", for release on Xbox 360, PlayStation 3, Wii U, iOS, PC, Xbox One, and PlayStation 4 in 2015, featuring characters from the "Star Wars" universe.
Fan works.
The "Star Wars" saga has inspired many fans to create their own non-canon material set in the "Star Wars" galaxy. In recent years, this has ranged from writing fan-fiction to creating fan films. In 2002, Lucasfilm sponsored the first annual Official "Star Wars" Fan Film Awards, officially recognizing filmmakers and the genre. Because of concerns over potential copyright and trademark issues, however, the contest was initially open only to parodies, mockumentaries, and documentaries. Fan-fiction films set in the "Star Wars" universe were originally ineligible, but in 2007 Lucasfilm changed the submission standards to allow in-universe fiction entries.
While many fan films have used elements from the licensed Expanded Universe to tell their story, they are not considered an official part of the "Star Wars" canon. However, the lead character from the "Pink Five" series was incorporated into Timothy Zahn's 2007 novel "Allegiance", marking the first time a fan-created "Star Wars" character has ever crossed into the official canon. Lucasfilm, for the most part, has allowed but not endorsed the creation of these derivative fan-fiction works, so long as no such work attempts to make a profit from or tarnish the "Star Wars" franchise in any way.
Attractions.
Before Disney's acquisition of the franchise, George Lucas had established a partnership in 1986 with Disney and its Walt Disney Imagineering division to create "Star Tours", an attraction that opened at Disneyland in 1987. The attraction also had subsequent incarnations at other Disney theme parks worldwide, with the exception of Hong Kong Disneyland.
The attractions at Disneyland and Disney's Hollywood Studios closed in 2010 and at Tokyo Disneyland in 2012 to allow the rides to be converted into "Star Tours—The Adventures Continue". The successor attraction opened at Disney's Hollywood Studios and Disneyland in 2011, and Tokyo Disneyland in 2013.
The "Jedi Training Academy" is a live show where children are selected to learn the teachings of the Jedi Knights and the Force to become Padawan learners. The show is present at the Rebels stage at Disney's Hollywood Studios and at the Tomorrowland Terrace at Disneyland.
Walt Disney World's Disney's Hollywood Studios park also hosts an annual festival, "Star Wars" Weekends, during specific dates from May to June. The event began in 1997.
Since August 2014, Disney has expressed plans to expand the franchise's presence in all of their theme parks, which is rumored to include a major "Star Wars"-themed expansion to Disney's Hollywood Studios. When asked whether or not Disney has an intellectual property franchise that's comparable to "Harry Potter" at Universal theme parks, Disney chairman Bob Iger mentioned "Cars" and the Disney Princesses, and promised that "Star Wars", "is going to be just that."
Organisms named after "Star Wars" characters.
Characters and other fictional elements from Star Wars have inspired several scientific names of organisms. Examples include "Midichloria", a genus of bacteria named after the fictional micro-organisms midichlorians associated with the Force, "Yoda purpurata", (an acorn worm) and "Agathidium vaderi" (beetle), and "Aptostichus sarlacc", a trapdoor spider named for the sarlacc, the pit-dwelling creature on Tatooine. Other examples include:
Legacy.
The "Star Wars" saga has had a significant impact on modern American pop culture. Both the films and characters have been parodied in numerous films and television.
In 1989, the Library of Congress selected the original "Star Wars" film for preservation in the U.S. National Film Registry, as being "culturally, historically, or aesthetically significant." Its sequel, "The Empire Strikes Back", was selected in 2010. Despite these callings for archival, it is unclear whether copies of the 1977 and 1980 theatrical sequences of "Star Wars" and "Empire"—or copies of the 1997 Special Edition versions—have been archived by the NFR, or indeed if any copy has been provided by Lucasfilm and accepted by the Registry.

</doc>
<doc id="26771" url="http://en.wikipedia.org/wiki?curid=26771" title="Spindletop">
Spindletop

Spindletop is a salt dome oil field located in the southern portion of Beaumont, Texas in the United States. The Spindletop dome was derived from the Louann Salt evaporite layer of the Jurassic geologic period. On January 10, 1901, a well at Spindletop struck oil ("came in"). The new oil field soon produced more than 100000 oilbbl of oil per day. Gulf Oil and Texaco, now part of Chevron Corporation, were formed to develop production at Spindletop.
The strike at Spindletop represented a turning point for Texas and the United States; no oil field in the world had ever been so productive. The frenzy of oil exploration and the economic development it generated in the state became known as the Texas Oil Boom. The United States soon became the world's leading oil producer.
History.
There had long been suspicions that oil might be under "Spindletop Hill." The area was known for its vast sulfur springs and bubbling gas seepages that would ignite if lit. In August 1892, George W. O'Brien, George W. Carroll, Pattillo Higgins and others formed the Gladys City Oil, Gas, and Manufacturing Company to do exploratory drilling on Spindletop Hill. The company drilled many dry holes and ran into trouble, as investors began to balk at pouring more money into drilling with no oil to show for it.
Pattillo Higgins left the company and teamed with Captain Anthony F. Lucas, the leading expert in the U.S. on salt dome formations. Lucas made a lease agreement in 1899 with the Gladys City Company and a subsequent agreement with Higgins. Lucas drilled to 575 ft before running out of money. He secured additional funding from John H. Galey and James M. Guffey of Pittsburgh, but the deal left Lucas with only a small share of the lease and Higgins with nothing.
Lucas continued drilling and on January 10, 1901, at a depth of 1,139 ft (347 m), what is known as the Lucas Gusher or the Lucas Geyser blew oil over 150 ft in the air at a rate of 100000 oilbbl/d(4,200,000 gallons). It took nine days before the well was brought under control. Spindletop was the largest gusher the world had seen and catapulted Beaumont into an oil-fueled boomtown. Beaumont's population of 10,000 tripled in three months and eventually rose to 50,000. Speculation led land prices to increase rapidly. By the end of 1902, more than 500 companies had been formed and 285 wells were in operation.
Spindletop was the first oil field found on the US Gulf Coast, and prompted further drilling, and further oil field discoveries. Oil drillers looking for another Spindletop particularly sought out other salt domes, and were often successful. The Gulf Coast turned into a major oil region.
Standard Oil, which then had a monopoly or near-monopoly on the petroleum industry in the eastern states, was prevented from moving aggressively into the new oil field by state antitrust laws. Populist sentiment against Standard Oil was particularly strong at the time of the Spindletop discovery. In 1900, an oil products marketing company affiliated with Standard Oil had been banned from the state for its cutthroat business practices. Although Standard built refineries in the area, Standard was unable to dominate the new Gulf Coast oil fields the way it had in the eastern states. As a result, a number of startup oil companies at Spindletop, such as Texaco and Gulf Oil, grew into formidable competitors to Standard Oil.
Among those drilling at Spindletop was W. Scott Heywood, a native of Cleveland, Ohio, who in 1901 made the first oil discovery in nearby Jeff Davis Parish in southwestern Louisiana. In 1932, Heywood was elected to a single term in the Louisiana State Senate.
Production at Spindletop began to decline rapidly after 1902, and the wells produced only 10000 oilbbl/d by 1904. On November 14, 1925, the Yount-Lee Oil Company brought in its McFaddin No. 2 at a depth of about 2500 ft, sparking a second boom, which culminated in the field's peak production year of 1927, during which 21 millions barrels (3.3 GL) were produced. Over the ten years following the McFaddin discovery, more than 72 million barrels (11.4 GL) of oil were produced, mostly from the newer areas of the field. Spindletop continued as a productive source of oil until about 1936. It was then mined for sulfur from the 1950s to about 1975.
Spindletop-Gladys City Boomtown Museum.
In 1976 Lamar University dedicated the Spindletop-Gladys City Boomtown Museum to preserve the history of the Spindletop oil gusher era in Beaumont. The museum features an oil derrick and many reconstructed Gladys City building interiors furnished with authentic artifacts from the Spindletop boomtown period.
The Lucas Gusher Monument is located at the museum. The Monument, erected at the wellhead in July, 1941, was moved to the Spindletop-Gladys City Museum after it became unstable due to ground subsidence. According to an article by Nedra Foster, LS in the July/August, 2000 issue of the Professional Surveyor Magazine, the Monument was originally located within four feet of the actual site of the Spindletop well.
Today a flag pole flying a Texas flag marks the location of the wellhead, at Spindletop Park, about 1.5 miles southwest of the museum, off West Port Arthur Road/Spur 93. There is a viewing platform with information placards there, about a quarter mile from the flagpole, which is in the middle of swampland on private land and not accessible. Directions to the site are available at the museum.
On December 4, 1955, the Spindletop story was dramatized in "Spindletop - The First Great Texas Oil Strike (January 10, 1901)" on the CBS history series, "You Are There". Robert Bray was cast as Pattillo Higgins; Mike Ragan as Marion Fletcher; Parley Baer as Captain Lucas, Jean Byron as Caroline Lucas, DeForest Kelley as Al Hammill, Tyler McVey as Mayor Wheat, and William Fawcett as a farmer.

</doc>
<doc id="26797" url="http://en.wikipedia.org/wiki?curid=26797" title="Sackbut">
Sackbut

A sackbut is a type of trombone from the Renaissance and Baroque eras. It is characterised by a telescopic slide used to vary the length of the tube to change pitch, allowing chromaticism easy and accurate doubling of voices. Sackbuts adjust tuning at the joint between the bell and slide. The sackbut differs from modern trombones by its smaller bore, its less-flared bell, and in the lack of a water key, slide lock, and tuning slide on the bell curve. More delicately constructed than their modern counterparts and featuring a softer, more flexible sound, they attracted a more sizeable repertoire of original chamber and vocal music than many instruments contemporary with them.
Terminological history.
The first reference to a slide instrument was probably "trompette des ménestrels", first found in Burgundy in the 1420s and later in other regions of Europe. The name distinguished the instrument from the "trompettes de guerre" (war trumpets), which were of fixed length.
The next word to appear in the 15th century that implied a slide was the "sackbut" group of words. There are two theories for the sources: it is either derived from the Middle French "sacquer" (to pull) and "bouter" (to push) or from the Spanish "sacar" (to draw or pull) and "bucha" (a tube or pipe). The term survives in numerous English spelling variations including sacbut, sackbutte, sagbut, shagbolt, sacabushe and shakbusshe.
Closely related to "sackbut" was the name used in France: "sacqueboute" and in Spain, where it was "sacabuche". These terms were used in England and France until the 18th century.
In Scotland in 1538 the slide instrument is referred to as "draucht trumpet" (drawn trumpet) as opposed to a "weir trumpet" (war trumpet), which had a fixed length.
In Germany, the original word was "Posaune", appearing about 1450 and is still used today. This (as well as "bason") derives from "busine," which is Latinate and meant straight trumpet.
In Italy it was (and remains) "trombone", which derived from trumpet in the Latin "tromba" or "drompten", used in the Low Countries. The first records of it being used are around 1440, but it is not clear whether this was just a nickname for a trumpet player. In 1487 a writer links the words "trompone" and "sacqueboute" and mentions the instrument as playing the contratenor part in a danceband.
History.
The trombone developed from the trumpet. Up until 1375 trumpets were simply a long straight tube with a bell flare.
There are various uses of "sackbut"-like words in the Bible, which has led to a faulty translation from the Latin bible that suggested the trombones date back as far as 600 BC, but there is no evidence of slides at this time.
From 1375 the iconography sees trumpets being made with bends, and some in 'S' shapes. Around 1400 we see the 'loop' shaped trumpet appear in paintings and at some point in the 15th century, a single slide was added. This slide trumpet was known as a 'trompette des ménestrels' in the alta capella bands.
The earliest clear evidence of a double slide instrument is in a fresco painting by Filippino Lippi in Rome - "The Assumption of the Virgin", dating from 1488-1493.
From the 15th to the 19th centuries, the instrument designs changed very little overall, apart from a slight widening of the bell in classical era. Since the 19th century, trombone bore sizes and bells have increased significantly.
It was one of the most important instruments in Baroque polychoral works, along with the cornetto and organ.
Instrument sizes.
Sackbuts come in several sizes. According to Michael Praetorius, these were:
The pitch of the trombones has (notionally) moved up a semi-tone since the 17th century, and this is explained in the section on Pitch.
Because the tenor instrument is described as "Gemeine" (common or ordinary), this is probably the most widely used trombone.
The basses, due to their longer slides, have a hinged handle on the slide stay, which is used to reach the long positions.
The giant Octav-Posaun / double bass trombone / contra-bass trombone in the style of the those made in 16th/17th centuries is represented by only a few existing instruments. There is an original instrument made by Georg Nicolaus Oller built in Stockholm in 1639 and housed in the Musikmuseet. In addition, Ewald Meinl has made a modern copy of this instrument, and it is currently owned and played by Wim Becu.
Construction.
The bore size of renaissance/baroque trombones is approximately 10 mm and the bell rarely more than 10.5 cm in diameter. This compares with modern tenor trombones, which commonly have bores 12.7 mm to 13.9 mm and bells 17.8 cm to 21.6 cm.
Modern reproductions of sackbuts sacrifice some authenticity to harness manufacturing techniques and inventions that make them more comfortable for modern players, while retaining much of the original character of the old instruments.
Some original instruments could be disassembled into the constituent straight tubes, bowed tubes, bell flare, and stays, with ferrules at the joints. Mersenne has a diagram. (Little imagination is needed to see how it could be reassembled - with an extra tube - into something approaching a natural trumpet.) There is a debate as to whether they used tight fittings, wax or another joining substance. Modern sackbut reproductions are usually soldered together. Some modern sackbut reproductions use glue as a compromise to give a loose fitting for high resonance without risk of falling apart.
Tuning slides came in during the very late 18th century. Early trombonists adjusted pitch with the slide, and by adding variously shaped and sized crooks. Modern reproductions often have a bell bow tuning slide or telescopic slide between the slide and bell sections. Crooks are still used, as are variously sized bell bow sections for larger changes. 
The stays on period sackbuts are flat. While the bell stay remained flat, from about 1660 the slide stays became tubular. On many modern reproductions round slide stays are much more comfortable to play and easier to make.
A loose connection between the bell stay and the bell is thought key to a resonant bell, and thus a better sackbut sound. Original instruments have a hinge joint. Modern copies with a tuning slide in the bell can need more support for operation of the slide, so either an extra stay by the tuning slide is provided or a joint without play in only one axis is employed.
The original way to make the slide tubes was to roll a flat piece of metal around a solid cylinder mandrel, and the joining edges soldered together. Modern manufacturers now draw the tubes. They also tend to have stockings, which were only invented around 1850. In addition, modern made slides are usually made of nickel silver with chrome plating, giving a smoother finish and quieter action than simply the brass that would have originally been used.
The water key was added in the 19th century, but modern reproductions often have them.
Pitch.
Until some time in the 18th century, the trombone was in A and the pitch of that A was about a half-step higher than it is today—460–480 Hz. There was a transition around the 18th century when trombones started to be thought of in Bb at around 440 Hz. This change did not require a change in the instrument, merely a new set of slide positions for each note. But it does mean that the baroque and renaissance repertoire was intended to be played at the higher pitch. There are many examples of evidence for this:
The tenor trombones that survive are pitched closest to Bb at A=440 Hz, which is the same as A at A=466 Hz. So what we now think of as a tenor trombone with Bb in first position, pitched at A=440 was actually thought of as a trombone in A (in first position), pitched at A=466. Surviving basses in D at A=466 (Eb at 440) - for example: Ehe, 1612 (Leipzig) and Hainlein, c.1630 (Nuremberg) confirm Praetorius' description. It is also worth noting that Rognoni's "Suzanne ung jour" setting descends repeatedly to BBb, which is a tone lower than the lowest note playable on a bass in F; on a bass in D, it falls in (modern) fifth position.
Many groups now perform at A=466 Hz for the sake of greater historical accuracy.
Timbre.
The sackbut was described as suitable for playing with the 'loud' ensembles in the outdoors, as well as the 'soft' ensembles inside.
The alta capella bands are seen in drawings as entertaining outside with ensembles including shawms, trumpets and trombones. When pushed, sackbuts can easily make a loud and brassy sound.
The sackbut also responds very well to rather soft playing - more so than a modern trombone. The sound is characterized by a more delicate, vocal timbre. The flat rims and shallow cups of the older mouthpieces are instrumental in providing the player with a much wider palette of articulations and tonal colours. This flexibility lends itself to a vocal style of playing and facilitates very characterful phrasing.
Mersenne wrote in 1636, "It should be blown by a skillful musician so that it may not imitate the sounds of the trumpet, but rather assimilate itself to the sweetness of the human voice, lest it should emit a warlike rather than a peaceful sound."
The Lorenzo da Lucca was said to have had "in his playing a certain grace and lightness with a manner so pleasing".
Performance practice.
Musicians of the 16th and 17th centuries benefited from a broader base of skills than the average performer today.
They would have to improvise new music. In the Middle Ages to the Renaissance, various music treatises include in their tuition improvising at sight fast moving melody over a cantus firmus, or extra contrapuntal lines to a plainchant. In a non-liturgical setting, an alta capella group (in which a slide trumpet or trombone often featured) would involve the tenor playing the main tune in long tones while two others improvised florid counterpart tunes.
These traditions continued into the baroque with musicians expected to give expression to the written music by ornamenting with a mixture of one-note “graces” and whole passage “divisions” (also known as “diminutions”). The suggestions for producing effective ornaments without disrupting the line and harmony are discussed alongside countless examples in the 16th and early 17th century Italian division tutors. Graces such as the accento, portar della voce, tremolo, groppo, trillo, esclamationo and intonatio are all to be considered by performers of any music in this period.
“Cornetts and trombones...play divisions that are neither scrappy, nor so wild and involved that they spoil the underlying melody and the composer's design: but are introduced at such moments and with such vivacity and charm that they give the music the greatest beauty and spirit”
Bottrigari, Venice 1594
Along with the improvisation, many of these tutors discuss articulation. Francesco Rognoni in 1620 describes the tonguing as the most important part of producing “a good and beautiful effect in playing wind instruments, and principally the cornetto” (which of course had a very similar role to the trombone). The treatises discuss the various strengths of consonants from “le” through “de” to “te”. But the focus of the text is for playing rapid notes “similar to the gorgia of the human voice” with “soft and smooth” double tonguing (“lingua riversa”) using “le re le re”. This is opposed to using “te che te che,” which is described as “harsh, barbarous and displeasing”. The natural ‘pairing’ of notes these articulations provide is similar to the instructions for string players who are instructed to slur (“lireggiar”) pairs of eighth notes with one bow stroke per quarter beat.
Another integral part of the early music sound-world is the musical temperament. Music in the middle-ages favours intervals of the 4th and 5th, which is why Pythagorean tuning was used. The interval of a third was used as a clash until the Renaissance, when it became consonant in compositions, which went hand-in-hand with the widespread use of Meantone temperament. During the 17th century, Well temperament began to become more and more popular as the range of keys increased. Temperament affects the colour of a composition, and therefore modern performances, typically employing equal temperament, may not be true representations of the composers' intentions.
These old tunings can come naturally on a sackbut. As the bell is smaller than a modern trombone, the harmonic series is closer to a perfect harmonic series, which is the basis for just tuning. Without adjusting the slide, the 1st to 2nd harmonic is a perfect octave, 2nd to 3rd harmonic is a 5th slightly wider than equal temperament and 4th to 5th harmonic is a major 3rd slightly narrower than in equal temperament. These adjusted intervals make chords ring and are the basis of meantone. In fact, Speer says, “Once you have found a good C (3rd position), this is also the place you will find your F♯.” Playing C and F♯ in exactly the same position on a modern orchestra sounds out of tune, but it tunes perfectly well on a sackbut if everyone plays meantone.
Plenty of musical understanding can be gathered from reading the original music print. Publishers such as SPES and Arnaldo Forni Edition provide facsimile copies of plenty of music for trombone from this era. To read these it one needs to become familiar with the old clefs, time signatures, ligatures and notational conventions of the era. There are myriad performance indicators embedded in the quirks of the old notation that are simply lost in modern editions.
When reading sackbut music, it is important to consider Musica ficta, to help solve some of the controversial pitches. The scores are unclear and composers were embarrassed to point out accidentals they felt were ‘obvious’ to performers. For example there are occasions where a leading note should be sharpened to a major 7th as you go into a cadence. There also are often questions about which notes accidental markings apply to. There are differences of opinion between editors and performers now, just as there were between performers then.
Repertoire.
Before 1600.
The sackbut replaced the slide trumpet in the 15th century alta capella wind bands that were common in towns throughout Europe playing courtly dance music. See Waits.
Another key use of the trombone was in ceremonies, in conjunction with the trumpet. In many towns in Germany and Northern Italy, 'piffari' bands were employed by local governments throughout the 16th century to give regular concerts in public squares and would lead processions for festivals. Piffari usually contained a mix of wind, brass and percussion instruments and sometimes viols.
Venice's doge had his own piffari company and they gave an hour-long concert in the Piazza each day, as well as sometimes performing for services in St. Mark's. Each of the six confraternities in Venice also had their own independent piffari groups too, which would all play at a lavish procession on the feast of Corpus Domini. These groups are in addition to the musicians employed by St. Mark's to play in the balconies with the choir (the piffari would play on the main level).
It also was used in church music both for instrumental service music and as a doubling instrument for choral music. The treble and high alto parts were most often played by cornetts or shawms, with the violin sometimes replacing the cornett in 17th century Italian music.
The first record of trombones being used in churches was in Innsbruck 1503. Seville Cathedral's records show employment of trombonists in 1526, followed by several other Spanish cathedrals during the 16th century, used not only for ceremonial music and processionals, but also for accompaniment of the liturgical texts as well, doubling voices.
The sacred use of trombones was brought to a fine art by the Andrea Gabrieli, Giovanni Gabrieli and their contemporaries c.1570-1620 Venice and there is also evidence of trombonists being employed in churches and cathedrals in Italy at times during the second half of the 16th century in Bologna, Rome, Padua, Mantua and Modena.
Since ensembles had flexible instrumentation at this time, there is relatively little music before Giovanni Gabrieli's publication "Symphoniae sacrae" (1597) that specifically mentions trombones. The only example currently known is the music by Francesco Corteccia for the Medici wedding 1539.
1600-1700.
Solo.
The 17th century brings two pieces of real solo trombone repertoire.
Giovanni Martino Cesare wrote "La Hieronyma," (Musikverlag Max Hieber, MH6012) the earliest known piece for accompanied solo trombone. It comes from Cesare's collection "Musicali Melodie per voci et instrumenti a una, due, tre, quattro, cinque, e sei" published in Munich 1621 of 28 pieces for a mixture of violins, cornetts, trombone, vocal soloists and organ continuo. The collection also contains "La Bavara" for four trombones.
The other solo trombone piece of the 17th century, "Sonata trombone & basso" (modern edition by H Weiner, Ensemble Publications), was written around 1665. This anonymous piece is also known as the 'St. Thomas Sonata' because it was kept in the library of the Saint Thomas Augustinian Monastery in Brno, Czech Republic.
Francesco Rognoni was another composer who specified the trombone in a set of divisions (variations) on the well-known song "Suzanne ung jour" (London Pro Musica, REP15). Rognoni was a master violin and gamba player whose treatise "Selva di Varie passaggi secondo l'uso moderno" (Milan 1620 and facsimile reprint by Arnaldo Forni Editore 2001) details improvisation of diminutions and Suzanne is given as one example. Although most diminutions are written for organ, string instruments or cornett, Suzanne is "per violone over Trombone alla bastarda". With virtuosic semiquaver passages across the range of the instrument, it reflects Praetorius' comments about the large range of the tenor and bass trombones, and good players of the Quartposaune (bass trombone in F) could play fast runs and leaps like a viola bastarda or cornetto. The term "bastarda" describes a technique that made variations on all the different voices of a part song, rather than just the melody or the bass: "considered illegitimate because it was not polyphonic".
Chamber music.
In the 17th century, a considerable repertoire of chamber music using sackbut with various combinations of violins, cornetts and dulcians, often with continuo, appeared. Composers included Dario Castello, Giovanni Battista Fontana, Giovanni Paolo Cima, Andrea Cima, Johann Heinrich Schmelzer and Matthias Weckmann.
Giovanni Paolo Cima, organist of S. Celso wrote the oldest known trio sonata and solo violin sonata. Contained in his "Concerti ecclesiastici" (Milan 1610) is his brother Andrea's "Capriccio" 'for cornett and trombone or violin and violone'.
Antonio Bertali wrote several trio sonatas for 2 violins, trombone and bass continuo in the mid-17th century. One such "Sonata a 3" is freely available in facsimile form from the Düben Collection website hosted by Uppsala universitet. A "Sonata a3 in C" is published by Musica Rara and attributed to Biber, although the authorship is unclear and it is more likely to have been written by Bertali.
Dario Castello, a wind player at St. Mark's Venice in the early 17th century had two books of "Sonate Concertate" published in 1621 and 1629. The sonatas of 1-4 parts with bass continuo often specify trombones, as well as cornett, violin and bassoon. The numerous reprints during the 17th century affirm his popularity then, as perhaps now.
Giuseppe Scarani joined St. Mark's Venice in 1629 as a singer and in the following year published "Sonate concertate", a volume of works for 2 or 3 (unspecified) instruments (and b.c.). The title has been suggested was chosen to try and capture some of Castello's success.
Tiburtio Massaino wrote a Canzona for eight trombones, published in Raverio's 1608 collection.
Johann Heinrich Schmelzer wrote several sonatas that included trombones—such as his "Sonata à 7" for two cornetts, two trumpets, three trombones, and basso continuo.
Daniel Speer published a four-part sonata in "Neu-gebachene Taffel-Schnitz" (1685). In 1687, Speer published the first written instruction in sackbut (and several other instruments) playing: "Grund-richtiger/kurtz/leicht und noethiger Unterricht der Musicalischen Kunst". The second edition in 1697 provides two three part sonatas for trombones.
An English work of note from this period is Matthew Locke's "Music for His Majestys Sagbutts and Cornetts", a suite for Charles II's coronation 1661.
Light music.
Non-serious music, often based on dances for festive occasions, rarely had specified instrumentation. Often you find something like "per diversi musici". Indeed the groups that would perform them would often be full of multi-instrumentalists.
Johann Pezel wrote for Stadtpfeifer with his "Hora decima musicorum" (1670), containing sonatas, as well as "Fünff-stimmigte blasende Music" (1685) with five-part intradas and dance pieces.
Well known pieces from Germany includes Samuel Scheidt's "Ludi Musici" (1621) and Johann Hermann Schein's "Banchetto musicale" (1617).
The first English piece scored for trombone is John Adson's "Courtly Masquing Ayres" (1611). Another light collection suitable for including trombones is Anthony Holborne's "Pavans, Galliards, Allmains, and other short Aeirs both Grave and Light in Five Parts for Viols, Violins or Other Musicall Winde Instruments" (1599).
Sacred music.
Venice.
Trombonists were in the regular ensemble at St. Mark's Venice from its formation in 1568 until they left the payroll in 1732. The first two ensemble directors - "maestro di concerti" - Girolamo Dalla Casa (1568–1601) and Giovanni Bassano (1601–1617) - were cornett players and the nucleus of the group was 2 cornetts and 2 trombones, although for the larger ceremonies many extra players were hired. During a mass attended by the Doge, evidence suggests they would have played a canzona in the Gradual after the Epistle and the Agnus Dei, a sonata in the Offertory as well as reinforcing vocal parts or substituting for absent singers.
This ensemble was used extensively by Giovanni Gabrieli in pieces substantially for brass, voices and organ in Venice up until his death in 1612. He was greatly influential in Venetian composers in other churches and confraternities, and his early baroque and cori spezzati style is seen in contemporaries like Giovanni Picchi and Giovanni Battista Grillo.
It is suggested that Monteverdi wrote his Vespro della Beata Vergine (1610) as a pitch for employment at St. Mark's as successor to Giovanni Gabrieli. In addition to the Magnificat, two movements specify trombones: the opening "Deus in adiutorium" is for 6 voices, 2 violins, 2 cornetts, 3 trombones, 5 viola da braccio and basso continuo; "Sonata sopra ‘Sancta Maria, ora pro nobis’" is for soprano, 2 violins, 2 cornetts, 3 trombones (one of which can be a viola da braccio), viola da braccio and basso continuo. Monteverdi also leaves the option to use trombones as part of the "sex instrumentis" of the "Dixit Dominus" and in the instrumental "Ritornello a 5" between verses of "Ave maris stella".
From around 1617, when the "maestro de' concerti" at St. Marks changed to violinist Francesco Bonfante and correspondingly the ensemble changed from basically a brass ensemble to being more evenly mixed with brass, wind and string instruments.
Monteverdi arrived at St. Mark's in 1613 and it is unsurprising that he includes trombones and strings for several more sacred works during his time here, published in his "Selva Morale e Spirituale" 1641. Of the c.40 items in this collection, six specify three or four trombones (or viola da braccio, ad lib): SV268 Beatus vir I, SV263 Dixit Dominus I, SV263 Dixit Dominus II, SV261 Et iterum venturus est, SV258 Gloria in excelsis Deo, SV281 Magnificat I. Each is for 3-8 voices with 3 violins (apart from SV261), the trombones/violas and basso continuo. Monteverdi also specified trombones in two more sacred works: SV198 Laetatus sum (i) (1650) for 6 voices, 2 violins, 2 trombones and bassoon and SV272 Laudate Dominum omnes gentes I (1641) for 5 voices ‘concertato’, 4 voice chorus ad lib, 4 viola da braccio or trombones and basso continuo.
Germany/Austria.
A prolific composer for trombones in Germany in the 17th century was Heinrich Schütz. His "Fili me, Absalon" (SWV 269) and "Attendite, popule meus" (SWV 270), are both scored for bass voice, four trombones (of which two are optionally violins) and basso continuo, are well known. They are part of his first "Symphoniae Sacrae" collection dating from 1629 and commentators have noted that the style reflects his studies in Venice with Giovanni Gabrieli 1609-1612. Other pieces that specify trombones (according to Grove) are (grouped by the collection they were published in): Concert mit 11 Stimmen (1618): SWV 21, in "Psalmen Davids" (Psalms of David) Op. 2 (1619): SWV 38, 40-46, Symphoniae Sacrae I Op.6 (1629): SWV 259, 269-271, 274, Symphoniae Sacrae II Op.10 (1647): SWV 344, Symphoniae Sacrae III Op. 12 (1650): SWV 398a, Historia (1664): SWV 435, 448, 449, 453, 461, 452, 466-470, 473, 474-476, Schwanengesang Psalm 119 (1671): SWV 500, although many others are suitable for trombones too.
Johann Hermann Schein specified trombones in some of his sacred vocal works in the "Opella nova, ander Theil, geistlicher Concerten" collection (Leipzig, 1626). For example, "Uns ist ein Kind geboren" is scored for violino, traversa, alto trombone, tenor voice, fagotto and basso continuo. "Mach dich auf, werde licht, Zion" uses Canto 1: violino, cornetto, flauto picciolo e voce, Canto 2: voce e traversa, Alto: Trombone e Voce, Tenore: Voce e Trombone, Basso: Fagotto Trombone e Voce and Basso Continuo, during which solos for each of the trombonists are specified. Of particular interest is "Maria, gegrüsset seist du, Holdselige," which uses soprano and tenor voices, alto trombone, 2 tenor trombones and on the bass line "trombone grosso," which goes down to pedal A, and a couple of diatonic scale passages from bottom C.
German composer Johann Rudolf Ahle wrote some notable sacred pieces for voices and trombones. "Höre, Gott" uses five favoriti singers, two ripieno choirs (which double other parts at intense moments) and seven trombones, with basso continuo. And his most famous "Neu-gepflanzte Thüringische Lust-Garten.." (1657–65) contains several sacred works with 3 or 4 trombones, including "Magnificat a 8" for SATB soloists, cornett, 3 trombones and continuo and "Herr nun lässestu deinen Diener a 5" for bass, 4 trombones and continuo.
Dieterich Buxtehude specifies trombones in a few sacred concertos using style derived from polychoral Venetian works and one secular piece. For example, "Gott fähret auf mit Jauchzen" (BuxWV33 from CW v, 44) is scored for SSB voices, 2 vn, 2 va, trbn, 2 cornetts, 2 tpt, bn and bc.
There are a few vocal works involving trombones in works by Andreas Hammerschmidt. These include "Lob- und Danck Lied aus dem 84. Psalm" for 9 voices, 5 tpt, 3 trbn, 5 va and bc (Freiberg, 1652). There is also "Hochzeitsgesang für Daniel Sartorius: Es ist nicht gut, dass der Mensch allein sei" for 5 voices, 2 vn, 2 trbn, bn and bc.
Johann Schelle has numerous sacred vocal works that use trombones. For instance "Vom Himmel kam der Engel Schar" is scored for soprano, tenor, SSATB choir, 2 violins, 2 violas, 2 cornetts, 3 trombones, 2 trumpets, timpani, basso continuo, and "Lobe den Herrn, meine Seele" is for two choirs of SSATB and similar instruments to the previous work.
The lesser known Austrian composer Christoph Strauss, Kapellmeister to the Habsburg Emperor Mathias 1616-1620, wrote two important collections for trombones, cornetts and voices. His motets published in Nova ac diversimoda sacrarum cantionum composition, seu motettae (Vienna, 1613) are in a similar tradition to Gabrieli's music. Of the sixteen motets in the collection, all are titled "concerto" apart from the "sonata" "Expectans Expectavi Dominum" for 6 trombones, cantus voice and tenor voice. In 1631 he published a number of masses, which were much more baroque, with basso continuo, rhetorical word painting and obligato usage of instruments.
Later in the 17th century, Heinrich Ignaz Franz Biber composed sacred works for voices and orchestra featuring trombones. His "Requiem" mass (1692) uses an orchestra of strings, 3 trombones and basso continuo. A similar ensemble accompanies 8 vocal lines in his "Lux perpetua" (c1673), and three more similar works in the 1690s.
Theatre.
Monteverdi ushers sackbuts into the first great opera - 'L'Orfeo' 1607. The orchestra at the first performance, as shown in the first publication, the list of "stromenti" at the front of the score specifies four trombones, but at one point in Act 3, however, the score calls for five trombones.
1700-1750.
There is relatively little repertoire for the trombone in the late baroque.
But Johann Sebastian Bach uses trombones in fourteen of his church cantatas - BWV 2, 3, 4, 21, 23, 25, 28, 38, 64, 68, 96, 101, 121, 135 as well as motet BWV 118. He uses the trombone sound to reflect the (by now) archaic sounds of the Renaissance trombones doubling voices (with cornett playing the soprano line), yet he also uses them independently, which John Eliot Gardiner says prepares the way for their use in Beethoven's "Symphony No. 5". The cantatas were either composed in Leipzig during 1723-1725, or (for BWV 4, 21 & 23) the trombone parts were added to the existing cantata during the same period. The cornett and trombone parts would have been played by the Stadtpfeifer.
In England, George Frideric Handel includes trombones in three of his oratorios: "Saul" (1738), "Israel in Egypt" (1738) and "Samson" (1741). There are no other documented groups or performances with trombone players in England at this time, and it has been suggested that the premiers took place with a visiting group from Germany, as was the custom in Paris at this time.
Vienna's Imperial court used trombones in church music:
Johann Joseph Fux was Hofkapellmeister in Vienna from 1715 until 1741. Many of his masses use the choir strengthened by strings, cornetts and trombones, often with independent moments for the instrumentalists and sometimes. "Missa SS Trinitatis" uses two choirs, which again points to the traditions going back to Gabrieli. His highly successful Requiem is for five vocal parts, two cornetts, two trombones, strings and continuo. He also uses the trombone in smaller motets and antiphons, such as his setting of "Alma Redemptoris mater" for soprano, alto trombone, strings and continuo. Some of his chamber music involves trombones, as do many of his operas, used as an obbligato instrument.
Also in the Vienna court was Antonio Caldara, vice-kapellmeister 1717-1736. Among his output are two Holy Week settings as Da Capo arias: "Deh sciogliete, o mesti lumi" for soprano, unison violins, bassoon, two trombones and organ and "Dio, qual sia" for soprano, trombone, bassoon and basso continuo.
1750-1800.
Again this period suffers from a lack of trombone players. Most of these works derive from Vienna and Salzburg.
Joseph Haydn uses trombones in "Il rotorno di Tobia", "Die Sieben Letzten Worte", The Creation, Die Jahreszeiten, "Der Sturm", "Orfeo de Euridice" and secular cantata choruses.
Wolfgang Amadeus Mozart uses trombones in connection with death or the supernatural. This includes the Requiem (K626, 1791), Great Mass in C minor (K423, 1783), "Coronation Mass (C major)" (K317, 1779), several other masses, "Vesperae Solennes de Confessore" (K339, 1780), "Vesperae de Dominica", his arrangement of Handel's "Messiah" plus two of his three great operas: Don Giovanni (K527, 1787) and Die Zauberflöte (K620, 1791). Mozart's first use of the trombone was an obligato line in the oratorio "Die Schuldigkeit des ersten Gebots" (K35, 1767)
Christoph Willibald Gluck includes trombones in five of his operas: "Iphigénie en Aulide" (1774), Orfeo ed Euridice (1774), "Alceste" (1776), Iphigénie en Tauride (1779) and "Echo et Narcisse" (1779), as well as ballet "Don Juan" (1761).
Some chamber music in this period includes trombone in an obligato role with voice, and also as a concerto instrument with string orchestra. Composers include the likes of Leopold Mozart, Georg Christoph Wagenseil, Johann Albrechtsberger, Michael Haydn and Johann Ernst Eberlin.
For works for trombone post-1800, please see trombone.
Modern performance.
Many groups specializing in period music make frequent and prominent use of the sackbut.
External links:
Recordings.
Plenty of recordings of the authentic sackbut are now available from the groups such as Concerto Palatino, HMSC, Gabrieli Consort and the Toulouse Sacqueboutiers. For a closer examination of the instrument, here are some recommended recordings where the sackbut is heavily featured in a 'solo' capacity.
Early surviving instruments.
The earliest instruments:
Other notable sackbuts:
For more information, see Herbert (2006).

</doc>
<doc id="26805" url="http://en.wikipedia.org/wiki?curid=26805" title="Sex">
Sex

Organisms of many species are specialized into male and female varieties, each known as a sex. Sexual reproduction involves the combining and mixing of genetic traits: specialized cells known as gametes combine to form offspring that inherit traits from each parent. Gametes can be identical in form and function (known as isogamy), but in many cases an asymmetry has evolved such that two sex-specific types of gametes (heterogametes) exist (known as anisogamy). By definition, male gametes are small, motile, and optimized to transport their genetic information over a distance, while female gametes are large, non-motile and contain the nutrients necessary for the early development of the young organism. Among humans and other mammals, males typically carry XY chromosomes, whereas females typically carry XX chromosomes, which are a part of the XY sex-determination system.
The gametes produced by an organism determine its sex: males produce male gametes (spermatozoa, or sperm, in animals; pollen in plants) while females produce female gametes (ova, or egg cells); individual organisms which produce both male and female gametes are termed hermaphroditic. Frequently, physical differences are associated with the different sexes of an organism; these sexual dimorphisms can reflect the different reproductive pressures the sexes experience.
Evolution.
It is considered that sexual reproduction first appeared about a billion years ago, evolved within ancestral single-celled eukaryotes. The reason for the initial evolution of sex, and the reason(s) it has survived to the present, are still matters of debate. Some of the many plausible theories include: that sex creates variation among offspring, sex helps in the spread of advantageous traits, and that sex helps in the removal of disadvantageous traits.
Sexual reproduction is a process specific to eukaryotes, organisms whose cells contain a nucleus and mitochondria. In addition to animals, plants, and fungi, other eukaryotes (e.g. the malaria parasite) also engage in sexual reproduction. Some bacteria use conjugation to transfer genetic material between cells; while not the same as sexual reproduction, this also results in the mixture of genetic traits.
What is considered defining of sexual reproduction in eukaryotes is the difference between the gametes and the binary nature of fertilization. Multiplicity of gamete types within a species would still be considered a form of sexual reproduction. However, no third gamete is known in multicellular animals.
While the evolution of sex itself dates to the prokaryote or early eukaryote stage, the origin of chromosomal sex determination may have been fairly early in eukaryotes. The ZW sex-determination system is shared by birds, some fish and some crustaceans. Most mammals, but also some insects ("Drosophila") and plants ("Ginkgo") use XY sex-determination.
X0 sex-determination is found in certain insects.
No genes are shared between the avian ZW and mammal XY chromosomes, and from a comparison between chicken and human, the Z chromosome appeared similar to the autosomal chromosome 9 in human, rather than X or Y, suggesting that the ZW and XY sex-determination systems do not share an origin, but that the sex chromosomes are derived from autosomal chromosomes of the common ancestor of birds and mammals.
A paper from 2004 compared the chicken Z chromosome with platypus X chromosomes and suggested that the two systems are related.
Sexual reproduction.
Sexual reproduction in eukaryotes is a process whereby organisms form offspring that combine genetic traits from both parents. Chromosomes are passed on from one generation to the next in this process. Each cell in the offspring has half the chromosomes of the mother and half of the father.
Genetic traits are contained within the deoxyribonucleic acid (DNA) of chromosomes—by combining one of each type of chromosomes from each parent, an organism is formed containing a doubled set of chromosomes. This double-chromosome stage is called "diploid", while the single-chromosome stage is "haploid". Diploid organisms can, in turn, form haploid cells (gametes) that randomly contain one of each of the chromosome pairs, via meiosis. Meiosis also involves a stage of chromosomal crossover, in which regions of DNA are exchanged between matched types of chromosomes, to form a new pair of mixed chromosomes. Crossing over and fertilization (the recombining of single sets of chromosomes to make a new diploid) result in the new organism containing a different set of genetic traits from either parent.
In many organisms, the haploid stage has been reduced to just gametes specialized to recombine and form a new diploid organism; in others, the gametes are capable of undergoing cell division to produce multicellular haploid organisms. In either case, gametes may be externally similar, particularly in size (isogamy), or may have evolved an asymmetry such that the gametes are different in size and other aspects (anisogamy).
By convention, the larger gamete (called an ovum, or egg cell) is considered female, while the smaller gamete (called a spermatozoon, or sperm cell) is considered male. An individual that produces exclusively large gametes is female, and one that produces exclusively small gametes is male. An individual that produces both types of gametes is a hermaphrodite; in some cases hermaphrodites are able to self-fertilize and produce offspring on their own, without a second organism.
Animals.
Most sexually reproducing animals spend their lives as diploid organisms, with the haploid stage reduced to single cell gametes. The gametes of animals have male and female forms—spermatozoa and egg cells. These gametes combine to form embryos which develop into a new organism.
The male gamete, a spermatozoon (produced within a testicle), is a small cell containing a single long flagellum which propels it.
Spermatozoa are extremely reduced cells, lacking many cellular components that would be necessary for embryonic development. They are specialized for motility, seeking out an egg cell and fusing with it in a process called fertilization.
Female gametes are egg cells (produced within ovaries), large immobile cells that contain the nutrients and cellular components necessary for a developing embryo.
Egg cells are often associated with other cells which support the development of the embryo, forming an egg. In mammals, the fertilized embryo instead develops within the female, receiving nutrition directly from its mother.
Animals are usually mobile and seek out a partner of the opposite sex for mating. Animals which live in the water can mate using external fertilization, where the eggs and sperm are released into and combine within the surrounding water. Most animals that live outside of water, however, must transfer sperm from male to female to achieve internal fertilization.
In most birds, both excretion and reproduction is done through a single posterior opening, called the cloaca—male and female birds touch cloaca to transfer sperm, a process called "cloacal kissing". In many other terrestrial animals, males use specialized sex organs to assist the transport of sperm—these male sex organs are called intromittent organs. In humans and other mammals this male organ is the penis, which enters the female reproductive tract (called the vagina) to achieve insemination—a process called sexual intercourse. The penis contains a tube through which semen (a fluid containing sperm) travels. In female mammals the vagina connects with the uterus, an organ which directly supports the development of a fertilized embryo within (a process called gestation).
Because of their motility, animal sexual behavior can involve coercive sex. Traumatic insemination, for example, is used by some insect species to inseminate females through a wound in the abdominal cavity—a process detrimental to the female's health.
Plants.
Like animals, plants have developed specialized male and female gametes. Within most familiar plants, male gametes are contained within hard coats, forming pollen. The female gametes of plants are contained within ovules; once fertilized by pollen these form seeds which, like eggs, contain the nutrients necessary for the development of the embryonic plant.
Female (left) and male (right) cones are the sex organs of pines and other conifers.
Many plants have flowers and these are the sexual organs of those plants. Flowers are usually hermaphroditic, producing both male and female gametes. The female parts, in the center of a flower, are the carpels—one or more of these may be merged to form a single pistil. Within carpels are ovules which develop into seeds after fertilization. The male parts of the flower are the stamens: these long filamentous organs are arranged between the pistil and the petals and produce pollen at their tips. When a pollen grain lands upon the top of a carpel, the tissues of the plant react to transport the grain down into the carpel to merge with an ovule, eventually forming seeds.
In pines and other conifers the sex organs are conifer cones and have male and female forms. The more familiar female cones are typically more durable, containing ovules within them. Male cones are smaller and produce pollen which is transported by wind to land in female cones. As with flowers, seeds form within the female cone after pollination.
Because plants are immobile, they depend upon passive methods for transporting pollen grains to other plants. Many plants, including conifers and grasses, produce lightweight pollen which is carried by wind to neighboring plants. Other plants have heavier, sticky pollen that is specialized for transportation by insects. The plants attract these insects with nectar-containing flowers. Insects transport the pollen as they move to other flowers, which also contain female reproductive organs, resulting in pollination.
Fungi.
Most fungi reproduce sexually, having both a haploid and diploid stage in their life cycles. These fungi are typically isogamous, lacking male and female specialization: haploid fungi grow into contact with each other and then fuse their cells. In some of these cases the fusion is asymmetric, and the cell which donates only a nucleus (and not accompanying cellular material) could arguably be considered "male".
Some fungi, including baker's yeast, have mating types that create a duality similar to male and female roles. Yeast with the same mating type will not fuse with each other to form diploid cells, only with yeast carrying the other mating type.
Fungi produce mushrooms as part of their sexual reproduction. Within the mushroom diploid cells are formed, later dividing into haploid spores—the height of the mushroom aids the dispersal of these sexually produced offspring.
Sex determination.
The most basic sexual system is one in which all organisms are hermaphrodites, producing both male and female gametes— this is true of some animals (e.g. snails) and the majority of flowering plants. In many cases, however, specialization of sex has evolved such that some organisms produce only male or only female gametes. The biological cause for an organism developing into one sex or the other is called sex determination.
In the majority of species with sex specialization, organisms are either male (producing only male gametes) or female (producing only female gametes). Exceptions are common—for example, in the roundworm "C. elegans" the two sexes are hermaphrodite and male (a system called androdioecy).
Sometimes an organism's development is intermediate between male and female, a condition called intersex. Sometimes intersex individuals are called "hermaphrodite"; but, unlike biological hermaphrodites, intersex individuals are unusual cases and are not typically fertile in both male and female aspects.
Genetic.
In genetic sex-determination systems, an organism's sex is determined by the genome it inherits. Genetic sex-determination usually depends on asymmetrically inherited sex chromosomes which carry genetic features that influence development; sex may be determined either by the presence of a sex chromosome or by how many the organism has. Genetic sex-determination, because it is determined by chromosome assortment, usually results in a 1:1 ratio of male and female offspring.
Humans and other mammals have an XY sex-determination system: the Y chromosome carries factors responsible for triggering male development. The default sex, in the absence of a Y chromosome, is female. Thus, XX mammals are female and XY are male. XY sex determination is found in other organisms, including the common fruit fly and some plants. In some cases, including in the fruit fly, it is the number of X chromosomes that determines sex rather than the presence of a Y chromosome (see below).
In birds, which have a ZW sex-determination system, the opposite is true: the W chromosome carries factors responsible for female development, and default development is male. In this case ZZ individuals are male and ZW are female. The majority of butterflies and moths also have a ZW sex-determination system. In both XY and ZW sex determination systems, the sex chromosome carrying the critical factors is often significantly smaller, carrying little more than the genes necessary for triggering the development of a given sex.
Many insects use a sex determination system based on the number of sex chromosomes. This is called X0 sex-determination—the 0 indicates the absence of the sex chromosome. All other chromosomes in these organisms are diploid, but organisms may inherit one or two X chromosomes. In field crickets, for example, insects with a single X chromosome develop as male, while those with two develop as female. In the nematode "C. elegans" most worms are self-fertilizing XX hermaphrodites, but occasionally abnormalities in chromosome inheritance regularly give rise to individuals with only one X chromosome—these X0 individuals are fertile males (and half their offspring are male).
Other insects, including honey bees and ants, use a haplodiploid sex-determination system. In this case diploid individuals are generally female, and haploid individuals (which develop from unfertilized eggs) are male. This sex-determination system results in highly biased sex ratios, as the sex of offspring is determined by fertilization rather than the assortment of chromosomes during meiosis.
Nongenetic.
For many species, sex is not determined by inherited traits, but instead by environmental factors experienced during development or later in life. Many reptiles have temperature-dependent sex determination: the temperature embryos experience during their development determines the sex of the organism. In some turtles, for example, males are produced at lower incubation temperatures than females; this difference in critical temperatures can be as little as 1–2 °C.
Many fish change sex over the course of their lifespan, a phenomenon called sequential hermaphroditism. In clownfish, smaller fish are male, and the dominant and largest fish in a group becomes female. In many wrasses the opposite is true—most fish are initially female and become male when they reach a certain size. Sequential hermaphrodites may produce both types of gametes over the course of their lifetime, but at any given point they are either female or male.
In some ferns the default sex is hermaphrodite, but ferns which grow in soil that has previously supported hermaphrodites are influenced by residual hormones to instead develop as male.
Sexual dimorphism.
Many animals and some plants have differences between the male and female sexes in size and appearance, a phenomenon called sexual dimorphism. Sex differences in humans include, generally, a larger size and more body hair in men; women have breasts, wider hips, and a higher body fat percentage. In other species, the differences may be more extreme, such as differences in coloration or bodyweight. In humans, biological sex is determined by five factors present at birth: the presence or absence of a Y chromosome, the type of gonads, the sex hormones, the internal reproductive anatomy (such as the uterus in females), and the external genitalia.
Sexual dimorphisms in animals are often associated with sexual selection – the competition between individuals of one sex to mate with the opposite sex. Antlers in male deer, for example, are used in combat between males to win reproductive access to female deer. In many cases the male of a species is larger than the female. Mammal species with extreme sexual size dimorphism tend to have highly polygynous mating systems—presumably due to selection for success in competition with other males—such as the elephant seals. Other examples demonstrate that it is the preference of females that drive sexual dimorphism, such as in the case of the stalk-eyed fly.
Other animals, including most insects and many fish, have larger females. This may be associated with the cost of producing egg cells, which requires more nutrition than producing sperm—larger females are able to produce more eggs. For example, female southern black widow spiders are typically twice as long as the males. Occasionally this dimorphism is extreme, with males reduced to living as parasites dependent on the female, such as in the anglerfish. Some plant species also exhibit dimorphism in which the females are significantly larger than the males, such as in the moss "Dicranum" and the liverwort "Sphaerocarpos". There is some evidence that, in these genera, the dimorphism may be tied to a sex chromosome, or to chemical signalling from females.
In birds, males often have a more colourful appearance and may have features (like the long tail of male peacocks) that would seem to put the organism at a disadvantage (e.g. bright colors would seem to make a bird more visible to predators). One proposed explanation for this is the handicap principle. This hypothesis says that, by demonstrating he can survive with such handicaps, the male is advertising his genetic fitness to females—traits that will benefit daughters as well, who will not be encumbered with such handicaps.
Further reading.
</dl>

</doc>
<doc id="26818" url="http://en.wikipedia.org/wiki?curid=26818" title="Stagflation">
Stagflation

In economics, stagflation, a portmanteau of "stagnation" and "inflation", is a situation where the inflation rate is high, the economic growth rate slows down, and unemployment remains steadily high. It raises a dilemma for economic policy since actions designed to lower inflation may exacerbate unemployment, and vice versa.
The term is generally attributed to a British Conservative Party politician who became chancellor of the exchequer in 1970, Iain Macleod, who coined the phrase in his speech to Parliament in 1965. 
Keynes didn't use the term, but some of his work refers to the conditions most would recognise as stagflation. In the version of Keynesian macroeconomic theory which was dominant between the end of WWII and the late-1970s, inflation and recession were regarded as mutually exclusive, the relationship between the two being described by the Phillips curve. Stagflation is very costly and difficult to eradicate once it starts, in human terms as well as in budget deficits.
One economic indicator, the misery index, is derived by the simple addition of the inflation rate to the unemployment rate.
The Great Inflation.
The term stagflation was first coined during a period of inflation and unemployment in the United Kingdom. The United Kingdom experienced an outbreak of inflation in the 1960s and 1970s. As early as 17 November 1965, Iain Macleod, the spokesman on economic issues for the United Kingdom’s Conservative Party, warned of the gravity of the UK economic situation in the House of Commons: "We now have the worst of both worlds—not just inflation on the one side or stagnation on the other, but both of them together. We have a sort of
“stagflation” situation. And history, in modern terms, is indeed being made."
With these words, Macleod coined the term ‘stagflation’. In a Bank of England working papers series article authors, Edward Nelson and Kalin Nikolov, (2002) examined causes and policy errors related to the Great Inflation in the United Kingdom in the 1970s, arguing that as inflation rose in the 1960s and 1970s, UK policy makers failed to recognize the primary role of monetary policy in controlling inflation. Instead, they attempted to use non-monetary policies and devices to respond to the economic crisis. Policy makers also made "inaccurate estimates of the degree of excess demand in the economy, contributed significantly to the outbreak of inflation in the United Kingdom in the 1960s and 1970s.
Stagflation was not limited to the UK, however. Economists have shown that stagflation was prevalent among seven major economies from 1973 to 1982. After inflation rates began to fall in 1982, economists' focus shifted from the causes of stagflation to the "determinants of productivity growth and the effects of real wages on the demand for labor".
Causes.
Economists offer two principal explanations for why stagflation occurs. First, stagflation can result when the productive capacity of an economy is reduced by an unfavorable supply shock that causes an increase in the price of oil for an oil importing country. Such an unfavorable supply shock tends to raise prices at the same time that it slows the economy by making production more costly and less profitable.
Milton Friedman famously described this situation as "too much money chasing too few goods".
Second, both stagnation and inflation can result from inappropriate macroeconomic policies. For example, central banks can cause inflation by permitting excessive growth of the money supply, and the government can cause stagnation by excessive regulation of goods markets and labour markets. Either of these factors can cause stagflation. Excessive growth of the money supply taken to such an extreme that it must be reversed abruptly can clearly be a cause. Both types of explanations are offered in analyses of the global stagflation of the 1970s: it began with a huge rise in oil prices, but then continued as central banks used excessively stimulative monetary policy to counteract the resulting recession, causing a runaway price/wage spiral.
Postwar Keynesian and monetarist views.
Early Keynesianism and monetarism.
Up to the 1960s many Keynesian economists ignored the possibility of stagflation, because historical experience suggested that high unemployment was typically associated with low inflation, and vice versa (this relationship is called the "Phillips curve"). The idea was that high demand for goods drives up prices, and also encourages firms to hire more; and likewise high employment raises demand. However, in the 1970s and 1980s, when stagflation occurred, it became obvious that the relationship between inflation and employment levels was not necessarily stable: that is, the Phillips relationship could shift. Macroeconomists became more skeptical of Keynesian theories, and the Keynesians themselves reconsidered their ideas in search of an explanation of stagflation.
The explanation for the shift of the Phillips curve was initially provided by the monetarist economist Milton Friedman, and also by Edmund Phelps. Both argued that when workers and firms begin to expect more inflation, the Phillips curve shifts up (meaning that more inflation occurs at any given level of unemployment). In particular, they suggested that if inflation lasted for several years, workers and firms would start to take it into account during wage negotiations, causing workers' wages and firms' costs to rise more quickly, thus further increasing inflation. While this idea was a severe criticism of early Keynesian theories, it was gradually accepted by most Keynesians, and has been incorporated into New Keynesian economic models.
Neo-Keynesianism.
Neo-Keynesian theory distinguished two distinct kinds of inflation: demand-pull (caused by shifts of the aggregate demand curve) and cost-push (caused by shifts of the aggregate supply curve). Stagflation, in this view, is caused by cost-push inflation. Cost-push inflation occurs when some force or condition increases the costs of production. This could be caused by government policies (such as taxes), or from purely external factors such as a shortage of natural resources or an act of war.
Contemporary Keynesian analyses argue that stagflation can be understood by distinguishing factors that affect aggregate demand from those that affect aggregate supply. While monetary and fiscal policy can be used to stabilise the economy in the face of aggregate demand fluctuations, they are not very useful in confronting aggregate supply fluctuations. In particular, an adverse shock to aggregate supply, such as an increase in oil prices, can give rise to stagflation.
Supply theory.
Fundamentals.
Supply theories are based on the neo-Keynesian cost-push model and attribute stagflation to significant disruptions to the supply side of the supply-demand market equation, for example, when there is a sudden real or relative scarcity of key commodities, natural resources, or natural capital needed to produce goods and services. Other factors may also cause supply problems, for example, social and political conditions such as policy changes, acts of war, extremely restrictive government control of production (For example monopolising Dictatorships). In this view, stagflation is thought to occur when there is an adverse supply shock (for example, a sudden increase in the price of oil or a new tax) that causes a subsequent jump in the "cost" of goods and services (often at the wholesale level). In technical terms, this results in contraction or negative shift in an economy's aggregate supply curve.
In the resource scarcity scenario (Zinam 1982), stagflation results when economic growth is inhibited by a restricted supply of raw materials. That is, when the actual or relative supply of basic materials (fossil fuels (energy), minerals, agricultural land in production, timber, etc.) decreases and/or cannot be increased fast enough in response to rising or continuing demand. The resource shortage may be a real physical shortage or a relative scarcity due to factors such as taxes or bad monetary policy which have affected the "cost" or availability of raw materials. This is consistent with the cost-push inflation factors in neo-Keynesian theory (above). The way this plays out is that after supply shock occurs, the economy will first try to maintain momentum – that is, consumers and businesses will begin paying higher prices in order to maintain their level of demand. The central bank may exacerbate this by increasing the money supply, by lowering interest rates for example, in an effort to combat a recession. The increased money supply props up the demand for goods and services, though demand would normally drop during a recession.
In the Keynesian model, higher prices will prompt increases in the supply of goods and services. However, during a supply shock (i.e. scarcity, "bottleneck" in resources, etc.), supplies don't respond as they normally would to these price pressures. So, inflation jumps and output drops, producing stagflation.
Explaining the 1970s stagflation.
Following Richard Nixon's imposition of wage and price controls on 15 August 1971, an initial wave of cost-push shocks in commodities were blamed for causing spiraling prices. Perhaps the most notorious factor cited at that time was the failure of the Peruvian anchovy fishery in 1972, a major source of livestock feed. The second major shock was the 1973 oil crisis, when the Organization of Petroleum Exporting Countries (OPEC) constrained the worldwide supply of oil. Both events, combined with the overall energy shortage that characterized the 1970s, resulted in actual or relative scarcity of raw materials. The price controls resulted in shortages at the point of purchase, causing, for example, queues of consumers at fuelling stations and increased production costs for industry.
Theoretical responses.
Under this set of theories, the solution to stagflation is to restore the supply of materials. In the case of a physical scarcity, stagflation is mitigated either by finding a replacement for the missing resources or by developing ways to increase economic productivity and energy efficiency so that more output is produced with less input. For example, in the late 1970s, the scarcity of oil was relieved by increases in both energy efficiency and global oil production. This factor, along with adjustments in monetary policies, helped end stagflation.
Recent views.
Through the mid-1970s, none of the major macroeconomic models (Keynesian, New Classical, and monetarist) were able to explain stagflation.
After several years of research, a convincing explanation was provided based on the effects of adverse supply shocks on both prices and output. According to Blanchard (2009), these adverse events were one of two components of stagflation; the other was "ideas", which Robert Lucas (famous for the Lucas Supply Curve), Thomas Sargent, and Robert Barro were cited as expressing as "wildly incorrect" and "fundamentally flawed" predictions [of Keynesian economics] which, they said, left stagflation to be explained by "contemporary students of the business cycle". In this discussion, Blanchard hypothesizes that the recent oil price increases could trigger another period of stagflation, although this has not yet happened (pg. 152).
Neoclassical views.
A purely neoclassical view of the macroeconomy rejects the idea that monetary policy can have real effects. Neoclassical macroeconomists argue that real economic quantities, like real output, employment, and unemployment, are determined by real factors only. Nominal factors like changes in the money supply only affect nominal variables like inflation. The neoclassical idea that nominal factors cannot have real effects is often called "monetary neutrality" or also the "classical dichotomy".
Since the neoclassical viewpoint says that real phenomena like unemployment are essentially unrelated to nominal phenomena like inflation, a neoclassical economist would offer two separate explanations for 'stagnation' and 'inflation'. Neoclassical explanations of stagnation (low growth and high unemployment) include inefficient government regulations or high benefits for the unemployed that give people less incentive to look for jobs. Another neoclassical explanation of stagnation is given by real business cycle theory, in which any decrease in labour productivity makes it efficient to work less. The main neoclassical explanation of inflation is very simple: it happens when the monetary authorities increase the money supply too much.
In the neoclassical viewpoint, the real factors that determine output and unemployment affect the aggregate supply curve only. The nominal factors that determine inflation affect the aggregate demand curve only. When some adverse changes in real factors are shifting the aggregate supply curve left at the same time that unwise monetary policies are shifting the aggregate demand curve right, the result is stagflation.
Thus the main explanation for stagflation under a classical view of the economy is simply policy errors that affect both inflation and the labour market. Ironically, a very clear argument in favour of the classical explanation of stagflation was provided by Keynes himself. In 1919, John Maynard Keynes described the inflation and economic stagnation gripping Europe in his book The Economic Consequences of the Peace. Keynes wrote:
Keynes explicitly pointed out the relationship between governments printing money and inflation.
Keynes also pointed out how government price controls discourage production.
Keynes detailed the relationship between German government deficits and inflation.
Keynesian in the short run, classical in the long run.
While most economists believe that changes in money supply can have some real effects in the short run, neoclassical and neo-Keynesian economists tend to agree that there are no long-run effects from changing the money supply. Therefore, even economists who consider themselves neo-Keynesians usually believe that in the long run, money is neutral. In other words, while neoclassical and neo-Keynesian models are often seen as competing points of view, they can also be seen as two descriptions appropriate for different time horizons. Many mainstream textbooks today treat the neo-Keynesian model as a more appropriate description of the economy in the short run, when prices are 'sticky', and treat the neoclassical model as a more appropriate description of the economy in the long run, when prices have sufficient time to adjust fully.
Therefore, while mainstream economists today might often attribute short periods of stagflation (not more than a few years) to adverse changes in supply, they would not accept this as an explanation of very prolonged stagflation. More prolonged stagflation would be explained as the effect of inappropriate government policies: excessive regulation of product markets and labor markets leading to long-run stagnation, and excessive growth of the money supply leading to long-run inflation.
Alternative views.
As differential accumulation.
Political economists Jonathan Nitzan and Shimshon Bichler have proposed an explanation of stagflation as part of a theory they call differential accumulation, which says firms seek to beat the average profit and capitalisation rather than maximise. According to this theory, periods of mergers and acquisitions oscillate with periods of stagflation. When mergers and acquisitions are no longer politically feasible (governments clamp down with anti-monopoly rules), stagflation is used as an alternative to have higher relative profit than the competition. With increasing mergers and acquisitions, the power to implement stagflation increases.
Stagflation appears as a societal crisis, such as during the period of the oil crisis in the 70s and in 2007 to 2010. Inflation in stagflation, however, doesn't affect all firms equally. Dominant firms are able to increase their own prices at a faster rate than competitors. While in the aggregate no one appears to be profiting, differentially dominant firms improve their positions with higher relative profits and higher relative capitalisation. Stagflation is not due to any actual supply shock, but because of the societal crisis that hints at a supply crisis. It is mostly a 20th and 21st century phenomenon that has been mainly used by the "weapondollar-petrodollar coalition" creating or using Middle East crises for the benefit of pecuniary interests.
Demand-pull stagflation theory.
Demand-pull stagflation theory explores the idea that stagflation can result exclusively from monetary shocks without any concurrent supply shocks or negative shifts in economic output potential. Demand-pull theory describes a scenario where stagflation can occur following a period of monetary policy implementations that cause inflation. This theory was first proposed in 1999 by Eduardo Loyo of Harvard University's John F. Kennedy School of Government.
Supply-side theory.
Supply-side economics emerged as a response to US stagflation in the 1970s. It largely attributed inflation to the ending of the Bretton Woods system in 1971 and the lack of a specific price reference in the subsequent monetary policies (Keynesian and Monetarism). Supply-side economists asserted that the contraction component of stagflation resulted from an inflation-induced rise in real tax rates (see bracket creep)
Austrian School of economics.
Adherents to the Austrian School maintain that creation of new money ex nihilo benefits the creators and early recipients of the new money relative to late recipients. Money creation is not wealth creation; it merely allows early money recipients to outbid late recipients for resources, goods, and services.
Since the actual producers of wealth are typically late recipients, increases in the money supply weakens wealth formation and undermines the rate of economic growth. Says Austrian economist Frank Shostak:
"The increase in the money supply rate of growth coupled with the slowdown in the rate of growth of goods produced is what the increase in the rate of price inflation is all about. (Note that a price is the amount of money paid for a unit of a good.) What we have here is a faster increase in price inflation and a decline in the rate of growth in the production of goods. But this is exactly what stagflation is all about, i.e., an increase in price inflation and a fall in real economic growth. Popular opinion is that stagflation is totally made up. It seems therefore that the phenomenon of stagflation is the normal outcome of loose monetary policy. This is in agreement with [Phelps and Friedman (PF)]. Contrary to PF, however, we maintain that stagflation is not caused by the fact that in the short run people are fooled by the central bank. Stagflation is the natural result of monetary pumping which weakens the pace of economic growth and at the same time raises the rate of increase of the prices of goods and services."
Jane Jacobs and the influence of cities on stagflation.
In 1984, journalist and activist Jane Jacobs proposed the failure of major macroeconomic theories to explain stagflation was due to their focus on the nation as the salient unit of economic analysis, rather than the city. She proposed the key to avoiding stagflation was for a nation to focus on the development of "import-replacing cities", which would experience economic ups and downs at different times, providing overall national stability and avoiding widespread stagflation. According to Jacobs, import-replacing cities are those which have developed economies balancing their own production with domestic imports, meaning they can respond with flexibility as economic supply and demand cycles change over time. While lauding her originality, clarity, and consistency, urban planning scholars have criticized Jacobs for not comparing her own ideas to those of major theorists (e.g., Adam Smith, Karl Marx) with the same depth and breadth they developed, as well as a lack of scholarly documentation. Despite these issues, Jacobs' work is notable for having widespread public readership and influence on decision-makers.
Responses.
Stagflation undermined support for Keynesian consensus. The rise of conservative theories of economics, including monetarism, can be traced to the failure of Keynesian policies to combat stagflation or explain it to the satisfaction of economists and policy-makers.
Federal Reserve chairman Paul Volcker very sharply increased interest rates from 1979–1983 in what was called a "disinflationary scenario." After U.S. prime interest rates had soared into the double-digits, inflation did come down; these interest rates were the highest long-term prime interest rates that had ever existed in modern capital markets. Volcker is often credited with having stopped at least the inflationary side of stagflation, although the American economy also dipped into recession. Starting in approximately 1983, growth began a recovery. Both fiscal stimulus and money supply growth were policy at this time. A five- to six-year jump in unemployment during the Volcker disinflation suggests Volcker may have trusted unemployment to self-correct and return to its natural rate within a reasonable period.
Further reading.
</dl>

</doc>
<doc id="26824" url="http://en.wikipedia.org/wiki?curid=26824" title="State Street Corporation">
State Street Corporation

State Street Corporation, known as State Street, is a US-based international financial services holding company. State Street was founded in 1792 and is the second oldest financial institution in the United States. The company’s headquarters are at One Lincoln Street in Boston and it has offices in 25 countries around the world.
State Street is organized into three main divisions. The Global Services business is a custodian bank with $28 trillion (USD) of assets under custody and administration. The Global Advisors business provides investment management services and has $2.3 trillion (USD) of assets under management. The Global Markets business offers investment research and trading services to institutional investors.
History.
State Street’s past can be dated back to the founding years of Boston’s banking industry. In 1792 the Union Bank became the third bank to be chartered in Boston and was located at the corner of State and Exchange Streets. State Street was known as the “Great Street to the Sea” as Boston became a flourishing maritime capital. The clipper in State Street’s logo today reflects this period.
In 1865 the Union Bank received a national charter and became the National Union Bank of Boston. State Street Deposit & Trust Co opened alongside National Union in 1891. It became the custodian of the first US mutual fund in 1924, the Massachusetts Investors Trust. State Street and National Union merged in 1925.
State Street’s growth during the mid-1900s was fuelled by mergers and acquisitions. It merged with the Second National Bank in 1955 and with the Rockland-Atlas National Bank in 1961. William Edgerly gained control in 1975 and shifted the company’s strategy from commercial banking to investments and securities processing.
The company began investing heavily in technologies for securities management and custodian processing. It was helped by a partial acquisition of Boston Financial Data Services in 1973. More than 100 top staff from IBM were headhunted by State Street as it set about implementing IBM mainframe systems.
State Street’s new building was completed in 1966 and became the first high-rise office tower in downtown Boston. In 1972 the company opened its first international office in Munich. For much of the 1980s and 1990s it expanded to foreign markets with offices in Montreal, Toronto, Dublin, London, Paris, Dubai, Sydney, Wellington, Hong Kong, and Tokyo.
It was the early 1990s before State Street brought its technology platform to international markets. By 1992 most of State Street’s revenue came from fees for holding securities, settling trades, keeping records, and performing accounting. It formed a new global asset management business in 1994 and in 1999 divested its retail and commercial banking businesses to Citizens Financial Group.
State Street acquired Kansas City, Missouri-based Investors Fiduciary Trust Co. in 1995 for $162 million (USD) from DST Systems, and Kemper Financial Services. In 2003 it purchased Deutsche Bank’s securities services division for $1.5 billion (USD). State Street purchased Investors Financial Services for $4.5 billion (USD) in 2007. In 2010 it acquired Mourant International Finance Administration and the securities services group of Intesa Sanpaolo.
State Street was named by the G-20 as amongst the world’s 29 systemic banks and must meet all conditions of the Basel III accord. The company now employs 29,530 people around the world. It claims to have funds under management of $2.3 trillion (USD) and assets under custody and administration of $28 trillion (USD), second to The Bank of New York Mellon.
Organization.
State Street Global Advisors.
Global Advisors is State Street’s asset management business and dates back to 1978. It provides investment management, research, and advisory services to corporations, mutual funds, insurance companies, and other institutional investors. Global Advisors develops both passive and active management strategies using both quantitative and fundamental approaches.
It created the first exchange-traded fund in 1993, the SPDR S&P 500, and is now one of the world’s largest ETF providers. Global Advisors has staff in 27 global offices and claims to have over $2.3 trillion (USD) of funds under management.
In November 2014, State Street Global Advisors sold SSARIS to senior management.
State Street Global Markets.
Global Markets is State Street’s securities business. It offers research, trading, and securities lending services for foreign exchange, equities, fixed income, and derivatives. The company claims to be a trading partner free from conflicted interests as it does not run proprietary trading books. Global Markets maintains trading desks in Boston, London, Sydney, Toronto, and Tokyo.
State Street Global Services.
Global Services is the investment servicing division of State Street, also known as the State Street Bank & Trust Co. It provides asset owners and managers with custodian (safekeeping, corporate actions), fund accounting (pricing and valuation), and administration (financial reporting, tax, compliance, and legal) services.
Global Services handles assets from many classes, including equities, derivatives, exchange-traded funds, fixed income assets, private equity, and real estate. State Street now administers 40 percent of the assets under administration in the US mutual fund market. Global Services also provides outsourcing for operations activities and handles $10.2 trillion (USD) of middle-office assets.
Regulation.
State Street is registered with the Board of Governors of the Federal Reserve System as a bank holding company pursuant to the Bank Holding Company Act of 1956. It is a member of the Federal Reserve System and its deposits are insured by the Federal Deposit Insurance Corporation. Certain aspects of State Street’s public disclosure are subject to the requirements of the Sarbanes–Oxley Act of 2002.
State Street’s broker-dealer operation, known as Global Markets, is registered with and regulated by the Securities and Exchange Commission and the New York Stock Exchange in the United States. The Prudential Regulation Authority and the London Stock Exchange regulate State Street in the United Kingdom.
Controversies.
In 2009 the State of California alleged on behalf of its pension funds CalPERS and CalSTRS that State Street had committed fraud on currency trades handled by the custodian bank. Two executives from State Street Global Markets left the company in October 2011 following enquiries over the pricing of a fixed income transaction.
State Street in December 2010 announced that it would be retrenching 5% of its workforce and effectively reducing the wages of remaining employees by 10%. In March 2011 it reversed its wage-reduction decision but declared that it would still require all employees to work a longer 40 hour week.
On 28 February 2012, State Street Global Advisors entered into a consent order with the Massachusetts Securities Division. The Division was investigating SSGA’s role as the investment manager of a $1.65 billion (USD) hybrid collateralized debt obligation. The investigation resulted in a fine of $5 million (USD) for the non-disclosure of certain initial investors taking a short position on portions of the CDO.
State Street Bank has been accused of "stealth outsourcing" or transferring American jobs to their outsourcing partners Syntel and HCL-India under the radar, in small increments to avoid any political backlash. State Street Bank - the second largest client of Syntel - also has a joint venture in the Indian city of Pune with them which they have the option to buy out. The controversy is compounded by the fact State Street bank received an $11.5 million tax incentive from the city of Boston to move into a new location in the South Boston Innovation District as well as 2 billion in TARP assistance all while still sending jobs overseas.
During the May 2012 annual shareholders meeting, chairman and chief executive Jay Hooley was shouted down on numerous occasions by protesters in relation to the outsourcing and other grievances.
See also.
"State Street Bank v. Signature Financial Group" is the landmark case in which the Court of Appeals for the Federal Circuit ruled (23 July 1998) that a computer algorithm can be patented to the extent that it produces "a useful, concrete and tangible result".

</doc>
<doc id="26826" url="http://en.wikipedia.org/wiki?curid=26826" title="Sodium">
Sodium

Sodium is a chemical element with symbol Na (from Latin: "natrium") and atomic number 11. It is a soft, silver-white, highly reactive metal and is a member of the alkali metals; its only stable isotope is 23Na. The free metal does not occur in nature, but instead must be prepared from its compounds. It was first isolated by Humphry Davy in 1807 by the electrolysis of sodium hydroxide. Sodium is the sixth most abundant element in the Earth's crust, and exists in numerous minerals such as feldspars, sodalite and rock salt (NaCl). Many salts of sodium are highly water-soluble. Sodium ions have been leached by the action of water so that sodium and chlorine (Cl) are the most common dissolved elements by weight in the Earth's bodies of oceanic water.
Many sodium compounds are useful, such as sodium hydroxide (lye) for soap-making, and sodium chloride for use as a de-icing agent and a nutrient (edible salt). Sodium is an essential element for all animals and some plants. In animals, sodium ions are used against potassium ions to build up charges on cell membranes, allowing transmission of nerve impulses when the charge is dissipated. The consequent need of animals for sodium causes it to be classified as a dietary inorganic macro-mineral nutrient.
Characteristics.
Physical.
Sodium at standard temperature and pressure is a soft silvery metal, that oxidizes to grayish white unless immersed in oil or inert gas. Sodium can be readily cut with a knife, and is a good conductor of electricity. These properties change dramatically at elevated pressures: at 1.5 Mbar, the color changes from silvery metallic to black; at 1.9 Mbar the material becomes transparent, with a red color; and at 3 Mbar sodium is a clear and transparent solid. All of these high-pressure allotropes are insulators and electrides.
When sodium or its compounds are introduced into a flame, they turn it yellow, because the excited 3s electrons of sodium emit a photon when they fall from 3p to 3s; the wavelength of this photon corresponds to the D line at 589.3 nm. Spin-orbit interactions involving the electron in the 3p orbital split the D line into two; hyperfine structures involving both orbitals cause many more lines.
Chemical.
When freshly cut, sodium has a bright, silvery luster. If exposed to air, the surface rapidly tarnishes, darkening at first and then forming a white coating of sodium hydroxide and sodium carbonate.
Sodium is generally less reactive than potassium and more reactive than lithium. Like all the alkali metals, it reacts exothermically with water, to the point that sufficiently large pieces melt to a sphere and may explode; this reaction produces caustic soda (sodium hydroxide) and flammable hydrogen gas. When burned in dry air, it mainly forms sodium peroxide as well as some sodium oxide. In moist air, sodium hydroxide results. Sodium metal is highly reducing, with the reduction of sodium ions requiring −2.71 volts. Hence, the extraction of sodium metal from its compounds (such as with sodium chloride) uses a significant amount of energy. However, potassium and lithium have even more negative potentials.
Isotopes.
20 isotopes of sodium are known, but only 23Na is stable. Two radioactive, cosmogenic isotopes are the byproduct of cosmic ray spallation: 22Na with a half-life of 2.6 years and 24Na with a half-life of 15 hours; all other isotopes have a half-life of less than one minute. Two nuclear isomers have been discovered, the longer-lived one being 24mNa with a half-life of around 20.2 microseconds. Acute neutron radiation, such as from a nuclear criticality accident, converts some of the stable 23Na in human blood to 24Na; by measuring the concentration of 24Na in relation to 23Na, the neutron radiation dosage of the victim can be calculated.
Occurrence.
23Na is created in the carbon-burning process in stars by fusing two carbon atoms together; this requires temperatures above 600 megakelvins and a star of at least three solar masses. The Earth's crust contains 2.6% sodium by weight, making it the sixth most abundant element on Earth. Because of its high reactivity, it is never found as a pure element. It is found in many different minerals, some very soluble, such as halite and natron, others much less soluble such as amphibole, and zeolite. The insolubility of certain sodium minerals such as cryolite and feldspar arises from their polymeric anions, which in the case of feldspar is a polysilicate. In the interstellar medium, sodium is identified by the D spectral line; though it has a high vaporization temperature, its abundance allowed it to be detected by Mariner 10 in Mercury's atmosphere.
Compounds.
See also: .
Sodium compounds are of immense commercial importance, being particularly central to industries producing glass, paper, soap, and textiles. The sodium compounds that are the most important include table salt (NaCl), soda ash (Na2CO3), baking soda (NaHCO3), caustic soda (NaOH), sodium nitrate (NaNO3), di- and tri-sodium phosphates, sodium thiosulfate (Na2S2O3·5H2O), and borax (Na2B4O7·10H2O). In its compounds, sodium is usually ionically bonded to water and anions, and is viewed as a hard Lewis acid.
Most soaps are sodium salts of fatty acids. Sodium soaps are harder (higher melting) soaps than potassium soaps. Sodium chloride is extensively used for anti-icing and de-icing and as a preservative; sodium bicarbonate is mainly used for cooking. Along with potassium, many important medicines have sodium added to improve their bioavailability; although in most cases potassium is the better ion, sodium is selected for its lower price and atomic weight.Sodium hydride is used as a base for various reactions (such as the aldol reaction) in organic chemistry, and as a reducing agent in inorganic chemistry.
Aqueous solutions.
Sodium tends to form water-soluble compounds, such as halides, sulfates, nitrates, carboxylates and carbonates. The main aqueous species are the aquo complexes [Na(H2O)"n"]+, where "n" = 4–6. The high affinity of sodium for oxygen-based ligands is the basis of crown ethers; macrolide antibiotics, which interfere with Na+ transport in the infecting organism, are functionally related and more complex.
Direct precipitation of sodium salts from aqueous solutions is rare, because sodium salts typically have a high affinity for water; an exception is sodium bismuthate (NaBiO3). Because of this, sodium salts are usually isolated as solids by evaporation or by precipitation with an organic solvent, such as ethanol; for example, only 0.35 g/L of sodium chloride will dissolve in ethanol. Crown ethers, like 15-crown-5, may be used as a phase-transfer catalyst.
Sodium content in bulk may be determined by treating with a large excess of uranyl zinc acetate; the hexahydrate (UO2)2ZnNa(CH3CO2)·6H2O precipitates and can be weighed. Caesium and rubidium do not interfere with this reaction, but potassium and lithium do. Lower concentrations of sodium may be determined by atomic absorption spectrophotometry or by potentiometry using ion-selective electrodes.
Electrides and sodides.
Like the other alkali metals, sodium dissolves in ammonia and some amines to give deeply colored solutions; evaporation of these solutions leaves a shiny film of metallic sodium. The solutions contain the coordination complex (Na(NH3)6)+, whose positive charge is counterbalanced by electrons as anions; cryptands permit the isolation of these complexes as crystalline solids. Cryptands, like crown ethers and other ionophores, have a high affinity for the sodium ion; derivatives of the alkalide Na− are obtainable by the addition of cryptands to solutions of sodium in ammonia via disproportionation.
Organosodium compounds.
Many organosodium compounds have been prepared. Because of the high polarity of the C-Na bonds, they behave like sources of carbanions (salts with organic anions). Some well known derivatives include sodium cyclopentadienide (NaC5H5) and trityl sodium ((C6H5)3CNa).
History.
Salt has been an important commodity in human activities, as shown by the English word "salary", which derives from "salarium", the wafers of salt sometimes given to Roman soldiers along with their other wages. In medieval Europe, a compound of sodium with the Latin name of "sodanum" was used as a headache remedy. The name sodium is thought to originate from the Arabic "suda" , meaning headache, as the headache-alleviating properties of sodium carbonate or soda were well known in early times. The chemical abbreviation for sodium was first published by Jöns Jakob Berzelius in his system of atomic symbols, and is a contraction of the element's New Latin name "natrium", which refers to the Egyptian "natron", a natural mineral salt primarily made of hydrated sodium carbonate. Natron historically had several important industrial and household uses, later eclipsed by other sodium compounds. Although sodium, sometimes called "soda", had long been recognised in compounds, the metal itself was not isolated until 1807 by Sir Humphry Davy through the electrolysis of sodium hydroxide.
Sodium imparts an intense yellow color to flames. As early as 1860, Kirchhoff and Bunsen noted the high sensitivity of a sodium flame test, and stated in Annalen der Physik und Chemie:
Commercial production.
Enjoying rather specialized applications, only about 100,000 tonnes of metallic sodium are produced annually. Metallic sodium was first produced commercially in 1855 by carbothermal reduction of sodium carbonate at 1100 °C, in what is known as the Deville process:
A related process based on the reduction of sodium hydroxide was developed in 1886.
Sodium is now produced commercially through the electrolysis of molten sodium chloride, based on a process patented in 1924. This is done in a Downs cell in which the NaCl is mixed with calcium chloride to lower the melting point below 700 °C. As calcium is less electropositive than sodium, no calcium will be deposited at the cathode. This method is less expensive than the previous Castner process of electrolyzing sodium hydroxide.
Reagent-grade sodium in tonne quantities sold for about US$3.30/kg in 2009; lower purity metal sells for considerably less. The market for sodium is volatile due to the difficulty in its storage and shipping; it must be stored under a dry inert gas atmosphere or anhydrous mineral oil to prevent the formation of a surface layer of sodium oxide or sodium superoxide. These oxides can react violently in the presence of organic materials. Smaller quantities of sodium cost far more, in the range of US$165/kg; the high cost is partially due to the expense of shipping hazardous material.
Applications.
Though metallic sodium has some important uses, the major applications of sodium use is in its many compounds; millions of tons of the chloride, hydroxide, and carbonate are produced annually.
Free element.
Metallic sodium is mainly used for the production of sodium borohydride, sodium azide, indigo, and triphenylphosphine. Previous uses were for the making of tetraethyllead and titanium metal; because applications for these chemicals were discontinued, the production of sodium declined after 1970. Sodium is also used as an alloying metal, an anti-scaling agent, and as a reducing agent for metals when other materials are ineffective. Note the free element is not used as a scaling agent, ions in the water are exchanged for sodium ions. Sodium vapor lamps are often used for street lighting in cities and give colours ranging from yellow-orange to peach as the pressure increases. By itself or with potassium, sodium is a desiccant; it gives an intense blue colouration with benzophenone when the desiccate is dry. In organic synthesis, sodium is used in various reactions such as the Birch reduction, and the sodium fusion test is conducted to qualitatively analyse compounds. Lasers emitting light at the D line, utilising sodium, are used to create artificial laser guide stars that assist in the adaptive optics for land-based visible light telescopes.
Heat transfer.
Liquid sodium is used as a heat transfer fluid in some fast reactors, due to its high thermal conductivity and low neutron absorption cross section, which is required to achieve a high neutron flux; the high boiling point allows the reactor to operate at ambient pressure. Drawbacks of using sodium include its opacity, which hinders visual maintenance, and its explosive properties. Radioactive sodium-24 may be formed by neutron activation during operation, posing a slight radiation hazard; the radioactivity stops within a few days after removal from the reactor. If a reactor needs to be frequently shut down, NaK is used; due to it being liquid at room temperature, cooling pipes do not freeze. In this case, the pyrophoricity of potassium means extra precautions against leaks need to be taken. Another heat transfer application is in high-performance internal combustion engines with poppet valves, where valve stems partially filled with sodium are used as a heat pipe to cool the valves.
Biological role.
In humans, sodium is an essential nutrient that regulates blood volume, blood pressure, osmotic equilibrium and pH; the minimum physiological requirement for sodium is 500 milligrams per day. Sodium chloride is the principal source of sodium in the diet, and is used as seasoning and preservative, such as for pickling and jerky; most of it comes from processed foods. The UL for sodium is 2.3 grams per day, the threshold which could lead to hypertension when exceeded, but on average people in the United States consume 3.4 grams per day. Hypertension causes 7.6 million premature deaths worldwide each year. (Note that salt contains about 39.3% sodium—the rest being chlorine and other trace chemicals; thus the UL of 2.3g sodium would be about 5.9g, or 2.7ml of salt—about half a US teaspoon)
The renin-angiotensin system regulates the amount of fluids and sodium in the body. Reduction of blood pressure and sodium concentration in the kidney result in the production of renin, which in turn produces aldosterone and angiotensin, retaining sodium in the urine. Because of the increase in sodium concentration, the production of renin decreases, and the sodium concentration returns to normal. Sodium is also important in neuron function and osmoregulation between cells and the extracellular fluid, their distribution mediated in all animals by Na+/K+-ATPase; hence, sodium is the most prominent cation in extracellular fluid.
Unusually low or high sodium levels in humans are recognized in medicine as hyponatremia and hypernatremia. These conditions may be caused by genetic factors, physical factors associated with ageing or illnesses involving vomiting or diarrhea.
In C4 plants, sodium is a micronutrient that aids in metabolism, specifically in regeneration of phosphoenolpyruvate and synthesis of chlorophyll. In others, it substitutes for potassium in several roles, such as maintaining turgor pressure and aiding in the opening and closing of stomata. Excess sodium in the soil limits the uptake of water due to decreased water potential, which may result in wilting; similar concentrations in the cytoplasm can lead to enzyme inhibition, which in turn causes necrosis and chlorosis. To avoid these problems, plants developed mechanisms that limit sodium uptake by roots, store them in cell vacuoles, and control them over long distances; excess sodium may also be stored in old plant tissue, limiting the damage to new growth.
Precautions.
Care is required in handling elemental sodium, as it generates flammable hydrogen and caustic sodium hydroxide upon contact with water; powdered sodium may spontaneously explode in the presence of an oxidizer. Excess sodium can be safely removed by hydrolysis in a ventilated cabinet; this is typically done by sequential treatment with isopropanol, ethanol and water. Isopropanol reacts very slowly, generating the corresponding alkoxide and hydrogen. Fire extinguishers based on water accelerate sodium fires; those based on carbon dioxide and bromochlorodifluoromethane lose their effectiveness when they dissipate. An effective extinguishing agent is Met-L-X, which comprises approximately 5% Saran in sodium chloride together with flow agents; it is most commonly hand-applied with a scoop. Other materials include Lith+, which has graphite powder and an organophosphate flame retardant, and dry sand.

</doc>
<doc id="26997" url="http://en.wikipedia.org/wiki?curid=26997" title="Scientist">
Scientist

A scientist, in a broad sense, is one engaging in a systematic activity to acquire knowledge. In a more restricted sense, a scientist may refer to an individual who uses the scientific method. The person may be an expert in one or more areas of science. This article focuses on the more restricted use of the word. Scientists perform research toward a more comprehensive understanding of nature, including physical, mathematical and social realms.
Philosophy is a distinct activity that is not generally considered science. Philosophers aim to provide a comprehensive understanding of intangible aspects of reality and experience that cannot be physically measured.
Scientists are also distinct from engineers, those who design, build and maintain devices for particular situations. When science is done with a goal toward practical utility, it is called applied science. An applied scientist may not be designing something in particular, but rather is conducting research with the aim of developing new technologies and practical methods. When science is done with an inclusion of intangible aspects of reality it is called "natural philosophy".
Description.
Science and technology have continually modified human existence through the engineering process. As a profession the scientist of today is widely recognized. Scientists include theoreticians who mainly develop new models to explain existing data and predict new results, and experimentalists who mainly test models by making measurements — though in practice the division between these activities is not clear-cut, and many scientists perform both tasks.
Law and Mathematics are often grouped with the sciences. Some of the greatest physicists have also been creative mathematicians and lawyers. There is a continuum from the most theoretical to the most empirical scientists with no distinct boundaries. In terms of personality, interests, training and professional activity, there is little difference between applied mathematicians and theoretical physicists.
Scientists can be motivated in several ways. Many have a desire to understand why the world is as we see it and how it came to be. They exhibit a strong curiosity about reality. Other motivations are recognition by their peers and prestige, or the desire to apply scientific knowledge for the benefit of people's health, the nations, the world, nature or industries (academic scientist and industrial scientist). Scientists tend to be less motivated by direct financial reward for their work than other careers. As a result, scientific researchers often accept lower average salaries when compared with many other professions which require a similar amount of training and qualification.
Demography.
The number of scientists is vastly different from country to country. For instance, there are only 4 full-time scientists per 10,000 workers in India while this number is 79 for the United States. The numbers (per 10,000 workers) for selected countries are as follows:
Historical development and etymology of the term.
Until the late 19th or early 20th century, scientists were called "natural philosophers" or "men of science".
English philosopher and historian of science William Whewell coined the term "scientist" in 1833, and it was first published in Whewell's anonymous 1834 review of Mary Somerville's "On the Connexion of the Physical Sciences" published in the "Quarterly Review". Whewell's suggestion of the term was partly satirical, a response to changing conceptions of science itself in which natural knowledge was increasingly seen as distinct from other forms of knowledge. Whewell wrote of "an increasing proclivity of separation and dismemberment" in the sciences; while highly specific terms proliferated—chemist, mathematician, naturalist—the broad term "philosopher" was no longer satisfactory to group together those who pursued science, without the caveats of "natural" or "experimental" philosopher. Members of the British Association for the Advancement of Science had been complaining about the lack of a good term at recent meetings, Whewell reported in his review; alluding to himself, he noted that "some ingenious gentleman proposed that, by analogy with "artist", they might form [the word] "scientist", and added that there could be no scruple in making free with this term since we already have such words as "economist", and "atheist"—but this was not generally palatable". Scientists are the people who ask a question about a phenomenon and proceed to systematically go about answering the question themselves. They are by nature curious, creative and well organized. They need to have the ability to observe something and see in it some of the properties other people overlook.
Whewell proposed the word again more seriously (and not anonymously) in his 1840 "The Philosophy of the Inductive Sciences":
As we cannot use physician for a cultivator of physics, I have called him a physicist. We need very much a name to describe a cultivator of science in general. I should incline to call him a Scientist. Thus we might say, that as an Artist is a Musician, Painter, or Poet, a Scientist is a Mathematician, Physicist, or Naturalist.
He also proposed the term "physicist" at the same time, as a counterpart to the French word "physicien". Neither term gained wide acceptance until decades later; "scientist" became a common term in the late 19th century in the United States and around the turn of the 20th century in Great Britain. By the twentieth century, the modern notion of science as a special brand of information about the world, practiced by a distinct group and pursued through a unique method, was essentially in place.
The social roles of "scientists", and their predecessors before the emergence of modern scientific disciplines, have evolved considerably over time. Scientists of different eras (and before them, natural philosophers, mathematicians, natural historians, natural theologians, engineers, and other who contributed to the development of science) have had widely different places in society, and the social norms, ethical values, and epistemic virtues associated with scientists—and expected of them—have changed over time as well. Accordingly, many different historical figures can be identified as early scientists, depending on which elements of modern science are taken to be essential. 
Some historians point to the 17th century as the period when science in a recognizably modern form developed (what is popularly called the Scientific Revolution). It wasn't until the 19th century that sufficient socioeconomic changes occurred for scientists to emerge as a major profession.
Ancient and medieval science.
Knowledge about nature in Classical Antiquity was pursued by many kinds of scholars. Greek contributions to science—including works of geometry and mathematical astronomy, early accounts of biological processes and catalogs of plants and animals, and theories of knowledge and learning—were produced by philosophers and physicians, as well as practitioners of various trades. These roles, and their associations with scientific knowledge, spread with the Roman Empire and, with the spread of Christianity, became closely linked to religious institutions in most of European countries. Astrology and astronomy became an important area of knowledge, and the role of astronomer/astrologer developed with the support of political and religious patronage. By the time of the medieval university system, knowledge was divided into the "trivium"—philosophy, including natural philosophy—and the "quadrivium"—mathematics, including astronomy. Hence, the medieval analogs of scientists were often either philosophers or mathematicians. Knowledge of plants and animals was broadly the province of physicians.
Science in medieval Islam generated some new modes of developing natural knowledge, although still within the bounds of existing social roles such as philosopher and mathematician. Many proto-scientists from the Islamic Golden Age and Italian Renaissance are considered polymaths, in part because of the lack of anything corresponding to modern scientific disciplines. Many of these early polymaths were also religious priests and theologians: for example, Alhazen and al-Biruni were mutakallimiin; the physician Avicenna was a hafiz; the physician Ibn al-Nafis was a hafiz, muhaddith and ulema; the botanist Otto Brunfels was a theologian and historian of Protestantism; the astronomer and physician Nicolaus Copernicus was a priest.
Historical scientists.
Descartes was not only a pioneer of analytic geometry but formulated a theory of mechanics and advanced ideas about the origins of animal movement and perception. Vision interested the physicists Young and Helmholtz, who also studied optics, hearing and music. Newton extended Descartes' mathematics by inventing calculus (contemporaneously with Leibniz). He provided a comprehensive formulation of classical mechanics and investigated light and optics. Fourier founded a new branch of mathematics — infinite, periodic series — studied heat flow and infrared radiation, and discovered the greenhouse effect. Von Neumann, Turing, Khinchin, Markov and Wiener, all mathematicians, made major contributions to science and probability theory, including the ideas behind computers, and some of the foundations of statistical mechanics and quantum mechanics. Many mathematically inclined scientists, including Galileo, were also musicians.
In the late 19th century, Louis Pasteur, an organic chemist, discovered that microorganisms can cause disease. A few years earlier, Oliver Wendell Holmes, Sr., the American physician, poet and essayist, noted that sepsis in women following childbirth was spread by the hands of medics, four years before Semmelweis in Europe. There are many compelling stories in medicine and biology, such as the development of ideas about the circulation of blood from Galen to Harvey. The flowering of genetics and molecular biology in the 20th century is replete with famous names. Ramón y Cajal won the Nobel Prize in 1906 for his remarkable observations in neuroanatomy.
Some see a dichotomy between experimental sciences and purely "observational" sciences such as astronomy, meteorology, oceanography and seismology. But astronomers have done basic research in optics, developed charge-coupled devices, and in recent decades have sent space probes to study other planets in addition to using the Hubble Telescope to probe the origins of the Universe some 14 billion years ago. Microwave spectroscopy has now identified dozens of organic molecules in interstellar space, requiring laboratory experimentation and computer simulation to confirm the observational data and starting a new branch of chemistry. Computer modeling and numerical methods are techniques required of students in every field of quantitative science.
Women in science.
The percent of women entering into science are usually intertwined with engineering stats but the combination of the percentages shows the low numbers that are involved. The number of science and engineering doctorates awarded to women rose from a mere 7 percent in 1970 to 34 percent in 1985 and in engineering alone the numbers of bachelor's degrees awarded to women rose from only 385 in 1975 to more than 11000 in 1985.
The inequality prevails into the professional setting in ways such as starting position inequality and income inequality. According to Eisenhart and Finkel women experiences, even when they have equal qualifications, are that they start in lower-positions while men are granted tenure track positions. This later predicts an inequality of tenures positions as scientist in universities, "as of 1989, 65 percent of men and only 40 percent of women held tenured positions." Income conflicts occur when median annual salaries for full-time employed civilian scientists, "salary for men is $48,000, and that for women is $42,000."
Types of scientists.
Those considering science as a career often look to the frontiers. These include cosmology and biology, especially molecular biology and the human genome project. Other areas of active research include the exploration of matter at the scale of elementary particles as described by high-energy physics, and materials science, which seeks to discover and design new materials. Although there have been remarkable discoveries with regard to brain function and neurotransmitters, the nature of the mind and human thought still remains unknown.

</doc>
<doc id="27001" url="http://en.wikipedia.org/wiki?curid=27001" title="Smoke">
Smoke

Smoke is a collection of airborne solid and liquid particulates and gases emitted when a material undergoes combustion or pyrolysis, together with the quantity of air that is entrained or otherwise mixed into the mass. It is commonly an unwanted by-product of fires (including stoves, candles, oil lamps, and fireplaces), but may also be used for pest control (fumigation), communication (smoke signals), defensive and offensive capabilities in the military (smoke-screen), cooking, or smoking (tobacco, cannabis, etc.). Smoke is used in rituals where incense, sage, or resin is burned to produce a smell for spiritual purposes. Smoke is sometimes used as a flavoring agent, and preservative for various foodstuffs. Smoke is also a component of internal combustion engine exhaust gas, particularly diesel exhaust.
Smoke inhalation is the primary cause of death in victims of indoor fires. The smoke kills by a combination of thermal damage, poisoning and pulmonary irritation caused by carbon monoxide, hydrogen cyanide and other combustion products.
Smoke particles are an aerosol (or mist) of solid particles and liquid droplets that are close to the ideal range of sizes for Mie scattering of visible light. This effect has been likened to three-dimensional textured privacy glass — a smoke cloud does not obstruct an image, but thoroughly scrambles it.
Chemical composition.
The composition of smoke depends on the nature of the burning fuel and the conditions of combustion.
Fires with high availability of oxygen burn at a high temperature and with small amount of smoke produced; the particles are mostly composed of ash, or with large temperature differences, of condensed aerosol of water. High temperature also leads to production of nitrogen oxides. Sulfur content yields sulfur dioxide, or in case of incomplete combustion, hydrogen sulfide. Carbon and hydrogen are almost completely oxidized to carbon dioxide and water. Fires burning with lack of oxygen produce a significantly wider palette of compounds, many of them toxic. Partial oxidation of carbon produces carbon monoxide, nitrogen-containing materials can yield hydrogen cyanide, ammonia, and nitrogen oxides. Hydrogen gas can be produced instead of water. Content of halogens such as chlorine (e.g. in polyvinyl chloride or brominated flame retardants) may lead to production of e.g. hydrogen chloride, phosgene, dioxin, and chloromethane, bromomethane and other halocarbons. Hydrogen fluoride can be formed from fluorocarbons, whether fluoropolymers subjected to fire or halocarbon fire suppression agents. Phosphorus and antimony oxides and their reaction products can be formed from some fire retardant additives, increasing smoke toxicity and corrosivity. Pyrolysis of polychlorinated biphenyls (PCB), e.g. from burning older transformer oil, and to lower degree also of other chlorine-containing materials, can produce 2,3,7,8-tetrachlorodibenzodioxin, a potent carcinogen, and other polychlorinated dibenzodioxins. Pyrolysis of fluoropolymers, e.g. teflon, in presence of oxygen yields carbonyl fluoride (which hydrolyzes readily to HF and CO2); other compounds may be formed as well, e.g. carbon tetrafluoride, hexafluoropropylene, and highly toxic perfluoroisobutene (PFIB).
Pyrolysis of burning material, especially incomplete combustion or smoldering without adequate oxygen supply, also results in production of a large amount of hydrocarbons, both aliphatic (methane, ethane, ethylene, acetylene) and aromatic (benzene and its derivates, polycyclic aromatic hydrocarbons; e.g. benzo[a]pyrene, studied as a carcinogen, or retene), terpenes. Heterocyclic compounds may be also present. Heavier hydrocarbons may condense as tar; smoke with significant tar content is yellow to brown. Presence of such smoke, soot, and/or brown oily deposits during a fire indicates a possible hazardous situation, as the atmosphere may be saturated with combustible pyrolysis products with concentration above the upper flammability limit, and sudden inrush of air can cause flashover or backdraft.
Presence of sulfur can lead to formation of e.g. hydrogen sulfide, carbonyl sulfide, sulfur dioxide, carbon disulfide, and thiols; especially thiols tend to get adsorbed on surfaces and produce a lingering odor even long after the fire. Partial oxidation of the released hydrocarbons yields in a wide palette of other compounds: aldehydes (e.g. formaldehyde, acrolein, and furfural), ketones, alcohols (often aromatic, e.g. phenol, guaiacol, syringol, catechol, and cresols), carboxylic acids (formic acid, acetic acid, etc.).
The visible particulate matter in such smokes is most commonly composed of carbon (soot). Other particulates may be composed of drops of condensed tar, or solid particles of ash. The presence of metals in the fuel yields particles of metal oxides. Particles of inorganic salts may also be formed, e.g. ammonium sulfate, ammonium nitrate, or sodium chloride. Inorganic salts present on the surface of the soot particles may make them hydrophilic. Many organic compounds, typically the aromatic hydrocarbons, may be also adsorbed on the surface of the solid particles. Metal oxides can be present when metal-containing fuels are burned, e.g. solid rocket fuels containing aluminium. Depleted uranium projectiles after impacting the target ignite, producing particles of uranium oxides. Magnetic particles, spherules of magnetite-like ferrous ferric oxide, are present in coal smoke; their increase in deposits after 1860 marks the beginning of the Industrial Revolution. (Magnetic iron oxide nanoparticles can be also produced in the smoke from meteorites burning in the atmosphere.) Magnetic remanence, recorded in the iron oxide particles, indicates the strength of Earth's magnetic field when they were cooled beyond their Curie temperature; this can be used to distinguish magnetic particles of terrestrial and meteoric origin. Fly ash is composed mainly of silica and calcium oxide. Cenospheres are present in smoke from liquid hydrocarbon fuels. Minute metal particles produced by abrasion can be present in engine smokes. Amorphous silica particles are present in smokes from burning silicones; small proportion of silicon nitride particles can be formed in fires with insufficient oxygen. The silica particles have about 10 nm size, clumped to 70-100 nm aggregates and further agglomerated to chains. Radioactive particles may be present due to traces of uranium, thorium, or other radionuclides in the fuel; hot particles can be present in case of fires during nuclear accidents (e.g. Chernobyl disaster) or nuclear war.
Smoke particulates, like other aerosols, are categorized into three modes based on particle size:
Most of the smoke material is primarily in coarse particles. Those undergo rapid dry precipitation, and the smoke damage in more distant areas outside of the room where the fire occurs is therefore primarily mediated by the smaller particles.
Aerosol of particles beyond visible size is an early indicator of materials in a preignition stage of a fire.
Burning of hydrogen-rich fuel produces water; this results in smoke containing droplets of water vapor. In absence of other color sources (nitrogen oxides, particulates...), such smoke is white and cloud-like.
Smoke emissions may contain characteristic trace elements. Vanadium is present in emissions from oil fired power plants and refineries; oil plants also emit some nickel. Coal combustion produces emissions containing aluminium, arsenic, chromium, cobalt, copper, iron, mercury, selenium, and uranium.
Traces of vanadium in high-temperature combustion products form droplets of molten vanadates. These attack the passivation layers on metals and cause high temperature corrosion, which is a concern especially for internal combustion engines. Molten sulfate and lead particulates also have such effect.
Some components of smoke are characteristic of the combustion source. Guaiacol and its derivatives are products of pyrolysis of lignin and are characteristic of wood smoke; other markers are syringol and derivates, and other methoxy phenols. Retene, a product of pyrolysis of conifer trees, is an indicator of forest fires. Levoglucosan is a pyrolysis product of cellulose. Hardwood vs softwood smokes differ in the ratio of guaiacols/syringols. Markers for vehicle exhaust include polycyclic aromatic hydrocarbons, hopanes, steranes, and specific nitroarenes (e.g. 1-nitropyrene). The ratio of hopanes and steranes to elemental carbon can be used to distinguish between emissions of gasoline and diesel engines.
Many compounds can be associated with particulates; whether by being adsorbed on their surfaces, or by being dissolved in liquid droplets. Hydrogen chloride is well absorbed in the soot particles.
Inert particulate matter can be disturbed and entrained into the smoke. Of particular concern are particles of asbestos.
Deposited hot particles of radioactive fallout and bioaccumulated radioisotopes can be reintroduced into the atmosphere by wildfires and forest fires; this is a concern in e.g. the Zone of alienation containing contaminants from the Chernobyl disaster.
Polymers are a significant source of smoke. Aromatic side groups, e.g. in polystyrene, enhance generation of smoke. Aromatic groups integrated in the polymer backbone produce less smoke, likely due to significant charring. Aliphatic polymers tend to generate the least smoke, and are non-self-extinguishing. However presence of additives can significantly increase smoke formation. Phosphorus-based and halogen-based flame retardants decrease production of smoke. Higher degree of cross-linking between the polymer chains has such effect too.
Visible and invisible particles of combustion.
The naked eye detects particle sizes greater than 7 µm (micrometres). Visible particles emitted from a fire are referred to as smoke. Invisible particles are generally referred to as gas or fumes. This is best illustrated when toasting bread in a toaster. As the bread heats up, the products of combustion increase in size. The fumes initially produced are invisible but become visible if the toast is burnt.
An ionization chamber type smoke detector is technically a product of combustion detector, not a smoke detector. Ionization chamber type smoke detectors detect particles of combustion that are invisible to the naked eye. This explains why they may frequently false alarm from the fumes emitted from the red-hot heating elements of a toaster, before the presence of visible smoke, yet they may fail to activate in the early, low-heat smoldering stage of a fire.
Smoke from a typical house fire contains hundreds of different chemicals and fumes. As a result, the damage caused by the smoke can often exceed that caused by the actual heat of the fire. In addition to the physical damage caused by the smoke of a fire – which manifests itself in the form of stains – is the often even harder to eliminate problem of a smoky odor. Just as there are contractors that specialize in rebuilding/repairing homes that have been damaged by fire and smoke, fabric restoration companies specialize in restoring fabrics that have been damaged in a fire.
Dangers of smoke.
Smoke from oxygen-deprived fires contains a significant concentration of compounds that are flammable. A cloud of smoke, in contact with atmospheric oxygen, therefore has the potential of being ignited – either by another open flame in the area, or by its own temperature. This leads to effects like backdraft and flashover. Smoke inhalation is also a danger of smoke that can cause serious injury and death.
Many compounds of smoke from fires are highly toxic and/or irritating. The most dangerous is carbon monoxide leading to carbon monoxide poisoning, sometimes with the additive effects of hydrogen cyanide and phosgene. Smoke inhalation can therefore quickly lead to incapacitation and loss of consciousness. Sulfur oxides, hydrogen chloride and hydrogen fluoride in contact with moisture form sulfuric, hydrochloric and hydrofluoric acid, which are corrosive to both lungs and materials. When asleep the nose does not sense smoke nor does the brain, but the body will wake up if the lungs become enveloped in smoke and the brain will be stimulated and the person will be awoken. This does not work if the person is incapacitated or under the influence of drugs and/or alcohol.
Cigarette smoke is a major modifiable risk factor for lung disease, heart disease, and many cancers.
Smoke can obscure visibility, impeding occupant exiting from fire areas. In fact, the poor visibility due to the smoke that was in the Worcester Cold Storage Warehouse fire in Worcester, Massachusetts was the exact reason why the trapped rescue firefighters couldn't evacuate the building in time. Because of the striking similarity that each floor shared, the dense smoke caused the firefighters to become disoriented.
Smoke corrosion.
Smoke contains a wide variety of chemicals, many of them aggressive in nature. Examples are hydrochloric acid and hydrobromic acid, produced from halogen-containing plastics and fire retardants, hydrofluoric acid released by pyrolysis of fluorocarbon fire suppression agents, sulfuric acid from burning of sulfur-containing materials, nitric acid from high-temperature fires where nitrous oxide gets formed, phosphoric acid and antimony compounds from P and Sb based fire retardants, and many others. Such corrosion is not significant for structural materials, but delicate structures, especially microelectronics, are strongly affected. Corrosion of circuit board traces, penetration of aggressive chemicals through the casings of parts, and other effects can cause an immediate or gradual deterioration of parameters or even premature (and often delayed, as the corrosion can progress over long time) failure of equipment subjected to smoke. Many smoke components are also electrically conductive; deposition of a conductive layer on the circuits can cause crosstalks and other deteriorations of the operating parameters or even cause short circuits and total failures. Electrical contacts can be affected by corrosion of surfaces, and by deposition of soot and other conductive particles or nonconductive layers on or across the contacts. Deposited particles may adversely affect the performance of optoelectronics by absorbing or scattering the light beams.
Corrosivity of smoke produced by materials is characterized by the corrosion index (CI), defined as material loss rate (angstrom/minute) per amount of material gasified products (grams) per volume of air (m3). It is measured by exposing strips of metal to flow of combustion products in a test tunnel. Polymers containing halogen and hydrogen (polyvinyl chloride, polyolefins with halogenated additives, etc.) have the highest CI as the corrosive acids are formed directly with water produced by the combustion, polymers containing halogen only (e.g. polytetrafluoroethylene) have lower CI as the formation of acid is limited to reactions with airborne humidity, and halogen-free materials (polyolefins, wood) have the lowest CI. However, some halogen-free materials can also release significant amount of corrosive products.
Smoke damage to electronic equipment can be significantly more extensive than the fire itself. Cable fires are of special concern; low smoke zero halogen materials are preferable for cable insulation.
When smoke comes into contact with the surface of any substance or structure, the chemicals contained in it are transferred to it. The corrosive properties of the chemicals cause the substance or structure to decompose at a rapid rate. Certain materials or structures absorb these chemicals, which is why clothing, unsealed surfaces, potable water, piping, wood, etc., are replaced in most cases of structural fires.
Secondhand smoke inhalation.
Secondhand smoke is the combination of both sidestream and mainstream smoke emissions. These emissions contain more than 50 carcinogenic chemicals. According to the Surgeon General's latest report on the subject, "Short exposures to secondhand smoke can cause blood platelets to become stickier, damage the lining of blood vessels, decrease coronary flow velocity reserves, and reduce heart variability, potentially increasing the risk of a heart attack". The American Cancer Society lists "heart disease, lung infections, increased asthma attacks, middle ear infections, and low birth weight" as ramifications of smoker's emission 
Measurement.
As early as the 15th century Leonardo da Vinci commented at length on the difficulty of assessing smoke, and distinguished between black smoke (carbonized particles) and white 'smoke' which is not a smoke at all but merely a suspension of harmless water particulates.
Smoke from heating appliances is commonly measured in one of the following ways:
In-line capture. A smoke sample is simply sucked through a filter which is weighed before and after the test and the mass of smoke found. This is the simplest and probably the most accurate method, but can only be used where the smoke concentration is slight, as the filter can quickly become blocked.
The ASTM smoke pump is a simple and widely-used method of in-line capture where a measured volume of smoke is pulled through a filter paper and the dark spot so formed is compared with a standard.
Filter/dilution tunnel. A smoke sample is drawn through a tube where it is diluted with air, the resulting smoke/air mixture is then pulled through a filter and weighed. This is the internationally recognized method of measuring smoke from combustion.
Electrostatic precipitation. The smoke is passed through an array of metal tubes which contain suspended wires. A (huge) electrical potential is applied across the tubes and wires so that the smoke particles become charged and are attracted to the sides of the tubes. This method can over-read by capturing harmless condensates, or under-read due to the insulating effect of the smoke. However, it is the necessary method for assessing volumes of smoke too great to be forced through a filter, i.e., from bituminous coal.
Ringelmann scale. A measure of smoke color. Invented by Professor Maximilian Ringelmann in Paris in 1888, it is essentially a card with squares of black, white and shades of gray which is held up and the comparative grayness of the smoke judged. Highly dependent on light conditions and the skill of the observer it allocates a grayness number from 0 (white) to 5 (black) which has only a passing relationship to the actual quantity of smoke. Nonetheless, the simplicity of the Ringelmann scale means that it has been adopted as a standard in many countries.
Optical scattering. A light beam is passed through the smoke. A light detector is situated at an angle to the light source, typically at 90°, so that it receives only light reflected from passing particles. A measurement is made of the light received which will be higher as the concentration of smoke particles becomes higher.
Optical obscuration. A light beam is passed through the smoke and a detector opposite measures the light. The more smoke particles are present between the two, the less light will be measured.
Combined optical methods. There are various proprietary optical smoke measurement devices such as the 'nephelometer' or the 'aethalometer' which use several different optical methods, including more than one wavelength of light, inside a single instrument and apply an algorithm to give a good estimate of smoke. It has been claimed that these devices can differentiate types of smoke and so their probable source can be inferred, though this is disputed
Inference from carbon monoxide. Smoke is incompletely burned fuel, carbon monoxide is incompletely burned carbon, therefore it has long been assumed that measurement of CO in flue gas (a cheap, simple and very accurate procedure) will provide a good indication of the levels of smoke. Indeed, several jurisdictions use CO measurement as the basis of smoke control. However it is far from clear how accurate the correspondence is.
Medicinal smoke.
Throughout recorded history, humans have used the smoke of medicinal plants to cure illness. A sculpture from Persepolis shows Darius the Great (522–486 BC), the king of Persia, with two censers in front of him for burning Peganum harmala and/or sandalwood Santalum album, which was believed to protect the king from evil and disease. More than 300 plant species in 5 continents are used in smoke form for different diseases. As a method of drug administration, smoking is important as it is a simple, inexpensive, but very effective method of extracting particles containing active agents. More importantly, generating smoke reduces the particle size to a microscopic scale thereby increasing the absorption of its active chemical principles.

</doc>
<doc id="27003" url="http://en.wikipedia.org/wiki?curid=27003" title="Swiss cheese">
Swiss cheese

Swiss cheese is a generic name in North America for several related varieties of cheese, mainly of North American manufacture, which resemble Emmental cheese, a yellow, medium-hard cheese that originated in the area around Emmental, in Switzerland. Some types of Swiss cheese have a distinctive appearance, as the blocks of the cheese are riddled with holes known as "eyes". Swiss cheese without eyes is known as "blind". (The term is applied to cheeses of this style made outside Switzerland, such as Jarlsberg cheese, which originates in Norway).
Production.
Three types of bacteria are used in the production of Emmental cheese: "Streptococcus salivarius" subspecies "thermophilus", "Lactobacillus" ("Lactobacillus helveticus" or "Lactobacillus delbrueckii" subspecies "bulgaricus"), and "Propionibacterium" ("Propionibacterium freudenreichii" subspecies "shermani"). In a late stage of cheese production, the propionibacteria consume the lactic acid excreted by the other bacteria and release acetate, propionic acid, and carbon dioxide gas. The carbon dioxide slowly forms the bubbles that develop the "eyes". The acetate and propionic acid give Swiss its nutty and sweet flavor. A hypothesis proposed by Swiss researchers in 2015 notes that particulate matter may also play a role in the holes' development and that reducing debris such as hay dust in the milk played a role in reduced hole size in Swiss cheeses. Historically, the holes were seen as a sign of imperfection and cheese makers originally tried to avoid them by pressing during production. In modern times, the holes have become an identifier of the cheese. 
In general, the larger the eyes in a Swiss cheese, the more pronounced its flavor because a longer fermentation period gives the bacteria more time to act. This poses a problem, however, because cheese with large eyes does not slice well and comes apart in mechanical slicers. As a result, industry regulators have limited the eye size by which Swiss cheese receives the Grade A stamp.
Varieties.
Baby Swiss and Lacy Swiss are two varieties of American Swiss cheeses. Both have small holes and a mild flavor. Baby Swiss is made from whole milk, and Lacy Swiss is made from low fat milk. Baby Swiss was developed in the mid-1960s outside of Charm, Ohio, by the Guggisberg Cheese Company, owned by Alfred Guggisberg.

</doc>
<doc id="27006" url="http://en.wikipedia.org/wiki?curid=27006" title="Serendipity">
Serendipity

Serendipity means a "fortunate happenstance" or "pleasant surprise". It was coined by Horace Walpole in 1754. In a letter he wrote to a friend Walpole explained an unexpected discovery he had made by reference to a Persian fairy tale, "The Three Princes of Serendip". The princes, he told his correspondent, were “always making discoveries, by accidents and sagacity, of things which they were not in quest of”.
The notion of serendipity is a common occurrence throughout the history of scientific innovation such as Alexander Fleming's accidental discovery of penicillin in 1928 and the invention of the microwave oven by Percy Spencer in 1945, the invention of the Post-it note by Spencer Silver in 1968.
The word has been voted one of the ten English words hardest to translate in June 2004 by a British translation company. However, due to its sociological use, the word has been exported into many other languages.
Etymology.
The first noted use of "serendipity" (meaning pleasant surprise) in the English language was by Horace Walpole (1717–1797). In a letter to Horace Mann (dated 28 January 1754) he said he formed it from the Persian fairy tale "The Three Princes of Serendip", whose heroes "were always making discoveries, by accidents and sagacity, of things they were not in quest of". The name stems from "Serendip", an old name for Sri Lanka (aka Ceylon), from Tamil "Ceralamdivu", Sanskrit "Simhaladvipa" and Persian "Sarandīp" (سرندیپ). Parts of Sri Lanka were under the rule of Tamil kings for extended periods of time in history. Kings of Kerala, India (Cheranadu) were called Ceran Kings and "divu", "tivu" or "dheep" means island, the island belonging to the Chera King was called "Cherandeep", hence "Sarandib" by Arab traders.
The structure of serendipity.
Serendipity is not just a matter of a random event, nor can it be taken simply as a synonym for "a happy accident" (Ferguson, 1999; Khan, 1999), "finding out things without being searching for them" (Austin, 2003), or "a pleasant surprise" (Tolson, 2004) ..
The New Oxford Dictionary of English defines serendipity as the occurrence and development of events by chance in a satisfactory or beneficial way, understanding the chance as any event that takes place in the absence of any obvious project (randomly or accidentally), which is not relevant to any present need, or in which the cause is unknown.
Innovations presented as examples of serendipity have an important characteristic: they were made by individuals able to "see bridges where others saw holes" and connect events creatively, based on the perception of a significant link.
The chance is an event, serendipity a capacity. The Nobel Prize laureate Paul Flory suggests that significant inventions are not mere accidents.
Serendipity and scientific discoveries.
The serendipitous can play an important role in the search for truth, but because of traditional scientific behavior and scientific thinking based on logic and predictability is often ignored in the scientific literature.
Successful researchers can observe the results with a careful attention in the mood to analyze a phenomenon under the most diverse and different perspectives. Question themselves on assumptions that do not fit with the empirical observations. Realizing that serendipitous events can generate important research ideas, these researchers recognize and appreciate the unexpected, encouraging their assistants to observe and discuss unexpected events.
Serendipity can be obtained in groups in that the "critical mass" of multidisciplinary scientists working together in an environment that fosters communication, establishing the idea that the work and the interest of a researcher can be shared with others who may find a new application for a new knowledge.
Serendipity in science and technology.
Various thinkers discuss the role that luck can play in science. One aspect of Walpole's original definition of serendipity, often missed in modern discussions of the word, is the need for an individual to be "sagacious" enough to link together apparently innocuous facts in order to come to a valuable conclusion. Indeed, the scientific method, and the scientists themselves, can be prepared in many other ways to harness luck and make discoveries.
Serendipity and innovation.
Serendipity is typically used incorrectly as a synonym for opportunity, coincidence, luck or providence, a concept that prejudices the appreciation of the term in relation to its contribution to the innovation process.
It is often a misunderstood quality for discovery and innovation. It may become a powerful tool in the contribution of innovative insights that lead to the attainment of entrepreneurial visions.
Understanding the processes of their development and uses allows managers, innovators and researchers as they can use "serendipity" as an important contribution to the competitive success of a given company.
Business and strategy.
M. E. Graebner describes serendipitous value in the context of the acquisition of a business as "windfalls that were not anticipated by the buyer prior to the deal": i.e., unexpected advantages or benefits incurred due to positive synergy effects of the merger.
Ikujiro Nonaka points out that the serendipitous quality of innovation is highly recognized by managers and links the success of Japanese enterprises to their ability to create knowledge not by processing information but rather by "tapping the tacit and often highly subjective insights, intuitions, and hunches of individual employees and making those insights available for testing and use by the company as a whole".
Serendipity is postulated by Napier and Vuong (2013) as a 'strategic advantage' with which a firm can tap its potential creativity.
Serendipity is a key concept in competitive intelligence because it is one of the tools for avoiding blind spots (see Blindspots analysis).
Uses.
Serendipity is used as a sociological method in Anselm L. Strauss' and Barney G. Glaser's Grounded Theory, building on ideas by sociologist Robert K. Merton, who in "Social Theory and Social Structure" (1949) referred to the "serendipity pattern" as the fairly common experience of observing an unanticipated, anomalous and strategic datum which becomes the occasion for developing a new theory or for extending an existing theory. Robert K. Merton also coauthored (with Elinor Barber) "The Travels and Adventures of Serendipity" which traces the origins and uses of the word "serendipity" since it was coined. The book is "a study in sociological semantics and the sociology of science", as the subtitle of the book declares. It further develops the idea of serendipity as scientific "method" (as juxtaposed with purposeful discovery by experiment or retrospective prophecy).
Related terms.
William Boyd coined the term zemblanity to mean somewhat the opposite of serendipity: "making unhappy, unlucky and expected discoveries occurring by design". A zemblanity is, effectively, an "unpleasant unsurprise". It derives from Novaya Zemlya (or Nova Zembla), a cold, barren land with many features opposite to the lush Sri Lanka (Serendip). On this island Willem Barents and his crew were stranded while searching for a new route to the east.
Bahramdipity is derived directly from Bahram Gur as characterized in the "The Three Princes of Serendip". It describes the "suppression" of serendipitous discoveries or research results by powerful individuals.
References.
</dl>

</doc>
<doc id="27009" url="http://en.wikipedia.org/wiki?curid=27009" title="Soap opera">
Soap opera

A soap opera, soapie, or soap is a serial drama on television or radio which features related story lines about the lives of multiple characters. The stories usually focus on emotional relationships to the point of melodrama. The term "soap opera" originated from such dramas being typically sponsored by soap manufacturers in the past.
Origin of the genre.
The first series considered to be a "soap opera" was "Painted Dreams", which debuted on October 20, 1930 on Chicago radio station WGN. Early radio series such as "Painted Dreams" were broadcast in weekday daytime slots, usually five days a week, when most of the listeners would be housewives; thus, the shows were aimed at and consumed by a predominantly female audience. In the name, "soap" refers to the soap and detergent commercials originally broadcast during the shows, which were aimed at women who were cleaning their houses at the time of listening or viewing, and "opera" refers to the melodramatic character of the shows.
The first nationally broadcast radio soap opera was "Clara, Lu, and Em", which aired on the NBC Blue Network at 10:30 p.m. Eastern Time on January 27, 1931.
Story and episode structure.
A crucial element that defines the soap opera is the open-ended serial nature of the narrative, with stories spanning several episodes. One of the defining features that makes a television program a soap opera, according to Albert Moran, is "that form of television that works with a continuous open narrative. Each episode ends with a promise that the storyline is to be continued in another episode". In 2012, "Los Angeles Times" columnist Robert Lloyd wrote of daily dramas, "Although melodramatically eventful, soap operas such as this also have a luxury of space that makes them seem more naturalistic; indeed, the economics of the form demand long scenes, and conversations that a 22-episodes-per-season weekly series might dispense with in half a dozen lines of dialogue may be drawn out, as here, for pages. You spend more time even with the minor characters; the apparent villains grow less apparently villainous."
Soap opera storylines run concurrently, intersect and lead into further developments. An individual episode of a soap opera will generally switch between several different concurrent narrative threads that may at times interconnect and affect one another or may run entirely independent of each other. Each episode may feature some of the show's current storylines, but not always all of them. Especially in daytime serials and those that are broadcast each weekday, there is some rotation of both storyline and actors so any given storyline or actor will appear in some but usually not all of a week's worth of episodes. Soap operas rarely bring all the current storylines to a conclusion at the same time. When one storyline ends, there are several other story threads at differing stages of development. Soap opera episodes typically end on some sort of cliffhanger, and the season finale (if a soap incorporates a break between seasons) ends in the same way, only to be resolved when the show returns for the start of a new yearly broadcast.
Evening soap operas and those that air at a rate of one episode per week are more likely to feature the entire cast in each episode, and to represent all current storylines in each episode. Evening soap operas and serials that run for only part of the year tend to bring things to a dramatic end-of-season cliffhanger.
In 1976, "Time" magazine described American daytime television as "TV's richest market," noting the loyalty of the soap opera fan base and the expansion of several half-hour series into hour-long broadcasts in order to maximize ad revenues. The article explained that at that time, many prime time series lost money, while daytime serials earned profits several times more than their production costs. The issue's cover notably featured its first daytime soap stars, Bill Hayes and Susan Seaforth Hayes of "Days of Our Lives", a married couple whose onscreen and real-life romance was widely covered by both the soap opera magazines and the mainstream press at large.
Plots and storylines.
The main characteristics that define soap operas are "an emphasis on family life, personal relationships, sexual dramas, emotional and moral conflicts; some coverage of topical issues; set in familiar domestic interiors with only occasional excursions into new locations". Fitting in with these characteristics, most soap operas follow the lives of a group of characters who live or work in a particular place, or focus on a large extended family. The storylines follow the day-to-day activities and personal relationships of these characters. "Soap narratives, like those of film melodramas, are marked by what Steve Neale has described as 'chance happenings, coincidences, missed meetings, sudden conversions, last-minute rescues and revelations, deus ex machina endings.'" These elements may be found across the gamut of soap operas, from "EastEnders" to "Dallas".
In many soap operas, in particular daytime serials in the United States, the characters are frequently attractive, seductive, glamorous and wealthy. Soap operas from the United Kingdom and Australia tend to focus on more everyday characters and situations, and are frequently set in working class environments. Many of the soaps produced in those two countries explore social realist storylines such as family discord, marriage breakdown or financial problems. Both UK and Australian soap operas feature comedic elements, often by way of affectionate comic stereotypes such as the gossip or the grumpy old man, presented as a sort of comic foil to the emotional turmoil that surrounds them. This diverges from U.S. soap operas where such comedy is rare. UK soap operas frequently make a claim to presenting "reality" or purport to have a "realistic" style. UK soap operas also frequently foreground their geographic location as a key defining feature of the show while depicting and capitalising on the exotic appeal of the stereotypes connected to the location. As examples, "EastEnders" focuses on the tough and grim life in London's east end; while "Coronation Street" invokes Manchester and its characters exhibit the stereotypical characteristic of "Northern straight talking".
Romance, secret relationships, extramarital affairs, and genuine love have been the basis for many soap opera storylines. In U.S. daytime serials, the most popular soap opera characters, and the most popular storylines, often involved a romance of the sort presented in paperback romance novels. Soap opera storylines sometimes weave intricate, convoluted and sometimes confusing tales of characters who have affairs, meet mysterious strangers and fall in love, and who commit adultery, all of which keeps audiences hooked on the unfolding story twists. Crimes such as kidnapping, rape, and even murder may go unpunished if the perpetrator is to be retained in the ongoing story.
Australian and UK soap operas also feature a significant proportion of romance storylines. In Russia, most popular serials explore the "romantic quality" of criminal and/or oligarch life.
In soap opera storylines, previously unknown children, siblings and twins (including the evil variety) of established characters often emerge to upset and reinvigorate the set of relationships examined by the series. Unexpected calamities disrupt weddings, childbirths, and other major life events with unusual frequency.
"If we want to blend an actor back into a show, there's always a way. You can generally find a way to twist and manipulate something. You rarely see a dead body, but hey, even if you do, he or she can always come back to play the evil identical twin."
 – Marlena Laird in 1992, during her time as a line producer and director for "General Hospital".
Much like comic books – another popular form of linear storytelling pioneered in the U.S. during the 20th Century – a character's death is not guaranteed to be permanent. On "The Bold and the Beautiful", Taylor Forrester (Hunter Tylo) was shown to flatline and have a funeral. When Tylo reprised the character in 2005, a retcon explained that Taylor had actually gone into a coma.
Stunts and complex physical action are largely absent, especially from daytime serials. Such story events often take place offscreen and are referred to in dialogue instead of being shown. This is because stunts or action scenes are difficult to adequately depict visually without complex action, multiple takes, and post production editing. When episodes were broadcast live, post production work was impossible. Though all serials have long switched to being taped, extensive post production work and multiple takes, while possible, are not feasible due to the tight taping schedules and low budgets.
United States.
Daytime serials on television.
The first network television soap opera was "Faraway Hill" in 1946. Soap operas became a staple of daytime television in the United States in the early 1950s. Along with game shows, reruns of situation comedies, and talk shows, the soap opera was traditionally a fixture on the daytime schedules of the American broadcast networks. Christina S. Beck argues the significance of soap operas are based on viewers' co-constructing narratives to show how both traditional and online soaps help negotiated the lived experience of people. In 1988, H. Wesley Kenney, who at the time served as the executive producer of "General Hospital", said to "The New York Times":
I think people like stories that continue so they can relate to these people. They become like a family, and the viewer becomes emotionally involved. There seem to be two attitudes by viewers. One, that the stories are similar to what happened to them in real life, or two, thank goodness that isn't me.—H. Wesley Kenney
Many long-running U.S. soap operas established particular environments for their stories. "The Doctors" and "General Hospital", in the beginning, told stories almost exclusively from inside the confines of a hospital. "As the World Turns" dealt heavily with Chris Hughes' law practice and the travails of his wife Nancy who, tired of being "the loyal housewife" in the 1970s, became one of the first older women on the American serials to enter the workforce. "Guiding Light" dealt with Bert Bauer (Charita Bauer) and her alcoholic husband Bill, and their endless marital troubles. When Bert's status shifted to caring mother and town matriarch, her children's marital troubles were showcased. "Search for Tomorrow" mostly told its story through the eyes of Joanne Gardner (Mary Stuart). Even when stories revolved around other characters, Joanne was frequently a key player in their storylines. "Days of our Lives" initially focused on Dr. Tom Horton and his steadfast wife Alice. The show later branched out to focus more on their five children. "The Edge of Night" featured as its central character Mike Karr, a police detective (later an attorney), and largely dealt with organized crime. "The Young and the Restless" first focused on two families, the prosperous Brooks family with four daughters, and the working class Foster family of a single working mother with three children. Its storylines explored realistic problems including cancer, mental illness, poverty and infidelity.
In contrast, "Dark Shadows" (1966–1971) and "Port Charles" (1997–2003) featured supernatural characters and dealt with fantasy and horror storylines. Their characters included vampires, witches, ghosts, goblins and angels.
The American soap opera "Guiding Light" (originally titled "The Guiding Light" until 1975) started as a radio drama in January 1937 and subsequently transferred to television in June 1952. With the exception of several years in the late 1940s when the show's creator Irna Phillips was involved in a dispute with Procter & Gamble, "Guiding Light" was heard or seen nearly every weekday since it began, until 2009 making it the longest story ever told in a broadcast medium.
Originally serials were broadcast as fifteen-minute installments each weekday in daytime slots. In 1956, "As the World Turns" and "The Edge of Night", both produced by Procter & Gamble Productions, debuted as the first half-hour soap operas on the CBS television network. All soap operas broadcast half-hour episodes by the end of the 1960s. With increased popularity in the 1970s, most soap operas had expanded to an hour in length by the end of the decade ("Another World" even expanded to 90 minutes for a short time). More than half of the serials had expanded to one-hour episodes by 1980. As of 2012, three of the four U.S. serials air one hour episodes each weekday; only "The Bold and the Beautiful" airs 30-minute episodes.
Soap operas were originally broadcast live from the studio, creating what many at the time regarded as a feeling similar to that of a stage play. As nearly all soap operas were originated at that time in New York City, a number of soap actors were also accomplished stage actors who performed live theatre during breaks from their soap roles. In the 1960s and 1970s, new serials such as "General Hospital", "Days of our Lives" and "The Young and the Restless" were produced in Los Angeles. Their success made the West Coast a viable alternative to New York-produced soap operas, which were becoming more costly to perform. By the early 1970s, nearly all soap operas had transitioned to being taped. "As the World Turns" and "The Edge of Night" were the last to make the switch, in 1975.
"Port Charles" used the practice of running 13-week "story arcs," in which the main events of the arc are played out and wrapped up over the 13 weeks, although some storylines did continue over more than one arc. According to the 2006 Preview issue of "Soap Opera Digest", it was briefly discussed that all ABC shows might do telenovela arcs, but this was rejected.
Though U.S. daytime soap operas are not generally rerun by their networks, occasionally they are rebroadcast elsewhere. Early episodes of "Dark Shadows" were rerun on PBS member stations in the early 1970s after the show's cancellation, and the entire series (except for a single missing episode) was rerun on the Sci-Fi Channel in the 1990s. After "The Edge of Night"‍ '​s 1984 cancellation, reruns of the show's final five years were shown late nights on USA Network from 1985 to 1989. On January 20, 2000, a digital cable and satellite network dedicated to the genre, SOAPnet, began re-airing soaps that originally aired on ABC, NBC and CBS.
Newer broadcast networks since the late 1980s, such as Fox and cable television networks, have largely eschewed soap operas in their daytime schedules, instead running syndicated programming and reruns. No cable television outlet has produced its own daytime serial, although DirecTV's The 101 Network took over existing serial "Passions", continuing production for one season. Fox, the fourth "major network," carried a short lived daytime soap "Tribes" in 1990. Yet other than this and a couple of pilot attempts, Fox mainly stayed away from daytime soaps, and has not attempted them since their ascension to major-network status in 1994 (it did later attempt a series of daily prime time soaps, which aired on newly created sister network MyNetworkTV, but the experiment was largely a failure).
Due to the masses of episodes produced for a series, release of soap operas to DVD (a popular venue for distribution of current and vintage television series) is considered impractical. With the exception of occasional specials, daytime soap operas are notable by their absence from DVD release schedules (an exception being the supernatural soap opera, "Dark Shadows", which did receive an essentially complete release on both VHS and DVD; the single lost episode #1219 is reconstructed by means of an off-the-air audio recording, still images, and recap material from adjacent episodes).
Performers.
Due to the longevity of these shows, it is not uncommon for a single character to be played by multiple actors. The key character of Mike Karr on "The Edge of Night" was played by three different actors.
Conversely, several actors have remained playing the same character for many years, or decades even. Helen Wagner played Hughes family matriarch Nancy Hughes on American soap "As the World Turns" from its April 2, 1956 debut through her death in May 2010. She is listed in the Guinness Book of World Records as the actor with the longest uninterrupted performance in a single role. A number of performers played roles for twenty years or longer, occasionally on more than one show. Rachel Ames played Audrey Hardy on both "General Hospital" and "Port Charles" from 1964 until 2007, and returned in 2009. Susan Lucci played Erica Kane in "All My Children" from the show's debut in January 1970 until it ended its network television run on ABC on September 23, 2011. Erika Slezak played Victoria Lord #3 on "One Life to Live" from 1971 until the show ended its network television run on ABC on January 13, 2012 and resumed the role in its short-lived online revival on April 29, 2013.
Other actors have played several characters on different shows. Millette Alexander, Bernard Barrow, Doris Belack, David Canary, Judith Chapman, Jordan Charney, Joan Copeland, Nicolas Coster, Jacqueline Courtney, Louis Edmonds, Dan Hamilton, Don Hastings, Vincent Irizarry, Lenore Kasdorf, Teri Keane, Lois Kibbee, John Loprieno, Maeve McGuire, James Mitchell, Christopher Pennock, Gary Pillar, Antony Ponzini, William Prince, Louise Shaffer, and Diana van der Vlis, among many others, have all played multiple soap roles.
Evolution of the daytime serial.
For several decades, most daytime soap operas concentrated on family and marital discord, legal drama and romance. The action rarely left interior settings, and many shows were set in fictional, medium-sized Midwestern towns.
Exterior shots were slowly incorporated into the series "The Edge of Night" and "Dark Shadows". Unlike many earlier serials which were set in fictional towns, "The Best of Everything" and "Ryan's Hope" were set in a real-world location, New York City.
The first exotic location shoot was made by "All My Children", to St. Croix in 1978. Many other soap operas planned lavish storylines after the success of the "All My Children" shoot. Soap operas "Another World" and "Guiding Light" both went to St. Croix in 1980, the former show culminating a long-running storyline between popular characters Mac, Rachel and Janice, and the latter to serve as an exotic setting for Alan Spaulding and Rita Bauer's torrid affair. "Search for Tomorrow" taped for two weeks in Hong Kong in 1981. Later that year, some of the cast and crew ventured to Jamaica to tape a love consummation storyline between the characters of Garth and Kathy.
During the 1980s, perhaps as a reaction to the evening drama series that were gaining high ratings, daytime serials began to incorporate action and adventure storylines, more big-business intrigue, and an increased emphasis on youthful romance.
One of the first and most popular couples was Luke Spencer and Laura Webber on "General Hospital". Luke and Laura helped to attract both male and female fans. Even actress Elizabeth Taylor was a fan and at her own request was given a guest role in Luke and Laura's wedding episode. Luke and Laura's popularity led to other soap producers striving to reproduce this success by attempting to create supercouples of their own.
With increasingly bizarre action storylines coming into vogue, Luke and Laura saved the world from being frozen, brought a mobster down by finding his black book in a Left-Handed Boy Statue, and helped a Princess find her Aztec Treasure in Mexico. Other soap operas attempted similar adventure storylines, often featuring footage shot on location – frequently in exotic locales.
During the 1990s, the mob, action and adventure stories fell out of favor with producers, due to generally declining ratings for daytime soap operas at the time, and the resultant budget cuts. In addition, soap operas were no longer able to go on expensive location shoots overseas as they were able to do in the 1980s. During that decade, soap operas increasingly focused on younger characters and social issues, such as Erica Kane's drug addiction on "All My Children", the re-emergence of Viki Lord's multiple personality disorder on "One Life to Live", and Stuart Chandler dealing with AIDS and death on "All My Children". Other social issues included cancer, homophobia, addiction, abuse, adoption and racism.
Some shows during the 2000s incorporated supernatural and science fiction elements into their storylines. One of the main characters on the earlier soap opera "Dark Shadows" were Barnabas Collins, a vampire, and "One Life to Live" featured an angel named Virgil. Both shows featured characters who travelled to and from the past.
Traditional grammar of daytime serials.
Modern U.S. daytime soap operas largely stay true to the original soap opera format. The duration and format of storylines and the visual grammar employed by U.S. daytime serials set them apart from soap operas in other countries and from evening soap operas. Stylistically, UK and Australian soap operas, which are usually produced for early evening timeslots, fall somewhere in-between U.S. daytime and evening soap operas. Similar to U.S. daytime soap operas, UK and Australian serials are shot on videotape, and the cast and storylines are rotated across the week's episodes so that each cast member will appear in some but not all episodes. UK and Australian soap operas move through storylines at a faster rate than daytime serials, making them closer to U.S. evening soap operas in this regard.
American daytime soap operas feature stylistic elements that set them apart from other shows:
Decline.
Statistics and trends.
Soap opera ratings have significantly fallen in the U.S. since the 2000s. No new major daytime soap opera has been created since "Passions" in 1999, while many have been cancelled. "The Young and the Restless", the highest-rated soap opera from 1988 to the present, had fewer than 5 million daily viewers as of February 2012, a number exceeded by several non-scripted programs such as "Judge Judy". Circulations of soap opera magazines have decreased and some have even ceased publication. SOAPnet, which largely aired soap opera reruns, began to be phased out in 2012 and fully ceased operations the following year. Since January 2012, four daytime soap operas – "General Hospital", "Days of our Lives", "The Young and the Restless" and "The Bold and the Beautiful" – continue to air on the three major networks, down from a total of 12 during the 1990–91 season and a high of 19 in the 1969–70 season. This marks the first time since 1953 that there are only four soap operas on broadcast television.
Several of America's most established soaps ended between 2009 and 2012. The longest-running drama in television and radio history, "Guiding Light", barely reached 2.1 million daily viewers in 2009 and ended on September 18 of that year, after a 72-year run. "As the World Turns" aired its final episode on September 17, 2010 after a 54-year run. "As the World Turns" was the last of 20 soap operas produced by Procter & Gamble, the soap and consumer goods company from which the genre got its name. "As The World Turns" and "Guiding Light" were also among the last of the soaps that originated from New York City. "All My Children", another New York-based soap, moved its production out to Los Angeles in an effort to reduce costs and raise sagging ratings, however, it along with "One Life to Live", each with a four-decade-plus run, were both cancelled in 2011. "All My Children" aired its network finale in September 2011 with "One Life to Live" following suit in January 2012. Both "All My Children" and "One Life to Live" were briefly revived online in 2013, before being canceled again that same year.
Causes.
As women increasingly worked outside of the home, daytime television viewing declined. New generations of potential viewers were not raised watching soap operas with their mothers, leaving the shows' long and complex storylines foreign to younger audiences. Now, as viewers age, ratings continue to drop among young adult women, the demographic group that soap opera advertisers pay the most for. Those who might watch in workplace breakrooms are not counted, as Nielsen does not track television viewing outside the home. The rise of cable and the internet has also provided new sources of entertainment during the day. Part of the genre's decline has even been attributed to audiences' switching to reality television as an alternative source of melodrama.
Daytime programming alternatives such as talk shows, game shows, and court shows cost up to 50% less to produce than scripted dramas, making those formats more profitable and attractive to networks, even if they receive the same or slightly lower ratings than soap operas. A network may even prefer to return a timeslot to its local stations to keeping a soap opera with disappointing ratings on the air, as was the case with "Sunset Beach" and "Port Charles". Compounding the financial pressure on scripted programming in the period from 2007 to 2010 was a decline in advertising, a cause of the late 2000s-early 2010s global recession, which led shows to reduce their budgets and cast sizes.
In addition to the decline comes a shift in demographics. Hispanic viewership has increased as more soap operas began to appear on major Spanish network stations, such as Televisa, Telemundo and Univision along with other Spanish network channels. Unlike some major network channels that only broadcast soap operas during the daytime it airs both daytime and primetime such as Telemundo. As American soap operas slowly declines some turn to Spanish-speaking soap operas despite the foreign language. However in recent years CC3 became a widely used for translating from Spanish to English thus gaining viewership in the Spanish soap opera, that trend will most likely continue.
The primetime serial.
Serials produced for primetime slots have also found success. The first real prime time soap opera was "Peyton Place" (1964–1969) on ABC. It was based in part on the eponymous 1957 film (which, in turn, was based on the 1956 novel).
The popularity of "Peyton Place" prompted the CBS network to spin-off popular "As the World Turns" character Lisa Miller into her own evening soap opera, "Our Private World" (originally titled "The Woman Lisa" in its planning stages). "Our Private World" was broadcast from May to September 1965. The character of Lisa returned to "As The World Turns" after the series ended.
The structure of "Peyton Place", with its episodic plots and long-running story arcs, set the mold for the primetime serials of the 1980s when the format reached its pinnacle.
The successful primetime serials of the 1980s included "Dallas", "Dynasty", "Knots Landing" and "Falcon Crest". These shows frequently dealt with wealthy families, and their personal and big-business travails. Common characteristics were sumptuous sets and costumes, complex storylines examining business schemes and intrigue, and spectacular disaster cliffhanger situations. Each of these series featured a wealthy, domineering, promiscuous, and passionate antagonist as a key character in the storyline – respectively, J. R. Ewing, Alexis Colby, Abby Cunningham and Angela Channing. These villainous schemers became immensely popular figures that audiences "loved to hate".
Unlike daytime serials, which are shot on video in a studio using the multi-camera setup, these evening series were shot on film using a single camera setup, and featured substantial location-shot footage, often in picturesque locales. "Dallas", its spin-off "Knots Landing", and "Falcon Crest" all initially featured episodes with self-contained stories and specific guest stars who appeared in just that episode. Each story was completely resolved by the end of the episode, and there were no end-of-episode cliffhangers. After the first couple of seasons, all three shows changed their story format to that of a pure soap opera, with interwoven ongoing narratives that ran over several episodes. "Dynasty" featured this format throughout its run.
The soap opera's distinctive open plot structure and complex continuity was increasingly incorporated into American primetime television programs of the period. The first significant drama series to do this was "Hill Street Blues". This series, produced by Steven Bochco, featured many elements borrowed from soap operas, such as an ensemble cast, multi-episode storylines, and extensive character development over the course of the series. It and the later "Cagney & Lacey" overlaid the police series formula with ongoing narratives exploring the personal lives and interpersonal relationships of the regular characters. The success of these series prompted other drama series, such as "St. Elsewhere" and situation comedy series, to incorporate serialized stories and story structure to varying degrees.
The primetime soap operas and drama series of the 1990s, such as "Beverly Hills, 90210", "Melrose Place" and "Dawson's Creek", focused more on younger characters. In the 2000s, ABC began to revitalize the primetime soap opera format with shows such as "Desperate Housewives", "Grey's Anatomy", "Brothers & Sisters", "Ugly Betty", "Private Practice", and more recently "Revenge", "Nashville", "Scandal", and formerly "Ringer", which its sister production company ABC Studios co-produced with CBS Television Studios for The CW. While not soaps in the traditional sense, these shows managed to appeal to wide audiences with their high drama mixed with humor, and are soap operas by definition. These successes led to NBC's launching serials, including "Heroes" and "Friday Night Lights". The upstart MyNetworkTV, a sister network of Fox, launched a line of primetime telenovelas (a genre similar to soap operas in terms of content) upon its launch in September 2006, but discontinued its use of the format in 2007, after disappointing ratings.
On June 13, 2012, a continuation of the 1980s soap opera, "Dallas", premiered on the cable network, TNT. The revived series (which entered its third season in 2014) has delivered solid ratings for the channel. In 2012, Nick at Nite debuted a primetime soap opera, "Hollywood Heights", which aired episodes five nights a week (on Monday through Fridays) in a manner similar to a daytime soap opera, instead of the once-a-week episode output common of other primetime soaps. The series, which was an adaptation of the Mexican telenovela "Alcanzar una estrella", suffered from low ratings (generally receiving less than one million viewers) and was later moved to sister cable channel TeenNick halfway through its run to burn off the remaining episodes.
In 2015, FOX debuted "Empire", a primetime musical soap opera centering on the power struggle between family members within the titular recording company. Created by Lee Daniels and Danny Strong and led by Oscar nominees Terrence Howard and Taraji P. Henson, the drama premiered to high ratings. The show is strongly influenced by other works such as William Shakespeare's "King Lear", James Goldman's "The Lion in Winter" and the 1980s soap opera "Dynasty".
Online serials.
Some web series are soap operas, such as "". In 2013, production company Prospect Park revived "All My Children" and "One Life to Live" for the web, retaining original creator Agnes Nixon as a consultant and keeping many of the same actors (Prospect Park purchased the rights to both series months after their cancellations by ABC in 2011, although it initially suspended plans to relaunch the soaps later that same year due to issues receiving approval from acting and production unions). Each show initially produced four half-hour episodes a week, but quickly cut back to two half-hour episodes each. In the midst of (though not directly related to) a lawsuit between Prospect Park and ABC, the experiment ended that same year, with both shows being canceled again.
United Kingdom.
In the United Kingdom, soap operas are one of the most popular genres, with most being broadcast during prime time. In comparison to U.S. serials which frequently portray romantic storylines in sumptuous and glamorous locales, most UK soap operas focus on more everyday, working-class communities.
The most popular soaps are "Coronation Street", "EastEnders", "Emmerdale", "Hollyoaks", "Doctors", and the Australian produced "Neighbours" and "Home and Away". The first three of these are consistently among the highest-rated shows on British television.
The 1986 Christmas Day episode of "EastEnders" is often referred to as the highest-rated UK soap opera episode ever, with 30.15 million viewers (in 2007, the UK had approximately 54 million television viewers). The figure of 30.15 million was actually a combination of the original broadcast which had just over 19 million viewers, and the Sunday omnibus edition with 10 million viewers. The combined 30.15 million audience figure often sees it attributed as the highest-rated program in UK television for the 1980s, comparable to the records set by the 1970 splashdown of Apollo 13 (28.6 million viewers) and Princess Diana's funeral in 1997 (32.1 million viewers).
"Coronation Street", "Emmerdale" and "EastEnders" are popularly known as the "flagship" soaps, as they are respectively the highest-rated programmes on ITV and the BBC. Poor ratings for a UK flagship serial sometimes brings with it questions about the associated channel. The soaps are so popular they are not routinely scheduled against each other. Episodes of serials have clashed only on isolated occasions when extended episodes have been broadcast.
Origins.
Soap operas in the UK began on radio and consequently were associated with the BBC. It had resisted soaps as antithetical to its quality image, but began broadcasting "Front Line Family" in April 1941 on its North American shortwave service to encourage American intervention on Britain's behalf in World War II. The BBC continues to broadcast the world's longest-running radio soap, "The Archers", which has been running nationally since 1951. It is currently broadcast on BBC Radio 4 and continues to attract over five million listeners, or roughly 25% of the radio listening population of the UK at that time of the evening.
An early television serial was "The Grove Family" on the BBC, which produced 148 episodes from 1954 to 1957. The programme was broadcast live and only a handful of recordings were retained in the archives. The UK's first twice-weekly serial was ITV's "Emergency - Ward 10", which was also the UK's first medical drama, running from 1957 until 1967. Although it was broadcast year-round, its status as a soap opera by today's standards is a subject of dispute.
In the 1960s "Coronation Street" revolutionised UK television and quickly became a British institution. The BBC also produced several serials: "Compact" was about the staff of a women's magazine; "The Newcomers" was about the upheaval caused by a large firm setting up a plant in a small town; "United!" contained 147 episodes and focused on a football team; "199 Park Lane" (1965) was an upper class serial, which ran for only 18 episodes. None of these serials came close to making the same impact as "Coronation Street". Indeed, most of the 1960s BBC serials were largely wiped.
During the 1960s, "Coronation Street"‍ '​s main rival was "Crossroads", a daily serial that began in 1964 and aired on ITV in the early evening. "Crossroads" was set in a Birmingham motel and, although the programme was popular, its purported low technical standard and bad acting were much mocked. By the 1980s, its ratings had begun to decline. Several attempts to revamp the programme through cast changes and, later, expanding the focus from the motel to the surrounding community were unsuccessful. "Crossroads" was cancelled in 1988 (a new version of "Crossroads" was later produced, running from 2001 until 2003).
A later rival to "Coronation Street" was ITV's "Emmerdale Farm" (later renamed "Emmerdale"), which began in 1972 in a daytime slot and was set in rural Yorkshire. Increased viewership resulted in "Emmerdale" being moved to a prime-time slot in the 1980s.
"Pobol y Cwm" ("People of the Valley") is a Welsh language serial that has been produced by the BBC since October 1974, and is the longest-running television soap opera produced by the broadcaster. "Pobol y Cwm" was originally broadcast on BBC Wales television from 1974 to 1982; it was then moved to the Welsh-language television station S4C when it opened in November 1982. The programme was occasionally shown on BBC1 in London during periods of regional optout in the mid- to late 1970s. "Pobol y Cwm" was briefly shown in the rest of the UK in 1994 on BBC2, with English subtitles; it is consistently the most watched programme each week on S4C.
The 1980s.
Daytime soap operas were non-existent until the 1970s because there was virtually no daytime television in the UK. ITV introduced "General Hospital", which later moved to a prime time slot, and Scottish Television had "Take the High Road", which lasted for over twenty years. Later, daytime slots were filled with an influx of older Australian soap operas such as "The Sullivans" (aired on ITV from 1977), "The Young Doctors" (from 1982), "Sons and Daughters" (from 1983), "A Country Practice" (from 1982), "Richmond Hill" (from 1988 to 1989) and eventually, "Neighbours" was acquired by the BBC in 1986, and "Home and Away" aired on ITV beginning in 1989. These achieved significant levels of popularity; "Neighbours" and "Home and Away" were moved to early-evening slots, helping begin the UK soap opera boom in the late 1980s.
The day Channel 4 began operations in 1982 it launched its own soap, the Liverpool-based "Brookside", which would redefine soaps over the next decade. The focus of "Brookside" was different from earlier soap operas in the UK; it was set in a middle-class new-build cul-de-sac, unlike "Coronation Street" and "Emmerdale Farm", which were set in established working-class communities. The characters in "Brookside" were generally either people who had advanced themselves from inner-city council estates, or the upper middle-class who had fallen on hard times. Though "Brookside" was still broadcast in a pre-watershed slot (8.00 p.m. and 8.30 p.m. on weekdays, around 5.00 p.m. for the omnibus on Saturdays), it was more liberal than other soaps of the time: the dialogue regularly included expletives. This stemmed from the overall more liberal policy of the channel during that period. The soap was also heavily politicised. Bobby Grant (Ricky Tomlinson), a militant trade-unionist anti-hero, was the most overtly political character. Storylines were often more sensationalist than on other soaps (throughout the soap's history, there were two armed sieges on the street) and were staged more graphically with violence (particularly, rape) often being featured.
In 1985, the BBC's London-based soap opera "EastEnders" debuted and became a near instant success with viewers and critics alike, with the first episode attracting over 17 million viewers. The Christmas Day 1986 episode was watched by 30.15 million viewers and contained a scene in which divorce papers were served to Angie Watts by her husband Den. Critics talked about the downfall of "Coronation Street", but the programme continued to perform successfully. In 1994, when the two serials were scheduled opposite each other, "Coronation Street" won the slot. For the better part of ten years, "EastEnders" has shared the number one position with "Coronation Street", with varying degrees of difference between the two.
A notable success in pioneering late-night broadcasting, in October 1984, Yorkshire Television began airing the cult Australian soap opera "Prisoner", which originally ran from 1979 to 1986. It was eventually broadcast on all regions of the UK in differing slots, usually around 23:00 (but never before 22:30 in any region), under the title "Prisoner: Cell Block H". It was probably most popular in the Midlands where Central Television consistently broadcast the serial three times a week from 1987 to 1991. Its airing in the UK was staggered, so different regions of the country saw it at a different pace. The programme was immensely successful, regularly achieving 10 million viewers when all regions' ratings per episode were added together. Central bowed to fan pressure to repeat the soap, of which the first 95 episodes aired. Then, rival station Channel 5 also acquired rights to repeat the entire rerun of the programme, starting in 1997. All 692 episodes have since been released on DVD in the UK.
The 1990s.
In 1992, the BBC debuted "Eldorado" to alternate with "EastEnders". The programme was heavily criticised and only lasted one year. Nevertheless soap operas gained increasing prominence on UK television schedules. In 1995, Channel 4 premiered "Hollyoaks", a soap with a youth focus. When Channel 5 launched in March 1997, it debuted the soap opera "Family Affairs", which was formatted as a weekdaily soap, airing Monday through Fridays.
"Brookside"‍ '​s premise evolved during the 1990s, phasing out the politicised stories of the 1980s and shifting the emphasis to controversial and sensationalist stories such as child rape, sibling incest, religious cults and drug addiction, including the infamous 'body under the patio' storyline which ran from 1993 to 1995, and gave the serial its highest ratings ever with 9 million viewers.
"Coronation Street" and "Brookside" began releasing straight-to-video features. The "Coronation Street" releases generally kept the pace and style of conventional programmes episodes with the action set in foreign locations. The "Brookside" releases were set in the programme's usual location, but featured stories with adult content not allowed on television pre-watershed, with these releases given '18' certificates.
A retooling of "Emmerdale Farm" led to the "Farm" being dropped from the programme's title in 1989. In 1993, many of the changes where executed via a plane crash that partially destroyed the village and killed several characters. This attracted criticism as it was broadcast near the fifth anniversary of the Lockerbie bombing. The storyline drew the soap its highest ever viewership at 18 million viewers, and helped massively with the revamp of the programme which became a success and helped grow "Emmerdale"‍ '​s in popularity.
Throughout the 1990s, "Brookside", "Coronation Street", "EastEnders" and "Emmerdale" continued to flourish. Each increased the number of episodes that aired on a weekly basis by at least one, further defining soap operas as the leading genre in British television.
The 2000s.
Since 2000, new soap operas have continued to be developed. Daytime drama "Doctors" began in March 2000, preceding "Neighbours" on BBC1. In 2002, as ratings for the Scottish serial "High Road" (formerly "Take The High Road") continued to decline, BBC Scotland launched "River City", which proved popular and effectively replaced "High Road" when it was cancelled in 2003. The long-running serial "Brookside" ended in November 2003 after 21 years on the air, leaving "Hollyoaks" as Channel 4's flagship serial.
A new version of "Crossroads" featuring a mostly new cast was produced by Carlton Television for ITV in 2001. It did not achieve high ratings and was cancelled in 2003. In 2001, ITV also launched a new early-evening serial entitled "Night and Day". This programme too attracted low viewership and, after being shifted to a late night time slot, was cancelled in 2003. "Family Affairs", which was broadcast opposite the racier "Hollyoaks", never achieved significantly high ratings leading to several dramatic casting revamps and marked changes in style and even location over its run. By 2004, "Family Affairs" had a larger fan base and won its first awards, but was cancelled in late 2005.
In 2008, ITV premiered "The Royal Today", a daily spin-off of popular 1960s drama "The Royal", which had been running in a primetime slot since 2002. Just days later, soap opera parody programmes "Echo Beach" premiered alongside its sister show, the comedy "Moving Wallpaper". Both "Echo Beach" and "The Royal Today" ended after just one series due to low ratings. Radio soap opera "Silver Street" debuted on the BBC Asian Network in 2004. Poor ratings and criticism of the programme led to its cancellation in 2010.
Format.
UK soap operas for many years usually only aired two nights a week. The exception was the original "Crossroads" which began as a weekdaily soap opera in the 1960s, but later had its number of weekly broadcasts reduced. Things started to change in 1989 when "Coronation Street" began airing three times a week. In 1996, it expanded again, to air four episodes a week. "Brookside" premiered in 1982 with two episodes a week. In 1990, it expanded to three episodes a week; the trend was followed by "EastEnders" in 1994 and "Emmerdale" in 1997. "Family Affairs" debuted as a weekdaily soap in 1997, producing five episodes a week throughout its entire run. The imported "Neighbours" screens as five new episodes a week, which are shown once at 1:45 p.m. and repeated at 5:30 p.m. on Channel 5 each weekday.
Currently, "Coronation Street" (which began airing two episodes on Monday nights in 2002) and "Hollyoaks" both produce five episodes a week, while "EastEnders" produces four each week. In 2002, "Brookside" expanded from three half-hour episodes on different weeknights to airing one 90-minute episode each week. In 2004, "Emmerdale" began airing six episodes a week. "Doctors" airs five episodes a week, and is the only soap without a weekend omnibus repeat screening.
Due to a January 2008 overhaul of the ITV network, the Sunday episodes of "Coronation Street" and "Emmerdale" were moved out of their slots. "Coronation Street" added a second episode on Friday evenings at 8:30 p.m. "Emmerdale"‍ '​s Tuesday edition was extended to an hour, putting it in direct competition with rival "EastEnders".
In July 2009, the schedules of these serials were changed again. On 23 July 2009, "Coronation Street" moved from the Wednesday slot it held for 49 years, to Thursday evenings. "Emmerdale" reverted to running just one 30-minute episode on Tuesday evenings and the other 30-minute installment was moved to Thursday evenings.
UK soap operas are shot on videotape in the studio using a multi-camera setup. Since the 1980s, these programmes routinely feature scenes shot outdoors in each episode. This footage is shot on videotape on a purpose-built outdoor set that represents the community that the soap focuses on. During their early years, "Coronation Street" and "Emmerdale" used 16 mm film while scenes were shot on location. Later soap operas such as "Hollyoaks" and "Family Affairs", started filming on high-definition video, a more modern filming process, as opposed to standard videotape, which features better quality and appears to look more like film than videotape.
UK soap operas do not incorporate recap sequences at the beginning of each episode, which would be appropriate for the fact that when an episode ends, it picks up the story during the following episode. However, in 2012, "Hollyoaks" began airing a recap sequence at the beginning of each episode. Soap operas in the UK also lack incidental music (apart from "Hollyoaks"), although "Eastenders" would sometimes feature music which plays over an ending scene if it was dramatic, with an alternative "Eastenders" theme known as "Julia's theme".
Australia.
Australia has had quite a number of well-known soap operas, some of which have gained cult followings in the UK, New Zealand and other countries. The majority of Australian television soap operas are produced for early evening or evening timeslots. They usually produce two or two-and-a-half hours of new material each week, either arranged as four or five half-hour episodes a week, or as two one-hour episodes. Stylistically, these series most closely resemble UK soap operas in that they are nearly always shot on videotape, are mainly recorded in a studio and use a multi-camera setup. The original Australian serials were shot entirely in-studio. During the 1970s occasional filmed inserts were used to incorporate sequences shot outdoors. Outdoor shooting later became commonplace and starting in the late 1970s, it became standard practice for some on-location footage to be featured in each episode of any Australian soap opera, often to capitalise on the attractiveness and exotic nature of these locations for international audiences. Most Australian soap operas focus on a mixed age range of middle-class characters and will regularly feature a range of locations where the various, disparate characters can meet and interact, such as the café, the surf club, the wine bar or the school.
Early serials.
The genre began in Australia, as in other countries, on radio. One such radio serial, "Big Sister", featured actress Thelma Scott in the cast and aired nationally for five years beginning in 1942. Probably the best known Australian radio serial was the long-running Gwen Meredith soap opera "Blue Hills" which ran from 1949 to 1976. With the advent of Australian television in 1956, daytime television serials followed. The first Australian television soap opera "Autumn Affair" (1958), with radio personality and "Blue Hills" star Queenie Ashton making the transition to television. Each episode of this serial ran for 15 minutes and aired each weekday on the Seven Network. The series failed to secure a sponsor and ended in 1959 after 156 episodes. This was followed by "The Story of Peter Grey" (1961), another Seven Network weekday series which aired in a daytime slot, with each episode running for 15 minutes; "The Story of Peter Grey" ran for 164 episodes.
The first successful wave of Australian evening soap operas started in 1967 with "Bellbird", produced by the Australian Broadcasting Corporation. This rural-based serial screened in an early evening slot in 15-minute installments as a lead-in to the evening news. "Bellbird" was a moderate success but built-up a consistent and loyal viewer base, especially in rural areas, and enjoyed a ten-year run. "Motel" (1968) was Australia's first half-hour soap opera; the daytime soap had a short run of 132 episodes.
The 1970s.
The first major soap opera hit in Australia was the sex-melodrama "Number 96", a nighttime series produced by Cash Harmon Television for Network Ten, which debuted March 1972. The program dealt with such topics as homosexuality, adultery, drug use, rape-within-marriage and racism which had rarely been explored on Australian television programs before. The series became famous for its sex scenes and nudity and for its comedic characters, many of whom became cult heroes in Australia. By 1973, "Number 96" had become Australia's highest-rated show. In 1974, the sexed-up antics of "Number 96" prompted the creation of "The Box", which rivaled it in terms of nudity and sexual situations and was scheduled in a nighttime slot. Produced by Crawford Productions, many critics considered "The Box" to be a more slickly produced and better written show than "Number 96". "The Box" also aired on the Ten Network, programmed to run right after "Number 96". For 1974 "Number 96" was again the highest rating show on Australian television, and that year "The Box" occupied the number two spot.
Also in 1974, the Reg Grundy Organisation created its first soap opera, and significantly Australia's first "teen" soap opera, "Class of '74". With its attempts to hint at the sex and sin shown more openly on "Number 96" and "The Box", its high school setting and early evening timeslot, "Class of '74" came under intense scrutiny from the Broadcasting Control Board, who vetted scripts and altered entire storylines.
By 1975, both "Number 96" and "The Box", perhaps as a reaction to declining ratings for both shows, de-emphasised the sex and nudity shifting more towards comedic plots. "Class of '74" was renamed "Class of '75" and also added more slapstick comedy for its second year, but the revamped show's ratings declined, resulting in its cancellation in mid-1975. That year Cash Harmon's newly launched second soap "The Unisexers" failed in its early evening slot and was cancelled after three weeks; the Reg Grundy Organisation's second soap "Until Tomorrow" ran in a daytime slot for 180 episodes.
A feature film version of "Bellbird" entitled "Country Town" was produced in 1971 by two of the show's stars, Gary Gray and Terry McDermott, without production involvement by the Australian Broadcasting Corporation. "Number 96" and "The Box" also released feature film versions, both of which had the same title as the series, released in 1974 and 1975 respectively. As Australian television had broadcast in black and white until 1975, these theatrical releases all had the novelty of being in colour. The film versions of "Number 96" and "The Box" also allowed more explicit nudity than could be shown on television at that time.
In November 1976 "The Young Doctors" debuted on the Nine Network. This Grundy Organization series eschewed the adult drama of "Number 96" and "The Box", focusing more on relationship drama and romance. It became a popular success but received few critical accolades. A week later "The Sullivans", a carefully-produced period serial chronicling the effects of World War II on a Melbourne family, also debuted on Nine. Produced by Crawford Productions, "The Sullivans" became a ratings success, attracted many positive reviews, and won television awards. During this period "Number 96" re-introduced nudity into its episodes, with several much-publicised full-frontal nude scenes, a cast revamp and a new range of shock storylines designed to boost the show's declining ratings. "Bellbird" experienced changes to its broadcast pattern with episodes screening in 60 minute blocks, and later in 30 minute installments.
"Bellbird", "Number 96" and "The Box", which had been experiencing declining ratings, were cancelled in 1977. Various attempts to revamp each of the shows with cast reshuffles or spectacular disaster storylines had proved only temporarily successful. "The Young Doctors" and "The Sullivans" continued to be popular. November 1977 saw the launch of successful soap opera/police procedural series "Cop Shop" (1977–1984) produced by Crawford Productions for Channel Seven. In early December 1977 Channel Ten debuted the Reg Grundy Organisation produced "The Restless Years" (1977–1981), a more standard soap drama focusing on several young school leavers.
The Seven Network, achieving success with "Cop Shop" produced by Crawford Productions, had Crawfords produce "Skyways", a series with a similar format but set in an airport, to compete with the Nine Network's popular talk show "The Don Lane Show". "Skyways", which debuted in July 1979, emphasised adult situations including homosexuality, marriage problems, adultery, prostitution, drug use and smuggling, crime, suicide, political intrigue, and murder, and featured some nudity. Despite this, the program achieved only moderate ratings and was cancelled in mid-1981.
The 1980s.
The Reg Grundy Organisation found major success with the women's-prison drama "Prisoner" (1979–1986) on Network Ten, and melodramatic family saga "Sons and Daughters" (1982–1987) on the Seven Network. Both shows achieved high ratings in their original runs, and unusually, found success in repeats after the programs ended.
Grundy soap "The Young Doctors" and Crawford Productions' "The Sullivans" continued on the Nine Network until late 1982. Thereafter Nine attempted many new replacement soap operas produced by the Reg Grundy Organisation: "Taurus Rising" (1982), "Waterloo Station" (1983), "Starting Out" (1983) and "Possession" (1985), along with "Prime Time" (1986) produced by Crawford Productions. None of these programs were successful and most were cancelled after only a few months. The Reg Grundy Organisation also created "Neighbours", a suburban-based daily serial devised as a sedate family drama with some comedic and lightweight situations, for the Seven Network in 1985.
Produced in Melbourne at the studios of HSV-7, "Neighbours" achieved high ratings in Melbourne, Brisbane and Adelaide, but not in Sydney, where it aired at 5.30 p.m. placing it against the hit dating game show "Perfect Match" on Channel 10. The Seven Network's Sydney station ATN-7 quickly lost interest in "Neighbours" as a result of the low ratings in Sydney. HSV-7 in Melbourne lobbied heavily to keep "Neighbours" on the air, but ATN-7 managed to convince the rest of the network to cancel the show and instead keep ATN-7's own Sydney-based dramas "A Country Practice" and "Sons and Daughters".
After the network cancelled "Neighbours", it was immediately picked up by Channel Ten, which revamped the cast and scripts slightly and aired the series in the 7.00 p.m. slot starting 20 January 1986. It initially attracted low audiences, however after a concerted publicity drive, Ten managed to transform the series into a major success, turning several of its actors into major international stars. The show's popularity eventually declined and it was moved to the 6.30 p.m. slot in 1992. As of 2015 the series airs on Eleven and is Australia's longest-running soap opera.
The success of "Neighbours" in the 1980s prompted the creation of somewhat similar suburban and family or teen-oriented soap operas such as "Home and Away" (1988–present) on Channel Seven and "Richmond Hill" (1988) on Channel Ten. Both proved popular, however "Richmond Hill" emerged as only a moderate success and was cancelled after one year to be replaced on Ten by "E Street" (1989–1993).
Nine continued trying to establish a successful new soap opera, without success. After the failure of family drama "Family and Friends" in 1990, it launched the raunchier and more extreme "Chances" in 1991, which resurrected the sex and melodrama of "Number 96" and "The Box" in an attempt to attract attention. "Chances" achieved only moderate ratings, and was moved to a late-night timeslot. It underwent several revamps that removed much of the original cast, and refocused the storylines to incorporate science-fiction and fantasy elements. The series continued in a late night slot until 1992, when it was cancelled due to low ratings despite the much-discussed fantasy storylines.
Australian soaps internationally.
Several Australian soap operas have also found significant international success. In the UK, starting in the mid-1980s, daytime broadcasts of "The Young Doctors", "The Sullivans", "Sons and Daughters" and "Neighbours" (which itself was subsequently moved to an early-evening slot) achieved significant success. Grundy's "Prisoner" began airing in the United States in 1979 and achieved high ratings in many regions there, however the show ended its run in that country three years into its run. "Prisoner" also aired in late-night timeslots in the UK beginning in the late 1980s, achieving enduring cult success there. The show became so popular in that country that it prompted the creation of two stage plays and a stage musical based on the show, all of which toured the UK, among many other spin-offs. In the late 1990s, Channel 5 repeated "Prisoner" in the UK. Between 1998 and 2005, Channel 5 ran late-night repeats of "Sons and Daughters". During the 1980s, the Australian attempts to emulate big-budget U.S. soap operas such as "Dallas" and "Dynasty" had resulted in the debuts of "Taurus Rising" and "Return to Eden", two slick soap opera dramas with big budgets that were shot entirely on film. Though their middling Australian ratings resulted in the shows running only for a single season, both programs were successfully sold internationally.
Other shows to achieve varying levels of international success include "Richmond Hill", "E Street", "Paradise Beach" (1993–1994), and "Pacific Drive" (1995–1997). Indeed these last two series were designed specifically for international distribution. Channel Seven's "Home and Away", a teen soap developed as a rival to "Neighbours", has also achieved significant and enduring success on UK television.
Teen-oriented serials to the world.
Since 1990, most new Australian serials have been based on the successful "Neighbours" formula of foregrounding youthful attractive casts in appealing locations. An exception to this was the Australian Broadcasting Corporation-produced "Something in the Air", a serial examining a range of characters in a small country town; this series ran from 2000 to 2002.
Attempts to replicate the success of daily teen-oriented serials "Neighbours" and "Home and Away" saw the creation of "Echo Point" (1995) and "Breakers" (1999) on Network Ten. None of these programs emerged as long-running successes and "Neighbours" and "Home and Away" remained the most visible and consistently successful Australian soap operas in production. In their home country, they both attract respectable although not spectacular ratings. By 2004, "Neighbours" was regularly attracting just under a million viewers per episode – a low figure for Australian prime time television. By March 2007, Australian viewership for "Neighbours" had fallen to fewer than 700,000 a night, prompting a revamp of the show's cast and visual presentation, and a de-emphasis on the action-oriented direction that the series had moved in to refocus the show on the family storylines that it is traditionally known for. However, "Neighbours" and "Home and Away" both continue to achieve significant ratings in the UK. This and other lucrative overseas markets, along with Australian broadcasting laws that enforce a minimum amount of domestic drama production on commercial television networks, help ensure that both programs remain in production. Both shows get higher total ratings in the UK than in Australia (the UK has three times the total population of Australia) and the UK channels make a major contribution to the production costs.
It has been suggested that with their emphasis on the younger, attractive and charismatic characters, "Neighbours" and "Home and Away" have found success in the middle ground between glamorous, fantastic U.S. soaps with their wealthy but tragic heroes and the more grim, naturalistic UK soap operas populated by older, unglamorous characters. The casts of "Neighbours" and "Home and Away" are predominantly younger and more attractive than the casts of UK soaps, and without excessive wealth and glamour of the U.S. daytime serial, a middle-ground in which they have found their lucrative niche.
"Neighbours" was carried in the United States on the Oxygen cable channel in March 2004; however it attracted few viewers, perhaps in part due to its scheduling opposite well-established and highly popular U.S. soap operas such as "All My Children" and "The Young and the Restless", and was dropped by the network shortly afterwards due to low ratings.
"headLand" made its debut on Channel Seven in November 2005, the series arose out of a proposed spinoff of "Home and Away" that was to have been produced in conjunction with "Home and Away"‍ '​s UK broadcaster, Five. The idea for the spin-off was scuttled after Five pulled out of the deal, which meant that the show could potentially air on a rival channel in the UK; as such, Five requested that the new show be developed as a standalone series and not be spun off from a series that it owned a stake in. The series premiered in Australia on November 15, 2005, but was not a ratings success and was cancelled two months later on January 23, 2006. The series broadcast on E4 and Channel 4 in the UK.
After losing the UK television rights to "Neighbours" to Five, the BBC commissioned a new serial "Out of the Blue", which was produced in Australia, as its replacement. It debuted as part of BBC One's weekday afternoon schedule on April 28, 2008 but due to viewership that was lower than desired, it was moved to BBC Two on May 19, 2008. The series was cancelled after its first season.
"Neighbours"' continued low ratings in Australia resulted in it being moved to Ten's new digital channel, Eleven on January 11, 2011. However, it continues to achieve reasonable ratings on Channel Five in the United Kingdom, and as of March 2013 still reportedly achieved significant international sales.
New Zealand.
Television.
Pioneering series "" aired over two years (1971–72) and was the NZBC's first continuing drama. It followed the goings-on of a North Island timber town.
"Close to Home" is a New Zealand television soap opera which ran on Television One (Formally NZBC, later becoming Television New Zealand) from 1975 to 1983. At its peak in 1977 nearly one million viewers tuned in twice weekly to watch the series co-created by Michael Noonan and Tony Isaac (who had initially only agreed to make the show on the condition they would get to make "The Governor"). "Gloss" is a television drama series that screened from 1987 to 1990. The series is about a fictional publishing empire run by the Redfern family. Gloss was NZ's answer to US soap "Dynasty", with the Carrington oil scions replaced by the wealthy Redferns and their Auckland magazine empire. It was a starting point for many actors who went on to many productions in New Zealand, Australia and around the world including Temuera Morrison, Miranda Harcourt, Peter Elliott, Lisa Chappell, Danielle Cormack and Kevin Smith. Many of them would go on to star in "Shortland Street" which has been New Zealand's most popular soap since its debut in 1992. It airs on TVNZ 2.
Radio.
Radio New Zealand began airing its first radio soap "You Me Now" in September 2010. It is available for podcast on its website.
Canada.
Relatively few daily soap operas have been produced on English Canadian television, with most Canadian stations and networks that carry soap operas airing those imported from the United States or the United Kingdom. Notable daily soaps that did exist included "Family Passions", "Scarlett Hill", "Strange Paradise", "Metropia", "Train 48" and the international co-production "Foreign Affairs". "Family Passions" was an hour-long program, as is typical of American daytime soaps; all of the others ran half-hour episodes. Unlike American or British soap operas, the most influential of which have run for years or even decades, even daily Canadian soap operas have run for a few seasons at most. Short-run soaps, including "49th & Main" and "North/South", have also aired. Many of these were produced in an effort to comply with Canadian content regulations, which require a percentage of programming on Canadian television to originate from Canada.
Notable prime time soap operas in Canada have included "Riverdale", "House of Pride", "Paradise Falls", "Lance et Compte" ("He Shoots, He Scores"), "Loving Friends and Perfect Couples" and "The City". The "Degrassi" franchise of youth dramas also incorporated some elements of the soap opera format.
On French-language television in Quebec, the "téléroman" has been a popular mainstay of network programming since the 1950s. Notable téléromans have included "Rue des Pignons", "Les Belles Histoires des pays d'en haut", "Diva", "La famille Plouffe", and the soap opera parody "Le Cœur a ses raisons".
India.
India has produced many soap operas. These started in the late 1980s, as more and more people began to purchase television sets. At the beginning of the 21st century, soap operas became an integral part of Indian culture. Indian soap operas mostly concentrate on the conflict between love and arranged marriages occurring in India, and many include conflicts between mothers-in-law and daughters-in-law. Indian serials are produced in the Hindi, Tamil, Punjabi, Marathi, Gujarati, Bengali, Kannada, Telugu, and Malayalam languages.
Many soap operas produced in India are also broadcast overseas in the UK, the United States, and some parts of Europe, South Africa and Australia. They are often mass-produced under large production banners, with companies like Balaji Telefilms running different language versions of the same serial on different television networks or channels.
Europe.
Remakes of Australian serials.
The Australian serial "The Restless Years" was remade in the Netherlands as "Goede tijden, slechte tijden" (which debuted in 1990) and in Germany as "Gute Zeiten, schlechte Zeiten" (which has aired since 1992): both titles translate to "good times, bad times". These remakes still airing, although they have long since diverged from the original Australian storylines. The two shows are the highest-rated soap operas in their respective countries.
A later Australian serial, "Sons and Daughters", has inspired five remakes produced under license from the original producers and based, initially, on original story and character outlines. These are "Verbotene Liebe" (Germany, 1995–present); "Skilda världar" (Sweden, 1996–2002); "Apagorevmeni agapi" (Greece, 1998); "Cuori Rubati" (Italy, 2002–2003) and "Zabranjena ljubav" (Croatia, 2004–2008). Both "The Restless Years" and "Sons and Daughters" were created and produced in Australia by the Reg Grundy Organisation.
Norway.
The Norwegian soap opera "Hotel Cæsar" has aired on TV 2 since 1998, and is the longest-running television drama in Scandinavia. Popular foreign soaps in the country include "Days of Our Lives" (broadcast on TV6), "The Bold and the Beautiful" (TNT) and "Home and Away" (TV 2), all of which are subtitled.
Netherlands.
Serials have included "Goede tijden, slechte tijden" (1990–present), "ONM" (1994–2010) and "Goudkust" (1996–2001). U.S. daytime serials "As The World Turns" and "The Bold and the Beautiful" have aired in the Netherlands; "As the World Turns" began airing in the country in 1990, with Dutch subtitles.
Germany.
In the 1980s, German networks successfully added American daytime and primetime soap operas to their schedule before Das Erste introduced its first self-produced weekly soap with "Lindenstraße", which was seen as a German counterpart to "Coronation Street". Like in other countries, the soap opera met with negative reviews, but eventually proved critics wrong with nearly 13 million viewers tuning in each week. Even though the format proved successful, it was not until 1992 before "Gute Zeiten, schlechte Zeiten" became the first German daily soap opera. Early ratings were bad as were the reviews, but the RTL network was willing to give its first soap opera a chance; ratings would improve, climbing to 7 million viewers by 2002. Not long after "Gute Zeiten, schlechte Zeiten", Das Erste introduced "Marienhof", which aired twice a week.
After successfully creating the first German daily soap, production company Grundy Ufa wanted to produce another soap for RTL. Like "GZSZ", the format was based on an Australian soap opera from Reg Watson. But RTL did not like the plot idea about separated twins who meet each other for the first time after 20 years and fall in love without knowing that they are related. The project was then taken to Das Erste, which commissioned the program, titled "Verbotene Liebe", which premiered on January 2, 1995. With the premiere of "Verbotene Liebe", the network turned "Marienhof" into a daily soap as well. In the meanwhile, RTL debuted the Grundy Ufa-produced "Unter uns" in late 1994.
ZDF started a business venture with Canada and co-produced the short-lived series "Family Passions", starring actors such as Gordon Thomson, Roscoe Born, Dietmar Schönherr and a young Hayden Christensen. The daytime serial premiered on December 5, 1994, lasting 130 episodes. After its cancellation, the network debuted "Jede Menge Leben". Even after a crossover with three soaps, "Freunde fürs Leben", "Forsthaus Falkenau" and "Unser Lehrer Doktor Specht", the soap was canceled after 313 episodes. Sat.1 tried to get into the soap business as well, after successfully airing the Australian soap opera "Neighbours", which was dropped in 1995 due to the talk show phenomenon which took over most of the daytime schedules of German networks. The network first tried to tell a family saga with "So ist das Leben – die Wagenfelds", before failing with "Geliebte Schwestern". RTL II made its own short-lived attempt with "Alle zusammen - jeder für sich".
The teen soap opera "Schloss Einstein" debuted on September 4, 1998, focusing on the life of a group of teenagers at the fictional titular boarding school near Berlin. As of July 2014, the series has produced over 815 episodes during the course of 17 seasons, a milestone in German television programming, and was renewed for an 18th season to debut in 2015.
In 1999, after the lasting success of "Gute Zeiten, schlechte Zeiten", "Marienhof", "Unter uns" and "Verbotene Liebe", ProSieben aired "Mallorca – Suche nach dem Paradies", set on the Spanish island with the same name. After nine months, the network canceled the program due to low viewership and high production costs. Even though ratings had improved, the show ended its run in a morning timeslot. The soap opera became something of a cult classic, as its 200-episode run was repeated several times on free-to-air and pay television.
In 2006, "Alles was zählt" became the last successful daily soap to make its debut, airing as a lead-in to "Gute Zeiten, schlechte Zeiten" and also produced by Grundy Ufa. Since Germany started to produce its own telenovelas, all soap operas faced declines in ratings. "Unter uns" was in danger of cancellation in 2009, but escaped such a fate due to budget cuts imposed by the show's producers and the firing of original cast member Holger Franke, whose firing and the death of his character outraged fans, resulting in a ratings spike in early 2010. After "Unter uns" was saved, Das Erste planned to make changes to its soap lineup. "Marienhof" had to deal with multiple issues in its storytelling, as well as in producing a successful half-hour show. Several changes were made within months, however "Marienhof" was canceled in June 2011. "Verbotene Liebe" was in danger of being canceled as well, but convinced the network to renew it with changes that it made in both 2010 and 2011; the soap was later expanded to an hour after "Marienhof" was canceled, and the network tried to decide on whether to revamp its lineup.
While "Gute Zeiten, schlechte Zeiten", "Unter uns", "Verbotene Liebe" and "Alles was zählt" are currently the only daily soaps on the air, the telenovelas "Sturm der Liebe" and "Rote Rosen" are considered soaps by the press as well, thanks to the changing protagonists every season.
Belgium.
In Belgium, the two major soap operas are "Thuis" ("Home") and "Familie" ("Family"). Soap operas have been very popular in Flanders, the Dutch-speaking part of Belgium. "Familie" debuted in late 1991, and with nearly 5,000 half-hour episodes, it has the highest episode total of any soap in Europe. The highest-rated soap opera is "Thuis", which ahs aired on "één" since late 1995. "Thuis" is often one of the five most-watched Belgian shows and regularly garners over one million viewers (with 6.3 million Flemmings in total).
During the 1990s, foreign soap operas such as "Neighbours" and "The Bold and the Beautiful" were extremely popular, the latter having achieved a cult status in Belgium and airing in the middle of the decade during prime time. Both soaps still air today, along with other foreign soaps such as "Days of Our Lives", Australia's "Home and Away" and Germany's "Sturm der Liebe". Vitaya unsuccessful attempted to air the Dutch soap opera "Goede Tijden, Slechte Tijden" in 2010. Other foreign soaps that previously aired on Belgian television include "The Young and the Restless", "EastEnders" (both on VTM), "Port Charles" (at één, then known as TV1) and "Coronation Street" (on Vitaya). "Santa Barbara" aired during the 1990s on VTM for its entire run.
The only teen soap opera on Belgian television was "Spring" ("Jump" in English), which aired on the youth-oriented Ketnet and produced over 600 15-minute episodes from late 2002 until 2009, when it was cancelled after a steady decline in ratings following the departures of many of its original characters.
Italy.
The most successful soap operas in Italy are the evening series "Un posto al sole" ("A Place Under the Sun"), which had aired on Rai 3 since 1996 whose format is based on Aussie's soap Neighbours and the daytime series "Centovetrine" ("Hundred Shop Windows"), which debuted in 2001 on Canale 5. Several other Italian soaps have been produced such as "Ricominciare" ("Starting Over"), "Cuori rubati" ("Stolen Hearts"), "Vivere" ("Living"), "Sottocasa" ("Downstairs") and "Agrodolce" ("Bittersweet").
The most popular Italian prime-time drama series, "Incantesimo" ("Enchantment"), which ran from 1998 to 2008, became a daytime soap opera for the final two years of its run, airing five days a week on Rai 1.
Ireland.
Television.
In the early years of RTÉ, the network produced several dramas but had not come close to launching a long-running serial. RTÉ's first television soap was "Tolka Row", which was set in urban Dublin. For several years, both "Tolka Row" and "The Riordans" were produced by RTÉ; however, the urban soap was soon dropped in favor of the more popular rural soap opera "The Riordans", which premiered in 1965. Executives from Yorkshire Television visited during on-location shoots for "The Riordans" in the early 1970s and in 1972, debuted "Emmerdale Farm" (now "Emmerdale"), based on the successful format of the Irish soap opera. In the late 1970s, "The Riordans" was controversially dropped. The creator of that series would then go on to produce the second of his "Agri-soap" trilogy "Bracken", starring Gabriel Byrne, whose character had appeared in the last few seasons of "The Riordans". "Braken" was soon replaced by the third "Agri-soap" "Glenroe", which ran until 2001. As RTÉ wanted a drama series for its Sunday night lineup rather than a soap opera, "On Home Ground" (2001–2002), "The Clinic" (2002–2009) and "RAW" (2010–2013) replaced the agri-soaps of the previous decades.
In 1989, RTÉ decided to produce its first Dublin-based soap opera since the 1960s. "Fair City", which is set in the fictional city of Carrickstown, initially aired one night a week during the 1989-90 season, and similar to its rural soaps, much of the footage was filmed on location – in a suburb of Dublin City. In 1992, RTÉ made a major investment into the series by copying the houses used in the on-location shoots for an on-site set in RTÉ's Headquarters in Dublin 4. By the early 1990s, it was airing two nights a week for 35 weeks a year. With competition from the UK soap operas, RTÉ expanded "Fair City" to three nights a week for most of the year and one night a week during the summer in 1996, later expanding to four nights a week and two nights during the summer. Until the early 2000s, the series produced four episodes a week, airing all 52 weeks of the year. "Fair City" airs Sundays, Tuesdays and Thursdays at 8.00 p.m. GMT on RTÉ One; however, after rival network TV3 moved "Coronation Street" to Thursday night, the Wednesday night episode of "Fair City" began airing at 7:30 p.m. each week.
TG4 produce the Irish language soap "Ros na Rún" ("Headland of the Secrets" or "Headland of the Sweethearts"); set in the fictional village of "Ros Na Rún", located outside Galway and near Spiddal, it centres on the domestic and professional lives of the town's residents. It is modeled on an average village in the West of Ireland, but with its own distinct personality – with a diverse population that share secrets, romances and friendships among other things. While the core community has remained the same, the look and feel of "Ros Na Rún" has changed and evolved over the years to incorporate the changing face of rural Ireland. It has an established a place not only in the hearts and minds of the Irish speaking public, but also the wider Irish audience. The programe has dealt with many topics, including domestic violence, infidelity, theft, arson, abortion, homosexuality, adoption, murder, rape, drugs, teen pregnancy and paedophilia. It runs twice a week for 35 weeks of the year, currently airing Tuesday and Thursday nights. "Ros na Rún" is the single largest independent production commissioned in the history of Irish broadcasting. Prior to TG4's launch, it originally aired on RTÉ One in the early 1990s.
Although Ireland has access to international soaps (such as "Coronation Street", "Emmerdale", "EastEnders", "Home and Away", "Hollyoaks" and "Neighbours"), "Fair City" continues to outperform them all, and is Ireland's most popular soap opera, with the show peaking at over 700,000 viewers.
January 2015 "Red Rock" has broadcast on TV3. Red Rock airs twice a week on Wednesday and Thursday nights. The series is base in a fishing village in Dublin. The soaps centres around the local Garda station but also includes stories from the village.
Radio.
RTÉ Radio produced its first radio soap, "Kennedys of Castleross", which ran from April 13, 1955 to 1975. In 1979 RTÉ long running TV soap The Riordans moved to Radio until December 24, 1985. In the mid-1980s, RTÉ debuted a new radio soap, "Harbour Hotel", which ran until the mid-1990s. The network later ran two short-lived radio soaps, "Konvenience Korner" and "Riverrun", which were followed in 2004 by "Driftwood". RTÉ does not run any radio soaps, however RTÉ Radio 1 continues to air radio dramas as part of its nighttime schedule.
France.
France had no real tradition of running daytime dramas; however in 2004, the primetime soap "Plus belle la vie" premiered on public television network France 3. After initially suffering from poor ratings, the show became a huge success and is one of the highest-rated series on the network. Other attempts were made by competitors to create soaps (including "Seconde Chance", "Cinq soeurs" and "Paris 16ème"), but none have achieved much success.
Greece.
In Greece, there have been several soap operas.
ANT1.
An early serial was "Sti skia tou hrimatos" ("Money Shadows"), which ran from 1990 to 1991. September 1991 saw the debut of "Lampsi" ("the Shining"), from creator Nicos Foskolos. The series would become Greece's longest-running soap opera. After the success of "Lampsi" came the short lived "To galazio diamandi" ("Blue Diamond") and "Simphonia siopis" ("Omertà"). "Lampsi" was canceled in June 2005 due to declining ratings. It was replaced by "Erotas" ("Love"), a soap that ran from 2005 to 2008. After that series ended, ANT1 abandoned the soap opera genre and focused on comedy series and weekly dramas.
Greece's second longest-running soap is "Kalimera Zoi" ("Goodmorning Life"), which ran from September 1993 until its cancellation in June 2006 due to low ratings.
MEGA.
Mega Channel began producing soap operas in 1990 with the prime time serial "I Dipsa" ("The Thirst"), which ran for 102 episodes. Other daytime soaps have included "Paralliloi dromoi" (1992–1994) and its successor "Haravgi" ("Daylight", 1994–1995), both of which were cancelled due to low viewership; as well as the serials "Apagorevmeni Agapi" ("Forbidden Love"), which ran from 1998 to 2006; "Gia mia thesi ston Ilio" ("A Spot Under the Sun"), which ran from 1998 to 2002; "Filodoxies" ("Expectations"), which ran from 2002 to 2006; and "Vera Sto Deksi" ("Ring on the Right Hand"), which ran from 2004 to 2006 and proved to be a successful competitor to "Lampsi", causing that show's ratings to decline.
"Ta Mistika Tis Edem" ("Edem Secrets"), which was created by the producers of "Vera Sto Deksi", debuted in 2008 and has eclipsed that show's success. Its ratings place it consistently among the three highest-rated daytime programs.
ERT.
IENED (which was renamed ERT2 in 1982) was responsible for the first Greek soap operas "I Kravgi Ton Likon" and "Megistanes". ERT also produced the long-running soap "O Simvoleografos". Since 2000 and with the introduction of private television, ERT produced additional daily soap operas – which included "Pathos" ("Passion"), "Erotika tis Edem" ("Loving in Eden") and "Ta ftera tou erota" ("The Wings of Love") – however, these failed to achieve high ratings and were canceled shortly after their premiere.
ALPHA.
Alpha produced "Kato apo tin Acropoli" ("Under the Acropolis"), which ran for 2½ years.
Cyprus.
Weekday shows.
The first daytime soap opera produced by a Cyprus channel was LOGOs TV's "Odos Den Ksehno" ("'Don't Forget' Street"), which ran from January to December 1996. It was followed by "To Serial", which also ran for one year from September 1997 to June 1998. CyBC created the third weekdaily soap, "Anemi Tou Pathous" ("Passion Winds"), running from January 2000 to June 2004, which was replaced by "I Platia" ("The Square") from September 2004 to July 2006. "Epikindini Zoni" ran from 2009 to 2010, and was cancelled after 120 episodes. "Vimata Stin Ammo" made its debut in September 2010.
Sigma TV first commissioned the weekdaily comedic soap "Sto Para Pente", which aired from September 1998 to June 2004, and first held the distinction of being the longest weekday show in Cyprus television history, before it was surpassed by "Se Fonto Kokkino", which ran from September 2008 to July 2012. Other Sigma TV weekday shows include "Akti Oniron" (which ran from 1999 to 2001), "Vourate Geitonoi" (which ran from 2001 to 2005, and was the most successful weekdaily series, achieving ratings shares of up to 70% of all television households in the country), "Oi Takkoi" (which ran from 2002 to 2005), "S' Agapo" (which ran from 2001 to 2002), "Vasiliki" (which ran from 2005 to 2006), "Vendetta" (which ran from September 2005 to December 2006), "30 kai Kati" (which ran from 2006 to 2007) and "Mila Mou" (which ran from September 2007 to January 2009).
ANT1 Cyprus aired the soap "I Goitia Tis Amartias" in 2002, which was soon canceled. "Dikse Mou To Filo Sou" followed from 2006 to 2009, along with "Gia Tin Agapi Sou", which ran from 2008 to 2009 and itself was followed by "Panselinos", which has aired since 2009.
Weekly shows.
The longest-running weekly show on Cyprus television is "Istories Tou Horkou" ("Villages Stories", which premiered on CyBC in March 1996 and ran until its cancellation in June 2006; it was revived in September 2010 but was cancelled again in March 2011 due to very low ratings), followed by "Manolis Ke Katina" ("Manolis and Katina", which ran from 1995 to 2004). The most controversial of these series was "To Kafenio" ("The Coffee Shop"), which premiered on CyBC on 1993 as a weekly series, before moving to MEGA Channel Cyprus six years later in 1999 as a weekdaily show and then moved to ANT1 Cyprus on 2000, which canceled the show one year later. There were plans to move the show back to CyBC as a weekly series in 2001, with the original cast, however this plan was never realised. The most successful weekly shows in Cyprus currently are ANT1's "Eleni I Porni" ("Eleni, The Whore"), which premiered in October 2010 and CyBC's "Stin Akri Tu Paradisou" ("At The Heaven's Edge"), which premiered in 2007. The most successful weekdaily soap was "Aigia Fuxia", which aired on ANT1 Cyprus from 2008 to 2010.
Finland.
The only daily Finnish soap opera so far is "Salatut elämät" ("Secret Lives"), which has achieved popularity in Finland since its 1999 debut on MTV3. It focuses on the lives of people along the imaginary Pihlajakatu street in Helsinki. The show has also spawned several Internet spin-off series and a film based on the show that was released in 2012.
Other soap-like shows in Finland are YLE shows "Uusi päivä" (which has aired since 2009) and "Kotikatu" (which ran from 1995 to 2012), however these programs do not adhere to a five-episode-a-week schedule.
Internet and mobile soap opera.
With the advent of internet television and mobile phones, several soap operas have also been produced specifically for these platforms, including "", a spin-off of the established "EastEnders". For those produced only for the mobile phone, episodes may generally consist of about six or seven pictures and accompanying text.
On September 13, 2011, TG4 launched a new 10-part online series titled, "Na Rúin" (an Internet spin-off of "Ros na Rún"). The miniseries took on the theme of a mystery; the viewer had to read Rachel and Lorcán's blogs as well as watch video diaries detailing each character's thoughts to solve the mystery of missing teenage character Ciara.
Parodies.
In motion pictures, the movie "Soapdish", stars Sally Field as an aging soap opera actress on a show called "The Sun Also Sets," who pines over her own neuroses and misfortunes, such as her live-in boyfriend who leaves her to go back to his wife, and the incidents of backstabbing and scheming of other members of the cast, some of which are more interesting than the stories on the program.
On television, several soap opera parodies have been produced:
References.
</dl>

</doc>
<doc id="27020" url="http://en.wikipedia.org/wiki?curid=27020" title="History of South Korea">
History of South Korea

The history of South Korea formally begins with its establishment on 15 August 1948, although Syngman Rhee had officially declared independence two days prior.
In the aftermath of the Japanese occupation of Korea which ended with Japan's defeat in World War II in 1945, Korea was divided at the 38th parallel north in accordance with a United Nations arrangement, to be administered by the Soviet Union in the north and the United States in the south. The Soviets and Americans were unable to agree on the implementation of Joint Trusteeship over Korea. This led in 1948 to the establishment of two separate governments, each claiming to be the legitimate government of all of Korea. Eventually, following the Korean War, the two separate governments stabilized into the existing political entities of North and South Korea.
South Korea's subsequent history is marked by alternating periods of democratic and autocratic rule. Civilian governments are conventionally numbered from the First Republic of Syngman Rhee to the contemporary Sixth Republic. The First Republic, arguably democratic at its inception, became increasingly autocratic until its collapse in 1960. The Second Republic was strongly democratic, but was overthrown in less than a year and replaced by an autocratic military regime. The Third, Fourth, and Fifth Republics were nominally democratic, but are widely regarded as the continuation of military rule. With the Sixth Republic, the country has gradually stabilized into a liberal democracy.
Since its inception, South Korea has seen substantial development in education, economy, and culture. Since the 1960s, the country has developed from one of Asia's poorest to one of the world's wealthiest nations. Education, particularly at the tertiary level, has expanded dramatically. It is said to be one of the "Four Tigers" of rising Asian states along with Singapore, Taiwan and Hong Kong.
U.S. Military administration 1945–1948.
After Japan's surrender to the Allied Powers, division at the 38th parallel marked the beginning of Soviet and U.S. trusteeship over the North and South, respectively. This division was meant to be temporary and was first intended to return a unified Korea back to its people until the United States, United Kingdom, Soviet Union, and Republic of China could arrange a trusteeship administration. In February 1945, the issue of trusteeship for Korea was discussed at the Yalta Conference. U.S. forces landed at Incheon on September 8, 1945 and established a military government shortly thereafter. They were commanded by Lt. General John R. Hodge, who then took charge of the government. Faced with mounting popular discontent, in October 1945 Hodge established the Korean Advisory Council. A year later, an interim legislature and interim government were established, headed by Kim Kyu-shik and Syngman Rhee respectively. However, these interim bodies lacked any independent authority or de jure sovereignty, which was still held by the Provisional Government of the Republic of Korea based in China, but U.S. leaders chose to ignore its legitimacy, partly due to that it was communist-aligned.
The country in this period was plagued by political and economic chaos, which arose from a variety of causes. The aftereffects of the Japanese exploitation were still felt in the country, as in the North. In addition, the U.S. military was largely unprepared for the challenge of administering the country, arriving with no knowledge of the language, culture or political situation. Thus, many of their policies had unintended destabilizing effects. Waves of refugees from North Korea and returnees from abroad also helped to keep the country in turmoil.
In December 1945, a conference convened in Moscow to discuss the future of Korea. A 5-year trusteeship
was discussed, and a US-Soviet joint commission was established. The commission met intermittently in Seoul but deadlocked over the issue of establishing a national government. In September 1947, with no solution in sight, the United States submitted the Korean question to the UN General Assembly.
The resolution from the UN General Assembly called for a UN-supervised general election in Korea, but with the North rejecting this proposition, a general election for a Constitutional Assembly was held in the South only, in May 1948. A constitution was adopted, setting forth a presidential form of government and specifying a four-year term for the presidency. According to the provisions of the Constitution, an indirect presidential election was held in July. Syngman Rhee, as head of the new assembly, assumed the presidency and proclaimed the Republic of Korea (South Korea) on August 15, 1948.
First Republic 1948–1960.
On August 15, 1948, the Republic of Korea was formally established, with Syngman Rhee as the first president. With the establishment of Rhee's government, de jure sovereignty also passed into the new government. On September 9, 1948, a communist regime, the Democratic People's Republic of Korea (North Korea), was proclaimed under Kim Il-sung. However, on December 12, 1948, by its resolution 195 in the Third General Assembly, the United Nations recognized the Republic of Korea as the sole legal government of Korea.
In 1946, the North implemented land reforms by confiscating private property, Japanese and pro-Japanese owned facilities and factories, and placed them under state ownership. Demand for land reform in the South grew strong, and it was eventually enacted in June 1949. Koreans with large landholdings were obliged to divest most of their land. Approximately 40 percent of total farm households became small landowners. However, because preemptive rights were given to people who had ties with landowners before liberation, many pro-Japanese groups obtained or retained properties.
The country now divided, the relationship between the two Koreas turned more antagonistic as time passed. The Soviet forces having withdrawn in 1948, North Korea pressured the South to expel the United States forces, but Rhee sought to align his government strongly with America, and against both North Korea and Japan. Although talks towards normalization of relations with Japan took place, they achieved little. Meanwhile, the government took in vast sums of American aid, in amounts sometimes near the total size of the national budget. The nationalist government also continued many of the practices of the U.S. military government. In 1948, the Rhee government repressed military uprisings in Jeju, Suncheon and Yeosu.
The main policy of the First Republic of South Korea was anti-communism and "unification by expanding northward". The South's military was neither sufficiently equipped nor prepared, but the Rhee administration was determined to reunify Korea by military force with aid from the United States. However, in the second parliamentary elections held on May 30, 1950, the majority of seats went to independents who did not endorse this position, confirming the lack of support and the fragile state of the nation.
On June 25, 1950, North Korean forces invaded South Korea. Led by the U.S., a 16-member coalition undertook the first collective action under the United Nations Command (UNC) in defense of South Korea. Oscillating battle lines inflicted a high number of civilian casualties and wrought immense destruction. With the People's Republic of China's entry on behalf of North Korea in late 1950, the fighting came to a stalemate close to the original line of demarcation. Armistice negotiations, initiated in July 1951, finally concluded on July 27, 1953 at Panmunjeom, now in the Demilitarized Zone (DMZ). Following the armistice, the South Korean government returned to Seoul on the symbolic date of August 15, 1953.
After the armistice, South Korea experienced political turmoil under years of autocratic leadership of Syngman Rhee, which was ended by student revolt in 1960. Throughout his rule, Rhee sought to take additional steps to cement his control of government. These began in 1952, when the government was still based in Busan due to the ongoing war. In May of that year, Rhee pushed through constitutional amendments which made the presidency a directly-elected position. To do this, he declared martial law, arrested opposing members of parliament, demonstrators, and anti-government groups. Rhee was subsequently elected by a wide margin.
Rhee regained control of parliament in the 1954 elections, and thereupon pushed through an amendment to exempt himself from the eight-year term limit, and was once again re-elected in 1956. Soon after, Rhee's administration arrested members of the opposing party and executed the leader after accusing him of being a North Korean spy.
The administration became increasingly repressive while dominating the political arena, and in 1958, it sought to amend the National Security Law to tighten government control over all levels of administration, including the local units. These measures caused much outrage among the people, but despite public outcry, Rhee's administration rigged the March 15, 1960 presidential elections and won by a landslide.
On that election day, protests by students and citizens against the irregularities of the election burst out in the city of Masan. Initially these protests were quelled with force by local police, but when the body of a student was found floating in the harbor of Masan, the whole country was enraged and protests spread nationwide. On April 19, students from various universities and schools rallied and marched in protest in the Seoul streets, in what would be called the April Revolution. The government declared martial law, called in the army, and suppressed the crowds with open fire. Subsequent protests throughout the country shook the government, and after an escalated protest with university professors taking to the streets on April 25, Rhee submitted his official resignation on April 26 and fled into exhile.
Second Republic 1960–1961.
After the student revolution, power was briefly held by an interim administration under the foreign minister Heo Jeong. A new parliamentary election was held on July 29, 1960. The Democratic Party, which had been in the opposition during the First Republic, easily gained power and the Second Republic was established. The revised constitution dictated the Second Republic to take the form of a parliamentary cabinet system where the President took only a nominal role. This was the first and the only instance South Korea turned to a parliamentary cabinet system instead of a presidential system. The assembly elected Yun Bo-seon as President and Chang Myon as the prime minister and head of government in August, 1960.
The Second Republic saw the proliferation of political activity which had been repressed under the Rhee regime. Much of this activity was from leftist and student groups, which had been instrumental in the overthrow of the First Republic. Union membership and activity grew rapidly during the later months of 1960, including the Teachers' Union, Journalists' Union, and the Federation of Korean Trade Union. Around 2,000 demonstrations were held during the eight months of the Second Republic.
Under pressure from the left, the Chang government carried out a series of purges of military and police officials who had been involved in anti-democratic activities or corruption. A Special Law to this effect was passed on October 31, 1960. 40,000 people were placed under investigation; of these, more than 2,200 government officials and 4,000 police officers were purged. In addition, the government considered reducing the size of the army by 100,000, although this plan was shelved.
In economic terms as well, the government was faced with mounting instability. The government formulated a five-year economic development plan, although it was unable to act on it prior to being overthrown. The Second Republic saw the "hwan" lose half of its value against the dollar between fall 1960 and spring 1961.
Although the government had been established with support of the people, it had failed to implement effective reforms which brought about endless social unrest, political turmoil and ultimately, the 16 May coup d'état.
Military rule 1961–1963.
The May 16 coup, led by Major General Park Chung-hee on May 16, 1961, put an effective end to the Second Republic. Park was one of a group of military leaders who had been pushing for the de-politicization of the military. Dissatisfied with the cleanup measures undertaken by the Second Republic and convinced that the current disoriented state would collapse into communism, they chose to take matters into their own hands.
The National Assembly was dissolved and military officers replaced the civilian officials. In May 1961, the junta declared "Pledges of the Revolution": anticommunism was to be the nation's main policy; friendly relations would be strengthened with allies of the free world, notably the United States; all corruption and government misdeed would be disposed and "fresh and clean morality" would be introduced; the reconstruction of a self-reliant economy would be priority; the nation's ability would be nurtured to fight against communism and achieve reunification; and that government would be returned to a democratic civilian government within two years.
As a means to check the opposition, the military authority created the Korean Central Intelligence Agency (KCIA) in June 1961, with Kim Jong-pil, a relative of Park, as its first director. In December 1962, a referendum was held on returning to a presidential system of rule, which was allegedly passed with a 78% majority. Park and the other military leaders pledged not to run for office in the next elections. However, Park became presidential candidate of the new Democratic Republican Party (DRP), which consisted of mainly KCIA officials, ran for president and won the election of 1963 by a narrow margin.
Third Republic 1963–1972.
Park's administration started the Third Republic by announcing the Five Year Economic development Plan, an export-oriented industrialization policy. Top priority was placed on the growth of a self-reliant economy and modernization; "Development First, Unification Later" became the slogan of the times and the economy grew rapidly with vast improvement in industrial structure, especially in the basic and heavy chemical industries. Capital was needed for such development, so the Park regime used the influx of foreign aid from Japan and the United States to provide loans to export businesses, with preferential treatment in obtaining low-interest bank loans and tax benefits. Cooperating with the government, these businesses would later become the "chaebol".
Relations with Japan were normalized by the Korea-Japan treaty ratified in June 1965. This treaty brought Japanese funds in the form of loans and compensation for the damages suffered during the colonial era without an official apology from the Japanese government, sparking much protest across the nation.
The government also kept close ties with the United States, and continued to receive large amounts of aid. A status of forces agreement was concluded in 1966, clarifying the legal situation of the US forces stationed there. Soon thereafter, Korea joined the Vietnam War, eventually sending a total of 300,000 soldiers from 1964 to 1973 to fight alongside US troops and South Vietnamese Armed Forces.
Economic and technological growth during this period improved the standard for living, which expanded opportunities for education. Workers with higher education were absorbed by the rapidly growing industrial and commercial sectors, and urban population surged. Construction of the Gyeongbu Expressway was completed and linked Seoul to the nation's southeastern region and the port cities of Incheon and Busan. Despite the immense economic growth, however, the standard of living for city laborers and farmers was still low. Laborers were working with low wages to increase the price competitiveness for the export-oriented economy plan, and farmers were in near poverty as the government controlled prices. As the rural economy steadily lost ground and caused dissent among the farmers, however, the government decided to implement measures to increase farm productivity and income by instituting the Saemauel Movement ("New Village Movement") in 1971. The movement's goal was to improve the quality of rural life, modernize both rural and urban societies and narrow the income gap between them.
Park ran again in the election of 1967, taking 51.4% of the vote. At the time the presidency was constitutionally limited to two terms, but a constitutional amendment was forced through the National Assembly in 1969 to allow him to seek a third term. Major protests and demonstrations against the constitutional amendment broke out, with large support gaining for the opposition leader Kim Dae-jung, but Park was again re-elected in the 1971 presidential election.
Parliamentary elections followed shortly after the presidential election where the opposition party garnered most of the seats, giving them the power to pass constitutional amendments. Park, feeling threatened, declared a state of national emergency on December 6, 1971. In the midst of this domestic insecurity, the Nixon Doctrine had eased tensions among the world superpowers on the international scene, which caused a dilemma for Park, who had justified his regime based on the state policy of anti-communism. In a sudden gesture, the government proclaimed a joint communiqué for reunification with North Korea on July 4, 1972, and held Red Cross talks in Seoul and Pyongyang. However, there was no change in government policy regarding reunification, and on October 17, 1972, Park declared martial law, dissolving the National Assembly and suspending the constitution.
Fourth Republic 1972–1979.
The Fourth Republic began with the adoption of the Yushin Constitution on November 21, 1972. This new constitution gave Park effective control over the parliament and the possibility of permanent presidency. The president would be elected through indirect election by an elected body, and the term of presidency was extended to six years with no restrictions on reappointment. The legislature and judiciary were controlled by the government, and educational guidelines was under direct surveillance as well. Textbooks supporting the ideology of the military government were authorized by the government, diminishing the responsibilities of the Ministry of Education.
Despite social and political unrest, the economy continued to flourish under the authoritarian rule with the export-based industrialization policy. The first two five-year economic development plans were successful, and the 3rd and 4th five-year plans focused on expanding the heavy and chemical industries, raising the capability for steel production and oil refining. However, large conglomerate "chaebols" continuously received preferential treatment and came to dominate the domestic market. As most of the development had come from foreign capital, most of the profit went back to repaying the loans and interest.
Students and activists for democracy continued their demonstrations and protests for the abolition of the Yushin system and in the face of continuing popular unrest, Park's administration promulgated emergency decrees in 1974 and 1975, which led to the jailing of hundreds of dissidents. The protests grew larger and stronger, with politicians, intellectuals, religious leaders, laborers and farmers all joining in the movement for democracy. In 1978, Park was elected to another term by indirect election, which was met with more demonstrations and protests. The government retaliated by removing the opposition leader Kim Young-sam from the assembly and suppressing the activists with violent means. In 1979, mass anti-government demonstrations occurred nationwide, in the midst of this political turmoil, Park Chung-hee was assassinated by the director of the KCIA, Kim Jae-gyu, thus bringing the 18-year rule of military regime to an end.
Fifth Republic 1979–1987.
After the assassination of Park Chung-hee, prime minister Choi Kyu-hah took the president's role only to be usurped 6 days later by Major General Chun Doo-hwan's 1979 Coup d'état of December Twelfth. In May of the following year, a vocal civil society composed primarily of university students and labor unions led strong protests against authoritarian rule all over the country. Chun Doo-hwan declared martial law on May 17, 1980, and protests escalated. Political opponents Kim Dae-jung and Kim Jong-pil were arrested, and Kim Young-sam was confined to house arrest.
On May 18, 1980, a confrontation broke out in the city of Gwangju between protesting students of Chonnam National University and the armed forces dispatched by the Martial Law Command. The incident turned into a citywide protest that lasted nine days until May 27 and resulted in the Gwangju massacre. Immediate estimates of the civilian death toll ranged from a few dozen to 2000, with a later full investigation by the civilian government finding nearly 200 deaths and 850 injured. In June 1980, Chun ordered the National Assembly to be dissolved. He subsequently created the National Defense Emergency Policy Committee, and installed himself as a member. On 17 July, he resigned his position of KCIA Director, and then held only the position of committee member. In September 1980, President Choi Kyu-ha was forced to resign from president to give way to the new military leader, Chun
In September of that year, Chun was elected president by indirect election and inaugurated in March of the following year, officially starting the 5th Republic. A new Constitution was established with notable changes; maintaining the presidential system but limiting it to a single 7-year term, strengthening the authority of the National Assembly, and conferring the responsibilities of appointing judiciary to the Chief Justice of the Supreme Court. However, the system of indirect election of the president stayed and many military persons were appointed to highly ranked government positions, keeping the remnants of the Yushin era.
The government promised a new era of economic growth and democratic justice. Tight monetary laws and low interest rates contributed to price stability and helped the economy boom with notable growth in the electronics, semi-conductor, and automobile industries. The country opened up to foreign investments and GDP rose as Korean exports increased. This rapid economic growth, however, widened the gap between the rich and the poor, the urban and rural regions, and also exacerbated inter-regional conflicts. These dissensions, added to the hard-line measures taken against opposition to the government, fed intense rural and student movements, which had continued since the beginning of the republic.
In foreign policy, ties with Japan were strengthened by state visits by Chun to Japan and Japanese Prime Minister Nakasone Yasuhiro to Korea. U.S. President Ronald Reagan also paid a visit, and relations with the Soviet Union and China improved. The relationship with North Korea was strained when in 1983 a terrorist bomb attack in Burma killed 17 high-ranking officials attending memorial ceremonies and North Korea was alleged to be behind the attacks. However, in 1980 North Korea had submitted a "one nation, two system" reunification proposal which was met with a suggestion from the South to meet and prepare a unification constitution and government through a referendum. The humanitarian issue of reuniting separated families was dealt with first, and in September 1985, families from both sides of the border made cross visits to Seoul and Pyongyang in an historic event.
The government made many efforts for cultural development: the National Museum of Korea, Seoul Arts Center, and National Museum of Contemporary Art were all constructed during this time. The 1986 Asian Games were held successfully, and the bid for the 1988 Summer Olympics in Seoul was successful as well.
Despite economic growth and success in diplomatic relations, the government that gained power by coup d'etat was essentially a military regime and the public's support and trust in it was low when the promises for democratic reform never materialized. In the 1985 National Assembly elections, opposition parties won more votes than the government party, clearly indicating that the public wanted a change. Many started to sympathize with the protesting students. The Gwangju Massacre was never forgotten and in January 1987, when a protesting Seoul National University student died under police interrogation, public fury was immense. In April 1987, President Chun made a declaration that measures would be taken to protect the current constitution, instead of reforming it to allow for the direct election of the president. This announcement consolidated and strengthened the opposition; in June 1987, more than a million students and citizens participated in the nationwide anti-government protests of the June Democracy Movement.
On June 29, 1987, the government's presidential nominee Roh Tae-woo gave in to the demands and announced the Declaration of Political Reforms which called for the holding of direct presidential elections and restoration of civil rights. In October 1987 a revised Constitution was approved by a national referendum and direct elections for a new president were held in December, bringing the 5th Republic to a close.
Sixth Republic 1987–present.
The Sixth Republic was established in 1987 and remains the current republic of South Korea.
Roh Tae-woo, 1988–1993.
Roh Tae-woo became president for the 13th presidential term in the first direct presidential election in 16 years. Although Roh was from a military background and one of the leaders of Chun's coup d'etat, the inability of the opposition leaders Kim Dae Jung and Kim Young Sam to agree on a unified candidacy led to his being elected.
Roh was officially inaugurated in February 1988. The government set out to eliminate past vestiges of authoritarian rule, by revising laws and decrees to fit democratic provisions. Freedom of the press was expanded, university autonomy recognised, and restrictions on overseas travels were lifted. However, the growth of the economy had slowed down compared to the 80s, with strong labor unions and higher wages reducing the competitiveness of Korean products on the international market, resulting in stagnant exports, while commodity prices kept on rising. 
Shortly after Roh's inauguration, the Seoul Olympics took place, raising South Korea's international recognition and also greatly influencing foreign policy. Roh's government announced the official unification plan, "Nordpolitik", and established diplomatic ties with the Soviet Union, China, and countries in East Europe.
An historic event was held in 1990 when North Korea accepted the proposal for exchange between the two Koreas, resulting in high-level talks, and cultural and sports exchanges. In 1991, a joint communiqué on denuclearization was agreed upon, and the two Koreas simultaneously became members of the UN .
Kim Young-sam, 1993–1998.
Kim Young-sam was elected president in the 1992 elections after Roh's tenure. He was the country's first civilian president in 30 years and promised to build a "New Korea". The government set out to correct the mistakes of the previous administrations. Local government elections were held in 1995, and parliamentary elections in 1996. In a response to popular demand, former presidents Chun and Roh were both indicted on charges linked to bribery, illegal funds, and in the case of Chun, responsibility for the incident in Gwangju. They were tried and sentenced to prison in December, 1996.
Relations with the North improved and a summit meeting was planned, but postponed indefinitely with the death of Kim Il-sung. Tensions varied between the two Koreas thereafter, with cycles of small military skirmishes and apologies. The government also carried out substantial financial and economical reforms, joining the OECD in 1996, but encountered difficulties with political and financial scandals. The country also faced a variety of catastrophes: a train collision and a ship sinking in 1993, and the Seongsu Bridge and Sampoong Department Store collapses in 1994. These incidents, which claimed many lives, were a blow to the civilian government.
In 1997, the nation suffered a severe financial crisis, and the government approached the International Monetary Fund for relief funds. This was the limit to what the nation could bear and led to the opposition leader Kim Dae-jung winning the presidency in the same year.
Kim Dae-jung 1998–2003.
In February 1998, Kim Dae-jung was officially inaugurated. South Korea had maintained its commitment to democratize its political processes and this was the first transfer of the government between parties by peaceful means. Kim's government faced the daunting task of overcoming the economic crisis, but with the joint efforts of the government's aggressive pursuit of foreign investment, cooperation from the industrial sector, and the citizen's gold-collecting campaign, the country was able to come out of the crisis in a relatively short period of time.
Industrial reconstruction of the big conglomerate "chaebols" was pursued, a national pension system was established in 1998, educational reforms were carried out, government support for the IT field was increased, and notable cultural properties were registered as UNESCO Cultural Heritage sites. The 2002 FIFA World Cup, co-hosted with Japan, was a major cultural event where millions of supporters gather to cheer in public places.
In diplomacy, Kim Dae-jung pursued the "Sunshine Policy", a series of efforts to reconcile with North Korea. This culminated in reunions of the separated families of the Korean War and a summit talk with North Korean leader Kim Jong-il. For these efforts, Kim Dae-jung was awarded the Nobel Peace Prize in 2000. However, between a lack of peaceful cooperation from North Korea and the terrorist attacks on the United States on September 11, 2001, changing the view of the U.S. on North Korea, the efficacy of the Sunshine Policy was brought into question. With added allegations of corruption, support waned in the later years of the administration.
Roh Moo-hyun, 2003–2008.
Roh Moo-hyun was elected to the presidency in December 2002 by direct election. His victory came with much support from the younger generation and civic groups who had hopes of a participatory democracy, and Roh's administration consequently launched with the motto of "participation government". Unlike the previous governments, the administration decided to take a long-term view and execute market-based reforms at a gradual pace. This approach did not please the public, however, and by the end of 2003, approval ratings were falling.
The Roh administration succeeded in overcoming regionalism in South Korean politics, diluting the collusive ties between politics and business, empowering the civil society, settling the Korea-United States FTA issue, continuing summit talks with North Korea, and launching the high-speed train system, KTX. But despite a boom in the stock market, youth unemployment rates were high, real estate prices skyrocketed and the economy lagged.
In March 2004, the National Assembly voted to impeach Roh on charges of breach of election laws and corruption. This motion rallied his supporters and affected the outcome of the parliamentary election held in April, with the ruling party becoming the majority. Roh was reinstated in May by the Constitutional Court, who had overturned the verdict. However, the ruling party then lost its majority in by-elections in 2005, as discontinued reform plans, continual labor unrest, Roh's personal feuds with the media, and diplomatic friction with the United States and Japan caused criticism of the government's competence on political and socioeconomic issues and on foreign affairs.
In April 2009, Roh Moo-hyun and his family members were investigated for bribery and corruption, Roh denied the charges.
On 23 May 2009, Roh committed suicide by jumping into a ravine.
Lee Myung-bak, 2008–2013.
Roh's successor, Lee Myung-bak, was inaugurated in February 2008. Stating "creative pragmatism" as a guiding principle, Lee's administration set out to revitalize the flagging economy, re-energize diplomatic ties, stabilize social welfare, and meet the challenges of globalization. In April 2008, the ruling party secured a majority in the National Assembly elections. Also that month, summit talks with the United States addressed the Korea-US Freed Trade Agreement and helped ease tensions between the two countries caused by the previous administrations. Lee agreed to lift the ban on US beef imports, which caused massive protests and demonstrations in the months that followed, as paranoia of potential mad cow disease gripped the country.
Many issues plagued the government in the beginning of the administration: controversies regarding the appointment of high-ranking government officials, rampant political conflicts, accusations of oppression of media and strained diplomatic relationships with North Korea and Japan. The economy was affected by the global recession as the worst economic crisis since 1997 hit the country. The Lee administration tackled these issues by actively issuing statements, reshuffling the cabinet, and implementing administrative and industrial reforms.
After regulatory and economic reforms, the economy bounced back, with the country's economy marking growth and apparently recovering from the global recession. The administration also pursued improved diplomatic relations by holding summit talks with the United States, China and Japan, and participating in the ASEAN-ROK Commemorative Summit to strengthen ties with other Asian countries. The 2010 G20 summit was held in Seoul, where issues regarding the global economic crisis were discussed.
Park Geun-hye, 2013–current.
Park Geun-hye was inaugurated in February 2013. She is the eleventh and current President of South Korea. She is the first woman to be elected as the South Korean president and is serving the 18th presidential term. She also is the first woman head of state in the modern history of Northeast Asia.

</doc>
<doc id="27058" url="http://en.wikipedia.org/wiki?curid=27058" title="Steel">
Steel

Steels are alloys of iron and carbon, widely used in construction and other applications because of their high tensile strengths and low costs. Carbon, other elements, and inclusions within iron act as hardening agents that prevent the movement of dislocations that otherwise occur in the crystal lattices of iron atoms.
The carbon in typical steel alloys may contribute up to 2.1% of its weight. Varying the amount of alloying elements, their formation in the steel either as solute elements, or as precipitated phases, retards the movement of those dislocations that make iron comparatively ductile and weak, and thus controls qualities such as the hardness, ductility, and tensile strength of the resulting steel. Steel's strength compared to pure iron is only possible at the expense of ductility, of which iron has an excess.
Although steel had been produced in bloomery furnaces for thousands of years, steel's use expanded extensively after more efficient production methods were devised in the 17th century for blister steel and then crucible steel. With the invention of the Bessemer process in the mid-19th century, a new era of mass-produced steel began. This was followed by Siemens-Martin process and then Gilchrist-Thomas process that refined the quality of steel. With their introductions, mild steel replaced wrought iron.
Further refinements in the process, such as basic oxygen steelmaking (BOS), largely replaced earlier methods by further lowering the cost of production and increasing the quality of the metal. Today, steel is one of the most common materials in the world, with more than 1.3 billion tons being produced annually. It is a major component in buildings, infrastructure, tools, ships, automobiles, machines, appliances, and weapons. Modern steel is generally identified by various grades defined by assorted standards organizations.
Definitions and related materials.
The carbon content of steel is between 0.002% and 2.1% by weight for plain iron-carbon alloys. These values vary depending on alloying elements such as manganese, chromium, nickel, iron, tungsten, carbon and so on. Basically, steel is an iron-carbon alloy that does not undergo eutectic reaction. In contrast, cast iron does undergo eutectic reaction. Too little carbon content leaves (pure) iron quite soft, ductile, and weak. Carbon contents higher than those of steel make an alloy commonly called pig iron that is brittle and not malleable. Alloy steel is steel to which alloying elements have been intentionally added to modify the characteristics of steel. Common alloying elements include: manganese, nickel, chromium, molybdenum, boron, titanium, vanadium, and niobium. Additional elements may be present in steel: phosphorus, sulfur, silicon, and traces of oxygen, nitrogen, and copper.
Alloys with a higher than 2.1% carbon content, depending on other element content and possibly on processing, are known as cast iron. Cast iron is not malleable even when hot, but it can be formed by casting as it has a lower melting point than steel and good castability properties. Steel is also distinguishable from wrought iron (now largely obsolete), which may contain a small amount of carbon but large amounts of slag. Note that the percentages of carbon and other elements quoted are on a weight basis.
Material properties.
Iron is commonly found in the Earth's crust in the form of an ore, usually an iron oxide, such as magnetite, hematite etc. Iron is extracted from iron ore by removing the oxygen through combination with a preferred chemical partner such as carbon that is lost to the atmosphere as carbon dioxide. This process, known as smelting, was first applied to metals with lower melting points, such as tin, which melts at approximately 250 C and copper, which melts at approximately 1100 C. In comparison, cast iron melts at approximately 1375 C. Small quantities of iron were smelted in ancient times, in the solid state, by heating the ore buried in a charcoal fire and welding the metal together with a hammer, squeezing out the impurities. With care, the carbon content could be controlled by moving it around in the fire.
All of these temperatures could be reached with ancient methods that have been used since the Bronze Age. Since the oxidation rate of iron increases rapidly beyond 800 C, it is important that smelting take place in a low-oxygen environment. Unlike copper and tin, liquid or solid iron dissolves carbon quite readily. Smelting results in an alloy (pig iron) that contains too much carbon to be called steel. The excess carbon and other impurities are removed in a subsequent step.
Other materials are often added to the iron/carbon mixture to produce steel with desired properties. Nickel and manganese in steel add to its tensile strength and make the austenite form of the iron-carbon solution more stable, chromium increases hardness and melting temperature, and vanadium also increases hardness while making it less prone to metal fatigue.
To inhibit corrosion, at least 11% chromium is added to steel so that a hard oxide forms on the metal surface; this is known as stainless steel. Tungsten interferes with the formation of cementite, allowing martensite to preferentially form at slower quench rates, resulting in high speed steel. On the other hand, sulfur, nitrogen, and phosphorus make steel more brittle, so these commonly found elements must be removed from the steel melt during processing.
The density of steel varies based on the alloying constituents but usually ranges between 7750 and, or 7.75 and.
Even in a narrow range of concentrations of mixtures of carbon and iron that make a steel, a number of different metallurgical structures, with very different properties can form. Understanding such properties is essential to making quality steel. At room temperature, the most stable form of pure iron is the body-centered cubic (BCC) structure called ferrite or α-iron. It is a fairly soft metal that can dissolve only a small concentration of carbon, no more than 0.005% at 0 C and 0.021 wt% at 723 C. At 910 °C pure iron transforms into a face-centered cubic (FCC) structure, called austenite or γ-iron. The FCC structure of austenite can dissolve considerably more carbon, as much as 2.1% (38 times that of ferrite) carbon at 1148 C, which reflects the upper carbon content of steel, beyond which is cast iron.
When steels with less than 0.8% carbon (known as a hypoeutectoid steel), are cooled, the austenitic phase (FCC) of the mixture attempts to revert to the ferrite phase (BCC). The carbon no longer fits within the FCC structure, resulting in an excess of carbon. One way for carbon to leave the austenite is for it to precipitate out of solution as cementite, leaving behind a surrounding phase of BCC iron that is low enough in carbon to take the form of ferrite, resulting in a ferrite matrix with cementite inclusions. Cementite is a hard and brittle intermetallic compound with the chemical formula of Fe3C. At the eutectoid, 0.8% carbon, the cooled structure takes the form of pearlite, named for its resemblance to mother of pearl. On a larger scale, it appears as a lamellar structure of ferrite and cementite. For steels that have more than 0.8% carbon, the cooled structure takes the form of pearlite and cementite.
Perhaps the most important polymorphic form of steel is martensite, a metastable phase that is significantly stronger than other steel phases. When the steel is in an austenitic phase and then quenched rapidly, it forms into martensite, as the atoms "freeze" in place when the cell structure changes from FCC to a distorted form of BCC as the atoms do not have time enough to migrate and form the cementite compound. Depending on the carbon content, the martensitic phase takes different forms. Below approximately 0.2% carbon, it takes an α ferrite BCC crystal form, but at higher carbon content it takes a body-centered tetragonal (BCT) structure. There is no thermal activation energy for the transformation from austenite to martensite. Moreover, there is no compositional change so the atoms generally retain their same neighbors.
Martensite has a lower density than does austenite, so that the transformation between them results in a change of volume. In this case, expansion occurs. Internal stresses from this expansion generally take the form of compression on the crystals of martensite and tension on the remaining ferrite, with a fair amount of shear on both constituents. If quenching is done improperly, the internal stresses can cause a part to shatter as it cools. At the very least, they cause internal work hardening and other microscopic imperfections. It is common for quench cracks to form when steel is water quenched, although they may not always be visible.
Heat treatment.
There are many types of heat treating processes available to steel. The most common are annealing, quenching, and tempering. Annealing is the process of heating the steel to a sufficiently high temperature to soften it. This process goes through three phases: recovery, recrystallization, and grain growth. The temperature required to anneal steel depends on the type of annealing to be achieved and the constituents of the alloy.
Quenching and tempering first involves heating the steel to the austenite phase then quenching it in water or oil. This rapid cooling results in a hard but brittle martensitic structure. The steel is then tempered, which is just a specialized type of annealing, to reduce brittleness. In this application the annealing (tempering) process transforms some of the martensite into cementite, or spheroidite and hence reduces the internal stresses and defects. The result is a more ductile and fracture-resistant steel.
Steel production.
When iron is smelted from its ore, it contains more carbon than is desirable. To become steel, it must be reprocessed to reduce the carbon to the correct amount, at which point other elements can be added. In modern facilities, this liquid is then continuously cast into long slabs or cast into ingots. Approximately 96% of steel is continuously cast, while only 4% is produced as ingots.
The ingots are then heated in a soaking pit and hot rolled into slabs, blooms, or billets. Slabs are hot or cold rolled into sheet metal or plates. Billets are hot or cold rolled into bars, rods, and wire. Blooms are hot or cold rolled into structural steel, such as I-beams and rails. In modern steel mills these processes often occur in one assembly line, with ore coming in and finished steel coming out. Sometimes after a steel's final rolling it is heat treated for strength, however this is relatively rare.
History of steelmaking.
Ancient steel.
Steel was known in antiquity, and may have been produced by managing bloomeries and crucibles, or iron-smelting facilities, in which they contained carbon.
The earliest known production of steel are pieces of ironware excavated from an archaeological site in Anatolia (Kaman-Kalehoyuk) and are nearly 4,000 years old, dating from 1800 BC. Horace identifies steel weapons like the "falcata" in the Iberian Peninsula, while Noric steel was used by the Roman military.
South Indian and Mediterranean sources including Alexander the Great (3rd c. BC) recount the presentation and export to the Greeks of 100 talents of South Indian steel. The reputation of "Seric iron" of South India (wootz steel) amongst the Greeks, Romans, Egyptians, East Africans, Chinese and the Middle East grew considerably, a high quality high carbon iron and steel imported from Tamil people of the dynasty Chera. Metal production sites in Sri Lanka utilized these novel techniques using unique wind furnaces driven by the monsoon winds, capable of producing high-carbon steel, as well as imported artefacts of ancient iron and steel from Kodumanal. Large-scale Wootz steel production in Tamilakam using crucibles they invented and carbon sources such as the plant Avāram occurred by the sixth century BC, the pioneering precursor to modern steel production and metallurgy.
Steel was produced in large quantities in Sparta around 650 BC.
The Chinese of the Warring States period (403–221 BC) had quench-hardened steel, while Chinese of the Han dynasty (202 BC – 220 AD) created steel by melting together wrought iron with cast iron, gaining an ultimate product of a carbon-intermediate steel by the 1st century AD. The Haya people of East Africa invented a type of furnace they used to make carbon steel at 1802 C nearly 2,000 years ago. East African steel has been suggested by Richard Hooker to date back to 1400 BC.
Wootz steel and Damascus steel.
Evidence of the earliest production of high carbon steel in the Indian Subcontinent are found in Kodumanal in Tamil Nadu area, Golconda in Andhra Pradesh area and Karnataka, and in Samanalawewa areas of Sri Lanka. This came to be known as Wootz steel, produced in South India by about sixth century BC and exported globally. The steel technology existed prior to 326 BC in the region as they are mentioned in literature of Sangam Tamil, Arabic and Latin as the finest steel in the world exported to the Romans, Egyptian, Chinese and Arabs worlds at that time - what they called "Seric Iron". A 200 BC Tamil trade guild in Tissamaharama, in the South East of Sri Lanka, brought with them some of the oldest iron and steel artefacts and production processes to the island from the classical period. The Chinese and locals in Anuradhapura, Sri Lanka had also adopted the production methods of creating Wootz steel from the Chera Dynasty Tamils of South India by the 5th century AD. In Sri Lanka, this early steel-making method employed a unique wind furnace, driven by the monsoon winds, capable of producing high-carbon steel. Since the technology was acquired from the Tamilians from South India, the origin of steel technology in India can be conservatively estimated at 400–500 BC.
Wootz, also known as Damascus steel, is famous for its durability and ability to hold an edge. It was originally created from a number of different materials including various trace elements, apparently ultimately from the writings of Zosimos of Panopolis. However, the steel was an old technology in India when King Porus presented a steel sword to the Emperor Alexander in 326 BC. It was essentially a complicated alloy with iron as its main component. Recent studies have suggested that carbon nanotubes were included in its structure, which might explain some of its legendary qualities, though given the technology of that time, such qualities were produced by chance rather than by design. Natural wind was used where the soil containing iron was heated by the use of wood. The ancient Sinhalese managed to extract a ton of steel for every 2 tons of soil, a remarkable feat at the time. One such furnace was found in Samanalawewa and archaeologists were able to produce steel as the ancients did.
Crucible steel, formed by slowly heating and cooling pure iron and carbon (typically in the form of charcoal) in a crucible, was produced in Merv by the 9th to 10th century AD. In the 11th century, there is evidence of the production of steel in Song China using two techniques: a "berganesque" method that produced inferior, inhomogeneous, steel, and a precursor to the modern Bessemer process that used partial decarbonization via repeated forging under a cold blast.
Modern steelmaking.
Since the 17th century the first step in European steel production has been the smelting of iron ore into pig iron in a blast furnace. Originally employing charcoal, modern methods use coke, which has proven more economical.
Processes starting from bar iron.
In these processes pig iron was "fined" in a finery forge to produce bar iron, which was then used in steel-making.
The production of steel by the cementation process was described in a treatise published in Prague in 1574 and was in use in Nuremberg from 1601. A similar process for case hardening armour and files was described in a book published in Naples in 1589. The process was introduced to England in about 1614 and used to produce such steel by Sir Basil Brooke at Coalbrookdale during the 1610s.
The raw material for this process were bars of iron. During the 17th century it was realized that the best steel came from oregrounds iron of a region north of Stockholm, Sweden. This was still the usual raw material source in the 19th century, almost as long as the process was used.
Crucible steel is steel that has been melted in a crucible rather than having been forged, with the result that it is more homogeneous. Most previous furnaces could not reach high enough temperatures to melt the steel. The early modern crucible steel industry resulted from the invention of Benjamin Huntsman in the 1740s. Blister steel (made as above) was melted in a crucible or in a furnace, and cast (usually) into ingots.
Processes starting from pig iron.
The modern era in steelmaking began with the introduction of Henry Bessemer's Bessemer process in 1855, the raw material for which was pig iron. His method let him produce steel in large quantities cheaply, thus mild steel came to be used for most purposes for which wrought iron was formerly used. The Gilchrist-Thomas process (or "basic Bessemer process") was an improvement to the Bessemer process, made by lining the converter with a basic material to remove phosphorus.
Another 19th-century steelmaking process was the Siemens-Martin process, which complemented the Bessemer process. It consisted of co-melting bar iron (or steel scrap) with pig iron.
These methods of steel production were rendered obsolete by the Linz-Donawitz process of basic oxygen steelmaking (BOS), developed in the 1950s, and other oxygen steel making methods. Basic oxygen steelmaking is superior to previous steelmaking methods because the oxygen pumped into the furnace limits impurities that previously had entered from the air used. Today, electric arc furnaces (EAF) are a common method of reprocessing scrap metal to create new steel. They can also be used for converting pig iron to steel, but they use a lot of electrical energy (about 440 kWh per metric ton), and are thus generally only economical when there is a plentiful supply of cheap electricity.
Steel industry.
It is common today to talk about "the iron and steel industry" as if it were a single entity, but historically they were separate products. The steel industry is often considered an indicator of economic progress, because of the critical role played by steel in infrastructural and overall economic development.
In 1980, there were more than 500,000 U.S. steelworkers. By 2000, the number of steelworkers fell to 224,000.
The economic boom in China and India has caused a massive increase in the demand for steel in recent years. Between 2000 and 2005, world steel demand increased by 6%. Since 2000, several Indian and Chinese steel firms have risen to prominence like Tata Steel (which bought Corus Group in 2007), Shanghai Baosteel Group Corporation and Shagang Group. ArcelorMittal is however the world's largest steel producer.
In 2005, the British Geological Survey stated China was the top steel producer with about one-third of the world share; Japan, Russia, and the US followed respectively.
In 2008, steel began trading as a commodity on the London Metal Exchange. At the end of 2008, the steel industry faced a sharp downturn that led to many cut-backs.
The world steel industry peaked in 2007. That year, ThyssenKrupp spent $12 billion to build the two most modern mills in the world, in Calvert, Alabama and Sepetiba, Rio de Janeiro, Brazil. The worldwide Great Recession starting in 2008, however, sharply lowered demand and new construction, and so prices fell. ThyssenKrupp lost $11 billion on its two new plants, which sold steel below the cost of production. Finally in 2013, ThyssenKrupp offered the plants for sale at under $4 billion.
Recycling.
Steel is one of the world's most-recycled materials, with a recycling rate of over 60% globally; in the United States alone, over 82,000,000 metric ton was recycled in the year 2008, for an overall recycling rate of 83%.
Contemporary steel.
Carbon steels.
Modern steels are made with varying combinations of alloy metals to fulfill many purposes. Carbon steel, composed simply of iron and carbon, accounts for 90% of steel production. Low alloy steel is alloyed with other elements, usually molybdenum, manganese, chromium, or nickel, in amounts of up to 10% by weight to improve the hardenability of thick sections. High strength low alloy steel has small additions (usually < 2% by weight) of other elements, typically 1.5% manganese, to provide additional strength for a modest price increase.
Recent Corporate Average Fuel Economy (CAFE) regulations have given rise to a new variety of steel known as Advanced High Strength Steel (AHSS). This material is both strong and ductile so that vehicle structures can maintain their current safety levels while using less material. There are several commercially available grades of AHSS, such as dual-phase steel, which is heat treated to contain both a ferritic and martensitic microstructure to produce a formable, high strength steel. Transformation Induced Plasticity (TRIP) steel involves special alloying and heat treatments to stabilize amounts of austenite at room temperature in normally austenite-free low-alloy ferritic steels. By applying strain, the austenite undergoes a phase transition to martensite without the addition of heat. Twinning Induced Plasticity (TWIP) steel uses a specific type of strain to increase the effectiveness of work hardening on the alloy.
Carbon Steels are often galvanized, through hot-dip or electroplating in zinc for protection against rust.
Alloy steels.
Stainless steels contain a minimum of 11% chromium, often combined with nickel, to resist corrosion. Some stainless steels, such as the ferritic stainless steels are magnetic, while others, such as the austenitic, are nonmagnetic. Corrosion-resistant steels are abbreviated as CRES.
Some more modern steels include tool steels, which are alloyed with large amounts of tungsten and cobalt or other elements to maximize solution hardening. This also allows the use of precipitation hardening and improves the alloy's temperature resistance. Tool steel is generally used in axes, drills, and other devices that need a sharp, long-lasting cutting edge. Other special-purpose alloys include weathering steels such as Cor-ten, which weather by acquiring a stable, rusted surface, and so can be used un-painted. Maraging steel is alloyed with nickel and other elements, but unlike most steel contains little carbon 0.01%). This creates a very strong but still malleable steel.
Eglin steel uses a combination of over a dozen different elements in varying amounts to create a relatively low-cost steel for use in bunker buster weapons. Hadfield steel (after Sir Robert Hadfield) or manganese steel contains 12–14% manganese which when abraded strain hardens to form an incredibly hard skin which resists wearing. Examples include tank tracks, bulldozer blade edges and cutting blades on the jaws of life.
In 2015 a breakthrough in creating a strong light aluminium steel alloy which might be suitable in applications such as aircraft was announced by researchers at Pohang University of Science and Technology. Adding small amounts of nickel was found to result in precipitation as nano particles of brittle B2 intermetallic compounds which had previously resulted in weakness. The result was a cheap strong light steel alloy which is slated for trial production at industrial scale by POSCO, a Korean steelmaker.
Standards.
Most of the more commonly used steel alloys are categorized into various grades by standards organizations. For example, the Society of Automotive Engineers has a series of grades defining many types of steel. The American Society for Testing and Materials has a separate set of standards, which define alloys such as A36 steel, the most commonly used structural steel in the United States.
Uses.
Iron and steel are used widely in the construction of roads, railways, other infrastructure, appliances, and buildings. Most large modern structures, such as stadiums and skyscrapers, bridges, and airports, are supported by a steel skeleton. Even those with a concrete structure employ steel for reinforcing. In addition, it sees widespread use in major appliances and cars. Despite growth in usage of aluminium, it is still the main material for car bodies. Steel is used in a variety of other construction materials, such as bolts, nails, and screws and other household products and cooking utensils.
Other common applications include shipbuilding, pipelines, mining, offshore construction, aerospace, white goods (e.g. washing machines), heavy equipment such as bulldozers, office furniture, steel wool, tools, and armour in the form of personal vests or vehicle armour (better known as rolled homogeneous armour in this role).
Historical.
Before the introduction of the Bessemer process and other modern production techniques, steel was expensive and was only used where no cheaper alternative existed, particularly for the cutting edge of knives, razors, swords, and other items where a hard, sharp edge was needed. It was also used for springs, including those used in clocks and watches.
With the advent of speedier and thriftier production methods, steel has become easier to obtain and much cheaper. It has replaced wrought iron for a multitude of purposes. However, the availability of plastics in the latter part of the 20th century allowed these materials to replace steel in some applications due to their lower fabrication cost and weight. Carbon fiber is replacing steel in some cost insensitive applications such as aircraft, sports equipment and high end automobiles.
Low-background steel.
Steel manufactured after World War II became contaminated with radionuclides due to nuclear weapons testing. Low-background steel, steel manufactured prior to 1945, is used for certain radiation-sensitive applications such as Geiger counters and radiation shielding.

</doc>
<doc id="27068" url="http://en.wikipedia.org/wiki?curid=27068" title="Sahara Desert (ecoregion)">
Sahara Desert (ecoregion)

The Sahara Desert ecoregion, as defined by the World Wide Fund for Nature (WWF), includes the hyper-arid center of the Sahara, between 18° and 30° N. It is one of several desert and xeric shrubland ecoregions that cover the northern portion of the African continent.
Setting.
The Sahara Desert is the world's largest hot desert, located in North Africa. It stretches from the Red Sea to the Atlantic Ocean. The vast desert encompasses several ecologically distinct regions. The Sahara Desert ecoregion covers an area of 4,619,260 km2 in the hot, hyper-arid center of the Sahara, surrounded on the north, south, east, and west by desert ecoregions with higher rainfall and more vegetation.
The North Saharan steppe and woodlands ecoregion lies to the north and west, bordering the Mediterranean climate regions of Africa's Mediterranean and North Atlantic coasts. The North Saharan steppe and woodlands receives more regular winter rainfall than the Sahara Desert ecoregion. The South Saharan steppe and woodlands ecoregion lies to the south, between the Sahara Desert ecoregion and the Sahel grasslands. The South Saharan steppe and woodlands receives most of its annual rainfall during the summer. The Red Sea coastal desert lies in the coastal strip between the Sahara Desert ecoregion and the Red Sea.
Some mountain ranges rise up from the desert and receive more rainfall and cooler temperatures. These Saharan mountains are home to two distinct ecoregions; the West Saharan montane xeric woodlands in the Ahaggar, Tassili n'Ajjer, Aïr, and other ranges in the western and central Sahara Desert, and the Tibesti-Jebel Uweinat montane xeric woodlands in the Tibesti and Jebel Uweinat of the eastern Sahara.
The surface of the desert ranges from large areas of sand dunes (erg), to stone plateaus (hamadas), gravel plains (reg), dry valleys (wadis), and salt flats. The only permanent river that crosses the ecoregion is the Nile River, which originates in central Africa and empties northwards into the Mediterranean Sea. Some areas encompass vast underground aquifers resulting in oases, while other regions severely lack water reserves.
Climate.
The Sahara Desert features a hot desert climate (Köppen climate classification "BWh"). The Sahara Desert is one of the driest and hottest regions of the world, with a mean temperature sometimes over 30 °C (86 °F) and the averages high temperatures in summer are over 40 °C (104 °F) and can even soar to 47 °C (116.6 °F) and often for over 3 months. In desert rocky mountains such as the Tibesti in Chad or the Hoggar in Algeria, averages highs in summer are slightly moderated by the high elevation and are between 35 °C (95 °F) and 42 °C (107.6 °F) towards 1,000 m (3,280 ft) or 1,500 m (4,921 ft) height. Daily variations may also be extreme: a swing from 37.5 to has been observed but are rather around 15 °C (59 °F) and 20 °C (68 °F). Precipitation in the Sahara Desert is extremely low and very scarce Bybys kiausai the year as the whole desert generally receives less than 100 mm (3,93 in) of rain per year except on the northernmost and southernmost edge as well as in the highest desert mountains and more than half of the desert area is hyper-arid and virtually rainless, with an average annual precipitation below 50 mm (1,97 in) and many consecutive years may pass without any rainfall in hyper-arid places. The south of the Sahara Desert, along the boundary with the hot semi-arid climate ("BSh") of the Sahel, receives most of his annual rainfall during the highest-sun months (summer) when the Inter-Tropical Convergence Zone moves up from the south while. Wind and sand storms occur as soon as early spring. Local inhabitants protect themselves from the heat and the sun as well as the dry air, the high diurnal temperature ranges and the sometimes dusty or sandy winds by covering their heads, such as the cheche worn by Tuareg.
History and conservation.
The Sahara was one of the first regions of Africa to be farmed. Some 5,000 years ago, the area was not so arid and the vegetation might have been closer to a savanna. Previous fauna may be recognised in stone carvings. However, desertification set in around 3000 BCE, and the area became much like it is today.
The Sahara is largely undisturbed. The most degradation is found in areas where there is water, such as aquifer oases or along the desert margins where some rain usually falls most years. In these areas, animals such as addaxes, scimitar-horned oryxes, and bustards are over-hunted for their meat. Only one area of conservation is recorded in the Sahara: the Zellaf Nature Reserve in Libya.

</doc>
<doc id="27081" url="http://en.wikipedia.org/wiki?curid=27081" title="Cardassian">
Cardassian

The Cardassians are an extraterrestrial species in the "Star Trek" science fiction franchise. First introduced in the 1991 ' episode, "", the species originated on the fictional Alpha Quadrant planet, Cardassia Prime. Cardassians were the dominant species in an interstellar empire known as the Cardassian Union during the 24th century, although they are not confirmed to have ruled any other species aside from, for fifty years, the Bajorans. The Cardassians later played a key role in the storyline of the series, ', as allies of the Dominion in the Dominion War. Several Cardassian characters, including Elim Garak and Gul Dukat, are prominently featured.
The Cardassians were developed by the writers of the Star Trek TV series "The Next Generation" to provide an enemy race with whom the protagonists could interact, unlike with the Borg, with whom such interpersonal drama was difficult due to their lack of personality and individualism.
Appearance.
Cardassians are a humanoid race, with light grey skin. Their faces have small ridges on their sides, which converge to a characteristic crest shape on their foreheads. This crest has led to the derogatory nickname "spoonheads" used by other races.
Obsidian Order.
The Obsidian Order is a Cardassian intelligence organization in the Star Trek universe. Security Chief Odo of Deep Space Nine remarked that it was one of the most brutally efficient organizations in the galaxy, being even more ruthless than the Romulan Tal Shiar. The Order kept close tabs on all Cardassian citizens to ensure loyalty, and was greatly feared. It was said that the average Cardassian could not sit down to dinner without the contents of the meal being noted and logged by the Order. Odo also noted that the Order caused people to disappear for even less than eating something of which the Order did not approve, although this statement may have just been an exaggeration for effect. The Obsidian Order's agent training program is so advanced that they are made immune to most forms of interrogation, including Vulcan mind melds.
The Obsidian Order frequently clashed with Central Command (the Cardassian military), partly because even the highest-ranking Command officers are not immune from Order inquiries. Elim Garak was a member of the Order, before being exiled from Cardassia to Deep Space Nine by his father, . Tain had retired for a time, the only director to ever live long enough to do so. Garak became an ally of the Federation who used his knowledge to aid them in the war against the Dominion.
In 2371, under the leadership of Tain who believed that the Central Command was being too complacent about the threat of the Dominion, the Obsidian Order and their Romulan equivalent, the Tal Shiar, allied in an attempt to destroy the Dominion. To this end, the Order began stockpiling a fleet of ships, albeit illegally without the approval or knowledge of the Central Command: according to the Cardassian governmental charter, the Order is expressly forbidden from developing or possessing military equipment of any kind, which includes warships and possibly starships in general (""). (Their lack of starships was noted when, in one DS9 episode, a high-ranking official from the Order had to "hitch a ride" with a Cardassian warship for transportation.) The plan, originated by Tain, involved a fleet of cloaked Romulan and Cardassian vessels traversing into the heart of Dominion territory in the Gamma Quadrant where they would annihilate the homeworld of the Founders.
The Founders soon learned of the plan, via a Changeling who impersonated Colonel Lovok of the Tal Shiar, and saw it as an opportunity to eliminate the two dangerous organizations. When the Romulan/Cardassian fleet arrived at the Founders' homeworld, they bombarded it, only to realize that the planet was deserted except for a token beacon. Moments later, the Dominion sprang the trap they arranged and a fleet of 150 Jem'Hadar fighters emerged from hiding in a nearby nebula and proceeded to wipe out the fleet. At least a few Romulan and Cardassian officers survived to be taken prisoner. Tain himself survived for two years in a Dominion internment camp before dying of heart failure.
The disastrous results of this attack crippled the Tal Shiar but more importantly (and perhaps through fear that the Order had clearly become too independent) it led to the downfall of the Order. The elimination of the Order is thought to have contributed to a political shakeup that led to the renewed empowerment of the civilian Detapa Council which proceeded to overthrow Central Command. This in turn paved the way for invasion of Cardassia by the Klingon Empire and eventual Dominion membership. After Cardassia joined the Dominion, an organization similar to the Obsidian Order was formed, which was called the Cardassian Intelligence Bureau.
Military ranks.
Cardassian military ranks are similar to those used by the United Federation of Planets, but with some key differences. For example a Legate is similar to an Admiral, but with considerably more political sway.
The Cardassian Central Command uses a system of hierarchical ranks, which is the same for all branches of the service. A garresh is the lowest-ranked soldier, the rank where all new recruits start. Garresh make up the vast bulk of the military. They are individually ranked on a five-number scale. The lowest commissioned rank is that of gil (sometimes seen as kel), followed by glinn, dalin, dal and gul.
Officers must generally hold a rank of at least glinn to be given command of a department on board a starship or within a unit. Larger vessels and units require dalin or dal level officers. Guls are the rough equivalent of Starfleet captains. They are the majority of the commanding officers in Central Command, controlling starships and bases, and serving as prefects and planetary governors throughout the client worlds of the Union. Many guls are quite influential, building up extensive vesala networks. Jaguls and Legates are the equivalent of Starfleet admirals, commanding entire Battalions and Orders.
Technology.
Known Cardassian starships include the "Galor"-class warship, a medium-sized cruiser which, throughout "The Next Generation", was the most powerful vessel in Cardassian service. Rick Sternbach designed the "Galor" class to be reminiscent of an ankh, an inspiration chosen because the Cardassians were the pharaohs to the slave Bajorans. The "Galor" continued to act as the backbone of the Cardassian fleet throughout the events of Star Trek: Deep Space Nine. Ships of this type are armed with a large phaser cannon (possibly a plasma cannon). They are also armed with numerous secondary phaser cannons mounted at other points across the hull, and they may carry a complement of photon torpedoes.
Although no clear indication of the true strength of a Galor class vessel has ever been given, they are shown to be weaker than the Galaxy class ships used by the Federation Starfleet ().
Technical descriptions indicate that Cardassian ships were designed to act in packs rather than as single ships, unlike the Federation's counterparts.
A more powerful Cardassian ship is the "Keldon" class starship (which is similar to the "Galor"-class with more defined aft wings and a large trapezoidal pod atop the main hull). This ship class is assumed to be comparable to the "Galaxy"-class in tactical capability; why they have not been seen in greater numbers is uncertain. As seen in later episodes, the Galaxy-class vessel was produced far more than its original run, so it does seem odd that the similarly advanced "Keldon" did not appear as often. The Cardassian Obsidian Order kept a fleet of "Keldon" class starships which were used in conjunction with the Romulan Tal Shiar during the sneak attack in the Omarion Nebula.
The Cardassians have also been known to operate small attack craft such as the "Hideki" class scout. It is a small attack craft composed of a semi-elliptical fore with a short aft extension ending in a pincer-shaped disruptor weapon. Due to the ship's limited offensive power the Hideki class is confined to border patrol duties during peace time. During the Dominion War the class was present in several major conflicts, they operate in large groups and swarm enemy ships, this allows them to overpower much heavier vessels.
All Cardassian warships seen so far are painted ochre, and have backwards-swept delta winged hulls (resembling the Cardassian national symbol); the delta wings resemble fins, giving the Cardassian ships the appearance of predatory rays.
Cardassian computers use data encoded on isolinear rods, in contrast to chips used for similar purposes by human-designed computers.
Cardassia Prime.
The Cardassians' homeworld, Cardassia Prime (also known simply as "Cardassia"), is the seventh Class-M planet of its system. Even though the planet's orbit is far out from its star - a Cardassian year is over 40 earth years - due to the enormity of the planet's sun - the climate is still warmer than that preferred by several species—summers tend to be in the 135-150 °F (57-68 °C) range and winters tend to be in the 95-110 °F (35-43 °C) range.
Human and Bajoran characters, among others, make comments throughout "'s" run about Cardassians' preference for heat, while characters like Elim Garak complain about the uncomfortably cool temperatures preferred by non-Cardassians. Its landscape is often arid, though animal and plant life are still plentiful on the surface.
History.
Pre-Dominion membership.
In "The Next Generation" episode "", David Warner's character states that in Cardassia's early history, its inhabitants were a peaceful and spiritual people. In the days of the First Hebitian Civilization, the Cardassians collected works of art from all over the Alpha Quadrant and the planet boasted a vast wealth of art and culture; the people were said to have elaborate burial vaults with unimaginable treasures. However, Cardassia's lack of natural resources caused terrible famine, and the Hebitian civilization fell into decay. Its ruins were plundered by starving Cardassians who sought to sell whatever they could to provide for themselves. A military dictatorship soon came to power, building fleets of warships and invading nearby worlds, although whether or not the Cardassians literally conquered and ruled worlds inhabited by other sentient races is unclear. An exception is Bajor which was occupied for fifty years, and the end of whose occupation destabilized the Cardassian government and formed a key story arc for "Deep Space Nine" episodes.
The date of first contact between the Cardassians and Starfleet is unknown, but is likely to have occurred mid-to-late 22nd Century, as a Cardassian exile, Iloja of Prim, lived on Vulcan during that time period. Sometime before 2347 the Cardassians attempted to expand into Federation territory and war broke out, lasting around twenty years. Captain Edward Jellico spearheaded successful attempts by Starfleet to negotiate a peace treaty which ended the war.
Shortly after the Cardassians withdrew from Bajor, a Federation presence was established aboard Terok Nor, renamed Deep Space Nine, to assist the Bajoran Provisional Government in rebuilding Bajor. However, the Federation officers discovered a wormhole to the Gamma Quadrant close to the station ("). Roughly four months later, the Federation-Cardassian borders were redefined, with the two sides buffered by a demilitarized zone. However, the new border treaty gave Cardassia control of several worlds inhabited by Federation colonists and the Federation colonies inhabited by Cardassians. Disgruntled former Federation colonists in the area, feeling that their opinions and wishes had been ignored by both sides, formed a resistance movement known as the Maquis.
In January 2372 (Stardate 49011) the Klingon Empire attacked the Cardassian Union, believing the Detapa Council of Cardassia (which had just come to power in the wake of the Cardassian Central Command being overthrown, making it the first civilian government of the Cardassian Union with Gul Dukat as military advisor) had been infiltrated by the Dominion ("). The attack was led by General Martok who, it turned out, had been himself replaced by a shapeshifter, one of the leaders of the Dominion (first revealed in "Apocalypse Rising"; see also "In Purgatory's Shadow").
Dominion membership.
Sometime between October 2372 and February 2373, with a Dominion attack on Deep Space Nine imminent, Gul Dukat announced the Cardassian Union's entry into the Dominion, shocking not only the Federation but most Cardassians as well. At the same time, Gul Dukat announced his ascension as leader of the Cardassian Union. Five days later, the Klingons had been expelled from Cardassian space and nearly the entire Maquis movement was slaughtered by the Dominion (except for those on the USS "Voyager", which was lost in the Delta Quadrant at the time). Otherwise, nearly all the other Maquis who had not died were in Federation prisons.
The Cardassians (as members of the Dominion) captured DS9 (""), but the Federation managed to block the Bajoran wormhole with self-replicating mines, preventing the Dominion from sending reinforcements from the Gamma Quadrant.
Gul Damar discovered a way to disable the self-replication of the mines and completed the procedure and fired on the minefield seconds before Rom and Kira disabled DS9's weapons in hopes to prevent just that. The USS "Defiant" attacked DS9 and managed to take it back when the Bajoran Prophets destroyed an entire Dominion fleet, sent to reinforce Dominion lines, on its way through the wormhole. Gul Dukat was captured after his daughter Ziyal was killed by Gul Damar, who was then promoted to Legate ("").
Under the leadership of Damar, the Cardassian Union, along with the Dominion, continued to gain ground over the Klingon-Federation alliance, and even after Benjamin Sisko and Elim Garak tricked the Romulans into breaking their nonaggression treaty with the Dominion and joining the alliance ("In the Pale Moonlight") they still managed to keep the upper hand.
A major figure in Cardassian history is Tret Akleen, revered as the "father" of the Cardassian Union. During the Dominion War, Akleen's family home lay in Dominion-controlled space; Elim Garak suggested that recapturing it would lead to a major propaganda victory for Federation forces. ("")
Opposition to the Dominion.
Damar, however, was not happy. While he had hoped that Cardassia's joining the Dominion would strengthen their power, he felt that they were no longer in control of even their own planet, having to report to the Dominion representative Weyoun and the Founders, and Cardassian troops were being sacrificed seemingly meaninglessly without his permission. For a time, Damar sank into heavy drinking. Shortly after the Breen joined the Dominion, almost guaranteeing the Dominion's victory, Damar organized a revolt but was betrayed by a man he attempted to involve in the conspiracy. Subsequently a Cardassian named Broca became Legate and puppet ruler of Cardassia with his information, and after treason within the Revolt, the Dominion crushed it and forced Damar into hiding.
The revolt started out as just a small legion of troops headed by Damar, but during the final assault on the Dominion over Cardassia Prime by the Federation-Klingon-Romulan alliance, Damar managed to get an open revolt started on Cardassia itself. In response to Cardassian citizens engaging in acts of sabotage, the Dominion punished the Cardassians by destroying Lakarian City killing millions of men, women, and children in the process of reducing it to ashes. As a result, the Cardassian fleet switched sides during battle and assisted the alliance, opening a hole in the Dominion lines and forcing the Jem'Hadar and the Breen to establish a new defense perimeter around Cardassia Prime itself. When word of the fleet's defection reached the Dominion command center, the Female Changeling ordered every Cardassian on the planet killed.
With the Cardassian fleet helping the alliance and the rebel's attack on the Dominion headquarters on Cardassia, the Dominion surrendered, ending the Dominion War.
The Cardassian cost due to the Dominion War was the highest of all the major powers. The homeworld was severely damaged by the Dominion, whose Founders ordered a "scorched earth" approach to the Cardassians' homeworld for their betrayal during the final battle of the war. The long-term effect on the ecology of the planet remains to be seen. Over 800 million Cardassians died on Cardassia alone. Several non-canon sources have placed the pre-war population at around seven billion, and with the canonically established Cardassian emphasis on the family unit, the race is safe from extinction.
Alternate versions.
Mirror universe Cardassians.
In the Mirror Universe, the Cardassians formed an alliance with the Klingon Empire to conquer the Terran Empire. Beyond that, the Cardassians of the Mirror universe appear to be more or less identical to their more familiar counterparts.
In cosmology.
In cosmology, the concept "Cardassian expansion" is a term used for a modification to the Friedmann equations. It is named after the fictional Star Trek race by the original authors, Katherine Freese and Matthew Lewis. In their 2002 paper (which has been cited more than 330 times), a footnote on the "Cardassian term" states:

</doc>
<doc id="27086" url="http://en.wikipedia.org/wiki?curid=27086" title="Maquis (Star Trek)">
Maquis (Star Trek)

In the American "Star Trek" science fiction franchise, the Maquis are a 24th-century paramilitary organization and/or terrorist group first introduced in the 1994 episode "The Maquis" of the television series ', who subsequently also appeared in ' and "".
Concept.
The concept of the Maquis was intentionally introduced by the creators of "Deep Space Nine" so that it could play a plot device in the upcoming "Voyager", which was scheduled to begin airing in 1995. As Jeri Taylor commented, "we knew that we wanted to include a renegade element in "Voyager", and that the show would involve a ship housing both Starfleet people and those idealistic freedom fighters that the Federation felt were outlaws [i.e. the Maquis]." Therefore the creators of "Star Trek" decided to create a backstory for the Maquis in several episodes of "Deep Space Nine" and "The Next Generation", and they named them after the French guerrilla fighters of the Second World War. The recurring characters of Michael Eddington (played by Kenneth Marshall) in "Deep Space Nine" and Ro Laren (played by Michelle Forbes) in "The Next Generation" became members of the Maquis, and "Voyager" contained three regular Maquis characters: Chakotay (Robert Beltran), Seska (Martha Hackett) and B'Elanna Torres (Roxann Dawson), as well as Tom Paris (Robert Duncan McNeill), a regular character who had been captured and imprisoned for joining the Maquis.
Fictional backstory.
According to the fictional storyline of the "Star Trek" universe, the Maquis were formed in the 24th Century after a peace treaty was enacted between the United Federation of Planets and the Cardassian Union, redesignating the demilitarized zone between the two powers, which resulted in the Federation ceding several of their colony worlds to the Cardassians. Although the colonists were offered free relocation to elsewhere in Federation territory, some insisted on remaining on the ceded worlds, effectively becoming Cardassian Union citizens. Some of these colonists subsequently formed the Maquis to protect themselves from Cardassian aggression, although they received no official support from the Federation, who feared breaking the peace treaty with the Cardassians, which would lead to war.
Nonetheless, various Federation members supported the Maquis' cause, and illegally helped to supply them with weapons and other technology that they could use in their struggle. In several cases the Federation actually intervened in the war between the Maquis and the Cardassians, aiding the latter in recognition of the peace treaty. In one case the Federation ship USS "Voyager" tracked a Maquis vessel to the Badlands with the intention of apprehending it but an alien-being sent both to the Delta Quadrant, in the opposite side of the Milky Way Galaxy. The two crews were forced to unite to survive against alien threats like the Kazon. In following years, when the Cardassian Union joined forces with the Dominion to fight in the Dominion War against the Federation, the Dominion aided the Cardassian military in obliterating the Maquis, a prelude to their war against the Federation and its allies.

</doc>
<doc id="27110" url="http://en.wikipedia.org/wiki?curid=27110" title="Romulan">
Romulan

The Romulans are a fictional extraterrestrial humanoid species in the science fiction franchise "Star Trek". First appearing in in the 1966 episode "Balance of Terror", they have since made appearances in all the main later "Star Trek" series: ', ', ', ' and '. In addition, they have appeared in various spin-off media, and prominently in the two films ' (2002) and "Star Trek" (2009).
Throughout the series, they are generally depicted as antagonists, and are usually at war with or in an tenuous truce with the United Federation of Planets. On extremely rare occasions, they have allied with the Federation.
The Romulans also act as a counterpoint to the logical Vulcan race, whom they resemble and with whom they share a common ancestry. As such, the Romulans are characterized as passionate, cunning, and opportunistic — in every way the opposite of the logical and "cold" Vulcans. The Romulans are the dominant race of the Romulan Star Empire. Although "Star Trek Star Charts" place the Romulan Empire's territory in the Beta Quadrant of the galaxy in "", they are referred to as an Alpha Quadrant power.
The Romulans were created by Paul Schneider, who said "it was a matter of developing a good Romanesque set of admirable antagonists ... an extension of the Roman civilization to the point of space travel". There are some differences in their history and the way they are portrayed on television, in the motion pictures and in several books by Diane Duane, called the "Rihannsu" series, after the term they use to refer to themselves in their Romulan native language.
Biology.
The Romulans began as a revolutionary group of Vulcans who refused to accept the Vulcan philosopher Surak's teachings of the complete suppression of emotions. At some point in their shared history, this dissident group left the planet of Vulcan, eventually settling on the planets Romulus and Remus. In the original series episode "Balance of Terror", Spock notes that while the events during the period of Surak are well-documented (as per episodes that detail such like "The Savage Curtain"), he is uncertain about their connections to the Romulans. He does state that he thinks them a likely offshoot of Vulcan. implies that Romulans, Vulcans, Cardassians, Klingons and Humans share a common ancestry.
Like Vulcans, Romulans have pointed ears, upswept eyebrows, and copper-based blood (Hemocyanin) that is green when oxygenated in the arteries and copper or rust colored when deoxygenated in the veins. In the original series, Romulans were essentially indistinguishable from Vulcans in appearance, but subsequent series and films introduced a V-shaped ridge above the bridge of their nose, a similar prosthetic make-up development to that of the Klingons. Like Vulcans, Romulans are almost always depicted as having dark or black hair.
Romulans share the longevity common to their Vulcan cousins. In "", the Romulan Senator Pardek shared a friendship with Ambassador Spock lasting at least 80 years. Romulans also have in common with Vulcans physical strength superior to that of humans. However, the similarities end when it comes to Vulcans' mental abilities, which the Romulans do not share.
Culture.
Cuisine.
Romulan ale is a fictional popular blue alcoholic beverage which was illegal because of a Federation trade embargo in the late 23rd century (per ' and ') through the late 24th century (per '). Despite this, it is often traded and consumed openly. During the alliance with the Federation during the Dominion War, Romulan ale was briefly legalized, even though it was later outlawed again after the war, as stated by Geordi La Forge in '.
Other Romulan drinks include Kali-fal, a blue drink with an aroma that should "forcibly open one's frontal sinuses before the first sip."
Fashion.
Romulan fashion of the late 24th century had distinctive squared shoulders. Hair is generally cut straight across the brow close to the eyebrows, with longer locks framing the face, cut following the cheekbones, a style reminiscent of a helmet.
In ', Romulan military uniforms consisted of a gray tunic with varying kinds of decorative sashes. Commanders wore red sashes, senior officers wore blue sashes, and most soldiers wore no sash at all. In subsequent series, such as ', Romulan uniforms were of a different style, with varying kinds of patterns and colors. The dominant uniform style thereafter was gray under a pattern of squares. The rank insignia on the "Next Generation"-era Romulan uniform consisted of a series of diamond and crescent shapes, worn on the left collar. Their uniforms tend to fit rather loosely, and often feature large phaser holsters that allow the entire weapon to be 'dropped in', hiding most of it from view.
As of "", Romulan uniforms were more standardized. Episodes of the fourth season of "Star Trek: Enterprise" depicted the 22nd century Romulans wearing exactly the same uniforms as those of the 24th century "Nemesis".
Romulan military uniforms follow a distinct pattern through the 23rd and 24th centuries. When one allows for the change in technology in the television industry and increased budgets, it's easy to see how uniforms from the 23rd century (TOS) evolved into the uniforms seen in the 24th century (TNG). Male hairstyles do not appear to change greatly, although 24th century hairstyles seem more distinct from Vulcan hairstyles. Females in the 23rd century wore long hair in a variety of styles. By the 24th century, females wear a style similar to males.
Design.
The emblem of the Romulan Star Empire depicts a large bird of prey clutching the worlds of Romulus and Remus. The avian motif also appears on their warbird starships. Those who rejected the teachings of Surak were said to be "beneath the raptor's wing".
Designer Herman F. Zimmerman has said regarding interior design, "the Romulans have possessed advanced technology a lot longer than the Federation, so the look was a combination of art deco and medievalism meets high tech. Most of the designs were inspired by Italian designer Carlo Scarpa."
Regarding exterior design of the Senate area, designer Syd Dutton said director "Stuart Baird wanted us to think about Albert Speer, the architect who did all the conceptual drawings for Hitler. Speer took that National Socialist idea to a extreme where everything was huge and classical and they have moats.
"The Romulans are a people who live in a marshy area. They had little houses on stilts around mudwork. The mudwork became part of this central core and that was where the old part of the city-–the Forum and Senate buildings-–was located. As the city expanded going away from that, the buildings became bigger and more technological."
Society and government.
The Romulan government is similar to that of the Roman Republic before it became the Roman Empire. The Romulan government consists of at least two parts:
It has been implied that Romulans use a caste system. The Romulan contempt for Vulcans, their treatment of other sentient species, such as the Remans, and their need for strict conformity, suggests that Romulan society is racist/speciesist.
The Romulan Star Empire once had an empress. A member of the Q Continuum informed Kathryn Janeway that he had considered having a child with the Romulan Empress ("The Q and the Grey"). However, it is unclear when the Romulans possessed this system of government, or how it functioned in relation to the Romulan Senate, or even if they have abandoned the monarchy.
The Romulan term for their mythological place of creation is "Vorta Vor".
It is generally accepted canon that Romulan females are equal to males, both having equal ability to rise through the ranks of the military. Notable females include Sela (Tasha Yar's daughter), Caithlin Dar ('), Donatra ('), Taris and Toreth (TNG: " and ", both played by Carolyn Seymour) and the Romulan Commander in the TOS episode "The Enterprise Incident", who is never referred to by name (in Star Trek CCG and some noncanon novels, she is called "Commander Liviana Charvanek").
Leadership.
Praetors
Senators
Proconsuls
Notable Military Officers
Military.
In "Star Trek" canon, the Romulan military appears to be a "combined" service, like Starfleet. Its version of "joint chiefs of staffs" is the "High Command" (TNG's ""), an institution most likely carried over from and modeled on the Vulcan High Command ("Star Trek: Enterprise").
Some Romulan military ranks are recognizable army and navy versions, while others are either carried over from the Vulcan High Command or entirely new. Perhaps army-specific ranks are reserved for the Tal Shiar.
One lowly rank in the Romulan Guard is "uhlan" (TNG's ""). This actually derives from an in-joke in an early fanzine, "Tricorder Readings" circa 1970. A fan wrote in explaining who the original Uhlans were, and wisecracked that because regiments of uhlans existed in the armies of many countries, Roman uhlans could be "Rom Uhlans". The joke was probably reprinted in several other fanzines. The real meaning of uhlan is "lancer".
The lowest commissioned military rank appears to be "sublieutenant". In TNG's "The Defector", Admiral Alidar Jarok initially used the cover name "Sublieutenant Setal", a mere "logistics clerk".
"Sublieutenant" presupposes the next-higher rank of "lieutenant", but this is speculation because no Romulan character has had that rank.
"Centurion" is the next-highest rank and probably the most common. In TNG's "", Centurion Bochra appears to be an ordinary, midlevel officer. However, the nameless Centurion in TOS's "Balance of Terror" seems to be a senior officer who holds sway with his commander, having served on more than 100 campaigns with him.
"Subcommander" may be a holdover rank from the Vulcan High Command (T'Pol initially holds it in ""). It's usually reserved for executive officers on Romulan ships (Tal in TOS's "The Enterprise Incident" and N'Vek in TNG's "), but in rare cases, subcommanders may captain their own Warbirds (Taris in TNG's "). In addition, some subcommanders have served in exchange-officer roles (T'Rul in DS9's ") or as government attaches (Velal in DS9's ").
"Commander" is the rank usually reserved for individual Warbird COs (Toreth in TNG's "Face of the Enemy", Sirol in "", and Tomalak in "The Enemy" and "The Defector"). However, some commanders have had charge over fleets of Warbirds (the unnamed Commander in TOS's "The Enterprise Incident" and Sela in TNG's "").
"Admiral" is the highest naval-style rank and is accorded to sector commanders (Jarok in TNG's "The Defector") or fleet commanders (Mendak in TNG's "Data's Day").
Other senior officer ranks, such as major, colonel and general, are army-style and may often be reserved for members of the Tal Shiar. Deanna Troi impersonated "Major Rakal" in TNG's "Face of the Enemy" and served as a kind of "political officer." A Founder impersonated "Colonel Lovok" in DS9's "" and commanded a fleet of Warbirds.
The two Romulan generals mentioned in "Star Trek" canon may or may not have been Tal Shiar operatives. Velal, elevated from subcommander to general as of DS9's "When It Rains..." and "", was probably not. Perhaps the unnamed Romulan general officer who rescued Tasha Yar and fathered Sela (TNG's "Redemption, Part II") was with the Tal Shiar: without official canon sources, these remain purely conjectural.
Tal Shiar.
The Tal Shiar is a Romulan intelligence organization. The name is an homage to the "Tal Shaya", a Vulcan method of execution from the original " series where the neck is broken with a swift stroke for a quick and merciful death.
Before the Dominion War, the Tal Shiar and the Cardassian Obsidian Order secretly worked together to launch a preemptive strike on the Founder homeworld. The plan was initiated by retired head of the Obsidian Order Enabran Tain, and was supported by Tal Shiar Colonel Lovok, who was later revealed to be a changeling, as a result the entire fleet was destroyed. Soon after, the Cardassian Union joined the Dominion while the Romulan Star Empire entered a pact of neutrality with them.
In the " season 6 episode ", Counselor Deanna Troi was surgically altered by a Romulan underground organization to impersonate a Tal Shiar officer Major Rakal.
In the " season 6 episode "In the Pale Moonlight", Elim Garak and Captain Sisko successfully fool the Tal Shiar into concluding that the assassination of Romulan Senator Vreenak was carried out by the Dominion.
Homeworld.
Romulus and Remus ("ch'Rihan" and "ch'Havran" respectively in Diane Duane's Rihannsu novels) are the primary two Romulan planets. Both planets orbit the same central star (as depicted in ""). Although they are not a binary planet system, Romulus and Remus are often referred to as "twin planets".
Romulus had no sentient species until a revolutionary Vulcan movement colonized it around 400 AD. These revolutionaries, over time, became the Romulans. A sapient species called the Remans developed on Remus and was conquered by the Romulans, later becoming a lower class in Romulan society.
The original colonization group of Romulans came to this system after they fled their homeworld in rebellion against the philosophy of peace and logic proposed by Surak. As Spock would later point out, if the Romulans retained the passions and expansionist outlook that the pre-Surak Vulcans once had, it would make them an extremely dangerous race.
Romulus.
The Romulan government resembles the Roman Republic before it became the Roman Empire. The Romulan government consists of several parts: the Romulan Senate, the main governing and legislative body in a large chamber on Romulus. The Senate is headed by the Praetor, followed by the Proconsul. The Senate does not sit on the third day of the Romulan week. The Romulan Senate also has the Continuing Committee, which is composed of senators and the chairman of the Tal Shiar and confirms the new praetor.
The Romulan military and government also have positions similar to those of the Roman Republic: Proconsul, Praetor and Senator. (See this article under the heading "Roman Republic" for more on these offices.)
In the Pocket Books novels by Diane Duane, the Romulan name for the planet is "ch'Rihan", hence the endonym for the people is "Rihannsu".
The destruction of Romulus is depicted in the 2009 film "Star Trek"; Spock Prime blames it on the explosion of a nearby star, said to happen 129 years after the events portrayed in the film.
Remus.
A sister planet or nearby solar system labeled "Romii" appeared on a star map in the episode "Balance of Terror", the first episode to feature the Romulans. The name "Remus" and its actual inhabitants were first mentioned in the 2002 sequel film '. (Two Remans appear in the background in an episode of ' which, while set two hundred years before the events of "Star Trek: Nemesis," was filmed after the movie's publication.)
As revealed in "Star Trek: Nemesis", the inhabitants of Remus are the bat-like Remans, who were subjugated ever since the Romulans staked their claim to Remus and set up their new homeworld in the system. The story did not clarify whether the Remans are native to Remus, or whether they are another Vulcanoid-offshoot race. Because the planet is tidally locked to its sun, one side of the planet is in constant darkness. Living on this dark side has made the Remans extremely sensitive to light.
The Remans were treated as second-class citizens within the Romulan Star Empire. Remus is the prime planet of dilithium mining and as such many Remans are forced into slave labor.
Much about Remus remains a mystery, including the source of its humanoid population. There is no onscreen depiction, one way or the other, to show whether Remans originated on Remus itself, or that they are mutated descendants of the original Vulcan colonists. Remans do, however, appear to have some telepathic abilities, green-tinted skin, and pointed ears, possibly implying a shared genetic lineage with Romulans and Vulcans. Hope for a more cooperative future between the United Federation of Planets and the Romulan Star Empire was strengthened after Shinzon's fall.
Technology.
Romulans are noted for their use of disruptor weapons, photon torpedoes, plasma torpedoes, and their signature cloaking technology, as well as having spaceships that are powered by artificial singularities (due to the nature of these engines, once activated, there is no way to shut them down). In the 22nd century, they displayed advanced holotechnology and telepresence.
A number of designs for Romulan starships have been presented. Since "", Romulan warships have been referred to as "Warbirds".
History.
Origins.
The Romulans began as a revolutionary group of Vulcans who were referenced as "those who march beneath the Raptor's wings" and refused to accept the Vulcan philosopher Surak's teachings of complete suppression of emotions. Around 400 AD, the dissident group split off from Vulcan society and began the long journey to search for a new homeworld. At some point, whether before or after reaching Romulus is unclear, dissidents developed among the Romulans themselves, and a faction of Romulans established a civilization later known as the Debrune.
22nd century.
The Romulan Star Empire was militaristic and bent on conquest. When Senator Valdore questioned this policy, he was dismissed from the Senate (although he later joined the military, and rose quickly through the ranks, becoming Admiral by 2154).
In 2152, Humans made first contact with the Romulans when the "Enterprise" (NX-01) encountered a Romulan-laid minefield. Communication was via audio only. The Romulans saw that Humans fostered a spirit of cooperation among the long-belligerent Vulcans, Andorians and Tellarites. Realizing that this would bring solidarity to the region and an obstacle to conquest, the Senate took steps to turn these species against each other.
In 2154, Romulans covertly conspired with V'Las, head of the Vulcan High Command, to invade Andoria. V'Las's Romulan contact had the stated agenda of reunification with the Vulcans.
A few months later, the Empire sent prototype holoships remote-controlled from Romulus to disrupt a peace conference between Andorians and Tellarites. The Romulans piloted the ships using an abducted Aenar; however, their scheme was thwarted by the combined efforts of the Humans, Vulcans, Andorians, and Tellarites, led by the "Enterprise". This enraged the Romulans, who vowed revenge upon humanity.
In 2156, the Earth-Romulan War began. Both sides used nuclear fusion bombs, which were some of the most advanced weapons technology available at the time. The war only ended after both sides had fought to the point of exhaustion, and realized that further conflict would result in mutual destruction. It has been implied that the Earth-Romulan War reached Earth, heavily damaging it: two centuries later, it was remarked that Earth had not been subjected to the horrors of total war since the date of the Romulan Wars. Though the war ended in a stalemate, it closed with the Battle of Cheron, which was a decisive Earth victory.
It has also been noted that no visual communication took place between the Humans and Romulans at that time. V'Las remained perhaps the only citizen of the future Federation to be aware of the true origins of the Romulans.
In 2160, the Romulans and the Humans signed a treaty ending the war and establishing a neutral zone one light year wide between their territories. The treaty was negotiated via subspace radio, again with no visual contact. In 2161, Humans, along with Vulcans and several other species, founded the United Federation of Planets, which continued this wary peace.
23rd century.
Using a cloaked ship, the Romulans broke the treaty of 2160 by attacking several Federation outposts, circa stardate 1709.21, in the year 2266 ("Balance of Terror"). In response, the USS "Enterprise" (NCC-1701) tracked down the cloaked Romulan ship and destroyed it. This was also the first time humans saw what the Romulans looked like physically, and Spock surmised a common ancestry.
Around 2267, it is thought that the Romulans entered a treaty with the Klingon Empire: in exchange for cloaking technology, the Romulans received D7-class battle cruisers, which were upgraded into extremely deadly war machines. (The script of "The Enterprise Incident" originally called for a Romulan ship to appear, but the original model was not available: rather than go to the expense of building a new one, the Klingon D7 model was substituted.) Spock mentioned intelligence about this treaty when the Klingon ship appeared on the viewscreen, at the outset of "The Enterprise Incident". The Romulan commander further implies the common ancestry when she confronts Kirk about violation of Romulan space. The events of 2154 suggest the Vulcans withheld from humans their prior knowledge of Romulan kinship with Vulcans.
Circa 2272, Klingon forces led by Kor had a victory over some Romulan opponents in the Battle of Klach D'Kel Brakt.
In 2293, the Romulan ambassador to the Federation, Nanclus, took part in a conspiracy to sabotage peace talks between the Klingons and the Federation. The attempt was unsuccessful, and Nanclus was arrested on Khitomer, along with several other conspirators. During the Khitomer conference, the Romulans signed a treaty with the Federation and the Klingons. Notably, Ambassador Sarek of Vulcan sat with his Romulan cousins during the conference, indicating a dialogue, or at least a display of goodwill between the two peoples.
24th century.
Isolation.
In 2311, an event known as the Tomed Incident occurred between the Romulans and the Federation, costing thousands of lives. The details of the Tomed Incident are never revealed canonically, but it is referred to by "Enterprise"-D crew members in the "Next Generation" episode "The Neutral Zone" as "disastrous"; it resulted in the signing of the Treaty of Algeron, which reaffirmed the Neutral Zone as a no-fly zone and prohibited the Federation from developing cloaking technology.
For the next fifty years, the Neutral Zone was quiet. There was no direct contact between the Federation and the Empire, nor were there any further Romulan incursions.
In 2344, four Romulan Warbirds attacked the Klingon outpost at Narendra III. The USS "Enterprise" (NCC-1701-C) responded to the outpost's distress call and engaged the Romulan ships, but was defeated and taken with its survivors (among them a version of Lieutenant Tasha Yar from the future of an alternate timeline) back to Romulan territory. Rumors circulated in the Federation that the "Enterprise's" broken hull was displayed on Romulus, to boost the morale of Romulan fleet academy students. The Klingons considered this action dishonorable, in contrast to the honor they saw in the Starfleet ship's attempt to defend Narendra III, despite being vastly outnumbered. As a result, relations strengthened between the Klingon Empire and the Federation.
Then in 2346 the Romulans covertly assisted by a Klingon traitor, Ja'rod, attacked another Klingon planet, Khitomer and killed or captured all but two of the planet's population.
In 2351, a Romulan science vessel on a deep space mission under the command of scientist Telek R'Mor encountered an unstable temporal micro-wormhole and made contact with the Federation starship USS "Voyager", stranded in the Delta Quadrant in 2371. The crew of "Voyager" desired for R'Mor to tell the Federation of their fate, but "Voyager" was 20 years into the future. As a result, R'Mor agreed to hold onto their messages for another 20 years before delivering them in order to preserve history. However, he died in 2367 before he could do so.
Circa stardate 41986.0 (December 26, 2364), the Romulan Star Empire ended its five decades of isolation when the USS "Enterprise" (NCC-1701-D) intercepted a Romulan Warbird. Several Romulan outposts had been destroyed by a then unknown force (later implied to be the Borg). The Romulans opened communications to see if they could glean the information from the Federation, who had suffered similar losses. The Romulan Commander Tebok explained that the Romulans had decided to concentrate on their own internal affairs for the past fifty years and said "We are back.", indicating that Romulans would again be active in galactic affairs.
In the " episode ", after Earth's history was inadvertently altered so that the Federation was never formed, the Romulans had established a presence in the Alpha Centauri system by the 24th century. Since Alpha Centauri is the star system closest to Earth, it indicates that, in the altered timeline, the Romulans invaded Earth's stellar region, despite not even being prompted by Earth's role in the encouragement of interplanetary cooperation.
Attempted conquests.
In 2366, the Romulans attempted to trick the Enterprise-D into crossing the Neutral Zone, where two Romulan Warbirds were waiting to capture or—if necessary—destroy it. However, Captain Jean-Luc Picard, foreseeing a possible trap, had quietly contacted the Klingons beforehand: as a result, the Enterprise was covertly accompanied into the Neutral Zone by three Klingon Birds-of-Prey. Now evenly matched, the Romulans decided that the price of the Enterprise's destruction was too high and retreated ("").
In 2367, the Romulans brainwashed Commander Geordi La Forge of the USS "Enterprise" in an attempt to incite a Federation-Klingon war, but the plan was exposed and foiled. (")
Later in 2367, during the Klingon Civil War, the Romulans secretly backed the House of Duras. Captain Picard, suspecting this, convinced pro-Federation leader Gowron to attack Duras' forces to draw Romulan reinforcements out of hiding; when the Romulans attempted to enter the fray, they were revealed by a Federation tachyon blockade, and the Duras family, exposed as traitors, lost all support. (")
Circa stardate 44995.3 (December 28, 2367), the Vulcan Ambassador Spock was discovered on Romulus, working with an underground movement for the reunification of the divided races. The Romulans saw this as a chance to conquer the Vulcans and sent a fleet of ships toward Vulcan, but their intent was discovered and the fleet destroyed by the Romulans themselves. Spock elected to remain underground on Romulus to teach the Vulcan heritage to those who might listen. These efforts facilitated the defection of Vice Proconsul M'Ret to the Federation. (", ")
Sometime in 2374, a 27-member Tal Shiar team commanded by Commander Rekar hijacked the USS "Prometheus", an advanced prototype starship to be analyzed by the Tal Shiar. Two Emergency Medical Holograms, one from the USS "Voyager", overthrew the Tal Shiar crew and the ship returned to Federation hands. ("")
Dominion War.
In 2371, the Romulan Tal Shiar intelligence agency and their Cardassian counterparts, the Obsidian Order, launched a preemptive strike against the Dominion; their entire fleet however was destroyed, and it was revealed that the Tal Shiar officer leading the attack was actually a Dominion agent. ("The Die Is Cast")
The Romulan Empire signed a non-aggression treaty with the Dominion towards the end of 2373, which had gained a foothold in the Alpha Quadrant in Cardassian space shortly before the Dominion War broke loose.
Circa stardate 51721.3 (September 20, 2374), Starfleet Captain Benjamin Sisko and Elim Garak tried to trick the Romulans into joining the war against the Dominion by faking recordings of a Dominion strategy meeting discussing the plan to conquer the Romulans along with the Klingons and the Federation. They showed them to a high-ranking Romulan senator, Vreenak, who had negotiated the Romulan non-aggression treaty with the Dominion and was vice-chairman of the Tal Shiar, secretary of the War Plans Council, and one of Proconsul Neral's most trusted advisors. Vreenak discovered the deception and attempted to leave to inform his government, but died in his shuttle sabotaged by Garak. When the Romulans examined the wreckage, they discovered the recordings; assuming the incriminating defects to have been caused by the explosion, the Romulan Star Empire entered the war against the Dominion, joining the Klingon-Federation alliance. ("In the Pale Moonlight")
In 2375, one year after Proconsul Neral became praetor, the Romulans established a presence on Deep Space Nine and secretly began stockpiling weapons on a Bajoran moon. However, Bajoran Colonel Kira Nerys, with the assistance of Starfleet Admiral William Ross, forced the Romulans to back down and remove their weapons. ("Shadows and Symbols")
The rogue Federation agency, Section 31, had an agent in the Tal Shiar to safeguard Federation interests feeling that after the Dominion War, the Federation and the Romulan Star Empire would be the only two powers left in the region and would go to war.
Later that year, the allied fleet broke through Dominion lines and headed for the Dominion high command on Cardassia Prime. However, the fight went badly until a Cardassian uprising headed by the former leader Damar turned the Cardassian fleet against the Dominion. After this, the Dominion was defeated and the devastating Dominion War ended. ("What You Leave Behind")
"Reman" coup.
Circa stardate 56844.9 (November 4, 2379), the Romulan Senate was briefly overthrown in a Reman uprising led by Shinzon. After the senior Romulan leadership was assassinated in the Romulan Senate, the Remans took over the Senate and Shinzon became Praetor; he was, however, dispatched by Captain Jean-Luc Picard shortly afterwards.
Relations between the Romulan Star Empire and the United Federation of Planets improved somewhat as a result of Picard's assistance ("").
Romulan Star Empire.
There is no canonical information concerning the actual size of the Romulan Empire in comparison to the Federation. Star Trek writer/producer Ronald D. Moore has indicated that it is larger than the Klingon Empire but smaller than the Federation. However, in the Voyager computer game, ', the Romulan Star Empire is about two-thirds larger than the Klingon Empire, and is well over five times as large as the Federation, considering the Romulans' expansive nature. In the "Star Trek Atlas", the Romulan Star Empire is about 1/3 the size of the Klingon Empire and surrounded by the Federation. Their territory has a spherical shape with a small tail shape extension heading to the Delta Quadrant. In the book "The Romulan Way" by Diane Duane and Peter Morewood set in the 23rd century, it is implied that the Federation's resources far outstrip the Romulans, and in any conflict, the Federation would prevail by sheer weight of numbers. Several other episodes and licensed materials such as the "Next Generation" episode "" and the video game ' support this, and give the indication that the force multipliers of cloaking technology and other such secrecy is the only way they can maintain an even footing with the numerically superior Federation.
Books.
The Romulans have been the focus of a number of books, and have appeared or been mentioned in many others. Among their key appearances have been:
Romulans in the Mirror Universe.
The Romulans did not appear on screen in the Mirror Universe. The only canonical mention was in the 1995 " episode ". Benjamin Sisko, impersonating his Mirror Universe counterpart, claims that he is going to negotiate with the Romulans to secure their aid for the Terran Rebellion, suggesting that the Romulans are a significant power. However, this is merely a cover-story to explain his departure from the mirror Deep Space Nine, after which he would return to his own universe.
Romulans in Alternate reality.
According to the 2009 film "Star Trek" and the prequel comic series "", Romulus is destroyed in 2387 by a star going supernova. Spock was dispatched to stop the supernova with a red matter device but arrived too late. However, the implosion of the supernova caused by the red matter opened a rift in space-time that sent Spock and the Romulan mining ship "Narada", along with its captain, Nero, and its crew, to go back in time more than 100 years, creating an alternate timeline. Driven mad with grief, mostly because of the death of his pregnant wife, Nero and his men seek revenge against the Federation, whom they view as having caused the catastrophe indirectly, including the older version of Spock using the superior technology of their ship and equipment. Nero also destroys the Federation starship USS "Kelvin", seven other Federation starships, as well as a fleet of Klingon ships, and the planet Vulcan in the film's timeline. The problems of the Hobus supernova being so devastating to other star systems is addressed in the computer game "Star Trek Online", by declaring the blast to have "traveled through subspace".
Romulans in the Star Fleet Battles and related games.
This information below originates from the board wargame "Star Fleet Battles", as well as related game systems such as "Prime Directive" and "Federation and Empire".
Background.
Much as occurred in the , Romulans of the (non-canon) Star Fleet Universe descended from the Vulcans, having rejected the movement toward an unemotional, logic-based society, and settled in a distant star system. The modern era Romulans initially relied on an elderly fleet of warships converted to employ primitive non-tactical scale Warp drive, powered by fusion "impulse" drives. While the other races developed antimatter-based "tactical" Warp drive, which allowed them to fight at warp speed and increase weapons power, the Romulans fell behind. This was as much due to political infighting as to Gorn sabotage. (This is the SFU explanation for the 'sublight' Romulan vessel seen in "Balance of Terror": in the SFU, sublight engines are a common term for Warp engines incapable of tactical combat maneuvering as they only allow a starship to fight at slower-than-light speeds.)
The Romulans eventually signed the "Treaty of Smarba" with the Klingon Empire, which supplied the Romulans with advanced Warp drive and a number of mothballed Klingon vessels in exchange for drawing Federation forces away from the Klingon border. These Klingon-built starships were fitted out with Romulan weaponry and cloaking devices (as seen in "The Enterprise Incident"). This technology allowed the Romulans to develop a new series of vessels which caused significant headaches to their enemies.
History.
The Romulans in the Star Fleet Universe held a long-standing enmity with the Gorn Confederation, but their old "Eagle" series warships were no match against the advanced Gorn warships. The Klingon Empire even launched devastating raids against Romulan frontier bases and squadrons, and was planning a full-scale invasion were it not for the misfortune of the Tholians' arrival. The extragalactic Tholians drove a wedge between the Klingons and Romulans, arriving in the region of Klingon space where the invasion was being prepared. The Klingons kept these events a secret from the Romulans for a long time. The Romulans eventually emerged in the post-Smarba period with a powerful new fleet composed of "Kestral" (Klingon-built vessels) and "Hawk" (hybrid) series fleets, along with extensively retrofitted "Eagle" vessels. After the Klingon Empire persuaded the Romulans to enter the General War, the Romulans invaded the already besieged Federation on what they called "The Day of the Eagle". Despite significant strategic advances into Federation space, they were eventually driven back to their own borders, and suffered a devastating wartime catastrophe on Remus.
After the General War, the Romulan Empire erupted in civil war, had their border stations destroyed by the Interstellar Concordium in the course of the ISC War of Pacification, and subsequently aided in resisting the Andromedan invasion. A bruised Romulan Republic emerged from the ruins of their once proud star empire.
Connection to Roman mythology.
Many of the terms used in relation to the Romulans are derived from Roman mythology and government. Romulus and Remus are the two brothers who founded the city of Rome. The proconsul and praetor were government officials during the Roman Republic and the Roman Senate was its governing body.
In TOS episode "Who Mourns for Adonais?", it is revealed that the classical Greek and Roman gods were actually a race of advanced beings who had visited Earth thousands of years ago. It has been postulated that the same beings had visited other worlds as well – such as Vulcan, or Romulus. The theory did at one time appear on the "Star Trek" website, and would explain the connection between the Romulans and Roman mythology, as well as the institutions of Roman government.

</doc>
<doc id="27117" url="http://en.wikipedia.org/wiki?curid=27117" title="Selenium">
Selenium

Selenium is a chemical element with symbol Se and atomic number 34. It is a nonmetal with properties that are intermediate between those of its periodic table column-adjacent chalcogen elements sulfur and tellurium. It rarely occurs in its elemental state in nature, or as pure ore compounds. Selenium (Greek σελήνη "selene" meaning "Moon") was discovered in 1817 by Jöns Jacob Berzelius, who noted the similarity of the new element to the previously known tellurium (named for the Earth).
Selenium is found impurely in metal sulfide ores, copper where it partially replaces the sulfur. Commercially, selenium is produced as a byproduct in the refining of these ores, most often during production. Minerals that are pure selenide or selenate compounds are known, but are rare. The chief commercial uses for selenium today are in glassmaking and in pigments. Selenium is a semiconductor and is used in photocells. Uses in electronics, once important, have been mostly supplanted by silicon semiconductor devices. Selenium continues to be used in a few types of DC power surge protectors and one type of fluorescent quantum dot.
Selenium salts are toxic in large amounts, but trace amounts are necessary for cellular function in many organisms, including all animals, and is an ingredient in many multi-vitamins and other dietary supplements, including infant formula. Selenium is a component of the antioxidant enzymes glutathione peroxidase and thioredoxin reductase (which indirectly reduce certain oxidized molecules in animals and some plants). It is also found in three deiodinase enzymes, which convert one thyroid hormone to another. Selenium requirements in plants differ by species, with some plants requiring relatively large amounts, and others apparently requiring none.
Characteristics.
Physical properties.
Selenium exists in several allotropes that interconvert upon heating and cooling carried out at different temperatures and rates. As prepared in chemical reactions, selenium is usually an amorphous, brick-red powder. When rapidly melted, it forms the black, vitreous form, which is usually sold industrially as beads. The structure of black selenium is irregular and complex and consists of polymeric rings with up to 1000 atoms per ring. Black Se is a brittle, lustrous solid that is slightly soluble in CS2. Upon heating, it softens at 50 °C and converts to gray selenium at 180 °C; the transformation temperature is reduced by presence of halogens and amines.
The red α, β and γ forms are produced from solutions of black selenium by varying evaporation rates of the solvent (usually CS2). They all have relatively low, monoclinic crystal symmetries and contain nearly identical puckered Se8 rings arranged in different fashions, as in sulfur. The packing is most dense in the α form. In the Se8 rings, the Se-Se distance is 233.5 pm and Se-Se-Se angle is 105.7 degrees. Other selenium allotropes may contain Se6 or Se7 rings.
The most stable and dense form of selenium is gray and has a hexagonal crystal lattice consisting of helical polymeric chains, where the Se-Se distance is 237.3 pm and Se-Se-Se angle is 130.1°. The minimum distance between chains is 343.6 pm. Gray Se is formed by mild heating of other allotropes, by slow cooling of molten Se, or by condensing Se vapor just below the melting point. Whereas other Se forms are insulators, gray Se is a semiconductor showing appreciable photoconductivity. Unlike the other allotropes, it is insoluble in CS2. It resists oxidation by air and is not attacked by non-oxidizing acids. With strong reducing agents, it forms polyselenides. Selenium does not exhibit the unusual changes in viscosity that sulfur undergoes when gradually heated.
Isotopes.
Selenium has six naturally occurring isotopes, five of which are stable: 74Se, 76Se, 77Se, 78Se, and 80Se. The last three also occur as fission products, along with 79Se, which has a half-life of 327,000 years. The final naturally occurring isotope, 82Se, has a very long half-life (~1020 yr, decaying via double beta decay to 82Kr), which, for practical purposes, can be considered to be stable. Twenty-three other unstable isotopes have been characterized.
"See also Selenium-79" for more information on recent changes in the measured half-life of this long-lived fission product, important for the dose calculations performed in the frame of the geological disposal of long-lived radioactive waste.
Chemical compounds.
Selenium compounds commonly exist in the oxidation states −2, +2, +4, and +6.
Chalcogen compounds.
Selenium forms two oxides: selenium dioxide (SeO2) and selenium trioxide (SeO3). Selenium dioxide is formed by the reaction of elemental selenium with oxygen:
It is a polymeric solid that forms monomeric SeO2 molecules in the gas phase. It dissolves in water to form selenous acid, H2SeO3. Selenous acid can also be made directly by oxidizing elemental selenium with nitric acid:
Unlike sulfur, which forms a stable trioxide, selenium trioxide is thermodynamically unstable and decomposes to the dioxide above 185 °C:
Selenium trioxide is produced in the laboratory by the reaction of anhydrous potassium selenate (K2SeO4) and sulfur trioxide (SO3).
Salts of selenous acid are called "selenites". These include silver selenite (Ag2SeO3) and sodium selenite (Na2SeO3).
Hydrogen sulfide reacts with aqueous selenous acid to produce selenium disulfide:
Selenium disulfide consists of 8-membered rings of a nearly statistical distribution of sulfur and selenium atoms. It has an approximate composition of SeS2, with individual rings varying in composition, such as Se4S4 and Se2S6. Selenium disulfide has been use in shampoo as an anti-dandruff agent, an inhibitor in polymer chemistry, a glass dye, and a reducing agent in fireworks.
Selenium trioxide may be synthesized by dehydrating selenic acid, H2SeO4, which is itself produced by the oxidation of selenium dioxide with hydrogen peroxide:
Hot, concentrated selenic acid is capable of dissolving gold, forming gold(III) selenate.
Halogen compounds.
Iodides of selenium are not well known. The only stable chloride is selenium monochloride (Se2Cl2), which might be better known as selenium(I) chloride; the corresponding bromide is also known. These species are structurally analogous to the corresponding disulfur dichloride. Selenium dichloride is an important reagent in the preparation of selenium compounds (e.g. the preparation of Se7). It is prepared by treating selenium with sulfuryl chloride (SO2Cl2). Selenium reacts with fluorine to form selenium hexafluoride:
In comparison with its sulfur counterpart (sulfur hexafluoride), selenium hexafluoride (SeF6) is more reactive and is a toxic pulmonary irritant.
Some of the selenium oxyhalides, such as selenium oxyfluoride (SeOF2) and selenium oxychloride (SeOCl2) have been used as specialty solvents.
Selenides.
Analogous to the behavior of other chalcogens, selenium forms a dihydride H2Se. It is a strongly odiferous, toxic, and colorless gas. It is more acidic than H2S. In solution it ionizes to HSe−. The selenide dianion Se2− forms a variety of compounds, including the minerals from which selenium is obtained commercially. Illustrative selenides include mercury selenide (HgSe), lead selenide (PbSe), zinc selenide (ZnSe), and copper indium gallium diselenide (Cu(Ga,In)Se2). These materials are semiconductors. With highly electropositive metals, such as aluminium, these selenides are prone to hydrolysis:
Alkali metal selenides react with selenium to form polyselenides, Sen2-, which exist as chains.
Other compounds.
Tetraselenium tetranitride, Se4N4, is an explosive orange compound analogous to tetrasulfur tetranitride (S4N4). It can be synthesized by the reaction of selenium tetrachloride (SeCl4) with [((CH3)3Si)2N]2Se.
Selenium reacts with cyanides to yield selenocyanates:
Organoselenium compounds.
Selenium, especially in the II oxidation state, forms stable bonds to carbon, which are structurally analogous to the corresponding organosulfur compounds. Especially common are selenides (R2Se, analogues of thioethers), diselenides (R2Se2, analogues of disulfides), and selenols (RSeH, analogues of thiols). Representatives of selenides, diselenides, and selenols include respectively selenomethionine, diphenyldiselenide, and benzeneselenol. The sulfoxide in sulfur chemistry is represented in selenium chemistry by the selenoxides (formula RSe(O)R), which are intermediates in organic synthesis, as illustrated by the selenoxide elimination reaction. Consistent with trends indicated by the double bond rule, selenoketones, R(C=Se)R, and selenaldehydes, R(C=Se)H, are rarely observed.
History.
Selenium (Greek σελήνη "selene" meaning "Moon") was discovered in 1817 by Jöns Jakob Berzelius and Johan Gottlieb Gahn. Both chemists owned a chemistry plant near Gripsholm, Sweden producing sulfuric acid by the lead chamber process. The pyrite from the Falun mine created a red precipitate in the lead chambers which was presumed to be an arsenic compound, and so the pyrite's use to make acid was discontinued. Berzelius and Gahn wanted to use the pyrite and they also observed that the red precipitate gave off a smell like horseradish when burned. This smell was not typical of arsenic, but a similar odor was known from tellurium compounds. Hence, Berzelius's first letter to Alexander Marcet stated that this was a tellurium compound. However, the lack of tellurium compounds in the Falun mine minerals eventually led Berzelius to reanalyze the red precipitate, and in 1818 he wrote a second letter to Marcet describing a newly found element similar to sulfur and tellurium. Because of its similarity to tellurium, named for the Earth, Berzelius named the new element after the Moon.
In 1873, Willoughby Smith found that the electrical resistance of grey selenium was dependent on the ambient light. This led to its use as a cell for sensing light. The first commercial products using selenium were developed by Werner Siemens in the mid-1870s. The selenium cell was used in the photophone developed by Alexander Graham Bell in 1879. Selenium transmits an electric current proportional to the amount of light falling on its surface. This phenomenon was used in the design of light meters and similar devices. Selenium's semiconductor properties found numerous other applications in electronics. The development of selenium rectifiers began during the early 1930s, and these replaced copper oxide rectifiers because of their superior efficiencies. These lasted in commercial applications until the 1970s, following which they were replaced with less expensive and even more efficient silicon rectifiers.
Selenium came to medical notice later because of its toxicity to human beings working in industries. Selenium was also recognized as an important veterinary toxin, which is seen in animals that have eaten high-selenium plants. In 1954, the first hints of specific biological functions of selenium were discovered in microorganisms. Its essentiality for mammalian life was discovered in 1957. In the 1970s, it was shown to be present in two independent sets of enzymes. This was followed by the discovery of selenocysteine in proteins. During the 1980s, it was shown that selenocysteine is encoded by the codon UGA. The recoding mechanism was worked out first in bacteria and then in mammals (see SECIS element).
Occurrence.
See also: .
Native (i.e., elemental) selenium is a rare mineral, which does not usually form good crystals, but, when it does, they are steep rhombohedra or tiny acicular (hair-like) crystals. Isolation of selenium is often complicated by the presence of other compounds and elements.
Selenium occurs naturally in a number of inorganic forms, including selenide-, selenate-, and selenite-containing minerals, but these minerals are rare. The common mineral selenite is "not" a selenium mineral, and contains no selenite ion, but is rather a type of gypsum (calcium sulfate hydrate) named like selenium for the moon well before the discovery of selenium. Selenium is most commonly found quite impurely, replacing a small part of the sulfur in sulfide ores of many metals.
In living systems, selenium is found in the amino acids selenomethionine, selenocysteine, and methylselenocysteine. In these compounds, selenium plays a role analogous to that of sulfur. Another naturally occurring organoselenium compound is dimethyl selenide.
Certain solids are selenium-rich, and selenium can be bioconcentrated by certain plants. In soils, selenium most often occurs in soluble forms such as selenate (analogous to sulfate), which are leached into rivers very easily by runoff. Ocean water contains significant amounts of selenium.
Anthropogenic sources of selenium include coal burning and the mining and smelting of sulfide ores.
Production.
Selenium is most commonly produced from selenide in many sulfide ores, such as those of copper, nickel, or lead. Electrolytic metal refining is particularly conducive to producing selenium as a byproduct, and it is obtained from the anode mud of copper refineries. Another source was the mud from the lead chambers of sulfuric acid plants but this method to produce sulfuric acid is no longer used. These muds can be processed by a number of means to obtain selenium. However, most elemental selenium comes as a byproduct of refining copper or producing sulfuric acid. Since the invention of solvent extraction and electrowinning (SX/EW) for the production of copper this method takes an increasing share of the world wide copper production. This changes the availability of selenium because only a comparably small part of the selenium in the ore is leached together with the copper.
Industrial production of selenium usually involves the extraction of selenium dioxide from residues obtained during the purification of copper. Common production from the residue then begins by oxidation with sodium carbonate to produce selenium dioxide. The selenium dioxide is then mixed with water and the solution is acidified to form selenous acid (oxidation step). Selenous acid is bubbled with sulfur dioxide (reduction step) to give elemental selenium.
About 2,000 tonnes of selenium was produced in 2011 worldwide, mostly in Germany (650 t), Japan (630 t), Belgium (200 t) and Russia (140 t), and the total reserves were estimated at 93,000 tonnes. These data however exclude two major producers, the United States and China. The price was relatively stable during 2004–2010 at ~30 US dollars per pound (per 100-pound lot) but increased to 65 $/lb in 2011. A previous sharp increase was observed in 2004 from 4–5 to 27 $/lb. The consumption in 2010 was divided as follows: metallurgy – 30%, glass manufacturing – 30%, agriculture – 10%, chemicals and pigments – 10%, electronics – 10%. China is the dominant consumer of selenium at 1,500–2,000 tonnes/year.
Applications.
Manganese electrolysis.
During the electro winning of manganese an addition of selenium dioxide decreases the power necessary to operate the electrolysis cells. China is the largest consumer of selenium dioxide for this purpose. For every tonne of manganese an average of 2 kg selenium oxide is used. 
Glass production.
The largest commercial use of Se, accounting for about 50% of consumption, is for the production of glass. Se compounds confer a red color to glass. This color cancels out the green or yellow tints that arise from iron impurities that are typical for most glass. For this purpose various selenite and selenate salts are added. For other applications, the red color may be desirable, in which case mixtures of CdSe and CdS are added.
Alloys.
Selenium is used with bismuth in brasses to replace more toxic lead. The regulation of lead in drinking water applications with the Safe Drinking Water Act of 1974 made a reduction of lead in brass necessary. The new brass is marketed under the name EnviroBrass. Like lead and sulfur, selenium improves the machinability of steel at concentrations of about 0.15%. The same improvement is also observed in copper alloys and therefore selenium is also used in machinable copper alloys.
Solar cells.
Copper indium gallium selenide is a material used in the production of solar cells.
Other uses.
Small amounts of organoselenium compounds are used to modify the vulcanization catalysts used in the production of rubber.
The demand for selenium by the electronics industry is declining, despite a number of continuing applications. Because of its photovoltaic and photoconductive properties, selenium is used in photocopying, photocells, light meters and solar cells. Its use as a photoconductor in plain-paper copiers once was a leading application but in the 1980s, the photoconductor application declined (although it was still a large end-use) as more and more copiers switched to the use of organic photoconductors. It was once widely used in selenium rectifiers. These uses have mostly been replaced by silicon-based devices or are in the process of being replaced. The most notable exception is in power DC surge protection, where the superior energy capabilities of selenium suppressors make them more desirable than metal oxide varistors.
Zinc selenide was the first material for blue LEDs but gallium nitride is dominating the market now. Cadmium selenide has played an important part in the fabrication of quantum dots. Sheets of amorphous selenium convert x-ray images to patterns of charge in xeroradiography and in solid-state, flat-panel x-ray cameras.
Selenium is a catalyst in some chemical reactions but it is not widely used because of issues with toxicity. In X-ray crystallography, incorporation of one or more selenium atoms in place of sulfur helps with Multi-wavelength anomalous dispersion and Single wavelength anomalous dispersion phasing.
Selenium is used in the toning of photographic prints, and it is sold as a toner by numerous photographic manufacturers. Its use intensifies and extends the tonal range of black-and-white photographic images and improves the permanence of prints.
75Se is used as a gamma source in industrial radiography.
Biological role.
Although it is toxic in large doses, selenium is an essential micronutrient for animals. In plants, it occurs as a bystander mineral, sometimes in toxic proportions in forage (some plants may accumulate selenium as a defense against being eaten by animals, but other plants such as locoweed require selenium, and their growth indicates the presence of selenium in soil). See more on plant nutrition below.
Selenium is a component of the unusual amino acids selenocysteine and selenomethionine. In humans, selenium is a trace element nutrient that functions as cofactor for reduction of antioxidant enzymes, such as glutathione peroxidases and certain forms of thioredoxin reductase found in animals and some plants (this enzyme occurs in all living organisms, but not all forms of it in plants require selenium).
The glutathione peroxidase family of enzymes (GSH-Px) catalyze certain reactions that remove reactive oxygen species such as hydrogen peroxide and organic hydroperoxides:
Selenium also plays a role in the functioning of the thyroid gland and in every cell that uses thyroid hormone, by participating as a cofactor for the three of the four known types of thyroid hormone deiodinases, which activate and then deactivate various thyroid hormones and their metabolites: the iodothyronine deiodinases are the subfamily of deiodinase enzymes that use selenium as the otherwise rare amino acid selenocysteine. (Only the deiodinase iodotyrosine deiodinase, which works on the last break-down products of thyroid hormone, does not use selenium.)
Selenium may inhibit Hashimoto's disease, in which the body's own thyroid cells are attacked as alien. A reduction of 21% on TPO antibodies was reported with the dietary intake of 0.2 mg of selenium.
Increased dietary selenium intakes reduce the effects of mercury toxicity, although this protective effect is only apparent at low to modest doses of mercury. Evidence suggests that the molecular mechanisms of mercury toxicity includes the irreversible inhibition of selenoenzymes that are required to prevent and reverse oxidative damage in brain and endocrine tissues.
Evolution in biology.
From about three billion years ago, prokaryotic selenoprotein families drive the evolution of selenocysteine, an amino acid. Selenium is incorporated into several prokaryotic selenoprotein families in bacteria, archaea and eukaryotes as selenocysteine, where selenoprotein peroxiredoxins protect bacterial and eukaryotic cells against oxidative damage. Selenoprotein families of GSH-Px and the deiodinases of eukaryotic cells seem to have a bacterial phylogenetic origin. The selenocysteine-containing form occurs in species as diverse as green algae, diatoms, sea urchin, fish, and chicken. Selenium enzymes are involved in utilization of the small reducing molecules glutathione and thioredoxin. One family of selenium-containing molecules (the glutathione peroxidases) destroy peroxide and repair damaged peroxidized cell membranes, using glutathione. Another selenium-containing enzyme in some plants and in animals (thioredoxin reductase) generates reduced thioredoxin, a dithiol that serves as an electron source for peroxidases and also the important reducing enzyme ribonucleotide reductase that makes DNA precursors from RNA precursors.
Trace elements involved in GSH-Px and superoxide dismutase enzymes activities, i.e. selenium, vanadium, magnesium, copper, and zinc, may have been lacking in some terrestrial mineral-deficient areas. Marine organisms retained and sometimes expanded their seleno-proteomes, whereas the seleno-proteomes of some terrestrial organisms were reduced or completely lost. These findings suggest that, with the exception of vertebrates, aquatic life supports selenium utilization, whereas terrestrial habitats lead to reduced use of this trace element. Marine fishes and vertebrate thyroid glands have the highest concentration of selenium and iodine. From about 500 Mya, freshwater and terrestrial plants slowly optimized the production of "new" endogenous antioxidants such as ascorbic acid (Vitamin C), polyphenols (including flavonoids), tocopherols, etc. A few of these appeared more recently, in the last 50–200 million years, in fruits and flowers of angiosperm plants. In fact, the angiosperms (the dominant type of plant today) and most of their antioxidant pigments evolved during the late Jurassic period.
The deiodinase isoenzymes constitute another family of eukaryotic selenoproteins with identified enzyme function. Deiodinases are able to extract electrons from iodides, and iodides from iodothyronines. They are, thus, involved in thyroid-hormone regulation, participating in the protection of thyrocytes from damage by H2O2 produced for thyroid-hormone biosynthesis. About 200 Mya, new selenoproteins were developed as mammalian GSH-Px enzymes.
Nutritional sources of selenium.
Dietary selenium comes from nuts, cereals, meat, mushrooms, fish, and eggs. Brazil nuts are the richest ordinary dietary source (though this is soil-dependent, since the Brazil nut does not require high levels of the element for its own needs). In descending order of concentration, high levels are also found in kidney, tuna, crab, and lobster.
Selenium as a dietary supplement is available in many forms, including multi-vitamins. In 2013 the U.S. Food and Drug Administration (FDA) proposed the requirement of minimum and maximum levels of selenium in infant formula.
The human body's content of selenium is believed to be in the 13–20 milligram range.
Indicator plant species.
Certain species of plants are considered indicators of high selenium content of the soil, since they require high levels of selenium to thrive. The main selenium indicator plants are "Astragalus" species (including some locoweeds), prince's plume ("Stanleya" sp.), woody asters ("Xylorhiza" sp.), and false goldenweed ("Oonopsis" sp.)
Medical use.
The substance loosely called selenium sulfide (approximate formula SeS2) is the active ingredient in some anti-dandruff shampoos. The selenium compound kills the scalp fungus "Malassezia", which causes shedding of dry skin fragments. The ingredient is also used in body lotions to treat tinea versicolor due to infection by a different species of "Malassezia" fungus.
Detection in biological fluids.
Selenium may be measured in blood, plasma, serum or urine to monitor excessive environmental or occupational exposure, confirm a diagnosis of poisoning in hospitalized victims or to assist in a forensic investigation in a case of fatal overdosage. Some analytical techniques are capable of distinguishing organic from inorganic forms of the element. Both organic and inorganic forms of selenium are largely converted to monosaccharide conjugates (selenosugars) in the body prior to being eliminated in the urine. Cancer patients receiving daily oral doses of selenothionine may achieve very high plasma and urine selenium concentrations.
Toxicity.
Although selenium is an essential trace element, it is toxic if taken in excess. Exceeding the Tolerable Upper Intake Level of 400 micrograms per day can lead to selenosis. This 400 µg Tolerable Upper Intake Level is based primarily on a 1986 study of five Chinese patients who exhibited overt signs of selenosis and a follow up study on the same five people in 1992. The 1992 study actually found the maximum safe dietary Se intake to be approximately 800 micrograms per day (15 micrograms per kilogram body weight), but suggested 400 micrograms per day to not only avoid toxicity, but also to avoid creating an imbalance of nutrients in the diet and to account for data from other countries. In China, people who ingested corn grown in extremely selenium-rich stony coal (carbonaceous shale) have suffered from selenium toxicity. This coal was shown to have selenium content as high as 9.1%, the highest concentration in coal ever recorded in literature.
Signs and symptoms of selenosis include a garlic odor on the breath, gastrointestinal disorders, hair loss, sloughing of nails, fatigue, irritability, and neurological damage. Extreme cases of selenosis can result in cirrhosis of the liver, pulmonary edema, and death. Elemental selenium and most metallic selenides have relatively low toxicities because of their low bioavailability. By contrast, selenates and selenites are very toxic, having an oxidant mode of action similar to that of arsenic trioxide. The chronic toxic dose of selenite for humans is about 2400 to 3000 micrograms of selenium per day for a long time. Hydrogen selenide is an extremely toxic, corrosive gas. Selenium also occurs in organic compounds, such as dimethyl selenide, selenomethionine, selenocysteine and methylselenocysteine, all of which have high bioavailability and are toxic in large doses.
On 19 April 2009, 21 polo ponies died shortly before a match in the United States Polo Open. Three days later, a pharmacy released a statement explaining that the horses had received an incorrect dose of one of the ingredients used in a vitamin/mineral supplement compound that had been incorrectly compounded by a compounding pharmacy. Analysis of blood levels of inorganic compounds in the supplement indicated the selenium concentrations were ten to fifteen times higher than normal in the horses' blood samples, and 15 to 20 times higher than normal in their liver samples. It was later confirmed that selenium was the ingredient in question.
Selenium poisoning of water systems may result whenever new agricultural runoff courses through normally dry, undeveloped lands. This process leaches natural soluble selenium compounds (such as selenates) into the water, which may then be concentrated in new "wetlands" as the water evaporates. Selenium pollution to waterways also occurs from leaching of selenium from coal flue ash, mining and metal smelting, crude oil processing and landfill. The resulting high selenium levels in waterways have been found to have caused certain congenital disorders in oviparous species such as wetland birds, and fish. Elevated dietary methylmercury levels can enhance the negative effects of selenium toxicity in oviparous species.
In fish and other wildlife, low levels of selenium cause deficiency while high levels cause toxicity. For example, in salmon, the optimal concentration of selenium in the fish tissue (whole body) is about 1 microgram selenium per gram of tissue (dry weight). At levels much below that concentration, young salmon die from selenium deficiency; much above that level they die from toxic excess.
Deficiency.
Selenium deficiency is rare in healthy, well-nourished individuals. It can occur in patients with severely compromised intestinal function, those undergoing total parenteral nutrition, and in those of advanced age (over 90). Also, people dependent on food grown from selenium-deficient soil are at risk. Although New Zealand has low levels of selenium in its soil, adverse health effects have not been detected.
Selenium deficiency as defined by low (<60% of normal) selenoenzyme activity levels in brain and endocrine tissues occurs only when a low selenium status is linked with an additional stress, such as high exposures to mercury or as a result of increased oxidant stress due to vitamin E deficiency.
There are interactions between selenium and other nutrients, such as iodine and vitamin E. The effect of selenium deficiency on health remains uncertain, particularly in relation to Kashin-Beck disease. Also, there are interactions between selenium and other minerals, such as zinc and copper. It seems that a high dose of Se supplements to pregnant animals might disturb the Zn:Cu ratio which, in turn, leads to Zn reduction. It can be concluded that the Zn status should be monitored when high doses of Se are supplemented to pregnant animals. Further studies need to be done with higher levels of Se supplement to conﬁrm these interactions.
In some regions (e.g. various regions within North America) where low available selenium levels in soil lead to low concentrations in dry matter of plants, Se deficiency in some animal species may occur unless dietary (or injected) selenium supplementation is done. Ruminants are particularly susceptible. In general, absorption of dietary selenium is lower in ruminants than in non-ruminants, and is lower from forages than from grain. Ruminants grazing certain forages, e.g. some white clover varieties containing cyanogenic glycosides, may have higher selenium requirements, presumably because of cyanide from the aglycone released by glucosidase activity in the rumen and inactivation of glutathione peroxidases due to absorbed cyanide's effect on the glutathione moiety. Neonate ruminants at risk of WMD (white muscle disease) may be administered both selenium and vitamin E by injection; some of the WMD myopathies respond only to selenium, some only to vitamin E, and some to either.
Controversial health effects.
A number of correlative epidemiological studies have implicated selenium deficiency (as measured by blood levels) in a number of serious or chronic diseases, such as cancer, diabetes, HIV/AIDS, and tuberculosis. In addition, selenium supplementation has been found to be a chemopreventive for some types of cancer in some types of rodents. However, in randomized, blinded, controlled prospective trials in humans, selenium supplementation has not succeeded in reducing the incidence of any disease, nor has a meta-analysis of such selenium supplementation studies detected a decrease in overall mortality.

</doc>
<doc id="27261" url="http://en.wikipedia.org/wiki?curid=27261" title="Demographics of São Tomé and Príncipe">
Demographics of São Tomé and Príncipe

This article is about the demographic features of the population of São Tomé and Príncipe, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
Of São Tomé and Príncipe's total population, about 131,000 live on São Tomé and 6,000 on Príncipe. All are descended from various ethnic groups that have migrated to the islands since 1485. Six groups are identifiable:
Although a small country, São Tomé and Príncipe has four national languages: Portuguese (the official language, spoken by 95% of the population), and the Portuguese-based creoles Forro (85%), Angolar (3%) and Principense (0.1%). French is also learned in schools, as the country is a member of Francophonie.
In the 1970s, there were two significant population movements—the exodus of most of the 4,000 Portuguese residents and the influx of several hundred São Toméan refugees from Angola. The islanders have been absorbed largely into a common Luso-African culture. Almost all belong to the Roman Catholic, Evangelical Protestant, or Seventh-day Adventist churches, which in turn retain close ties with churches in Portugal. There is a small but growing Muslim population.
Population.
According to the 2010 revison of the World Population Prospects the total population was 165,000 in 2010, compared to only 60,000 in 1950. The proportion of children below the age of 15 in 2010 was 40.3%, 55.8% was between 15 and 65 years of age, while 3.9% was 65 years or older
Vital statistics.
Registration of vital events is in São Tomé & Príncipe not available for recent years. The Population Departement of the United Nations prepared the following estimates.
CIA World Factbook demographic statistics.
The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.
Sex ratio.
<br>"at birth:"
1.03 male(s)/female
<br>"under 15 years:"
1.03 male(s)/female
<br>"15–64 years:"
0.93 male(s)/female
<br>"65 years and over:"
0.84 male(s)/female
<br>"total population:"
0.97 male(s)/female (2000 est.)
Life expectancy at birth.
<br>"total population:"
65.25 years
<br>"male:"
63.84 years
<br>"female:"
66.7 years (2000 est.)
Nationality.
<br>"noun:"
São Toméan(s)
<br>"adjective:"
São Toméan
Ethnic groups.
Mestiços, angolares (descendants of Angolan slaves), forros (descendants of freed slaves), serviçais (contract laborers from Angola, Mozambique, and Cape Verde), tongas (children of serviçais born on the islands) and Europeans (primarily Portuguese)
Religions.
Christian 97% (Roman Catholic, Evangelical Protestant, Seventh-Day Adventist), Muslim 2%, other 1%
Languages.
Portuguese (official)
Literacy.
<br>"definition:"
age 15 and over can read and write
<br>"total population:"
73%
<br>"male:"
85%
<br>"female:"
62% (1991 est.)

</doc>
<doc id="27262" url="http://en.wikipedia.org/wiki?curid=27262" title="Politics of São Tomé and Príncipe">
Politics of São Tomé and Príncipe

The politics of São Tomé and Príncipe takes place in a framework of a unitary semi-presidential representative democratic republic, whereby the President of São Tomé and Príncipe is head of state and the Prime Minister of São Tomé and Príncipe is head of government, and of a multi-party system. Executive power is exercised by the government. Legislative power is vested in both the government and the National Assembly. The Judiciary is independent of the executive and the legislature. São Tomé has functioned under a multiparty system since 1990. Following the promulgation of a new constitution in 1990, São Tomé and Príncipe held multiparty elections for the first time since independence. Shortly after the constitution took effect, the National Assembly formally legalized opposition parties. Independent candidates also were permitted to participate in the January 1991 legislative elections.
Executive branch.
The president of the republic is elected to a five-year term by direct universal suffrage and a secret ballot, and may hold up to two consecutive terms. Candidates are chosen at their party's national conference (or individuals may run independently). A presidential candidate must obtain an outright majority of the popular vote in either a first or second round of voting in order to be elected president. The prime minister is named by the president but must be ratified by the majority party and thus normally comes from a list of its choosing. The prime minister, in turn, names the 14 members of the cabinet.
Legislative branch.
The National Assembly ("Assembleia Nacional") has 55 members, elected for a four-year term in seven multi-member constituencies by proportional representation. It is the supreme organ of the state and the highest legislative body, and meets semiannually.
Judicial branch.
Justice is administered at the highest level by the Supreme Court. Formerly responsible to the National Assembly, the judiciary is now independent under the new constitution.
Administrative divisions.
Administratively, the country is divided into seven municipal districts, six on São Tomé and one comprising Príncipe. Governing councils in each district maintain a limited number of autonomous decision-making powers, and are reelected every 5 years. Príncipe has had self-government since 29 April 1995
Human rights and democracy.
Since the constitutional reforms of 1990 and the elections of 1991, São Tomé and Príncipe has made great strides toward developing its democratic institutions and further guaranteeing the civil and human rights of its citizens. São Toméans have freely changed their government through peaceful and transparent elections, and while there have been disagreements and political conflicts within the branches of government and the National Assembly, the debates have been carried out and resolved in open, democratic, and legal fora, in accordance with the provisions of São Toméan law. A number of political parties actively participate in government and openly express their views. Freedom of the press is respected, and there are several independent newspapers in addition to the government bulletin. The government's respect for human rights is exemplary; the government does not engage in repressive measures against its citizens, and respect for individuals' rights to due process and protection from government abuses is widely honored. Freedom of expression is accepted, and the government has taken no repressive measures to silence critics.
A briefly successful coup d'état led by Major Fernando "Cobo" Pereira took place on 16 July 2003.
International organization participation.
The country is member of the ACCT, ACP, AfDB, CEEAC, ECA, FAO, G-77, IBRD, ICAO, ICRM, IDA, IFAD, IFRCS, ILO, IMF, International Maritime Organization, Intelsat (nonsignatory user), Interpol, IOC, IOM (observer), ITU, NAM, OAU, United Nations, UNCTAD, UNESCO, UNIDO, UPU, WHO, WIPO, WMO, World Tourism Organization, World Trade Organization (applicant)

</doc>
<doc id="27274" url="http://en.wikipedia.org/wiki?curid=27274" title="Telecommunications in Saudi Arabia">
Telecommunications in Saudi Arabia

This article is about telecommunications in Saudi Arabia which includes fixed and cellular phones, internet as well as radio and television broadcasting and issues relating to the provision of these services.
Saudi Telecom Company (STC) is the first company in Saudi Arabia, and then allowed Communications Commission to compete with other companies in Saudi Arabia and then it becomes a number of telecom companies in Saudi Arabia four companies: (1). STC Mobile: It STC includes landlines and mobile, and includes a mobile (phone), (Sawa) and (us). (2) Integrated Telecom Company (ITC) second operator after STC, established in 2005 and offers internet, broadband, connectivity and satellite services for businesses, consumers and wholesale (3). Mobily: the UAE's telecommunications company, which is the mobile and internet Fabraupetk (Fiber Optic) New Ground. (4). ZIN Zain: a Kuwaiti company, which is the only mobile (5). GO ATHEEB: a Saudi modern, with an Internet connection line is similar to Ground.
NOTE:All telecommunications companies are here in Saudi Arabia, high prices compared to the Gulf or global. The individual citizen or resident assigned contact Ground equivalent to 0.6 SR Mobile while lower prices all the individual companies cost the equivalent of 0.35 SR per minute. For example, compared to the global cost of the individual in India for Mobile and not to the Ground (0.5) equivalent to Rs (0.02) SR.
Telephones.
Telephones - main lines in use:
4,633,158 (2011 - source: ITU)
Telephones - mobile cellular:
53,705,808 (2011 - source: ITU) 
<br>"note:"
In 2004, the Saudi Telecom Company (STC) monopolization was broken by authorizing Etihad Etisalat/Mobily to compete in mobile communication, in June 2009 GO Telecom (Etihad Atheeb Telecom) or "جو" also entered the market.
Digital Radio Trunking- 100,000 (Unofficial):
<br>In late 2005, bravO! Telecom was launched as the country's digital radio trunking operator under a B.O.T agreement with the incumbent operator STC, with an estimated 100,000 subscribers as of Nov'07.
Telephone system:
<br>"domestic:"
extensive microwave radio relay and coaxial and fiber-optic cable systems
"international:"
<br>
International undersea cables: EIG, I-ME-WE, FEA, MENA, SEA-ME-WE 3, SEA-ME-WE 4, FALCON, SAS-1, SAS-2, Gulf Bridge International, TATA TGN-Gulf
Microwave radio relay to Bahrain, Jordan, Kuwait, Qatar, UAE, Yemen, and Sudan;
Coaxial cable to Kuwait and Jordan; 
Satellite earth stations - 5 Intelsat (3 Atlantic Ocean and 2 Indian Ocean), 1 Arabsat, and 1 Inmarsat (Indian Ocean region)
Radio.
Radio broadcast stations:
AM 43, FM 31, shortwave 2 (1998)
Radios:
6.25 million (1997)
Television.
Television broadcast stations:
117 (1997)
Televisions:
5.1 million (1997)
ISPs.
Internet Service Providers (ISPs):
22 (2005)
Internet Users:
13,000,000 (2012)
Country code (Top level domain): .sa
Saudi Post
Broadband Internet access.
An ADSL service in Saudi Arabia has become available since 2001. As part of its monopoly on all methods of communication in Saudi Arabia, the Saudi Telecom Company is the only complete provider, though several ISPs are available, with the permission of STC.
STC is the only provider for telephone lines in Saudi Arabia. For this reason customers have to pay two fees, one to STC for activation of the ADSL service across the telephone line, and a second to an ISP to provide Internet service across the ADSL line. STC has been highly criticized for their service in providing ADSL access as customers had to wait many months to receive ADSL service on their phone lines. In 2006, STC had invested in increasing the size of their ADSL infrastructure and since then the wait times had improved, but many customers are still on waiting lists.
As of 2011, the maximum Internet speed available is currently 200 Mbit/s.
As of October 2006, the 20 Internet service providers in Saudi Arabia became connected through "Data service provider" -licensed companies, such as Saudi Telecom Company and Integrated Telecom Company. The ISPs were previously connected through KACST (King Abdulaziz City for Science and Technology), who was also responsible for DNS and filtering traffic. Since October 2006 the Communications and Information Technology Commission is responsible for DNS and filtering services.
There are several reasons for the service being unpopular, in particular the unreasonably expensive prices and incompetent low-quality service. Perhaps this was proved when STC submitted a request to enter neighboring Egypt as a provider and was refused due to lack of experience, staff, equipments and such.
In late 2005 it was announced that a company by the name of Electronet would start providing broadband connections through electric lines by mid-2006. However, as of early 2009 the service has not been implemented and the company's website has been idle for years. Some of the developed countries are having trouble implementing internet connections via power lines so it is highly unlikely this technology will ever appear in Saudi Arabia. Electronet is dissolved
There are huge public complaints about the poor Internet services in Saudi Arabia all of which falls on deaf ears since there is no reason or force pressuring the monopoly.

</doc>
<doc id="27342" url="http://en.wikipedia.org/wiki?curid=27342" title="Politics of Slovenia">
Politics of Slovenia

The politics of Slovenia takes place in a framework of a parliamentary representative democratic republic, whereby the Prime Minister of Slovenia is the head of government, and of a multi-party system. Executive power is exercised by the Government of Slovenia. Legislative power is vested in the National Assembly and in minor part in the National Council. The judiciary of Slovenia is independent of the executive and the legislature.
Political developments.
As a young independent republic, Slovenia pursued economic stabilization and further political openness, while emphasizing its Western outlook and central European heritage. Today, with a growing regional profile, a participant in the SFOR peacekeeping deployment in Bosnia and the KFOR deployment in Kosovo, and a charter World Trade Organization member, Slovenia plays a role on the world stage quite out of proportion to its small size.
From 1998 to 2000, Slovenia occupied a nonpermanent seat on the UN Security Council and in that capacity distinguished itself with a constructive, creative, and consensus-oriented activism. Slovenia has been a member of the United Nations since May 1992 and of the Council of Europe since May 1993. Slovenia signed an association agreement with the European Union in 1996 and is a member of the Central European Free Trade Agreement. Slovenia also is a member of all major international financial institutions (the International Monetary Fund, the World Bank Group, and the European Bank for Reconstruction and Development) as well as 40 other international organizations, among them the World Trade Organization, of which it is a founding member.
Since the breakup of the former Yugoslavia, Slovenia has instituted a stable, multi-party, democratic political system, characterized by regular elections, a free press, and an excellent human rights record. However, Slovenia is the only former Communist state that has never carried out lustration. By Constitution of Slovenia the country is a parliamentary democracy and a republic. Within its government, power is shared between a directly elected president, a prime minister, and an incompletely bicameral legislature. The legislative body is composed of the 90-member National Assembly—which takes the lead on virtually all legislative issues—and the National Council, a largely advisory body composed of representatives from social, economic, professional, and local interests. The Constitutional Court has the highest power of review of legislation to ensure its consistency with Slovenia's constitution. Its nine judges are elected for 9-year terms.
In 1997, elections were held to elect both a president and representatives to Parliament's upper house, the National Council. Milan Kučan, elected President of the Yugoslav Republic of Slovenia in 1990, led his country to independence in 1991. He was elected the first President of independent Slovenia in 1992 and again in November 1997 by a comfortable margin.
Janez Drnovšek of the center-left Liberal Democratic Party of Slovenia (LDS) was reelected Prime Minister in the 15 October 2000 parliamentary elections. Drnovšek's coalition held an almost two-thirds majority in Parliament.
The government, most of the Slovenian polity, shares a common view of the desirability of a close association with the West, specifically of membership in both the European Union and NATO. For all the apparent bitterness that divides left and right wings, there are few fundamental philosophical differences between them in the area of public policy. Slovenian society is built on consensus, which has converged on a social-democrat model. Political differences tend to have their roots in the roles that groups and individuals played during the years of communist rule and the struggle for independence.
As the most prosperous republic of the former Yugoslavia, Slovenia emerged from its brief ten-day war of secession in 1991 as an independent nation for the first time in its history. Since that time, the country has made steady but cautious progress toward developing a market economy. Economic reforms introduced shortly after independence led to healthy economic growth. Despite the halting pace of reform and signs of slowing GDP growth today, Slovenians now enjoy the highest per capita income of all the transition economies of central Europe.
The Slovenians have pursued internal economic restructuring with caution. The first phase of privatization (socially owned property under the SFRY system) is now complete, and sales of remaining large state holdings are planned for next year. Trade has been diversified toward the West (trade with EU countries make up 66% of total trade in 2000) and the growing markets of central and eastern Europe. Manufacturing accounts for most employment, with machinery and other manufactured products comprising the major exports. Labor force surveys put unemployment at approximately 6.6% (Dec. 2000), with 106,153 registrations for unemployment assistance. Inflation has remained below double-digit levels, 6.1% (1999) and 8.9% (2000). Gross domestic product grew by about 4.8% in 2000 and is expected to post a slightly lower rate of 4.5% in 2001, as export demand lags. The currency is stable, fully convertible, and backed by substantial reserves. The economy provides citizens with a good standard of living.
Ten years after independence, Slovenia has made tremendous progress establishing democratic institutions, enshrining respect for human rights, establishing a market economy and adapting its military to Western norms and standards. In contrast to its neighbors, civil tranquility and strong economic growth have marked this period. Upon achieving independence, Slovenia offered citizenship to all residents, regardless of ethnicity or origin, avoiding a sectarian trap that has caught out many central European countries. Slovenia willingly accepted refugees from the fighting in Bosnia and has since participated in international stabilization efforts in the region.
On the international front, Slovenia has advanced rapidly toward integration into the Euro-Atlantic community of nations. Invited to begin accession negotiations with the European Union in November 1998, Slovenia has achieve two of its primary foreign policy goals: membership in the EU and NATO. Slovenia also participates in the Southeast Europe Cooperation Initiative (SECI).
Slovenia remains firmly committed to achieving NATO membership in a second round of enlargement. Slovenia has been an active participant in Partnership for Peace (PfP) and has sought to demonstrate its preparedness to take on the responsibilities and burdens of membership in the Alliance. The United States looks to Slovenia to play a productive role in continuing security efforts throughout the region. It has done much– contributing to the success of IFOR, SFOR, efforts in Albania, the Republic of Macedonia, Montenegro, Kosovo, and elsewhere– and has continued to expand actively its constructive regional engagement.
Slovenia is one of the focus countries for the United States' southeast European policy, aimed at reinforcing regional stability and integration. The Slovenian Government is well-positioned to be an influential role model for other southeast European governments at different stages of reform and integration. To these ends, the United States urges Slovenia to maintain momentum on internal economic, political, and legal reforms, while expanding their international cooperation as resources allow. Although harmonization with EU law and standards will require great efforts, already underway, the EU accession process will serve to advance Slovenia's structural reform agenda. U.S. and Allied efforts to assist Slovenia's military restructuring and modernization efforts are ongoing.
Constitution.
The constitution was adopted on 23 December 1991, effective 23 December 1991.
Executive branch.
The president is elected by popular vote for a five-year term. Following National Assembly elections, the leader of the majority party or the leader of a majority coalition is usually nominated to become prime minister by the president and elected by the National Assembly. The Council of Ministers is nominated by the prime minister and elected by the National Assembly.
Legislative branch.
The National Assembly ("Državni zbor") has 90 members, elected for a four-year term, 88 members elected by proportional representation using D'Hondt formula and 2 members elected by ethnic minorities using the Borda count.
The President of the National Assembly of Slovenia is elected by the deputies and requires 46 votes to be elected. Currently, this position is held by Gregor Virant, who won against his opponent Maša Kociper with 52 votes against 38. Virant was supported by the Slovenian Democratic Party, the Democratic Party of Pensioners of Slovenia, New Slovenia – Christian People's Party, the Slovenian People's Party and his own party the Gregor Virant's Civic List.
Administrative divisions.
Slovenia is divided into 211 municipalities, of which 11 are urban municipalities with a greater degree of autonomy.
International organization participation.
Slovenia is member of BIS, CCC, CE, CEI, EAPC, EBRD, ECE, EU, FAO, IADB, IAEA, IBRD, ICAO, ICC, ICRM, IDA, IFC, IFRCS, ILO, IMF, IMO, Intelsat (nonsignatory user), Interpol, IOC, IOM (observer), ISO, ITU, NAM (guest), NATO, OPCW, OSCE, PCA, PFP, SECI, UN, UNCTAD, UNESCO, UNFICYP, UNIDO, UNTSO, UPU, WEU (associate partner), WHO, WIPO, WMO, WToO, WTrO

</doc>
<doc id="27367" url="http://en.wikipedia.org/wiki?curid=27367" title="Foreign relations of Somalia">
Foreign relations of Somalia

Foreign relations of Somalia are handled by the President as the head of state, the Prime Minister as the head of government, and the Minister of Foreign Affairs of the Federal Government of Somalia.
Northeast Africa.
Djibouti.
As the headquarters of the Intergovernmental Authority on Development regional body, Djibouti has been an active participant in the Somalian peace process. It hosted the Arta conference in 2000, as well as the 2008-2009 talks between the Transitional Federal Government and the Alliance for the Reliberation of Somalia, which led to the formation of a coalition government. In 2011, Djibouti joined the African Union Mission to Somalia. Following the establishment of the Federal Government of Somalia in 2012, a Djibouti delegation also attended the inauguration ceremony of Somalia's new president.
Ethiopia.
Relations between the peoples of Somalia and Ethiopia stretch back to antiquity, to a common origin. The Ethiopian region is one of the proposed homelands of the Horn of Africa's various Afro-Asiatic communities.
During the Middle Ages, Imam Ahmad ibn Ibrihim al-Ghazi (Ahmad Gurey or Gragn) led a Conquest of Abyssinia ("Futuh al-Habash"), which brought three-quarters of the Christian Ethiopian Empire under the power of the Muslim Adal Sultanate. With an army mainly composed of Somalis, Many historians trace the origins of tensions between Somalia and Ethiopia to this war.
In the 1960s and 1970s, a territorial dispute over the Ogaden region led to various armed confrontations between the Somali
and Ethiopian militaries. The tensions culminated in the Ogaden War, which saw the Somali army capture most of the disputed territory by September 1977, before finally being expelled by a coalition of communist forces.
With changes in leadership in the early 1990s brought on by the start of the Somali Civil War and Ethiopian Civil War, respectively, relations between the Somali and Ethiopian authorities entered a new phase of military cooperation against the Islamic Courts Union (ICU) rebel group and its more radical successor Al-Shabaab. In October 2011, a coordinated multinational operation began against Al-Shabaab in southern Somalia; the Ethiopian military eventually joined the Transitional Federal Government-led mission the following month.
The Federal Government of Somalia was later established on August 20, 2012, representing the first permanent central government in the country since the start of the civil war. The following month, Hassan Sheikh Mohamud was elected as the new Somali government's first President, with the Ethiopian authorities welcoming his selection and newly appointed Prime Minister of Ethiopia Hailemariam Desalegn attending Mohamud's inauguration ceremony.
Egypt.
Relations between the territories of present-day Egypt and Somalia stretch back to antiquity. In the Middle Ages and early modern era, the various Somali Sultanates also maintained close relations with their counterparts in Egypt.
During the ensuing colonial period, Egypt and Somalia kept close ties through the UN delegate to Somalia Kamal El Din Salah, who supported the territorial integrity of the Somali territories. Upon independence of the Somali Republic in 1960, Egypt was among the first nations to recognize the nascent country. It subsequently invested heavily in the education sector, with Cairo's Al-Azhar University leading scholastic and Muslim missions in Mogadishu, among other areas. In 1969, Somalia and Egypt were among the founding members of the Organisation of Islamic Cooperation (OIC). Both nations are also members of the League of Arab States.
After the start of the civil war in Somalia in 1991, Egypt maintained diplomatic relations with the Transitional National Government and its successor the Transitional Federal Government, and supported their state-building initiatives. As part of the International Contact Group, the Egyptian authorities participated in various global summits in support of the Somalian peace process, including the Khartoum Conference in 2006, the Djibouti Conference in 2008, and the Cairo Conference in 2010. It also organized diplomatic training for Somali government officials in conjunction with the Somali Institute for Diplomatic Studies.
The subsequent establishment of the Federal Government of Somalia in August 2012 was welcomed by the Egyptian authorities, who re-affirmed Egypt's continued support for Somalia's government, its territorial integrity and sovereignty.
Europe.
Denmark.
Diplomatic relations between Somalia and Denmark were established on 9 July 1960, shortly after the Somali Republic's independence.
During the Siad Barre administration, Somalia and Denmark strengthened cooperation. The Danish International Development Agency agreed to provide a $1.4 million loan toward the development of Somalia's northern fisheries industry. Additionally, the Somalian and Danish foreign ministries signed a loan agreement in 1981, wherein 45 million DKK ($8,284,410.00 USD) was issued to Somalia to finance imports of Danish capital goods, as well as local cost expenditures and purchases of Danish capital equipment and services.
In September 1992, Danish Foreign Minister Uffe Ellemann Jensen and other senior officials visited southern Somalia, one of the first foreign delegations to do so since the start of the civil war the year before. Although the Danish embassy in Mogadishu closed down operations, the Danish authorities in the ensuing years maintained relations with Somalia's newly established Transitional National Government and its successor the Transitional Federal Government.
The subsequent establishment of the Federal Government of Somalia in August 2012 was welcomed by the Danish authorities, who re-affirmed Denmark's continued support for Somalia's government, its territorial integrity and sovereignty. In December 2013, the Danish government appointed Geert Aagaard Andersen as the new Danish Ambassador to Somalia, the first in twenty years. Andersen presented his credentials to Somalian President Hassan Sheikh Mohamud at a ceremony in Mogadishu.
France.
Bilateral relations between France and Somalia were established shortly after Somalia's independence. The French government opened an embassy in Mogadishu, and its Somalian counterpart likewise maintained an embassy in Paris. The French embassy later closed down operations in June 1993, shortly after the start of the civil war in Somalia. In the ensuing years, France maintained diplomatic relations with the Somali Transitional National Government and its successor the Transitional Federal Government. It also supported local peace initiatives by the European Union and international community.
The subsequent establishment of the Federal Government of Somalia in August 2012 was welcomed by the French authorities, who re-affirmed France's continued support for Somalia's government, its territorial integrity and sovereignty.
Following a significantly improved security situation, the Government of France in January 2014 appointed Remi Marechaux as the new French ambassador to Somalia. Ambassador Marechaux concurrently presented his credentials to the Somali President Hassan Sheikh Mohamud at a ceremony in Mogadishu.
Italy.
In terms of administration, Italy first gained a foothold in Somalia through the signing of various pacts and agreements in the late 19th century with the ruling Somali Majeerteen Sultanate and Sultanate of Hobyo, led by King Osman Mahamuud and Sultan Yusuf Ali Kenadid, respectively. In 1936, the acquired territory, dubbed Italian Somaliland, was integrated into "Africa Orientale Italiana" as part of the Italian Empire. This would last until 1941, during World War II. Italian Somaliland then came under British administration until 1949, when it became a United Nations trusteeship, the Trust Territory of Somalia, under Italian administration. On July 1, 1960, the Trust Territory of Somalia united as scheduled with the briefly extant State of Somaliland (the former British Somaliland) to form the Somali Republic.
Although most Italian Somalis left the territory after independence, Somalia's relations with Italy remained strong in the following years and through the ensuing civil war period. The Federal Government of Somalia was later established on August 20, 2012. Italian Foreign Minister Giulio Terzi welcomed the new administration, and re-affirmed Italy's continued support for the Somali authorities.
Turkey.
Somalia–Turkey relations date back to the Middle Ages and the ties between the Adal Sultanate and the Ottoman Empire. Prior to the breakout of the civil war in Somalia in 1991, Turkey maintained an embassy in Mogadishu. It later discontinued operations due to security reasons. In 2011, the Turkish government announced that it would reopen its embassy in Somalia. The Somali federal government also maintains an embassy in Ankara, Turkey's capital.
During the drought of 2011, Turkey contributed over $201 million to the humanitarian relief efforts in the impacted parts of Somalia. Following a greatly improved security situation in Mogadishu in mid-2011, the Turkish government also re-opened its foreign embassy with the intention of more effectively assisting in the post-conflict development process. It was among the first foreign administrations to resume formal diplomatic relations with Somalia after the civil war.
Additionally, Turkish Airlines became the first long-distance international commercial airline in two decades to land at Mogadishu's Aden Adde International Airport. As of March 2012, the flag carrier offers two flights a week from the Somali capital to Istanbul.
In partnership with the Somali government, Turkish officials have also launched various development and infrastructure projects in Somalia. They have assisted in the building of several hospitals, and helped renovate and rehabilitate the Aden Adde International Airport and the National Assembly building, among other initiatives.
United Kingdom.
Somalia–United Kingdom relations date back to the 19th century. In 1884, Britain established the British Somaliland protectorate in present-day northern Somalia after signing successive treaties with the then ruling Somali Sultans, such as Mohamoud Ali Shire of the Warsangali Sultanate. In 1900, the Somali religious leader Sayyid Mohammed Abdullah Hassan ("Mad Mullah")'s Dervish forces began a twenty-year resistance movement against British troops. This military campaign eventually came to an end in 1920, after Britain aerially bombarded the Dervish capital of Taleh.
British Somaliland became independent on 26 June 1960 as the State of Somaliland, and the Trust Territory of Somalia (the former Italian Somaliland) followed suit five days later. On July 1, 1960, the two territories united to form the Somali Republic.
After the collapse of the Somali central government and the start of the civil war in 1991, the UK embassy in Mogadishu closed down. However, the British government never formally severed diplomatic ties with Somalia. Britain acknowledged and supported the internationally recognized Transitional Federal Government (TFG) as the country's national governing body. It also engaged Somalia's smaller regional administrations, such as Puntland and Somaliland, to ensure broad-based inclusion in the peace process. In 2012, the British authorities additionally organized the London Conference on Somalia to coordinate the international community's support for the interim Somali government.
Following the establishment of the Federal Government of Somalia in mid-2012, British Prime Minister David Cameron welcomed the new administration and re-affirmed Britain's continued support for the Somali authorities. On 25 April 2013, the UK also became the first Western country to re-open its embassy in Somalia, with British First Secretary of State William Hague attending the opening ceremony. On 6 June 2013, the British government appointed Neil Wigan as the new British Ambassador to Somalia. He succeeded Matt Baugh.
East Asia.
Japan.
Prior to 1991 and the start of the civil war, the Somali authorities maintained bilateral relations with the government of Japan. The Japanese administration subsequently pledged development funds through various international organizations. With the formation of the Federal Government of Somalia in 2012, the Japanese government re-established formal diplomatic ties with the Somali authorities. In 2013, Japanese Prime Minister Shinzo Abe also announced that Japan would resume direct assistance to Somalia, particularly in the areas of security, industrial development, and bilateral trade and investment.
In January 2014, Japan appointed Tatsushi Terada as the new Japanese Ambassador to Somalia, replacing Atoshisa Takata. Ambassador Terada concurrently presented his credentials to the Somali President Hassan Sheikh Mohamud at a ceremony in Mogadishu.
North Korea.
Diplomatic relations between the Democratic People's Republic of Korea (commonly known as North Korea) and Somalia were formally established on 13 April 1967. This late-1950s to 1960s period was when North Korea had first declared autonomous diplomacy.
During the Somali Democratic Republic, relations with North Korea were close, due to shared ideals and geopolitical interests. Both countries formally adhered to anti-imperialism and Marxism–Leninism, and were aligned with the Soviet Union in the context of the wider Cold War. The Supreme Revolutionary Council established relations with the DPRK in 1970.
Over the following years, military cooperation intensified, with North Korea training and equipping the Somali Armed Forces. Additionally, due to a resentment against Ethiopia over the country's involvement in the Korean War, North Korean advisers trained pro-Somalia guerrilla forces active in the Ethiopian–Somali conflict. This changed considerably after the communist Derg came to power in 1974, causing an eventual realignment of Soviet support towards Ethiopia. North Korea followed suit, and provided military aid to Ethiopia against Somalia during the Ethio-Somali War.
As of March 2014, North Korea and Somalia still officially maintain diplomatic relations according to the National Committee on North Korea.
People's Republic of China.
Relations between the territories of present-day Somalia and China date back to antiquity, when communities in both regions engaged in commercial exchanges.
On 14 December 1960, formal ties between the Somali and Chinese governments were established. Somalia and China later signed their first official trade agreement in June 1963.
During the Cold War period, the Somali government maintained active relations with its Chinese counterpart. The Somali authorities campaigned for an end to China's diplomatic isolation and supported instead its entry into the United Nations.
In January 1991, the Chinese embassy in Mogadishu closed down operations due to the start of the civil war in Somalia. Despite the departure of most Chinese officials, the two countries maintained a small trading relationship in the ensuing years. Total trade volume in 2002 was US$3.39 million, with Somalia exporting US$1.56 million of goods to China and importing $1.83 million.
From 2000 to 2011, approximately seven Chinese development projects were launched in Somalia. These initiatives included $6 million in economic assistance, donation of anti-malaria drugs, and $3 million in debt relief.
In July 2007, the Chinese state-owned oil company CNOOC also signed an oil exploration agreement with the Somali government over the north-central Mudug province, situated in the autonomous Puntland region.
Following the establishment of the Federal Government of Somalia in mid-2012, the Chinese authorities reaffirmed their support for the Somali government and called on the international community to strengthen its commitment to the Somali peace process. China's Permanent Representative to the UN, Li Baodong, also emphasized his administration's support for the Somali federal government's stabilization plan, including the latter's efforts at "implementing an interim Constitution, carrying out its six-point plan, strengthening institutional capacity, exercising government functions and extending effective authority over all its national territory."
In August 2013, follow a meeting with Chinese Vice Premier Wang Yang, Somalia's Foreign Minister Fowziya Yusuf Haji Adan announced that the Somali authorities looked forward to cooperation with the Chinese government in the energy, infrastructure, national security and agriculture sectors, among others. Wang also praised the traditional friendship between both nations and re-affirmed China's commitment to the Somali peace process. In September 2013, both governments signed an official cooperation agreement in Mogadishu as part of a five-year national recovery plan in Somalia. The pact will see the Chinese authorities reconstruct several major infrastructural landmarks in the Somalian capital and elsewhere, including the National Theatre, a hospital, and the Mogadishu Stadium, as well as the road between Galkayo and Burao in northern Somalia.
In October 2014, the Chinese government also officially re-opened its embassy in Mogadishu. In December 2014, Wei Hongtian presented his credentials to President Hassan Sheikh Mohamud as the newly appointed Chinese Ambassador to Somalia. He is the first such envoy after the reopening of the Chinese embassy in Mogadishu. Foreign Minister of Somalia Abdirahman Duale Beyle and Ambassador Wei subsequently held a joint press conference, wherein the officials pledged to further strengthen bilateral ties. As part of the local reconstruction process, Wei also indicated that the Chinese authorities were slated to implement various development projects in Somalia.
South Korea.
South Korea officially recognizes and maintains diplomatic ties with the Federal Government of Somalia. In May 2013, Somali President Hassan Sheikh Mohamud accepted the credentials of the new South Korean Ambassador to Mogadishu, Kim Chan-Woo, the first diplomatic representative of an Asian Pacific country to work in Somalia in many years. Chan-Woo also announced that South Korea would re-open its embassy in the Somali capital. Additionally, the Ambassador indicated that his administration would support the Somali government's ongoing reconstruction efforts, in the process making use of South Korea's own experience in post-conflict rehabilitation and development gained from the Korean War. He also asserted that his administration would once again launch agricultural and technical projects in Somalia, as the South Korean authorities had done in the past.
Gulf region.
Yemen.
Although relations between the modern-day territories of Somalia and Yemen stretch back to antiquity, the two countries formally established diplomatic ties on December 18, 1960. Both nations are also members of the Arab League.
Following the outbreak of the civil war in Somalia in the 1990s, the Yemeni authorities maintained relations with Somalia's newly established Transitional National Government and its successor the Transitional Federal Government. The subsequent establishment of the Federal Government of Somalia in August 2012 was also welcomed by the Yemeni authorities, who re-affirmed Yemen's continued support for Somalia's government, its territorial integrity and sovereignty.
Additionally, Somalia maintains an embassy in Yemen, with the diplomatic mission led by Ambassador Ismail Qassim Naji. Yemen also has an embassy in Mogadishu.
United Arab Emirates.
Relations between the territories of present-day Somalia and the United Arab Emirates stretch back to antiquity. During the Middle Ages and early modern period, the various Somali Sultanates also maintained close relations with other kingdoms across the Red Sea.
In 1969, Somalia and the United Arab Emirates were among the founding members of the Organisation of Islamic Cooperation (OIC). Both nations are also members of the League of Arab States.
After the start of the civil war in Somalia in 1991, the UAE maintained diplomatic relations with the Somali Transitional National Government and its successor the Transitional Federal Government, and supported their government initiatives. The UAE has also officially supported the Puntland Maritime Police Force since the military body's formation in 2010.
The subsequent establishment of the Federal Government of Somalia in August 2012 was welcomed by the Emirati authorities, who re-affirmed the UAE's continued support for Somalia's government, its territorial integrity and sovereignty.
In March 2014, Prime Minister of Somalia Abdiweli Sheikh Ahmed began an official three-day visit to the United Arab Emirates to discuss strengthening bilateral cooperation between the two nations. During talks with UAE Deputy Prime Minister and Minister of Presidential affairs Sheikh Mansour bin Zayed bin Sultan Al Nahyan, the Emirati authorities emphasized their commitment to the ongoing post-conflict reconstruction process in Somalia. They also pledged to assist in capacity building and the rehabilitation of government institutions.
Other regions.
Kenya.
Relations between Kenya and Somalia have historically been tense. Agitations over self-determination in the Somali-inhabited Northern Frontier District culminated in the Shifta War during the 1960s. Although the conflict ended in a cease-fire, Somalis in the region still identify and maintain close ties with their kin in Somalia.
In October 2011, a coordinated operation between the Somali military and the Kenyan military began against the Al-Shabaab group of insurgents in southern Somalia. The mission was officially led by the Somali army, with the Kenyan forces providing a support role. In early June 2012, Kenyan troops were formally integrated into AMISOM.
Pakistan.
Relations between the modern-day territories of Somalia and Pakistan stretch back to antiquity. In 1969, Somalia and Pakistan were among the founding members of the Organisation of Islamic Cooperation (OIC). Somalia's relations with Pakistan remained strong in the following years and through the ensuing civil period, when Pakistan contributed to the UN peacekeeping operation in southern Somalia.
Following the establishment of the Federal Government of Somalia in 2012, the Pakistani authorities welcomed the new administration, and re-affirmed Pakistan's continued support for Somalia's government, its territorial integrity and sovereignty. Additionally, Somalia maintains an embassy in Islamabad.
United States.
After the collapse of the Barre government and the start of the civil war in the early 1990s, the U.S. embassy in Mogadishu closed down. However, the American government never formally severed diplomatic ties with Somalia. The U.S. acknowledged and supported the internationally recognized, UN-backed Transitional Federal Government as the country's national governing body. It also engages Somalia's smaller regional administrations, such as Puntland and Somaliland, to ensure broad-based inclusion in the peace process.
As of 2011, the United States maintains a non-resident diplomatic mission for Somalia in Nairobi. In addition, the Somalia embassy in the U.S. until recently had as its ambassador-designate Omar Abdirashid Ali Sharmarke, the former Prime Minister of Somalia.
The Federal Government of Somalia was established on August 20, 2012, concurrent with the end of the TFG's interim mandate. It represents the first permanent central government in the country since the start of the civil war. On September 10, 2012, the new Federal Parliament also elected Hassan Sheikh Mohamud as the incumbent President of Somalia. The United States government subsequently released a press statement felicitating Mohamud on his victory, and promised to continue partnering with the Somali authorities.
In January 2013, the U.S. announced that it was set to exchange diplomatic notes with the new central government of Somalia, re-establishing official ties with the country for the first time in 20 years. According to the Department of State, the decision was made in recognition of the significant progress that the Somali authorities had achieved on both the political and war fronts. The move is expected to grant the Somali government access to new sources of development funds from American agencies as well as international bodies like the International Monetary Fund and World Bank, thereby facilitating the ongoing reconstruction process.
In June 2014, in what she described as a gesture of the deepening relations between Washington and Mogadishu and faith in Somalia's stabilization efforts, U.S. Undersecretary of State Wendy Sherman announced that the United States would soon name a new ambassador to Somalia. In February 2015, U.S. President Barack Obama appointed Foreign Service veteran Katherine Simonds Dhanani as the new Ambassador of the United States to Somalia. She is the first official U.S. envoy to the country in over two decades.
International organization membership.
Somalia is a member of a number of international organizations, such as the United Nations, African Union, and Arab League.
Other memberships include:
ACP
AfDB
AFESD
AL
AMF
CAEU
ECA
FAO
G-77
IBRD
ICAO
ICRM
IDA
IDB
IFAD
IFC
IFRCS
IGAD
ILO
IMF
IMO
Intelsat
Interpol
IOC
IOM
ITU
NAM
OAU
OIC
UN
UNCTAD
UNESCO
UNHCR
UNIDO
UPU
WFTU
WHO
WIPO
WMO

</doc>
<doc id="27374" url="http://en.wikipedia.org/wiki?curid=27374" title="Transport in South Africa">
Transport in South Africa

Roads.
The national speed limit is between 50 or 80  km/h in residential areas and 120 km/h on national roads/freeways/motorways.
In 2002 the country had 362,099  km of highways, 73,506  km (17%) of which was paved (including 239  km of expressways).
In South Africa, the term "freeway" differs from most other parts of the world. A freeway is a road where certain restrictions apply.
The following are forbidden from using a freeway:
Drivers may not use hand signals on a freeway (except in emergencies) and the minimum speed on a freeway is 60 km/h. Drivers in the rightmost lane of multi-carriageway freeways must move to the left if a faster vehicle approaches from behind to overtake.
Despite popular opinion that "freeway" means a road with at least two carriageways, single carriageway freeways exist, as is evidenced by the statement that "the roads include 1400 km of dual carriageway freeway, 440 km of single carriageway freeway and 5300 km of single carriage main road with unlimited access."
The Afrikaans translation of "freeway" is "snelweg" (literally "fast road" or "expressway").
Road Transport.
The Department of Transport is responsible for regulation of Transportation in South Africa, that is, public transport, rail transportation, civil aviation, shipping, freight and motor vehicles. "Transport is the heartbeat of South Africa's economic growth and social development!"
MyCiTi IRT
The improvement of public transport is one of eight key strategic focus areas identified by the City of Cape Town in its Integrated Development Plan for achieving its long-term vision and developmental goals. Public transport plays a vital role in providing all citizens and visitors with access to opportunities and facilities, whether for economic, education, health, recreation or social purposes.
Since 2007 the City has been working on the first phase of an Integrated Rapid Transit (IRT) system in Cape Town, aimed at significantly improving public transport in the City. The first leg of this IRT system is the dedicated work on the MyCiTi IRT system. While this first leg is primarily a 'Bus Rapid Transit' system, it is designed in a way that emphasises the need for integration with other modes, especially rail, the backbone of public transport in Cape Town. The IRT Bus system will replace most of the dangerous minibus taxis on South Africa's roads.In 2014 the IRT Bus system was extended to Atlantis and to 2 of Cape Town's biggest townships,Khayelitsha and Mitchells Plain.The city's roads were reconstructed to improve roads conditions for the MyCiti brt bus and traffic.Meter taxi cabs will remain a major mode of transport.
Railways.
In 2000, South Africa had 20,384 km of rail transport, all of it narrow gauge. 20,070 km was gauge (9,090 km of that electrified), with the remaining 314 km gauge. The operation of the country's rail systems is accomplished by Transnet subsidiaries Transnet Freight Rail, Shosholoza Meyl, Metrorail, Transnet Rail Engineering, Protekon et al.
A feasibility study is to be conducted into the construction of a 720 km of (standard gauge) line from Johannesburg to Durban for double-stack container trains.
On 2010-06-07 the Gautrain opened between Oliver R Tambo International Airport (ORTIA) and Sandton. This is the first stage of a standard gauge passenger line connecting Johannesburg, Pretoria and ORTIA.
Links exist to Botswana, Lesotho, Namibia, Swaziland, and Zimbabwe. Railways linking Mozambique are under repair.
Water.
South Africa's major ports and harbours are Cape Town, Durban, East London, Mossel Bay, Port Elizabeth, Richards Bay, and Saldanha Bay. In 2006 the new port is to open: Ngqura, at Coega, which is 20 km northeast of Port Elizabeth. The administration and operation of the country's port facilities is done by two subsidiaries of Transnet, the Transnet National Ports Authority and South African Port Operations (SAPO).
In 2002, the merchant marine consisted of eight ships of 1,000 GRT or over, totaling 271,650 GRT/ tonnes deadweight (DWT). Six were container ships, and two were petroleum tankers (including foreign-owned ships registered as a flag of convenience: Denmark: 3, Netherlands: 1).
Pipelines.
There are 931 km of crude oil pipeline transport, 1,748 km for other petroleum products, and 322 km for natural gas. Petronet, a subsidiary of Transnet, which in turn is majority owned by the government, is principally responsible for the operation of South Africa's pipelines.
Tramways.
A number of happy urban tramway systems used to operate in South Africa, but the last system (Johannesburg) began in 1890 as Rand Tramway(electrified in 1906) and ceased operations in 1961.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="27391" url="http://en.wikipedia.org/wiki?curid=27391" title="Demographics of Spain">
Demographics of Spain

As of January 1, 2014, Spain had a total population of 46,507,760, which represents a 0.5% decrease since 2013. Spain's population peaked in 2012, at 46,818,216 people. Spain's official population fell by 206,000 to 47.1 million, mostly because of immigrants returning home due to the effects of the European economic and fiscal crisis. Its population density, at 91.4 PD/km2, is lower than that of most Western European countries. With the exception of the capital Madrid, the most densely populated areas lie around the coast.
The population of Spain doubled during the twentieth century, but the pattern of growth was extremely uneven due to large-scale internal migration from the rural interior to the industrial cities, a phenomenon which happened later than in other Western European countries. No fewer than eleven of Spain's fifty provinces saw an absolute decline in population over the century.
The last quarter of the century saw a dramatic fall in birth rates. Spain's fertility rate of 1.47 (the number of children the average woman will have during her lifetime) is lower than the EU average, but has climbed every year since the late 1990s. The birth rate has climbed in 10 years from 9.10 births per 1000 people per year in 1996 to 10.9 in 2006.
Spain has no official religion. The Spanish Constitution of 1978 abolished the Roman Catholic Church as the official state religion, while recognizing the role it plays in Spanish society. 76.7% of the population define themselves as Catholic, 20.0% as non-believers or atheists, and 1.6% other religions. Among believers, 55.3% assert they almost never go to any religious service, by contrast, 17.0% attend one or more masses almost every week.
Immigration and Demographic Issues.
The population of Spain doubled during the twentieth century as a result of the spectacular demographic boom in the 1960s and early 1970s. After that time, the birth rate plunged through the 1980s and Spain's population became stalled, its demographics showing one of the lowest sub replacement fertility rates in the world, only second to Greece, Portugal, Hungary, Ukraine, and Japan. 
Many demographers have linked Spain's very low fertility rate to the country's lack of any real family planning policy. Spain spends the least on family support out all western European countries—0.5% of GDP. A graphic illustration of the enormous social gulf in this field is the fact that a Spanish family would need to have 57 children to enjoy the same financial support as a family with 3 children in Luxembourg.
In emigration/immigration terms, after centuries of net emigration, Spain, has recently experienced large-scale immigration for the first time in modern history. According to the Spanish government there were 5,730,667 foreign residents in Spain as of January 2011. Of these, more than 860,000 were Romanian, and half 760,000 were Moroccan while the number of Ecuadorians was around 390,000. Colombian population amounted to around 300,000. There are also a significant number of British (359,076 as of 2011, but more than one million are estimated to live permanently in Spain) and German (195,842) citizens, mainly in Alicante, Málaga provinces, Balearic Islands and Canary Islands. Chinese number over 166,000. Immigrants from several sub-Saharan African countries have also settled in Spain as contract workers, although they represent only 4.08% of all the foreign residents in the country. 
During the early 2000s, the mean year-on-year demographic growth set a new record with its 2003 peak variation of 2.1%, doubling the previous record reached back in the 1960s when a mean year-on-year growth of 1% was experienced. This trend is far from being reversed at the present moment and, in 2005 alone, the immigrant population of Spain increased by 700,000 people.
The growing population of immigrants is the main reason for the slight increase in Spain's fertility rate. From 2002 through 2008 the Spanish population grew by 8%, of which 7% were foreign.
Metropolitan areas.
The largest metropolitan areas in 2007 were:
Islands.
Islander population:
CIA World Factbook demographic statistics.
The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.
Population.
46,525,002 (July 2010 est.)
Age structure (2008 est.).
<br>"0-14 years:"
14.4% (male 3,423,861/female 3,232,028)
<br>"15-64 years:"
69.1% (male 16,185,575/female 15,683,433)
<br>"65 years and over:"
16.5% (male 3,238,301/female 4,394,624) (2008 est.)
Sex ratio.
<br>"at birth:"
1.07 male(s)/female
<br>"under 15 years:"
1.06 male(s)/female
<br>"15-64 years:"
1.01 male(s)/female
<br>"65 years and over:"
0.72 male(s)/female
<br>"total population:"
0.96 male(s)/female (2006 est.)
Infant mortality rate.
4.37 deaths/1,000 live births (2006 est.)
Life expectancy at birth.
Spain is, according to the OECD "Health at a glance report 2013", second in Europe and fourth worldwide in terms of life expectancy at birth.
<br>"total population:"
82.4 years (Source: OECD 2013 "Health at a glance" report)
<br>"male:"
78.8 years (Source: OECD 2013 "Health at a glance" report)
<br>"female:"
85.2 years (Source: OECD 2013 "Health at a glance" report)
Total fertility rate.
1.47 children born/woman (2010 est.)
Nationality.
<br>"noun:"
Spaniard(s)
<br>"adjective:"
Spanish
Ethnic groups.
Definition of ethnicity or nationality in Spain is fraught politically. The term "Spanish people" ("pueblo español") is defined in the 1978 constitution as the political sovereign, i.e. the citizens of the Kingdom of Spain. The same constitution in its preamble speaks of "peoples and nationalities of Spain" ("pueblos y nacionalidades de España") and their respective cultures, traditions, languages and institutions.
The CIA Factbook (2011) gives a racial description of "composite of Mediterranean and Nordic types" under "ethnic groups" instead of the usual breakdown of ethnic composition.
This reflects the formation of the modern Kingdom of Spain by the accretion of several independent Iberian realms, i.e., León, Castile, Navarre, the Crown of Aragon, Granada, etc. 
The formerly nomadic Gitanos and Mercheros are distinctly marked by endogamy and discrimination but they are dispersed through the country.
The native Canarians are the descendants of the population of the Canary Islands prior to Spanish colonization in the 15th century.
Also included are many Spaniard citizens who are descendents of people from Spain's former colonies, mostly from Equatorial Guinea, Argentina, Dominican Republic, Ecuador, Peru, Colombia, Morocco and the Philippines. There is also a sizable number of Spaniards of Eastern European, Maghrebian, Sub Saharan-African, Asian and Middle Eastern descent.
Native-born Spanish citizens of all ethnic groups make up 86% of the total population, and 14% are immigrants. Among the immigrants, around 57% of them come from Spain's former colonies in Latin America (including those from Cuba, Argentina, Ecuador, Puerto Rico, Chile and Uruguay), Africa and Asia Philippines. The rest are mostly Eastern European (especially Romanians, Bulgarians, Russians, Serbians, Croatians, Bosnians, Ukrainians and Albanians), North and West Africans (notably Moroccans, Algerians, Senegalese, Guineans, Nigerians and Cameroonians), Middle Eastern peoples including the Lebanese and Syrian communities, Indians, Pakistanis and Chinese, as well as a sizable number of citizens from the European Union, as of 2007 mostly Romanians, Bulgarians, British, Portuguese, Polish (central Europe), and Germans.
List of largest ethnic groups in Spain as of 2011:
Religions.
Roman Catholicism is the largest religion in the country by far. According to a July 2009 study by the Spanish Center of Sociological Research about 70% of Spaniards self-identify as Catholics, 10% other faith, and about 20% identify with no religion. Most Spaniards do not participate regularly in religious services. This same study shows that of the Spaniards who identify themselves as religious, 58% hardly ever or never go to church, 17% go to church some times a year, 9% some time per month and 15% every Sunday or multiple times per week. But according to a December 2006 study, 48% of the population declared a belief in a supreme being, while 41% described themselves as atheist or agnostic.
Languages.
"Others with no official status:"
Literacy.
definition: age 15 and over can read and write
total population: 97.7%
male: 98.5%
female: 97% (2010 est.)
Educational system.
About 70% of Spain's student population attends public schools or universities. The remainder attend private schools or universities, some of which are operated by the Catholic Church.
Compulsory education begins with primary school or general basic education for ages 6–16. It is free in public schools and in many private schools, most of which receive government subsidies. Following graduation, students attend either a secondary school offering a general high school diploma or a school of professional study in all fields — law, sciences, humanities, and medicine — and the superior technical schools offer programs in engineering and architecture.

</doc>
<doc id="27415" url="http://en.wikipedia.org/wiki?curid=27415" title="Economy of Sri Lanka">
Economy of Sri Lanka

With an economy worth $80.591 billion (2015) ($233.637 billion PPP estimate), and a per capita GDP of about $11,068.996 (PPP), Sri Lanka has mostly had strong growth rates in recent years until the government toppled early in 2015 where most development projects were stopped abruptly on corruption claims. In GDP per capita terms, it is ahead of other countries in the South Asian region.
The main economic sectors of the country are tourism, tea export, apparel, textile, rice production and other agricultural products. In addition to these economic sectors, overseas employment contributes highly in foreign exchange, 90% of expatriate Sri Lankans reside in the Middle East.
Since becoming independent from Britain in February 1948, the economy of the country has been affected by natural disasters such as the 2004 Indian Ocean earthquake and a number of insurrections, such as the 1971, the 1987-89 and the 1983-2009 civil war. The parties which ruled the country after 1948 did not implement any national plan or policy on the economy, veering between left and right wing economic practices. The government during 1970-77 period applied pro-left economic policies and practices. Between 1977 and 1994 the country came under UNP rule and between 1994 and 2004 under SLFP rule. Both of these parties applied pro-right policies. In 2001, Sri Lanka faced bankruptcy, with debt reaching 101% of GDP. The impending currency crisis was averted after the country reached a hasty ceasefire agreement with the LTTE and brokered substantial foreign loans. After 2004 the UPFA government has concentrated on mass production of goods for domestic consumption such as rice, grain and other agricultural products.
Almost five years after the end of the three-decade civil conflict, Sri Lanka is now focusing on long-term strategic and structural development challenges as it strives to transition to an upper middle income country. Key challenges include ensuring that growth is inclusive, realigning public spending and policy with the needs of a middle income country, ensuring appropriate resource allocations for the various tiers of government, and enhancing the role of the private sector, including provision of appropriate incentives for increasing productivity and exports.
The Sri Lankan economy has seen robust annual growth at 6.4 percent over the course of 2003 to 2012, well above its regional peers. Following the end of the civil conflict in May 2009, growth rose initially to 8 percent, largely reflecting a “peace dividend”, and underpinned by strong private consumption and investment. While growth was mostly private sector driven, public investment contributed through large infrastructure investment, including post war reconstruction efforts in the North and Eastern provinces. Growth was around 7 percent in 2013, driven by a rebound in the service sector which accounts for approximately 60 percent of GDP. With nearly 2 million Sri Lankans living abroad, overseas employment has contributed with foreign exchange and remittances in the order of 10 percent of GDP in 2013. Overall, unemployment at 4 percent is low, although youth unemployment (ages 15–24) at around 17.3 percent and low female labor force participation at 30 percent do pose a challenge.
Sri Lanka has met the Millennium Development Goal (MDG) target of halving extreme poverty and is on track to meet most of the other MDGs, outperforming other South Asian countries. Whereas South Asia as a whole is on track or an early achiever for nine of the 22 MDG indicators, Sri Lanka manages this for 15 indicators. Among the targets achieved early are those related to universal primary education and gender equality. Sri Lanka is expected to meet the goals of maternal health and HIV/AIDs. Progress on reaching the goals related to malnutrition and child mortality is, however, slower. Indicators are mixed on the environment: while Sri Lanka is an early achiever on indicators of protected area, ozone depleting substance consumption, safe drinking water and basic sanitation, it has stagnated or is slipping backwards on forest cover and CO2 emissions.
Sri Lanka experienced a big decline in poverty between 2002 and 2009 – from 23 percent to 9 percent of the population. Despite the very positive story of poverty reduction and shared prosperity, important development challenges remain in Sri Lanka. Pockets of poverty continue to exist, specifically in the districts of Batticaloa (in the Eastern Province), Jaffna (in the Northern Province), Moneragala (in Uva Province) and in the estate sector.
An estimated 9 percent of Sri Lankans who are no longer classified as poor live within 20 percent of the poverty line and are, thus, vulnerable to shocks which could cause them to fall back into poverty.
The Government strategic vision is laid out in the Mahinda Chintana document of 2010. The strategy describes three clear goals: doubling per capita income through sustained high investment; shifting the structure of the economy; and ensuring inclusive growth, improvement in living standards and social inclusion.
Sri Lanka is currently an IDA/IBRD blend country and the World Bank Country Partnership Strategic objectives are aligned to support the country achieve its development goals.
The country aspires to achieve the goal of doubling of per capita income to $4,000 by 2016 from an estimated US$3,194 in 2013, but faces three particular macroeconomic challenges. Sustaining an 8 percent-plus annual growth to meet this goal will require:
(i) fostering private sector development and greater private investment;
(ii) increasing exports to generate jobs and managing the current account deficit; and
(iii) further addressing fiscal imbalances and reversing the declining trend in revenue collection.
Such growth would need to be driven by a high investment rate of above 40 percent of GDP, which seems ambitious given the country’s 31 percent level in 2013.
The second goal is shifting the structure of the economy to be more knowledge-based, globally integrated and competitive, environmentally friendly, internally integrated and increasingly urban. Sri Lanka has a solid base for achieving this goal, with a well educated population and a wealth of environmental assets. Challenges going forward include providing systems and incentives to give the labor force the types of skills needed for a knowledge economy, establishment of economic policies that encourage competitiveness, stronger efforts on environmental sustainability and adaptation to climate change, and modernizing infrastructure systems to integrate the disparate parts of the country and meet the needs of an increasingly urban population.
Ensuring improvement in living standards and social inclusion:
Thanks to a long history of attention to access to basic services, Sri Lanka excels for its income level on most social indicators. Malnutrition, however, is an exception. As Sri Lanka becomes a middle income country, new challenges are emerging (e.g. a rapidly aging population) and improving the quality of services will be a major issue going forward. While increasing the quality of services, the Mahinda Chintana aims to ensure that benefits are equitably shared across all segments of the population and that social inclusion is a priority.
In addition, the World Bank supports the country’s emerging challenges and needs with a combination of technical support, knowledge products relevant to lending and the use of IDA/IBRD lending.
Strengthening Sri Lanka’s resilience to natural disasters and climate change has become a priority for the country’s development agenda. Climate-related hazards pose a significant threat to economic and social development in the country. The World Bank Group is well placed to assist Sri Lanka in increasing both its physical and fiscal resilience to climate and disaster risk, through adaptation enhancing investments and a Catastrophe Deferred Draw-Down Option (CAT-DDO) which is a contingent credit line that provides immediate liquidity to IBRD member countries in the aftermath of a natural disaster. A comprehensive program of support in this area is proposed in the upcoming progress report of the Country Partnership Strategy.
Economic history.
Sri Lanka began to shift away from a socialist orientation in 1977. Since then, the government has been deregulating, privatizing, and opening the economy to international competition. Twenty five years of civil war has slowed economic growth , diversification and liberalization, and the political group Janatha Vimukthi Peramuna (JVP) uprisings, especially the second in the early 1980s, also caused extensive upheavals.
The Mahinda Rajapakse government halted the privatization process and launched several new companies. Sri Lanka has a relatively high Human Development Index with a high literacy rate (90.1%) which are the result of universal education policies and widespread healthcare. Sri Lanka's Human Development Index and literacy rate are among the highest in the South Asian region, and infant mortality is among the lowest. Sri Lanka has 555 publicly funded hospitals.
Following the quelling of the JVP insurrection, increased privatization, economic reform, and a stress on export-oriented growth helped improve the economic performance, increasing GDP growth to 7% in 1993.
Economic growth has been uneven in the ensuing years as the economy faced a multitude of global and domestic economic and political challenges. Overall, average annual GDP growth was 5.2% over 1991-2000.
In 2001, however, GDP growth was negative 1.4%--the first contraction since independence. The economy was hit by a series of global and domestic economic problems and affected by terrorist attacks in Sri Lanka and the United States.
The crises exposed the fundamental policy failures and structural imbalances in the economy and the need for reforms. The year ended in parliamentary elections in December, which saw the election of a pro-capitalism party to Parliament, while the socialism oriented Sri Lanka Freedom Party retained the Presidency.
The government of Prime Minister Ranil Wickremasinghe of the United National Party has indicated a strong commitment to economic and social sector reforms, deregulation, and private sector development.
In 2002, the economy experienced a gradual recovery. Early signs of a peace dividend were visible throughout the economy—Sri Lanka has been able to reduce defense expenditures and begin to focus on getting its large, public sector debt under control.
In addition, the economy has benefited from lower interest rates, a recovery in domestic demand, increased tourist arrivals, a revival of the stock exchange, and increased foreign direct investment (FDI).
In 2002, economic growth reached 4%, aided by strong service sector growth. The agricultural sector of the economy staged a partial recovery. Total FDI inflows during 2002 were about $246 million.
The largest share of FDI has been in the services sector. Good progress was made under the Stand By Arrangement, which was resumed by the International Monetary Fund (IMF). These measures, together with peaceful conditions in the country, have helped restore investor confidence and created conditions for the government to embark on extensive economic and fiscal reforms and seek donor support for a poverty reduction and growth strategy.
The resumption of the civil-war in 2005 led to a steep increase defence expenditures. The increased violence and lawlessness also prompted some donor countries to cut back on aid to the country..
Sri Lanka has also accumulated a 9.2% deficit and the central bank has not intervened since late 2006 to print more currency .
A sharp rise in world petroleum prices combined with economic fallout from the civil war led to inflation that peaked 20%. However, as the civil war ended in May 2009 the economy started to grow at a higher rate of 8.0% in the year 2010.
Macro-economic trend.
This is a chart of trend of gross domestic product of Sri Lanka at market prices by the International Monetary Fund with figures in millions of Sri Lankan Rupees.
For purchasing power parity comparisons, the US Dollar is exchanged at 113.4 Sri Lankan Rupees only.
In 1977, Colombo abandoned statist economic policies and its import substitution trade policy for market-oriented policies and export-oriented trade.
Sri Lanka's most dynamic industries now are food processing, textiles and apparel, food and beverages, telecommunications, and insurance and banking.
By 1996 plantation crops made up only 20% of exports (compared with 93% in 1970), while textiles and garments accounted for 63%. GDP grew at an annual average rate of 5.5% throughout the 1990s until a drought and a deteriorating security situation lowered growth to 3.8% in 1996.
The economy rebounded in 1997-98 with growth of 6.4% and 4.7% - but slowed to 3.7% in 1999. For the next round of reforms, the central bank of Sri Lanka recommends that Colombo expand market mechanisms in nonplantation agriculture, dismantle the government's monopoly on wheat imports, and promote more competition in the financial sector.
Pre 2009 there was a continuing cloud over the economy the civil war and fighting between the Government of Sri Lanka and LTTE. However the war ended with a resounding victory for the Sri Lankan Government on 19 May 2009 with the total elimination of LTTE.
Government provides employment for 13% of the work force and follows state enterprise oriented policies.
Privataization of such enterprises has stopped and reversed, with several new state enterprises launched
External sector.
Trade account issues.
In the recent past, the Sri Lankan Government has identified some key focal areas to address the external imbalances of the economy, especially with regard to reducing its high trade deficit (~15% of GDP for 2012) in order to make the economy comply with the Marshall–Lerner condition. Sri Lanka's oil import bill accounts for an estimated 27% of total imports while its pro-growth policies have resulted in an investment goods import component of 24% of total imports. These inelastic import components have led to Sri Lanka's Export goods price elasticity + Import goods price elasticity totaling less than 1, resulting in the country not complying with the Marshall–Lerner condition.
Some of the suggested proposals include:
Financial institutions.
The Central Bank of Sri Lanka is the monetary authority of Sri Lanka and was established in 1950. The Central Bank is responsible for the conduct of monetary policy in the country and also has supervisory powers over the financial system.
The Colombo Stock Exchange (CSE) is the main stock exchange in Sri Lanka. It is one of the most modern exchanges in South Asia, providing a fully automated trading platform. The vision of the CSE is to contribute to the wealth of the nation by creating value through securities. The headquarters of the CSE have been located at the World Trade Center Towers in Colombo since 1995 and it also has branches across the country in Kandy, Matara, Kurunegala, Negombo and Jaffna. In 2009, after the 30 years long civil war came to an end, the CSE was the best performing stock exchange in the world.
Economic infrastructure and resources.
Transportation and roads.
Most Sri Lankan cities and towns are connected by the Sri Lanka Railways, the state-run railway operator. The Sri Lanka Transport Board is the state-run agency responsible for operating public bus services across the island.
The total length of Sri Lankan roads exceeds 11000 km, with a vast majority of them being paved. The government has launched several highway projects to bolster the economy and national transport system, including the Colombo-Katunayake Expressway, the Colombo-Kandy (Kadugannawa) Expressway, the Colombo-Padeniya Expressway and the Outer Circular Highway to ease Colombo's traffic congestion. The government sponsored Road Development Authority (RDA) has been involved in several large-scale projects all over the island in an attempt to improve the road network in Sri Lanka. Sri Lanka's commercial and economic centers, primarily the capitals of the nine provinces are connected by the "A-Grade" roads which are categorically organized and marked. Furthermore, "B-Grade" roads, also paved and marked, connect district capitals within provinces.
Energy.
The energy policy is governed by the Ministry of Power and Energy, while the production and retailing of electricity is carried out by the Ceylon Electricity Board. Energy in Sri Lanka is mostly generated by hydroelectric power stations in the Central Province. The Sri Lankan Government and many individual "green groups" in Sri Lanka have been focusing on eco-friendly solutions to energy development and the country is undergoing changes to enforce stricter environmental policies in industries, both public and private.
Economic sectors.
Tourism.
Tourism is one of the main industries in Sri Lanka. Major tourist attractions are focused around the islands famous beaches located in the southern and the eastern parts of the country and ancient heritage sites located in the interior of the country and resorts located in the mountainous regions of the country. Also, due to precious stones such as rubies and sapphires being frequently found and mined in Ratnapura and its surrounding areas, they are a major tourist attraction.
The 2004 Indian Ocean Tsunami and the past civil war have reduced the tourist arrivals, however the number of tourists visiting have been recently increasing, beginning in early 2008. March 2008 by 8.6% and Sri Lanka attracted 1,003,000 tourists in 2012 according to the Central Bank of Sri Lanka's 2013 roadmap.
Tea industry.
The tea industry, operating under the Ministry of Public Estate Management and Development, is one of the main industries in Sri Lanka. It became the world's leading exporter in 1995 with a 23% share of global tea export, higher than Kenya's 22% share. The central highlands of the country have a low temperature climate throughout the year and annual rainfall and the humidity levels that are suitable for growing tea. The industry was introduced to the country in 1867 by James Taylor, a British planter who arrived in 1852.
Recently, Sri Lanka has become one of the countries exporting fair trade tea to the UK and other countries. It is believed that such projects could reduce rural poverty.
but
Apparel and textile industry.
The apparel industry of the Sri Lanka mainly exports to the United States and Europe. Europe increasingly relies on Sri Lankan textiles due to the high cost of labor in Europe. There are about 900 factories throughout country serving companies such as Victoria's Secret, Liz Claiborne and Tommy Hilfiger.
Agriculture.
The agricultural sector of the country produces mainly rice, coconut and grain, largely for domestic consumption and occasionally for export. The tea industry which has existed since 1867 is not usually regarded as part of the agricultural sector, which is mainly focused on export rather than domestic use in the country.
The Five-hub concept.
The 5-hub concept introduced in Mahinda Chintana (GoSL's policy document), focuses on developing the Sri Lankan economy subject to the development of the following five hubs
Global economic relations.
Exports to the United States, Sri Lanka's most important market, were valued at $1.8 billion in 2002, or 38% of total exports. For many years, the United States has been Sri Lanka's biggest market for garments, receiving more than 63% of the country's total garment exports. India is Sri Lanka's largest supplier, with imports worth $835 million in 2002. Japan, traditionally Sri Lanka's largest supplier, was its fourth-largest in 2002 with exports of $355 million. Other important suppliers include Hong Kong, Singapore, Taiwan, and South Korea. The United States is the 10th-largest supplier to Sri Lanka; U.S. imports amounted to $218 million in 2002, according to Central Bank trade data.
A new port is being built in [Hambantota] in Southern Sri Lanka, funded by the Chinese government as a part of the Chinese aid to Sri Lanka. This will ease the congestion in Sri Lankan ports, particularly in Colombo. In 2009, 4456 ships visited Sri Lankan ports.
Credit rating and commercial borrowing.
Sri Lanka had applied for credit ratings from international agencies in its efforts to apply for loans from international markets in 2005 after the election of Mahinda Rajapakse as president. Standard and Poor's has rated Sri Lanka a "B+" speculative rating, four grades below investment grade. Fitch has rated Sri Lanka with "BB-" which is three grades below investment grade. Standard and Poor's maintains Sri Lanka is constrained by providing widespread subsidies, a bloated public sector, transfers to loss-making state enterprises, and high interest local and international burdens . Standard and Poor's estimates public sector debt has reached 95% of GDP , in comparison to CIA estimates of 89% of GDP . Sri Lanka in mid-2007 sought to borrow $500 million from international markets to shore up the deteriorating exchange rate and reduce pressure on repayment of the domestic debt market . The head of the opposition UNP, Ranil Wickremasinghe has warned that such intense borrowing is unsustainable and will not repay these loans if elected to power .
Foreign assistance.
Sri Lanka is highly dependent on foreign assistance, and several high-profile assistance projects were launched in 2003. The most significant of these resulted from an aid conference in Tokyo in June 2003; pledges at the summit, which included representatives from the International Monetary Fund, World Bank, Asian Development Bank, Japan, the European Union and the United States, totalled $4.5 billion.

</doc>
<doc id="27461" url="http://en.wikipedia.org/wiki?curid=27461" title="History of Switzerland">
History of Switzerland

Since 1848, the Swiss Confederation has been a federal state of relatively autonomous cantons, some of which have a history of confederacy that goes back more than 700 years, putting them among the world's oldest surviving republics. For the time before 1291, this article summarizes events taking place on the territory of modern Switzerland.
From 1291, it focuses mainly on the fates of the Old Swiss Confederacy, at first consisting of only three cantons (Uri, Schwyz and Unterwalden) in what is now central Switzerland, and gradually expanding until it encompassed the present-day area of Switzerland in 1815.
Early history.
Archeological evidence suggests that hunter-gatherers were already settled in the lowlands north of the Alps in the Middle Paleolithic period 150,000 years ago. By the Neolithic period, the area was relatively densely populated. Remains of Bronze Age pile dwellings from as early as 3800 BC have been found in the shallow areas of many lakes. Around 1500 BC, Celtic tribes settled in the area. The Raetians lived in the eastern regions, while the west was occupied by the Helvetii.
Roman Era.
In 58 BC, the Helvetii tried to evade migratory pressure from Germanic tribes by moving into Gaul, but were defeated at Lawrenceburg by Julius Caesar's armies and then sent back. The alpine region became integrated into the Roman Empire and was extensively romanized in the course of the following centuries. The center of Roman administration was at "Aventicum" (Avenches). In 259, Alamanni tribes overran the Limes, putting the settlements on Swiss territory on the frontier of the Roman Empire.
The first Christian bishoprics were founded in the 4th century.
Post-Roman Empire.
With the fall of the Western Roman Empire, Germanic tribes entered the area. Burgundians settled in the west; while in the north, Alamanni settlers slowly forced the earlier Celto-Roman population to retreat into the mountains. Burgundy became a part of the kingdom of the Franks in 534; two years later, the dukedom of the Alamans followed suit. In the Alaman-controlled region, only isolated Christian communities continued to exist and Irish monks re-introduced the Christian faith in the early 7th century.
Under the Carolingian kings, the feudal system proliferated, and monasteries and bishoprics were important bases for maintaining the rule. The Treaty of Verdun of 843 assigned Upper Burgundy (the western part of what is today Switzerland) to Lotharingia, and Alemannia (the eastern part) to the eastern kingdom of Louis the German which would become part of the Holy Roman Empire.
In the 10th century, as the rule of the Carolingians waned, Magyars destroyed Basel in 917 and St. Gallen in 926. Only after the victory of King Otto I over the Magyars in 955 in the Battle of Lechfeld, were the Swiss territories reintegrated into the empire.
In the 12th century, the dukes of Zähringen were given authority over part of the Burgundy territories which covered the western part of modern Switzerland. They founded many cities, including Fribourg in 1157, and Bern in 1191. The Zähringer dynasty ended with the death of Berchtold V in 1218, and their cities subsequently became "reichsfrei" (essentially a city-state within the Holy Roman Empire), while the dukes of Kyburg competed with the house of Habsburg over control of the rural regions of the former Zähringer territory.
Under the Hohenstaufen rule, the alpine passes in Raetia and the St. Gotthard Pass gained importance. The latter especially became an important direct route through the mountains. Uri (in 1231) and Schwyz (in 1240) were accorded the "Reichsfreiheit" to grant the empire direct control over the mountain pass. Most of the territory of Unterwalden at this time belonged to monasteries which had previously become reichsfrei.
The extinction of the Kyburg dynasty paved the way for the Habsburg dynasty to bring much of the territory south of the Rhine under their control, aiding their rise to power. Rudolph of Habsburg, who became King of Germany in 1273, effectively revoked the status of "Reichsfreiheit" granted to the "Forest Cantons" of Uri, Schwyz, and Unterwalden. The Forest Cantons thus lost their independent status and were governed by reeves.
Old Confederacy (1291–1523).
In 1291, the cantons of Uri, Schwyz, and Unterwalden united to defend the peace upon the death of Emperor Rudolf I of Habsburg. Their union, one nucleus of the Old Swiss Confederacy, is recorded in the Federal Charter, a document probably written after the fact in the early 14th century. At the battles of Morgarten in 1315 and Sempach 1386, the Swiss defeated the Habsburgs, gaining increased autonomy within the Holy Roman Empire.
By 1353, the three original cantons had been joined by the cantons of Glarus and Zug and the city states of Lucerne, Zürich, and Bern, forming the "Old Federation" of eight states that persisted during much of the 15th century. Zürich was expelled from the Confederation from 1440 to 1450 due to a conflict over the territory of Toggenburg (the Old Zürich War). The Confederation's power and wealth increased significantly, with victories over Charles the Bold of Burgundy during the 1470s and the success of Swiss mercenaries.
The traditional listing order of the cantons of Switzerland reflects this state, listing the eight "Old Cantons" first, with the city states preceding the founding cantons, followed by cantons that joined the Confederation after 1481, in historical order.
The Swiss defeated the Swabian League in 1499 and gained greater collective autonomy within the Holy Roman Empire, including exemption from the Imperial reforms of 1495 and immunity from most Imperial courts. In 1506, Pope Julius II engaged the Swiss Guard, which continues to serve the papacy to the present day. The expansion of the Confederation and the reputation of invincibility acquired during the earlier wars suffered a first setback in 1515 with the Swiss defeat in the Battle of Marignano.
Reformation (1523–1648).
The Reformation in Switzerland began in 1523, led by Huldrych Zwingli, priest of the Great Minster church in Zürich since 1518. Zürich adopted the Protestant religion, joined by Berne, Basel, and Schaffhausen, while Lucerne, Uri, Schwyz, Nidwalden, Zug, Fribourg and Solothurn remained Catholic. Glarus and Appenzell were split. This led to inter-cantonal religious wars ("Kappeler Kriege") in 1529 and 1531, because each canton usually made the opposing religion illegal, and to the formation of two diets, the Protestant one meeting in Aarau and the Catholic one in Lucerne (as well as the formal full diet still meeting usually in Baden) but the Confederation survived.
During the Thirty Years' War, Switzerland was a relative "oasis of peace and prosperity" (Grimmelshausen) in war-torn Europe, mostly because all major powers in Europe depended on Swiss mercenaries, and would not let Switzerland fall into the hands of one of their rivals. Politically, they all tried to take influence, by way of mercenary commanders such as Jörg Jenatsch or Johann Rudolf Wettstein. The "Drei Bünde" of Grisons, at that point not yet a member of the Confederacy, were involved in the war from 1620, which led to their loss of the Valtellina in 1623.
Ancien Régime (1648–1798).
At the Treaty of Westphalia in 1648, Switzerland attained legal independence from the Holy Roman Empire. The Valtellina became a dependency of the "Drei Bünde" again after the Treaty and remained so until the founding of the Cisalpine Republic by Napoleon Bonaparte in 1797.
In 1653, peasants of territories subject to Lucerne, Bern, Solothurn, and Basel revolted because of currency devaluation. Although the authorities prevailed in this Swiss peasant war, they did pass some tax reforms and the incident in the long term prevented an absolutist development as would occur at some other courts of Europe. The confessional tensions remained, however, and erupted again in the Battles of Villmergen in 1656 and 1712
French Revolution and Napoleon.
During the French Revolutionary Wars, the French army invaded Switzerland and turned it into an ally known as the "Helvetic Republic," (1798–1803). It had a central government with little role for cantons. The interference with localism and traditional liberties was deeply resented, although some modernizing reforms took place. 
Resistance was strongest in the more traditional Catholic bastions, with armed uprisings breaking out in spring 1798 in the central part of Switzerland. The French Army suppressed the uprisings but support for revolutionary ideals steadily declined. The reform element was weak, and most Swiss resented their loss of local democracy, the centralization, the new taxes, the warfare, and the hostility to religion.
Major steps taken to emancipate the Jews included the repeal of special taxes and oaths in 1798. However reaction took place in 1815, and not until 1879 were the Jews granted equal rights with the Christians.
In 1803, Napoleon's Act of Mediation partially restored the sovereignty of the cantons, and the former tributary and allied territories of Aargau, Thurgau, Grisons, St. Gallen, Vaud and Ticino became cantons with equal rights. Napoleon and his enemies fought numerous campaigns in Switzerland that ruined many localities.
The Congress of Vienna of 1815 fully reestablished Swiss independence and the European powers agreed to recognize permanent Swiss neutrality. At this time, Valais, Neuchatel and Geneva also joined Switzerland as new cantons, thereby extending Swiss territory to its current boundaries.
The long-term impact of the French Revolution has been assessed by Martin: 
Civil War in 1840s.
The Radical Party and liberals made up of urban bourgeoisie and burghers, who were strong in the largely Protestant cantons, obtained the majority in the Federal Diet in the early 1840s. They proposed a new Constitution for the Swiss Confederation which would draw the several cantons into a closer relationship. In 1843, the conservative city patricians and mountain or "Ur-Swiss" from the largely Catholic cantons were opposed to the new constitution. In addition to the centralization of the Swiss government, the new Constitution also included protections for trade and other progressive reform measures.
In 1847, the Catholic cantons formed a separate union within the Confederation (the "Sonderbund"). This led to the "Sonderbundskrieg". The Radicals, fearful of a Jesuit takeover, used their control of the national government and ordered the Sonderbund disbanded. When it refused the national army attacked in a brief civil war between the Catholic and the Protestant cantons. The Sonderbund was easily defeated in less than a month; there were about 130 killed. Apart from small riots, this was the last armed conflict on Swiss territory.
Switzerland as a federal state (1848–1914).
As a consequence of the civil war, Switzerland adopted a federal constitution in 1848, amending it extensively in 1874 and establishing federal responsibility for defense, trade, and legal matters, leaving all other matters to the cantonal governments. From then, and over much of the 20th century, continuous political, economic, and social improvement has characterized Swiss history.
While Switzerland was primarily rural, the cities experienced an industrial revolution in the late 19th century, focused especially on textiles. In Basel, for example, textiles, including silk, were the leading industry. In 1888 women made up 44% of the wage earners. Nearly half the women worked in the textile mills, with household servants the second largest job category. The share of women in the workforce was higher between 1890 and 1910 than it was in the late 1960s and 1970s. 
World Wars (1914–1945).
The major powers respected Switzerland's neutrality during World War I. In the Grimm–Hoffmann Affair, the Allies complained when a proposal by one politician to negotiate peace on the eastern Front was denounced by the Allies, who wanted the war there to go on so as to tie Germany down.
While the industrial sector began to grow in the mid-19th century, Switzerland's emergence as one of the most prosperous nations in Europe--the "Swiss miracle"-- was a development of the short 20th century, among other things tied to the role of Switzerland during the World Wars.
World War II.
During World War II, Germany considered invading, but never attacked. Under General Henri Guisan, the Swiss army prepared for mass mobilization of militia forces against invasion, and prepared strong, well-stockpiled positions high in the Alps known as the "Réduit". Switzerland remained independent and neutral through a combination of military deterrence, economic concessions to Germany, and good fortune as larger events during the war delayed an invasion.
Attempts by Switzerland's small Nazi party to cause an "Anschluss" with Germany failed miserably, largely due to Switzerland's multicultural heritage, strong sense of national identity, and long tradition of direct democracy and civil liberties. The Swiss press vigorously criticized the Third Reich, often infuriating German leaders. Switzerland was an important base for espionage by both sides in the conflict and often mediated communications between the Axis and Allied powers.
Switzerland's trade was blockaded by both the Allies and by the Axis. Both sides openly exerted pressure on Switzerland not to trade with the other. Economic cooperation and extension of credit to the Third Reich varied according to the perceived likelihood of invasion, and the availability of other trading partners. Concessions reached their zenith after a crucial rail link through Vichy France was severed in 1942, leaving Switzerland completely surrounded by the Axis. Switzerland relied on trade for half of its food and essentially all of its fuel, but controlled vital trans-alpine rail tunnels between Germany and Italy.
Switzerland's most important exports during the war were precision machine tools, watches, jewel bearings (used in bombsights), electricity, and dairy products. During World War Two, the Swiss franc was the only remaining major freely convertible currency in the world, and both the Allies and the Germans sold large amounts of gold to the Swiss National Bank. Between 1940 and 1945, the German Reichsbank sold 1.3 billion francs worth of gold to Swiss Banks in exchange for Swiss francs and other foreign currency.
Hundreds of millions of francs worth of this gold was monetary gold plundered from the central banks of occupied countries. 581,000 francs of "Melmer" gold taken from Holocaust victims in eastern Europe was sold to Swiss banks. In total, trade between Germany and Switzerland contributed about 0.5% to the German war effort but did not significantly lengthen the war.
Over the course of the war, Switzerland interned 300,000 refugees. 104,000 of these were foreign troops interned according to the Rights and Duties of Neutral Powers outlined in the Hague Conventions. The rest were foreign civilians and were either interned or granted tolerance or residence permits by the cantonal authorities. Refugees were not allowed to hold jobs. 60,000 of the refugees were civilians escaping persecution by the Nazis. Of these, 26,000 to 27,000 were Jews. Between 10,000 and 25,000 civilian refugees were refused entry. At the beginning of the war, Switzerland had a Jewish population of between 18,000 and 28,000 and a total population of about 4 million.
Within Switzerland at the time of the conflict there was moderate polarization. Some were pacifists. Some took sides according to international capitalism or international communism. Others leaned more towards their language group, with some in French-speaking areas more pro-Allied, and some in Swiss-German areas more pro-Axis. The government attempted to thwart the activities of any individual, party, or faction in Switzerland that acted with extremism or attempted to break the unity of the nation. The Swiss-German speaking areas moved linguistically further away from the standard (high) German spoken in Germany, with more emphasis on local Swiss dialects.
In the 1960s, significant controversy arose among historians regarding the nation's relations with Nazi Germany.
By the 1990s the controversies included a class-action lawsuit brought in New York over Jewish assets in Holocaust-era bank accounts. The government commissioned an authoritative study of Switzerland's interaction with the Nazi regime. The final report by this independent panel of international scholars, known as the Bergier Commission, was issued in 2002.
After 1945.
During the Cold War, Swiss authorities considered the construction of a Swiss nuclear bomb. Leading nuclear physicists at the Federal Institute of Technology Zurich such as Paul Scherrer made this a realistic possibility. However, financial problems with the defense budget prevented the substantial funds from being allocated, and the Nuclear Non-Proliferation Treaty of 1968 was seen as a valid alternative. All remaining plans for building nuclear weapons were dropped by 1988.
From 1959, the Federal Council, elected by the parliament, is composed of members of the four major parties, the Protestant Free Democrats, the Catholic Christian Democrats, the left-wing Social Democrats and the right-wing People's Party, essentially creating a system without a sizeable parliamentary opposition (see concordance system), reflecting the powerful position of an opposition in a direct democracy.
In 1963, Switzerland joined the Council of Europe. In 1979, parts of the canton of Bern attained independence, forming the new canton of Jura.
Switzerland's role in many United Nations and international organizations helped to mitigate the country's concern for neutrality. In 2002, Switzerland voters gave 55% of their vote in 2002 in favour of the UN, and joined the United Nations. It followed decades of debate and its rejection of membership in 1986 by a 3-1 popular vote.
Switzerland is not a member state of the EU, but has been (together with Liechtenstein) surrounded by EU territory since the joining of Austria in 1995. In 2005, Switzerland agreed to join the Schengen treaty and Dublin Convention by popular vote.
Women's suffrage.
After a century of agitation, women were finally granted the right to vote in 1971. An equal rights amendment was ratified in 1981. In one area, however, Switzerland was a pioneer for rights of women from other countries. From 1870 to 1914 thousands of women from all over the world went to Swiss universities to study medicine; indeed the first women physicians in most European countries received their medical education there.
See also.
General:

</doc>
<doc id="27475" url="http://en.wikipedia.org/wiki?curid=27475" title="Politics of Syria">
Politics of Syria

Politics in the Syrian Arab Republic takes place in the framework of what is officially a semi-presidential republic, but others disagree with that assessment. The CIA claims that the power is in the hands of the President of Syria and his family, all members of the ruling Arab Socialist Ba'ath Party which is a cell of the Syrian-dominated Ba'ath Party (established in 1966 when the original Ba'ath Party was dissolved and split into two).
Decrees issued by the president must be approved by the People's Council to become law, except during a state of emergency which was in force until 21 April 2011 when it was lifted during the Syrian uprising, (the end of it being one of the key demands of the protesters). The Ba'ath Party is Syria's ruling party and the previous Syrian constitution of 1973 stated that "the Arab Socialist Ba'ath Party leads society and the state." At least 167 seats of the 250-member parliament were guaranteed for the National Progressive Front, which is a coalition of the Ba'ath Party and several other much smaller allied parties. The new Syrian constitution of 2012 introduced multi-party system based on the principle of political pluralism without guaranteed leadership of any political party. The Syrian army and security services maintained a considerable presence in the neighbouring Lebanese Republic from 1975 until 24 April 2005.
Background.
Hafez al-Assad took power in 1970, and after his death in 2000 his son Bashar al-Assad succeeded him as President. A surge of interest in political reform took place after Bashar al-Assad assumed power in 2000. Human-rights activists and other civil-society advocates, as well as some parliamentarians, became more outspoken during a period referred to as the "Damascus Spring" (July 2000-February 2001). Assad also made a series of appointments of reform-minded advisors to formal and less formal positions, and included a number of similarly oriented individuals in his Cabinet.
Neo-Ba'athism.
The Ba'ath platform is proclaimed succinctly in the party's slogan: "Unity, freedom, and socialism." The party is both socialist, advocating state ownership of the means of industrial production and the redistribution of agricultural land (in practice, Syria's nominally socialist economy is effectively a mixed economy, composed of large state enterprises and private small businesses), and revolutionary, dedicated to carrying a pan-Arab revolution to every part of the Arab world. Founded by Michel Aflaq, a Syrian Christian, Salah al-Din al-Bitar, a Syrian Sunni, and Zaki al-Arsuzi, an alawite, the Arab Socialist Ba'ath Party, which was dissolved in 1966 following the 1966 Syrian coup d'état which led to the establishment of one Iraqi-dominated ba'ath movement and one Syrian-led ba'ath movement. The party embraces secularism and has attracted supporters of all faiths in many Arab countries, especially Iraq, Jordan, and Lebanon. 
Since August 1990, however, the party has tended to de-emphasize socialism and to stress pan-Arab unity.
Six smaller political parties are permitted to exist and, along with the Ba'ath Party, make up the National Progressive Front (NPF), a grouping of parties that represents the sole framework of legal political party participation for citizens. While created ostensibly to give the appearance of a multi-party system, the NPF is dominated by the Ba'ath Party and does not change the essentially one-party character of the political system. Non-Ba'ath Party members of the NPF exist as political parties largely in name only and conform strictly to Ba'ath Party and government policies. There were reports in 2000 that the government was considering legislation to expand the NPF to include new parties and several parties previously banned; these changes have not taken place. However, one such party- the Syrian Social Nationalist Party- was legalised in 2005.
Traditionally, the parties of the NPF accepted the socialist and Arab nationalist ideology of the government. However, the SSNP was the first party that is neither socialist nor Arab nationalist in orientation to be legalised and admitted to the NPF. This has given rise to suggestions that broader ideological perspectives may be afforded some degree of toleration in the future, but ethnically-based (Kurdish and Assyrian) parties continue to be repressed and a strict ban on religious parties is still enforced.
Syria's Emergency Law was in force from 1963, when the Ba'ath Party came to power, until 21 April 2011 when it was rescinded by Bashar al-Assad (decree 161). The law, justified on the grounds of the continuing war with Israel and the threats posed by terrorists, suspended most constitutional protections.
Government administration.
The previous Syrian constitution of 1973 vested the Ba'ath Party (formally the Arab Ba'ath Socialist Party) with leadership functions in the state and society and provided broad powers to the president. The president, approved by referendum for a 7-year term, was also Secretary General of the Ba'ath Party and leader of the National Progressive Front. During the 2011–2012 Syrian uprising, a new constitution was put to a referendum. Amongst other changes, it abolished the old article 8 which entrenched the power of the Ba'ath party. The new article 8 reads: "The political system of the state shall be based on the principle of political pluralism, and exercising power democratically through the ballot box". In a new article 88, it introduced presidential elections and limited the term of office for the president to seven years with a maximum of one re-election. The referendum resulted in the adoption of the new constitution, which came into force on 27 February 2012.
The president has the right to appoint ministers (Council of Ministers), to declare war and states of emergency, to issue laws (which, except in the case of emergency, require ratification by the People's Council), to declare amnesty, to amend the constitution, and to appoint civil servants and military personnel. The late President Hafiz al-Asad was confirmed by unopposed plebiscites five times. His son and current President Bashar al-Asad, was confirmed by an unopposed referendum in July 2000. He was confirmed again on 27 May 2007 (next to be held in May 2014) with 97.6% of the vote
Along with the National Progressive Front, the president decides issues of war and peace and approves the state's 5-year economic plans. The National Progressive Front also acts as a forum in which economic policies are debated and the country's political orientation is determined.
The Syrian constitution of 2012 requires that the president be Muslim but does not make Islam the state religion. The judicial system in Syria is an amalgam of Ottoman, French, and Islamic laws, with three levels of courts: courts of first instance, courts of appeals, and the constitutional court, the highest tribunal. In addition, religious courts handle questions of personal and family law.
The Ba'ath Party emphasizes socialism and secular Pan-Arabism. Despite the Ba'ath Party's doctrine on building national rather than ethnic identity, the issues of ethnic, religious, and regional allegiances still remain important in Syria.
Legislative branch.
The People's Council ("Majlis al-Sha'ab") has 250 members elected for a four-year term in 15 multi-seat constituencies. According to the previous Syrian constitution of 1973 Syria was a single-party state and only one political party, the Arab Socialist Ba'ath Party was legally allowed to hold effective power. Of the 250 seats in the council, 167 were guaranteed for the National Progressive Front (founded in 1972) and 134 of these (as of 2007) were members of the Ba'ath Party. The minor parties in the Progressive Front, were legally required to accept the leadership of the Ba'ath Party. The other parties in the Progressive Front, for example, are not allowed to canvass for supporters in the army or the student body which are "reserved exclusively for the Ba'ath." The new Syrian constitution of 2012 introduced multi-party system without guaranted leadership of any political party.
Political parties and elections.
The last parliamentary election was on 7 May 2012 and the results were announced on 15 May. 
The Baath party won an even larger victory than it did in previous elections. They won a majority of around 60% of the 250 parliamentary seats. Previously, the Baath had a majority of just over 50% of the seats in parliament. If one adds in the independent MPs aligned with the Baath Party, the MPs who support the president make up over 90% of the seats in new parliament. The National Unity List, which is dominated by the Syrian Baath Party, won more than 150 seats in the 250 member parliament. Independent individuals won more than 90 seats. Among the newly established opposition parties (established since August 2011), only one single seat was won, namely a seat in Aleppo won by the Syrian Democratic Party, Ahmad Koussa. In addition three representatives of longstanding opposition parties have been elected to Parliament: Qadri Jamil and Ali Haydar from the Front for Change and Liberation, and Amro Osi from the Initiative of Syrian Kurds.
International organization participation.
Syria is a member of the Arab Bank for Economic Development in Africa, Arab Fund for Economic and Social Development, Arab Monetary Fund, Council of Arab Economic Unity, Customs Cooperation Council, Economic and Social Commission for Western Asia, Food and Agricultural Organization, Group of 24, Group of 77, International Atomic Energy Agency, International Bank for Reconstruction and Development, International Civil Aviation Organization, International Chamber of Commerce, International Development Association, Islamic Development Bank, International Fund for Agricultural Development, International Finance Corporation, International Labour Organization, International Monetary Fund, International Maritime Organization, INTELSAT, INTERPOL, International Olympic Committee, International Organization for Standardization, International Telecommunication Union, International Federation of Red Cross and Red Crescent Societies, Non-Aligned Movement, Organization of Arab Petroleum Exporting Countries, Organisation of Islamic Cooperation, United Nations, UN Commission on Human Rights, UN Conference on Trade and Development, UN Industrial Development Organization, UN Relief and Works Agency for Palestine Refugees in the Near East, Universal Postal Union, World Federation of Trade Unions, World Health Organization, World Meteorological Organization, and World Tourism Organization.
Syria's diplomats last sat on the UN Security Council, (as a non-permanent member) in December 2003.

</doc>
<doc id="27578" url="http://en.wikipedia.org/wiki?curid=27578" title="Survey sampling">
Survey sampling

In statistics, survey sampling describes the process of selecting a sample of elements from a target population to conduct a survey. 
The term "survey" may refer to many different types or techniques of observation. In survey sampling it most often involves a questionnaire used to measure the characteristics and/or attitudes of people. Different ways of contacting members of a sample once they have been selected is the subject of survey data collection. The purpose of sampling is to reduce the cost and/or the amount of work that it would take to survey the entire target population. A survey that measures the entire target population is called a census.
Survey samples can be broadly divided into two types: probability samples and non-probability samples. Probability-based samples implement a sampling plan with specified probabilities (perhaps adapted probabilities specified by an adaptive procedure). Probability-based sampling allows design-based inference about the target population. The inferences are based on a known objective probability distribution that was specified in the study protocol. Inferences from probability-based surveys may still suffer from many types of bias.
Surveys that are not based on probability sampling have greater difficulty measuring their bias or sampling error. Surveys based on non-probability samples often fail to represent the people in the target population. 
In academic and government survey research, probability sampling is a standard procedure. In the USA, the Office of Management and Budget's "List of Standards for Statistical Surveys" states that federally funded surveys must be performed:
selecting samples using generally accepted statistical methods (e.g., probabilistic methods that can provide estimates of sampling error). Any use of nonprobability sampling methods (e.g., cut-off or model-based samples) must be justified statistically and be able to measure estimation error.
Besides, random sampling and design-based inference are supplemented by other statistical methods, such as model-assisted sampling and model-based sampling.
For example, many surveys have substantial amounts of nonresponse. Even though the units are initially chosen with known probabilities, the nonresponse mechanisms are unknown. For surveys with substantial nonresponse, statisticians have proposed statistical models, with which data sets are analyzed.
Probability sampling.
In a probability sample (also called "scientific" or "random" sample) each member of the target population has a known and non-zero probability of inclusion in the sample. A survey based on a probability sample can in theory produce statistical measurements of the target population that are: 
A probability-based survey sample is created by constructing a list of the target population, called the sample frame, a randomized process for selecting units from the sample frame, called a selection procedure, and a method of contacting selected units to and enabling them complete the survey, called a data collection method or mode. For some target populations this process may be easy, for example, sampling the employees of a company by using payroll list. However, in large, disorganized populations simply constructing a suitable sample frame is often a complex and expensive task. 
Common methods of conducting a probability sample of the household population in the United States are Area Probability Sampling, Random Digit Dial telephone sampling, and more recently, Address-Based Sampling. 
Within probability sampling, there are specialized techniques such as stratified sampling and cluster sampling that improve the precision or efficiency of the sampling process without altering the fundamental principles of probability sampling. 
Stratification is the process of dividing members of the population into homogeneous subgroups before sampling. The strata should be mutually exclusive: every element in the population must be assigned to only one stratum. The strata should also be collectively exhaustive: no population element can be excluded. Then methods such as simple random sampling or systematic sampling can be applied within each stratum. This often improves the representativeness of the sample by reducing sampling error.
Bias in probability sampling.
Bias in surveys is undesirable, but often unavoidable. The major types of bias that may occur in the sampling process are:
Non-probability sampling.
Many surveys are not based on probability samples, but rather on finding a suitable collection of respondents to complete the survey. Some common examples of non-probability sampling are:
In non-probability samples the relationship between the target population and the survey sample is immeasurable and potential bias is unknowable. Sophisticated users of non-probability survey samples tend to view the survey as an experimental condition, rather than a tool for population measurement, and examine the results for internally consistent relationships.
Further reading.
The textbook by Groves et alia provides an overview of survey methodology, including recent literature on questionnaire development (informed by cognitive psychology) : 
The other books focus on the statistical theory of survey sampling and require some knowledge of basic statistics, as discussed in the following textbooks:
The elementary book by Scheaffer et alia uses quadratic equations from high-school algebra: 
More mathematical statistics is required for Lohr, for Särndal et alia, and for Cochran (classic):
The historically important books by Deming and Kish remain valuable for insights for social scientists (particularly about the U.S. census and the Institute for Social Research at the University of Michigan): 

</doc>
<doc id="27579" url="http://en.wikipedia.org/wiki?curid=27579" title="Statistical theory">
Statistical theory

The theory of statistics provides a basis for the whole range of techniques, in both study design and data analysis, that are used within applications of statistics. The theory covers approaches to statistical-decision problems and to statistical inference, and the actions and deductions that satisfy the basic principles stated for these different approaches. Within a given approach, statistical theory gives ways of comparing statistical procedures; it can find a best possible procedure within a given context for given statistical problems, or can provide guidance on the choice between alternative procedures.
Apart from philosophical considerations about how to make statistical inferences and decisions, much of statistical theory consists of mathematical statistics, and is closely linked to probability theory, to utility theory, and to optimization.
Scope.
Statistical theory provides an underlying rationale and provides a consistent basis for the choice of methodology used in applied statistics.
Modelling.
Statistical models describe the sources of data and can have different types of formulation corresponding to these sources and to the problem being studied. Such problems can be of various kinds:
Statistical models, once specified, can be tested to see whether they provide useful inferences for new data sets. Testing a hypothesis using the data that was used to specify the model is a fallacy, according to the natural science of Bacon and the scientific method of Peirce.
Data collection.
Statistical theory provides a guide to comparing methods of data collection, where the problem is to generate informative data using optimization and randomization while measuring and controlling for observational error. Optimization of data collection reduces the cost of data while satisfying statistical goals, while randomization allows reliable inferences. Statistical theory provides a basis for good data collection and the structuring of investigations in the topics of:
Summarising data.
The task of summarising statistical data in conventional forms (also known as descriptive statistics) is considered in theoretical statistics as a problem of defining what aspects of statistical samples need to be described and how well they can be described from a typically limited sample of data. Thus the problems theoretical statistics considers include:
Interpreting data.
Besides the philosophy underlying statistical inference, statistical theory has the task of considering the types of questions that data analysts might want to ask about the problems they are studying and of providing data analytic techniques for answering them. Some of these tasks are:
When a statistical procedure has been specified in the study protocol, then statistical theory provides well-defined probability statements for the method when applied to all populations that could have arisen from the randomization used to generate the data. This provides an objective way of estimating parameters, estimating confidence intervals, testing hypotheses, and selecting the best. Even for observational data, statistical theory provides a way of calculating a value that can be used to interpret a sample of data from a population, it can provide a means of indicating how well that value is determined by the sample, and thus a means of saying corresponding values derived for different populations are as different as they might seem; however, the reliability of inferences from post-hoc observational data is often worse than for planned randomized generation of data.
Applied statistical inference.
Statistical theory provides the basis for a number of data analytic methods that are common across scientific and social research. Some of these are: 
Interpreting data is an important objective of statistical research:
Many of the standard methods for these tasks rely on certain statistical assumptions (made in the derivation of the methodology) actually holding in practice. Statistical theory studies the consequences of departures from these assumptions. In addition it provides a range of robust statistical techniques that are less dependent on assumptions, and it provides methods checking whether particular assumptions are reasonable for a give data-set.

</doc>
<doc id="27581" url="http://en.wikipedia.org/wiki?curid=27581" title="Statistical assembly">
Statistical assembly

In statistics, for example in statistical quality control, a statistical assembly is a collection of parts or components which makes up a statistical unit. Thus a statistical unit, which would be the prime item of concern, is made of discrete components like organs or machine parts. The reliability of the statistical unit is, in part, determined by the reliability of the components in the statistical assembly, and by their interactions.
Much of the observation of a statistical assembly requires special preparation of the unit, which demands that the intervention must not prejudice the observations. A simple version of this kind of research uses the stimulus-response model.
In other contexts, statistical assembly refers to the process of constructing a manufactured item which must be carefully specified to contain given amounts of nonuniformity within it.
External links.
http://adcats.et.byu.edu/ADCATS/Theory/AutoCAD_Analyzer/1_2D_Overview/1_2D_Overview-.html

</doc>
<doc id="27585" url="http://en.wikipedia.org/wiki?curid=27585" title="Statistical population">
Statistical population

In statistics, a population is a complete set of items that share at least one property in common that is the subject of a statistical analysis. For example, the population of German people share a common geographic origin, language, literature, and genetic heritage, among other traits, that distinguish them from people of different nationalities. As another example, the Milky Way galaxy comprises a star population. In contrast, a statistical sample is a subset drawn from the population to represent the population in a statistical analysis. If a sample is chosen properly, characteristics of the entire population that the sample is drawn from can be inferred from corresponding characteristics of the sample.
Subpopulation.
A subset of a population is called a subpopulation if they share one or more additional properties. For example, if the population is all German people, a subpopulation is all German males; if the population is all pharmacies in the world, a subpopulation is all pharmacies in Egypt.
In contrast, a subset of a population that does not require the sharing of any additional property is called a "sample".
Descriptive statistics may yield different results for different subpopulations. For instance, a particular medicine may have different effects on different subpopulations, and these effects may be obscured or dismissed if such special subpopulations are not identified and examined in isolation.
Similarly, one can often estimate parameters more accurately if one separates out subpopulations: the distribution of heights among people is better modeled by considering men and women as separate subpopulations, for instance.
Populations consisting of subpopulations can be modeled by mixture models, which combine the distributions within subpopulations into an overall population distribution. Even if subpopulations are well-modeled by given simple models, the overall population may be poorly fit by a given simple model – poor fit may be evidence for existence of subpopulations. For example, given two equal subpopulations, both normally distributed, if they have the same standard deviation and different means, the overall distribution will exhibit low kurtosis relative to a single normal distribution – the means of the subpopulations fall on the shoulders of the overall distribution. If sufficiently separated, these form a bimodal distribution, otherwise it simply has a wide peak. Further, it will exhibit overdispersion relative to a single normal distribution with the given variation. Alternatively, given two subpopulations with the same mean and different standard deviations, the overall population will exhibit high kurtosis, with a sharper peak and heavier tails (and correspondingly shallower shoulders) than a single distribution.

</doc>
<doc id="27608" url="http://en.wikipedia.org/wiki?curid=27608" title="Science fiction on television">
Science fiction on television

Science fiction first appeared on a television program during the Golden Age of Science Fiction. Special effects and other production techniques allow creators to present a living visual image of an imaginary world not limited by the constraints of reality.
Science fiction television production process and methods.
The need to portray imaginary settings or characters with properties and abilities beyond the reach of current reality obliges producers to make extensive use of specialized techniques of television production.
Through most of the 20th century, many of these techniques were expensive and involved a small number of dedicated craft practitioners, while the reusability of props, models, effects, or animation techniques made it easier to keep using them. The combination of high initial cost and lower maintenance cost pushed producers into building these techniques into the basic concept of a series, influencing all the artistic choices. By the late 1990s, improved technology and more training and cross-training within the industry made all of these techniques easier to use, so that directors of individual episodes could make decisions to use one or more methods, so such artistic choices no longer needed to be baked into the series concept.
Special effects.
Special effects (or "SPFX") have been an essential tool throughout the history of science fiction on television: small explosives to simulate the effects of various rayguns, squibs of blood and gruesome prosthetics to simulate the monsters and victims in horror series, and the wire-flying entrances and exits of George Reeves as Superman.
The broad term "special effects" includes all the techniques here, but more commonly there are two categories of effects. Visual effects ("VFX") involve photographic or digital manipulation of the onscreen image, usually done in post-production. Mechanical or physical effects involve props, pyrotechnics, and other physical methods used during principal photography itself. Some effects involved a combination of techniques; a ray gun might require a pyrotechnic during filming, and then an optical glowing line added to the film image in post-production. Stunts are another important category of physical effects. In general, all kinds of special effects must be carefully planned during pre-production.
Computer-generated imagery.
"Babylon 5" was the first series to use computer-generated imagery, or "CGI", for all exterior space scenes, even those with characters in space suits. The technology has made this more practical, so that today models are rarely used. In the 1990s, CGI required expensive processors and customized applications, but by the 2000s (decade), computing power has pushed capabilities down to personal laptops running a wide array of software.
Models and Puppets.
Models have been an essential tool in science fiction television since the beginning, when Buck Rogers took flight in spark-scattering spaceships wheeling across a matte backdrop sky. The original "" required a staggering array of models; the USS "Enterprise" had to be built in several different scales for different needs. Models fell out of use in filming in the 1990s as CGI became more affordable and practical, but even today, designers sometimes construct scale models which are then digitized for use in animation software.
Models of characters are puppets. Gerry Anderson created a series of shows using puppets living in a universe of models and miniature sets, notably "Thunderbirds". In recent years, series like "Greg the Bunny" and "Puppets Who Kill" have portrayed puppets as an oppressed minority, for which the politically correct term is "fabricated-Americans" and the racial epithet is "sock". "ALF" depicted an alien living in a family, while "Farscape" included two puppets as regular characters. In "Stargate SG-1", the Asgard characters are puppets in scenes where they are sitting, standing, or lying down.
Animation.
As animation is completely free of the constraints of gravity, momentum, and physical reality, it is an ideal technique for science fiction and fantasy on television. In a sense, virtually all animated series allow characters and objects to perform in unrealistic ways, so they are almost all considered to fit within the broadest category of speculative fiction (in the context of awards, criticism, marketing, etc.) The artistic affinity of animation to comic books has led to a large amount of superhero-themed animation, much of this adapted from comics series, while the impossible characters and settings allowed in animation made this a preferred medium for both fantasy and for series aimed at young audiences.
Originally, animation was all hand-drawn by artists, though in the 1980s, beginning with "Captain Power", computers began to automate the task of creating repeated images; by the 1990s, hand-drawn animation became defunct.
Animation in live-action.
In recent years as technology has improved, this has become more common, notably since the development of the Massive software application permits producers to include hordes of non-human characters to storm a city or space station. The robotic Cylons in the new version of "Battlestar Galactica" are usually animated characters, while the Asgard in "Stargate SG-1" are animated when they are shown walking around or more than one is on screen at once.
Science fiction television economics and distribution.
In general, science fiction series are subject to the same financial constraints as other television shows. However, high production costs increase the financial risk, while limited audiences further complicate the business case for continuing production. ' was the first television series to cost more than $100,000 per episode, while ' was the first to cost more than $1 million per episode.
The innovative nature of science fiction means that new shows cannot rely on predictable market-tested formulas like legal dramas or sitcoms; the involvement of creative talent outside the Hollywood mainstream introduces more variables to the budget forecasts.
In the past, science fiction television shows have maintained a family friendly format that rendered them suitable for all ages, especially children, as the majority of them were of the action-adventure format. This enabled merchandising such as toy lines, animated cartoon adaptations, and other licensing. However, many modern shows include a significant amount of adult themes (such as sexual situations, nudity, profanity and graphic violence) rendering them unsuitable for young audiences, and severely limiting the remaining audience demographic and the potential for merchandising.
The perception, more than the reality, of science fiction series being cancelled unreasonably is greatly increased by the attachment of fans to their favorite series, which is much stronger in science fiction fandom than it is in the general population. While mainstream shows are often more strictly episodic, where ending shows can allow viewers to imagine that characters live happily, or at least normally, ever after, science fiction series generate questions and loose ends that, when unresolved, cause dissatisfaction among devoted viewers. Creative settings also often call for broader story arcs than is often found in mainstream television, requiring science fiction series many episodes to resolve an ongoing major conflict. Science fiction television producers will sometimes end a season with a dramatic cliffhanger episode to attract viewer interest, but the short-term effect rarely influences financial partners. "Dark Angel" is one of many shows ending with a cliffhanger scene that left critical questions open when the series was cancelled.
Media fandom.
One of the earliest forms of media fandom was Star Trek fandom. Fans of the series became known to each other through the science fiction fandom. In 1968, NBC decided to cancel '. Bjo Trimble wrote letters to contacts in the National Fantasy Fan Foundation, asking people to organize their local friends to write to the network to demand the show remain on the air. Network executives were overwhelmed by an unprecedented wave of correspondence, and they kept the show on the air. Although the series continued to receive low ratings and was canceled a year later, the enduring popularity of the series resulted in Paramount creating a set of movies, and then a new series ', which by the early 1990s had become one of the most popular dramas on American television.
Although somewhat smaller, "Doctor Who" fandom considerably predates "Star Trek" fandom. Meanwhile, "Star Trek" fans continued to grow in numbers, and began organizing conventions in the 1970s. No other show attracted a large organized following until the 1990s, when "Babylon 5" attracted both "Star Trek" fans and a large number of literary SF fans who previously had not been involved in media fandom. Other series began to attract a growing number of followers.
In the late 1990s, "Buffy the Vampire Slayer" drew a large mainstream audience into fandom; greater demand allowed (even obliged, for the sake of time management) "Buffy" actors to charge much higher appearance fees than the "Star Trek" actors had. This pushed appearances out of the reach of some volunteer non-profit fan groups towards commercial event promoters. At the same time, a market for celebrity autographs emerged on eBay, which created a new source of income for actors, who began to charge money for autographs that they had previously been doing for free. This became significant enough that lesser-known actors would come to conventions without requesting any appearance fee, simply to be allowed to sell their own autographs (commonly on publicity photos). Today most events with actor appearances are organized by commercial promoters, though a number of fan-run conventions still exist, such as Toronto Trek and Shore Leave.
The 1985 series "Robotech" is most often credited as the catalyst for the Western interest in anime. The series inspired a few fanzines such as "Protoculture Addicts" and "Animag" both of which in turn promoted interest in the wide world of anime in general. Anime's first notable appearance at SF or comic book conventions was in the form of video showings of popular anime, untranslated and often low quality VHS bootlegs. Starting in the 1990s, anime fans began organizing conventions. These quickly grew to sizes much larger than other science fiction and media conventions in the same communities; many cities now have anime conventions attracting five to ten thousand attendees. Many anime conventions are a hybrid between non-profit and commercial events, with volunteer organizers handling large revenue streams and dealing with commercial suppliers and professional marketing campaigns.
For decades, the majority of science fiction media fandom has been represented by males of all ages and for most of its modern existence, a fairly diverse racial demographic. The most highly publicized demographic for science fiction fans is the male adolescent; roughly the same demographic for American comic books. Female fans, while always present, were far fewer in number and less conspicuously present in fandom. With the rising popularity of fanzines, female fans became increasingly vocal. Starting in the 2000s (decade), genre series began to offer more prominent female characters. Many series featured women as the main characters with males as supporting characters. "True Blood" is an example. Also, such shows premises moved away from heroic action-adventure and focused more on characters and their relationships. This has caused the rising popularity of fanfiction, a large majority of which is categorized as slash fanfiction. Female fans comprise the majority of fanfiction writers.
Science fiction television history and culture.
U.S. television science fiction.
U.S. television science fiction has produced "" and its various spin-off shows of the Star Trek franchise, "The Twilight Zone," "The X-Files," and many others.
British television science fiction.
British television science fiction began when the broadcast medium was in its infancy. Despite an occasionally chequered history, popular programmes in the genre have been produced by both the BBC and the largest commercial channel, ITV. "Doctor Who" is listed in the "Guinness World Records" as the longest-running science fiction television show in the world and as the "most successful" science fiction series of all time.
Canadian science fiction television.
Science fiction in Canada was produced by the CBC as early as the 1950s. In the 1970s, CTV produced "The Starlost". In the 1980s, Canadian animation studios including Nelvana, began producing a growing proportion of the world market in animation.
In the 1990s, Canada became an important player in live action speculative fiction on television, with dozens of series like "Forever Knight", "", and most notably "The X-Files" and "Stargate SG-1". Many series have been produced for youth and children's markets, including "Deepwater Black" and "MythQuest".
In the first decade of the 21st century, changes in provincial tax legislation prompted many production companies to move from Toronto to Vancouver. Recent popular series produced in Vancouver include "The Dead Zone", "Smallville", "Andromeda", "Stargate Atlantis", "Stargate Universe", "The 4400", "Sanctuary" and the reimagined "Battlestar Galactica".
Because of the small size of the domestic television market, most Canadian productions involve partnerships with production studios based in the United States and Europe. However, in recent years, new partnership arrangements are allowing Canadian investors a growing share of control of projects produced in Canada and elsewhere.
Australian science fiction television.
Australia's best known Science Fiction series was "Farscape"; made with American co-production, it ran from 1999 to 2003. Early series made in the 1960s included "The Interparis" (1968) "Vega 4" (1967), and "Phoenix Five" (1970). A significant proportion of Australian produced Science Fiction programmes are made for the teens/young Adults market, including "The Girl from Tomorrow", the long-running "Mr. Squiggle", "Halfway Across the Galaxy and Turn Left", "Ocean Girl", "Crash Zone", "Watch This Space" and "Spellbinder".
Other series like "Time Trax", "Roar" and "" were filmed in Australia, but used mostly US crew and actors.
Japanese television science fiction.
Japan has a long history of producing science fiction series for television. Some of the most famous are anime such as Osamu Tezuka's "Astro Boy", the Super Robots such as Mitsuteru Yokoyama's "Tetsujin 28-go" ("Gigantor") and Go Nagai's "Mazinger Z", and the Real Robots such as Yoshiyuki Tomino's "Gundam" series and Shōji Kawamori's "Macross" series.
Other primary aspects of Japanese science fiction television are the superhero "tokusatsu" (a term literally meaning special effects) series, pioneered by programs such as "Moonlight Mask" and "Planet Prince". The suitmation technique has been used in long running franchises include Eiji Tsuburaya's Ultra Series, Shotaro Ishinomori's Kamen Rider Series, and the Super Sentai Series.
In addition, several dramas utilize science fiction elements as framing devices, but are not labeled as "tokusatsu" as they do not utilize actors in full body suits and other special effects.
Continental European science fiction series.
Northern European series.
Among the notable German language productions is "Lexx" and "Raumpatrouille", a German series first broadcast in 1966. Mmovies by Rainer Erler, include the miniseries "Das Blaue Palais".<br>
"Star Maidens" (1975, aka "Medusa" or "Die Mädchen aus dem Weltraum") was a British-German coproduction of pure SF. Danish television broadcast the children's TV-series "Crash" in 1984 about a boy who finds out that his room is a space ship.
Early Dutch television series were "" (tomorrow it will happen), broadcast from 1957 to 1959, about a group of Dutch space explorers and their adventures, "De duivelsgrot" (the devil's cave), broadcast from 1963 to 1964, about a scientist who finds the map of a cave that leads to the center of the earth and "Treinreis naar de Toekomst" (train journey to the future) about two young children who are taken to the future by robots who try to recreate humanity, but are unable to give the cloned humans a soul. All three of these television series where aimed mostly at children.
Later television series were "Professor Vreemdeling" (1977) about a strange professor who wants to make plants speak and "" (1997) a nationalistic post-apocalyptic series where the Netherlands has been built full of housing and the highways are filled with traffic jams. The protagonist, a female superhero, wears traditional folkloric clothes and tries to save traditional elements of Dutch society against the factory owners.
Italian series.
Italian TV shows include "A come Andromeda" (1972) was a remake of 1962 BBC miniseries (from the novels of Hoyle and Elliott), two original teleplays created by Flavio Nicolini : "Gamma" (1974) and "La traccia verde" (1975) and "Geminus" (1968) and "Il segno del comando"(1970) a mystery series.
French series.
French series are "" French science-fiction/fantasy television series (both co-produced with Canada) and a number of smaller fiction/fantasy television series, including "Tang" in 1971, about a secret organization that attempts to control the world with a new super weapon, "Les atomistes" and 1970 miniseries "La brigade des maléfices".
Another French-produced science fiction series was the new age animated series "Il était une fois... l'espace" (English: Once upon a time...space). Anime-influenced animation includes a series of French-Japanese cartoons/anime, including such titles as "Ulysses 31" (1981), "The Mysterious Cities of Gold" (1982), and "Ōban Star-Racers" (2006).
Eastern European series.
Serbia produced "The Collector" ("Sakupljač"), a science fiction television series based upon Zoran Živković's story, winner of a World Fantasy Award. Several science-fiction series were also produced in various European countries, and never translated into English.
Significant creative influences.
For a list of notable science fiction series and programs on television, see: List of science fiction television programs.
People who have influenced science fiction on television include:

</doc>
<doc id="27615" url="http://en.wikipedia.org/wiki?curid=27615" title="Steel guitar">
Steel guitar

Steel guitar is a type of guitar or the method of playing the instrument. Developed in Hawaii in the late 19th and early 20th centuries, a steel guitar is usually positioned horizontally; strings are plucked with one hand, while the other hand changes the pitch of one or more strings with the use of a bar or slide called a steel (generally made of metal, but also of glass or other materials). The earliest use of an electrified steel guitar was first made in the early 1930s by Bob Dunn of Milton Brown and His Brownies, the original Western Swing Band from Fort Worth, Texas; the instrument was perfected in the mid to late 1930s by Fort Worth's Leon McAluff, who played for Western Swing's greatest band ever, Bob Wills and His Texas Playboys. Nashville later picked up the use of the steel guitar in the early days of the late 1940s and early 1950s "Honky Tonk" country & western music with a number of fine steel guitarists backing names like :Hank Williams, Lefty Frizell and Webb Pierce. The term steel guitar is often mistakenly used to describe any metal body resophonic guitar.
Steel guitar can describe:
Technique.
Steel guitar refers to a method of playing on a guitar held horizontally, with the treble strings uppermost and the bass strings towards the player, and using a type of slide called a steel above the fingerboard rather than fretting the strings with the fingers. This may be done with any guitar, but is most common on instruments designed and produced for this style of play.
The technique was invented and popularized in Hawaii. Thus, the "lap steel guitar" is sometimes known as the Hawaiian guitar, particularly in documents from the early 20th century, and today any steel guitar is frequently called a Hawaiian steel guitar. However, in Hawaiian music, "Hawaiian guitar" means "slack string guitar", played in the conventional or Spanish position.
Bottleneck guitar may have actually developed from Steel guitar technique. It is similar, with the exception that the guitar is held in the conventional position, and using a different form of slide to accommodate this playing position.
Instruments.
A steel guitar is one designed to be played in steel-guitar fashion.
Historically, these have been of many types, but two dominate:
Lap steel guitar.
The lap steel typically has 6 strings and is tuned to either standard guitar tuning, or an open chord. It differs from a conventional or Spanish guitar in having a higher action and often a neck that is square in cross section. The frets, unused in steel style playing, may be replaced by markers.
There are three main types:
Early lap steel guitars were Spanish guitars modified by raising both the bridge and head nut. The string height at the head nut was raised to about half an inch by using a "head nut converter" or "converter nut". This type of guitar is claimed to have been invented in about 1889 by Joseph Kekuku in Hawaii.
Some lap slide guitars, particularly those of Weissenborn and their imitators, have two 6-string necks, but electric and resonator lap steel guitars are normally single neck instruments.
Square-necked resonator guitars are always played in lap steel fashion, and so are specialized lap steel guitars. Round-necked varieties can be played in lap steel fashion, with some restrictions on the available tunings, but can also be played in Spanish position.
The Rickenbacker "frying pan", an electric lap steel guitar produced from 1931 to 1939, was the first commercially successful solid body electric guitar.
Console steel guitar.
The console steel guitar (also known as a "table steel guitar") is an electric instrument, intermediate between the lap steel from which it developed and the pedal steel which in turn developed from the console steel. There are no pedals, so the player has only as many tunings available as there are necks.
The development of the lap steel guitar into the console steel guitar saw the introduction of amplification as standard, multiple necks, and additional strings on each neck, first to seven, and eight strings per neck is now common. One, two, three and four neck instruments are not uncommon. The two neck, eight string per neck configuration is particularly favored in Hawaiian music.
The distinction between console steel guitar and lap steel guitar is fuzzy at best, and some makers and authorities do not use the term "console steel guitar" at all, but refer to any steel guitar without pedals as a "lap steel guitar" even if playing it in lap steel position would be quite impossible.
Pedal steel guitar.
The pedal steel guitar is an electric instrument normally with 10 to 14 strings per neck, and sometimes two or even three necks, each in a different tuning. Up to ten pedals (not counting the volume pedal) and up to eight knee-levers are used to alter the tunings of different strings, which gives the instrument its distinctive voice, most often heard in country music.
The extra strings and use of pedals gives even a single neck pedal steel guitar far more versatility than any table steel guitar, but at the same time makes playing far more complex.
Steels.
The type of slide called a steel which gives the technique its name was probably originally made of steel, or the name may come from the legend that the first steel was a railroad track.
Many materials are used, but nickel-plated brass is popular for the highest-quality slides, which are shaped to fit the hand and as a result have a cross-section not unlike a railroad track. Another traditional and popular variety is a cylindrical shaped steel bar that needs to be balanced between the thumb and the middle finger with the forefinger providing for varying degrees of pressure on the string. The cylindrical bar is used most often on the pedal steel guitar.
Some cautions on terminology.
The term "steel guitar" should not be confused with "steel-strung guitar", which is a standard acoustic guitar that has steel rather than the nylon, catgut or brass/nickel strings used for classical guitar, and is built with extra bracing, a stronger neck, and higher-geared machine heads to compensate for the much higher tension of steel strings. The "steel guitar" takes its name from the type of slide used, not from the material of the strings.
Also, the term "steel guitar" does not describe what the guitar itself is made out of. Acoustic steel guitars used in Hawaiian music are made completely out of wood (with the exception of the tuning hardware), and some resonator guitars are made out of steel or brass but aren't "steel guitars" due to the manner in which they are played.
The term "Hawaiian guitar" is often used for various types of steel guitar, but in Hawaiian music "Hawaiian guitar" means slack-key guitar, a way of tuning a steel stringed acoustic guitar which is then played in the conventional position.
See also "slide (guitar)".

</doc>
<doc id="27629" url="http://en.wikipedia.org/wiki?curid=27629" title="Senegal River">
Senegal River

Senegal River at Dagana, Senegal
Average monthly flow (m3/s) at the Dagana hydrometric station over the period 1903-1974
The Senegal River (French: "Fleuve Sénégal") is a 1790 km long river in West Africa that forms the border between Senegal and Mauritania.
Geography.
The Senegal's headwaters are the Semefé (Bakoye) and Bafing rivers which both originate in Guinea; they form a small part of the Guinean-Malian border before coming together at Bafoulabé in Mali. From there, the Senegal river flows west and then north through Talari Gorges near Galougo and over the Gouina Falls, then flows more gently past Kayes, where it receives the Kolimbiné. After flowing together with the Karakoro, it prolongs the former's course along the Mali-Mauritanian border for some tens of kilometers till Bakel where it flows together with the Falémé River, which also has its source in Guinea, subsequently runs along a small part of the Guinea-Mali frontier to then trace most of the Senegal-Mali border up to Bakel. The Senegal further flows through semi-arid land in the north of Senegal, forming the border with Mauritania and into the Atlantic. In Kaedi it accepts the Gorgol from Mauritania. Flowing through Bogué it reaches Richard Toll where it is joined by the Ferlo coming from inland Senegal's Lac de Guiers. It passes through Rosso and, approaching its mouth, around the Senegalese island on which the city of Saint-Louis is located, to then turn south. It is separated from the Atlantic Ocean by a thin strip of sand called the Langue de Barbarie before it pours into the ocean itself.
The river has two large dams along its course, the multi-purpose Manantali Dam in Mali and the Maka-Diama Dam downstream on the Mauritania-Senegal border, near the outlet to the sea, preventing access of salt water upstream. In between Manantali and Maka-Diama is the Félou Hydroelectric Plant which was originally completed in 1927 and uses a weir. The power station was replaced in 2014. In 2013, construction of the Gouina Hydroelectric Plant upstream of Felou at Gouina Falls began.
The Senegal River has a drainage basin of 270,000 km2, a mean flow of 680 m3/s and an annual discharge of 21.5 km3. Important tributaries are the Falémé River, Karakoro River, and the Gorgol River.
Downstream of Kaédi the river divides into two branches. The left branch called the Doué runs parallel to the main river to the north. After 200 km the two branches rejoin a few kilometres downstream of Pondor. The long strip of land between the two branches is called the Île á Morfil.
In 1972 Mali, Mauritania and Senegal founded the Organisation pour la mise en valeur du fleuve Sénégal (OMVS) to manage the river basin. Guinea joined in 2005.
At the present time, only very limited use is made of the river for the transport of goods and passengers. The OMVS have looked at the feasibility of creating a navigable channel 55 m in width between the small town of Ambidédi in Mali and Saint-Louis, a distance of 905 km. It would give landlocked Mali a direct route to the Atlantic Ocean.
The aquatic fauna in the Senegal River basin is closely associated with that of the Gambia River basin, and the two are usually combined under a single ecoregion known as the Senegal-Gambia Catchments. Although the species richness is moderately high, only three species of frogs and one fish are endemic to this ecoregion.
History.
The existence of the Senegal River was known to the early Mediterranean civilizations. It was called "Bambotus" by Pliny the Elder (from Phoenician "behemoth" for hippopotamus) and "Nias" by Claudius Ptolemy. It was visited by Hanno the Carthaginian around 450 BCE at his navigation from Carthage through the pillars of Herakles to Theon Ochema (Mount Cameroon) in the Gulf of Guinea. There was trade from here to the Mediterranean World, until the destruction of Carthage and its west African trade net in 146 BCE.
Arab sources.
In the Early Middle Ages (c. 800 CE), the Senegal River restored contact with the Mediterranean world with the establishment of the Trans-Saharan trade route between Morocco and the Ghana Empire. Arab geographers, like al-Masudi of Baghdad (957), al-Bakri of Spain (1068) and al-Idrisi of Sicily (1154), provided some of the earliest descriptions of the Senegal River. Early Arab geographers believed the upper Senegal River and the upper Niger River were connected to each other, and formed a single river flowing from east to west, which they called the "Western Nile" or the "Nile of the Blacks". It was believed to be either a western branch of the Egyptian Nile River or drawn from the same source (variously conjectured to some great internal lakes of the Mountains of the Moon, or Ptolemy's Ghir or the Biblical Gihon stream).
Arab geographers Abd al-Hassan Ali ibn Omar (1230), Ibn Said al-Maghribi (1274) and Abulfeda (1331), label the Senegal as the "Nile of Ghana" (Nil Gana or Nili Ganah).
As the Senegal River reached into the heart of the gold-producing Ghana Empire and later the Mali Empire, Trans-Saharan traders gave the Senegal its famous nickname as the "River of Gold". The Trans-Saharan stories about the "River of Gold" reached the ears of Mediterranean merchants that frequented the ports of Morocco and the lure proved irresistible. Arab historians report at least three separate Arab maritime expeditions - the last one organized by a group of eight "mughrarin" ("wanderers") of Lisbon (before 1147) - that tried to sail down the Atlantic coast, possibly in an effort find the mouth of the Senegal.
Cartographic representation.
Drawing from Classical legend and Arab sources, the "River of Gold" found its way into European maps in the 14th century. In the Hereford Mappa Mundi (c. 1300), there is a river labelled "Nilus Fluvius" drawn "parallel" to the coast of Africa, albeit without communication with Atlantic (it ends in a lake). It depicts some giant ants digging up gold dust from its sands, with the note ""Hic grandes formice auream serican arenas". In the mappa mundi made by Pietro Vesconte for the c. 1320 atlas of Marino Sanuto, there is an unnamed river stemming from the African interior and opening in the Atlantic ocean. The 1351 Medici-Laurentian Atlas shows both the Egyptian Nile and the western Nile stemming from the same internal mountain range, with the note that "Ilic coligitur aureaum"". The portolan chart of Giovanni da Carignano (1310s-20s) has the river with the label, "iste fluuis exit de nilo ubi multum aurum repperitur".
In the more accurately-drawn portolan charts, starting with the 1367 chart of Domenico and Francesco Pizzigano and carried on in the 1375 Catalan Atlas, the 1413 chart of Mecia de Viladestes, etc. the "River of Gold" is depicted (if only speculatively), draining into the Atlantic Ocean somewhere just south of Cape Bojador. The legend of Cape Bojador as a terrifying obstacle, the 'cape of no return' to European sailors, emerged around the same time (possibly encouraged by Trans-Saharan traders who did not want to see their land route sidestepped by sea).
The river is frequently depicted with a great river island midway, the "Island of Gold", first mentioned by al-Masudi, and famously called ""Wangara" by al-Idrisi and "Palolus"" in the 1367 Pizzigani brothers chart. It is conjectured that this riverine "island" is in fact just the Bambuk-Buré goldfield district, which is practically surrounded on all sides by rivers - the Senegal river to the north, the Falémé River to the west, the Bakhoy to the east and the Niger and Tinkisso to the south.
The 1413 portolan chart of Mecia de Viladestes gives perhaps the most detailed depiction of the early state of European knowledge about the Senegal River prior to the 1440s. Viladestes labels it "River of Gold" (""riu del or") and locates it a considerable distance south of Cape Bojador ("buyeter") - indeed, south of a mysterious "cap de abach"" (possibly Cape Timris). There are extensive notes about the plentifulness of ivory and gold in the area, including a note that reads
"This river is called Wad al-Nil and also is called the River of Gold, for one can here obtain the gold of Palolus. And know that the greater part of those that live here occupy themselves collecting gold on the shores of the river which, at its mouth, is a league wide, and deep enough for the largest ship of the world."
The galley of Jaume Ferrer is depicted off the coast on the left, with a quick note about his 1346 voyage. The golden round island at the mouth of the Senegal River is the indication (customary on portolan charts) of river mouth bars or islands - in this case, probably a reference to the Langue de Barbarie or the island of Saint-Louis). The first town, by the mouth of the Senegal, is called "isingan" (arguably the etymological source of the term "Senegal"). East of that, the Senegal forms a riverine island called ""insula de bronch" (Île à Morfil). By its shores lies the city of "tocoror" (Takrur). Above it is a depiction of the Almoravid general Abu Bakr ibn Umar ("Rex Bubecar") on a camel. Further east, along the river, is the seated emperor (mansa) of Mali ("Rex Musa Meli", prob. Mansa Musa), holding a gold nugget. His capital, "civitat musa meli"" is shown on the shores of the river, and the range of Emperor of Mali's sway is suggested by all the black banners (an inscription notes "This lord of the blacks is called Musa Melli, Lord of Guinea, the greatest noble lord of these parts for the abundance of the gold which is collected in his lands". Curiously, there is a defiant gold-bannered town south of the river, labelled "tegezeut" (probably the Ta'adjast of al-Idrisi), and might be an ichoate reference to Djenné.
East of Mali, the river forms a lake or "Island of Gold" shown here studded with river-washed gold nuggets (this is what the Pizzigani brothers called the island of "Palolus", and most commentators take to indicate the Bambuk-Buré goldfields). It is connected by many streams to the southerly "mountains of gold" (labelled ""montanies del lor", the Futa Djallon/Bambouk Mountains and Loma Mountains of Sierra Leone). It is evident the Senegal river morphs east, unbroken, into the Niger River - the cities of "tenbuch" (Timbuktu), "geugeu" (Gao) and "mayna"" (Niamey? or a misplaced Niani?) are denoted along the same single river. South of them (barely visible) are what seem like the towns of Kukiya (on the eastern shore of the Island of Gold), and east of that, probably Sokoto (called "Zogde" in the Catalan Atlas) and much further southeast, probably Kano. 
North of the Senegal-Niger are the various oases and stations of the trans-Saharan route (""Tutega" = Tijigja, "Anzica" = In-Zize, "Tegaza" = Taghaza, etc.) towards the Mediterranean coast. There is an unlabeled depiction of a black African man on a camel traveling from "Uuegar"" (prob. Hoggar) to the town of "Organa" (""ciutat organa", variously identified as Kanem or Ouargla or possibly even a misplaced depiction of Ghana - long defunct, but, on the other hand, contemporaneous with the depicted Abu Bakr). Nearby sits its Arab-looking king ("Rex Organa"") holding a scimitar. The River of Gold is sourced at a circular island, what seem like the Mountains of the Moon (albeit unlabeled here). From this same source also flows north the White Nile towards Egypt, which forms the frontier between the Muslim "king of Nubia" (""Rex Onubia", his range depicted by crescent-on-gold banners) and the Christian Prester John ("Preste Joha"), i.e. the emperor of Ethiopia in the garb of a Christian bishop (coincidentally, this is the first visual depiction of Prester John on a portolan chart).
Uniquely, the Viladestes map shows another river, south of the Senegal, which it labels the "flumen gelica" (poss. "angelica"), which some have taken to depict the Gambia River. In the 1459 mappa mundi of Fra Mauro, drawn a half-century later, after the Portuguese had already visited the Senegal (albeit still trying to respect Classical sources), shows "two" parallel rivers running east to west, both of them sourced from the same great internal lake (which, Fra Mauro asserts, is also the same source as the Egyptian Nile). Mauro names the two parallel rivers differently,calling one "flumen Mas" ("Mas River"), the other the "canal dal oro" ("Channel of Gold"), and makes the note that "Inne larena de questi do fiume se trova oro de paiola" ("In the sands of both these rivers gold of 'palola' may be found"), and nearer to the sea, "Qui se racoce oro" ("Here gold is collected"), and finally, on the coast, "Terra de Palmear" ("Land of Palms"). It is notable that Fra Mauro knew of the error of Henry the Navigator's captains about the Daklha inlet, which Mauro carefully labels ""Reodor"" ("Rio do Ouro", Western Sahara), distinctly from the "Canal del Oro" (Senegal River).
European contact.
Christian Europeans soon began attempting to find the sea route to the mouth of the Senegal. The first known effort may have been by the Genoese brothers Vandino and Ugolino Vivaldi, who set out down the coast in 1291 in a pair of ships (nothing more is heard of them). In 1346, the Majorcan sailor, Jaume Ferrer set out on a galley with the explicit objective of finding the "River of Gold" ("Riu de l'Or"), where he heard that most people along its shores were engaged in the collection of gold and that the river was wide and deep enough for the largest ships. Nothing more is heard of him either. In 1402, after establishing the first European colony on the Canary Islands, the French Norman adventurers Jean de Béthencourt and Gadifer de la Salle set about immediately probing the African coast, looking for directions to the mouth of Senegal.
The project of finding the Senegal was taken up in the 1420s by the Portuguese Prince Henry the Navigator, who invested heavily to reach it. In 1434, one of Henry's captains, Gil Eanes, finally surpassed Cape Bojador and returned to tell about it. Henry immediately dispatched a follow up mission in 1435, under Gil Eanes and Afonso Gonçalves Baldaia. Going down the coast, they turned around the al-Dakhla peninsula in the Western Sahara and emerged into an inlet, which they excitedly believed to be the mouth of the Senegal River. The name they mistakenly bestowed upon the inlet - "Rio do Ouro" - is a name it would remain stuck with down to the 20th century.
Realizing the mistake, Henry kept pressing his captains further down the coast, and in 1445, the Portuguese captain Nuno Tristão finally reached the Langue de Barbarie, where he noticed the desert end and the treeline begin, and the population change from 'tawny' Sanhaja Berbers to 'black' Wolof people. Bad weather or lack of supplies prevented Tristão from actually reaching the mouth of the Senegal River, but he rushed back to Portugal to report he had finally found the "Land of the Blacks" ("Terra dos Negros"), and that the "Nile" was surely nearby. Shortly after (possibly still within that same year) another captain, Dinis Dias (sometimes given as Dinis Fernandes) was the first known European since antiquity to finally reach the mouth of the Senegal River. However, Dias did not sail upriver, but instead kept sailing down the Grande Côte to the bay of Dakar.
The very next year, in 1446, the Portuguese slave-raiding fleet of Lançarote de Freitas arrived at the mouth of the Senegal. One of its captains, Estêvão Afonso, volunteered to take a launch to explore upriver for settlements, thus becoming the first European to actually enter the Senegal river. He didn't get very far. Venturing ashore at one point along the river bank, Afonso tried to kidnap two Wolof children from a woodsman's hut. But he ran into their father, who proceeded to chase the Portuguese back to their launch and gave them such a beating that the explorers gave up on going any further, and turned back to the waiting caravels.
Sometime between 1448 and 1455, the Portuguese captain Lourenço Dias opened regular trade contact on the Senegal River, with the Wolof statelets of Waalo (near the mouth of the Senegal River) and Cayor (a little below that), drumming up a profitable business exchanging Mediterranean goods (notably, horses) for gold and slaves. Chronicler Gomes Eanes de Zurara, writing in 1453, still called it the "Nile River", but Alvise Cadamosto, writing in the 1460s, was already calling it the "Senega" ["sic"], and it is denoted as "Rio do Çanagà" on most subsequent Portuguese maps of the age. Cadamosto relates the legend that both the Senegal and the Egyptian Nile were branches of the Biblical Gihon River that stems from the Garden of Eden and flows through Ethiopia. He also notes that the Senegal was called "the Niger" by the ancients - probably a reference to Ptolemy's legendary 'Nigir' (below the Gir), which would be later identified by Leo Africanus with the modern Niger River. Much the same story is repeated by Marmol in 1573, with the additional note that both the Senegal River and Gambia River were tributaries of the Niger River. However, the contemporary African atlas of Venetian cartographer Livio Sanuto, published in 1588, sketches the Senegal, the Niger and the Gambia as three separate, parallel rivers.
Portuguese chronicler João de Barros (writing in 1552) says the river's original local Wolof name was "Ovedech" (which according to one source, comes from "vi-dekh", Wolof for "this river"). His contemporary, Damião de Góis (1567) records it as "Sonedech" (from "sunu dekh", Wolof for "our river"). Writing in 1573, the Spanish geographer Luis del Marmol Carvajal asserts that the Portuguese called it "Zenega", the 'Zeneges' (Berber Zenaga) called it the "Zenedec", the 'Gelofes' (Wolofs) call it "Dengueh", the 'Tucorones' (Fula Toucouleur) called it "Mayo", the 'Çaragoles' (Soninke Sarakole of Ngalam) called it "Colle" and further along (again, Marmol assuming Senegal was connected to the Niger), the people of Bagamo' (Bambara of Bamako?) called it "Zimbala" (Jimbala?) and the people of Timbuktu called it the "Yça".
Etymology.
The 16th-century chronicler Joao de Barros asserts the Portuguese renamed it "Senegal" because that was the personal name of a local Wolof chieftain who frequently conducted business with the Portuguese traders. But this etymology is doubtful (e.g. the ruler of Senegalese river state of Waalo bears the title 'Brak', and Cadamosto gives the personal name of the Senegal river chieftain as "Zucholin"). The confusion may have arisen because Cadamosto says the Portuguese interacted frequently with a certain Wolof chieftain south of the river, somewhere on the Grande Côte, which he refers to as "Budomel". "Budomel" is almost certainly a reference to the ruler of Cayor, a combination of his formal title ("Damel"), prefixed by the generic Wolof term "bor" ("lord"). Curiously, Budomel is reminiscent of "Vedamel" already used by the Genoese back in the 14th century as an alternative name of the Senegal River. It is almost certain that the Genoese "Vedamel" are corruptions from the Arabic, either "Wad al-mal" ("River of Treasure", i.e. Gold) or, alternatively, "Wad al-Melli" ("River of Mali") or even, by transcription error, "Wad al-Nill" ("River of Nile").
Other etymological theories for "Senegal" abound. A popular one, first proposed by Fr. David Boilat (1853), was that "Senegal" comes from the Wolof phrase "sunu gaal", meaning "our canoe" (more precisely, "our pirogue"). Bailot speculates the name probably arose as a misunderstanding, that when a Portuguese captain came across some Wolof fishermen and asked them what the name of the river was, they believed he was asking who their fishing boat belonged to, and replied simply "it is our canoe" ("sunu gaal"). The "our canoe" theory has been popularly embraced in modern Senegal for its charm and appeal to national solidarity ("we're all in one canoe", etc.).
More recent historians suggest the name "Senegal" is probably a derivation of "Azenegue", the Portuguese term for the Saharan Berber Zenaga people that lived north of it.
A strong challenge to this theory is that "Senegal" is much older, and might derive from "Sanghana" (also given as Isenghan, Asengan, Singhanah), a city described by the Arab historian al-Bakri in 1068 as located by the mouth of the Senegal River (straddling both banks) and the capital of a local kingdom. The location "Senegany" is depicted in 1351 Genoese map known as the Medici Atlas (Laurentian Gaddiano portolan). This town ("Isingan") is fantastically depicted in the 1413 portolan map of Majorcan cartographer Mecia de Viladestes. The name itself might be of Berber Zenaga origin, speculatively related to 'Ismegh' ('black slave', analogous to the Arabic 'abd) or 'sagui nughal' ('border'). Some sources claim 'Isinghan' remained the usual Berber term to refer to the Wolof kingdom of Cayor.
Some Serer people from the south have advanced the claim that the river's name is originally derived from the compound of the Serer term "Sene" (from Rog Sene, Supreme Deity in Serer religion) and "O Gal" (meaning "body of water").

</doc>
<doc id="27686" url="http://en.wikipedia.org/wiki?curid=27686" title="Spreadsheet">
Spreadsheet

A spreadsheet is an interactive computer application program for organization, analysis and storage of data in tabular form. Spreadsheets developed as computerized simulations of paper accounting worksheets. The program operates on data represented as cells of an array, organized in rows and columns. Each cell of the array is a model–view–controller element that may contain either numeric or text data, or the results of formulas that automatically calculate and display a value based on the contents of other cells.
Spreadsheet users may adjust any stored value and observe the effects on calculated values. This makes the spreadsheet useful for "what-if" analysis since many cases can be rapidly investigated without manual recalculation. Modern spreadsheet software can have multiple interacting sheets, and can display data either as text and numerals, or in graphical form.
Besides performing basic arithmetic and mathematical functions, modern spreadsheets provide built-in functions for common financial and statistical operations. Such calculations as net present value or standard deviation can be applied to tabular data with a pre-programmed function in a formula. Spreadsheet programs also provide conditional expressions, functions to convert between text and numbers, and functions that operate on strings of text.
Spreadsheets have replaced paper-based systems throughout the business world. Although they were first developed for accounting or bookkeeping tasks, they now are used extensively in any context where tabular lists are built, sorted, and shared.
LANPAR was the first electronic spreadsheet on mainframe and time sharing computers. VisiCalc was the first electronic spreadsheet on a microcomputer, and it helped turn the Apple II computer into a popular and widely used system. Lotus 1-2-3 was the leading spreadsheet when DOS was the dominant operating system. Excel now has the largest market share on the Windows and Macintosh platforms. A spreadsheet program is a standard feature of an office productivity suite; since the advent of web apps, office suites now also exist in web app form.
Usage.
A spreadsheet consists of a table of "cells" arranged into rows and columns and referred to by the X and Y locations. X locations, the columns, are normally represented by letters, "A", "B", "C", etc., while rows are normally represented by numbers, 1, 2, 3, etc. A single cell can be referred to by addressing its row and column, "C10" for instance. Additionally, spreadsheets have the concept of a "range", a group of cells, normally contiguous. For instance, one can refer to the first ten cells in the first column with the range "A1:A10". This system of cell references was introduced in VisiCalc, and known as "A1 notation".
In modern spreadsheet applications, several spreadsheets, often known as "worksheets" or simply "sheets", are gathered together to form a "workbook". A workbook is physically represented by a file, containing all the data for the book, the sheets and the cells with the sheets. Worksheets are normally represented by tabs that flip between pages, each one containing one of the sheets, although Numbers changes this model significantly. Cells in a multi-sheet book add the sheet name to their reference, for instance, "Sheet 1!C10". Some systems extend this syntax to allow cell references to different workbooks.
Users interact with sheets primarily through the cells. A given cell can hold data by simply entering it in, or a formula, which is normally created by preceding the text with an equals sign. Data might include the string of text codice_1, the number codice_2 or the date codice_3. A formula would begin with the equals sign, codice_4, but this would normally be invisible because the display shows the "result" of the calculation, codice_5 in this case, not the formula itself. This may lead to confusion in some cases.
The key feature of spreadsheets is the ability for a formula to refer to the contents of other cells, which may in turn be the result of a formula. To make such a formula, one simply replaces a number with a cell reference. For instance, the formula codice_6 would produce the result of multiplying the value in cell C10 by the number 5. If C10 holds the value codice_7 the result will be codice_5. But C10 might also hold its own formula referring to other cells, and so on.
The ability to chain formulas together is what gives a spreadsheet its power. Many problems can be broken down into a series of individual mathematical step, and these can be assigned to individual formulas in cells. Some of these formulas can apply to ranges as well, like the codice_9 function that adds up all the numbers within a range.
Spreadsheets share many principles and traits of databases, but spreadsheets and databases are not the same thing. A spreadsheet is essentially just one table, whereas a database is a collection of many tables with machine-readable semantic relationships between them. While it is true that a workbook that contains three sheets is indeed a file containing multiple tables that can interact with each other, it lacks the relational structure of a database. Spreadsheets and databases are interoperable—sheets can be imported into databases to become tables within them, and database queries can be exported into spreadsheets for further analysis.
A spreadsheet program is one of the main components of an office productivity suite, which usually also contains a word processor, a presentation program, and a database management system. Programs within a suite use similar commands for similar functions. Usually sharing data between the components is easier than with a non-integrated collection of functionally equivalent programs. This was particularly an advantage at a time when many personal computer systems used text-mode displays and commands, instead of a graphical user interface.
History.
Paper spreadsheets.
The word "spreadsheet" came from "spread" in its sense of a newspaper or magazine item (text or graphics) that covers two facing pages, extending across the center fold and treating the two pages as one large one. The compound word "spread-sheet" came to mean the format used to present book-keeping ledgers—with columns for categories of expenditures across the top, invoices listed down the left margin, and the amount of each payment in the cell where its row and column intersect—which were, traditionally, a "spread" across facing pages of a bound ledger (book for keeping accounting records) or on oversized sheets of paper (termed "analysis paper") ruled into rows and columns in that format and approximately twice as wide as ordinary paper.
Early implementations.
Batch spreadsheet report generator.
A batch "spreadsheet" is indistinguishable from a batch compiler with added input data, producing an output report, "i.e.", a 4GL or conventional, non-interactive, batch computer program. However, this concept of an electronic spreadsheet was outlined in the 1961 paper "Budgeting Models and System Simulation" by Richard Mattessich. The subsequent work by Mattessich (1964a, Chpt. 9, "Accounting and Analytical Methods") and its companion volume, Mattessich (1964b, "Simulation of the Firm through a Budget Computer Program") applied computerized spreadsheets to accounting and budgeting systems (on mainframe computers programmed in FORTRAN IV). These batch Spreadsheets dealt primarily with the addition or subtraction of entire columns or rows (of input variables), rather than individual "cells".
In 1962 this concept of the spreadsheet, called BCL for Business Computer Language, was implemented on an IBM 1130 and in 1963 was ported to an IBM 7040 by R. Brian Walsh at Marquette University, Wisconsin. This program was written in Fortran. Primitive timesharing was available on those machines. In 1968 BCL was ported by Walsh to the IBM 360/67 timesharing machine at Washington State University. It was used to assist in the teaching of finance to business students. Students were able to take information prepared by the professor and manipulate it to represent it and show ratios etc. In 1964, a book entitled "Business Computer Language" was written by Kimball, Stoffells and Walsh and both the book and program were copyrighted in 1966 and years later that copyright was renewed
Applied Data Resources had a FORTRAN preprocessor called Empires.
In the late 1960s Xerox used BCL to develop a more sophisticated version for their timesharing system.
LANPAR spreadsheet compiler.
A key invention in the development of electronic spreadsheets was made by Rene K. Pardo and Remy Landau, who filed in 1970 U.S. Patent on spreadsheet automatic natural order recalculation algorithm. While the patent was initially rejected by the patent office as being a purely mathematical invention, following 12 years of appeals, Pardo and Landau won a landmark court case at the CCPA (Predecessor Court of the Federal Circuit) overturning the Patent Office in 1983—establishing that "something does not cease to become patentable merely because the point of novelty is in an algorithm." However, in 1995 the United States Court of Appeals for the Federal Circuit ruled the patent unenforceable.
The actual software was called LANPAR—LANguage for Programming Arrays at Random. This was conceived and entirely developed in the summer of 1969 following Pardo and Landau's recent graduation from Harvard University. Co-inventor Rene Pardo recalls that he felt that one manager at Bell Canada should not have to depend on programmers to program and modify budgeting forms, and he thought of letting users type out forms in any order and having computer calculating results in the right order ("Forward Referencing/Natural Order Calculation"). Pardo and Landau developed and implemented the software in 1969.
LANPAR was used by Bell Canada, AT&T and the 18 operating telcos nationwide for their local and national budgeting operations. LANPAR was also used by General Motors. Its uniqueness was Pardo's co-invention incorporating forward referencing/natural order calculation (one of the first "non-procedural" computer languages) as opposed to left-to-right, top to bottom sequence for calculating the results in each cell that was used by VisiCalc, Supercalc, and the first version of Multiplan. Without forward referencing/natural order calculation, the user had to manually recalculate the spreadsheet as many times as necessary until the values in all the cells had stopped changing. Forward Referencing/Natural Order Calculation by a compiler was the cornerstone functionality required for any spreadsheet to be practical and successful.
The LANPAR system was implemented on GE400 and Honeywell 6000 online timesharing systems enabling users to program remotely via computer terminals and modems. Data could be entered dynamically either by paper tape, specific file access, on line, or even external data bases. Sophisticated mathematical expressions including logical comparisons and "if/then" statements could be used in any cell, and cells could be presented in any order.
Autoplan/Autotab spreadsheet programming language.
In 1968, three former employees from the General Electric computer company headquartered in Phoenix, Arizona set out to start their own software development house. A. Leroy Ellison, Harry N. Cantrell, and Russell E. Edwards found themselves doing a large number of calculations when making tables for the business plans that they were presenting to venture capitalists. They decided to save themselves a lot of effort and wrote a computer program that produced their tables for them. This program, originally conceived as a simple utility for their personal use, would turn out to be the first software product offered by the company that would become known as Capex Corporation. "AutoPlan" ran on GE’s Time-sharing service; afterward, a version that ran on IBM mainframes was introduced under the name "AutoTab". (National CSS offered a similar product, CSSTAB, which had a moderate timesharing user base by the early 1970s. A major application was opinion research tabulation.) AutoPlan/AutoTab was not a WYSIWYG interactive spreadsheet program, it was a simple scripting language for spreadsheets. The user defined the names and labels for the rows and columns, then the formulas that defined each row or column.
Works Records System.
The Works Records System was a spreadsheet system designed in 1974 at ICI in the UK. It was a company-internal system that ran on IBM mainframes, and was in use essentially unchanged for 27 years. It was intended for use by non-programmers and had a WYSIWIG interface.
IBM Financial Planning and Control System.
The IBM Financial Planning and Control System was developed in 1976, by Brian Ingham at IBM Canada. It was implemented by IBM in at least 30 countries. It ran on an IBM mainframe and was among the first applications for financial planning developed with APL that completely hid the programming language from the end-user. Through IBM's VM operating system, it was among the first programs to auto-update each copy of the application as new versions were released. Users could specify simple mathematical relationships between rows and between columns. Compared to any contemporary alternatives, it could support very large spreadsheets. It loaded actual financial data drawn from the legacy batch system into each user's spreadsheet on a monthly basis. It was designed to optimize the power of APL through object kernels, increasing program efficiency by as much as 50 fold over traditional programming approaches.
APLDOT modeling language.
An example of an early "industrial weight" spreadsheet was APLDOT, developed in 1976 at the United States Railway Association on an IBM 360/91, running at The Johns Hopkins University Applied Physics Laboratory in Laurel, MD. The application was used successfully for many years in developing such applications as financial and costing models for the US Congress and for Conrail. APLDOT was dubbed a "spreadsheet" because financial analysts and strategic planners used it to solve the same problems they addressed with paper spreadsheet pads.
VisiCalc.
Because of Dan Bricklin and Bob Frankston's implementation of VisiCalc on the Apple II in 1979 and the IBM PC in 1981, the spreadsheet concept became widely known in the late 1970s and early 1980s. VisiCalc was the first spreadsheet that combined all essential features of modern spreadsheet applications (except for forward referencing/natural order recalculation), such as WYSIWYG interactive user interface, automatic recalculation, status and formula lines, range copying with relative and absolute references, formula building by selecting referenced cells. Unaware of LANPAR at the time "PC World" magazine called VisiCalc the first electronic spreadsheet.
Bricklin has spoken of watching his university professor create a table of calculation results on a blackboard. When the professor found an error, he had to tediously erase and rewrite a number of sequential entries in the table, triggering Bricklin to think that he could replicate the process on a computer, using the blackboard as the model to view results of underlying formulas. His idea became VisiCalc, the first application that turned the personal computer from a hobby for computer enthusiasts into a business tool.
VisiCalc went on to become the first killer app, an application that was so compelling, people would buy a particular computer just to use it. VisiCalc was in no small part responsible for the Apple II's success. The program was later ported to a number of other early computers, notably CP/M machines, the Atari 8-bit family and various Commodore platforms. Nevertheless, VisiCalc remains best known as an Apple II program.
Lotus 1-2-3 and other MS-DOS spreadsheets.
The acceptance of the IBM PC following its introduction in August, 1981, began slowly, because most of the programs available for it were translations from other computer models. Things changed dramatically with the introduction of Lotus 1-2-3 in November, 1982, and release for sale in January, 1983. Since it was written especially for the IBM PC, it had good performance and became the killer app for this PC. Lotus 1-2-3 drove sales of the PC due to the improvements in speed and graphics compared to VisiCalc on the Apple II.
Lotus 1-2-3, along with its competitor Borland Quattro, soon displaced VisiCalc. Lotus 1-2-3 was released on January 26, 1983, started outselling then-most-popular VisiCalc the very same year, and for a number of years was the leading spreadsheet for DOS.
Microsoft Excel.
Microsoft released the first version of Excel for the Macintosh on September 30, 1985, and then ported it to Windows, with the first version being numbered 2.05 (to synchronize with the Macintosh version 2.2) and released in November 1987. The Windows 3.x platforms of the early 1990s made it possible for Excel to take market share from Lotus. By the time Lotus responded with usable Windows products, Microsoft had begun to assemble their Office suite. By 1995, Excel was the market leader, edging out Lotus 1-2-3, and in 2013, IBM discontinued Lotus-1-2-3 altogether.
Open source software.
Gnumeric is a free, cross-platform spreadsheet program that is part of the GNOME Free Software Desktop Project. OpenOffice.org Calc and the very closely related LibreOffice Calc (using the LGPL license) are free and open-source spreadsheets.
Web based spreadsheets.
With the advent of advanced web technologies such as Ajax circa 2005, a new generation of online spreadsheets has emerged. Equipped with a rich Internet application user experience, the best web based online spreadsheets have many of the features seen in desktop spreadsheet applications. Some of them such as Office Online, ZOHO, Google Spreadsheets, EditGrid or ZK Spreadsheet also have strong multi-user collaboration features and / or offer real time updates from remote sources such as stock prices and currency exchange rates.
Other products.
A number of companies have attempted to break into the spreadsheet market with programs based on very different paradigms. Lotus introduced what is likely the most successful example, Lotus Improv, which saw some commercial success, notably in the financial world where its powerful data mining capabilities remain well respected to this day. 
Quantrix is built on the same paradigm as the discontinued Lotus Improv, except has many powerful new features, making it the application of choice for many financial professionals worldwide. 
Spreadsheet 2000 attempted to dramatically simplify formula construction, but was generally not successful.
Concepts.
The main concepts are those of a grid of cells, called a sheet, with either raw data, called values, or formulas in the cells. Formulas say how to mechanically compute new values from existing values. Values are generally numbers, but can also be pure text, dates, months, etc. Extensions of these concepts include logical spreadsheets. Various tools for programming sheets, visualizing data, remotely connecting sheets, displaying cells' dependencies, etc. are commonly provided.
Cells.
A "cell" can be thought of as a box for holding data. A single cell is usually referenced by its column and row (A2 would represent the cell containing the value 10 in the example table below). Usually rows, representing the dependent variables, are referenced in decimal notation starting from 1, while columns representing the independent variables use 26-adic bijective numeration using the letters A-Z as numerals. Its physical size can usually be tailored to its content by dragging its height or width at box intersections (or for entire columns or rows by dragging the column- or row-headers).
An array of cells is called a "sheet" or "worksheet". It is analogous to an array of variables in a conventional computer program (although certain unchanging values, once entered, could be considered, by the same analogy, constants). In most implementations, many worksheets may be located within a single spreadsheet. A worksheet is simply a subset of the spreadsheet divided for the sake of clarity. Functionally, the spreadsheet operates as a whole and all cells operate as global variables within the spreadsheet (each variable having 'read' access only except its own containing cell).
A cell may contain a value or a formula, or it may simply be left empty.
By convention, formulas usually begin with = sign.
Values.
A value can be entered from the computer keyboard by directly typing into the cell itself. Alternatively, a value can be based on a formula (see below), which might perform a calculation, display the current date or time, or retrieve external data such as a stock quote or a database value.
The Spreadsheet "Value Rule"
Computer scientist Alan Kay used the term "value rule" to summarize a spreadsheet's operation: a cell's value relies solely on the formula the user has typed into the cell.
The formula may rely on the value of other cells, but those cells are likewise restricted to user-entered data or formulas. There are no 'side effects' to calculating a formula: the only output is to display the calculated result inside its occupying cell. There is no natural mechanism for permanently modifying the contents of a cell unless the user manually modifies the cell's contents. In the context of programming languages, this yields a limited form of first-order functional programming.
Automatic recalculation.
A standard of spreadsheets since the 1980s, this optional feature eliminates the need to manually request the spreadsheet program to recalculate values (nowadays typically the default option unless specifically 'switched off' for large spreadsheets, usually to improve performance). Some earlier spreadsheets required a manual request to recalculate, since recalculation of large or complex spreadsheets often reduced data entry speed. Many modern spreadsheets still retain this option.
Real-time update.
This feature refers to updating a cell's contents periodically with a value from an external source—such as a cell in a "remote" spreadsheet. For shared, Web-based spreadsheets, it applies to "immediately" updating cells another user has updated. All dependent cells must be updated also.
Locked cell.
Once entered, selected cells (or the entire spreadsheet) can optionally be "locked" to prevent accidental overwriting. Typically this would apply to cells containing formulas but might be applicable to cells containing "constants" such as a kilogram/pounds conversion factor (2.20462262 to eight decimal places). Even though individual cells are marked as locked, the spreadsheet data are not protected until the feature is activated in the file preferences.
Data format.
A cell or range can optionally be defined to specify how the value is displayed. The default display format is usually set by its initial content if not specifically previously set, so that for example "31/12/2007" or "31 Dec 2007" would default to the cell format of "date".
Similarly adding a % sign after a numeric value would tag the cell as a percentage cell format. The cell contents are not changed by this format, only the displayed value.
Some cell formats such as "numeric" or "currency" can also specify the number of decimal places.
This can allow invalid operations (such as doing multiplication on a cell containing a date), resulting in illogical results without an appropriate warning.
Cell formatting.
Depending on the capability of the spreadsheet application, each cell (like its counterpart the "style" in a word processor) can be separately formatted using the attributes of either the content (point size, color, bold or italic) or the cell (border thickness, background shading, color). To aid the readability of a spreadsheet, cell formatting may be conditionally applied to data; for example, a negative number may be displayed in red.
A cell's formatting does not typically affect its content and depending on how cells are referenced or copied to other worksheets or applications, the formatting may not be carried with the content.
Named cells.
In most implementations, a cell, or group of cells in a column or row, can be "named" enabling the user to refer to those cells by a name rather than by a grid reference. Names must be unique within the spreadsheet, but when using multiple sheets in a spreadsheet file, an identically named cell range on each sheet can be used if it is distinguished by adding the sheet name. One reason for this usage is for creating or running macros that repeat a command across many sheets. Another reason is that formulas with named variables are readily checked against the algebra they are intended to implement (they resemble Fortran expressions). Use of named variables and named functions also makes the spreadsheet structure more transparent.
Cell reference.
In place of a named cell, an alternative approach is to use a cell (or grid) reference. Most cell references indicate another cell in the same spreadsheet, but a cell reference can also refer to a cell in a different sheet within the same spreadsheet, or (depending on the implementation) to a cell in another spreadsheet entirely, or to a value from a remote application.
A typical cell reference in "A1" style consists of one or two case-insensitive letters to identify the column (if there are up to 256 columns: A–Z and AA–IV) followed by a row number (e.g. in the range 1–65536). Either part can be relative (it changes when the formula it is in is moved or copied), or absolute (indicated with $ in front of the part concerned of the cell reference). The alternative "R1C1" reference style consists of the letter R, the row number, the letter C, and the column number; relative row or column numbers are indicated by enclosing the number in square brackets. Most current spreadsheets use the A1 style, some providing the R1C1 style as a compatibility option.
When the computer calculates a formula in one cell to update the displayed value of that cell, cell reference(s) in that cell, naming some other cell(s), cause the computer to fetch the value of the named cell(s).
A cell on the same "sheet" is usually addressed as:
 =A1
A cell on a different sheet of the same spreadsheet is usually addressed as:
 =SHEET2!A1 (that is; the first cell in sheet 2 of same spreadsheet).
Some spreadsheet implementations allow a cell references to another spreadsheet (not the current open and active file) on the same computer or a local network. It may also refer to a cell in another open and active spreadsheet on the same computer or network that is defined as shareable. These references contain the complete filename, such as:
 ='C:\Documents and Settings\Username\My spreadsheets\[main sheet]Sheet1!A1
In a spreadsheet, references to cells automatically update when new rows or columns are inserted or deleted. Care must be taken, however, when adding a row immediately before a set of column totals to ensure that the totals reflect the additional rows values—which they often do not.
A circular reference occurs when the formula in one cell refers—directly, or indirectly through a chain of cell references—to another cell that refers back to the first cell. Many common errors cause circular references. However, some valid techniques use circular references. These techniques, after many spreadsheet recalculations, (usually) converge on the correct values for those cells.
Cell ranges.
Likewise, instead of using a named range of cells, a range reference can be used. Reference to a range of cells is typically of the form (A1:A6), which specifies all the cells in the range A1 through to A6. A formula such as "=SUM(A1:A6)" would add all the cells specified and put the result in the cell containing the formula itself.
Sheets.
In the earliest spreadsheets, cells were a simple two-dimensional grid. Over time, the model has expanded to include a third dimension, and in some cases a series of named grids, called sheets. The most advanced examples allow inversion and rotation operations which can slice and project the data set in various ways.
Formulas.
A formula identifies the calculation needed to place the result in the cell it is contained within. A cell containing a formula therefore has two display components; the formula itself and the resulting value. The formula is normally only shown when the cell is selected by "clicking" the mouse over a particular cell; otherwise it contains the result of the calculation.
A formula assigns values to a cell or range of cells, and typically has the format:
where the expression consists of:
When a cell contains a formula, it often contains references to other cells. Such a cell reference is a type of variable. Its value is the value of the referenced cell or some derivation of it. If that cell in turn references other cells, the value depends on the values of those. References can be relative (e.g., codice_13, or codice_14), absolute (e.g., codice_25, or codice_26) or mixed row– or column-wise absolute/relative (e.g., codice_27 is column-wise absolute and codice_28 is row-wise absolute).
The available options for valid formulas depends on the particular spreadsheet implementation but, in general, most arithmetic operations and quite complex nested conditional operations can be performed by most of today's commercial spreadsheets. Modern implementations also offer functions to access custom-build functions, remote data, and applications.
A formula may contain a condition (or nested conditions)—with or without an actual calculation—and is sometimes used purely to identify and highlight errors. In the example below, it is assumed the sum of a column of percentages (A1 through A6) is tested for validity and an explicit message put into the adjacent right-hand cell.
Further examples:
The best way to build up conditional statements is step by step composing followed by trial and error testing and refining code.
A spreadsheet does not, in fact, have to contain any formulas at all, in which case it could be considered merely a collection of data arranged in rows and columns (a database) like a calendar, timetable or simple list. Because of its ease of use, formatting and hyperlinking capabilities, many spreadsheets are used solely for this purpose.
Functions.
Spreadsheets usually contain a number of supplied functions, such as arithmetic operations (for example, summations, averages and so forth), trigonometric functions, statistical functions, and so forth. In addition there is often a provision for "user-defined functions". In Microsoft Excel these functions are defined using Visual Basic for Applications in the supplied Visual Basic editor, and such functions are automatically accessible on the worksheet. In addition, programs can be written that pull information from the worksheet, perform some calculations, and report the results back to the worksheet. In the figure, the name "sq" is user-assigned, and function "sq" is introduced using the "Visual Basic" editor supplied with Excel. "Name Manager" displays the spreadsheet definitions of named variables "x" & "y".
Subroutines.
Functions themselves cannot write into the worksheet, but simply return their evaluation. However, in Microsoft Excel, subroutines can write values or text found within the subroutine directly to the spreadsheet. The figure shows the Visual Basic code for a subroutine that reads each member of the named column variable "x", calculates its square, and writes this value into the corresponding element of named column variable "y". The "y" column contains no formula because its values are calculated in the subroutine, not on the spreadsheet, and simply are written in.
Remote spreadsheet.
Whenever a reference is made to a cell or group of cells that are not located within the current physical spreadsheet file, it is considered as accessing a "remote" spreadsheet. The contents of the referenced cell may be accessed either on first reference with a manual update or more recently in the case of web based spreadsheets, as a near real time value with a specified automatic refresh interval.
Charts.
Many spreadsheet applications permit charts, graphs or histograms to be generated from specified groups of cells that are dynamically re-built as cell contents change. The generated graphic component can either be embedded within the current sheet or added as a separate object.
Multi-dimensional spreadsheets.
In the late 1980s and early 1990s, first Javelin Software and later Lotus Improv appeared and unlike models in a conventional spreadsheet, they utilized models built on objects called variables, not on data in cells of a report. These multi-dimensional spreadsheets enabled viewing data and algorithms in various self-documenting ways, including simultaneous multiple synchronized views. For example, users of Javelin could move through the connections between variables on a diagram while seeing the logical roots and branches of each variable. This is an example of what is perhaps its primary contribution of the earlier Javelin—the concept of traceability of a user's logic or model structure through its twelve views. A complex model can be dissected and understood by others who had no role in its creation, and this remains unique even today. Javelin was used primarily for financial modeling, but was also used to build instructional models in college chemistry courses, to model the world's economies, and by the military in the early Star Wars project. It is still in use by institutions for which model integrity is mission critical.
In these programs, a time series, or any variable, was an object in itself, not a collection of cells that happen to appear in a row or column. Variables could have many attributes, including complete awareness of their connections to all other variables, data references, and text and image notes. Calculations were performed on these objects, as opposed to a range of cells, so adding two time series automatically aligns them in calendar time, or in a user-defined time frame. Data were independent of worksheets—variables, and therefore data, could not be destroyed by deleting a row, column or entire worksheet. For instance, January's costs are subtracted from January's revenues, regardless of where or whether either appears in a worksheet. This permits actions later used in pivot tables, except that flexible manipulation of report tables was but one of many capabilities supported by variables. Moreover, if costs were entered by week and revenues by month, Javelin's program could allocate or interpolate as appropriate. This object design enabled variables and whole models to reference each other with user-defined variable names, and to perform multidimensional analysis and massive, but easily editable consolidations.
Trapeze, a spreadsheet on the Mac, went further and explicitly supported
not just table columns, but also matrix operators.
Logical spreadsheets.
Spreadsheets that have a formula language based upon logical expressions, rather than arithmetic expressions are known as logical spreadsheets. Such spreadsheets can be used to reason deductively about their cell values.
Programming issues.
Just as the early programming languages were designed to generate spreadsheet printouts, programming techniques themselves have evolved to process tables (also known as spreadsheets or matrices) of data more efficiently in the computer itself.
End-user development.
Spreadsheets are a popular End-user development tool. EUD denotes activities or techniques in which people who are not professional developers create automated behavior and complex data objects without significant knowledge of a programming language. Many people find it easier to perform calculations in spreadsheets than by writing the equivalent sequential program. This is due to several traits of spreadsheets.
Spreadsheet programs.
A "spreadsheet program" is designed to perform general computation tasks using spatial relationships rather than time as the primary organizing principle.
It is often convenient to think of a spreadsheet as a mathematical graph, where the nodes are spreadsheet cells, and the edges are references to other cells specified in formulas. This is often called the dependency graph of the spreadsheet. References between cells can take advantage of spatial concepts such as relative position and absolute position, as well as named locations, to make the spreadsheet formulas easier to understand and manage.
Spreadsheets usually attempt to automatically update cells when the cells they depend on change. The earliest spreadsheets used simple tactics like evaluating cells in a particular order, but modern spreadsheets calculate following a minimal recomputation order from the dependency graph. Later spreadsheets also include a limited ability to propagate values in reverse, altering source values so that a particular answer is reached in a certain cell. Since spreadsheet cells formulas are not generally invertible, though, this technique is of somewhat limited value.
Many of the concepts common to sequential programming models have analogues in the spreadsheet world. For example, the sequential model of the indexed loop is usually represented as a table of cells, with similar formulas (normally differing only in which cells they reference).
Spreadsheets have evolved to use scripting programming languages like VBA as a tool for extensibility beyond what the spreadsheet language makes easy.
Shortcomings.
While spreadsheets represented a major step forward in quantitative modeling, they have deficiencies. Their shortcomings include the perceived unfriendliness of alpha-numeric cell addresses.
Other problems associated with spreadsheets include:
While there are built-in and third-party tools for desktop spreadsheet applications that address some of these shortcomings, awareness and use of these is generally low. A good example of this is that 55% of Capital market professionals "don't know" how their spreadsheets are audited; only 6% invest in a third-party solution
Spreadsheet risk.
Spreadsheet risk is the risk associated with deriving a materially incorrect value from a spreadsheet application that will be utilised in making a related (usually numerically based) decision. Examples include the valuation of an asset, the determination of financial accounts, the calculation of medicinal doses or the size of load-bearing beam for structural engineering. The risk may arise from inputting erroneous or fraudulent data values, from mistakes (or incorrect changes) within the logic of the spreadsheet or the omission of relevant updates (e.g. out of date exchange rates). Some single-instance errors have exceeded US$1 billion. Because spreadsheet risk is principally linked to the actions (or inaction) of individuals it is defined as a sub-category of operational risk.
In the report into the 2012 JPMorgan Chase trading loss, a lack of control over spreadsheets used for critical financial functions was cited as a factor in the trading losses of more than six billion dollars which were reported as a result of derivatives trading gone bad.
Despite this, research carried out by ClusterSeven revealed that around half (48%) of c-level executives and senior managers at firms reporting annual revenues over £50m said there were either no usage controls at all or poorly applied manual processes over the use of spreadsheets at the firms.
In 2013 Thomas Herndon, a graduate student of economics at the University of Massachusetts Amherst found major coding flaws in the spreadsheet used by the economists Carmen Reinhart and Kenneth Rogoff in a very influential 2010 journal article. The Reinhart and Rogoff article was widely used as justification to drive 2010-13 European austerity programs.

</doc>
<doc id="27691" url="http://en.wikipedia.org/wiki?curid=27691" title="Social security">
Social security

Social security is a concept enshrined in Article 22 of the Universal Declaration of Human Rights, which states: 
Everyone, as a member of society, has the right to social security and is entitled to realization, through national effort and international co-operation and in accordance with the organization and resources of each State, of the economic, social and cultural rights indispensable for his dignity and the free development of his personality. 
In simple terms, the signatories agree that society in which a person lives should help them to develop and to make the most of all the advantages (culture, work, social welfare) which are offered to them in the country.
Social security may also refer to the action programs of government intended to promote the welfare of the population through assistance measures guaranteeing access to sufficient resources for food and shelter and to promote health and well-being for the population at large and potentially vulnerable segments such as children, the elderly, the sick and the unemployed. Services providing social security are often called social services.
Terminology in this area in the United States is somewhat different from in the rest of the English-speaking world. The general term for an action program in support of the well being of the population in the United States is "welfare program" and the general term for all such programs is simply "welfare". In American society, the term "welfare" arguably has negative connotations. The term "Social Security", in the United States, refers to a specific social insurance program for the retired and the disabled. Elsewhere the term is used in a much broader sense, referring to the economic security society offers when people are faced with certain risks. In its 1952 Social Security (Minimum Standards) Convention (nr. 102), the International Labour Organization (ILO) defined the traditional contingencies covered by social security as including:
People who cannot reach a guaranteed social minimum for other reasons may be eligible for "social assistance" (or welfare, in American English).
Modern authors often consider the ILO approach too narrow. In their view, social security is not limited to the provision of cash transfers, but also aims at security of work, health, and social participation; and new social risks (single parenthood, the reconciliation of work and family life) should be included in the list as well.
Social security may refer to:
A report published by the ILO in 2014 estimated that only 27% of the world's population has access to comprehensive social security.
History.
While several of the provisions to which the concept refers have a long history (especially in poor relief), the notion of "social security" itself is a fairly recent one. The earliest examples of use date from the 19th century. In a speech to mark the independence of Venezuela, Simón Bolívar (1819) pronounced: "El sistema de gobierno más perfecto es aquel que produce mayor suma de felicidad posible, mayor suma de "seguridad social" y mayor suma de estabilidad política" (which translates to "The most perfect system of government is that which produces the greatest amount of happiness, the greatest amount of social security and the greatest amount of political stability").
In the Roman Empire, social welfare to help the poor was enlarged by the Emperor Trajan. Trajan's program brought acclaim from many, including Pliny the Younger.
In Jewish tradition, charity (represented by tzedakah) is a matter of religious obligation rather than benevolence. Contemporary charity is regarded as a continuation of the Biblical Maaser Ani, or poor-tithe, as well as Biblical practices, such as permitting the poor to glean the corners of a field and harvest during the Shmita (Sabbatical year). Voluntary charity, along with prayer and repentance, is befriended to ameliorate the consequences of bad acts.
The Song dynasty (c.1000AD) government supported multiple forms of social assistance programs, including the establishment of retirement homes, public clinics, and pauper's graveyards
According to Robert Henry Nelson, "The medieval Roman Catholic Church operated a far-reaching and comprehensive welfare system for the poor..."
The concepts of welfare and pension were put into practice in the early Islamic law of the Caliphate as forms of "Zakat" (charity), one of the Five Pillars of Islam, since the time of the Rashidun caliph Umar in the 7th century. The taxes (including "Zakat" and "Jizya") collected in the treasury of an Islamic government were used to provide income for the needy, including the poor, elderly, orphans, widows, and the disabled. According to the Islamic jurist Al-Ghazali (Algazel, 1058–1111), the government was also expected to store up food supplies in every region in case a disaster or famine occurred. (See Bayt al-mal for further information.)
There is relatively little statistical data on transfer payments before the High Middle Ages. In the medieval period and until the Industrial Revolution, the function of welfare payments in Europe was principally achieved through private giving or charity. In those early times, there was a much broader group considered to be in poverty as compared to the 21st century.
Early welfare programs in Europe included the English Poor Law of 1601, which gave parishes the responsibility for providing poverty relief assistance to the poor. This system was substantially modified by the 19th-century Poor Law Amendment Act, which introduced the system of workhouses.
It was predominantly in the late 19th and early 20th centuries that an organized system of state welfare provision was introduced in many countries. Otto von Bismarck, Chancellor of Germany, introduced one of the first welfare systems for the working classes in 1883. In Great Britain the Liberal government of Henry Campbell-Bannerman and David Lloyd George introduced the National Insurance system in 1911, a system later expanded by Clement Attlee. The United States did not have an organized welfare system until the Great Depression, when emergency relief measures were introduced under President Franklin D. Roosevelt. Even then, Roosevelt's New Deal focused predominantly on a program of providing work and stimulating the economy through public spending on projects, rather than on cash payment.
Income maintenance.
This policy is usually applied through various programs designed to provide a population with income at times when they are unable to care for themselves. Income maintenance is based in a combination of five main types of program:
Social protection.
Social protection refers to a set of benefits available (or not available) from the state, market, civil society and households, or through a combination of these agencies, to the individual/households to reduce multi-dimensional deprivation. This multi-dimensional deprivation could be affecting less active poor persons (such as the elderly or the disabled) and active poor persons (such as the unemployed).
This broad framework makes this concept more acceptable in developing countries than the concept of social security. Social security is more applicable in the conditions, where large numbers of citizens depend on the formal economy for their livelihood. Through a defined contribution, this social security may be managed.
But, in the context of widespread informal economy, formal social security arrangements are almost absent for the vast majority of the working population. Besides, in developing countries, the state's capacity to reach the vast majority of the poor people may be limited because of its limited infrastructure and resources. In such a context, multiple agencies that could provide for social protection, including health care, is critical for policy consideration. The framework of social protection is thus holds the state responsible for providing for the poorest populations by regulating non-state agencies.
Collaborative research from the Institute of Development Studies debating Social Protection from a global perspective, suggests that advocates for social protection fall into two broad categories: "instrumentalists" and "activists". Instrumentalists argue that extreme poverty, inequality, and vulnerability is dysfunctional in the achievement of development targets (such as the MDGs). In this view, social protection is about putting in place risk management mechanisms that will compensate for incomplete or missing insurance (and other) markets, until a time that private insurance can play a more prominent role in that society. Activist arguments view the persistence of extreme poverty, inequality, and vulnerability as symptoms of social injustice and structural inequality and see social protection as a right of citizenship. Targeted welfare is a necessary step between humanitarianism and the ideal of a "guaranteed minimum income" where entitlement extends beyond cash or food transfers and is based on citizenship, not philanthropy.

</doc>
<doc id="27692" url="http://en.wikipedia.org/wiki?curid=27692" title="Steam engine">
Steam engine

A steam engine is a heat engine that performs mechanical work using steam as its working fluid.
Using boiling water to produce mechanical motion goes back over 2000 years, but early devices were not practical. The Spanish inventor Jerónimo de Ayanz y Beaumont patented in 1606 the first steam engine. In 1698 Thomas Savery patented a steam pump that used steam in direct contact with the water being pumped. Savery's steam pump used condensing steam to create a vacuum and draw water into a chamber, and then applied pressurized steam to further pump the water. Thomas Newcomen's "atmospheric engine" was the first commercial true steam engine using a piston, and was used in 1712 for pumping in a mine. 
In 1781 James Watt patented a steam engine that produced continuous rotary motion. Watt's ten-horsepower engines enabled a wide range of manufacturing machinery to be powered. The engines could be sited anywhere that water and coal or wood fuel could be obtained. By 1883, engines that could provide 10,000 hp had become feasible. Steam engines could also be applied to vehicles such as traction engines and the railway locomotives. The stationary steam engine was a key component of the Industrial Revolution, allowing factories to locate where water power was unavailable.
Steam engines are external combustion engines, where the working fluid is separate from the combustion products. Non-combustion heat sources such as solar power, nuclear power or geothermal energy may be used. The ideal thermodynamic cycle used to analyze this process is called the Rankine cycle. In the cycle, water is heated and transforms into steam within a boiler operating at a high pressure. When expanded through pistons or turbines, mechanical work is done. The reduced-pressure steam is then condensed and pumped back into the boiler.
In general usage, the term "steam engine" can refer to either the integrated steam plants (including boilers etc.) such as railway steam locomotives and portable engines, or may refer to the piston or turbine machinery alone, as in the beam engine and stationary steam engine. Specialized devices such as steam hammers and steam pile drivers are dependent on steam supplied from a separate boiler. Reciprocating piston type steam engines remained the dominant source of power until the early 20th century, when advances in the design of electric motors and internal combustion engines gradually resulted in the replacement of reciprocating (piston) steam engines in commercial usage, and the ascendancy of steam turbines in power generation. Considering that the great majority of worldwide electric generation is produced by turbine type steam engines, the "steam age" is continuing with energy levels far beyond those of the turn of the 19th century.
History.
History and modifications:
The Aeolipile (hero of Alexandria) is considered to be the first recorded steam engine which produced torque by steam jets exiting the turbine.
Thomas Savery, in 1698, patented first practical, atmospheric pressure, steam engine of one horse power (1 hp). It had no piston or moving parts but taps. It was a "fire engine", a kind of thermic syphon, in which steam was admitted to an empty container and then condensed. The vacuum thus created was used to suck water
from the sump at the bottom of the mine. The "fire engine" was not very effective and could not work beyond a limited depth of around thirty feet.
Thomas Newcomen, in 1712, modified Savery’s engine and developed first commercially successful steam engine of five horse power (5 hp). Its principle was to harness power of steam to produce mechanical work.
James Watt, in 1781, patented steam engine that produced continued rotary motion and the power about 10 hp. It was the first type of steam engine to make use of steam at a pressure just above atmospheric to drive the piston helped by a partial vacuum. It was an improvement of Newcomen’s engine.
From 1883 to now on-wards, engines having power upto 10000 hp are available.
Since the early 18th century, steam power has been applied to a variety of practical uses. At first it was applied to reciprocating pumps, but from the 1780s rotative engines (i.e. those converting reciprocating motion into rotary motion) began to appear, driving factory machinery such as spinning mules and power looms. At the turn of the 19th century, steam-powered transport on both sea and land began to make its appearance becoming ever more dominant as the century progressed.
Steam engines can be said to have been the moving force behind the Industrial Revolution and saw widespread commercial use driving machinery in factories, mills and mines; powering pumping stations; and propelling transport appliances such as railway locomotives, ships and road vehicles. Their use in agriculture led to an increase in the land available for cultivation.
The weight of boilers and condensors generally makes the power-to-weight ratio of a steam plant lower than for internal combustion engines. For mobile applications steam has been largely superseded by internal combustion engines or electric motors. However most electric power is generated using steam turbine plant, so that indirectly the world's industry is still dependent on steam power. Recent concerns about fuel sources and pollution have incited a renewed interest in steam both as a component of cogeneration processes and as a prime mover. This is becoming known as the Advanced Steam movement.
Early experiments.
The history of the steam engine stretches back as far as the first century AD; the first recorded rudimentary steam engine being the aeolipile described by Greek mathematician Hero of Alexandria. In the following centuries, the few steam-powered "engines" known were, like the aeolipile, essentially experimental devices used by inventors to demonstrate the properties of steam. A rudimentary steam turbine device was described by Taqi al-Din in 1551 and by Giovanni Branca in 1629. Jerónimo de Ayanz y Beaumont received patents in 1606 for fifty steam powered inventions, including a water pump for draining inundated mines. Denis Papin, a Huguenot refugee, did some useful work on the steam digester in 1679, and first used a piston to raise weights in 1690.
Pumping engines.
The first commercial steam-powered device was a water pump, developed in 1698 by Thomas Savery. It used a vacuum to raise water from below, then used steam pressure to raise it higher. Small engines were effective though larger models were problematic. They proved only to have a limited lift height and were prone to boiler explosions. It received some use in mines, pumping stations and for supplying water wheels used to power textile machinery. An attractive feature of the Savery engine was its low cost. Bento de Moura Portugal, , introduced an ingenious improvement of Savery's construction "to render it capable of working itself", as described by John Smeaton in the Philosophical Transactions published in 1751. It continued to be manufactured until the late 18th century. One engine was still known to be operating in 1820.
Piston steam engines.
The first commercially successful true engine, in that it could generate power and transmit it to a machine, was the atmospheric engine, invented by Thomas Newcomen around 1712. It made use of technologies discovered by Savery and Papin. Newcomen's engine was relatively inefficient, and in most cases was used for pumping water. It worked by creating a partial vacuum by condensing steam under a piston within a cylinder. It was employed for draining mine workings at depths hitherto impossible, and also for providing a reusable water supply for driving waterwheels at factories sited away from a suitable "head". Water that had passed over the wheel was pumped back up into a storage reservoir above the wheel.
In 1720 Jacob Leupold described a two-cylinder high-pressure steam engine. The invention was published in his major work "Theatri Machinarum Hydraulicarum". The engine used two lead-weighted pistons providing a continuous motion to a water pump. Each piston was raised by the steam pressure and returned to its original position by gravity. The two pistons shared a common four way rotary valve connected directly to a steam boiler.
The next major step occurred when James Watt developed (1763–1775) an improved version of Newcomen's engine, with a separate condenser. Boulton and Watt's early engines used half as much coal as John Smeaton's improved version of Newcomen's. Newcomen's and Watt's early engines were "atmospheric". They were powered by air pressure pushing a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam. The engine cylinders had to be large because the only usable force acting on them was due to atmospheric pressure.
Watt proceeded to develop his engine further, modifying it to provide a rotary motion suitable for driving factory machinery. This enabled factories to be sited away from rivers, and further accelerated the pace of the Industrial Revolution.
High-pressure engines.
Around 1800 Richard Trevithick and, separately, Oliver Evans in 1801 introduced engines using high-pressure steam; Trevithick obtained his high-pressure engine patent in 1802. These were much more powerful for a given cylinder size than previous engines and could be made small enough for transport applications. Thereafter, technological developments and improvements in manufacturing techniques (partly brought about by the adoption of the steam engine as a power source) resulted in the design of more efficient engines that could be smaller, faster, or more powerful, depending on the intended application.
The Cornish engine was developed by Trevithick and others in the 1810s. It was a compound cycle engine that used high-pressure steam expansively, then condensed the low-pressure steam, making it relatively efficient. The Cornish engine had irregular motion and torque though the cycle, limiting it mainly to pumping. Cornish engines were used in mines and for water supply until the late 19th century.
Horizontal stationary engine.
Early builders of stationary steam engines considered that horizontal cylinders would be subject to excessive wear. Their engines were therefore arranged with the piston axis vertical. In time the horizontal arrangement became more popular, allowing compact, but powerful engines to be fitted in smaller spaces.
The acme of the horizontal engine was the Corliss steam engine, patented in 1849, which was a four-valve counter flow engine with separate steam admission and exhaust valves and automatic variable steam cutoff. When Corliss was given the Rumford medal the committee said that "no one invention since Watt's time has so enhanced the efficiency of the steam engine". In addition to using 30% less steam, it provided more uniform speed due to variable steam cut off, making it well suited to manufacturing, especially cotton spinning.
Marine engines.
Near the end of the 19th century compound engines came into widespread use. Compound engines exhausted steam in to successively larger cylinders to accommodate the higher volumes at reduced pressures, giving improved efficiency. These stages were called expansions, with double and triple expansion engines being common, especially in shipping where efficiency was important to reduce the weight of coal carried. Steam engines remained the dominant source of power until the early 20th century, when advances in the design of electric motors and internal combustion engines gradually resulted in the replacement of reciprocating (piston) steam engines, with shipping in the 20th-century relying upon the steam turbine.
Steam locomotives.
As the development of steam engines progressed through the 18th century, various attempts were made to apply them to road and railway use. In 1784, William Murdoch, a Scottish inventor, built a prototype steam road locomotive. An early working model of a steam rail locomotive was designed and constructed by steamboat pioneer John Fitch in the United States probably during the 1780s or 1790s.
His steam locomotive used interior bladed wheels guided by rails or tracks.
The first full-scale working railway steam locomotive was built by Richard Trevithick in the United Kingdom and, on 21 February 1804, the world's first railway journey took place as Trevithick's unnamed steam locomotive hauled a train along the tramway from the Pen-y-darren ironworks, near Merthyr Tydfil to Abercynon in south Wales. The design incorporated a number of important innovations that included using high-pressure steam which reduced the weight of the engine and increased its efficiency. Trevithick visited the Newcastle area later in 1804 and the colliery railways in north-east England became the leading centre for experimentation and development of steam locomotives.
Trevithick continued his own experiments using a trio of locomotives, concluding with the Catch Me Who Can in 1808. Only four years later, the successful twin-cylinder locomotive "Salamanca" by Matthew Murray was used by the edge railed rack and pinion Middleton Railway. In 1825 George Stephenson built the "Locomotion" for the Stockton and Darlington Railway. This was the first public steam railway in the world and then in 1829, he built "The Rocket" which was entered in and won the Rainhill Trials. The Liverpool and Manchester Railway opened in 1830 making exclusive use of steam power for both passenger and freight trains.
Steam locomotives continued to be manufactured until the late twentieth century in places such as China and the former East Germany.
Steam turbines.
The final major evolution of the steam engine design was the use of steam turbines starting in the late part of the 19th century. Steam turbines are generally more efficient than reciprocating piston type steam engines (for outputs above several hundred horsepower), have fewer moving parts, and provide rotary power directly instead of through a connecting rod system or similar means. Steam turbines virtually replaced reciprocating engines in electricity generating stations early in the 20th century, where their efficiency, higher speed appropriate to generator service, and smooth rotation were advantages. Today most electric power is provided by steam turbines. In the United States 90% of the electric power is produced in this way using a variety of heat sources. Steam turbines were extensively applied for propulsion of large ships throughout most of the 20th century.
Present development.
Although the reciprocating steam engine is no longer in widespread commercial use, various companies are exploring or exploiting the potential of the engine as an alternative to internal combustion engines. The company Energiprojekt AB in Sweden has made progress in using modern materials for harnessing the power of steam. The efficiency of Energiprojekt's steam engine reaches some 27-30% on high-pressure engines. It is a single-step, 5-cylinder engine (no compound) with superheated steam and consumes approx. 4 kg of steam per kWh.
Components and accessories of steam engines.
There are two fundamental components of a steam plant: the boiler or steam generator, and the "motor unit", referred to itself as a "steam engine". Stationary steam engines in fixed buildings may have the boiler and engine in separate buildings some distance apart. For portable or mobile use, such as steam locomotives, the two are mounted together.
The widely used reciprocating engine typically consisted of a cast iron cylinder, piston, connecting rod and beam or a crank and flywheel, and miscellaneous linkages. Steam was alternately supplied and exhausted by one or more valves. Speed control was either automatic, using a governor, or by a manual valve. The cylinder casting contained steam supply and exhaust ports.
Engines equipped with a condenser are a separate type than those that exhaust to the atmosphere.
Other components are often present; pumps (such as an injector) to supply water to the boiler during operation, condensers to recirculate the water and recover the latent heat of vaporisation, and superheaters to raise the temperature of the steam above its saturated vapour point, and various mechanisms to increase the draft for fireboxes. When coal is used, a chain or screw stoking mechanism and its drive engine or motor may be included to move the fuel from a supply bin (bunker) to the firebox. See: Mechanical stoker
Heat source.
The heat required for boiling the water and supplying the steam can be derived from various sources, most commonly from burning combustible materials with an appropriate supply of air in a closed space (called variously combustion chamber, firebox). In some cases the heat source is a nuclear reactor or geothermal energy.
Boilers.
Boilers are pressure vessels that contain water to be boiled, and some kind of mechanism for transferring the heat to the water so as to boil it.
The two most common methods of transferring heat to the water are:
Fire tube boilers were the main type used for early high-pressure steam (typical steam locomotive practice), but they were to a large extent displaced by more economical water tube boilers in the late 19th century for marine propulsion and large stationary applications.
Once turned to steam, many boilers raise the temperature of the steam further, turning 'wet steam' into 'superheated steam'. This use of superheating avoids the steam condensing within the engine, and allows significantly greater efficiency.
Motor units.
In a steam engine, a piston or steam turbine or any other similar device for doing mechanical work takes a supply of steam at high pressure and temperature and gives out a supply of steam at lower pressure and temperature, using as much of the difference in steam energy as possible to do mechanical work.
These "motor units" are often called 'steam engines' in their own right. They will also operate on compressed air or other gas.
Cold sink.
As with all heat engines, a considerable quantity of waste heat at relatively low temperature is produced and must be disposed of.
The simplest cold sink is to vent the steam to the environment. This is often used on steam locomotives, as the released steam is released in the chimney so as to increase the draw on the fire, which greatly increases engine power, but is inefficient.
Sometimes the waste heat is useful itself, and in those cases very high overall efficiency can be obtained. For example, combined heat and power (CHP) systems use the waste steam for district heating.
Where CHP is not used, steam turbines in power stations use surface condensers as a cold sink. The condensers are cooled by water flow from oceans, rivers, lakes, and often by cooling towers which evaporate water to provide cooling energy removal. The resulting condensed hot water output from the condenser is then put back into the boiler via a pump. A dry type cooling tower is similar to an automobile radiator and is used in locations where water is costly. Evaporative (wet) cooling towers use the rejected heat to evaporate water; this water is kept separate from the condensate, which circulates in a closed system and returns to the boiler. Such towers often have visible plumes due to the evaporated water condensing into droplets carried up by the warm air. Evaporative cooling towers need less water flow than "once-through" cooling by river or lake water; a 700 megawatt coal-fired power plant may use about 3600 cubic metres of make-up water every hour for evaporative cooling, but would need about twenty times as much if cooled by river water. 
Water pump.
The Rankine cycle and most practical steam engines have a water pump to recycle or top up the boiler water, so that they may be run continuously. Utility and industrial boilers commonly use multi-stage centrifugal pumps; however, other types are used. Another means of supplying lower-pressure boiler feed water is an injector, which uses a steam jet usually supplied from the boiler. Injectors became popular in the 1850s but are no longer widely used, except in applications such as steam locomotives.
Monitoring and control.
For safety reasons, nearly all steam engines are equipped with mechanisms to monitor the boiler, such as a pressure gauge and a sight glass to monitor the water level.
Many engines, stationary and mobile, are also fitted with a governor (see below) to regulate the speed of the engine without the need for human interference (similar to cruise control in some cars).
The most useful instrument for analyzing the performance of steam engines is the steam engine indicator. Early versions were in use by 1851, but the most successful indicator was developed for the high speed engine inventor and manufacturer Charles Porter by Charles Richard and exhibited at London Exhibition in 1862. The steam engine indicator traces on paper the pressure in the cylinder throughout the cycle, which can be used to spot various problems and calculate developed horsepower. It was routinely used by engineers, mechanics and insurance inspectors. The engine indicator can also be used on internal combustion engines. See image of indicator diagram below (in "Types of motor units" section).
Governor.
The centrifugal governor was adopted by James Watt for use on a steam engine in 1788 after Watt’s partner Boulton saw one at a flour mill Boulton & Watt were building. The governor could not actually hold a set speed, because it would assume a new constant speed in response to load changes. The governor was able to handle smaller variations such as those caused by fluctuating heat load to the boiler. Also, there was a tendency for oscillation whenever there was a speed change. As a consequence, engines equipped only with this governor were not suitable for operations requiring constant speed, such as cotton spinning. The governor was improved over time and coupled with variable steam cut off, good speed control in response to changes in load was attainable near the end of the 19th century.
Engine configuration.
Simple engine.
In a simple engine the charge of steam works only once in a cylinder. It is then exhausted directly into the atmosphere or into a condenser. As steam expands in a high-pressure engine its temperature drops because no heat is added to the system; this is known as adiabatic expansion and results in steam entering the cylinder at high temperature and leaving at low temperature. This causes a cycle of heating and cooling of the cylinder with every stroke, which is a source of inefficiency.
Compound engines.
A method to lessen the magnitude of this heating and cooling was invented in 1804 by British engineer Arthur Woolf, who patented his "Woolf high-pressure compound engine" in 1805. In the compound engine, high-pressure steam from the boiler expands in a high-pressure (HP) cylinder and then enters one or more subsequent lower-pressure (LP) cylinders. The complete expansion of the steam now occurs across multiple cylinders and as less expansion now occurs in each cylinder less heat is lost by the steam in each. This reduces the magnitude of cylinder heating and cooling, increasing the efficiency of the engine. By staging the expansion in multiple cylinders, torque variability can be reduced. To derive equal work from lower-pressure steam requires a larger cylinder volume as this steam occupies a greater volume. Therefore the bore, and often the stroke, are increased in low-pressure cylinders resulting in larger cylinders.
Double expansion (usually known as compound) engines expanded the steam in two stages. The pairs may be duplicated or the work of the large low-pressure cylinder can be split with one high-pressure cylinder exhausting into one or the other, giving a 3-cylinder layout where cylinder and piston diameter are about the same making the reciprocating masses easier to balance.
Two-cylinder compounds can be arranged as:
With two-cylinder compounds used in railway work, the pistons are connected to the cranks as with a two-cylinder simple at 90° out of phase with each other ("quartered").
When the double expansion group is duplicated, producing a 4-cylinder compound, the individual pistons within the group are usually balanced at 180°, the groups being set at 90° to each other. In one case (the first type of Vauclain compound), the pistons worked in the same phase driving a common crosshead and crank, again set at 90° as for a two-cylinder engine.
With the 3-cylinder compound arrangement, the LP cranks were either set at 90° with the HP one at 135° to the other two, or in some cases all three cranks were set at 120°.
The adoption of compounding was common for industrial units, for road engines and almost universal for marine engines after 1880; it was not universally popular in railway locomotives where it was often perceived as complicated. This is partly due to the harsh railway operating environment and limited space afforded by the loading gauge (particularly in Britain, where compounding was never common and not employed after 1930). However, although never in the majority, it was popular in many other countries.
Multiple expansion engines.
It is a logical extension of the compound engine (described above) to split the expansion into yet more stages to increase efficiency. The result is the multiple expansion engine. Such engines use either three or four expansion stages and are known as "triple" and "quadruple expansion engines" respectively. These engines use a series of cylinders of progressively increasing diameter. These cylinders are designed to divide the work into equal shares for each expansion stage. As with the double expansion engine, if space is at a premium, then two smaller cylinders may be used for the low-pressure stage. Multiple expansion engines typically had the cylinders arranged inline, but various other formations were used. In the late 19th century, the Yarrow-Schlick-Tweedy balancing 'system' was used on some marine triple expansion engines. Y-S-T engines divided the low-pressure expansion stages between two cylinders, one at each end of the engine. This allowed the crankshaft to be better balanced, resulting in a smoother, faster-responding engine which ran with less vibration. This made the 4-cylinder triple-expansion engine popular with large passenger liners (such as the Olympic class), but this was ultimately replaced by the virtually vibration-free turbine engine.
The image to the right shows an animation of a triple expansion engine. The steam travels through the engine from left to right. The valve chest for each of the cylinders is to the left of the corresponding cylinder.
Land-based steam engines could exhaust much of their steam, as feed water was usually readily available. Prior to and during World War I, the expansion engine dominated marine applications where high vessel speed was not essential. It was however superseded by the British invention steam turbine where speed was required, for instance in warships, such as the dreadnought battleships, and ocean liners. HMS "Dreadnought" of 1905 was the first major warship to replace the proven technology of the reciprocating engine with the then-novel steam turbine.
Types of motor units.
Reciprocating piston.
In most reciprocating piston engines, the steam reverses its direction of flow at each stroke (counterflow), entering and exhausting from the cylinder by the same port. The complete engine cycle occupies one rotation of the crank and two piston strokes; the cycle also comprises four "events" – admission, expansion, exhaust, compression. These events are controlled by valves often working inside a "steam chest" adjacent to the cylinder; the valves distribute the steam by opening and closing steam "ports" communicating with the cylinder end(s) and are driven by valve gear, of which there are many types.
The simplest valve gears give events of fixed length during the engine cycle and often make the engine rotate in only one direction. Most however have a reversing mechanism which additionally can provide means for saving steam as speed and momentum are gained by gradually "shortening the cutoff" or rather, shortening the admission event; this in turn proportionately lengthens the expansion period. However, as one and the same valve usually controls both steam flows, a short cutoff at admission adversely affects the exhaust and compression periods which should ideally always be kept fairly constant; if the exhaust event is too brief, the totality of the exhaust steam cannot evacuate the cylinder, choking it and giving excessive compression ("kick back").
In the 1840s and 50s, there were attempts to overcome this problem by means of various patent valve gears with a separate, variable cutoff expansion valve riding on the back of the main slide valve; the latter usually had fixed or limited cutoff. The combined setup gave a fair approximation of the ideal events, at the expense of increased friction and wear, and the mechanism tended to be complicated. The usual compromise solution has been to provide "lap" by lengthening rubbing surfaces of the valve in such a way as to overlap the port on the admission side, with the effect that the exhaust side remains open for a longer period after cut-off on the admission side has occurred. This expedient has since been generally considered satisfactory for most purposes and makes possible the use of the simpler Stephenson, Joy and Walschaerts motions. Corliss, and later, poppet valve gears had separate admission and exhaust valves driven by trip mechanisms or cams profiled so as to give ideal events; most of these gears never succeeded outside of the stationary marketplace due to various other issues including leakage and more delicate mechanisms.
Before the exhaust phase is quite complete, the exhaust side of the valve closes, shutting a portion of the exhaust steam inside the cylinder. This determines the compression phase where a cushion of steam is formed against which the piston does work whilst its velocity is rapidly decreasing; it moreover obviates the pressure and temperature shock, which would otherwise be caused by the sudden admission of the high-pressure steam at the beginning of the following cycle.
The above effects are further enhanced by providing "lead": as was later discovered with the internal combustion engine, it has been found advantageous since the late 1830s to advance the admission phase, giving the valve "lead" so that admission occurs a little before the end of the exhaust stroke in order to fill the "clearance volume" comprising the ports and the cylinder ends (not part of the piston-swept volume) before the steam begins to exert effort on the piston.
Uniflow (or unaflow) engine.
Uniflow engines attempt to remedy the difficulties arising from the usual counterflow cycle where, during each stroke, the port and the cylinder walls will be cooled by the passing exhaust steam, whilst the hotter incoming admission steam will waste some of its energy in restoring working temperature. The aim of the uniflow is to remedy this defect and improve efficiency by providing an additional port uncovered by the piston at the end of each stroke making the steam flow only in one direction. By this means, the simple-expansion uniflow engine gives efficiency equivalent to that of classic compound systems with the added advantage of superior part-load performance, and comparable efficiency to turbines for smaller engines below one thousand horsepower. However, the thermal expansion gradient uniflow engines produce along the cylinder wall gives practical difficulties.
Turbine engines.
A steam turbine consists of one or more "rotors" (rotating discs) mounted on a drive shaft, alternating with a series of "stators" (static discs) fixed to the turbine casing. The rotors have a propeller-like arrangement of blades at the outer edge. Steam acts upon these blades, producing rotary motion. The stator consists of a similar, but fixed, series of blades that serve to redirect the steam flow onto the next rotor stage. A steam turbine often exhausts into a surface condenser that provides a vacuum. The stages of a steam turbine are typically arranged to extract the maximum potential work from a specific velocity and pressure of steam, giving rise to a series of variably sized high- and low-pressure stages. Turbines are only efficient if they rotate at relatively high speed, therefore they are usually connected to reduction gearing to drive lower speed applications, such as a ship's propeller. In the vast majority of large electric generating stations, turbines are directly connected to generators with no reduction gearing. Typical speeds are 3600 revolutions per minute (RPM) in the USA with 60 Hertz power, 3000 RPM in Europe and other countries with 50 Hertz electric power systems. In nuclear power applications the turbines typically run at half these speeds, 1800 RPM and 1500 RPM. A turbine rotor is also only capable of providing power when rotating in one direction. Therefore a reversing stage or gearbox is usually required where power is required in the opposite direction.
Steam turbines provide direct rotational force and therefore do not require a linkage mechanism to convert reciprocating to rotary motion. Thus, they produce smoother rotational forces on the output shaft. This contributes to a lower maintenance requirement and less wear on the machinery they power than a comparable reciprocating engine.
The main use for steam turbines is in electricity generation (in the 1990s about 90% of the world's electric production was by use of steam turbines) however the recent widespread application of large gas turbine units and typical combined cycle power plants has resulted in reduction of this percentage to the 80% regime for steam turbines. In electricity production, the high speed of turbine rotation matches well with the speed of modern electric generators, which are typically direct connected to their driving turbines. In marine service, (pioneered on the "Turbinia"), steam turbines with reduction gearing (although the Turbinia has direct turbines to propellers with no reduction gearbox) dominated large ship propulsion throughout the late 20th century, being more efficient (and requiring far less maintenance) than reciprocating steam engines. In recent decades, reciprocating Diesel engines, and gas turbines, have almost entirely supplanted steam propulsion for marine applications.
Virtually all nuclear power plants generate electricity by heating water to provide steam that drives a turbine connected to an electrical generator. Nuclear-powered ships and submarines either use a steam turbine directly for main propulsion, with generators providing auxiliary power, or else employ turbo-electric transmission, where the steam drives a turbo generator set with propulsion provided by electric motors. A limited number of steam turbine railroad locomotives were manufactured. Some non-condensing direct-drive locomotives did meet with some success for long haul freight operations in Sweden and for express passenger work in Britain, but were not repeated. Elsewhere, notably in the U.S.A., more advanced designs with electric transmission were built experimentally, but not reproduced. It was found that steam turbines were not ideally suited to the railroad environment and these locomotives failed to oust the classic reciprocating steam unit in the way that modern diesel and electric traction has done.
Oscillating cylinder steam engines.
An oscillating cylinder steam engine is a variant of the simple expansion steam engine which does not require valves to direct steam into and out of the cylinder. Instead of valves, the entire cylinder rocks, or oscillates, such that one or more holes in the cylinder line up with holes in a fixed port face or in the pivot mounting (trunnion). These engines are mainly used in toys and models, because of their simplicity, but have also been used in full size working engines, mainly on ships where their compactness is valued.
Rotary steam engines.
It is possible to use a mechanism based on a pistonless rotary engine such as the Wankel engine in place of the cylinders and valve gear of a conventional reciprocating steam engine. Many such engines have been designed, from the time of James Watt to the present day, but relatively few were actually built and even fewer went into quantity production; see link at bottom of article for more details. The major problem is the difficulty of sealing the rotors to make them steam-tight in the face of wear and thermal expansion; the resulting leakage made them very inefficient. Lack of expansive working, or any means of control of the cutoff is also a serious problem with many such designs.
By the 1840s, it was clear that the concept had inherent problems and rotary engines were treated with some derision in the technical press. However, the arrival of electricity on the scene, and the obvious advantages of driving a dynamo directly from a high-speed engine, led to something of a revival in interest in the 1880s and 1890s, and a few designs had some limited success.
Of the few designs that were manufactured in quantity, those of the Hult Brothers Rotary Steam Engine Company of Stockholm, Sweden, and the spherical engine of Beauchamp Tower are notable. Tower's engines were used by the Great Eastern Railway to drive lighting dynamos on their locomotives, and by the Admiralty for driving dynamos on board the ships of the Royal Navy. They were eventually replaced in these niche applications by steam turbines.
Rocket type.
The aeolipile represents the use of steam by the rocket-reaction principle, although not for direct propulsion.
In more modern times there has been limited use of steam for rocketry – particularly for rocket cars. Steam rocketry works by filling a pressure vessel with hot water at high pressure and opening a valve leading to a suitable nozzle. The drop in pressure immediately boils some of the water and the steam leaves through a nozzle, creating a propulsive force.
Safety.
Steam engines possess boilers and other components that are pressure vessels that contain a great deal of potential energy. Steam escapes and boiler explosions (typically BLEVEs) can and have in the past caused great loss of life. While variations in standards may exist in different countries, stringent legal, testing, training, care with manufacture, operation and certification is applied to ensure safety. See: Pressure vessel
Failure modes may include:
Steam engines frequently possess two independent mechanisms for ensuring that the pressure in the boiler does not go too high; one may be adjusted by the user, the second is typically designed as an ultimate fail-safe. Such safety valves traditionally used a simple lever to restrain a plug valve in the top of a boiler. One end of the lever carried a weight or spring that restrained the valve against steam pressure. Early valves could be adjusted by engine drivers, leading to many accidents when a driver fastened the valve down to allow greater steam pressure and more power from the engine. The more recent type of safety valve uses an adjustable spring-loaded valve, which is locked such that operators may not tamper with its adjustment unless a seal illegally is broken. This arrangement is considerably safer. 
Lead fusible plugs may be present in the crown of the boiler's firebox. If the water level drops, such that the temperature of the firebox crown increases significantly, the lead melts and the steam escapes, warning the operators, who may then manually suppress the fire. Except in the smallest of boilers the steam escape has little effect on dampening the fire. The plugs are also too small in area to lower steam pressure significantly, depressurizing the boiler. If they were any larger, the volume of escaping steam would itself endanger the crew.
Steam cycle.
The Rankine cycle is the fundamental thermodynamic underpinning of the steam engine. The cycle is an arrangement of components as is typically used for simple power production, and utilizes the phase change of water (boiling water producing steam, condensing exhaust steam, producing liquid water)) to provide a practical heat/power conversion system. The heat is supplied externally to a closed loop with some of the heat added being converted to work and the waste heat being removed in a condenser. The Rankine cycle is used in virtually all steam power production applications. In the 1990s, Rankine steam cycles generated about 90% of all electric power used throughout the world, including virtually all solar, biomass, coal and nuclear power plants. It is named after William John Macquorn Rankine, a Scottish polymath.
The Rankine cycle is sometimes referred to as a practical Carnot cycle because, when an efficient turbine is used, the TS diagram begins to resemble the Carnot cycle. The main difference is that heat addition (in the boiler) and rejection (in the condenser) are isobaric (constant pressure) processes in the Rankine cycle and isothermal (constant temperature) processes in the theoretical Carnot cycle. In this cycle a pump is used to pressurize the working fluid which is received from the condenser as a liquid not as a gas. Pumping the working fluid in liquid form during the cycle requires a small fraction of the energy to transport it compared to the energy needed to compress the working fluid in gaseous form in a compressor (as in the Carnot cycle). The cycle of a reciprocating steam engine differs from that of turbines because of condensation and re-evaporation occurring in the cylinder or in the steam inlet passages.
The working fluid in a Rankine cycle can operate as a closed loop system, where the working fluid is recycled continuously, or may be an "open loop" system, where the exhaust steam is directly released to the atmosphere, and a separate source of water feeding the boiler is supplied. Normally water is the fluid of choice due to its favourable properties, such as non-toxic and unreactive chemistry, abundance, low cost, and its thermodynamic properties. Mercury is the working fluid in the mercury vapor turbine. Low boiling hydrocarbons can be used in a binary cycle.
The steam engine contributed much to the development of thermodynamic theory; however, the only applications of scientific theory that influenced the steam engine were the original concepts of harnessing the power of steam and atmospheric pressure and knowledge of properties of heat and steam. The experimental measurements made by Watt on a model steam engine led to the development of the separate condenser. Watt independently discovered latent heat, which was confirmed by the original discoverer Joseph Black, who also advised Watt on experimental procedures. Watt was also aware of the change in the boiling point of water with pressure. Otherwise, the improvements to the engine itself were more mechanical in nature. The thermodynamic concepts of the Rankine cycle did give engineers the understanding needed to calculate efficiency which aided the development of modern high-pressure and -temperature boilers and the steam turbine.
Efficiency.
The efficiency of an engine can be calculated by dividing the energy output of mechanical work that the engine produces by the energy input to the engine by the burning fuel.
The historical measure of a steam engine's energy efficiency was its "duty". The concept of duty was first introduced by Watt in order to illustrate how much more efficient his engines were over the earlier Newcomen designs. Duty is the number of foot-pounds of work delivered by burning one bushel (94 pounds) of coal. The best examples of Newcomen designs had a duty of about 7 million, but most were closer to 5 million. Watt's original low-pressure designs were able to deliver duty as high as 25 million, but averaged about 17. This was a three-fold improvement over the average Newcomen design. Early Watt engines equipped with high-pressure steam improved this to 65 million.
No heat engine can be more efficient than the Carnot cycle, in which heat is moved from a high temperature reservoir to one at a low temperature, and the efficiency depends on the temperature difference. For the greatest efficiency, steam engines should be operated at the highest steam temperature possible (superheated steam), and release the waste heat at the lowest temperature possible.
The efficiency of a Rankine cycle is usually limited by the working fluid. Without the pressure reaching supercritical levels for the working fluid, the temperature range the cycle can operate over is quite small; in steam turbines, turbine entry temperatures are typically 565 °C (the creep limit of stainless steel) and condenser temperatures are around 30 °C. This gives a theoretical Carnot efficiency of about 63% compared with an actual efficiency of 42% for a modern coal-fired power station. This low turbine entry temperature (compared with a gas turbine) is why the Rankine cycle is often used as a bottoming cycle in combined-cycle gas turbine power stations.
One of the principal advantages the Rankine cycle holds over others is that during the compression stage relatively little work is required to drive the pump, the working fluid being in its liquid phase at this point. By condensing the fluid, the work required by the pump consumes only 1% to 3% of the turbine power and contributes to a much higher efficiency for a real cycle. The benefit of this is lost somewhat due to the lower heat addition temperature. Gas turbines, for instance, have turbine entry temperatures approaching 1500 °C. Nonetheless, the efficiencies of actual large steam cycles and large modern gas turbines are fairly well matched.
In practice, a steam engine exhausting the steam to atmosphere will typically have an efficiency (including the boiler) in the range of 1-10%, but with the addition of a condenser and multiple expansion, and high steam pressure/temperature, it may be greatly improved, historically into the regime of 10-20%, and very rarely slightly higher.
A modern large electrical power station (producing several hundred megawatts of electrical output) with steam reheat, economizer etc. will achieve efficiency in the mid 40% range, with the most efficient units approaching 50% thermal efficiency.
It is also possible to capture the waste heat using cogeneration in which the waste heat is used for heating a lower boiling point working fluid or as a heat source for district heating via saturated low-pressure steam.

</doc>
<doc id="27712" url="http://en.wikipedia.org/wiki?curid=27712" title="Sugar">
Sugar

Sugar is the generalized name for sweet, short-chain, soluble carbohydrates, many of which are used in food. They are carbohydrates, composed of carbon, hydrogen, and oxygen. There are various types of sugar derived from different sources. Simple sugars are called monosaccharides and include glucose (also known as dextrose), fructose and galactose. The table or granulated sugar most customarily used as food is sucrose, a disaccharide. (In the body, sucrose hydrolyses into fructose and glucose.) Other disaccharides include maltose and lactose. Longer chains of sugars are called oligosaccharides. Chemically-different substances may also have a sweet taste, but are not classified as sugars. Some are used as lower-calorie food substitutes for sugar described as artificial sweeteners.
Sugars are found in the tissues of most plants, but are present in sufficient concentrations for efficient extraction only in sugarcane and sugar beet. Sugarcane refers to any of several species of giant grass in the genus "Saccharum" that have been cultivated in tropical climates in South Asia and Southeast Asia since ancient times. A great expansion in its production took place in the 18th century with the establishment of sugar plantations in the West Indies and Americas. This was the first time that sugar became available to the common people, who had previously had to rely on honey to sweeten foods. Sugar beet, a cultivated variety of "Beta vulgaris", is grown as a root crop in cooler climates and became a major source of sugar in the 19th century when methods for extracting the sugar became available. Sugar production and trade have changed the course of human history in many ways, influencing the formation of colonies, the perpetuation of slavery, the transition to indentured labour, the migration of peoples, wars between sugar-trade–controlling nations in the 19th century, and the ethnic composition and political structure of the new world.
The world produced about 168 million tonnes of sugar in 2011. The average person consumes about 24 kg of sugar each year (33.1 kg in industrialised countries), equivalent to over 260 food calories per person, per day.
Since the latter part of the twentieth century, it has been questioned whether a diet high in sugars, especially refined sugars, is good for human health. Sugar has been linked to obesity, and suspected of, or fully implicated as a cause in the occurrence of diabetes, cardiovascular disease, dementia, macular degeneration, and tooth decay. Numerous studies have been undertaken to try to clarify the position, but with varying results, mainly because of the difficulty of finding populations for use as controls that do not consume or are largely free of any sugar consumption.
Etymology.
The etymology reflects the spread of the commodity. The English word "sugar" originates from the Arabic word سكر "sukkar", which came from the Persian شکر "shekar",
itself derived from Sanskrit "शर्करा" "śarkarā", which originated from Tamil "சக்கரை" "Sakkarai". It most probably came to England by way of Italian merchants. The contemporary Italian word is "zucchero", whereas the Spanish and Portuguese words, "azúcar" and "açúcar" respectively, have kept a trace of the Arabic definite article. The Old French word is "zuchre" – contemporary French "sucre". The earliest Greek word attested is σάκχαρις ("sákkʰaris"). A satisfactory "pedigree" explaining the spread of the word has yet to be done. The English word "jaggery", a coarse brown sugar made from date palm sap or sugar cane juice, has a similar etymological origin; Portuguese "xagara" or "jagara", derived from Malayalam "chakkarā" from the Sanskrit "śarkarā".
History.
Ancient times and Middle Ages.
Sugar has been produced in the Indian subcontinent since ancient times. It was not plentiful or cheap in early times and honey was more often used for sweetening in most parts of the world. Originally, people chewed raw sugarcane to extract its sweetness. Sugarcane was a native of tropical South Asia and Southeast Asia. Different species seem to have originated from different locations with "Saccharum barberi" originating in India and "S. edule" and "S. officinarum" coming from New Guinea. One of the earliest historical references to sugarcane is in Chinese manuscripts dating back to 8th century BC that state that the use of sugarcane originated in India.
Sugar remained relatively unimportant until the Indians discovered methods of turning sugarcane juice into granulated crystals that were easier to store and to transport. Crystallized sugar was discovered by the time of the Imperial Guptas, around the 5th century AD. In the local Indian language, these crystals were called "khanda" (Devanagari:खण्ड,Khaṇḍa), which is the source of the word "candy".
Indian sailors, who carried clarified butter and sugar as supplies, introduced knowledge of sugar on the various trade routes they travelled. Buddhist monks, as they travelled around, brought sugar crystallization methods to China. During the reign of Harsha (r. 606–647) in North India, Indian envoys in Tang China taught methods of cultivating sugarcane after Emperor Taizong of Tang (r. 626–649) made known his interest in sugar. China then established its first sugarcane plantations in the seventh century. Chinese documents confirm at least two missions to India, initiated in 647 AD, to obtain technology for sugar refining. In South Asia, the Middle East and China, sugar became a staple of cooking and desserts.
The triumphant progress of Alexander the Great was halted on the banks of the Indus River by the refusal of his troops to go further east. They saw people in the Indian subcontinent growing sugarcane and making "granulated, salt-like sweet powder", locally called "Sharkara" (Devanagari:शर्करा,Śarkarā), pronounced as saccharum (ζάκχαρι). On their return journey, the Macedonian soldiers carried the "honey-bearing reeds" home with them. Sugarcane remained a little-known crop in Europe for over a millennium, sugar a rare commodity, and traders in sugar wealthy.
Crusaders brought sugar home with them to Europe after their campaigns in the Holy Land, where they encountered caravans carrying "sweet salt". Early in the 12th century, Venice acquired some villages near Tyre and set up estates to produce sugar for export to Europe, where it supplemented honey, which had previously been the only available sweetener.
Crusade chronicler William of Tyre, writing in the late 12th century, described sugar as "very necessary for the use and health of mankind". In the 15th century, Venice was the chief sugar refining and distribution centre in Europe.
Modern history.
In August 1492, Christopher Columbus stopped at La Gomera in the Canary Islands, for wine and water, intending to stay only four days. He became romantically involved with the governor of the island, Beatriz de Bobadilla y Ossorio, and stayed a month. When he finally sailed, she gave him cuttings of sugarcane, which became the first to reach the New World.
The Portuguese took sugar to Brazil. By 1540, there were 800 cane sugar mills in Santa Catarina Island and there were another 2,000 on the north coast of Brazil, Demarara, and Surinam. The first sugar harvest happened in Hispaniola in 1501; and, many sugar mills had been constructed in Cuba and Jamaica by the 1520s.
Sugar was a luxury in Europe prior to the 18th century, when it became more widely available. It then became popular and by the 19th century, sugar came to be considered a necessity. This evolution of taste and demand for sugar as an essential food ingredient unleashed major economic and social changes. It drove, in part, colonization of tropical islands and nations where labor-intensive sugarcane plantations and sugar manufacturing could thrive. The demand for cheap labor to perform the hard work involved in its cultivation and processing increased the demand for the slave trade from Africa (in particular West Africa). After slavery was abolished, there was high demand for indentured laborers from South Asia (in particular India). Millions of slave and indentured laborers were brought into the Caribbean and the Americas, Indian Ocean colonies, southeast Asia, Pacific Islands, and East Africa and Natal. The modern ethnic mix of many nations that have been settled in the last two centuries has been influenced by the demand for sugar.
Sugar also led to some industrialization of former colonies. For example, Lieutenant J. Paterson, of the Bengal establishment, persuaded the British Government that sugar cane could be cultivated in British India with many advantages and at less expense than in the West Indies. As a result, sugar factories were established in Bihar in eastern India.
During the Napoleonic Wars, sugar beet production increased in continental Europe because of the difficulty of importing sugar when shipping was subject to blockade. By 1880, the sugar beet was the main source of sugar in Europe. It was cultivated in Lincolnshire and other parts of England, although the United Kingdom continued to import the main part of its sugar from its colonies.
Until the late nineteenth century, sugar was purchased in loaves, which had to be cut using implements called Sugar nips. In later years, granulated sugar was more usually sold in bags.
Sugar cubes were produced in the nineteenth century. The first inventor of a process to make sugar in cube form was Moravian Jakub Kryštof Rad, director of a sugar company in Dačice. He began sugar cube production after being granted a five-year patent for the invention on January 23, 1843. Henry Tate of Tate & Lyle was another early manufacturer of sugar cubes at his refineries in Liverpool and London. Tate purchased a patent for sugar cube manufacture from German Eugen Langen, who in 1872 had invented a different method of processing of sugar cubes.
Chemistry.
Scientifically, "sugar" loosely refers to a number of carbohydrates, such as monosaccharides, disaccharides, or oligosaccharides. Monosaccharides are also called "simple sugars," the most important being glucose. Almost all sugars have the formula CnH2nOn (n is between 3 and 7). Glucose has the molecular formula C6H12O6. The names of typical sugars end with "ose", as in "glucose", "dextrose", and "fructose". Sometimes such words may also refer to any types of carbohydrates soluble in water. The acyclic mono- and disaccharides contain either aldehyde groups or ketone groups. These carbon-oxygen double bonds (C=O) are the reactive centers. All saccharides with more than one ring in their structure result from two or more monosaccharides joined by glycosidic bonds with the resultant loss of a molecule of water (H2O) per bond.
Monosaccharides in a closed-chain form can form glycosidic bonds with other monosaccharides, creating disaccharides (such as sucrose) and polysaccharides (such as starch). Enzymes must hydrolyze or otherwise break these glycosidic bonds before such compounds become metabolized. After digestion and absorption the principal monosaccharides present in the blood and internal tissues include glucose, fructose, and galactose. Many pentoses and hexoses can form ring structures. In these closed-chain forms, the aldehyde or ketone group remains non-free, so many of the reactions typical of these groups cannot occur. Glucose in solution exists mostly in the ring form at equilibrium, with less than 0.1% of the molecules in the open-chain form.
Natural polymers of sugars.
Biopolymers of sugars are common in nature. Through photosynthesis, plants produce glyceraldehyde-3-phosphate (G3P), a phosphated 3-carbon sugar that is used by the cell to make monosaccharides such as glucose (C6H12O6) or (as in cane and beet) sucrose (C12H22O11). Monosaccharides may be further converted into structural polysaccharides such as cellulose and pectinfor cell wall construction or into energy reserves in the form of storage polysaccharides such as starch or inulin. Starch, consisting of two different polymers of glucose, is a readily degradable form of chemical energy stored by cells, and can be converted to other types of energy. Another polymer of glucose is cellulose, which is a linear chain composed of several hundred or thousand glucose units. It is used by plants as a structural component in their cell walls. Humans can digest cellulose only to a very limited extent, though ruminants can do so with the help of symbiotic bacteria in their gut. DNA and RNA are built up of the monosaccharides deoxyribose and ribose, respectively. Deoxyribose has the formula C5H10O4 and ribose the formula C5H10O5.
Flammability.
Sugars are organic substances that burn easily upon exposure to an open flame. Because of this, the handling of sugars presents a risk for dust explosion. The 2008 Georgia sugar refinery explosion, which resulted in 14 deaths, 40 injured, and more than half of the facility's destruction, was caused by the ignition of sugar dust.
Types of sugar.
Monosaccharides.
Glucose, fructose and galactose are all simple sugars, monosaccharides, with the general formula C6H12O6. They have five hydroxyl groups (−OH) and a carbonyl group (C=O) and are cyclic when dissolved in water. They each exist as several isomers with dextro- and laevo-rotatory forms that cause polarized light to diverge to the right or the left.
Glucose, dextrose or grape sugar occurs naturally in fruits and plant juices and is the primary product of photosynthesis. Most ingested carbohydrates are converted into glucose during digestion and it is the form of sugar that is transported around the bodies of animals in the bloodstream. It can be manufactured from starch by the addition of enzymes or in the presence of acids. Glucose syrup is a liquid form of glucose that is widely used in the manufacture of foodstuffs. It can be manufactured from starch by enzymatic hydrolysis.
Fructose or fruit sugar occurs naturally in fruits, some root vegetables, cane sugar and honey and is the sweetest of the sugars. It is one of the components of sucrose or table sugar. It is used as a high-fructose syrup, which is manufactured from hydrolyzed corn starch that has been processed to yield corn syrup, with enzymes then added to convert part of the glucose into fructose.
In general, galactose does not occur in the free state but is a constituent with glucose of the disaccharide lactose or milk sugar. It is less sweet than glucose. It is a component of the antigens found on the surface of red blood cells that determine blood groups.
Disaccharides.
Sucrose, maltose, and lactose are all compound sugars, disaccharides, with the general formula C12H22O11. They are formed by the combination of two monosaccharide molecules with the exclusion of a molecule of water.
Sucrose is found in the stems of sugar cane and roots of sugar beet. It also occurs naturally alongside fructose and glucose in other plants, in particular fruits and some roots such as carrots. The different proportions of sugars found in these foods determines the range of sweetness experienced when eating them. A molecule of sucrose is formed by the combination of a molecule of glucose with a molecule of fructose. After being eaten, sucrose is split into its constituent parts during digestion by a number of enzymes known as sucrases.
Maltose is formed during the germination of certain grains, the most notable being barley, which is converted into malt, the source of the sugar's name. A molecule of maltose is formed by the combination of two molecules of glucose. It is less sweet than glucose, fructose or sucrose. It is formed in the body during the digestion of starch by the enzyme amylase and is itself broken down during digestion by the enzyme maltase.
Lactose is the naturally occurring sugar found in milk. A molecule of lactose is formed by the combination of a molecule of galactose with a molecule of glucose. It is broken down when consumed into its constituent parts by the enzyme lactase during digestion. Children have this enzyme but some adults no longer form it and they are unable to digest lactose.
Sources.
The sugar contents of common fruits and vegetables are presented in Table 1.
All data with a unit of g (gram) are based on 100 g of a food item.
The fructose/glucose ratio is calculated by dividing the sum of free fructose plus half sucrose by the sum of free glucose plus half sucrose.
Production.
Sugar beet.
Sugar beet ("Beta vulgaris") is a biennial plant in the Family Amaranthaceae, the tuberous root of which contains a high proportion of sucrose. It is cultivated in temperate regions with adequate rainfall and requires a fertile soil. The crop is harvested mechanically in the autumn and the crown of leaves and excess soil removed. The roots do not deteriorate rapidly and may be left in a clamp in the field for some weeks before being transported to the processing plant. Here the crop is washed and sliced and the sugar extracted by diffusion. Milk of lime is added to the raw juice and carbonatated in a number of stages in order to purify it. Water is evaporated by boiling the syrup under a vacuum. The syrup is then cooled and seeded with sugar crystals. The white sugar that crystallizes out can be separated in a centrifuge and dried. It requires no further refining.
Sugarcane.
Sugarcane ("Saccharum spp.") is a perennial grass in the family Poaceae. It is cultivated in tropical and sub-tropical regions for the sucrose that is found in its stems. It requires a frost-free climate with sufficient rainfall during the growing season to make full use of the plant's great growth potential. The crop is harvested mechanically or by hand, chopped into lengths and conveyed rapidly to the processing plant. Here, it is either milled and the juice extracted with water or extracted by diffusion. The juice is then clarified with lime and heated to kill enzymes. The resulting thin syrup is concentrated in a series of evaporators, after which further water is removed by evaporation in vacuum containers. The resulting supersaturated solution is seeded with sugar crystals and the sugar crystallizes out and is separated from the fluid and dried. Molasses is a by-product of the process and the fiber from the stems, known as bagasse, is burned to provide energy for the sugar extraction process. The crystals of raw sugar have a sticky brown coating and either can be used as they are or can be bleached by sulfur dioxide or can be treated in a carbonatation process to produce a whiter product.
Refining.
Refined sugar is made from raw sugar that has undergone a refining process to remove the molasses. Raw sugar is a sucrose which is synthesized from sugar cane or sugar beet and cannot immediately be consumed before going through the refining process to produce refined sugar or white sugar.
The sugar may be transported in bulk to the country where it will be used and the refining process often takes place there. The first stage is known as affination and involves immersing the sugar crystals in a concentrated syrup that softens and removes the sticky brown coating without dissolving them. The crystals are then separated from the liquor and dissolved in water. The resulting syrup is treated either by a carbonatation or by a phosphatation process. Both involve the precipitation of a fine solid in the syrup and when this is filtered out, many of the impurities are removed at the same time. Removal of colour is achieved by using either a granular activated carbon or an ion-exchange resin. The sugar syrup is concentrated by boiling and then cooled and seeded with sugar crystals, causing the sugar to crystallize out. The liquor is spun off in a centrifuge and the white crystals are dried in hot air and ready to be packaged or used. The surplus liquor is made into refiners' molasses.
The International Commission for Uniform Methods of Sugar Analysis sets standards for the measurement of the purity of refined sugar, known as ICUMSA numbers; lower numbers indicate a higher level of purity in the refined sugar.
Refined sugar is widely used for industrial needs for higher quality. Refined sugar is purer (ICUMSA below 300) than raw sugar (ICUMSA over 1,500). The level of purity associated with the colors of sugar, expressed by standard number ICUMSA (International Commission for Uniform Methods of sugar Analysis), the smaller ICUMSA numbers indicate that higher purity of sugar.
Producing countries.
The five largest producers of sugar in 2011 were Brazil, India, the European Union, China and Thailand. In the same year, the largest exporter of sugar was Brazil, distantly followed by Thailand, Australia and India. The largest importers were the European Union, United States and Indonesia. At present, Brazil has the highest per capita consumption of sugar, followed by Australia, Thailand, and the European Union.
Forms and uses.
Granulated sugars are used at the table to sprinkle on foods and to sweeten hot drinks and in home baking to add sweetness and texture to cooked products. They are also used as a preservative to prevent micro-organisms from growing and perishable food from spoiling as in jams, marmalades, and candied fruits.
Milled sugars (known as powdered sugar and confectioner's sugar) are ground to a fine powder. They are used as icing sugar, for dusting foods and in baking and confectionery.
Screened sugars are crystalline products separated according to the size of the grains. They are used for decorative table sugars, for blending in dry mixes and in baking and confectionery.
Brown sugars are granulated sugars with the grains coated in molasses to produce a light, dark, or demerara sugar. They are used in baked goods, confectionery, and toffees.
Sugar cubes (sometimes called sugar lumps) are white or brown granulated sugars lightly steamed and pressed together in block shape. They are used to sweeten drinks.
Liquid sugars are strong syrups consisting of 67% granulated sugar dissolved in water. They are used in the food processing of a wide range of products including beverages, ice cream, jams, and hard candy.
Invert sugars and syrups are blended to manufacturers specifications and are used in breads, cakes, and beverages for adjusting sweetness, aiding moisture retention and avoiding crystallization of sugars.
Syrups and treacles are dissolved invert sugars heated to develop the characteristic flavors. Treacles have added molasses. They are used in a range of baked goods and confectionery including toffees and licorice.
Low-calorie sugars and sweeteners are often made of maltodextrin with added sweeteners. Maltodextrin is an easily digestible synthetic polysaccharide consisting of short chains of glucose molecules and is made by the partial hydrolysis of starch. The added sweeteners are often aspartame, saccharin, stevia, or sucralose.
Polyols are sugar alcohols and are used in chewing gums where a sweet flavor is required that lasts for a prolonged time in the mouth.
In winemaking, fruit sugars are converted into alcohol by a fermentation process. If the must formed by pressing the fruit has a low sugar content, additional sugar may be added to raise the alcohol content of the wine in a process called chaptalization. In the production of sweet wines, fermentation may be halted before it has run its full course, leaving behind some residual sugar that gives the wine its sweet taste.
Molasses is commonly used to make rum, and sugar byproducts are used to make ethanol for fuel.
Consumption.
In most parts of the world, sugar is an important part of the human diet, making food more palatable and providing food energy. After cereals and vegetable oils, sugar derived from sugar cane and beet provided more kilocalories per capita per day on average than other food groups. According to the FAO, an average of 24 kg of sugar, equivalent to over 260 food calories per day, was consumed annually per person of all ages in the world in 1999. Even with rising human populations, sugar consumption is expected to increase to 25.1 kg per person per year by 2015.
Data collected in multiple nationwide surveys between 1999 and 2008 show that the intake of added sugars has declined by 24 percent with declines occurring in all age, ethnic and income groups.
The per capita consumption of refined sugar in the United States has varied between 27 and in the last 40 years. In 2008, American per capita total consumption of sugar and sweeteners, exclusive of artificial sweeteners, equalled 61.9 kg per year. This consisted of 29.65 kg pounds of refined sugar and 31 kg pounds of corn-derived sweeteners per person.
Health effects.
Some studies involving the health impact of sugars are effectively inconclusive. The WHO and FAO meta studies have shown directly contrasting impacts of sugar in refined and unrefined forms and since most studies do not use a population that do not consume any "free sugars" at all, the baseline is effectively flawed. Hence, there are articles such as "Consumer Reports on Health" that stated in 2008, "Some of the supposed dietary dangers of sugar have been overblown. Many studies have debunked the idea that it causes hyperactivity, for example".
Blood glucose levels.
It used to be believed that sugar raised blood glucose levels more quickly than did starch because of its simpler chemical structure. However, it turned out that white bread or French fries have the same effect on blood sugar as pure glucose, while fructose, although a simple carbohydrate, has a minimal effect on blood sugar. As a result, as far as blood sugar is concerned, carbohydrates are classified according to their glycemic index, a system for measuring how quickly a food that is eaten raises blood sugar levels, and glycemic load, which takes into account both the glycemic index and the amount of carbohydrate in the food. This has led to carbohydrate counting, a method used by diabetics for planning their meals.
Obesity and diabetes.
Controlled trials have now shown unequivocally that consumption of sugar-sweetened beverages increases body weight and body fat, and that replacement of sugar by artificial sweeteners reduces weight.
Studies on the link between sugars and diabetes are inconclusive, with some suggesting that eating excessive amounts of sugar does not increase the risk of diabetes, although the extra calories from consuming large amounts of sugar can lead to obesity, which may itself increase the risk of developing this metabolic disease. 
Other studies show correlation between refined sugar (free sugar) consumption and the onset of diabetes, and negative correlation with the consumption of fiber. These included a 2010 meta-analysis of eleven studies involving 310,819 participants and 15,043 cases of type 2 diabetes. 
This found that "SSBs (sugar-sweetened beverages) may increase the risk of metabolic syndrome and type 2 diabetes not only through obesity but also by increasing dietary glycemic load, leading to insulin resistance, β-cell dysfunction, and inflammation". As an overview to consumption related to chronic disease and obesity, the World Health Organization's independent meta-studies specifically distinguish free sugars ("all monosaccharides and disaccharides added to foods by the manufacturer, cook or consumer, plus sugars naturally present in honey, syrups and fruit juices") from sugars occurring naturally in food. The reports prior to 2000 set the limits for free sugars at a maximum of 10% of carbohydrate intake, measured by energy, rather than mass, and since 2002 have aimed for a level across the entire population of less than 10%. The consultation committee recognized that this goal is "controversial. However, the Consultation considered that the studies showing no effect of free sugars on excess weight have limitations".
Cardiovascular disease.
Studies in animals have suggested that chronic consumption of refined sugars can contribute to metabolic and cardiovascular dysfunction. Some experts have suggested that refined fructose is more damaging than refined glucose in terms of cardiovascular risk. Cardiac performance has been shown to be impaired by switching from a carbohydrate diet including fiber to a high-carbohydrate diet.
Switching from saturated fatty acids to carbohydrates with high glycemic index values shows a statistically-significant increase in the risk of myocardial infarction. Other studies have shown that the risk of developing coronary heart disease is decreased by adopting a diet high in polyunsaturated fatty acids but low in sugar, whereas a low-fat, high-carbohydrate diet brings no reduction. This suggests that consuming a diet with a high glycemic load typical of the "junk food" diet is strongly associated with an increased risk of developing coronary heart disease.
The consumption of added sugars has been positively associated with multiple measures known to increase cardiovascular disease risk amongst adolescents as well as adults.
Studies are suggesting that the impact of refined carbohydrates or high glycemic load carbohydrates are more significant than the impact of saturated fatty acids on cardiovascular disease.
A high dietary intake of sugar (in this case, sucrose or disaccharide) can substantially increase the risk of heart and vascular diseases. According to a Swedish study of 4301 people undertaken by Lund University and Malmö University College, sugar was associated with higher levels of bad blood lipids, causing a high level of small and medium low-density lipoprotein (LDL) and reduced high-density lipoprotein (HDL). In contrast, the amount of fat eaten did not affect the level of blood fats. Incidentally quantities of alcohol and protein were linked to an increase in the good HDL blood fat.
Alzheimer's disease.
Claims have been made of a sugar–Alzheimer's disease connection, but debate continues over whether cognitive decline is attributable to dietary fructose or to overall energy intake.
Tooth decay.
In regard to contributions to tooth decay, the role of free sugars is also recommended to be below an absolute maximum of 10% of energy intake, with a minimum of zero. There is "convincing evidence from human intervention studies, epidemiological studies, animal studies and experimental studies, for an association between the amount and frequency of free sugars intake and dental caries" while other sugars (complex carbohydrate) consumption is normally associated with a lower rate of dental caries. Lower rates of tooth decay have been seen in individuals with hereditary fructose intolerance.
Also, studies have shown that the consumption of sugar and starch have different impacts on oral health with the ingestion of starchy foods and fresh fruit being associated with low levels of dental caries.
Addiction.
Sugar addiction is the term for the relationship between sugar and the various aspects of food addiction including "bingeing, withdrawal, craving and cross-sensitization". Some scientists assert that consumption of sweets or sugar could have a heroin addiction-like effect.
Hyperactivity.
There is a common notion that sugar leads to hyperactivity, in particular in children, but studies and meta-studies tend to disprove this. Some articles and studies do refer to the increasing evidence supporting the links between refined sugar and hyperactivity. The WHO FAO meta-study suggests that such inconclusive results are to be expected when some studies do not effectively segregate or control for free sugars as opposed to sugars still in their natural form (entirely unrefined) while others do. One study followed thirty-five 5-to-7-year-old boys who were reported by their mothers to be behaviorally "sugar-sensitive". They were randomly assigned to experimental and control groups. In the experimental group, mothers were told that their children were fed sugar, and, in the control group, mothers were told that their children received a placebo. In fact, all children received the placebo, but mothers in the sugar expectancy condition rated their children as significantly more hyperactive. This result suggests that the real effect of sugar is that it increases worrying among parents with preconceived notions.
Measurements.
Different culinary sugars have different densities due to differences in particle size and inclusion of moisture.
Domino Sugar gives the following weight to volume conversions (in United States customary units):
Another source gives different values for the bulk densities:

</doc>
<doc id="27811" url="http://en.wikipedia.org/wiki?curid=27811" title="Sleep and learning">
Sleep and learning

Many competing theories have been made to discover the possible connections between sleep and learning in humans. Research indicates that sleep does more than allow the brain to rest. It also aids in the consolidation of long-term memories. REM sleep and slow-wave sleep play different roles in memory consolidation. REM is associated with the consolidation of nondeclarative (implicit) memories. An example of a nondeclarative memory would be a task that we can do without consciously thinking about it, such as riding a bike. Slow-wave, or non-REM (NREM) sleep, is associated with the consolidation of declarative (explicit) memories. These are facts that need to be consciously remembered, such as dates for a history class.
Increased learning.
Popular sayings can reflect the notion that remolded memories produce new creative associations in the morning, and that performance often improves after a time-interval that includes sleep. Current studies demonstrate that a healthy sleep produces a significant learning-dependent performance boost. The idea is that sleep helps the brain to edit its memory, looking for important patterns and extracting overarching rules which could be described as 'the gist', and integrating this with existing memory. The 'synaptic scaling' hypothesis suggests that sleep plays an important role in regulating learning that has taken place while awake, enabling more efficient and effective storage in the brain, making better use of space and energy.
Healthy sleep must include the appropriate sequence and proportion of NREM and REM phases, which play different roles in the memory consolidation-optimization process. During a normal night of sleep, a person will alternate between periods of NREM and REM sleep. Each cycle is approximately 90 minutes long, containing a 20-30 minute bout of REM sleep. NREM sleep consists of sleep stages 1–4, and is where movement can be observed. A person can still move their body when they are in NREM sleep. If you are observing someone sleeping and you see them turn, toss, or roll over, this indicates that they are in NREM sleep. REM sleep is characterized by the lack muscle activity. Physiological studies have shown that aside from the occasional twitch, a person actually becomes paralyzed during REM sleep. In motor skill learning, an interval of sleep may be critical for the expression of performance gains; without sleep these gains will be delayed (Korman et al., 2003).
Procedural memories are a form of nondeclarative memory, so they would most benefit from slow-wave, or NREM sleep. In a study, procedural memories have been shown to benefit from sleep (Walker et al., 2002, as cited in Walker, 2009). Subjects were tested using a tapping task, where they used their fingers to tap a specific sequence of numbers on a keyboard, and their performances were measured by accuracy and speed. This finger-tapping task was used to simulate learning a motor skill. The first group was tested, retested 12 hours later while awake, and finally tested another 12 hours later with sleep in between. The other group was tested, retested 12 hours later with sleep in between, and then retested 12 hours later while awake. The results showed that in both groups, there was only a slight improvement after a 12 hour wake session, but a significant increase in performance after each group slept. This study gives evidence that NREM sleep is a significant factor in consolidating motor skill procedural memories, therefore sleep deprivation can impair performance on a motor learning task. This memory decrement results specifically from the loss of stage 2, NREM sleep.
Declarative memory has also been shown to benefit from sleep, but not in the same way as procedural memory. Declarative memories benefit from REM sleep. A study was conducted where the subjects learned word pairs, and the results showed that sleep not only prevents the decay of memory, but also actively fixates declarative memories (Payne et al., 2006). Two of the groups learned word pairs, then either slept or stayed awake, and were tested again. The other two groups did the same thing, except they also learned interference pairs right before being retested to try to disrupt the previously learned word pairs. The results showed that sleep was of "some" help in retaining the word pair associations, while against the interference pair, sleep helped "significantly".
After sleep, there is increased insight. This is because sleep helps people to reanalyze their memories. The same patterns of brain activity that occur during learning have been found to occur again during sleep, only faster. One way that sleep strengthens memories is by weeding out the less successful connections between neurons in the brain. This weeding out is essential to prevent overactivity. The brain compensates for strengthening some synapses (connections) between neurons, by weakening others. The weakening process occurs mostly during sleep. This weakening during sleep allows for strengthening of other connections while we are awake. Learning is the process of strengthening connections, therefore this process could be a major explanation for the benefits that sleep has on memory.
Research has shown that taking an afternoon nap increases learning capacity. A study (Mednick et al. 2009) tested two groups of subjects on a nondeclarative memory task. One group engaged in REM sleep, and one group did not (meaning that they engaged in NREM sleep). The investigators found that the subjects who engaged only in NREM sleep did not show much improvement. The subjects who engaged in REM sleep performed significantly better, indicating that REM sleep facilitated the consolidation of nondeclarative memories. More recently Holtz et al. (2012) demonstrated that a procedural task was learned and retained better if it was encountered immediately before going to sleep, while a declarative task was learned better in the afternoon 
Electrophysiological evidence in rats.
A 2009 study based on electrophysiological recordings of large ensembles of isolated cells in the prefrontal cortex of rats revealed that cell assemblies that formed upon learning were more preferentially active during subsequent sleep episodes. More specifically, those replay events were more prominent during slow wave sleep and were concomittant with hippocampal reactivation events. This study has shown that neuronal patterns in large brain networks are tagged during learning so that they are replayed, and supposedly consolidated, during subsequent sleep.
Sleep in relation to school.
Sleep has been directly linked to the grades of students. One in four U.S. high school students admit to falling asleep in class at least once a week. Consequently, results have shown that those who sleep less do poorly. In the United States sleep deprivation is common with students because almost all schools begin early in the morning and many of these students either choose to stay awake late into the night or cannot do otherwise due to delayed sleep phase syndrome. As a result, students that should be getting between 8.5 and 9.25 hours of sleep are getting only 7 hours. Perhaps because of this sleep deprivation, their grades lower and their concentration is impaired. As a result of studies showing the effects of sleep deprivation on grades, and the different sleep patterns for teenagers, a school in New Zealand, changed its start time to 10:30 a.m., in 2006, to allow students to keep to a schedule that allowed more sleep. In 2009, Monkseaton High School, in North Tyneside, had 800 pupils aged 13–19 starting lessons at 10 a.m. instead of the normal 9 a.m. and has reported that general absence has dropped by 8% and persistent absenteeism by 27%. Similarly, a high school in Copenhagen has committed to providing at least one class per year for students who will start at 10 a.m. or later.
College students represent one of the most sleep-deprived segments of our population. Only 11% of American college students sleep well, and 40% of students feel well rested only two days per week. About 73% have experienced at least some occasional sleep issues. This poor sleep is thought to have a severe impact on their ability to learn and remember information because the brain is being deprived of time that it needs to consolidate information which is essential to the learning process.
Other theories.
Other researchers' theories on additional functions of sleep differ significantly. One older idea is the "energy conservation" theory. Others claim that REM sleep is needed to "refresh" the brain after the NREM phase, or that REM is needed to prevent stasis of fluids in the eye (Roth Ari-Even et al., 2005).

</doc>
<doc id="27814" url="http://en.wikipedia.org/wiki?curid=27814" title="Sine (disambiguation)">
Sine (disambiguation)

Sine is a trigonometric function.
Sine may also refer to:

</doc>
<doc id="27843" url="http://en.wikipedia.org/wiki?curid=27843" title="September 30">
September 30

September 30 is the day of the year in the Gregorian calendar.

</doc>
<doc id="27871" url="http://en.wikipedia.org/wiki?curid=27871" title="Stephen III">
Stephen III

Stephen III may refer to:

</doc>
<doc id="27872" url="http://en.wikipedia.org/wiki?curid=27872" title="Stephen II">
Stephen II

Stephen II may refer to:

</doc>
<doc id="27875" url="http://en.wikipedia.org/wiki?curid=27875" title="Stephen Jay Gould">
Stephen Jay Gould

Stephen Jay Gould (; September 10, 1941 – May 20, 2002) was an American paleontologist, evolutionary biologist and historian of science. He was also one of the most influential and widely read writers of popular science of his generation.
Gould spent most of his career teaching at Harvard University and working at the American Museum of Natural History in New York. In the later years of his life, Gould also taught biology and evolution at New York University.
Gould's most significant contribution to evolutionary biology was the theory of punctuated equilibrium, which he developed with Niles Eldredge in 1972. The theory proposes that most evolution is marked by long periods of evolutionary stability, which is punctuated by rare instances of branching evolution. The theory was contrasted against phyletic gradualism, the popular idea that evolutionary change is marked by a pattern of smooth and continuous change in the fossil record.
Most of Gould's empirical research was based on the land snail genera "Poecilozonites" and "Cerion". He also contributed to evolutionary developmental biology, and has received wide praise for his book "Ontogeny and Phylogeny". In evolutionary theory he opposed strict selectionism, sociobiology as applied to humans, and evolutionary psychology. He campaigned against creationism and proposed that science and religion should be considered two distinct fields (or "magisteria") whose authorities do not overlap.
Gould was known by the general public mainly from his 300 popular essays in the magazine "Natural History", and his books written for a non-specialist audience.
In April 2000, the US Library of Congress named him a "Living Legend".
Biography.
Stephen Jay Gould was born and raised in the community of Bayside, a neighborhood of the northeastern section of Queens in New York City. His father, Leonard, was a court stenographer, and his mother, Eleanor, was an artist whose parents were Jewish immigrants living and working in the city's Garment District. When Gould was five years old, his father took him to the Hall of Dinosaurs in the American Museum of Natural History, where he first encountered "Tyrannosaurus rex". "I had no idea there were such things—I was awestruck," Gould once recalled. It was in that moment that he decided to become a paleontologist.
Raised in a secular Jewish home, Gould did not formally practice religion and preferred to be called an agnostic, though Jerry Coyne, an academic who had Gould on his theses committee, has called him a "die hard atheist". 
When asked whether he was an agnostic in an interview in Skeptic magazine, he responded
Though he "had been brought up by a Marxist father", he stated that his father's politics were "very different" from his own. In describing his own political views, he has said they "tend to the left of center". According to Gould the most influential political books he read were C. Wright Mills' "The Power Elite" and the political writings of Noam Chomsky.
While attending Antioch College in the early 1960s, Gould was active in the civil rights movement and often campaigned for social justice. When he attended the University of Leeds as a visiting undergraduate, he organized weekly demonstrations outside a Bradford dance hall which refused to admit Blacks. Gould continued these demonstrations until the policy was revoked. Throughout his career and writings, he spoke out against cultural oppression in all its forms, especially what he saw as the pseudoscience used in the service of racism and sexism.
Interspersed throughout his scientific essays for "Natural History" magazine, Gould frequently referred to his nonscientific interests and pastimes. As a boy he collected baseball cards and remained a New York Yankees fan throughout his life. As an adult he was fond of science fiction movies, but often lamented the poor quality of their presentation of science and of their storytelling. His other interests included singing in the Boston Cecilia, and he was a great aficionado of Gilbert and Sullivan operas. He collected rare antiquarian books and textbooks. He often traveled to Europe, and spoke French, German, Russian, and Italian. He admired Renaissance architecture. When discussing the Judeo-Christian tradition, he usually referred to it simply as "Moses". He sometimes alluded ruefully to his tendency to put on weight.
Marriage and family.
Gould was married twice. His first marriage was to artist Deborah Lee on October 3, 1965. Gould met Lee while they were students together at Antioch College. They had two sons, Jesse and Ethan. His second marriage, in 1995, was to artist and sculptor Rhonda Roland Shearer, who is the mother of two children, Jade and London Allen, stepchildren of Gould.
First bout of cancer.
In July 1982, Gould was diagnosed with peritoneal mesothelioma, a deadly form of cancer affecting the abdominal lining and frequently found in people who have been exposed to asbestos or rock dust. After a difficult two-year recovery, Gould published a column for "Discover magazine", entitled, "The Median Isn't the Message", which discusses his reaction to discovering that mesothelioma patients had a median lifespan of only eight months after diagnosis. He then describes the true significance behind this number, and his relief upon realizing that statistical averages are just useful abstractions, and do not encompass the full range of variation.
The median is the halfway point, which means that 50% of patients will die before eight months, but the other half will live longer, potentially much longer. He then needed to determine where his personal characteristics placed him within this range. Given that the cancer was detected early, the fact he was young, optimistic, and had the best treatments available, Gould concluded that he should be in the favorable half of the upper statistical range. After an experimental treatment of radiation, chemotherapy, and surgery, Gould made a full recovery, and his column became a source of comfort for many cancer patients.
Gould was also an advocate of medical marijuana. While suffering from cancer, he smoked the illegal drug to alleviate the nausea associated with his medical treatments. According to Gould, his use of marijuana had a "most important effect" on his eventual recovery. In 1998, he testified in the case of Jim Wakeford, a Canadian medical-marijuana user and activist.
Final illness and death.
Gould survived for 20 years until another cancer ended his life. Gould died on May 20, 2002, from a metastatic adenocarcinoma of the lung, a form of cancer which had spread to his brain. This cancer was unrelated to his abdominal cancer. He died in his home "in a bed set up in the library of his SoHo loft, surrounded by his wife Rhonda, his mother Eleanor, and the many books he loved."
Scientific career.
Gould began his higher education at Antioch College, graduating with a double major in geology and philosophy in 1963. During this time, he also studied at the University of Leeds in the United Kingdom. After completing graduate work at Columbia University in 1967 under the guidance of Norman Newell, he was immediately hired by Harvard University where he worked until the end of his life (1967–2002). In 1973, Harvard promoted him to Professor of Geology and Curator of Invertebrate paleontology at the institution's Museum of Comparative Zoology; he very often described himself as a taxonomist.
In 1982 Harvard awarded him the title of Alexander Agassiz Professor of Zoology. The following year, 1983, he was awarded fellowship of the American Association for the Advancement of Science, where he later served as president (1999–2001). The AAAS news release cited his "numerous contributions to both scientific progress and the public understanding of science". He also served as president of the Paleontological Society (1985–1986) and of the Society for the Study of Evolution (1990–1991).
In 1989 Gould was elected into the body of the National Academy of Sciences. Through 1996–2002 Gould was Vincent Astor Visiting Research Professor of Biology at New York University. In 2001, the American Humanist Association named him the Humanist of the Year for his lifetime of work. In 2008, he was posthumously awarded the Darwin-Wallace Medal, along with 12 other recipients. (Until 2008, this medal had been awarded every 50 years by the Linnean Society of London.)
Punctuated equilibrium.
Early in his career, Gould and Niles Eldredge developed the theory of punctuated equilibrium, according to which evolutionary change occurs relatively rapidly, alternating with longer periods of relative evolutionary stability. Although Gould coined the term "punctuated equilibria", the idea was first presented in Eldredge's doctoral dissertation on Devonian trilobites and in an article published the previous year on allopatric speciation. According to Gould, punctuated equilibrium revised a key pillar "in the central logic of Darwinian theory".
Some evolutionary biologists have argued that while punctuated equilibrium was "of great interest to biology", it merely modified neo-Darwinism in a manner that was fully compatible with what had been known before. For example, George Gaylord Simpson in "Tempo and Mode in Evolution" (1941), described evolutionary history as being characterized by mostly gradual change (horotely), but also included slow (bradytely) and rapid (tachytely) rates of evolution. Other biologists emphasize the theoretical novelty of punctuated equilibrium, and argued that evolutionary stasis had been "unexpected by most evolutionary biologists" and "had a major impact on paleontology and evolutionary biology". Some critics jokingly referred to the theory of punctuated equilibrium as "evolution by jerks", which prompted Gould to respond in kind by describing phyletic gradualism as "evolution by creeps".
Evolutionary developmental biology.
Gould made significant contributions to evolutionary developmental biology, especially in his work "Ontogeny and Phylogeny". In this book he emphasized the process of heterochrony, which encompasses two distinct processes: neoteny and terminal additions. Neoteny is the process where ontogeny is slowed down and the organism does not reach the end of its development. Terminal addition is the process by which an organism adds to its development by speeding and shortening earlier stages in the developmental process. Gould's influence in the field of evolutionary developmental biology continues to be seen in such areas as the study of evolution of feathers.
Selectionism and sociobiology.
Gould championed biological constraints such as the limitations of developmental pathways on evolutionary outcomes, as well as other non-selectionist forces in evolution. In particular, he considered many higher functions of the human brain to be the unintended side consequence or by-product of natural selection, rather than direct adaptations. To describe such co-opted features he coined the term exaptation with Elisabeth Vrba. Gould believed this understanding undermines an essential premise of human sociobiology and evolutionary psychology.
Against "Sociobiology".
In 1975, Gould's Harvard colleague E. O. Wilson introduced his analysis of animal behavior (including human behavior) based on a sociobiological framework that suggested that many social behaviors have a strong evolutionary basis. In response, Gould, Richard Lewontin, and others from the Boston area wrote the subsequently well-referenced letter to "The New York Review of Books" entitled, "Against 'Sociobiology'". This open letter criticized Wilson's notion of a "deterministic view of human society and human action".
But Gould did not rule out sociobiological explanations for many aspects of animal behavior, and later wrote: "Sociobiologists have broadened their range of selective stories by invoking concepts of inclusive fitness and kin selection to solve (successfully I think) the vexatious problem of altruism—previously the greatest stumbling block to a Darwinian theory of social behavior... Here sociobiology has had and will continue to have success. And here I wish it well. For it represents an extension of basic Darwinism to a realm where it should apply."
Spandrels and the Panglossian Paradigm.
With Richard Lewontin, Gould wrote an influential 1979 paper entitled, "The Spandrels of San Marco and the panglossian paradigm", 
which introduced the architectural term "spandrel" into evolutionary biology. In architecture, a spandrel is a curved area of masonry which exists between arches supporting a dome. Spandrels, also called pendentives in this context, are found particularly in Gothic churches.
When visiting Venice in 1978, Gould noted that the spandrels of the San Marco cathedral, while quite beautiful, were not spaces planned by the architect. Rather the spaces arise as "necessary architectural byproducts of mounting a dome on rounded arches." Gould and Lewontin thus defined "spandrels" in the evolutionary biology context, to mean any biological feature of an organism that arises as a necessary side consequence of other features, which is not directly selected for by natural selection. Proposed examples include the "masculinized genitalia in female hyenas, exaptive use of an umbilicus as a brooding chamber by snails, the shoulder hump of the giant Irish deer, and several key features of human mentality."
In Voltaire's "Candide", Dr. Pangloss is portrayed as a clueless scholar who, despite the evidence, insists that "all is for the best in this best of all possible worlds". Gould and Lewontin asserted that it is Panglossian for evolutionary biologists to view all traits as atomized things that had been naturally selected for, and criticised biologists for not granting theoretical space to other causes, such as phyletic and developmental constraints. The relative frequency of spandrels, so defined, versus adaptive features in nature, remains a controversial topic in evolutionary biology. An illustrative example of Gould's approach can be found in Elisabeth Lloyd's case study suggesting that the female orgasm is a by-product of shared developmental pathways. Gould also wrote on this topic in his essay "Male Nipples and Clitoral Ripples", prompted by Lloyd's earlier work.
Evolutionary progress.
Gould favored the argument that evolution has no inherent drive towards long-term "progress". Uncritical commentaries often portray evolution as a ladder of progress, leading towards bigger, faster, and smarter organisms, the assumption being that evolution is somehow driving organisms to get more complex and ultimately more like humankind. Gould argued that evolution's drive was not towards complexity, but towards diversification. Because life is constrained to begin with a simple starting point (like bacteria), any diversity resulting from this start, by random walk, will have a skewed distribution and therefore be perceived to move in the direction of higher complexity. But life, Gould argued, can also easily adapt towards simplification, as is often the case with parasites.
In a review of "", Richard Dawkins approved of Gould's general argument, but suggested that he saw evidence of a "tendency for lineages to improve cumulatively their adaptive fit to their particular way of life, by increasing the numbers of features which combine together in adaptive complexes. ... By this definition, adaptive evolution is not just incidentally progressive, it is deeply, dyed-in-the-wool, indispensably progressive."
Cladistics.
Gould never embraced cladistics as a method of investigating evolutionary lineages and process, possibly because he was concerned that such investigations would lead to neglect of the details in historical biology, which he considered all-important. In the early 1990s this led him into a debate with Derek Briggs, who had begun to apply quantitative cladistic techniques to the Burgess Shale fossils, about the methods to be used in interpreting these fossils. Around this time cladistics rapidly became the dominant method of classification in evolutionary biology. Inexpensive but increasingly powerful personal computers made it possible to process large quantities of data about organisms and their characteristics. Around the same time the development of effective polymerase chain reaction techniques made it possible to apply cladistic methods of analysis to biochemical and genetic features as well.
Technical work on land snails.
Most of Gould's empirical research pertained to land snails. He focused his early work on the Bermudian genus "Poecilozonites", while his later work concentrated on the West Indian genus "Cerion". According to Gould ""Cerion" is the land snail of maximal diversity in form throughout the entire world. There are 600 described species of this single genus. In fact, they're not really species, they all interbreed, but the names exist to express a real phenomenon which is this incredible morphological diversity. Some are shaped like golf balls, some are shaped like pencils. ... Now my main subject is the evolution of form, and the problem of how it is that you can get this diversity amid so little genetic difference, so far as we can tell, is a very interesting one. And if we could solve this we'd learn something general about the evolution of form."
Given "Cerion's" extensive geographic diversity, Gould later lamented that if Christopher Columbus had only cataloged a single "Cerion" it would have ended the scholarly debate about which island Columbus had first set foot on in America.
Influence.
Gould is one of the most frequently cited scientists in the field of evolutionary theory. His 1979 "spandrels" paper has been cited more than 4,000 times. In "Paleobiology"—the flagship journal of his own speciality—only Charles Darwin and George Gaylord Simpson have been cited more often. Gould was also a considerably respected historian of science. Historian Ronald Numbers has been quoted as saying: "I can't say much about Gould's strengths as a scientist, but for a long time I've regarded him as the second most influential historian of science (next to Thomas Kuhn)."
"The Structure of Evolutionary Theory".
Shortly before his death, Gould published a long treatise recapitulating his version of modern evolutionary theory: "The Structure of Evolutionary Theory" (2002).
As a public figure.
Gould became widely known through his popular essays on evolution in the "Natural History" magazine. His essays were published in a series titled "This View of Life" (a phrase from the concluding paragraph of Charles Darwin's "Origin of Species") starting from January 1974 and ended in January 2001, amounting to a continuous publication of 300 essays. Many of his essays were reprinted in collected volumes that became bestselling books such as "Ever Since Darwin" and "The Panda's Thumb", "Hen's Teeth and Horse's Toes", and "The Flamingo's Smile".
A passionate advocate of evolutionary theory, Gould wrote prolifically on the subject, trying to communicate his understanding of contemporary evolutionary biology to a wide audience. A recurring theme in his writings is the history and development of pre-evolutionary and evolutionary thought. He was also an enthusiastic baseball fan and sabermetrician, and made frequent reference to the sport in his essays. Many of his baseball essays were anthologized in his posthumously published book "Triumph and Tragedy in Mudville" (2003).
Although a proud Darwinist, Gould's emphasis was less gradualist and reductionist than most neo-Darwinists. He fiercely opposed many aspects of sociobiology and its intellectual descendant evolutionary psychology. He devoted considerable time to fighting against creationism, creation science, and intelligent design. Most notably, Gould provided expert testimony against the equal-time creationism law in "McLean v. Arkansas". Gould later developed the term "non-overlapping magisteria" (NOMA) to describe how, in his view, science and religion could not comment on each other's realm. Gould went on to develop this idea in some detail, particularly in the books "Rocks of Ages" (1999) and "The Hedgehog, the Fox, and the Magister's Pox" (2003). In a 1982 essay for "Natural History" Gould wrote:
Our failure to discern a universal good does not record any lack of insight or ingenuity, but merely demonstrates that nature contains no moral messages framed in human terms. Morality is a subject for philosophers, theologians, students of the humanities, indeed for all thinking people. The answers will not be read passively from nature; they do not, and cannot, arise from the data of science. The factual state of the world does not teach us how we, with our powers for good and evil, should alter or preserve it in the most ethical manner.
The anti-evolution petition "A Scientific Dissent From Darwinism" spawned the National Center for Science Education's pro-evolution counterpart Project Steve, which is named in Gould's honor.
Gould also became a noted public face of science, often appearing on television. In 1984 Gould received his own "NOVA" special on PBS. Other appearances included interviews on CNN's "Crossfire", NBC's "The Today Show", and regular appearances on the "Charlie Rose" show. Gould was also a guest in all seven episodes of the Dutch talk series "A Glorious Accident", in which he appeared with Oliver Sacks.
Gould was featured prominently as a guest in Ken Burns's PBS documentary "Baseball", as well as PBS's "Evolution" series. Gould was also on the Board of Advisers to the influential Children's Television Workshop television show "3-2-1 Contact", where he made frequent guest appearances.
In 1997 he voiced a cartoon version of himself on the television series "The Simpsons". In the episode "Lisa the Skeptic", Lisa finds a skeleton that many people believe is an apocalyptic angel. Lisa contacts Gould and asks him to test the skeleton's DNA. The fossil is discovered to be a marketing gimmick for a new mall. During production the only phrase Gould objected to was a line in the script that introduced him as the "world's most brilliant paleontologist". In 2002 the show paid tribute to Gould after his death, dedicating the season 13 finale to his memory. Gould had died two days before the episode aired.
Controversy.
Gould received many accolades for his scholarly work and popular expositions of natural history,
but was not immune from criticism by biologists who felt his public presentations were out of step with mainstream evolutionary theory.
The public debates between Gould's supporters and detractors have been so quarrelsome that they have been dubbed "The Darwin Wars" by several commentators.
John Maynard Smith, an eminent British evolutionary biologist, was among Gould's strongest critics. Maynard Smith thought that Gould misjudged the vital role of adaptation in biology, and was critical of Gould's acceptance of species selection as a major component of biological evolution. In a review of Daniel Dennett's book "Darwin's Dangerous Idea", Maynard Smith wrote that Gould "is giving non-biologists a largely false picture of the state of evolutionary theory." But Maynard Smith has not been consistently negative, writing in a review of "The Panda's Thumb" that "Stephen Gould is the best writer of popular science now active... Often he infuriates me, but I hope he will go right on writing essays like these." Maynard Smith was also among those who welcomed Gould's reinvigoration of evolutionary paleontology.
One reason for criticism was that Gould appeared to be presenting his ideas as a revolutionary way of understanding evolution, and argued for the importance of mechanisms other than natural selection, mechanisms which he believed had been ignored by many professional evolutionists. As a result, many non-specialists sometimes inferred from his early writings that Darwinian explanations had been proven to be unscientific (which Gould never tried to imply). Along with many other researchers in the field, Gould's works were sometimes deliberately taken out of context by creationists as "proof" that scientists no longer understood how organisms evolved. Gould himself corrected some of these misinterpretations and distortions of his writings in later works.
As documented by Kim Sterelny among others, Gould disagreed with Richard Dawkins about the importance of gene selection in evolution. Dawkins argued that evolution is best understood as competition among genes (or replicators), while Gould advocated the importance of multi-level selection, including selection amongst genes, cell lineages, organisms, demes, species, and clades.
Dawkins also said that Gould deliberately played down the difference between rapid gradualism and macromutation in his theory of punctuated equilibrium. Criticism of Gould and his theory of punctuated equilibrium can be found in Dawkins' "The Blind Watchmaker" and "Unweaving the Rainbow", as well as chapter 10 of Dennett's "Darwin's Dangerous Idea".
Cambrian fauna.
Gould's interpretation of the Cambrian Burgess Shale fossils in his book "Wonderful Life" emphasized the striking morphological disparity (or "weirdness") of the Burgess Shale fauna, and the role of chance in determining which members of this fauna survived and flourished. He used the Cambrian fauna as an example of the role of contingency in the broader pattern of evolution.
His view was criticized by Simon Conway Morris in his 1998 book "The Crucible of Creation". Conway Morris stressed those members of the Cambrian fauna that resemble modern taxa. He also promoted convergent evolution as a mechanism producing similar forms in similar environmental circumstances, and argued in a subsequent book that the appearance of human-like animals is likely. Paleontologists Derek Briggs and Richard Fortey have also argued that much of the Cambrian fauna may be regarded as stem groups of living taxa, though this is still a subject of intense research and debate, and the relationship of many Cambrian taxa to modern phyla has not been established in the eyes of many palaeontologists.
Paleontologist Richard Fortey noted that prior to the release of "Wonderful Life", Conway Morris shared many of Gould's sentiments and views. It was only after publication of "Wonderful Life" that Conway Morris revised his interpretation and adopted a more progressive stance towards the history of life.
Richard Dawkins also disagreed with Gould's interpretation of the Burgess Shale, arguing:
The extreme Gouldian view—certainly the view inspired by his rhetoric, though it is hard to tell from his own words whether he literally holds it himself—is radically different from and utterly incompatible with the standard neo-Darwinian model. ... For a new body plan—a new phylum—to spring into existence, what actually has to happen on the ground is that a child is born which suddenly, out of the blue, is as different from its parents as a snail is from an earthworm. No zoologist who thinks through the implications, not even the most ardent saltationist, has ever supported any such notion.
Opposition to sociobiology and evolutionary psychology.
Gould also had a long-running public feud with E. O. Wilson and other evolutionary biologists concerning the disciplines of human sociobiology and evolutionary psychology, both of which Gould and Lewontin opposed, but which Richard Dawkins, Maynard Smith, Daniel Dennett, and Steven Pinker advocated. These debates reached their climax in the 1970s, and included strong opposition from groups like the Sociobiology Study Group and Science for the People. Pinker accuses Gould, Lewontin, and other opponents of evolutionary psychology of being "radical scientists", whose stance on human nature is influenced by politics rather than science. Gould stated that he made "no attribution of motive in Wilson's or anyone else's case" but cautioned that all human beings are influenced, especially unconsciously, by our personal expectations and biases. He wrote:
I grew up in a family with a tradition of participation in campaigns for social justice, and I was active, as a student, in the civil rights movement at a time of great excitement and success in the early 1960s. Scholars are often wary of citing such commitments. … [but] it is dangerous for a scholar even to imagine that he might attain complete neutrality, for then one stops being vigilant about personal preferences and their influences—and then one truly falls victim to the dictates of prejudice. Objectivity must be operationally defined as fair treatment of data, not absence of preference.
Gould's primary criticism held that human sociobiological explanations lacked evidential support, and argued that adaptive behaviors are frequently assumed to be genetic for no other reason than their supposed universality, or their adaptive nature. Gould emphasized that adaptive behaviors can be passed on through culture as well, and either hypothesis is equally plausible. Gould did not deny the relevance of biology to human nature, but reframed the debate as "biological potentiality vs. biological determinism". Gould stated that the human brain allows for a wide range of behaviors. Its flexibility "permits us to be aggressive or peaceful, dominant or submissive, spiteful or generous… Violence, sexism, and general nastiness "are" biological since they represent one subset of a possible range of behaviors. But peacefulness, equality, and kindness are just as biological—and we may see their influence increase if we can create social structures that permit them to flourish."
"The Mismeasure of Man".
Gould was the author of "The Mismeasure of Man" (1981), a history and inquiry of psychometrics and intelligence testing. Gould investigated the methods of nineteenth century craniometry, as well as the history of psychological testing. Gould claimed that both theories developed from an unfounded belief in biological determinism, the view that "social and economic differences between human groups—primarily races, classes, and sexes—arise from inherited, inborn distinctions and that society, in this sense, is an accurate reflection of biology."
It was reprinted in 1996 with the addition of a new foreword and a critical review of "The Bell Curve". "The Mismeasure of Man" has generated perhaps the greatest controversy of all of Gould's books. It has received both widespread praise and extensive criticism, including claims of misrepresentation.
In 2011, a study conducted by six anthropologists reanalyzed Gould's claim that Samuel Morton unconsciously manipulated his skull measurements, and concluded that Gould's analysis was poorly supported and incorrect. They praised Gould for his "staunch opposition to racism" but concluded, "we find that Morton's initial reputation as the objectivist of his era was well-deserved." Ralph Holloway, one of the co-authors of the study, commented, "I just didn't trust Gould. ... I had the feeling that his ideological stance was supreme. When the 1996 version of 'The Mismeasure of Man' came and he never even bothered to mention Michael's study, I just felt he was a charlatan." 
The group's paper was reviewed in the journal "Nature", which recommended a degree of caution, stating "the critique leaves the majority of Gould's work unscathed," and notes that "because they couldn't measure all the skulls, they do not know whether the average cranial capacities that Morton reported represent his sample accurately." The journal stated that Gould's opposition to racism may have biased his interpretation of Morton's data, but also noted that "Lewis and his colleagues have their own motivations. Several in the group have an association with the University of Pennsylvania, and have an interest in seeing the valuable but understudied skull collection freed from the stigma of bias." The group's paper was also criticized by philosopher of science Michael Weisberg, also of the University of Pennsylvania. Weisberg argues that "most of Gould's arguments against Morton are sound. Although Gould made some errors and overstated his case in a number of places, he provided "prima facia" evidence, as yet unrefuted, that Morton did indeed mismeasure his skulls in ways that conformed to 19th century racial biases." Biologists and philosophers Jonathan Kaplan, Massimo Pigliucci, and Joshua Alexander Banta also published a critique of the groups's paper, arguing that many of its claims were misleading and the re-measurements were "completely irrelevant to an evaluation of Gould's published analysis." They also argued that the "methods deployed by Morton and Gould were both inappropriate" and that "Gould's statistical analysis of Morton's data is in many ways no better than Morton's own."
Non-overlapping magisteria.
In his book "Rocks of Ages" (1999), Gould put forward what he described as "a blessedly simple and entirely conventional resolution to ... the supposed conflict between science and religion." He defines the term "magisterium" as "a domain where one form of teaching holds the appropriate tools for meaningful discourse and resolution." The non-overlapping magisteria (NOMA) principle therefore divides the magisterium of science to cover "the empirical realm: what the Universe is made of (fact) and why does it work in this way (theory). The magisterium of religion extends over questions of ultimate meaning and moral value. These two magisteria do not overlap, nor do they encompass all inquiry." He suggests that "NOMA enjoys strong and fully explicit support, even from the primary cultural stereotypes of hard-line traditionalism" and that NOMA is "a sound position of general consensus, established by long struggle among people of goodwill in both magisteria."
However, this view has not been without criticism. For example, in his book "The God Delusion", Richard Dawkins argues that the division between religion and science is not as simple as Gould claims, as few religions exist without claiming the existence of miracles, which "by definition, violate the principles of science". Dawkins also opposes the idea that religion has anything meaningful to say about ethics and values, and therefore has no authority to claim a magisterium of its own. He goes on to say that he believes Gould is disingenuous in much of what he says in "Rocks of Ages". Similarly, humanist philosopher Paul Kurtz argues that Gould was wrong to posit that science has nothing to say about questions of ethics. In fact, Kurtz claims that science is a much better method than religion for determining moral principles.
Publications.
Articles.
Gould's publications were numerous. One review of his publications between 1965 and 2000 noted 479 peer-reviewed papers, 22 books, 300 essays, and 101 "major" book reviews. A select number of his papers are .
Books.
The following is a list of books either written or edited by Stephen Jay Gould, including those published posthumously, after his death in 2002. While some books have been republished at later dates, by multiple publishers, the list below comprises the original publisher and publishing date.
Notes and references.
Specific citations:
General references:
</dl>

</doc>
<doc id="27964" url="http://en.wikipedia.org/wiki?curid=27964" title="Sikhism">
Sikhism

Sikhism (; Punjabi: ਸਿੱਖੀ, "sikkhī", ]) is a panentheistic religion founded during the 15th century in the Punjab region of the Indian subcontinent, by Guru Nanak. It continued to progress through the ten successive Sikh gurus, the eleventh and last guru being the holy scripture "Guru Granth Sahib". The Guru Granth Sahib is a collection of the Sikh gurus' writings that was compiled by the fifth Sikh guru. Sikhism is the fifth-largest organized religion in the world, with approximately 30 million adherents. Punjab, India is the only state in the world with a majority Sikh population.
Adherents of Sikhism are known as Sikhs ("students" or "disciples"). According to Devinder Singh Chahal, "The word 'Sikhi' (commonly known as Gurmat) gave rise to the modern anglicized word 'Sikhism' for the modern world." Gurmat means literally "wisdom of the guru," in contrast to "Manmat", or self-willed impulses.
According to Sewa Singh Kalsi, "The central teaching in Sikhism is the belief in the concept of the oneness of God." Sikhism considers spiritual life and secular life to be intertwined. Guru Nanak, the first Sikh guru established the system of the Langar, or communal kitchen, in order to demonstrate the need to share and have equality among all people. Sikhs also believe that "all religious traditions are equally valid and capable of enlightening their followers." In addition to sharing with others, Guru Nanak inspired people to earn an honest living without exploitation and also promoted the need for remembrance of the divine name (God). Guru Nanak described living an "active, creative, and practical life" of "truthfulness, fidelity, self-control, and purity" as being higher than a purely contemplative life. Guru Hargobind, the sixth Sikh guru, established the political/temporal (Miri) and spiritual (Piri) realms to be mutually coexistent.
According to the ninth Sikh guru, Tegh Bahadhur, the ideal Sikh should have both Shakti (power that resides in the temporal) and Bhakti (spiritual meditative qualities). Finally, the concept of the baptized saint-soldier of the Khalsa was formed by the tenth Sikh guru, Gobind Singh, in 1699 at Anandpur Sahib. Sikhs are expected to embody the qualities of a "Sant-Sipāhī"—a saint-soldier. Sikhs are expected to have control over the so-called "Five Thieves," or "Five Vices," and dispel these by means of the "Five Virtues."
Philosophy and Teachings.
"Sikh" means a person who professes the Sikh religion, believes and follows the teachings of Sri Guru Granth Sahib and the ten Gurus only, and keeps unshorn hair. ... "I solemnly affirm and declare that I am a "Keshadhari" Sikh, that I believe in and follow the teachings of Sri Guru Granth Sahib and the ten Gurus only, and that I have no other belief.
 "Definition of a Sikh and Sikh affirmation in the Delhi Gurdwara Act of 1971".
The origins of Sikhism lie in the teachings of Guru Nanak and his successors. The essence of Sikh teaching is summed up by Guru Nanak in these words: "Realization of Truth is higher than all else. Higher still is truthful living". Sikh teaching emphasizes the principle of equality of all humans and rejects discrimination on the basis of caste, creed, and gender. Sikh principles encourage living life as a householder.
Sikhism is a Panentheistic and a revealed religion. In Sikhism, the concept of "God" is "Vāhigurū"—is shapeless, timeless, and sightless (i.e., unable to be seen with the physical eye): "niraṅkār", "akaal", and "alakh". The beginning of the first composition of Sikh scripture is the figure "1"—signifying the universality of "God". It states that "God" is omnipresent and infinite with power over everything, and is signified by the term "Ik Onkar". Sikhs believe that before creation, all that existed was "God" and "God's" "hukam" (will or order). When God willed, the entire cosmos was created. From these beginnings, God nurtured "enticement and attachment" to "māyā", or the human perception of reality.
The all pervading spirit - The concept of "god" in Sikhism.
The concept of "god" is different in Sikhism than that of other religions. It is known as "Ik Onkar" or "one constant" or the all pervading spirit (which is taken to mean god). It is found in the Gurmukhi script This "spirit" has no gender in Sikhism (though translations may present it as masculine); it is also "Akaal Purkh" (beyond time and space) and "Nirankar" (without form). In addition, Nanak wrote that there are many worlds on which it has created life.
Nanak further states that the understanding of "Akaal" is beyond human beings, but at the same time not wholly unknowable. "Akaal" is omnipresent ("sarav viāpak") in all creation and visible everywhere to the spiritually awakened. Nanak stressed that god must be seen from "the inward eye", or the "heart", of a human being: devotees must meditate to progress towards enlightenment of heavenly life. Guru Nanak emphasized the revelation through meditation, as its rigorous application permits the existence of communication between god and human beings.
The Mool Mantar, the opening line of the Guru Granth Sahib and each subsequential "Raga":
Liberation.
Guru Nanak's teachings are founded not on a final destination of heaven or hell but on a spiritual union with the Akal which results in salvation or "Jivanmukta", Guru Gobind Singh makes it clear that human birth is obtained with great fortune, therefore one needs to be able to make the most of this life. There has been some confusion among scholars, interpreting the pertinent religious texts as evidence that Sikhs believe in reincarnation and karma as the same as Hinduism and Buddhists when such is not the case. In Sikhism karma "is modified by the concept of God's grace" ("nadar, mehar, kirpa, karam" etc.). Guru Nanak states "The body takes birth because of karma, but salvation is attained through grace". To get closer to God: Sikhs avoid the evils of "Maya", keep the everlasting truth in mind, practice "Shabad Kirtan", meditate on "Naam", and serve humanity. Sikhs believe that being in the company of the "Satsang" or "Sadh Sangat" is one of the key ways to achieve liberation from the cycles of reincarnation.
Worldly illusion.
"Māyā"—defined as a temporary illusion or "unreality"—is one of the core deviations from the pursuit of God and salvation: where worldly attractions which give only illusory temporary satisfaction and pain which distract the process of the devotion of God. However, Nanak emphasised māyā as not a reference to the unreality of the world, but of its values. In Sikhism, the influences of ego, anger, greed, attachment, and lust—known as the "Five Thieves"—are believed to be particularly distracting and hurtful. Sikhs believe the world is currently in a state of "Kali Yuga" (Age of Darkness) because the world is lead astray by the love of and attachment to Maya. The fate of people vulnerable to the Five Thieves ('Pānj Chor'), is separation from God, and the situation may be remedied only after intensive and relentless devotion.
The timeless truth.
According to Nanak the supreme purpose of human life is to reconnect with Akal (The Timeless One), however, egotism is the biggest barrier in doing this. Using the Guru's teaching remembrance of "nām" (the divine word or the name of the Lord) leads to the end of egotism. Guru Nanak designated the word 'guru' (meaning "teacher") to mean the voice of "the spirit": the source of knowledge and the guide to salvation. As Ik Onkar is universally immanent, "guru" is indistinguishable from "Akal" and are one and the same. One connects with "guru" only with accumulation of selfless search of truth. Ultimately the seeker realizes that it is the consciousness within the body which is seeker/follower and the Word is the true "guru". The human body is just a means to achieve the reunion with Truth. Once truth starts to shine in a person's heart, the essence of current and past holy books of all religions is understood by the person.
Singing and music.
Sikhs refer to the hymns of the Gurus as Gurbani (The Guru's word). Shabad Kirtan is the singing of Gurbani. The entire Guru Granth Sahib is written in a form of poetry and rhyme. Guru Nanak started the Shabad Kirtan tradition and taught that listening to kirtan is a powerful way to achieve tranquility while meditating; Singing of the glories of the Supreme Timeless One (God) with devotion is the most effective way to come in communion with the Supreme Timeless One. The three morning prayers for Sikhs consist of Japji Sahib, Jaap Sahib and Tav-Prasad Savaiye. Baptized Sikhs rise early and meditate and then recite all the Five Banis of Nitnem before breakfast.
Remembrance.
A key practice by Sikhs is remembrance of the Divine Name (Naam – the Name of the Lord). This contemplation is done through "Nām Japna" (repetition of the divine name) or "Naam Simran" (remembrance of the divine Name through recitation). The verbal repetition of the name of God or a sacred syllable is an established practice in religious traditions in India but Guru Nanak's interpretation emphasized inward, personal observance. Guru Nanak's ideal is the total exposure of one's being to the divine Name and a total conforming to Dharma or the "Divine Order". Nanak described the result of the disciplined application of "nām simraṇ" as a "growing towards and into God" through a gradual process of five stages. The last of these is "sach khaṇḍ" ("The Realm of Truth")—the final union of the spirit with God.
Service and action.
Meditation is unfruitful without service and action. 
Sikhs are taught that selfless service, or "sēvā", and charitable work enables the devotee to kill the ego. 
Service in Sikhism takes three forms: "Tan" - physical service; "Man" - mental service (such as studying to help others); and "Dhan" - material service. Guru Nanak stressed now "kirat karō": that a Sikh should balance work, worship, and charity, and should defend the rights of all human beings. They are encouraged to have a "chaṛdī kalā", or "optimistic" - "resilience", view of life. Sikh teachings also stress the concept of sharing—"vaṇḍ chakkō"—through the distribution of free food at Sikh gurdwaras ("laṅgar"), giving charitable donations, and working for the good of the community and others ("sēvā").
Justice and equality.
Sikhism regards "Justice" and "Restorative Justice" and "divine justice" as trumping any subjective codes of moral order. The word in Punjabi used to depict this is "Niau" which means justice. The word "dharam" (righteousness) is also used to convey justice "in the sense of the moral order". "An attack on dharam is an attack on justice, on righteousness, and on the moral order generally". According to the Tenth Sikh Guru, Guru Gobind Singh "when all efforts to restore peace prove useless and no words avail, lawful is the flash of steel, it is right to draw the sword".
Men and women are equal in Sikhism and share the same rights. In contrast, while churches have been arguing in recent times on female priest ordination, women have been leading in prayers at Sikh temples since the founding of Sikhism.
The ten gurus and religious authority.
The term guru comes from the Sanskrit "gurū", meaning teacher, guide, or mentor. The traditions and philosophy of Sikhism were established by ten specific gurus from 1469 to 1708. Each guru added to and reinforced the message taught by the previous, resulting in the creation of the Sikh religion. Guru Nanak was the first guru and appointed a disciple as successor. Guru Gobind Singh was the final guru in human form. Before his death, Guru Gobind Singh decreed that the Gurū Granth Sāhib would be the final and perpetual guru of the Sikhs.
Guru Angad succeeded Guru Nanak. Later, an important phase in the development of Sikhism came with the third successor, Guru Amar Das. Guru Nanak's teachings emphasised the pursuit of salvation; Guru Amar Das began building a cohesive community of followers with initiatives such as sanctioning distinctive ceremonies for birth, marriage, and death. Amar Das also established the "manji" (comparable to a diocese) system of clerical supervision.
Guru Amar Das's successor and son-in-law Guru Ram Das founded the city of Amritsar, which is home of the Harimandir Sahib and regarded widely as the holiest city for all Sikhs. Guru Arjan was captured by Mughal authorities who were suspicious and hostile to the religious order he was developing. His persecution and death inspired his successors to promote a military and political organization of Sikh communities to defend themselves against the attacks of Mughal forces.
The Sikh gurus established a mechanism which allowed the Sikh religion to react as a community to changing circumstances. The sixth guru, Guru Hargobind, was responsible for the creation of the concept of Akal Takht ("throne of the timeless one"), which serves as the supreme decision-making centre of Sikhism and sits opposite the Harmandir Sahib. The "Sarbat Ḵẖālsā" (a representative portion of the Khalsa Panth) historically gathers at the Akal Takht on special festivals such as Vaisakhi or Hola Mohalla and when there is a need to discuss matters that affect the entire Sikh nation. A "gurmatā" (literally, "guru's intention") is an order passed by the Sarbat Ḵẖālsā in the presence of the Gurū Granth Sāhib. A gurmatā may only be passed on a subject that affects the fundamental principles of Sikh religion; it is binding upon all Sikhs. The term "hukamnāmā" (literally, "edict" or "royal order") is often used interchangeably with the term gurmatā. However, a hukamnāmā formally refers to a hymn from the Gurū Granth Sāhib which is a given order to Sikhs.
History.
Guru Nanak (1469–1539), the founder of Sikhism, was born in the village of "Rāi Bhōi dī Talwandī", now called Nankana Sahib (in present-day Pakistan). His parents were Khatris. As a boy, Nanak was fascinated by God and religion. He would not partake in religious rituals or customs and oddly meditated alone. His desire to explore the mysteries of life eventually led him to leave home and take missionary journeys.
In his early teens, Nanak caught the attention of the local landlord Rai Bular Bhatti, who was moved by his amazing intellect and divine qualities. Rai Bular Bhatti was witness to many incidents in which Nanak enchanted him and as a result Rai Bular Bhatti and Nanak's sister Bibi Nanki, became the first persons to recognise the divine qualities in Nanak. Both of them then encouraged and supported Nanak to study and travel. At the age of thirty, Nanak went missing and was presumed to have drowned after going for one of his morning baths to a local stream called the "Kali Bein". He reappeared three days later. It was from this moment that Nanak would begin to spread the teachings of what was then the beginning of Sikhism. Although the exact account of his itinerary is disputed, he is widely acknowledged to have made five major journeys, spanning thousands of miles, the first tour being east towards Bengal and Assam, the second south towards Andhra and Tamil Nadu, the third north towards Kashmir, Ladakh, and Tibet, and the fourth tour west towards Baghdad and Mecca. In his last and final tour, he returned to the banks of the Ravi River to end his days.
Growth of Sikhism.
In 1539, Guru Nanak chose his disciple "Lahiṇā" as a successor to the guruship rather than either of his sons. Lahiṇā was named Guru Angad and became the second guru of the Sikhs. Nanak conferred his choice at the town of Kartarpur on the banks of the river Ravi, where Nanak had finally settled down after his travels. Though Sri Chand was not an ambitious man, the Udasis believed that the Guruship should have gone to him, since he was a man of pious habits in addition to being Nanak's son. On Nanak's advice, Guru Angad moved from Kartarpur to Khadur, where his wife Khivi and children were living, until he was able to bridge the divide between his followers and the Udasis. Guru Angad continued the work started by Guru Nanak and is widely credited for standardising the Gurmukhī script as used in the sacred scripture of the Sikhs.
Guru Amar Das became the third Sikh guru in 1552 at the age of 73. Goindval became an important centre for Sikhism during the guruship of Guru Amar Das. He preached the principle of equality for women by prohibiting purdah and sati. Guru Amar Das also encouraged the practice of langar and made all those who visited him attend laṅgar before they could speak to him. In 1567, Emperor Akbar sat with the ordinary and poor people of Punjab to have laṅgar. Guru Amar Das also trained 146 apostles of which 52 were women, to manage the rapid expansion of the religion. Before he died in 1574 aged 95, he appointed his son-in-law Jēṭhā, a Khatri of the Sodhi clan, as the fourth Sikh guru.
"Jēṭhā" became Guru Ram Das and vigorously undertook his duties as the new guru. He is responsible for the establishment of the city of Ramdaspur later to be named Amritsar. Before Ramdaspur, Amritsar was known as Guru Da Chakk. In 1581, Guru Arjan—youngest son of the fourth guru—became the fifth guru of the Sikhs. In addition to being responsible for building the Harimandir Sahib, he prepared the Sikh sacred text known as the Ādi Granth (literally "the first book") and included the writings of the first five gurus and other enlightened Hindu and Muslim saints. In 1606, he was tortured and killed by the Mughal Emperor, Jahangir, for refusing to make changes to the Granth and for supporting an unsuccessful contender to the throne.
Political advancement.
Guru Hargobind became the sixth guru of the Sikhs. He carried two swords—one for spiritual and the other for temporal reasons (known as "mīrī" and "pīrī" in Sikhism). Sikhs grew as an organized community and under the 10th Guru the Sikhs developed a trained fighting force to defend their independence. In 1644, Guru Har Rai became guru followed by Guru Har Krishan, the boy guru, in 1661. Guru Har Krishan helped to heal many sick people. Coming into contact with so many people every day, he too was infected and taken seriously ill and later died. No hymns composed by these three gurus are included in the Guru Granth Sahib.
Guru Tegh Bahadur became guru in 1665 and led the Sikhs until 1675. Guru Tegh Bahadur was executed by Aurangzeb for helping to protect one's right to freedom of religion, after a delegation of Kashmiri Pandits came to him for help when the Emperor began to persecute those who refused to convert to Islam. He was succeeded by his son, Gobind Rai who was just nine years old at the time of his father's death. Gobind Rai further militarised his followers, and was baptised by the "Pañj Piārē" when he inaugurated the Khalsa on 30 March 1699. From here on in he was known as Guru Gobind Singh.
From the time of Nanak the Sikhs had significantly transformed. Even though the core Sikh spiritual philosophy was never affected, the followers now began to develop a political identity. Conflict with Mughal authorities escalated during the lifetime of Guru Teg Bahadur and Guru Gobind Singh.
Sikh Confederacy and the rise of the Khalsa.
The tenth guru of Sikhism, Guru Gobind Singh, inaugurated the Khalsa (the collective body of all initiated Sikhs) as the Sikh temporal authority in the year 1699. The Khalsa is a disciplined community that combines its spiritual purpose and goals with political and military duties. Shortly before his death, Guru Gobind Singh proclaimed the Gurū Granth Sāhib (the Sikh Holy Scripture) to be the ultimate spiritual authority for the Sikhs.
The Sikh Khalsa's rise to power began in the 17th century during a time of growing militancy against Mughal rule. The creation of a Sikh Empire began when Guru Gobind Singh sent a Sikh general, Banda Singh Bahadur, to fight the Mughal rulers of India and those who had committed atrocities against Pir Buddhu Shah. Banda Singh advanced his army towards the main Muslim Mughal city of Sirhind and, following the instructions of the guru, punished all the culprits. Soon after the invasion of Sirhind, while resting in his chamber after the Rehras prayer Guru Gobind Singh was stabbed by a Pathan assassin hired by Mughals. Gobind Singh killed the attacker with his sword. Though a European surgeon stitched the Guru's wound, the wound re-opened as the Guru tugged at a hard strong bow after a few days, causing profuse bleeding that led to Gobind Singh's death.
After the Guru's death, Baba Banda Singh Bahadur became the commander-in-chief of the Khalsa. He organized the civilian rebellion and abolished or halted the Zamindari system in time he was active and gave the farmers proprietorship of their own land. Banda Singh was executed by the emperor Farrukh Siyar after refusing the offer of a pardon if he converted to Islam. The confederacy of Sikh warrior bands known as "misls" alongside the development of the Dal Khalsa achieved a series of sweeping military and diplomatic victories, eventually creating a Sikh Empire in the Punjab under the emperor, Maharaja Ranjit Singh in 1799.
The vast Sikh empire with its capital in Lahore and limits reaching the Khyber Pass and the borders of China comprised almost 200000 mi2 of what is now Afghanistan, Pakistan and Northern India. The Sikh nation's embrace of military and political organisation made it a considerable regional force in 19th century India and allowed it to retain control of the Sikh Empire in the face of numerous local uprisings. The order, traditions and discipline developed over centuries culminated at the time of Ranjit Singh to give rise to a common religious and social identity.
After the death of Ranjit Singh in 1839, the Sikh Empire fell into disorder and, after the assassination of several successors, eventually fell on the shoulders of his youngest son, Maharaja Duleep Singh. Soon after, the British began to attack the Sikh Kingdom. Both British and Sikh sides sustained heavy losses of both troops and materials in the hard-fought First and Second Anglo-Sikh Wars. The Empire was eventually annexed by the United Kingdom, bringing the Punjab under the British Raj.
Partition.
A quarter of a century later, Sikhs formed the Shiromani Gurdwara Prabandhak Committee and the Shiromani Akali Dal to preserve Sikhs' religious and political organization. Of the violence that accompanied the Partition of India, historians Ian Talbot and Gurharpal Singh write:
There are numerous eyewitness accounts of the maiming and mutilation of victims. The catalogue of horrors includes the disembowelling of pregnant women, the slamming of babies' heads against brick walls, the cutting off of victims limbs and genitalia and the display of heads and corpses. While previous communal riots had been deadly, the scale and level of brutality was unprecedented. Although some scholars question the use of the term 'genocide' with respect to the Partition massacres, much of the violence manifested as having genocidal tendencies. It was designed to cleanse an existing generation as well as prevent its future reproduction.
The newly formed governments were completely unequipped to deal with migrations of such staggering magnitude, and massive violence and slaughter occurred on both sides of the border. Estimates of the number of deaths vary, with low estimates at 200,000 and high estimates at 1,000,000.
The emergency meeting of the joint defense council on 16 August agreed to strengthen the Punjab boundary force as quickly as possible. Nehru and liquat visited Lahore, Ambala, Jilandur and Amritsar together to see for themselves what was going on and to appeal for peace. They tried to remind everyone that both India and Pakistan had pledged to protect the minorities after the partition and that there was no need for anyone to move home but they were shouting against the hurricane. Each new outrage, each new massacre brought the thirst for revenge and desperate need to flee from the terror as the scale of disaster mounted, Tara Singh and other Sikh leaders toured the province in military vehicles, appealing to stop the violence, but their followers had tasted blood, and it was too late for Tara Singh to stop what he had begun.
Sikhs faced initial opposition from the Government in forming a linguistic state that other states in India were afforded. The Akali Dal started a non-violence movement for Sikh and Punjabi rights. Jarnail Singh Bhindranwale emerged as a leader of the Damdami Taksal in 1977 and promoted a more militant solution to the problem. In June 1984, Indian Prime Minister Indira Gandhi ordered the Indian army to launch Operation Blue Star to remove Bhindranwale and his followers from the Darbar Sahib. Bhindranwale and his accompanying followers, as well as many innocent Sikhs visiting the temple, were killed during the army's operations. In October, Indira Gandhi was assassinated by two of her Sikh bodyguards. The assassination was followed by the 1984 anti-Sikh riots. and Hindu-Sikh conflicts in Punjab, as a reaction to Operation Blue Star and the assassination.
Scriptures.
There is one primary source of scripture for the Sikhs: the Gurū Granth Sāhib. The Gurū Granth Sāhib may be referred to as the Ādi Granth—literally, "The First Volume"—and the two terms are often used synonymously. Here, however, the Ādi Granth refers to the version of the scripture created by Guru Arjan in 1604. The Gurū Granth Sāhib is the final version of the scripture created by Guru Gobind Singh.
There are other sources of scriptures such as the Dasam Granth and so called Janamsakhis. These however, have been the subject of controversial debate amongst the Sikh community.
Adi Granth (The Sacred Scripture).
The Ādi Granth was a religious text compiled primarily by Bhai Gurdas under the supervision of Guru Arjan between the years 1603 and 1604. It is written in the Gurmukhī script, which is a descendant of the Laṇḍā script used in the Punjab at that time. The Gurmukhī script was standardised by Guru Angad, the second guru of the Sikhs, for use in the Sikh scriptures and is thought to have been influenced by the Śāradā and Devanāgarī scripts. An authoritative scripture was created to protect the integrity of hymns and teachings of the Sikh gurus and fifteen bhagats. These fifteen bhagats are Namdev, Ravidas, Jaidev, Trilocan, Beni, Ramanand, Sainu, Dhanna, Sadhna, Pipa, Sur, Bhikhan, Paramanand, Farid, and Kabir. At the time, Arjan Sahib tried to prevent undue influence from the followers of Prithi Chand, the guru's older brother and rival.
Guru Granth Sahib (The Eternal Sikh Holy Scripture).
The Guru Granth Sahib Ji is the most important Sikh scripture. Its name means "eternal guru", and the scripture is treated as a living guru. It is based on the writings and collections of the ten gurus. The final version of the Gurū Granth Sāhib was compiled by Guru Gobind Singh in 1678. It consists of the original Ādi Granth with the addition of Guru Tegh Bahadur's hymns. The Guru Granth Sahib is considered the Eleventh and final spiritual authority of the Sikhs.
It contains compositions by the first five Gurus, Guru Teg Bahadur and just one "śalōk" ("couplet") from Guru Gobind Singh. It also contains the traditions and teachings of "sants" ("saints") such as Kabir, Namdev, Ravidas, and Sheikh Farid along with several others.
The bulk of the scripture is classified into "rāgs", with each rāg subdivided according to length and author. There are 31 rāgs within the Gurū Granth Sāhib. In addition to the rāgs, there are clear references to the folk music of Punjab. The main language used in the scripture is known as "Sant Bhāṣā", a language related to both Punjabi and Hindi and used extensively across medieval northern India by proponents of popular devotional religion. As per the name "Gurmukhi", it is not merely a script but it is the language which came out of Guru's mouth – by using this definition, all words in Guru Granth Sahib constitute "Gurbani" words, thus making Gurmukhi language which then constitute two components – spoken Gurmukhi words (in form of Gurbani) which originated from different languages (like world's different languages have similar roots) and Gurmukhi script. The text further comprises over 5000 "śabads" (hymns), which are poetically constructed and set to classical form of music rendition, can be set to predetermined musical "tāl" (rhythmic beats).
The Granth begins with the "Mūl Mantra", an iconic verse created by Nanak:
All text within the Granth is known as "gurbānī". And Gurbani is the Guru "Baani Guru Guru hai Baani" (The word is the Guru and Guru is the word) and "Shabd Guru Surat Dhun Chaylaa" (The Shabad is the Guru, upon whom I lovingly focus my consciousness; I am the disciple.). Therefore, as evident from the message of the Guru Nanak (first Guru) Shabad (or word) was always the Guru (the enlightener); however, as Sikhism stand on the dual strands of Miri-Piri, the Guru in Sikhism is a combination of teacher-leader. Therefore, the lineage from Guru Nanak to Guru Gobind Singh was of the teacher-leaders eventually wherein the temporal authority was passed on to the Khalsa and spiritual authority, which always was with, passed to Adi Granth(thence the Guru Granth Sahib).
Therefore, Guru Granth Sahib and its 11th body -the Khalsa is the Guru, teacher-leader, of the Sikhs till eternity.
Dasam Granth (The Book of Gurus).
The Dasam Granth is a minor holy scripture of Sikhs which contains texts attributed to the Tenth Guru. The scripture sets out the ideas, thoughts and guidelines for the future of Sikhism and the Khalsa. The Dasam Granth holds a significance of great amount for Sikhs, however it does not have the same authority as Adi Granth. Some compositions of the Dasam Granth like Jaap Sahib, (Amrit Savaiye), and Benti Chaupai are part of the daily prayers/lessons (Nitnem) of/for Sikhs.
The authenticity of the Dasam Granth is amongst the most debated topics within Sikhism.
Janamsakhis (Biographies of the Gurus).
The Janamsākhīs (literally "birth stories"), are writings which profess to be biographies of Nanak. Although not scripture in the strictest sense, they provide an interesting look at Nanak's life and the early start of Sikhism. There are several—often contradictory and sometimes unreliable—Janamsākhīs and they are not held in the same regard as other sources of scriptural knowledge.
Observances.
Observant Sikhs adhere to long-standing practices and traditions to strengthen and express their faith. The daily recitation from memory of specific passages from the Gurū Granth Sāhib, especially the "Japu" (or "Japjī", literally "chant") hymns is recommended immediately after rising and bathing. Family customs include both reading passages from the scripture and attending the gurdwara (also "gurduārā", meaning "the doorway to God"; sometimes transliterated as "gurudwara"). There are many gurdwaras prominently constructed and maintained across India, as well as in almost every nation where Sikhs reside. Gurdwaras are open to all, regardless of religion, background, caste, or race.
Worship in a gurdwara consists chiefly of singing of passages from the scripture. Sikhs will commonly enter the gurdwara, touch the ground before the holy scripture with their foreheads. The recitation of the eighteenth century "ardās" is also customary for attending Sikhs. The ardās recalls past sufferings and glories of the community, invoking divine grace for all humanity.
The Sikh faith also participates in the custom of "Langar" or the community meal. All gurdwaras are open to anyone of any faith for a free meal. People can enter and eat together and are served by faithful members of the community. This is the main cost associated with gurdwaras and where monetary donations are primarily spent.
Sikh festivals and events.
Technically, there are no festivals in Sikhism. However, the events mostly centred around the lives of the Gurus and Sikh martyrs are commemorated. The SGPC, the Sikh organisation in charge of upkeep of the historical gurdwaras of Punjab, organises celebrations based on the new Nanakshahi calendar. This calendar is highly controversial among Sikhs and is not universally accepted. Sikh festivals include the following:
Nagar Kirtan crowd listening to Kirtan at Yuba City.
Ceremonies and customs.
Guru Nanak taught that rituals, religious ceremonies, or idol worship are of little use and Sikhs are discouraged from fasting or going on pilgrimages. Sikhs do not believe in converting people but converts to Sikhism by choice are welcomed. The morning and evening prayers take around two hours a day, starting in the very early morning hours. The first morning prayer is Guru Nanak's "Jap Ji". Jap, meaning "recitation", refers to the use of sound, as the best way of approaching the divine. Like combing hair, hearing and reciting the sacred word is used as a way to comb all negative thoughts out of the mind. The second morning prayer is Guru Gobind Singh's universal Jaap Sahib. The Guru addresses God as having no form, no country, and no religion but as the seed of seeds, sun of suns, and the song of songs. The Jaap Sahib asserts that God is the cause of conflict as well as peace, and of destruction as well as creation. Devotees learn that there is nothing outside of God's presence, nothing outside of God's control. Devout Sikhs are encouraged to begin the day with private meditations on the name of God.
Upon a child's birth, the Guru Granth Sahib is opened at a random point and the child is named using the first letter on the top left hand corner of the left page. All boys are given the last name Singh, and all girls are given the last name Kaur (this was once a title which was conferred on an individual upon joining the Khalsa). Sikhs are joined in wedlock through the "anand kāraj" ceremony. Sikhs are required to marry when they are of a sufficient age (child marriage is taboo), and without regard for the future spouse's caste or descent. The marriage ceremony is performed in the company of the Guru Granth Sahib; around which the couple circles four times. After the ceremony is complete, the husband and wife are considered "a single soul in two bodies."
According to Sikh religious rites, neither husband nor wife is permitted to divorce unless special circumstances arise. A Sikh couple that wishes to divorce may be able to do so in a civil court. 
Sikhism requires that upon death a body be treated respectfully and disposed of in an appropriate and simple manner. Upon death, the body of a Sikh is usually cremated. If this is not possible, any respectful means of disposing the body may be employed. The "kīrtan sōhilā" and "ardās" prayers are performed during the funeral ceremony (known as "antim sanskār").
Baptism and the Khalsa.
Khalsa (meaning "Sovereign") is the collective name given by Gobind Singh to all Sikhs, male or female, who have been baptised or initiated by taking "ammrit" in a ceremony called "ammrit sañcār". The first time that this ceremony took place was on Vaisakhi, which fell on 30 March 1699 at Anandpur Sahib in Punjab. It was on that occasion that Gobind Singh baptised the Pañj Piārē—the five beloved ones, who in turn baptised Gobind Singh himself. The last name, Singh, meaning lion, is given to baptized Sikh males, and the last name Kaur, meaning princess/lioness, is given to baptized Sikh females.
Baptised Sikhs are bound to wear the Five Ks (in Punjabi known as "pañj kakkē" or "pañj kakār") at all times. The 5 items are: "kēs" (uncut hair), "kaṅghā" (small wooden comb), "kaṛā" (circular steel or iron bracelet), "kirpān" (sword/dagger), and "kacchera" (special undergarment). The Five Ks have both practical and symbolic purposes.
Sikh People.
Sikhs firmly believe in sewa (service to community and God) and simran (remembrance of God), the two tenets of Sikh life. The list of prominent Sikhs in humanitarian activities include Bhai Kanhaiya (1648–1718), Bhagat Puran Singh (1904-1992), Bhai Trilochan Singh Panesar (1937-2010).
According to Sewa Singh Kalsi, the Sikh people have gained a reputation through history for being sturdy, hardworking and adventurous; they are a people who have earned the reputation for being extremely brave and loyal soldiers. They have also become known for being a militant people.
Beginning in 1968, Yogi Bhajan (later of the 3HO movement) began to teach classes kundalini yoga, resulting in a number of non-Punjabi converts to Sikhism (known as white Sikhs) in the United States. Since then, thousands of non-Punjabis have taken up the Sikh belief and lifestyle primarily in the United States, Canada, Latin America, the Far East and Australia.
Since 2010, the Sikh Directory has organized The Sikh Awards, the first Sikh award ceremony in the world.
Sikhism and the caste system.
Although the Sikh Gurus and Sikh principals discourage the use of the caste system, it is still prevalent in some aspects of Indian Sikh society. According to Sunrinder S, Jodhka, the Sikh religion does not permit discrimination against any caste or creed, however, in practice, Sikhs belonging to the landowning dominant castes have not shed all their prejudices against the dalits, also known as the "lower castes". In some rural villages, while dalits would be allowed entry into the village gurudwaras, they would not be permitted to cook or serve langar (Communal meal). Therefore, wherever they could mobilise resources, the dalits of Punjab have tried to construct their own gurudwara and other local level institutions in order to attain a certain degree of religious autonomy.
Through a policy of affirmative action, in the Shiromani Gurdwara Prabandhak Committee, 20 of the 140 seats are reserved for low-caste Sikhs.
Over 60% of Sikhs traditionally belong to the Jat caste, which is a powerful and land owning agrarian caste. Despite being very small in numbers, the typically wealthy mercantile Khatri and Arora castes wield considerable influence within the Sikh community. Other Indian castes, common in Sikhism, include the Ramgarhias (artisans), the Ahluwalias (formerly Kalals [brewers] and the two Dalit castes, known in Punjabi terminology as the Mazhabis (the Chuhras) and the Ramdasias (the Chamars).
Sikh Diaspora.
Worldwide, there are 25.8 million Sikhs, which makes up 0.39% of the world's population. Approximately 75% of Sikhs live in the Punjab, where they constitute about 60% of the state's population. Large communities of Sikhs live in the neighboring states such as Indian State of Haryana which is home to the second largest Sikh population in India with 1.1 million Sikhs as per 2001 census, and large communities of Sikhs can be found across India. However, Sikhs only comprise about 2% of the Indian population.
Sikh migration to Canada began in the 19th century and led to the creation of significant Sikh communities, predominantly in South Vancouver, British Columbia, Surrey, British Columbia, and Brampton, Ontario. Today temples, newspapers, radio stations, and markets cater to these large, multi-generational Indo-Canadian groups. Sikh festivals such as Diwali and Vaisakhi are celebrated in those Canadian cities by the largest groups of followers in the world outside of the Punjab.
Sikhs also migrated to East Africa, West Africa, the Middle East, Southeast Asia, the United Kingdom as well as United States and Australia. These communities developed as Sikhs migrated out of Punjab to fill in gaps in imperial labour markets. In the early twentieth century a significant community began to take shape on the west coast of the United States. Smaller populations of Sikhs are found within many countries in Western Europe, Mauritius, Malaysia, Fiji, Nepal, China, Pakistan, Afghanistan, Iraq, Singapore, Mexico, the United States and many other countries.
Prohibitions in Sikhism.
There are a number of religious prohibitions in Sikhism based on commitment to the faith. Observant or orthodox Sikhs, known as Amritdhari or Khalsa Sikhs are strictly required to observe the following prohibitions.
Prohibitions include:

</doc>
<doc id="28024" url="http://en.wikipedia.org/wiki?curid=28024" title="Sontaran">
Sontaran

The Sontarans are a fictional extraterrestrial race of humanoids from the British science fiction television series "Doctor Who", and also seen in spin-off series "The Sarah Jane Adventures". A warrior race who live to kill, they are characterised by their ruthlessness and fearlessness of death.
They were created by writer Robert Holmes. During rehearsals for their first appearance, actor Kevin Lindsay, who portrayed the original Sontaran, Linx, pronounced the race's name as ""son-TAR-an"." Alan Bromly, the director, tried to correct him by saying it should be pronounced with the stress on the first syllable. Lindsay declared "Well, I think it's "son-TAR-an", and since I'm from the f**king place, I should know." His preferred pronunciation was retained.
Culture.
The Sontarans are a race of humanoids with a stocky build, greenish brown skin, a distinctive dome-shaped head, and they have only three fingers on each hand. Their musculature is designed for load-bearing rather than leverage, because of the significant amount of gravity on their home planet. Ross Jenkins in "The Sontaran Stratagem" describes a Sontaran as resembling "a talking baked potato". Sontarans come from a large, dense planet named Sontar in the "southern spiral arm of the galaxy" which has a very strong gravitational field, which explains their compact stocky form. They are far stronger than humans, and in the recent series are shorter than the average human male.
The Sontarans have an extremely militaristic and warlike culture; every aspect of their society is geared toward warfare, and every experience is viewed in terms of its martial relevance. In "The Sontaran Experiment", the Fourth Doctor comments that "Sontarans never do anything without a military reason." In fact, to die heroically in battle is their ultimate goal. Aside from a ritualistic chant in "The Sontaran Strategem"/"The Poison Sky", they are never seen to engage in any activity that would be considered recreation, though a few offhand comments by Commander Skorr in "The Poison Sky" suggest they do consider hunting a sport. According to their creator Robert Holmes, Sontarans do have a highly developed artistic culture, but have put it on hold for the duration of the war, while the opening chapter of the novelisation of "The Time Warrior", based on Holmes' incomplete draft, refers to Linx listening to the Sontaran anthem while his spaceship is in flight.
The Sontarans depicted in the series have detached, smug personalities, and a highly developed sense of honour; on multiple occasions, the Doctor has used his knowledge of their pride in their species to manipulate them. In "The Sontaran Stratagem", the Doctor nevertheless referred to them as "the finest soldiers in the galaxy".
Although physically formidable, the Sontarans' weak spot is the "probic vent" at the back of their neck, through which they draw nutrition. It is also part of their cloning process. It provides incentive to continue moving forward in battle since retreat would expose this area to their enemies. They have been killed by targeting that location with a knife ("The Invasion of Time"), a screwdriver (""), and an arrow ("The Time Warrior"). Even something as simple as a squash ball aimed at that point ("The Sontaran Stratagem") or contact by the heel of a shoe ("The Last Sontaran") is capable of incapacitating them temporarily. They are also vulnerable to "coronic acid" ("The Two Doctors"). While the Sontaran wear protective helmets in battle, to fight without their helmets, or to be "open-skinned," is an honour for the Sontaran.
In the episode "The Poison Sky", it is revealed that the Sontaran Empire have been at war with the Rutan Host for more than 50,000 years, and which, at a time around 2008, they are losing. The war is still raging at least 20,000 years later, in the serial "The Sontaran Experiment".
All the Sontarans depicted in the television series have monosyllabic names, many beginning with an initial 'st' sound (e.g. Styre ("The Sontaran Experiment"), Stor ("The Invasion of Time"), Stike ("The Two Doctors"), Staal ("The Sontaran Stratagem"), Skorr ("The Sontaran Stratagem"), Stark ("The Pandorica Opens"), and Strax ("A Good Man Goes To War"); exceptions are Linx ("The Time Warrior"), Varl ("The Two Doctors"), Jask ("The End of Time"), and Kaagh ("The Sarah Jane Adventures")). Subdivisions of the Sontaran military structure mentioned in the series include the Sontaran G3 Military Assessment Survey and the Grand Strategic Council, the Ninth Sontaran Battle Group, the Fifth Army Space Fleet of the Sontaran Army Space Corps, and the Tenth Sontaran Battle Fleet. Military titles include Commander, Group Marshal, Field Major, and General. Agnomens include "the Undefeated", "the Bloodbringer", "the Avenger" and "the Slayer".
Reproduction.
The Sontarans reproduce by means of cloning rather than sexual reproduction, and thus for the most part are extremely similar in appearance. Human characters in both "The Sontaran Experiment" and "The Sontaran Stratagem" comment on how closely individual Sontarans resemble one another; however, it should be noted that their height, skin tone, facial features, vocal timbre and accent, hair, spacing of teeth and even number of fingers have varied from story to story, and sometimes within stories. When Luke Rattigan asks how they can tell each other apart in "The Sontaran Stratagem", General Staal remarks that they say the same of humans.
In "The Time Warrior", Linx states that "at the Sontaran Military Academy we have hatchings of a million cadets at each muster parade." The Doctor also comments in "The Invasion of Time" that Sontarans can mass-clone themselves at rates up to a million embryos every four minutes. Thereafter the clones take just ten minutes to grow to adulthood. When the Sontaran reach adulthood, under the charge of Sontaran High Command, each warrior is immediately given a rank and dispatched on a battle mission. From day one, the Sontarans are sent to battle.
Sontarans reproduce asexually and all the Sontarans depicted in the television series are of one gender; referred to with masculine pronouns, however it is not known if they possess distinctly male physiologies. General Staal comments that "words are the weapons of womenfolk" and that the clone of Martha Jones performed well "for a female" as commentary on the gender inequalities of other species. This typifies a Sontaran trait: interested only in the strongest fighters in any group or race. Despite this, Strax appeared perfectly comfortable with the prospect of wearing dresses in "The Battle of Demon's Run - Two Days Later"; he ultimately dressed in human gentleman's attire, nevertheless. In "The Time Warrior", when Linx examines Sarah Jane, he comments on how the human reproduction system is 'inefficient' and that humans 'should change it'. As multiple genders are foreign to them, Sontarans are known to confuse human sexes; Strax routinely addresses young women as "Boy" and vice versa. and claims not to have known that River Song was a woman.
In "The Sontaran Stratagem", the Sontarans are seen to create human clones by growing them in tubs of green fluid. "Enemy of the Bane" confirms that Sontarans are cloned in the same way. In a human clone, the umbilical corresponds to the probic vent on the back of a Sontaran's neck, suggesting that the vent is not unlike the human navel, albeit clearly more complex.
Technology.
The Tenth Sontaran Battle Fleet in the new series consists of a Command Ship and a number of capsules that can be moved into position when Battle Status is enjoined. Sontaran ships are impervious to nuclear missiles. In both the classic and new series, Sontarans are depicted using spherical or semi-spherical single-occupant spacecraft known as capsules. Each capsule is small enough to avoid detection by radar and is piloted by an individual Sontaran. "The Sontaran Stratagem" also saw the introduction of a large mothership from which the small Sontaran capsules could be seen to originate. The Doctor notes that the one ship by itself is enough to completely wipe out Earth.
The Sontarans have a variety of weapons. Their trade-mark weapon is a small rod with two handles and a plunger at one end, giving it a syringe style. This is so it can be held and fired using three fingers. This weapon fires a disabling beam that can temporarily render a person useless and emits an energy pulse that can repair systems like the teleport, and has appeared in every Sontaran story except "The Sontaran Experiment". When first used by Commander Linx in "The Time Warrior", it shows the ability to fire a beam which can disarm by knocking the weapon out of the wielder's hand, hypnotise, as well as cutting through wood, disabling limbs and killing.
In "The Sontaran Experiment", Field Major Styre instead used a small red laser pistol which only killed (although it did not kill the Doctor, because of a small metal plate the Doctor had been keeping in his inside pocket). "The Invasion of Time" saw Commander Stor using the small rod again, but also in episode six, a Sontaran trooper uses a short black rifle-like laser to try to burn through a lock on a door inside the TARDIS. "The Two Doctors" introduced a weapon called the Meson Gun (as named in the Jim'll Fix It Sketch, "A Fix with Sontarans"), a large silver rifle with a red fuel tank in the centre which was used by Group Marshal Stike and Varl in the third episode. It seemed to be some kind of flame-thrower as it fired a jet of flames very briefly. Group Marshal Stike was also seen carrying a baton.
It would not be until "The Sontaran Stratagem" that General Staal would show that the baton can fire an orange beam that could stun the target. In "The Poison Sky", Commander Skorr and his troops carry large laser rifles into battle. These rifles are the Sontaran gun of the Tenth Sontaran Battle Fleet. Each rifle has a laser beam that kills instantly and is designed for a three-fingered grip. In "The Invasion of Time", their armour is shown to be resistant to Time Lord stasers and K-9's blaster. However, their armour is vulnerable to standard human firearms in "The Poison Sky", but the Sontarans in that episode used a 'cordolane signal' which caused the copper-lined bullets to expand, jamming most firearms instantly. UNIT troops overcame this by switching to steel-lined bullets.
"The Sarah Jane Adventures" story "The Last Sontaran" showed further technological advancements of the modern Sontarans. Commander Kaagh, a surviving pilot from the tenth Sontaran battle fleet, had slightly different armour due to being from the special forces. His suit featured no gloves, so his bare hands were visible. And on his left arm was a control panel for his suit and ship. His helmet could fold up and retract and both his suit and ship featured cloaking devices, turning them both invisible. While the soldiers of the tenth fleet were armed with large laser rifles, Kaagh has a smaller laser carbine. Rather than hypnotising humans (as Sarah pointed out they usually do), instead, Kaagh fixed neural control devices to the back of the necks of his human agents. A red light flashes when it is operational, and Kaagh can activate and deactivate them when he wants with his control panel.
A pair of Sontarans that tried to invade Trenzalore in "Time of the Doctor" used a two-man craft with an invisibility field.
Appearances.
Television.
The Sontarans made their first appearance in 1973 in the serial "The Time Warrior" by Robert Holmes, where a Sontaran named Linx is stranded in the Middle Ages. Another Sontaran named Styre appears in "The Sontaran Experiment", experimenting on captured astronauts on a future Earth. They later appear in "The Invasion of Time", where they successfully invade Gallifrey, but are driven out again after less than a day. They appear for the final time in the original series in "The Two Doctors". The Sontarans also appeared in a skit for the BBC children's programme "Jim'll Fix It" titled "A Fix with Sontarans", along with Colin Baker as the Sixth Doctor and Janet Fielding as Tegan Jovanka. Though the two species are never seen together, several references are made in Sontaran episodes to the Rutan Host, an equally militaristic race with whom the Sontarans have been at war for thousands of years. The Rutans appear in the serial "Horror of Fang Rock".
Sporting an updated design, Sontarans returned to the revived series in the series 4 (2008) episodes "The Sontaran Strategem" and "The Poison Sky". The Sontarans plan to terraform the Earth into a new clone world, but their plans are averted by the Tenth Doctor (David Tennant). It is also revealed that the race was excluded from the Time War of the revived series' backstory. In "Turn Left", the same events are depicted in a parallel universe, where through exposition describes their plan as foiled by Torchwood (characters from the spin-off show of that name), at the cost of their lives, with Torchwood leader Jack Harkness being captured by the Sontarans. In "The Stolen Earth", UNIT is revealed to have developed a teleportation device based on Sontaran technology. A lone survivor from the events of "The Poison Sky", Kaagh (Anthony O'Donnell), next appears in "The Last Sontaran", from spin-off series "The Sarah Jane Adventures". Kaagh appears again in "Enemy of the Bane". In "Doctor Who"‍ '​s "The End of Time, Part Two" (2010), a Sontaran sniper (Dan Starkey) briefly appears pursuing the Doctor's former companions Mickey Smith (Noel Clarke) and his wife Martha Jones (Freema Agyeman), but is defeated by the Doctor before he can assassinate them. Alongside the Eleventh Doctor (Matt Smith), Sontarans battle fleets are seen in series five (2010) finale episode "The Pandorica Opens", as part of an alliance of the Doctor's enemies. Series 6 episode "A Good Man Goes to War" (2011) introduces Strax (Starkey), a Sontaran nurse who has been assigned this role as a means of making penance. He fights on the side of the Doctor and his allies, which include the Silurian warrior Vastra (Neve McIntosh) and her lover Jenny (Catrin Stewart). Strax appears again in Christmas special "The Snowmen" (2012), now serving as Vastra and Jenny's butler, and assists them in their capacity as Victorian era detectives. This trio were featured also in the episodes "The Crimson Horror", "The Name of the Doctor" (both 2013) and "Deep Breath" (2014). A troop of Sontarans are also seen as invaders of the planet Trenzalore in the 2013 Christmas special "The Time of the Doctor".
Games.
The origins of the Sontarans have not been revealed in the television series. The "Doctor Who" role-playing game published by FASA claimed that they were all descended from the genetic stock of General Sontar (or Sontaris), who used newly developed bioengineering techniques to clone millions of duplicates of himself and annihilated the non-clone population. He renamed the race after himself and turned the Sontarans into an expansionist and warlike society set on universal conquest. However, this origin has no basis in anything seen in the television series.
The Sontarans have also appeared as a character in the PC game "Destiny of the Doctors" released on 5 December 1997, by BBC Multimedia. They can be defeated by firing the occupants of an angry beehive at them.
The Sontarans appear in the "" episode, "The Gunpowder Plot".
Other appearances.
Big Finish Productions first used the Sontarans for their audio drama "Heroes of Sontar" - a 2011 Fifth Doctor story. They next featured in The Five Companions and were stuck in an alternative version of the Death Zone with the Fifth Doctor and various companions. In 2012, "The First Sontarans" was released. A Sixth Doctor Lost Story from the mid-1980s, written by Andrew Smith, it features the Sontarans and the Rutans on nineteenth century Earth, tracking down a scientist named Jacob, who escaped through time and space. It is revealed that Jacob is from Sontar, and was responsible for genetically creating the Sontarans as a defence against a Rutan invasion. They were first developed on Sontar's gravity-heavy moon and quickly proved themselves to be at least on par with the unstoppable Rutan horde. However, believing themselves to be superior, the Sontarans turned on their creators, conquering the planet Sontar and changing it to suit their biology.
Other appearances by the Sontarans include the spin-off videos "Mindgame", "" and "Do You Have A License To Save This Planet?"; three audio plays by BBV: "Silent Warrior", "Old Soldiers" and "Conduct Unbecoming"; the Faction Paradox audio "The Shadow Play"; and a cameo appearance in "Infidel's Comet". "Shakedown" marks the only occasion in which the Sontarans and their Rutan foes appear on screen together, and was adapted into a Virgin New Adventures novel.
They have also appeared in several spin-off novels, including "Lords of the Storm" by David A. McIntee and "The Infinity Doctors" by Lance Parkin. In "The Infinity Doctors", the Doctor negotiated a peace between the Sontarans and the Rutan Host when two of them were left trapped in a TARDIS for several hours and got to talking due to their inability to kill each other. General Sontar also made an appearance in that novel. In "The Crystal Bucephalus" by Craig Hinton, the name of their planet was given as Sontara. The Sontarans also briefly appear in "The Eight Doctors", sent to the Eye of Orion by an agent of the Celestial Intervention Agency to kill the Fifth and Eighth Doctors.
In 1982, Jean Airey's novella "The Doctor and the Enterprise" featured a crossover between the universes of "Doctor Who" and "Star Trek", in which the Fourth Doctor finds himself on the USS "Enterprise". The "Enterprise" is attacked by a Sontaran fleet (which is unrecognizable to Captain Kirk and crew), prompting the Doctor to urgently warn the crew to flee the area.
They appear in 2009, in the novella "The Sontaran Games" by Jacqueline Rayner, featuring the Tenth Doctor and appeared in the New Series Adventures (Doctor Who) book "The Taking of Chelsea 426" by David Llewellyn, featuring the Tenth Doctor, fighting both times against the Rutan Host.
In 2008, as part of Character options first series 4 2008 wave of action figures, they released some Sontaran action figures. These include General Staal, Commander Skorr and several Sontaran soldiers.
The Sontarans are mentioned in the audio book Wraith World, when Clyde Langer remarks he cannot understand why Luke and Rani would want to read about made up adventures, when they have faced Sontarans.
Comic books.
The Sontarans have also appeared several times in the "Doctor Who Magazine" comic strip, both as adversaries of the Doctor and in strips not involving the Doctor. In "The Outsider" (DWM #25-26), by Steve Moore and David Lloyd, a Sontaran named Skrant invaded the world of Brahtilis with the unwitting help of Demimon, a local astrologer. The Fourth Doctor faced the Sontarans in "Dragon's Claw" (DWM #39-#45), by Steve Moore and Dave Gibbons, where a crew of Sontarans menaced China in 1522 AD.
In Steven Moffat's short story (the basis for the Tenth Doctor episode "Blink"), the Ninth Doctor has a rooftop sword fight with two Sontarans in 21st century Istanbul, defeating them with the help of spy Sally Sparrow, apparently before the events of "Rose" in his personal timeline.
The Sontaran homeworld was destroyed in the future during the events of the Seventh Doctor strip "Pureblood" (DWM #193-196) but the Sontaran race pool survived, allowing for further cloning; the strip introduced the concept of "pureblood" Sontarans not born of cloning. The Sontarans also feature in the Kroton solo strip "Unnatural Born Killers" (DWM #277) and the Tenth Doctor's comic strip debut "The Betrothal of Sontar" (DWM #365-#368), by John Tomlinson and Nick Abadzis, where a Sontaran mining rig on the ice planet Serac comes under attack by a mysterious force.

</doc>
<doc id="28064" url="http://en.wikipedia.org/wiki?curid=28064" title="Financial assistance following the September 11 attacks">
Financial assistance following the September 11 attacks

Charities and relief agencies raised over $657 million in the three weeks following the September 11, 2001 attacks, the vast bulk going to immediate survivors and victims' families.
Government assistance.
In the morning hours of September 21, 2001, the Congress approved a bill to aid the airline industry and establish a federal fund for victims. The cost of the mostly open-ended fund reached $7 billion (the average payout was $1.8 million per family). Victims of earlier terrorist attacks, including those linked to al-Qaida, were not included in the fund—nor were those who would not surrender the right to hold the airlines legally responsible.
American Red Cross.
From the donations to the Emergency Relief Fund, as of 19 November 2001, the American Red Cross granted 3,165 checks to 2,776 families totaling $54.3 million.
172,612 cases were referred to mental health contacts. The 866-GET INFO number received 29,820 calls. As of 3:10 p.m. November 20, 2001, there had been 1,592,295 blood donations since September 11.
"Fire Donations" took charitable contributions on behalf of firefighters, EMS, and rescue workers.
Emergency supplies.
On Thursday and Friday, September 14–15 September 2001, various relief supplies for the World Trade Center relief effort were collected from the New York City area, and dropped off at the Javits Convention Center or at a staging area at Union Square. By Saturday morning, enough supplies (and volunteers) were collected.
Memorial funds.
Many families and friends of victims have set up memorial funds and projects to give back to their communities and change the world in honor of their loved ones' lives. Examples include:

</doc>
<doc id="28082" url="http://en.wikipedia.org/wiki?curid=28082" title="Timeline for October following the September 11 attacks">
Timeline for October following the September 11 attacks

This article summarizes the events in October 2001 that were related to the September 11 attacks. All times, except where otherwise noted, are in Eastern Daylight Time (EDT), or .

</doc>
<doc id="28117" url="http://en.wikipedia.org/wiki?curid=28117" title="SAC">
SAC

SAC or Sac may refer to:

</doc>
<doc id="28130" url="http://en.wikipedia.org/wiki?curid=28130" title="Sign">
Sign

A sign is an object, quality, event, or entity whose presence or occurrence indicates the probable presence or occurrence of something else. A natural sign bears a causal relation to its object—for instance, thunder is a sign of storm, or medical symptoms signify a disease. A conventional sign signifies by agreement, as a full stop signifies the end of a sentence; similarly the words and expressions of a language, as well as bodily gestures, can be regarded as signs, expressing particular meanings. The physical objects most commonly referred to as signs (notices, road signs, etc., collectively known as signage) generally inform or instruct using written text, symbols, pictures or a combination of these.
The philosophical study of signs and symbols is called semiotics; this includes the study of semiosis, which is the way in which signs (in the semiotic sense) operate.
Nature.
Semiotics, epistemology, logic, and philosophy of language are concerned about the nature of signs, what they are and how they signify. The nature of signs and symbols and significations, their definition, elements, and types, is mainly established by Aristotle, Augustine, and Aquinas. According to these classic sources, significance is a relationship between two sorts of things: signs and the kinds of things they signify (intend, express or mean), where one term necessarily causes something else to come to the mind. Distinguishing natural signs and conventional signs, the traditional theory of signs (Augustine) sets the following threefold partition of things:
all sorts of indications, evidences, symptoms, and physical signals, there are signs which are "always" signs (the entities of the mind as ideas and images, thoughts and feelings, constructs and intentions); and there are signs that "have" to get their signification (as linguistic entities and cultural symbols). So, while natural signs serve as the source of signification, the human mind is the agency through which signs signify naturally occurring things, such as objects, states, qualities, quantities, events, processes, or relationships. Human language and discourse, communication, philosophy, science, logic, mathematics, poetry, theology, and religion are only some of fields of human study and activity where grasping the nature of signs and symbols and patterns of signification may have a decisive value.
Types.
A sign can denote any of the following:
Christianity.
St. Augustine and Signs.
St. Augustine was the first man who synthesized the classical and Hellenistic theories of signs. For him a sign is a thing which is used to signify other things and to make them come to mind ("De Doctrina Christiana" [hereafter DDC] 1.2.2; 2.1.1). The most common signs are spoken and written words (DDC 1.2.2; 2.3.4-2.4.5). Although God cannot be fully expressible, Augustine gave emphasis to the possibility of God’s communication with humans by signs in Scripture (DDC 1.6.6). Augustine endorsed and developed the classical and Hellenistic theories of signs. Among the main stream in the theories of signs, i.e., that of Aristotle and that of Stoics, the former theory filtered into the works of Cicero (106-43 BC, "De inventione rhetorica" 1.30.47-48) and Quintilian (circa 35-100, "Institutio Oratoria" 5.9.9-10), which regarded the sign as an instrument of inference. In his commentary on Aristotle’s "De Interpretatione", Ammonius said, “according to the division of the philosopher Theophrastus, the relation of speech is twofold, first in regard to the audience, to which speech signifies something, and secondly in regard to the things about which the speaker intends to persuade the audience.” If we match DDC with this division, the first part belongs to DDC Book IV and the second part to DDC Books I-III. Augustine, although influenced by these theories, advanced his own theological theory of signs, with whose help one can infer the mind of God from the events and words of Scripture. 
Books II and III of DDC enumerate all kinds of signs and explain how to interpret them. Signs are divided into natural ("naturalia") and conventional ("data"); the latter is divided into animal ("bestiae") and human ("homines"); the latter is divided into non-words ("cetera") and words ("verba"); the latter is divided into spoken words ("voces") and written words ("litterae"); the latter is divided into unknown signs ("signa ignota") and ambiguous signs ("signa ambigua"); both the former and the latter are divided respectively into particular signs ("signa propria") and figurative signs ("signa translata"), among which the unknown figurative signs belong to the pagans. 
In addition to exegetical knowledge (Quintilian, "Institutio Oratoria" 1.4.1-3 and 1.8.1-21) which follows the order of reading ("lectio"), textual criticism ("emendatio"), explanation ("enarratio"), and judgment ("iudicium"), one needs to know the original language (Hebrew and Greek) and broad background information on Scripture (DDC 2.9.14-2.40.60).
Augustine’s understanding of signs includes several hermeneutical presuppositions as important factors. First, the interpreter should proceed with humility, because only a humble person can grasp the truth of Scripture (DDC 2.41.62). Second, the interpreter must have a spirit of active inquiry and should not hesitate to learn and use pagan education for the purpose of leading to Christian learning, because all truth is God’s truth (DDC 2.40.60-2.42.63). Third, the heart of interpreter should be founded, rooted, and built up in love which is the final goal of the entire Scriptures (DDC 2.42.63).
The sign does not function as its own goal, but its purpose lies in its role as a signification ("res significans", DDC 3.9.13). God gave signs as a means to reveal himself; Christians need to exercise hermeneutical principles in order to understand that divine revelation. Even if the Scriptural text is obscure, it has meaningful benefits. For the obscure text prevents us from falling into pride, triggers our intelligence (DDC 2.6.7), tempers our faith in the history of revelation (DDC 3.8.12), and refines our mind to be suitable to the holy mysteries (DDC 4.8.22). When interpreting signs, the literal meaning should first be sought, and then the figurative meaning (DDC 3.10.14-3.23.33). Augustine suggests the hermeneutical principle that the obscure Scriptural verse is interpreted with the help of plain and simple verses, which formed the doctrine of “scriptura scripturae interpres” (Scripture is the Interpreter of Scripture) in the Reformation Era. Moreover, he introduces the seven rules of Tyconius the Donatist to interpret the obscure meaning of the Bible, which demonstrates his understanding that all truth belongs to God (DDC 3.3.42-3.37.56). In order to apply Augustine’s hermeneutics of the sign appropriately in modern times, every division of theology must be involved and interdisciplinary approaches must be taken.

</doc>
<doc id="28132" url="http://en.wikipedia.org/wiki?curid=28132" title="Sidehill gouger">
Sidehill gouger

Sidehill gougers are North American folkloric creatures adapted to living on hillsides by having legs on one side of their body shorter than the legs on the opposite side. This peculiarity allows them to walk on steep hillsides, although only in one direction; when lured or chased into the plain, they are trapped in an endless circular path. The creature is variously known as the Sidehill Ousel, Gyascutus, Sidewinder, Wampus, Gudaphro, Hunkus, Rickaboo Racker, Prock, Gwinter, or Cutter Cuss.
Sidehill gougers are herbivorous mammals who dwell in hillside burrows, and are occasionally depicted as laying eggs. There are usually 6 to 8 pups to a litter. Since the gouger is footed for hillsides, it cannot stand up on level ground. If by accident a gouger falls from a hill, it can easily be captured or starve to death. 
When a clockwise gouger meets a counter-clockwise gouger, they have to fight to the death since they can only go in one direction.
Gougers are said to have migrated to the west from New England, a feat accomplished by a pair of gougers who clung to each other in a fashion comparable to "a pair of drunks going home from town" with their longer legs on the outer sides. 
A Vermont variation is known as the Wampahoofus. It was reported that farmers crossbreed them with their cows so they could graze easily on mountain sides. There is also a similar mythical creature in France known as the dahu.
Frank C. Whitmore and Nicholas Hotton, in their joint tongue-in-cheek response to an article "Fantastic Animals" ("Smithsonian Magazine", 1972), expounded the taxonomy of sidehill gougers ("Membriinequales declivitous"), noting in particular "the sidehill dodger, which inhabits the Driftless Area of Wisconsin; the dextrosinistral limb ratio approaches unity although the metapodials on the downhill side are noticeably stouter."

</doc>
<doc id="28133" url="http://en.wikipedia.org/wiki?curid=28133" title="Strike">
Strike

Strike may refer to:

</doc>
<doc id="28136" url="http://en.wikipedia.org/wiki?curid=28136" title="Slovak language">
Slovak language

Slovak (Slovak: "slovenský jazyk", ]; "slovenčina" ]; not to be confused with "slovenski jezik" or "slovenščina", the native names of the Slovene language) is an Indo-European language that belongs to the West Slavic languages (together with Czech, Polish, Silesian, Kashubian, and Sorbian).
Slovak is the official language of Slovakia where it is spoken by approximately 5.51 million people (2014). Slovak speakers are also found in the United States, the Czech Republic, Argentina, Serbia, Ireland, Romania, Poland, Canada, Hungary, Croatia, the United Kingdom, Australia, Austria, and Ukraine.
Orthography.
Slovak uses the Latin script with small modifications that include the four diacritics (ˇ, ´, ¨, ˆ) placed above certain letters.
The primary principle of Slovak spelling is the phonemic principle. The secondary principle is the morphological principle: forms derived from the same stem are written in the same way even if they are pronounced differently. An example of this principle is the assimilation rule (see below). The tertiary principle is the etymological principle, which can be seen in the use of "i" after certain consonants and of "y" after other consonants, although both "i" and "y" are pronounced the same way.
Finally, the rarely applied grammatical principle is present when, for example, the basic singular form and plural form of masculine adjectives are written differently with no difference in pronunciation (e.g. pekný = nice – singular versus pekní = nice – plural).
In addition, the following rules are present:
Most foreign words receive Slovak spelling immediately or after some time. For example, "weekend" is spelled "víkend", "software" – "softvér", "gay" – "gej" (both not exclusively), and "quality" is spelled "kvalita" (possibly from Italian "qualità"). Personal and geographical names from other languages using Latin alphabets keep their original spelling unless a fully Slovak form of the name exists (e.g. "Londýn" for "London").
Slovak features some heterophonic homographs (words with identical spelling but different pronunciation and meaning), the most common examples being "krásne" /ˈkraːsne/ (beautiful) versus "krásne" /ˈkraːsɲe/ (beautifully).
Syntax.
The main features of Slovak syntax are as follows:
Some examples include the following:
Word order in Slovak is relatively free, since strong inflection enables the identification of grammatical roles (subject, object, predicate, etc.) regardless of word placement. This relatively free word order allows the use of word order to convey topic and emphasis.
Some examples are as follows:
The unmarked order is subject–verb–object. Variation in word order is generally possible, but word order is not completely free.
In the above example, the noun phrase "ten veľký muž" cannot be split up, so that the following combinations are not possible:
And the following are stylistically not correct:
Morphology.
Articles.
Slovak does not have articles. The demonstrative pronoun "ten" (fem: "tá", neuter: "to") may be used in front of the noun in situations where definiteness must be made explicit.
Nouns, adjectives, pronouns.
Slovak nouns are inflected for case and number. There are six cases: nominative, genitive, dative, accusative, locative, and instrumental. The vocative is no longer morphologically marked. There are two numbers: singular and plural. Nouns have inherent gender. There are three genders: masculine, feminine, and neuter. Adjectives agree with nouns in case, number, and gender.
Numerals.
The numerals 0–10 have unique forms. 11–19 are formed by the numeral plus "násť." Compound numerals (21, 1054) are combinations of these words formed in the same order as their mathematical symbol is written (e.g. 21 = dvadsaťjeden, literally "twenty one").
The numerals are as follows:
(1) jeden (jedno (neuter), jedna (feminine)),
(2) dva (dve (neuter, feminine)),
(3) tri,
(4) štyri,
(5) päť,
(6) šesť,
(7) sedem,
(8) osem,
(9) deväť,
(10) desať, (11) jedenásť, (12) dvanásť, (13) trinásť, (14) štrnásť, (15) pätnásť, (16) šestnásť, (17) sedemnásť, (18) osemnásť, (19) devätnásť, (20) dvadsať, (21) dvadsaťjeden... (30) tridsať, (31) tridsaťjeden... (40) štyridsať... (50) päťdesiat... (60) šesťdesiat... (70) sedemdesiat... (80) osemdesiat... (90) deväťdesiat... (100) sto, (101) stojeden... (200) dvesto... (300) tristo... (900)deväťsto... (1,000) tisíc... (1,100) tisícsto... (2,000) dvetisíc... (100,000) stotisíc... (200,000) dvestotisíc... (1,000,000) milión... (1,000,000,000) miliarda...
Counted nouns have two forms. The most common form is the plural genitive (e.g. "päť domov" = five houses or "stodva žien" = one hundred two women), while the plural form of the noun when counting the amounts of 2, 3, 4, etc., is the nominative form without counting (e.g. "dva domy" = two houses or "dve ženy" = two women).
Verbs.
Verbs have three major conjugations. Three persons and two numbers (singular and plural) are distinguished. Several conjugation paradigms exist as follows:
Adverbs.
Adverbs are formed by replacing the adjectival ending with the ending -o or -e/-y. Sometimes both -o and -e are possible. Examples include the following:
The comparative/superlative of adverbs is formed by replacing the adjectival ending with a comparative/superlative ending -(ej)ší or -(ej)šie. Examples include the following:
Prepositions.
Each preposition is associated with one or more grammatical cases. The noun governed by a preposition must appear in the case required by the preposition in the given context (e.g. from friends = od priateľov). Priateľov is the genitive case of priatelia. It must appear in this case because the preposition od (=from) always calls for its objects to be in the genitive.
Po has a different meaning depending on the case of its governed noun.
Relationships to other languages.
The Slovak language is a descendant of Proto-Slavic, itself a descendant of Proto-Indo-European. It is closely related to the other West Slavic languages, primarily to Czech. It has been also influenced by German, English, Latin and Hungarian.
Czech.
Although most dialects of Czech and Slovak are mutually intelligible (see Comparison of Slovak and Czech), eastern Slovak dialects are less intelligible to speakers of Czech; they differ from Czech and from other Slovak dialects, and mutual contact between speakers of Czech and speakers of the eastern dialects is limited.
Since the dissolution of Czechoslovakia it has been allowed to use Czech in TV broadcasting and—like any other language of the world—during court proceedings (Administration Procedure Act 99/1963 Zb.). From 1999 to August 2009, the Minority Language Act 184/1999 Z.z., in its section (§) 6, contained the variously interpreted unclear provision saying that "When applying this act, it holds that the use of the Czech language fulfills the requirement of fundamental intelligibility with the state language" ; the state language is Slovak and the Minority Language Act basically refers to municipalities with more than 20% ethnic minority population (no such Czech municipalities are found in Slovakia). Since 1 September 2009 (due to an amendment to the State Language Act 270/1995 Z.z.) a language "fundamentally intelligible with the state language" (i.e. the Czech language) may be used in contact with state offices and bodies by its native speakers, and documents written in it and issued by bodies in the Czech Republic are officially accepted. Regardless of its official status, Czech is used commonly both in Slovak mass media and in daily communication by Czech natives as an equal language.
Czech and Slovak have a long history of interaction and mutual influence well before the creation of Czechoslovakia in 1918. Literary Slovak shares significant orthographic features with Czech, as well as technical and professional terminology dating from the Czechoslovak period, but phonetic, grammatical, and vocabulary differences do exist.
Other Slavic languages.
Slavic language varieties tend to be closely related, and have had a large degree of mutual influence, due to the complicated ethnopolitical history of their historic ranges. This is reflected in the many features Slovak shares with neighboring language varieties. Standard Slovak shares high degrees of mutual intelligibility with many Slavic varieties. Despite this closeness to other Slavic varieties, significant variation exists among Slovak dialects. In particular, eastern varieties differ significantly from the standard language, which is based on central and western varieties.
Eastern Slovak dialects have the greatest degree of mutual intelligibility with Rusyn of all the Slovak dialects, but both lack technical terminology and upper register expressions. Polish and Sorbian also differ quite considerably from Czech and Slovak in upper registers, but non-technical and lower register speech is readily intelligible. Some mutual intelligibility occurs with spoken Rusyn, Ukrainian, and even Russian (in this order), although their orthographies are based on the Cyrillic script.
Slovak also exhibits numerous linguistic traits that set it apart from the other West Slavic languages. The Central Slovak dialect, upon which the Standard Slovak language is based, shares various features with South Slavic languages, most notably with the Kajkavian dialect of Serbo-Croatian and with Slovene language.
English.
weekend – víkend, football – futbal, ham & eggs – hemendex, offside – ofsajd, out (football) – aut,
body check (hockey) – bodyček, couch – gauč
German.
German loanwords include "coins," Slovak "mince", German "Münze"; "to wish", Slovak "vinšovať" (colloquially, the standard term is "želať"), German "wünschen"; "funfair," Slovak "jarmok ", German "Jahrmarkt" and "color," Slovak "farba", German "Farbe".
Hungarian.
Hungarians and Slovaks have had a language interaction ever since the settlement of Hungarians in the Carpathian area. Hungarians adopted many words from various Slavic languages related to agriculture and administration, and a number of Hungarian loanwords are found in Slovak. Some examples are as follows:
Romanian.
Romanian words entered the Slovak language in the course of the so-called "Wallachian colonization" in the 14th–16th century when sheep breeding became common in Slovak mountains. Many of today's Slovak rustic-pastoral words like "bača" ("shepherd"; Rmn. "baci"), "valach" ("young shepherd"; cf. the dated exonym for Romanians, "Valach"), "magura" ("hill"; Rmn. "măgura"), "koliba"("chalet"; Rmn. "coliba"), "bryndza" (a variety of sheep cheese; Rmn. "brânză"), "striga" ("witch", "demon"; Rmn. "strigă/strigoi"), etc. were introduced into the Slovak language by Romanian shepherds during the Late Middle Ages and the Early Modern Times. The Romanian influence is most strongly felt in the dialects of the Moravian Wallachia region.
Dialects.
There are many Slovak dialects, which are divided into the following four basic groups:
The fourth group of dialects is often not considered a separate group, but a subgroup of Central and Western Slovak dialects (see e.g. Štolc, 1968), but it is currently undergoing changes due to contact with surrounding languages (Serbo-Croatian, Romanian, and Hungarian) and long-time geographical separation from Slovakia (see the studies in "Zborník Spolku vojvodinských slovakistov", e.g. Dudok, 1993).
For an external map of the three groups in Slovakia see .
The dialect groups differ mostly in phonology, vocabulary, and tonal inflection. Syntactic differences are minor. Central Slovak forms the basis of the present-day standard language. Not all dialects are fully mutually intelligible. It may be difficult for an inhabitant of the Slovak capital Bratislava (in western Slovakia) to understand a dialect from eastern Slovakia.
The dialects are fragmented geographically, separated by numerous mountain ranges. The first three groups already existed in the 10th century. All of them are spoken by the Slovaks outside Slovakia (USA, Canada, Croatian Slavonia, and elsewhere), and central and western dialects form the basis of the lowland dialects (see above).
The western dialects contain features common with the Moravian dialects in the Czech Republic, the southern central dialects contain a few features common with South Slavic languages, and the eastern dialects a few features common with Polish and the East Slavonic languages (cf. Štolc, 1994). Lowland dialects share some words and areal features with the languages surrounding them (Serbo-Croatian, Hungarian, and Romanian).

</doc>
<doc id="28165" url="http://en.wikipedia.org/wiki?curid=28165" title="Sharable Content Object Reference Model">
Sharable Content Object Reference Model

Sharable Content Object Reference Model (SCORM) is a collection of standards and specifications for web-based electronic educational technology (also called e-learning). It defines communications between client side content and a host system (called "the run-time environment"), which is commonly supported by a learning management system. SCORM also defines how content may be packaged into a transferable ZIP file called "Package Interchange Format."
SCORM is a specification of the Advanced Distributed Learning (ADL) Initiative from the Office of the United States Secretary of Defense.
SCORM 2004 introduced a complex idea called sequencing, which is a set of rules that specifies the order in which a learner may experience content objects. In simple terms, they constrain a learner to a fixed set of paths through the training material, permit the learner to "bookmark" their progress when taking breaks, and assure the acceptability of test scores achieved by the learner. The standard uses XML, and it is based on the results of work done by AICC, IMS Global, IEEE, and Ariadne.
SCORM versions.
SCORM 1.1.
SCORM 1.1 is the first production version. It used a Course Structure Format XML file based on the AICC specifications to describe content structure, but lacked a robust packaging manifest and support for metadata. Quickly abandoned in favor of SCORM 1.2.
SCORM 1.2.
This was the first version that was widely used. It is still widely used and is supported by most Learning Management Systems.
SCORM 2004.
This is the current version. It is based on new standards for API and content object-to-runtime environment communication, with many ambiguities of previous versions resolved. Includes ability to specify adaptive sequencing of activities that use the content objects. Includes ability to share and use information about the success status for multiple learning objectives or competencies across content objects and across courses for the same learner within the same learning management system. A more robust test suite helps ensure good interoperability.
Experience API (Tin Can API).
The Experience API (also known as xAPI or Tin Can API) was finalized to version 1.0 in April 2013. The Experience API solves many of the problems inherent with older versions of SCORM. Just like SCORM, ADL is the steward of the Experience API. AICC with their CMI-5 planned to use xAPI as their transport standard, but AICC membership decided to dissolve the organization and transferred CMI-5 to ADL.
The Experience API (Tin Can API) is a web service that allows software clients to read and write experiential data in the form of “statement” objects. In their simplest form, statements are in the form of “I did this”, or more generally “actor verb object”. More complex statement forms can be used. There is also a built-in query API to help filter recorded statements, and a state API that allows for a sort of “scratch space” for consuming applications. Experience API statements are stored in a data store called a Learning Record Store, which can exist on its own or within a Learning Management System.

</doc>
<doc id="28167" url="http://en.wikipedia.org/wiki?curid=28167" title="Sejm">
Sejm

The Sejm of the Republic of Poland (; Polish: "Sejm Rzeczypospolitej Polskiej") is the lower house of the Polish parliament. It consists of 460 deputies (posłowie, literally "envoys", in Polish) elected by universal ballot and is presided over by a speaker called the "Marshal of the "Sejm" of the Republic of Poland" ("Marszałek Sejmu Rzeczypospolitej Polskiej").
In the Kingdom of Poland, "Sejm" referred to the entire three-chamber parliament of Poland, comprising the lower house (the Chamber of Envoys; Polish: "Izba Poselska"), the upper house (the Senate; Polish: "Senat") and the King. It was thus a three-estate parliament. Since the Second Polish Republic (1918–1939), "Sejm" has referred only to the lower house of the parliament; the upper house is called the "Senat Rzeczypospolitej Polskiej" ("Senate of the Republic of Poland").
History.
Kingdom of Poland.
"Sejm" stems from an Old Slavic word meaning "gathering". Its origin was the King's Councils ("wiece"), which gained power during the time of Poland's fragmentation (1146–1295). The 1182 "Sejm" in Łęczyca (known as the "First Sejm") was the most notable of these councils, in that for the first time in Poland's history it established laws constraining the power of the ruler. It forbade arbitrary sequestration of supplies in the countryside and takeover of bishopric lands after the death of a bishop. However, these early "Sejm"s were not a regular event and were formed only at the King's behest.
After the 1493 "Sejm" in Piotrków, it became a regularly convening body, to which indirect elections were held every two years. The bicameral system was also established there. The "Sejm" now comprised two chambers: the "Senat" (Senate) of 81 bishops and other dignitaries; and the Chamber of Envoys, made up of 54 envoys elected by smaller local "sejmik" (assemblies of landed nobility) in each of the Kingdom's provinces. At the time, Poland's nobility, which accounted for around 10% of the state's population (then the highest amount in Europe), was becoming particularly influential, and with the eventual development of the Golden Liberty, the "Sejm"'s powers increased dramatically.
Polish-Lithuanian Commonwealth.
Over time, the number of envoys in the lower chamber grew in number and power as they pressed the king for more privileges. The "Sejm" eventually became even more active in supporting the goals of the privileged classes when the King ordered that the landed nobility and their estates (peasants) be drafted into military service. After the Union of Lublin in 1569, the Kingdom of Poland became, through personal union with the Grand Duchy of Lithuania, the Polish-Lithuanian Commonwealth, and thus the "Sejm" was supplemented with new envoys from among the Lithuanian nobility. This "Commonwealth of Both Nations" ensured that the state of affairs surrounding the three-estates system continued, with the "Sejm", Senate and King forming the estates and supreme deliberating body of the state. In the first few decades of the 16th century, the Senate had established its precedence over the "Sejm"; however, from the mid-1500s onwards, the "Sejm" became a very powerful representative body of the "Szlachta" ("middle nobility"). Soon, the "Sejm" began to limit the king's powers severely. Its chambers reserved the final decisions in legislation, taxation, budget, and treasury matters (including military funding), foreign affairs, and the confirment of nobility. The 1573 Warsaw Confederation saw the nobles of the "Sejm" officially sanction and guarantee religious tolerance in Commonwealth territory, ensuring a refuge for those fleeing the ongoing Reformation and Counter-Reformation wars in Europe.
Until the end of the 16th century, unanimity was not required, and the majority-voting process was the most commonly used system for voting. Later, with the rise of the Polish magnates and their increasing power, the unanimity principle was re-introduced with the institution of the nobility's right of "liberum veto" (Latin: "I freely forbid"). Additionally, if the envoys were unable to reach a unanimous decision within six weeks (the time limit of a single session), deliberations were declared void and all previous acts passed by that "Sejm" were annulled. From the mid-17th century onward, any objection to a "Sejm" resolution, by either an envoy or a senator, automatically caused the rejection of other, previously approved resolutions. This was because all resolutions passed by a given session of the "Sejm" formed a whole resolution, and, as such, was published as the annual "constituent act" of the "Sejm", e.g. the ""Anno Domini" 1667" act. In the 16th century, no single person or small group dared to hold up proceedings, but, from the second half of the 17th century, the "liberum veto" was used to virtually paralyze the "Sejm", and brought the Commonwealth to the brink of collapse. The "liberum veto" was finally abolished with the adoption of Poland's 3rd May Constitution in 1791, a piece of legislation which was passed as the "Government Act", and for which the "Sejm" required four years to propagate and adopt. The constitution's acceptance, and the possible long-term consequences it may have had, is arguably the reason for which the powers of Austria-Hungary, Russia and Prussia then decided to partition the Polish-Lithuanian Commonwealth, thus putting an end to over 300 years of Polish parliamentary continuity.
It is estimated that between 1493 and 1793, a "Sejm" was held 240 times, the total debate-time sum of which was 44 years.
Partitions.
After the fall of the Duchy of Warsaw, which existed as a Napoleonic client state between 1807 and 1815, and its short-lived "Sejm" of the Duchy of Warsaw, the "Sejm" of Congress Poland was established in the "Kongresówka" (Congress Poland) of Russia; it was composed of the king (the Russian emperor), the upper house (Senate), and the lower house (Chamber of Envoys).
Overall, during the period from 1795 until the re-establishment of Poland's sovereignty in 1918, little power was actually held by any Polish legislative body and the occupying powers of Russia, Prussia (later united Germany) and Austria-Hungary propagated legislation for their own respective formerly-Polish territories at a national level.
Congress Poland.
The Chamber of Envoys, despite its name, consisted not only of 77 envoys (sent by local assemblies) from the hereditary nobility, but also of 51 deputies, elected by the non-noble population. All deputies were covered by Parliamentary immunity, with each individual serving for a term of office of six years, with half of the deputies being elected every two years. Candidates for deputy had to be able to read and write, and have a certain amount of wealth. The legal voting age was 21, except for those citizens serving in the military, the personnel of which were not allowed to vote. Parliamentary sessions were initially convened every two years, and lasted for (at least) 30 days. However, after many clashes between liberal deputies and conservative government officials, sessions were later called only four times (1818, 1820, 1826, and 1830, with the last two sessions being secret).
The "Sejm" had the right to call for votes on civil and administrative legal issues, and, with permission from the king, it could also vote on matters related to the fiscal policy and the military. It had the right to exercise control over government officials, and to file petitions. The 64-member Senate on the other hand, was composed of "voivodes" and "kasztelans" (both types of provincial governors), Russian "princes of the blood", and nine bishops. It acted as the Parliamentary Court, had the right to control "citizens' books", and had similar legislative rights as did the Chamber of Deputies.
Germany and Austria-Hungary.
In the Free City of Cracow (1815–1846), a unicameral Assembly of Representatives was established, and from 1827, a unicameral provincial "sejm" existed in the Grand Duchy of Poznań. Poles were elected to and represented the majority in both of these legislatures; however, they were largely powerless institutions and exercised only very limited power. After numerous failures in securing legislative sovereignty in the early 19th century, many Poles simply gave up trying to attain a degree of independence from their foreign master-states. In the Austrian partition, a relatively powerless "Sejm" of the Estates operated until the time of the Spring of Nations. After this, in the mid to late 19th century, only in autonomous Galicia (1861–1914) was there a unicameral and functional National "Sejm", the "Sejm" of the Land. It is recognised today as having played a major and overwhelming positive role in the development of Polish national institutions.
In the second half of the 19th century, Poles were able to become members of the parliaments of Austria, Prussia and Russia, where they formed Polish Clubs. Deputies of Polish nationality were elected to the Prussian "Landtag" from 1848, and then to the German Empire's "Reichstag" from 1871. Polish Deputies were members of the Austrian State Council (from 1867), and from 1906 were also elected to the Russian Imperial State "Duma" (lower chamber) and to the State Council (upper chamber).
Second Republic.
After the First World War and re-establishment of Polish independence, the convocation of parliament, under the democratic electoral law of 1918, became an enduring symbol of the new state's wish to demonstrate and establish continuity with the 300 year Polish parliamentary traditions established before the time of the partitions. Maciej Rataj emphatically paid tribute to this with the phrase: "There is Poland there, and so is the "Sejm".
During the interwar period of Poland's independence, the first Legislative "Sejm" of 1919, a Constituent Assembly, passed the Small Constitution of 1919, which introduced a parliamentary republic and proclaimed the principle of the "Sejm"’s sovereignty. This was then strengthened, in 1921, by the March Constitution, one of the most democratic European constitutions enacted after the end of World War I. The constitution established a political system which was based on Montesquieu's doctrine of separation of powers, and which restored the bicameral "Sejm" consisting of a lower house (to which alone the name of "Sejm"" was from then on applied) and an upper house, the Senate. In 1919, Roza Pomerantz-Meltzer, a member of the Zionist party, became the first woman ever elected to the "Sejm".
The legal content of the March Constitution allowed for "Sejm" supremacy in the system of state institutions at the expense of the executive powers, thus creating a parliamentary republic out of the Polish state. An attempt to strengthen executive powers in 1926 (through the August Amendment) proved too limited and largely failed in helping avoid legislative grid-lock which had ensued as a result of too-great parliamentary power in a state which had numerous diametrically-opposed political parties sitting in its legislature. In 1935, the parliamentary republic was weakened further when, by way of, Józef Piłsudski's May Coup, the president was forced to sign the April Constitution of 1935, an act through which the head of state assumed the dominant position in legislating for the state and the Senate increased its power at the expense of the "Sejm".
On 2 September 1939, the "Sejm" held its final pre-war session, during which it declared Poland's readiness to defend itself against invading German forces. On 2 November 1939, the President dissolved the "Sejm" and the Senate, which were then, according to plan, to resume their activity within two months after the end of the Second World War; this, however, never happened. During wartime, the National Council (1939–1945) was established to represent the legislature as part of the Polish Government in Exile. Meanwhile, in Nazi-occupied Poland, the Council of National Unity was set up; this body functioned from 1944 to 1945 as the parliament of the Polish Underground State. With the cessation of hostilities in 1945, and subsequent rise to power of the Communist-backed Provisional Government of National Unity, the Second Polish Republic legally ceased to exist.
Polish People's Republic.
The "Sejm" in the Polish People's Republic had 460 deputies throughout most of its history. At first, this number was declared to represent one deputy per 60,000 citizens (425 were elected in 1952), but, in 1960, as the population grew, the declaration was changed: The constitution then stated that the deputies were representative "of" the people and could be recalled "by" the people, but this article was never used, and, instead of the "five-point electoral law", a non-proportional, "four-point" version was used. Legislation was passed with majority voting.
The "Sejm" voted on the budget as well as on the periodic national plans that were a fixture of communist economies. The "Sejm" deliberated in sessions that were ordered to convene by the State Council.
The "Sejm" also chose a "Prezydium" ("presiding body") from among its members; the marshall of which was always a member of the United People's Party. In its preliminary session, the "Sejm" also nominated the Prime Minister, the Council of Ministers of Poland, and members of the State Council. It also chose many other government officials, including the head of the Supreme Chamber of Control and members of the State Tribunal and the Constitutional Tribunal, as well as the Ombudsman (the last three bodies of which were created in the 1980s).
The Senate of Poland was abolished by the Polish people's referendum in 1946, after which the "Sejm" became the sole legislative body in Poland. Even though the "Sejm" was largely subservient to the Communist party, it is worth noting that a single brave deputy, Romuald Bukowski (Independent) voted against the imposition of martial law in 1982.
Today.
After the end of communism in 1989, the Senate was reinstated as the upper house of a bicameral national assembly, while the "Sejm" became the lower house. The "Sejm" is now composed of 460 deputies elected by proportional representation every four years.
Between 7 and 19 deputies are elected from each constituency using the d'Hondt method (with one exception, in 2001, when the Sainte-Laguë method was used), their number being proportional to their constituency's population. Additionally, a threshold is used, so that candidates are chosen only from parties that gained at least 5% of the nationwide vote (candidates from ethnic-minority parties are exempt from this threshold).

</doc>
<doc id="28207" url="http://en.wikipedia.org/wiki?curid=28207" title="Short Message Service">
Short Message Service

Short Message Service (SMS) is a text messaging service component of phone, Web, or mobile communication systems. It uses standardized communications protocols to allow fixed line or mobile phone devices to exchange short text messages.
SMS was the most widely used data application, with an estimated 3.5 billion active users, or about 80% of all mobile phone subscribers at the end of 2010. The term "SMS" is used for both the user activity and all types of short text messaging in many parts of the world. SMS is also employed in direct marketing, known as SMS marketing. As of September 2014, global SMS messaging business is said to be worth over USD 100 billion, and SMS accounts for almost 50 percent of all the revenue generated by mobile messaging.
SMS as used on modern handsets originated from radio telegraphy in radio memo pagers using standardized phone protocols. These were defined in 1985 as part of the Global System for Mobile Communications (GSM) series of standards<ref name="GSM 28/85">GSM Doc 28/85 "Services and Facilities to be provided in the GSM System" rev2, June 1985</ref> as a means of sending messages of up to 160 characters to and from GSM mobile handsets. Though most SMS messages are mobile-to-mobile text messages, support for the service has expanded to include other mobile technologies, such as ANSI CDMA networks and Digital AMPS, as well as satellite and landline networks.
History.
Initial concept.
Adding text messaging functionality to mobile devices began in the early 1980s. The first action plan of the CEPT Group GSM was approved in December 1982, requesting that, "The services and facilities offered in the public switched telephone networks and public data networks ... should be available in the mobile system." This plan included the exchange of text messages either directly between mobile stations, or transmitted via message handling systems in use at that time.
The SMS concept was developed in the Franco-German GSM cooperation in 1984 by Friedhelm Hillebrand and Bernard Ghillebaert. The GSM is optimized for telephony, since this was identified as its main application. The key idea for SMS was to use this telephone-optimized system, and to transport messages on the signalling paths needed to control the telephone traffic during periods when no signalling traffic existed. In this way, unused resources in the system could be used to transport messages at minimal cost. However, it was necessary to limit the length of the messages to 128 bytes (later improved to 160 seven-bit characters) so that the messages could fit into the existing signalling formats. Based on his personal observations and on analysis of the typical lengths of postcard and Telex messages, Hillebrand argued that 160 characters was sufficient to express most messages succinctly.
SMS could be implemented in every mobile station by updating its software. Hence, a large base of SMS-capable terminals and networks existed when people began to use SMS. A new network element required was a specialized short message service centre, and enhancements were required to the radio capacity and network transport infrastructure to accommodate growing SMS traffic.
Early development.
The technical development of SMS was a multinational collaboration supporting the framework of standards bodies. Through these organizations the technology was made freely available to the whole world.
The first proposal which initiated the development of SMS was made by a contribution of Germany and France into the GSM group meeting in February 1985 in Oslo. This proposal was further elaborated in GSM subgroup WP1 Services (Chairman Martine Alvernhe, France Telecom) based on a contribution from Germany. There were also initial discussions in the subgroup WP3 network aspects chaired by Jan Audestad (Telenor). The result was approved by the main GSM group in a June '85 document which was distributed to industry. The input documents on SMS had been prepared by Friedhelm Hillebrand (Deutsche Telekom) with contributions from Bernard Ghillebaert (France Télécom). The definition that Friedhelm Hillebrand and Bernard Ghillebaert brought into GSM called for the provision of a message transmission service of alphanumeric messages to mobile users "with acknowledgement capabilities". The last three words transformed SMS into something much more useful than the prevailing messaging paging that some in GSM might have had in mind.
SMS was considered in the main GSM group as a possible service for the new digital cellular system. In GSM document "Services and Facilities to be provided in the GSM System," both mobile-originated and mobile-terminated short messages appear on the table of GSM teleservices.
The discussions on the GSM services were concluded in the recommendation GSM 02.03 "TeleServices supported by a GSM PLMN." Here a rudimentary description of the three services was given:
The material elaborated in GSM and its WP1 subgroup was handed over in Spring 1987 to a new GSM body called IDEG (the Implementation of Data and Telematic Services Experts Group), which had its kickoff in May 1987 under the chairmanship of Friedhelm Hillebrand (German Telecom). The technical standard known today was largely created by IDEG (later WP4) as the two recommendations GSM 03.40 (the two point-to-point services merged) and GSM 03.41 (cell broadcast).
WP4 created a Drafting Group Message Handling (DGMH), which was responsible for the specification of SMS. Finn Trosby of Telenor chaired the draft group through its first 3 years, in which the design of SMS was established. DGMH had five to eight participants, and Finn Trosby mentions as major contributors Kevin Holley, Eija Altonen, Didier Luizard and Alan Cox. The first action plan mentions for the first time the Technical Specification 03.40 "Technical Realisation of the Short Message Service". Responsible editor was Finn Trosby. The first and very rudimentary draft of the technical specification was completed in November 1987. However, drafts useful for the manufacturers followed at a later stage in the period. A comprehensive description of the work in this period is given in.
The work on the draft specification continued in the following few years, where Kevin Holley of Cellnet (now Telefónica O2 UK) played a leading role. Besides the completion of the main specification GSM 03.40, the detailed protocol specifications on the system interfaces also needed to be completed.
Support in other architectures.
The Mobile Application Part (MAP) of the SS7 protocol included support for the transport of Short Messages through the Core Network from its inception. MAP Phase 2 expanded support for SMS by introducing a separate operation code for Mobile Terminated Short Message transport. Since Phase 2, there have been no changes to the Short Message operation packages in MAP, although other operation packages have been enhanced to support CAMEL SMS control.
From 3GPP Releases 99 and 4 onwards, CAMEL Phase 3 introduced the ability for the Intelligent Network (IN) to control aspects of the Mobile Originated Short Message Service, while CAMEL Phase 4, as part of 3GPP Release 5 and onwards, provides the IN with the ability to control the Mobile Terminated service. CAMEL allows the gsmSCP to block the submission (MO) or delivery (MT) of Short Messages, route messages to destinations other than that specified by the user, and perform real-time billing for the use of the service. Prior to standardized CAMEL control of the Short Message Service, IN control relied on switch vendor specific extensions to the Intelligent Network Application Part (INAP) of SS7.
Early implementations.
The first SMS message was sent over the Vodafone GSM network in the United Kingdom on 3 December 1992, from Neil Papworth of Sema Group (now Mavenir Systems) using a personal computer to Richard Jarvis of Vodafone using an Orbitel 901 handset. The text of the message was "Merry Christmas."
The first commercial deployment of a short message service center (SMSC) was by Aldiscon part of Logica (now part of Acision) with Telia (now TeliaSonera) in Sweden in 1993, followed by Fleet Call (now Nextel) in the US, Telenor in Norway and BT Cellnet (now O2 UK) later in 1993. All first installations of SMS gateways were for network notifications sent to mobile phones, usually to inform of voice mail messages. The first commercially sold SMS service was offered to consumers, as a person-to-person text messaging service by Radiolinja (now part of Elisa) in Finland in 1993. Most early GSM mobile phone handsets did not support the ability to send SMS text messages, and Nokia was the only handset manufacturer whose total GSM phone line in 1993 supported user-sending of SMS text messages.
Initial growth was slow, with customers in 1995 sending on average only 0.4 messages per GSM customer per month. One factor in the slow takeup of SMS was that operators were slow to set up charging systems, especially for prepaid subscribers, and eliminate billing fraud which was possible by changing SMSC settings on individual handsets to use the SMSCs of other operators. Initially, networks in the UK only allowed customers to send messages to other users on the same network, limiting the usefulness of the service. This restriction was lifted in 1999.
Over time, this issue was eliminated by switch billing instead of billing at the SMSC and by new features within SMSCs to allow blocking of foreign mobile users sending messages through it. By the end of 2000, the average number of messages reached 35 per user per month, and on Christmas Day 2006, over 205 million messages were sent in the UK alone.
It is also alleged that the fact that roaming customers, in the early days, rarely received bills for their SMSs after holidays abroad which gave a boost to text messaging as an alternative to voice calls.
Text messaging outside GSM.
SMS was originally designed as part of GSM, but is now available on a wide range of networks, including 3G networks. However, not all text messaging systems use SMS, and some notable alternative implementations of the concept include J-Phone's "SkyMail" and NTT Docomo's "Short Mail", both in Japan. Email messaging from phones, as popularized by NTT Docomo's i-mode and the RIM BlackBerry, also typically uses standard mail protocols such as SMTP over TCP/IP.
SMS today.
In 2010[ [update]], 6.1 trillion (6.1 × 1012) SMS text messages were sent. This translates into an average of 193,000 SMS per second. SMS has become a huge commercial industry, earning $114.6 billion globally in 2010. The global average price for an SMS message is US$0.11, while mobile networks charge each other interconnect fees of at least US$0.04 when connecting between different phone networks.
In 2015, the actual cost of sending an SMS in Australia was found to be $0.00016 per SMS.
In 2014, developed the world's first SMS-based voter registration system in Libya. So far, more than 1.5 million people have registered using that system, providing Libyan voters with unprecedented access to the democratic process.
While SMS is still a growing market, traditional SMS is becoming increasingly challenged by alternative messaging services such as Facebook Messenger, WhatsApp and Viber available on smart phones with data connections, especially in Western countries where these services are growing in popularity. Enterprise SMS-messaging, also known as application-to-peer messaging (A2P Messaging) or 2-way SMS, continue to grow steadily at a rate of 4% annually. Enterprise SMS applications are primarily focused on CRM and delivering highly targeted service messages such as parcel-delivery alerts, real-time notification of credit/debit card purchase confirmations to protect against fraud, and appointment confirmations. Services have evolved towards "actionable" alerts and notifications, following successful iSMS-type implementations broadly adopted in Finland, first by Finnair, then banks, telecoms, and logistics companies. Another primary source of growing A2P message volumes is two-step verification (alternatively referred to as 2-factor authentication) processes whereby users are delivered a one-time passcode over SMS, and then are asked to enter that passcode online in order to verify their identity.
Technical details.
GSM.
The "Short Message Service—Point to Point (SMS-PP)"—was originally defined in GSM recommendation 03.40, which is now maintained in 3GPP as TS 23.040. GSM 03.41 (now 3GPP TS 23.041) defines the "Short Message Service—Cell Broadcast (SMS-CB)", which allows messages (advertising, public information, etc.) to be broadcast to all mobile users in a specified geographical area.
Messages are sent to a short message service center (SMSC), which provides a "store and forward" mechanism. It attempts to send messages to the SMSC's recipients. If a recipient is not reachable, the SMSC queues the message for later retry. Some SMSCs also provide a "forward and forget" option where transmission is tried only once. Both mobile terminated (MT, for messages sent "to" a mobile handset) and mobile originating (MO, for those sent "from" the mobile handset) operations are supported. Message delivery is "best effort," so there are no guarantees that a message will actually be delivered to its recipient, but delay or complete loss of a message is uncommon, typically affecting less than 5 percent of messages. Some providers allow users to request delivery reports, either via the SMS settings of most modern phones, or by prefixing each message with *0# or *N#. However, the exact meaning of confirmations varies from reaching the network, to being queued for sending, to being sent, to receiving a confirmation of receipt from the target device, and users are often not informed of the specific type of success being reported.
SMS is a stateless communication protocol in which every SMS message is considered entirely independent of other messages. Enterprise applications using SMS as a communication channel for stateful dialogue (where an MO reply message is paired to a specific MT message) requires that session management be maintained external to the protocol through proprietary methods as .
Message size.
Transmission of short messages between the SMSC and the handset is done whenever using the Mobile Application Part (MAP) of the SS7 protocol. Messages are sent with the MAP MO- and MT-ForwardSM operations, whose payload length is limited by the constraints of the signaling protocol to precisely 140 octets (140 octets * 8 bits / octet = 1120 bits). Short messages can be encoded using a variety of alphabets: the default GSM 7-bit alphabet, the 8-bit data alphabet, and the 16-bit UCS-2 alphabet. Depending on which alphabet the subscriber has configured in the handset, this leads to the maximum individual short message sizes of 160 7-bit characters, 140 8-bit characters, or 70 16-bit characters. GSM 7-bit alphabet support is mandatory for GSM handsets and network elements, but characters in languages such as Arabic, Chinese, Korean, Japanese, or Cyrillic alphabet languages (e.g., Ukrainian, Serbian, Bulgarian, etc.) must be encoded using the 16-bit UCS-2 character encoding (see Unicode). Routing data and other metadata is additional to the payload size.
Larger content (concatenated SMS, multipart or segmented SMS, or "long SMS") can be sent using multiple messages, in which case each message will start with a User Data Header (UDH) containing segmentation information. Since UDH is part of the payload, the number of available characters per segment is lower: 153 for 7-bit encoding, 134 for 8-bit encoding and 67 for 16-bit encoding. The receiving handset is then responsible for reassembling the message and presenting it to the user as one long message. While the standard theoretically permits up to 255 segments, 6 to 8 segment messages are the practical maximum, and long messages are often billed as equivalent to multiple SMS messages. Some providers have offered length-oriented pricing schemes for messages, however, the phenomenon is disappearing.
Gateway providers.
SMS gateway providers facilitate SMS traffic between businesses and mobile subscribers, including SMS for enterprises, content delivery, and entertainment services involving SMS, e.g. TV voting. Considering SMS messaging performance and cost, as well as the level of messaging services, SMS gateway providers can be classified as aggregators or SS7 providers.
The aggregator model is based on multiple agreements with mobile carriers to exchange two-way SMS traffic into and out of the operator's SMSC, also known as local termination model. Aggregators lack direct access into the SS7 protocol, which is the protocol where the SMS messages are exchanged. SMS messages are delivered to the operator's SMSC, but not the subscriber's handset; the SMSC takes care of further handling of the message through the SS7 network.
Another type of SMS gateway provider is based on SS7 connectivity to route SMS messages, also known as international termination model. The advantage of this model is the ability to route data directly through SS7, which gives the provider total control and visibility of the complete path during SMS routing. This means SMS messages can be sent directly to and from recipients without having to go through the SMSCs of other mobile operators. Therefore, it is possible to avoid delays and message losses, offering full delivery guarantees of messages and optimized routing. This model is particularly efficient when used in mission-critical messaging and SMS used in corporate communications. Moreover, these SMS gateway providers are providing branded SMS services with masking but after misuse of these gateways most countries's Governments have taken serious steps to block these gateways.
Interconnectivity with other networks.
Message Service Centers communicate with the Public Land Mobile Network (PLMN) or PSTN via Interworking and Gateway MSCs.
Subscriber-originated messages are transported from a handset to a service center, and may be destined for mobile users, subscribers on a fixed network, or Value-Added Service Providers (VASPs), also known as application-terminated. Subscriber-terminated messages are transported from the service center to the destination handset, and may originate from mobile users, from fixed network subscribers, or from other sources such as VASPs.
On some carriers nonsubscribers can send messages to a subscriber's phone using an Email-to-SMS gateway. Additionally, many carriers, including AT&T Mobility, T-Mobile USA, Sprint, and Verizon Wireless, offer the ability to do this through their respective websites.
For example, an AT&T subscriber whose phone number was 555-555-5555 would receive e-mails addressed to 5555555555@txt.att.net as text messages. Subscribers can easily reply to these SMS messages, and the SMS reply is sent back to the original email address. Sending email to SMS is free for the sender, but the recipient is subject to the standard delivery charges. Only the first 160 characters of an email message can be delivered to a phone, and only 160 characters can be sent from a phone.
Text-enabled fixed-line handsets are required to receive messages in text format. However, messages can be delivered to nonenabled phones using text-to-speech conversion.
Short messages can send binary content such as ringtones or logos, as well as Over-the-air programming (OTA) or configuration data. Such uses are a vendor-specific extension of the GSM specification and there are multiple competing standards, although Nokia's Smart Messaging is common. An alternative way for sending such binary content is EMS messaging, which is standardized and not dependent on vendors.
SMS is used for M2M (Machine to Machine) communication. For instance, there is an LED display machine controlled by SMS, and some vehicle tracking companies use SMS for their data transport or telemetry needs. SMS usage for these purposes is slowly being superseded by GPRS services owing to their lower overall cost. GPRS is offered by smaller telco players as a route of sending SMS text to reduce the cost of SMS texting internationally.
AT commands.
Many mobile and satellite transceiver units support the sending and receiving of SMS using an extended version of the Hayes command set, a specific command language originally developed for the Hayes Smartmodem 300-baud modem in 1977.
The connection between the terminal equipment and the transceiver can be realized with a serial cable (e.g., USB), a Bluetooth link, an infrared link, etc. Common AT commands include AT+CMGS (send message), AT+CMSS (send message from storage), AT+CMGL (list messages) and AT+CMGR (read message).
However, not all modern devices support receiving of messages if the message storage (for instance the device's internal memory) is not accessible using AT commands.
Premium-rated short messages.
Short messages may be used normally to provide premium rate services to subscribers of a telephone network.
Mobile-terminated short messages can be used to deliver digital content such as news alerts, financial information, logos, and ring tones. The first premium-rate media content delivered via the SMS system was the world's first paid downloadable ringing tones, as commercially launched by Saunalahti (later Jippii Group, now part of Elisa Grous), in 1998. Initially only Nokia branded phones could handle them. By 2002 the ringtone business globally had exceeded $1 billion of service revenues, and nearly $5 billion by 2008. Today, they are also used to pay smaller payments online—for example, for file-sharing services, in mobile application stores, or VIP section entrance. Outside the online world, one can buy a bus ticket or beverages from ATM, pay a parking ticket, order a store catalog or some goods (e.g., discount movie DVDs), make a donation to charity, and much more.
Premium-rated messages are also used in Donors Message Service to collect money for charities and foundations. DMS was first launched at April 1, 2004, and is very popular in the Czech Republic. For example, the Czech people sent over 1.5 million messages to help South Asia recover from the 2004 Indian Ocean Earthquake.
The Value-added service provider (VASP) providing the content submits the message to the mobile operator's SMSC(s) using an TCP/IP protocol such as the short message peer-to-peer protocol (SMPP) or the External Machine Interface (EMI). The SMSC delivers the text using the normal Mobile Terminated delivery procedure. The subscribers are charged extra for receiving this premium content; the revenue is typically divided between the mobile network operator and the VASP either through revenue share or a fixed transport fee. Submission to the SMSC is usually handled by a third party.
Mobile-originated short messages may also be used in a premium-rated manner for services such as televoting. In this case, the VASP providing the service obtains a short code from the telephone network operator, and subscribers send texts to that number. The payouts to the carriers vary by carrier; percentages paid are greatest on the lowest-priced premium SMS services. Most information providers should expect to pay about 45 percent of the cost of the premium SMS up front to the carrier. The submission of the text to the SMSC is identical to a standard MO Short Message submission, but once the text is at the SMSC, the Service Center (SC) identifies the Short Code as a premium service. The SC will then direct the content of the text message to the VASP, typically using an IP protocol such as SMPP or EMI. Subscribers are charged a premium for the sending of such messages, with the revenue typically shared between the network operator and the VASP. Short codes only work within one country, they are not international.
An alternative to inbound SMS is based on long numbers (international number format, e.g. +44 762 480 5000), which can be used in place of short codes for SMS reception in several applications, such as TV voting, product promotions and campaigns. Long numbers work internationally, allow businesses to use their own numbers, rather than short codes, which are usually shared across many brands. Additionally, long numbers are nonpremium inbound numbers.
Threaded SMS.
Threaded SMS is a visual styling orientation of SMS message history that arranges messages to and from a contact in chronological order on a single screen. It was first invented by a developer working to implement the SMS client for the BlackBerry, who was looking to make use of the blank screen left below the message on a device with a larger screen capable of displaying far more than the usual 160 characters, and was inspired by threaded Reply conversations in email. Visually, this style of representation provides a back-and-forth chat-like history for each individual contact. Hierarchical-threading at the conversation-level (as typical in blogs and on-line messaging boards)is not widely supported by SMS messaging clients. This limitation is due to the fact that there is no session identifier or subject-line passed back and forth between sent and received messages in the header data (as specified by SMS protocol) from which the client device can properly thread an incoming message to a specific dialogue, or even to a specific message within a dialogue. Most smart phone text-messaging-clients are able to create some contextual threading of "group messages" which narrows the context of the thread around the common interests shared by group members. On the other hand, advanced enterprise messaging applications which push messages from a remote server often display a dynamically changing reply number (multiple numbers used by the same sender), which is used along with the sender's phone number to create session-tracking capabilities analogous to the functionality that cookies provide for web-browsing. As one pervasive example, this technique is used to extend the functionality of many Instant Messenger (IM) applications such that they are able to communicate over two-way dialogues with the much larger SMS user-base. In cases where multiple reply numbers are used by the enterprise server to maintain the dialogue, the visual conversation threading on the client may be separated into multiple threads.
Application-to-Person (A2P) SMS.
While SMS reached its popularity as a person-to-person messaging, another type of SMS is growing fast: application-to-person (A2P) messaging. A2P is a type of SMS sent from a subscriber to an application or sent from an application to a subscriber. It is commonly used by financial institutions, airlines, hotel booking sites, social networks, and other organizations sending SMS from their systems to their customers. According to research in 2011, A2P traffic is growing faster than P2P messaging traffic.
Satellite phone networks.
All commercial satellite phone networks except ACeS and OptusSat support SMS. While early Iridium handsets only support incoming SMS, later models can also send messages. The price per message varies for different networks. Unlike some mobile phone networks, there is no extra charge for sending international SMS or to send one to a different satellite phone network. SMS can sometimes be sent from areas where the signal is too poor to make a voice call.
Satellite phone networks usually have web-based or email-based SMS portals where one can send free SMS to phones on that particular network.
Unreliability.
Unlike dedicated texting systems like the Simple Network Paging Protocol and Motorola's ReFLEX protocol, SMS message delivery is not guaranteed, and many implementations provide no mechanism through which a sender can determine whether an SMS message has been delivered in a timely manner. SMS messages are generally treated as lower-priority traffic than voice, and various studies have shown that around 1% to 5% of messages are lost entirely, even during normal operation conditions, and others may not be delivered until long after their relevance has passed. The use of SMS as an emergency notification service in particular has been starkly criticized.
Vulnerabilities.
The Global Service for Mobile communications (GSM), with the greatest worldwide number of users, succumbs to several security vulnerabilities. In the GSM, only the airway traffic between the Mobile Station (MS) and the Base Transceiver Station (BTS) is optionally encrypted with a weak and broken stream cipher (A5/1 or A5/2). The authentication is unilateral and also vulnerable. There are also many other security vulnerabilities and shortcomings. Such vulnerabilities are inherent to SMS as one of the superior and well-tried services with a global availability in the GSM networks. SMS messaging has some extra security vulnerabilities due to its store-and-forward feature, and the problem of fake SMS that can be conducted via the Internet. When a user is roaming, SMS content passes through different networks, perhaps including the Internet, and is exposed to various vulnerabilities and attacks. Another concern arises when an adversary gets access to a phone and reads the previous unprotected messages.
In October 2005, researchers from Pennsylvania State University published an analysis of vulnerabilities in SMS-capable cellular networks. The researchers speculated that attackers might exploit the open functionality of these networks to disrupt them or cause them to fail, possibly on a nationwide scale.
SMS spoofing.
The GSM industry has identified a number of potential fraud attacks on mobile operators that can be delivered via abuse of SMS messaging services. The most serious threat is SMS Spoofing, which occurs when a fraudster manipulates address information in order to impersonate a user that has roamed onto a foreign network and is submitting messages to the home network. Frequently, these messages are addressed to destinations outside the home network—with the home SMSC essentially being "hijacked" to send messages into other networks.
The only sure way of detecting and blocking spoofed messages is to screen incoming mobile-originated messages to verify that the sender is a valid subscriber and that the message is coming from a valid and correct location. This can be implemented by adding an intelligent routing function to the network that can query originating subscriber details from the HLR before the message is submitted for delivery. This kind of intelligent routing function is beyond the capabilities of legacy messaging infrastructure. While there is no way to entirely prevent spoofed messages, enterprise applications can verify origination of MO reply messages (for example, confirming a purchase made on a credit card) using the .
Limitation.
In an effort to limit telemarketers who had taken to bombarding users with hordes of unsolicited messages India introduced new regulations in September 2011, including a cap of 3,000 SMS messages per subscriber per month, or an average of 100 per subscriber per day. Due to representations received from some of the service providers and consumers, TRAI (Telecom Regulatory Authority of India) has raised this limit to 200 SMS messages per SIM per day in case of prepaid services, and up to 6,000 SMS messages per SIM per month in case of postpaid services with effect from 1 November 2011. However, it was ruled unconstitutional by the Delhi high court, but there are some limitations.
Flash SMS.
A Flash SMS is a type of SMS that appears directly on the main screen without user interaction and is not automatically stored in the inbox. It can be useful in emergencies, such as a fire alarm or cases of confidentiality, as in delivering one-time passwords.
Silent SMS.
In Germany in 2010 almost half a million "silent SMS" messages were sent by the federal police, customs and the secret service "Verfassungsschutz" (offices for protection of the constitution). These silent messages, also known as "silent TMS", "stealth SMS" or "stealth ping", are used to locate a person and thus to create a complete movement profile. They do not show up on a display, nor trigger any acoustical signal when received. Their primary purpose was to deliver special services of the network operator to any cell phone. The mobile provider, often at the behest of the police, will capture data such as subscriber identification IMSI.

</doc>
<doc id="28399" url="http://en.wikipedia.org/wiki?curid=28399" title="Silesia">
Silesia

Silesia ( or ; Polish: "Śląsk" ; German:    ]; Silesian German: "Schläsing"; Czech: "Slezsko"; Silesian: "Ślůnsk" [ɕlonsk]; Latin: "Silesia") is a region of Central Europe now located mostly in Poland, with small parts in the Czech Republic and Germany. It has about 40,000 km2 and almost 8,000,000 inhabitants. Silesia is located along the Odra river. It consists of Lower Silesia and Upper Silesia. 
The region is rich in mineral and natural resources and includes several important industrial areas. Silesia's largest city is Wrocław (German: "Breslau"). The biggest metropolitan area is the Upper Silesian metropolitan area, the centre of which is Katowice. Parts of the Czech city of Ostrava fall within the borders of Silesia.
Silesia's borders and national affiliation have changed over time, both when it was a hereditary possession of noble houses and after the rise of modern nation-states. The first known states to hold power there were probably those of Greater Moravia at end of the 9th century and Bohemia early in the 10th century. In the 10th century Silesia was incorporated into the early Polish state, and after its division in the 12th century became Piast duchy. In the 14th century it became a constituent part of the Bohemian Crown Lands under the Holy Roman Empire, which passed to the Austrian Habsburg Monarchy in 1526.
Most of Silesia was conquered by Prussia in 1742, later becoming part of the German Empire, the Weimar Republic and Nazi Germany up to 1945. After World War I the easternmost part of this region, i.e. an eastern strip of Upper Silesia, was awarded to Poland by the Entente Powers after rebellions by Silesian Polish people and the Upper Silesian plebiscite. In 1945, after World War II the bulk of Silesia was transferred to Polish jurisdiction by the Potsdam Agreement of the victorious Allied Powers and became part of Poland. The remaining former Austrian parts of Silesia were partitioned to Czechoslovakia, and are today part of the Czech Republic. The small Lusatian strip west of the Oder-Neisse line, which belonged to Silesia since 1815, remained in Germany.
Most inhabitants of Silesia today speak the national languages of their respective countries (Polish, Czech). The population of Upper Silesia is native (with many immigrants from Poland who came in 19th century), while the Lower Silesia was settled by Germans before 1945. There is an ongoing debate whether a local Silesian speech should be considered a Polish dialect or a separate language. There also exists a Silesian German or Lower Silesian language, although due to the near total ethnic cleansing of Lower Silesia's German majority after World War II, this form of German is almost extinct.
Etymology.
The names of Silesia in the different languages most likely share their etymology—Latin and English: "Silesia"; Polish: "Śląsk"; Old Polish: "Ślążsk[o]"; Silesian: "Ślůnsk"; German: "Schlesien"; Silesian German: "Schläsing"; Czech: "Slezsko"; Slovak: "Sliezsko"; Kashubian: "Sląsk"; Upper Sorbian: "Šleska"; Lower Sorbian: "Šlazyńska". The names all relate to the name of a river (now Ślęza) and mountain (Mount Ślęża) in mid-southern Silesia. The mountain is believed to have served as a holy place in prehistoric times.
"Ślęża" is listed as one of the numerous Pre-Indo-European topographic names in the region (see old European hydronymy).
According to some Polish Slavists the name ‘Ślęża’ or ‘Ślęż’ is directly related to the Old Slavic words "ślęg" or "śląg" , which means dampness, moisture or humidity. They disagree with the hypothesis of an origin for the name "Śląsk" from the name of the Silings tribe, an etymology preferred by some German authors.
History.
In the 4th century BC Celts entered Silesia, settling around Mount Ślęża near modern Wrocław, Oława, and Strzelin. Germanic Lugii tribes were first recorded within Silesia in the 1st century. Slavic peoples arrived in the region around the 7th century, and by the early 9th century their settlements had stabilized. Local Slavs started to erect boundary structures like the Silesian Przesieka and the Silesia Walls. The eastern border of Silesian settlement was situated to the west of the Bytom, and east from Racibórz and Cieszyn. East of this line dwelt a closely related Slav tribe, the Vistulans. Their northern border was in the valley of the Barycz river, north of which lived the Polans.
The first known states in Silesia were Greater Moravia and Bohemia. In the 10th century, the Polish ruler Mieszko I of the Piast dynasty incorporated Silesia into the Polish state. During the Fragmentation of Poland, Silesia, as well as the rest of the country, was divided among many independent duchies ruled by various Silesian dukes. During this time, German cultural and ethnic influence increased as a result of immigration from German-speaking parts of the Holy Roman Empire. In 1178, parts of the Duchy of Kraków around Bytom, Oświęcim, Chrzanów and Siewierz were transferred to the Silesian Piasts, although their population was primarily Vistulan and not of Silesian descent.
Between 1289 and 1292 Bohemian king Wenceslaus II became suzerain of some of the Upper Silesian duchies. It wasn't until 1335, however, that Polish kings renounced their hereditary rights to Silesia. The province became part of the Bohemian Crown under the Holy Roman Empire, and passed with that crown to the Habsburg Monarchy of Austria in 1526.
In the 15th century several changes were made to Silesia's borders. Parts of the territories which had been transferred to the Silesian Piasts in 1178 were bought by the Polish kings in the second half of the 15th century (the Duchy of Oświęcim in 1457; the Duchy of Zator in 1494). The Bytom area remained in the possession of the Silesian Piasts, even though it was a part of the Diocese of Kraków. The Duchy of Crossen was inherited by the Margraviate of Brandenburg in 1476, and with the renunciation of King Ferdinand I and the estates of Bohemia in 1538, became an integral part of Brandenburg.
In 1742, most of Silesia was seized by King Frederick the Great of Prussia in the War of the Austrian Succession, becoming the Prussian Province of Silesia; consequently, Silesia became part of the German Empire when it was proclaimed in 1871.
After World War I, Upper Silesia was contested by Germany and the newly independent Second Polish Republic. The League of Nations organized a plebiscite to decide the issue in 1921. It resulted in 60% of votes being cast for Germany and 40% for Poland. Following the third Silesian Uprising (1921), however, the easternmost portion of Upper Silesia (including Katowice), with a majority ethnic Polish population, was awarded to Poland, becoming the Silesian Voivodeship. The Prussian Province of Silesia within Germany was then divided into the provinces of Lower Silesia and Upper Silesia. Meanwhile Austrian Silesia, the small portion of Silesia retained by Austria after the Silesian Wars, was mostly awarded to the new Czechoslovakia (becoming known as Czech Silesia), although most of Cieszyn and territory to the east of it went to Poland (see Zaolzie). 
Polish Silesia was among the first regions invaded during Germany's 1939 attack on Poland. One of the claimed goals of Nazi occupation, particularly in Upper Silesia, was the extermination of those who Nazis viewed as subhuman, namely Jews and ethnic Poles. The Polish and Jewish population of Silesia was subjected to genocide involving ethnic cleansing and mass murder, while German colonists were settled in pursuit of Lebensraum. Two thousand Polish intellectuals, politicians and businessmen were murdered in the "Intelligenzaktion Schlesien" in 1940 as part of a Poland wide Germanization program. Silesia also housed one of the two main wartime centers where medical experiments were conducted on kidnapped Polish children by Nazis.
The Potsdam Conference of 1945 concluded that the Oder-Neisse line would be the official border between Germany and Poland. Millions of Germans in Silesia, either fled or were expelled, and were replaced later by Polish population settled from other regions. Furthermore, the newly formed Polish United Workers' Party created a Ministry of the Recovered Territories that claimed half of the available arable land for state-run collectivized farms. Many Silesian residents not only resented the Germans for their invasion in 1939 and brutality in occupation, but now also the newly formed Polish communist government for their population shifting and interference in agricultural, as well as industrial, affairs.
The administrative division of Silesia within Poland has changed several times since 1945. Since 1999 it has been divided between Lubusz Voivodeship, Lower Silesian Voivodeship, Opole Voivodeship, and Silesian Voivodeship. Czech Silesia is now part of the Czech Republic, forming the Moravian-Silesian Region and the northern part of the Olomouc Region. Germany retains the Silesia-Lusatia region ("Niederschlesien-Oberlausitz" or "Schlesische Oberlausitz") west of the Neisse, which is part of the federal state of Saxony.
Geography.
Most of Silesia is relatively flat, although its southern border is generally mountainous. It is primarily located in a swath running along both banks of the upper and middle Oder (Odra) river, but it extends eastwards to the upper Vistula river. The region also includes many tributaries of the Oder, including the Bóbr (and its tributary the Kwisa), the Barycz and the Nysa Kłodzka. The Sudeten mountains run along most of the southern edge of the region, though at its south-eastern extreme it reaches the Silesian Beskids and Moravian-Silesian Beskids, which belong to the Carpathian range.
Historically, Silesia was bounded to the west by the Kwisa and Bóbr rivers, while the territory west of the Kwisa was in Upper Lusatia (earlier "Milsko"). However, because part of Upper Lusatia was included in the Province of Silesia in 1815, in Germany Görlitz, Niederschlesischer Oberlausitzkreis and neighbouring areas are considered parts of historical Silesia. Those districts, along with Poland's Lower Silesian Voivodeship and parts of Lubusz Voivodeship, make up the geographic region of Lower Silesia.
Silesia has undergone a similar notional extension at its eastern extreme. Historically it extended only as far as the Brynica river, which separates it from Zagłębie Dąbrowskie in the Lesser Poland region. However to many Poles today, Silesia ("Śląsk") is understood to cover all of the area around Katowice, including Zagłębie. This interpretation is given official sanction in the use of the name Silesian Voivodeship ("województwo śląskie") for the province covering this area. In fact the word "Śląsk" in Polish (when used without qualification) now commonly refers exclusively to this area (also called "Górny Śląsk" or Upper Silesia).
As well as the Katowice area, historical Upper Silesia also includes the Opole region (Poland's Opole Voivodeship) and Czech Silesia. Czech Silesia consists of a part of the Moravian-Silesian Region and the Jeseník District in the Olomouc Region.
Natural resources.
Silesia is a resource-rich and populous region.
Since the middle of the 18th century, coal has been mined. The industry grew during German rule and peaked in the 1970s under the People's Republic of Poland. During this period Silesia became one of the world's largest producers of coal, with a record tonnage in 1979. Coal mining declined during the next two decades but has increased again following the end of Communist rule.
There are 41 coal mines in Silesia, most forming part of the Gornoslaskie Zaglebie Weglowe coalfield which lies in the Silesian Upland. The field has an area of about 4,500 km2. Deposits in Lower Silesia have proven to be difficult to exploit and the area's unprofitable mines were closed in 2000. In 2008 an estimated 35 billion tonnes of lignite reserves was found near Legnica, making them some of the largest in the world.
From the fourth century BC iron ore has been mined in the upland areas of Silesia. The same period saw lead, copper, silver and gold mining. Zinc, cadmium, arsenic, and uranium have also been mined in the region. Lower Silesia features large copper mining and processing between the cities of Legnica, Głogów, Lubin and Polkowice.
The region is known for stone quarrying to produce limestone, marl, marble, and basalt.
The region also has a thriving agricultural sector, which produces cereals (wheat, rye, barley, oats, corn), potatoes, rapeseed, sugar beets and others. Milk production is well developed. The Opole Silesia has for decades occupied the top spot in Poland for their indices of effectiveness of agricultural land use.
Mountainous parts of southern Silesia feature many significant and attractive tourism destinations (e.g., Karpacz, Szczyrk, Wisła). Silesia is generally well forested. This is because greenness is generally highly desirable by the local population, particularly in the highly industrialized parts of Silesia.
Demographics.
Modern Silesia is inhabited by Poles, Germans, Czechs and Silesians. The last Polish census of 2002 showed that the Silesians are the largest national minority in Poland, Germans being the second; both groups are located mostly in Upper Silesia. The Czech part of Silesia is inhabited by Czechs, Moravians, Silesians and Poles.
Before the Second World War, Silesia was inhabited mostly by Germans and Poles, with a Czech and Jewish minority. In 1905, a census showed that 75% of the population were Germans and 25% Poles. The German population tended to be based in the urban centres and in the rural areas to the north and west, whilst the Polish population was generally rural and in the east.
Historically, Silesia was about equally split between Protestants and Catholics; in 1890 Catholics made up a slight majority of 53% in the German-ruled part. Protestants were generally concentrated in the larger cities; the population tended to be Catholic the further east one went. After World War II, the religious demographic changed fundamentally as the Germans, many of whom were Protestants, were expelled from the region and mostly Catholic Poles were resettled in the area. Today Silesia is predominantly Catholic, reflecting the general religious makeup of Poland.
Existing from the Twelfth Century Silesia's Jewish community was concentrated around Wrocław and Upper Silesia, numbered 48,003 (1.1% of the population) in 1890, decreasing to 44,985 persons (0.9%) by 1910. In Polish East Upper Silesia the number of Jews was around 90,000-100,000. Historically the community had suffered a number of localised expulsions such as their 1453 expulsion from Wrocław.
After the German invasion of Poland in 1939, following Nazi racial policy, the Jewish population of Silesia was subjected to Nazi genocide with executions performed by Einsatzgruppe z. B.V led by Udo von Woyrsch and Einsatzgruppe I led by Bruno Streckenbach, imprisonment in ghettos and ethnic cleansing to the General Government. In their efforts to exterminate Poles and Jews through murder and ethnic cleansing Nazi established in Silesia province the Auschwitz and Gross-Rosen camps. Expulsions were carried out openly and reported in the local press. Those sent to ghettos would from 1942 be expelled to concentration and work camps. Between 5 May and 17 June, 20,000 Silesian Jews were sent to Birkenau to gas chambers and during August 1942 10,000 to 13,000 Silesian Jews were murdered by gassing at Auschwitz. Most Jews in Silesia were exterminated by the Nazis. After the war Silesia became a major centre for repatriation of Jewish population in Poland which survived Nazi German extermination and in Autumn 1945 there were 15,000 Jews in Lower Silesia mostly Polish Jews returned from territories now belonging to Soviet Union, rising in 1946 to seventy thousand as Jewish survivors from other regions in Poland were relocated.
The majority of Germans fled or were expelled from the present-day Polish and Czech parts of Silesia during and after World War II. From June 1945 to January 1947, 1.77 million Germans were expelled from Lower Silesia, and 310,000 from Upper Silesia. Today, most German Silesians and their descendants live in the territory of the Federal Republic of Germany, many of them in the Ruhr area working as miners, like their ancestors in Silesia. In order to smooth their integration into West German society after 1945, they were placed into officially recognized organizations, like the Landsmannschaft Schlesien, with financing from the federal Western German budget. One of its most notable but controversial spokesmen was the CDU politician Herbert Hupka.
The expulsion of Germans led to widespread underpopulation. The population of the town of Glogau fell from 33,500 to 5,000, and from 1939 to 1966 the population of Wrocław fell by 25%. Attempts to repopulate Silesia proved unsuccessful in the 1940s and 1950s, and it was not until the late 1970s that Silesia's population reached pre-war levels.
Cities.
The following table lists the cities in Silesia with a population greater than 100,000 (2006) 

</doc>
<doc id="28438" url="http://en.wikipedia.org/wiki?curid=28438" title="Skyclad (Neopaganism)">
Skyclad (Neopaganism)

In Wicca and Paganism/Neo-paganism, the term skyclad is used to refer to ritual nudity. Some Wiccan groups, or Traditions, perform some or all of their rituals skyclad. Whilst nudity and the practice of witchcraft have long been associated in the visual arts, this contemporary ritual nudity is typically attributed to either the influence of Gerald Gardner or to a passage from "Aradia, or the Gospel of the Witches", and as such is mainly attributed to the Gardnerian and Aradian covens.
Gardner's "Witchcraft Today" was published in 1954. The book claimed to report on the contemporary practice of Pagan religious witchcraft in England, which had supposedly survived as an underground religion for centuries. Ritual nudity was included as a regular part of Wiccan practice, and remains associated with Gardnerian Wicca. The "Charge of the Goddess", a part of Gardnerian ritual liturgy, instructs Wiccans to practice ritual in the nude. Gardner spent several years in India, and may have picked up the concept from the Digambara Jains, a religious sect in which the monks may not wear clothing.
The origins of this instruction have been traced to Charles Godfrey Leland's 1899 book, "Aradia, or the Gospel of the Witches". The following speech by Aradia appears at the end of the book's first chapter;
<poem>
And as the sign that ye are truly free,
Ye shall be naked in your rites, both men
And women also: this shall last until
The last of your oppressors shall be dead;
</poem>
Dr. Leo Ruickbie also notes that the traditional and artistic representation of witches cannot be overlooked as a source for nudity in Gardner's system, citing artists such as Albrecht Dürer and Salvator Rosa.
Doreen Valiente, one of Gardner's priestesses, recalls Gardner's surprise at Valiente's recognition of material from "Aradia" in the original version of the "Charge" that she was given. Valiente later rewrote the "Charge", preserving the lines from "Aradia". Valiente's version was then widely circulated and reprinted.
Accepting "Aradia" as the source of skyclad practice, Robert Chartowich points to the 1998 Pazzaglini translation of these lines, which read "Men and Women / You will all be naked, until / Yet he shall be dead, the last / Of your oppressors is dead." Chartowich argues that the ritual nudity of Wicca was based upon Leland's mistranslation of these lines by incorporating the clause "in your rites".
Ritual nudity is not exclusive to traditional Wicca. Amongst those groups that do, only some rituals may involve ritual nudity. The Arician tradition, as an example, practices skyclad for six months of the year, and performs their ceremonies in ritual robes for the other half of the year. Within, and especially outside of, specific areas of Wicca and Pagan, reasons other than tradition may be given to explain a preference for skyclad worship. Starhawk states in "Spiral Dance", "The naked body represents truth, the truth that goes deeper than social custom" and "is a sign that a Witch's loyalty is to the truth before any ideology or any comforting illusions.'"
The term "skyclad" is derived from Indian religions, where the term "Digambara" literally means "sky-clad". England had close links with India at the time when Wicca first became public in England, so this usage could well have been familiar to English speakers with a knowledge of Far Eastern religions. In particular, Gerald Gardner, who first popularized Wicca in England, was a noted folklorist with an interest in Far Eastern culture who spent much of his adult life in Ceylon and Burma, so it seems very plausible that he could have been familiar with this Hindu term.[]

</doc>
<doc id="28448" url="http://en.wikipedia.org/wiki?curid=28448" title="Speech processing">
Speech processing

Speech processing is the study of speech signals and the processing methods of these signals. The signals are usually processed in a digital representation, so speech processing can be regarded as a special case of digital signal processing, applied to speech signal. Aspects of speech processing includes the acquisition, manipulation, storage, transfer and output of speech signals.

</doc>
<doc id="28450" url="http://en.wikipedia.org/wiki?curid=28450" title="Swahili language">
Swahili language

The Swahili language, also known as Kiswahili, is a Bantu language and the first language of the Swahili people. It is a lingua franca of the African Great Lakes region and other parts of Southeast Africa, including Tanzania, Kenya, Uganda, Rwanda, Burundi, Mozambique and the Democratic Republic of the Congo. The closely related Comorian language, spoken in the Comoros Islands, is sometimes considered a dialect.
Although only around fifteen to fifty million people speak Swahili as their first language, it is used as a lingua franca in much of Southeast Africa. Estimates of the total number of Swahili speakers vary widely, from 60 million to over 150 million. Swahili serves as a national or official language of four nations: Tanzania, Kenya, Uganda and the Democratic Republic of the Congo. Its dialects are used as official languages in Comoros - Shikomor and Mayotte - Shimaore. It is also one of the official languages of the African Union and East African Community.
A significant fraction of Swahili vocabulary is derived from Arabic through contact with Arabic-speaking Muslim inhabitants of the Swahili Coast. It has also incorporated German, Portuguese, English, Hindustani and French words into its vocabulary through contact with empire builders, traders and slavers during the past five centuries.
History.
Origin.
Swahili is traditionally regarded as being the language of coastal areas of Tanzania and Kenya, formalised after independence by presidents of the African Great Lakes region. It was first spoken by natives of the coastal mainland and spread as a fisherman's language to the various islands surrounding the Swahili Coast. Traders from these islands had extensive contact with the coastal peoples from at least the 2nd century A.D. and Swahili began to spread along the Swahili Coast from at least the 6th century. There is also cultural evidence of early Zaramo people settlement on Zanzibar from Dar-es-salaam in present-day Tanzania. The African population of the island holds the tradition that it is descended from these early settlers.
Clove farmers from Oman and the Persian Gulf farmed the Zanzibar Archipelago, slowly spreading Islam and adding a few words to Swahili language and building forts and castles in major trading and cultural centers as far as Sofala (Mozambique) and Kilwa (Tanzania) to the south, Mombasa and Lamu in Kenya, the Comoros Islands and northern Madagascar in the Indian Ocean, and Barawa to the north in southern Somalia. Demand for cloves soon established permanent trade routes, and Swahili-speaking merchants settled in stops along the new trade routes. For the most part, this process started the development of the modern Swahili language. However, the spread was hampered during the European colonial era and did not occur west of Lake Malawi, in what was then called the Belgian Congo, and is now Katanga Province of the Democratic Republic of the Congo, thus making it a secondary rather than a primary language in that region.
The earliest known documents written in Swahili are letters written in Kilwa in 1711 A.D. in the Arabic script. They were sent to the Portuguese of Mozambique and their local allies. The original letters are now preserved in the Historical Archives of Goa, India. Another ancient written document is an epic poem in the Arabic script titled "Utendi wa Tambuka" ("The History of Tambuka"); it is dated 1728. However, the Latin script later became standard under the influence of European colonial powers.
Colonial period.
After Germany attacked the region known as Tanganyika (present-day mainland Tanzania) for a colony in 1886, it took notice of the wide prevalence of Swahili, and soon designated Swahili as a colony-wide official administrative language. The British did not do so in neighbouring Kenya, even though they made moves in that direction. The British and Germans both sought to facilitate their rule over colonies where the inhabitants spoke dozens of different languages – thus the colonial authorities selected a single local language which they hoped the natives would find acceptable. Swahili was the only good candidate in these two colonies.
In the aftermath of Germany's defeat in World War I, it was dispossessed of all its overseas territories. Tanganyika fell into British hands. The British authorities, with the collaboration of British Christian missionary institutions active in these colonies, increased their resolve to institute Swahili as a common language for primary education and low-level governance throughout their East African colonies (Uganda, Tanganyika, Zanzibar, and Kenya). Swahili was to be subordinate to English: university education, much secondary education, and governance at the highest levels would be conducted in English.
One key step in spreading Swahili was to create a standard written language. In June 1928, an inter-territorial conference took place at Mombasa, at which the Zanzibar dialect, Kiunguja, was chosen to be the basis for standardizing Swahili. Today's standard Swahili, the version taught as a second language, is for practical purposes Zanzibar Swahili, even though there are minor discrepancies between the written standard and the Zanzibar vernacular.
Current status.
Swahili has become a second language spoken by tens of millions in three African Great Lakes countries, Tanzania, Kenya, and the Democratic Republic of Congo, where it is an official or national language. The neighboring nation of Uganda made Swahili a required subject in primary schools in 1992—although this mandate has not been well implemented—and declared it an official language in 2005 in preparation for the East African Federation. Swahili, or other closely related languages, is spoken by relatively small numbers of people in Burundi, the Comoros, Rwanda, northern Zambia, Malawi, and Mozambique. and the language was still understood in the southern ports of the Red Sea and along the coasts of southern Arabia and the Persian Gulf in the twentieth century.
At the present time, some 80 percent of approximately 49 million Tanzanians speak Swahili in addition to their first languages. Many of the rising generation of Tanzania, however, speak Swahili as a primary language due to decreases traditional culture and the rise of a more unified culture in urban areas. Kenya's population is comparable as well, with a greater part of the nation being able to speak Swahili. Most educated Kenyans are able to communicate fluently in Swahili, since it is a compulsory subject in school from grade one to high school and a distinct academic discipline in many of the public and private universities.
The five eastern provinces of the Democratic Republic of Congo are Swahili-speaking. Nearly half the 66 million Congolese reportedly speak it, and it is starting to rival Lingala as the most important national language of that country.
In Uganda, the Baganda and residents of Buganda generally do not speak Swahili, but it is in common use among the 25 million people elsewhere in the country and is currently being implemented in schools nationwide in preparation for the East African Community.
The usage of Swahili in other countries is commonly overstated, being widespread only in market towns, among returning refugees, or near the borders of Kenya and Tanzania. Even so, Swahili speakers may number some 120 to 150 million people. Many of the world's institutions have responded to Swahili's growing prominence.
Methali ("e.g." Haraka haraka haina baraka – Hurry hurry has no blessing), "i.e." "wordplay, risqué or suggestive puns and lyric rhyme, are deeply inscribed in Swahili culture, in form of Swahili parables, proverbs, and allegory". Methali is uncovered globally within 'Swah' rap music. It provides the music with rich cultural, historical, and local textures and insight.
Name.
"Kiswahili" is the Swahili word for the language, and this is also sometimes used in English. The name "Kiswahili" comes from the plural "sawāḥil" (سواحل) of the Arabic word "sāḥil" (ساحل), meaning "boundary" or "coast", used as an adjective meaning "coastal dwellers". With the prefix "ki-", it means "coastal language", "ki-" being a prefix attached to nouns of the noun class that includes languages.
Phonology.
Swahili is unusual among African languages in having lost the feature of lexical tone (with the exception of the numerically important Mvita dialect, the dialect of Kenya's second city, the Indian Ocean port of Mombasa).
Stress is on the penultimate syllable.
Vowels.
Standard Swahili has five vowel phonemes: /ɑ/, /ɛ/, /i/, /ɔ/, and /u/. The pronunciation of the phoneme /u/ stands between International Phonetic Alphabet [u] and [o]. Vowels are never reduced, regardless of stress. The vowels are pronounced as follows:
Swahili has no diphthongs; in vowel combinations, each vowel is pronounced separately. Therefore, the Swahili word for "leopard", "chui", is pronounced /tʃu.i/; that is, as two syllables.
Consonants.
Notes:
Orthography.
Swahili is currently written in a slightly defective alphabet using the Latin script; the defectiveness comes in not distinguishing aspirated consonants, though those are not distinguished in all dialects. (These were, however, distinguished as "kh" etc. in the old German colonial Latin alphabet.) There are two digraphs for native sounds, "ch" and "sh"; "c" is not used apart from unassimilated English loans and occasionally as a substitute for "k" in advertisements. There are in addition several digraphs for Arabic sounds which are not distinguished in pronunciation outside of traditional Swahili areas.
The language had previously been written in the Arabic script. Unlike adaptations of the Arabic script for other languages, relatively little accommodation was made for Swahili. There were also differences in orthographic conventions between cities, authors, and over the centuries, some quite precise, but others defective enough to cause difficulties with intelligibility.
Vowel diacritics were generally written, effectively making the Swahili-Arabic script an abugida. /e/ and /i/, /o/ and /u/ were often conflated, but in some orthographies /e/ was distinguished from /i/ by rotating the kasra 90°, and /o/ from /u/ by writing the damma backwards.
Several Swahili consonants do not have equivalents in Arabic, and for these often no special letters were created, as they were for example in Persian and Urdu. Instead, the closest Arabic sound is substituted. Not only does this mean that one letter often stands for more than one sound, but also that writers made different choices as to which consonant to substitute. Some of the equivalents between Arabic Swahili and Roman Swahili are:
This was the general situation, but conventions from Urdu were adopted by some authors; for example, to distinguish aspiration and /p/ from /b/: پھا /pʰaa/ 'gazelle', پا /paa/ 'roof'. Although not found in Standard Swahili today, there is a distinction between dental and alveolar consonants in some dialects, and this is reflected in some orthographies, for example in كُٹَ "-kuta" 'to meet' vs. كُتَ "-kut̠a" 'to be satisfied'. A "k" with the dots of "y", ڱ, was used for "ch" in some conventions; this "ky" is historically and even contemporaneously a more accurate transcription than Roman "ch". In Mombasa, it was common to use the Arabic emphatics for Cw, for example in صِصِ "swiswi" (standard "sisi") 'we' and كِطَ "kit̠wa" (standard "kichwa") 'head'.
Word division differs from Roman norms. Particles such as "ya, na, si, kwa, ni" are joined to the following noun, and possessives such as "yangu" and "yako" are joined to the preceding noun, but verbs are written as two words, with the subject and tense–aspect–mood morphemes separated from the object and root, as in "aliye niambia" "he who asked me".
Noun classes.
In common with all Bantu languages, Swahili grammar arranges nouns into a number of classes. The ancestral system had 22 classes (counting singular and plural separately, according to the Meinhof convention), with most Bantu languages sharing at least ten of these. Swahili employs sixteen: six classes that usually indicate singular nouns, five classes that usually indicate plural nouns, a class for abstract nouns, a class for verbal infinitives used as nouns, and three classes to indicate location.
Nouns beginning with "m-" in the singular and "wa-" in the plural denote animate beings, especially people. Examples are "mtu", meaning 'person' (plural "watu"), and "mdudu", meaning 'insect' (plural "wadudu"). A class with "m-" in the singular but "mi-" in the plural often denotes plants, such as "mti" 'tree', "miti" trees. The infinitive of verbs begins with "ku-", e.g. "kusoma" 'to read'. Other classes are more difficult to categorize. Singulars beginning in "ki-" take plurals in "vi-"; they often refer to hand tools and other artefacts. This "ki-/vi-" alteration even applies to foreign words where the "ki-" was originally part of the root, so "vitabu" "books" from "kitabu" "book" (from Arabic "kitāb" "book"; similar to how Arabic itself deals with the name Alexandria). This class also contains languages (such as the name of the language "Kiswahili"), and diminutives, which had been a separate class in earlier stages of Bantu. Words beginning with "u-" are often abstract, with no plural, e.g. "utoto" 'childhood'.
A fifth class begins with "n-" or "m-" or nothing, and its plural is the same. Another class has "ji-" or no prefix in the singular, and takes "ma-" in the plural; this class is often used for augmentatives. When the noun itself does not make clear which class it belongs to, its concords do. Adjectives and numerals commonly take the noun prefixes, and verbs take a different set of prefixes.
The same noun root can be used with different noun-class prefixes for derived meanings: human "mtoto (watoto)" "child (children)", abstract "utoto" "childhood", diminutive "kitoto (vitoto)" "infant(s)", augmentative "toto (matoto)" "big child (children)". Also vegetative "mti (miti)" "tree(s)", artefact "kiti (viti)" "chair(s)", augmentative "jiti (majiti)" "large tree", "kijiti (vijiti)" "stick(s)", "ujiti (njiti)" "tall slender tree".
Semantic motivation.
The "ki-/vi-" class historically consisted of two separate genders, artefacts (Bantu class 7/8, utensils and hand tools mostly) and diminutives (Bantu class 12), that were conflated at a stage ancestral to Swahili. Examples of the first are "kisu" "knife", "kiti" "chair" (from "mti" "tree, wood"), "chombo" "vessel" (a contraction of "ki-ombo"). Examples of the latter are "kitoto" "infant", from "mtoto" "child"; "kitawi" "frond", from "tawi" "branch"; and "chumba" ("ki-umba") "room", from "nyumba" "house". It is the diminutive sense that has been furthest extended. An extension common to diminutives in many languages is "approximation" and "resemblance" (having a 'little bit' of some characteristic, like "-y" or "-ish" in English). For example, there is "kijani" "green", from "jani" "leaf" (compare English 'leafy'), "kichaka" "bush" from "chaka" "clump", and "kivuli" "shadow" from "uvuli" "shade". A 'little bit' of a verb would be an instance of an action, and such "instantiations" (usually not very active ones) are found: "kifo" "death", from the verb "-fa" "to die"; "kiota" "nest" from "-ota" "to brood"; "chakula" "food" from "kula" "to eat"; "kivuko" "a ford, a pass" from "-vuka" "to cross"; and "kilimia" "the Pleiades", from "-limia" "to farm with", from its role in guiding planting. A resemblance, or being a bit like something, implies marginal status in a category, so things that are marginal examples of their class may take the "ki-/vi-" prefixes. One example is "chura" ("ki-ura") "frog", which is only half terrestrial and therefore marginal as an animal. This extension may account for disabilities as well: "kilema" "a cripple", "kipofu" "a blind person", "kiziwi" "a deaf person". Finally, diminutives often denote contempt, and contempt is sometimes expressed against things that are dangerous. This might be the historical explanation for "kifaru" "rhinoceros", "kingugwa" "spotted hyena", and "kiboko" "hippopotamus" (perhaps originally meaning "stubby legs").
Another class with broad semantic extension is the "m-/mi-" class (Bantu classes 3/4). This is often called the 'tree' class, because "mti, miti" "tree(s)" is the prototypical example. However, it seems to cover vital entities which are neither human nor typical animals: trees and other plants, such as "mwitu" 'forest' and "mtama" 'millet' (and from there, things made from plants, like "mkeka" 'mat'); supernatural and natural forces, such as "mwezi" 'moon', "mlima" 'mountain', "mto" 'river'; active things, such as "moto" 'fire', including active body parts ("moyo" 'heart', "mkono" 'hand, arm'); and human groups, which are vital but not themselves human, such as "mji" 'village', and, by analogy, "mzinga" 'beehive/cannon'. From the central idea of "tree", which is thin, tall, and spreading, comes an extension to other long or extended things or parts of things, such as "mwamvuli" 'umbrella', "moshi" 'smoke', "msumari" 'nail'; and from activity there even come active instantiations of verbs, such as "mfuo" "metal forging", from "-fua" "to forge", or "mlio" "a sound", from "-lia" "to make a sound". Words may be connected to their class by more than one metaphor. For example, "mkono" is an active body part, and "mto" is an active natural force, but they are also both long and thin. Things with a trajectory, such as "mpaka" 'border' and "mwendo" 'journey', are classified with long thin things, as in many other languages with noun classes. This may be further extended to anything dealing with time, such as "mwaka" 'year' and perhaps "mshahara" 'wages'. Animals which are exceptional in some way and therefore do not fit easily in the other classes may be placed in this class.
The other classes have foundations that may at first seem similarly counterintuitive. In short,
Verb affixation.
Swahili verbs consist of a root and a number of affixes (mostly prefixes) which can be attached to express grammatical persons, tense, and subordinate clauses, which require a conjunction in languages such as English.
Verbs of Bantu origin end in '-a' in the indicative. This vowel changes to indicate the subjunctive and negation.
In most dictionaries, verbs are listed in their indicative root form, for example "-kata" meaning 'to cut/chop'. In a simple sentence, prefixes for grammatical tense and person are added, as "ninakata" 'I cut'. Here "ni-" means 'I' and "na-" indicates a specific time (present tense unless stated otherwise).
Verb conjugation.
This sentence can be modified either by changing the subject prefix or the tense prefix, for example:
The animate/human subject and object prefixes, with the "m-/wa-" (human class) in the third person, is:
In Standard Swahili, 2pl and 3pl objects are both "-wa-." However, in Nairobi Swahili, 2pl is "-mu-."
The most common tense prefixes are:
The indefinite (gnomic tense) prefix is used for generic statements such as "birds fly", and the vowels of the subject prefixes are assimilated. Thus, "nasoma" means 'I read', although colloquially it is also short for "ninasoma."
Conditional:
The English conjunction 'if' is translated by "-ki-".
A third prefix is the object prefix. It is placed just before the root and refers a particular object, either a person, or rather as "the" does in English:
The "-a" suffix listed by dictionaries is the positive indicative mood. Other forms occur with negation and the subjunctive, as in "sisomi":
Other instances of this change of the final vowel include the subjunctive in "-e." This goes only for Bantu verbs ending with "-a"; Arabic-derived verbs do not change their final vowel.
Other suffixes are placed before the end vowel, such as the applicative "-i-" and passive "-w-":
Concord.
Swahili phrases agree with nouns in a system of concord, though if the noun refers to a human, they accord with noun classes 1 & 2 regardless of noun class. Verbs agree with the noun class of their subjects and objects; adjectives, prepositions, and demonstratives agree with the noun class of their nouns. In Standard Swahili "(Kiswahili sanifu)" which was based on the dialect spoken in Zanzibar the system is rather complex; however, it is drastically simplified in many local variants where Swahili is not the native language, such as in Nairobi.
In places where Standard Swahili is not commonly used, concord reflects only animacy. Human subjects and objects trigger "a-, wa-" and "m-, wa-" in verbal concord, while non-human subjects and objects—of whatever class—trigger "i-, zi-," and infinitive verbs vary between standard "ku-" and reduced "i-." ("Of" is animate "wa" and inanimate "ya, za.") In Standard Swahili, human subjects and objects of whatever class trigger animacy concord in "a-, wa-" and "m-, wa-," while non-human subjects and objects trigger a variety of gender-concord prefixes.
Dialects of Swahili and languages closely related to Swahili.
This list is based on Nurse, Derek, and Hinnebusch, Thomas J. "Swahili and Sabaki: a linguistic history".
Dialects of Swahili.
Modern standard Swahili is based on "Kiunguja," the dialect spoken in Zanzibar town.
There are numerous dialects of Swahili, some of which are mutually unintelligible, including the following.
Old dialects.
Maho (2009) considers the following to be distinct languages:
The rest of the dialects he divides into two groups:
Maho includes the various Comorian dialects as a third group. Other authorities consider Comorian to be a Sabaki language distinct from Swahili.
Other regions.
In Somalia, where the Afro-Asiatic Somali language predominates, a variant of Swahili referred to as Chimwiini (also known as Chimbalazi) is spoken along the Benadir coast by the Bravanese people. Another Swahili dialect known as Kibajuni also serves as the mother tongue of the Bajuni minority ethnic group, the latter of whom inhabit the tiny Bajuni Islands as well as the southern Kismayo region.
In Oman, an estimated 22,000 people speak Swahili. Most are descendants of those who repatriated after the fall of the Sultanate of Zanzibar.

</doc>
<doc id="28484" url="http://en.wikipedia.org/wiki?curid=28484" title="Sputnik 1">
Sputnik 1

Sputnik 1 (Russian: "Спу́тник-1" ], "Satellite-1", ПС-1 ["PS-1", i.e. "Простейший Спутник-1" or "Elementary Satellite-1"]) was the first artificial Earth satellite. The Soviet Union launched it into an elliptical low Earth orbit on 4 October 1957. It was a 58 cm diameter polished metal sphere, with four external radio antennas to broadcast radio pulses. It was visible all around the Earth and its radio pulses were detectable. This surprise 1957 success precipitated the American Sputnik crisis and triggered the Space Race, a part of the larger Cold War. The launch ushered in new political, military, technological, and scientific developments.
"Sputnik" itself provided scientists with valuable information. The density of the upper atmosphere could be deduced from its drag on the orbit, and the propagation of its radio signals gave information about the ionosphere.
"Sputnik 1" was launched during the International Geophysical Year from Site No.1/5, at the 5th Tyuratam range, in Kazakh SSR (now at the Baikonur Cosmodrome). The satellite travelled at about 29000 km/h, taking 96.2 minutes to complete each orbit. It transmitted on 20.005 and 40.002 MHz which were monitored by amateur radio operators throughout the world. The signals continued for 21 days until the transmitter batteries ran out on 26 October 1957. "Sputnik 1" burned up on 4 January 1958, as it fell from orbit upon reentering Earth's atmosphere, after travelling about 70 million km (43.5 million miles) and spending 3 months in orbit.
Before the launch.
Satellite construction project.
On 17 December 1954, chief Soviet rocket scientist Sergei Korolev addressed Minister of Defence Dimitri Ustinov and proposed a developmental plan for an artificial satellite. Korolev forwarded a report by Mikhail Tikhonravov with an overview of similar projects abroad. Tikhonravov had emphasized that the launch of an orbital satellite was an inevitable stage in the development of rocket technology.
On 29 July 1955, U.S. President Dwight D. Eisenhower announced through his press secretary that the United States would launch an artificial satellite during the International Geophysical Year (IGY). A week later, on 8 August, the Politburo of the Communist Party of the Soviet Union approved the proposal to create an artificial satellite. On 30 August Vasily Ryabikov – the head of the State Commission on R-7 rocket test launches – held a meeting where Korolev presented calculation data for a spaceflight trajectory to the Moon. They decided to develop a three-stage version of the R-7 rocket for satellite launches.
On 30 January 1956 the Council of Ministers approved practical work on an artificial Earth-orbiting satellite. This satellite, named "Object D", was planned to be completed in 1957–58; it would have a mass of 1000 to and would carry 200 to of scientific instruments. The first test launch of "Object D" was scheduled for 1957. Work on the satellite was to be divided between institutions as follows:
Under the direction of main engineer Dimitrij Sergeevich Mordasov, preliminary design work was completed by July 1956 and the scientific tasks to be carried out by the satellite were defined. It included measuring the density of the atmosphere, its ion composition, the solar wind, magnetic fields, and cosmic rays. These data would be valuable in the creation of future artificial satellites. A system of ground stations was to be developed to collect data transmitted by the satellite, observe the satellite's orbit, and transmit commands to the satellite. Because of the limited time frame, observations were planned for only 7 to 10 days and orbit calculations were not expected to be extremely accurate.
By the end of 1956 it became clear that the complexity of the ambitious design meant that 'Object D' could not be launched in time because of difficulties creating scientific instruments and the low specific impulse produced by the completed R-7 engines (304 sec instead of the planned 309 to 310 sec). Consequently the government re-scheduled the launch for April 1958. Object D would later fly as "Sputnik 3".
Fearing the U.S. would launch a satellite before the USSR, OKB-1 suggested the creation and launch of a satellite in April–May 1957, before the IGY began in July 1957. The new satellite would be simple, light (100 kg), and easy to construct, forgoing the complex, heavy scientific equipment in favour of a simple radio transmitter. On 15 February 1957 the Council of Ministers of the USSR approved this simple satellite, designated 'Object PS'. This version allowed the satellite to be tracked visually by Earth-based observers while in orbit, and transmit tracking signals to ground-based receiving stations. The launch of two satellites, PS-1 and PS-2, with two R-7 rockets (8K71) was approved, but only after successful testing of the R-7 launch vehicle.
Launch vehicle preparation and launch site selection.
The R-7 Semyorka was initially designed as an ICBM by OKB-1. The decision to build it was made by the Central Committee of the Communist Party of the Soviet Union and the Council of Ministers of the USSR on 20 May 1954. The R-7 was also known by its GRAU (later GURVO) designation 8K71. At the time, the R-7 was known to western sources as the T-3 or M-104, and Type A. A special reconnaissance commission selected Tyuratam for the construction of a rocket proving ground (the 5th Tyuratam range, usually referred to as "NIIP-5", or "GIK-5" in the post-Soviet time). The selection was approved on 12 February 1955 by the Council of Ministers of the USSR, but the site would not be completed until 1958. Actual work on the construction of the site began on 20 July by military building units. On 14 June 1956 Sergei Korolev decided to adapt the R-7 rocket to the 'Object D', that would later be replaced by the much lighter 'Object PS'.
The first launch of an R-7 rocket (8K71 No.5L) occurred on 15 May 1957. The flight was controlled until the 98th second, but a fire in a strap-on booster led to an unintended crash 400 km from the site. Three attempts to launch the second rocket (8K71 No.6) were made on 10–11 June, but an assembly defect prevented launch. The unsuccessful launch of the third R-7 rocket (8K71 No.7) took place on 12 July. During the flight the rocket experienced an uncontrolled roll about its longitudinal axis and its engines were automatically shut off. The flight lasted 32 seconds, and the R-7 crashed 7 km from the site and exploded.
The launch of the fourth rocket (8K71 No.8), on 21 August at 15:25 Moscow Time, was successful. The rocket's core boosted the dummy warhead to the target altitude and velocity, reentered the atmosphere, and broke apart at a height of 10 km after traveling 6,000 km. On 27 August TASS in the USSR issued a statement on the successful launch of a long-distance multistage ICBM. The launch of the fifth R-7 rocket (8K71 No.9), on 7 September was also successful, but the dummy was also destroyed on atmospheric reentry, and hence needed a redesign to completely fulfill its military purpose. The rocket, however, was deemed suitable for satellite launches, and Korolev was able to convince the State Commission to allow the use of the next R-7 to launch PS-1, allowing the delay in the rocket's military exploitation to launch the PS-1 and PS-2 satellites.
On 22 September a modified R-7 rocket, named Sputnik and indexed as 8K71PS, arrived at the proving ground and preparations for the launch of PS-1 began. Compared to the military R-7 test vehicles, the mass of 8K71PS was reduced from 280 tonnes to 272 tonnes; its length with PS-1 was 29.167 m and the thrust at lift off was 3.90 MN.
Observation complex.
The measurement complex at the proving ground for monitoring the launch vehicle from its launch was completed prior to the first R-7 rocket test launches in December 1956. It consisted of six static stations: IP-1 through IP-6, with IP-1 situated at a distance of 1 km from the launch pad. The main monitoring devices of these stations were telemetry and trajectory measurement stations, "Tral," developed by OKB MEI. They received and monitored data from the "Tral" system transponders mounted on the R-7 rocket's core stage, which provided telemetric data about "Sputnik" 1's launch vehicle. The data was useful even after the satellite's separation from the second stage of the rocket; "Sputnik" 1's location was calculated from the data on the second stage's location (which followed "Sputnik 1" at a known distance) using nomograms developed by P.E. Elyasberg.
An additional observation complex, established to track the satellite after its separation from the rocket, was completed by a group led by Colonel Yu.A.Mozzhorin in accordance with the General Staff directive of 8 May 1957. It was called the Command-Measurement Complex and consisted of the coordination center in "NII-4" by the Ministry of Defence of the USSR (at Bolshevo) and seven ground tracking stations, situated along the line of the satellite's ground track. They were: NIP-1 (at Tyuratam station, Kazakh SSR, situated not far from IP-1), NIP-2 (at Makat station, Guryev Oblast), NIP-3 (at Sary-Shagan station, Dzhezkazgan Oblast), NIP-4 (at Yeniseysk), NIP-5 (at village Iskup, Krasnoyarsk Krai), NIP-6 (at Yelizovo) and NIP-7 (at Klyuchi). The complex had a communication channel with the launch pad. Stations were equipped with radar, optical instruments, and communications systems. PS-1 was not designed to be controlled, it could only be observed. Data from stations were transmitted by telegraphs into "NII-4" where ballistics specialists calculated orbital parameters. The complex became an early prototype of the Soviet Mission Control Center.
In the West, the satellite was tracked by amateur radio operators, and the booster rocket was located and tracked by the Lovell Telescope at the Jodrell Bank Observatory. The Newbrook Observatory was the first facility in North America to photograph Sputnik 1.
Design.
The chief constructor of "Sputnik 1" at OKB-1 was M. S. Khomyakov. The satellite was a 585 mm diameter sphere, assembled from two hemispheres which were hermetically sealed using o-rings and connected using 36 bolts, and had a mass of 83.6 kg. The hemispheres were 2 mm thick, and were covered with a highly polished 1 mm-thick heat shield made of aluminium-magnesium-titanium AMG6T alloy ("AMG" is an abbreviation for "aluminium-magnesium" and "T" stands for "titanium", the alloy contains 6% of magnesium and 0.2% of titanium). The satellite carried two pairs of antennas designed by the Antenna Laboratory of OKB-1 led by M. V. Krayushkin. Each antenna was made up of two whip-like parts: 2.4 and in length, and had an almost spherical radiation pattern, so that the satellite beeps were transmitted with equal power in all directions, making reception of the transmitted signal independent of the satellite's rotation.
The power supply, with a mass of 51 kg, was in the shape of an octagonal nut with the radio transmitter in its hole. It consisted of three silver-zinc batteries, developed at the All-Union Research Institute of Current Sources (VNIIT) under the leadership of N. S. Lidorenko. Two of these batteries powered the radio transmitter and one powered the temperature regulation system. The batteries had an expected lifetime of two weeks, and operated for 22 days. The power supply was turned on automatically at the moment of the satellite's separation from the second stage of the rocket.
The satellite had a one-watt, 3.5 kg radio transmitting unit inside, developed by V. I. Lappo from "NII-885," that worked on two frequencies, 20.005 and 40.002 MHz. Signals on the first frequency were transmitted in 0.3 sec pulses (under normal temperature and pressure conditions on-board), with pauses of the same duration filled by pulses on the second frequency. Analysis of the radio signals was used to gather information about the electron density of the ionosphere. Temperature and pressure were encoded in the duration of radio beeps. A temperature regulation system contained a fan, a dual thermal switch, and a control thermal switch. If the temperature inside the satellite exceeded 36 C the fan was turned on and when it fell below 20 C the fan was turned off by the dual thermal switch. If the temperature exceeded 50 C or fell below 0 C, another control thermal switch was activated, changing the duration of the radio signal pulses. "Sputnik 1" was filled with dry nitrogen, pressurized to 1.3 atm. The satellite had a barometric switch, activated if the pressure inside the satellite fell below 130 kPa, which would have indicated failure of the pressure vessel or puncture by a meteor, and would have changed the duration of radio signal impulse.
While attached to the rocket, "Sputnik 1" was protected by a cone-shaped payload fairing, with a height of 80 cm. The fairing separated from both "Sputnik 1" and the spent R-7 second stage at the same time as the satellite was ejected. Tests of the satellite were conducted at OKB-1 under the leadership of O. G. Ivanovsky.
Launch and mission.
The control system of the Sputnik rocket was adjusted to an intended orbit of 223 km by 1450 km, with an orbital period of 101.5 min. The trajectory was calculated earlier by Georgi Grechko, using the USSR Academy of Sciences' mainframe computer.
The Sputnik rocket was launched on 4 October 1957 at 19:28:34 UTC (5 October at the launch site) from Site No.1 at NIIP-5. Telemetry indicated the side boosters separated 116 seconds into the flight and the core-stage engine shut down 295.4 seconds into the flight. At shut down, the 7.5 tonne core stage with PS-1 attached had attained an altitude of 223 km above sea level, a velocity of 7780 m/s and velocity vector inclination to the local horizon of 0 degrees 24 minutes. This resulted in an initial orbit of 223 km by 950 km, with an apogee approximately 500 km lower than intended, and an inclination of 65.1 degrees and a period of 96.2 minutes.
19.9 seconds after engine cut-off, PS-1 separated from the second stage and the satellite's transmitter was activated. These signals were detected at the IP-1 station by Junior Engineer-Lieutenant V.G. Borisov, where reception of Sputnik's "beep-beep-beep" tones confirmed the satellite's successful deployment. Reception lasted for two minutes, until PS-1 fell below the horizon. The Tral telemetry system on the R-7 core stage continued to transmit and was detected on its second orbit.
The designers, engineers and technicians who developed the rocket and satellite watched the launch from the range. After the launch they drove to the mobile radio station to listen for signals from the satellite. They waited about 90 minutes to ensure that the satellite had made one orbit and was transmitting, before Korolyov called Soviet premier Nikita Khrushchev.
On the first orbit the Telegraph Agency of the Soviet Union (TASS) transmitted: "As result of great, intense work of scientific institutes and design bureaus the first artificial Earth satellite has been built". The R-7 core stage, with a mass of 7.5 tonnes and a length of 26 meters, also reached Earth orbit and was visible from the ground at night as a first magnitude object following the satellite. Deployable reflective panels were placed on the booster in order to increase its visibility for tracking. The satellite itself, a small, highly polished sphere, was barely visible at sixth magnitude, and thus more difficult to follow optically. A third object, the payload fairing, also achieved orbit.
The core stage of the R-7 remained in orbit for two months until 2 December 1957, while "Sputnik 1" orbited until 4 January 1958, having completed 1,440 orbits of the Earth.
Reaction.
Our movies and television programs in the fifties were full of the idea of going into space. What came as a surprise was that it was the Soviet Union that launched the first satellite. It is hard to recall the atmosphere of the time.—John Logsdon
The Soviets provided details of Sputnik before the launch but few outside the Soviet Union noticed. After reviewing information publicly available before the launch, Willy Ley wrote in 1958: If somebody tells me that he has the rockets to shoot — which we know from other sources, anyway — and tells me what he will shoot, how he will shoot it, and in general says virtually everything except for the precise date — well, what should I feel like if I'm surprised when the man shoots?"
 Teams of visual observers at 150 stations in the United States and other countries were alerted during the night to watch for the Soviet sphere at dawn and during the evening twilight. They had been organized in Project Moonwatch to sight the satellite through binoculars or telescopes as it passed overhead. The USSR asked radio amateurs and commercial stations to record the sound of the satellite on magnetic tape.
News reports at the time pointed out that "anyone possessing a short wave receiver can hear the new Russian earth satellite as it hurtles over his area of the globe". Directions, provided by the American Radio Relay League were to "Tune in 20 megacycles sharply, by the time signals, given on that frequency. Then tune to slightly higher frequencies. The 'beep, beep' sound of the satellite can be heard each time it rounds the globe," The first recording of "Sputnik 1"'s signal was made by RCA engineers near Riverhead, Long Island. They then drove the tape recording into Manhattan for broadcast to the public over NBC radio. However, as "Sputnik" rose higher over the East Coast, its signal was picked up by ham station W2AEE, the ham radio station of Columbia University. Students working in the university's FM station, WKCR, made a tape of this, and were the first to rebroadcast the "Sputnik 1" signal to the American public (or as much of it as could be received by the FM station).
At first the Soviet Union agreed to use equipment "compatible" with that of the United States, but later announced the lower frequencies. The White House declined to comment on military aspects of the launch, but said "it did not come as a surprise." On 5 October the Naval Research Laboratory announced it had recorded four crossings of "Sputnik-1" over the United States. The USAF Cambridge Research Center collaborated with Bendix-Friez, Westinghouse Broadcasting Co., Smithsonian Astrophysical Observatory, and MIT, to obtain a motion picture of the rocket body of Sputnik 1 crossing the pre-dawn sky of Baltimore, broadcast on 12 October by WBZ-TV in Boston. U.S. President Eisenhower obtained photographs of the Soviet facilities from Lockheed U-2 flights conducted since 1956.
The success of Sputnik seemed to have changed minds around the world regarding a shift in power to the Soviets.
The USSR's launch of Sputnik spurred the United States to create the Advanced Research Projects Agency (ARPA or DARPA) in February 1958 to regain a technological lead. :1957 – October 4th – the USSR launches Sputnik, the first artificial earth satellite.
The organization united some of America's most brilliant people, who developed the United States' first successful satellite in 18 months. Several years later ARPA began to focus on computer networking and communications technology.
In Britain the media and population initially reacted with a mixture of fear for the future, but also amazement about humankind's progress. Many newspapers and magazines heralded the arrival of the Space Age. However, when the Soviet Union launched a second craft containing the dog Laika, the media narrative returned to one of anti-communism and many people sent protests to the Russian embassy and the RSPCA.
Propaganda.
"Sputnik 1" was not immediately used by Soviet propaganda. The Soviets had kept quiet about their earlier accomplishments in rocketry, fearing that it would lead to secrets being revealed and failures being exploited by the West. When the Soviets began using "Sputnik" in their propaganda, they emphasized pride in the achievement of Soviet technology, arguing that it demonstrated the Soviets' superiority over the West. People were encouraged to listen to "Sputnik"'s signals on the radio and to look out for Sputnik in the night sky. While "Sputnik" itself had been highly polished, its small size made it barely visible to the naked eye. What most watchers actually saw was the much more visible 26 meter core stage of the R-7. Shortly after the launch of PS-1, Krushchev pressed Korolev to launch another satellite in time for the 40th anniversary of the October Revolution on 7 November 1957.
The launch of "Sputnik" surprised the American public and shattered the perception, furthered by American propaganda, of the United States as the technological superpower and the Soviet Union as a backward country. Privately, however, the CIA and President Dwight D. Eisenhower were aware of progress being made by the Soviets on Sputnik from secret spy plane imagery. Together with the Jet Propulsion Laboratory (JPL), the Army Ballistic Missile Agency built "Explorer 1," and launched it on 31 January 1958. Before work was completed, however, the Soviet Union launched a second satellite, "Sputnik 2", on 3 November 1957. Meanwhile, the televised failure of "Vanguard TV3" on 6 December 1957 deepened American dismay over the country's position in the Space Race. The Americans took a more aggressive stance in the emerging space race, resulting in an emphasis on science and technological research and reforms in many areas from the military to education systems. The federal government began investing in science, engineering and mathematics at all levels of education. An advanced research group was assembled for military purposes. These research groups developed weapons such as ICBMs and missile defense systems, as well as spy satellites for the US.
Replicas.
A model of Sputnik 1 was given to the United Nations and now decorates the Entry Hall of its Headquarters in New York City. Other replicas are on display at the Smithsonian's National Air and Space Museum in Washington, D.C.; at the Kansas Cosmosphere and Space Center in Hutchinson, Kansas; at the Science Museum, London; at the World Museum in Liverpool; and hanging in the Noble Planetarium at the Fort Worth Museum of Science and History in Fort Worth, Texas.
Three one-third scale student-built replicas of Sputnik 1 were deployed from the Mir space station between 1997 and 1999. The first, named Sputnik 40 to commemorate the fortieth anniversary of the launch of Sputnik 1, was deployed in November 1997. Sputnik 41 was launched a year later, and Sputnik 99 was deployed in February 1999. A fourth replica was launched but never deployed, and was destroyed when Mir was deorbited.
A full-scale model of Sputnik 1 is on display at the California Science Center in Los Angeles, California.
A full-scale replica of Sputnik 1 is on display at The Armstrong Air and Space Museum is a museum in Wapakoneta, Ohio, the hometown of Neil Armstrong, first man to set foot on the moon. Built by Retro Rocket, a model making company in Ventura, Ca.
Impact.
On Friday, October 4, 1957, the Soviets had orbited the world's first artificial satellite. Anyone who doubted its existence could walk into the backyard just after sunset and see it.—Mike Gray, "Angle of Attack"
Initially U.S. President Eisenhower was not surprised by "Sputnik". He had been forewarned of the R-7s capabilities by information derived from U2 spy plane overflight photos as well as signals and telemetry intercepts. The Eisenhower administration's first response was low-key and almost dismissive. Eisenhower was even pleased that the USSR, not the USA, would be the first to test the waters of the still-uncertain legal status of orbital satellite overflights. Eisenhower had suffered the Soviet protests and shoot-downs of Project Genetrix (Moby Dick) balloons and was concerned about the probability of a U-2 being shot down. In order to set a precedent for "freedom of space" before the launch of America's secret WS-117L Spy Satellites the USA had launched Project Vanguard as its own "civilian" satellite entry for the International Geophysical Year. Eisenhower greatly underestimated the reaction of the American public, which was shocked by the launch of "Sputnik 1" and by the televised failure of the Vanguard Test Vehicle 3 launch attempt. The sense of fear was inflamed by Democratic politicians and professional cold warriors which portrayed the United States as woefully behind. One of the many books which suddenly appeared for the lay-audience noted 7 points of "impact" upon the nation. Those points of impact were, Western Leadership, Western Strategy and Tactics, Missile Production, Applied Research, Basic Research, Education, and Democratic Culture. The USA soon had a number of successful satellites, including Explorer 1, Project SCORE, and Courier 1B. However, public reaction to the Sputnik crisis led to the creation of the Advanced Research Projects Agency (renamed the Defense Advanced Research Projects Agency or DARPA in 1972), NASA, and an increase in U.S. government spending on scientific research and education. Not only did the launch of Sputnik spur America to action in the space race, it also led directly to the creation of N.A.S.A. through the space act bill. Sputnik also contributed directly to advancement in science and technology. This came about when President Eisenhower enacted a bill called the National Defense Education Act. This bill encouraged students to go to college and study math and science. The students' tuition fees would be paid for. This led to a new emphasis on science and technology in American schools. Sputnik also created building blocks which probably led to the general establishment of the way science is conducted in the United States today. After the launch of Sputnik a poll conducted and published by the University of Michigan, showed that 26% of Americans surveyed thought that Russian sciences and engineering were superior to that of the United States of America. Although a year later that had dropped to 10% as the U.S. were able to launch their own satellites into space.
One consequence of the Sputnik shock was the perception of a "missile gap." This was to become a dominant issue in the 1960 Presidential campaign.
One irony of the "Sputnik" event was the initially low-key response of the Soviet Union. The Communist Party newspaper Pravda only printed a few paragraphs about 'Sputnik 1' on 4 October. In the days following the world's startled response, the Soviets started celebrating their great accomplishment.
Sputnik also inspired a generation of engineers and scientists. Harrison Storms, the North American designer who was responsible for the X-15 rocket plane, and went on to head the effort to design the Apollo Command/Service Module and Saturn V launch vehicle's second stage was moved by the launch of Sputnik to think of space as being the next step for America. Astronauts Alan Shepard, who was the first American in space, and Deke Slayton later wrote of how the sight of Sputnik I passing overhead inspired them to their new careers. Homer Hickam's memoir "Rocket Boys" and the movie "October Sky" tell the story of how a coal miner's son, inspired by Sputnik, started building rockets in the mining town where he lived.
The launch of "Sputnik 1" inspired U.S. writer Herb Caen to coin the term "beatnik" in an article about the Beat Generation in the "San Francisco Chronicle" on 2 April 1958.
External links.
Authentic recordings of the signal.
This Russian page contains signals which are probably the faster pulsations from Sputnik-2:
A NASA history website on Sputnik contains this commonly copied recording, which is some pulse-duration-modulated signal of an unknown spacecraft :

</doc>
<doc id="28506" url="http://en.wikipedia.org/wiki?curid=28506" title="Spacecraft propulsion">
Spacecraft propulsion

Spacecraft propulsion is any method used to accelerate spacecraft and artificial satellites. There are many different methods. Each method has drawbacks and advantages, and spacecraft propulsion is an active area of research. However, most spacecraft today are propelled by forcing a gas from the back/rear of the vehicle at very high speed through a supersonic de Laval nozzle. This sort of engine is called a rocket engine.
All current spacecraft use chemical rockets (bipropellant or solid-fuel) for launch, though some (such as the Pegasus rocket and SpaceShipOne) have used air-breathing engines on their first stage. Most satellites have simple reliable chemical thrusters (often monopropellant rockets) or resistojet rockets for orbital station-keeping and some use momentum wheels for attitude control. Soviet bloc satellites have used electric propulsion for decades, and newer Western geo-orbiting spacecraft are starting to use them for north-south stationkeeping and orbit raising. Interplanetary vehicles mostly use chemical rockets as well, although a few have used ion thrusters and Hall effect thrusters (two different types of electric propulsion) to great success.
Requirements.
Artificial satellites must be launched into orbit and once there they must be placed in their nominal orbit. Once in the desired orbit, they often need some form of attitude control so that they are correctly pointed with respect to Earth, the Sun, and possibly some astronomical object of interest. They are also subject to drag from the thin atmosphere, so that to stay in orbit for a long period of time some form of propulsion is occasionally necessary to make small corrections (orbital stationkeeping). Many satellites need to be moved from one orbit to another from time to time, and this also requires propulsion. A satellite's useful life is over once it has exhausted its ability to adjust its orbit.
Spacecraft designed to travel further also need propulsion methods. They need to be launched out of the Earth's atmosphere just as satellites do. Once there, they need to leave orbit and move around.
For interplanetary travel, a spacecraft must use its engines to leave Earth orbit. Once it has done so, it must somehow make its way to its destination. Current interplanetary spacecraft do this with a series of short-term trajectory adjustments. In between these adjustments, the spacecraft simply falls freely along its trajectory. The most fuel-efficient means to move from one circular orbit to another is with a Hohmann transfer orbit: the spacecraft begins in a roughly circular orbit around the Sun. A short period of thrust in the direction of motion accelerates or decelerates the spacecraft into an elliptical orbit around the Sun which is tangential to its previous orbit and also to the orbit of its destination. The spacecraft falls freely along this elliptical orbit until it reaches its destination, where another short period of thrust accelerates or decelerates it to match the orbit of its destination. Special methods such as aerobraking or aerocapture are sometimes used for this final orbital adjustment.
Some spacecraft propulsion methods such as solar sails provide very low but inexhaustible thrust; an interplanetary vehicle using one of these methods would follow a rather different trajectory, either constantly thrusting against its direction of motion in order to decrease its distance from the Sun or constantly thrusting along its direction of motion to increase its distance from the Sun. The concept has been successfully tested by the Japanese IKAROS solar sail spacecraft.
Spacecraft for interstellar travel also need propulsion methods. No such spacecraft has yet been built, but many designs have been discussed. Because interstellar distances are very great, a tremendous velocity is needed to get a spacecraft to its destination in a reasonable amount of time. Acquiring such a velocity on launch and getting rid of it on arrival will be a formidable challenge for spacecraft designers.
Effectiveness.
When in space, the purpose of a propulsion system is to change the velocity, or "v", of a spacecraft. Because this is more difficult for more massive spacecraft, designers generally discuss momentum, "mv". The amount of change in momentum is called impulse. So the goal of a propulsion method in space is to create an impulse.
When launching a spacecraft from Earth, a propulsion method must overcome a higher gravitational pull to provide a positive net acceleration.
In orbit, any additional impulse, even very tiny, will result in a change in the orbit path.
The rate of change of velocity is called acceleration, and the rate of change of momentum is called force. To reach a given velocity, one can apply a small acceleration over a long period of time, or one can apply a large acceleration over a short time. Similarly, one can achieve a given impulse with a large force over a short time or a small force over a long time. This means that for maneuvering in space, a propulsion method that produces tiny accelerations but runs for a long time can produce the same impulse as a propulsion method that produces large accelerations for a short time. When launching from a planet, tiny accelerations cannot overcome the planet's gravitational pull and so cannot be used.
Earth's surface is situated fairly deep in a gravity well. The escape velocity required to get out of it is 11.2 kilometers/second. As human beings evolved in a gravitational field of 1g (9.8 m/s²), an ideal propulsion system would be one that provides a continuous acceleration of 1g (though human bodies can tolerate much larger accelerations over short periods). The occupants of a rocket or spaceship having such a propulsion system would be free from all the ill effects of free fall, such as nausea, muscular weakness, reduced sense of taste, or leaching of calcium from their bones.
The law of conservation of momentum means that in order for a propulsion method to change the momentum of a space craft it must change the momentum of something else as well. A few designs take advantage of things like magnetic fields or light pressure in order to change the spacecraft's momentum, but in free space the rocket must bring along some mass to accelerate away in order to push itself forward. Such mass is called reaction mass.
In order for a rocket to work, it needs two things: reaction mass and energy. The impulse provided by launching a particle of reaction mass having mass "m" at velocity "v" is "mv". But this particle has kinetic energy "mv"²/2, which must come from somewhere. In a conventional solid, liquid, or hybrid rocket, the fuel is burned, providing the energy, and the reaction products are allowed to flow out the back, providing the reaction mass. In an ion thruster, electricity is used to accelerate ions out the back. Here some other source must provide the electrical energy (perhaps a solar panel or a nuclear reactor), whereas the ions provide the reaction mass.
When discussing the efficiency of a propulsion system, designers often focus on effectively using the reaction mass. Reaction mass must be carried along with the rocket and is irretrievably consumed when used. One way of measuring the amount of impulse that can be obtained from a fixed amount of reaction mass is the specific impulse, the impulse per unit weight-on-Earth (typically designated by formula_1). The unit for this value is seconds. Because the weight on Earth of the reaction mass is often unimportant when discussing vehicles in space, specific impulse can also be discussed in terms of impulse per unit mass. This alternate form of specific impulse uses the same units as velocity (e.g. m/s), and in fact it is equal to the effective exhaust velocity of the engine (typically designated formula_2). Confusingly, both values are sometimes called specific impulse. The two values differ by a factor of "g"n, the standard acceleration due to gravity 9.80665 m/s² (formula_3).
A rocket with a high exhaust velocity can achieve the same impulse with less reaction mass. However, the energy required for that impulse is proportional to the exhaust velocity, so that more mass-efficient engines require much more energy, and are typically less energy efficient. This is a problem if the engine is to provide a large amount of thrust. To generate a large amount of impulse per second, it must use a large amount of energy per second. So high-mass-efficient engines require enormous amounts of energy per second to produce high thrusts. As a result, most high-mass-efficient engine designs also provide lower thrust due to the unavailability of high amounts of energy.
Methods.
Propulsion methods can be classified based on their means of accelerating the reaction mass. There are also some special methods for launches, planetary arrivals, and landings.
Reaction engines.
A reaction engine is an engine which provides propulsion by expelling reaction mass, in accordance with Newton's third law of motion. This law of motion is most commonly paraphrased as: "For every action force there is an equal, but opposite, reaction force".
Examples include both duct engines and rocket engines, and more uncommon variations such as Hall effect thrusters, ion drives and mass drivers. Duct engines are obviously not used for space propulsion due to the lack of air; however some proposed spacecraft have these kinds of engines to assist takeoff and landing.
Delta-v and propellant.
Exhausting the entire usable propellant of a spacecraft through the engines in a straight line in free space would produce a net velocity change to the vehicle; this number is termed 'delta-v' (formula_4).
If the exhaust velocity is constant then the total formula_4 of a vehicle can be calculated using the rocket equation, where "M" is the mass of propellant, "P" is the mass of the payload (including the rocket structure), and formula_6 is the velocity of the rocket exhaust. This is known as the Tsiolkovsky rocket equation:
For historical reasons, as discussed above, formula_6 is sometimes written as
where formula_1 is the specific impulse of the rocket, measured in seconds, and formula_11 is the gravitational acceleration at sea level.
For a high delta-v mission, the majority of the spacecraft's mass needs to be reaction mass. Because a rocket must carry all of its reaction mass, most of the initially-expended reaction mass goes towards accelerating reaction mass rather than payload. If the rocket has a payload of mass "P", the spacecraft needs to change its velocity by
formula_4, and the rocket engine has exhaust velocity "ve", then the mass "M" of reaction mass which is needed can be calculated using the rocket equation and the formula for formula_1:
For formula_4 much smaller than "ve", this equation is roughly linear, and little reaction mass is needed. If formula_4 is comparable to "ve", then there needs to be about twice as much fuel as combined payload and structure (which includes engines, fuel tanks, and so on). Beyond this, the growth is exponential; speeds much higher than the exhaust velocity require very high ratios of fuel mass to payload and structural mass.
For a mission, for example, when launching from or landing on a planet, the effects of gravitational attraction and any atmospheric drag must be overcome by using fuel. It is typical to combine the effects of these and other effects into an effective mission delta-v. For example a launch mission to low Earth orbit requires about 9.3–10 km/s delta-v. These mission delta-vs are typically numerically integrated on a computer.
Some effects such as Oberth effect can only be significantly utilised by high thrust engines such as rockets, i.e. engines that can produce a high g-force (thrust per unit mass, equal to delta-v per unit time).
Power use and propulsive efficiency.
For all reaction engines (such as rockets and ion drives) some energy must go into accelerating the reaction mass.
Every engine will waste some energy, but even assuming 100% efficiency, to accelerate an exhaust the engine will need energy amounting to
This energy is not necessarily lost- some of it usually ends up as kinetic energy of the vehicle, and the rest is wasted in residual motion of the exhaust.
Comparing the rocket equation (which shows how much energy ends up in the final vehicle) and the above equation (which shows the total energy required) shows that even with 100% engine efficiency, certainly not all energy supplied ends up in the vehicle - some of it, indeed usually most of it, ends up as kinetic energy of the exhaust.
The exact amount depends on the design of the vehicle, and the mission. However there are some useful fixed points:
Some drives (such as VASIMR or Electrodeless plasma thruster) actually can significantly vary their exhaust velocity. This can help reduce propellant usage or improve acceleration at different stages of the flight. However the best energetic performance and acceleration is still obtained when the exhaust velocity is close to the vehicle speed. Proposed ion and plasma drives usually have exhaust velocities enormously higher than that ideal (in the case of VASIMR the lowest quoted speed is around 15000 m/s compared to a mission delta-v from high Earth orbit to Mars of about 4000m/s).
It might be thought that adding power generation capacity is helpful, and although initially this can improve performance, this inevitably increases the weight of the power source, and eventually the mass of the power source and the associated engines and propellant dominates the weight of the vehicle, and then adding more power gives no significant improvement.
For, although solar power and nuclear power are virtually unlimited sources of "energy", the maximum "power" they can supply is substantially proportional to the mass of the powerplant (i.e. specific power takes a largely constant value which is dependent on the particular powerplant technology). For any given specific power, with a large formula_2 which is desirable to save propellant mass, it turns out that the maximum acceleration is inversely proportional to formula_2. Hence the time to reach a required delta-v is proportional to formula_2. Thus the latter should not be too large.
Energy.
In the ideal case formula_23 is useful payload and formula_24 is reaction mass (this corresponds to empty tanks having no mass, etc.). The energy required can simply be computed as
This corresponds to the kinetic energy the expelled reaction mass would have at a speed equal to the exhaust speed. If the reaction mass had to be accelerated from zero speed to the exhaust speed, all energy produced would go into the reaction mass and nothing would be left for kinetic energy gain by the rocket and payload. However, if the rocket already moves and accelerates (the reaction mass is expelled in the direction opposite to the direction in which the rocket moves) less kinetic energy is added to the reaction mass. To see this, if, for example, formula_6=10 km/s and the speed of the rocket is 3 km/s, then the speed of a small amount of expended reaction mass changes from 3 km/s forwards to 7 km/s rearwards. Thus, although the energy required is 50 MJ per kg reaction mass, only 20 MJ is used for the increase in speed of the reaction mass. The remaining 30 MJ is the increase of the kinetic energy of the rocket and payload.
In general:
Thus the specific energy gain of the rocket in any small time interval is the energy gain of the rocket including the remaining fuel, divided by its mass, where the energy gain is equal to the energy produced by the fuel minus the energy gain of the reaction mass. The larger the speed of the rocket, the smaller the energy gain of the reaction mass; if the rocket speed is more than half of the exhaust speed the reaction mass even loses energy on being expelled, to the benefit of the energy gain of the rocket; the larger the speed of the rocket, the larger the energy loss of the reaction mass.
We have
where formula_29 is the specific energy of the rocket (potential plus kinetic energy) and formula_4 is a separate variable, not just the change in formula_31. In the case of using the rocket for deceleration, i.e. expelling reaction mass in the direction of the velocity, formula_31 should be taken negative.
The formula is for the ideal case again, with no energy lost on heat, etc. The latter causes a reduction of thrust, so it is a disadvantage even when the objective is to lose energy (deceleration).
If the energy is produced by the mass itself, as in a chemical rocket, the fuel value has to be formula_33, where for the fuel value also the mass of the oxidizer has to be taken into account. A typical value is formula_34 = 4.5 km/s, corresponding to a fuel value of 10.1 MJ/kg. The actual fuel value is higher, but much of the energy is lost as waste heat in the exhaust that the nozzle was unable to extract.
The required energy formula_35 is
Conclusions:
These results apply for a fixed exhaust speed.
Due to the Oberth effect and starting from a nonzero speed, the required potential energy needed from the propellant may be "less" than the increase in energy in the vehicle and payload. This can be the case when the reaction mass has a lower speed after being expelled than before – rockets are able to liberate some or all of the initial kinetic energy of the propellant.
Also, for a given objective such as moving from one orbit to another, the required formula_4 may depend greatly on the rate at which the engine can produce formula_4 and maneuvers may even be impossible if that rate is too low. For example, a launch to LEO normally requires a formula_4 of ca. 9.5 km/s (mostly for the speed to be acquired), but if the engine could produce formula_4 at a rate of only slightly more than "g", it would be a slow launch requiring altogether a very large formula_4 (think of hovering without making any progress in speed or altitude, it would cost a formula_4 of 9.8 m/s each second). If the possible rate is only formula_48 or less, the maneuver can not be carried out at all with this engine.
The power is given by
where formula_50 is the thrust and formula_51 the acceleration due to it. Thus the theoretically possible thrust per unit power is 2 divided by the specific impulse in m/s. The thrust efficiency is the actual thrust as percentage of this.
If e.g. solar power is used this restricts formula_51; in the case of a large formula_34 the possible acceleration is inversely proportional to it, hence the time to reach a required delta-v is proportional to formula_34; with 100% efficiency:
Examples:
Thus formula_34 should not be too large.
Power to thrust ratio.
The power to thrust ratio is simply:
Thus for any vehicle power P, the thrust that may be provided is:
Example.
Suppose we want to send a 10,000 kg space probe to Mars. The required formula_4 from LEO is approximately 3000 m/s, using a Hohmann transfer orbit. For the sake of argument, let us say that the following thrusters may be used:
Observe that the more fuel-efficient engines can use far less fuel; its mass is almost negligible (relative to the mass of the payload and the engine itself) for some of the engines. However, note also that these require a large total amount of energy. For Earth launch, engines require a thrust to weight ratio of more than one. To do this with the ion or more theoretical electrical drives, the engine would have to be supplied with one to several gigawatts of power — equivalent to a major metropolitan generating station. From the table it can be seen that this is clearly impractical with current power sources.
Alternative approaches include some forms of laser propulsion, where the reaction mass does not provide the energy required to accelerate it, with the energy instead being provided from an external laser or other Beam-powered propulsion system. Small models of some of these concepts have flown, although the engineering problems are complex and the ground based power systems are not a solved problem.
Instead, a much smaller, less powerful generator may be included which will take much longer to generate the total energy needed. This lower power is only sufficient to accelerate a tiny amount of fuel per second, and would be insufficient for launching from Earth. However, over long periods in orbit where there is no friction, the velocity will be finally achieved. For example, it took the SMART-1 more than a year to reach the Moon, whereas with a chemical rocket it takes a few days. Because the ion drive needs much less fuel, the total launched mass is usually lower, which typically results in a lower overall cost, but the journey takes longer.
Mission planning therefore frequently involves adjusting and choosing the propulsion system so as to minimise the total cost of the project, and can involve trading off launch costs and mission duration against payload fraction.
Rocket engines.
Most rocket engines are internal combustion heat engines (although non combusting forms exist). Rocket engines generally produce a high temperature reaction mass, as a hot gas. This is achieved by combusting a solid, liquid or gaseous fuel with an oxidiser within a combustion chamber. The extremely hot gas is then allowed to escape through a high-expansion ratio nozzle. This bell-shaped nozzle is what gives a rocket engine its characteristic shape. The effect of the nozzle is to dramatically accelerate the mass, converting most of the thermal energy into kinetic energy. Exhaust speed reaching as high as 10 times the speed of sound at sea level are common.
Rocket engines provide essentially the highest specific powers and high specific thrusts of any engine used for spacecraft propulsion.
Ion propulsion rockets can heat a plasma or charged gas inside a magnetic bottle and release it via a magnetic nozzle, so that no solid matter need come in contact with the plasma. Of course, the machinery to do this is complex, but research into nuclear fusion has developed methods, some of which have been proposed to be used in propulsion systems, and some have been tested in a lab.
See rocket engine for a listing of various kinds of rocket engines using different heating methods, including chemical, electrical, solar, and nuclear.
Electromagnetic propulsion.
Rather than relying on high temperature and fluid dynamics to accelerate the reaction mass to high speeds, there are a variety of methods that use electrostatic or electromagnetic forces to accelerate the reaction mass directly. Usually the reaction mass is a stream of ions. Such an engine typically uses electric power, first to ionize atoms, and then to create a voltage gradient to accelerate the ions to high exhaust velocities.
The idea of electric propulsion dates back to 1906, when Robert Goddard considered the possibility in his personal notebook.
Konstantin Tsiolkovsky published the idea in 1911.
For these drives, at the highest exhaust speeds, energetic efficiency and thrust are all inversely proportional to exhaust velocity. Their very high exhaust velocity means they require huge amounts of energy and thus with practical power sources provide low thrust, but use hardly any fuel.
For some missions, particularly reasonably close to the Sun, solar energy may be sufficient, and has very often been used, but for others further out or at higher power, nuclear energy is necessary; engines drawing their power from a nuclear source are called nuclear electric rockets.
With any current source of electrical power, chemical, nuclear or solar, the maximum amount of power that can be generated limits the amount of thrust that can be produced to a small value. Power generation adds significant mass to the spacecraft, and ultimately the weight of the power source limits the performance of the vehicle.
Current nuclear power generators are approximately half the weight of solar panels per watt of energy supplied, at terrestrial distances from the Sun. Chemical power generators are not used due to the far lower total available energy. Beamed power to the spacecraft shows some potential.
Some electromagnetic methods:
In electrothermal and electromagnetic thrusters, both ions and electrons are accelerated simultaneously, no neutralizer is required.
Without internal reaction mass.
The law of conservation of momentum is usually taken to imply that any engine which uses no reaction mass cannot accelerate the center of mass of a spaceship (changing orientation, on the other hand, is possible). But space is not empty, especially space inside the Solar System; there are gravitation fields, magnetic fields, electromagnetic waves, solar wind and solar radiation. Electromagnetic waves in particular are known to contain momentum, despite being massless; specifically the momentum flux density P of an EM wave is quantitatively 1/c times the Poynting vector S, i.e. P = S/c, where c is the velocity of light. Field propulsion methods which do not rely on reaction mass thus must try to take advantage of this fact by coupling to a momentum-bearing field such as an EM wave that exists in the vicinity of the craft. However, because many of these phenomena are diffuse in nature, corresponding propulsion structures need to be proportionately large.
There are several different space drives that need little or no reaction mass to function. A tether propulsion system employs a long cable with a high tensile strength to change a spacecraft's orbit, such as by interaction with a planet's magnetic field or through momentum exchange with another object. Solar sails rely on radiation pressure from electromagnetic energy, but they require a large collection surface to function effectively. The magnetic sail deflects charged particles from the solar wind with a magnetic field, thereby imparting momentum to the spacecraft. A variant is the mini-magnetospheric plasma propulsion system, which uses a small cloud of plasma held in a magnetic field to deflect the Sun's charged particles. An E-sail would use very thin and lightweight wires holding an electric charge to deflect these particles, and may have more controllable directionality.
As a proof of concept, NanoSail-D became the first nanosatellite to orbit Earth. There are plans to add them to future Earth orbit satellites, enabling them to de-orbit and burn up once they are no longer needed. Cube sail aims to tackle space junk.
Japan also launched its own solar sail powered spacecraft IKAROS in May 2010. IKAROS successfully demonstrated propulsion and guidance and is still flying today.
A satellite or other space vehicle is subject to the law of conservation of angular momentum, which constrains a body from a net change in angular velocity. Thus, for a vehicle to change its relative orientation without expending reaction mass, another part of the vehicle may rotate in the opposite direction. Non-conservative external forces, primarily gravitational and atmospheric, can contribute up to several degrees per day to angular momentum, so secondary systems are designed to "bleed off" undesired rotational energies built up over time. Accordingly, many spacecraft utilize reaction wheels or control moment gyroscopes to control orientation in space.
A gravitational slingshot can carry a space probe onward to other destinations without the expense of reaction mass. By harnessing the gravitational energy of other celestial objects, the spacecraft can pick up kinetic energy. However, even more energy can be obtained from the gravity assist if rockets are used.
Planetary and atmospheric propulsion.
Launch-assist mechanisms.
There have been many ideas proposed for launch-assist mechanisms that have the potential of drastically reducing the cost of getting into orbit. Proposed non-rocket spacelaunch launch-assist mechanisms include:
Airbreathing engines.
Studies generally show that conventional air-breathing engines, such as ramjets or turbojets are basically too heavy (have too low a thrust/weight ratio) to give any significant performance improvement when installed on a launch vehicle itself. However, launch vehicles can be air launched from separate lift vehicles (e.g. B-29, Pegasus Rocket and White Knight) which do use such propulsion systems. Jet engines mounted on a launch rail could also be so used.
On the other hand, very lightweight or very high speed engines have been proposed that take advantage of the air during ascent:
Normal rocket launch vehicles fly almost vertically before rolling over at an altitude of some tens of kilometers before burning sideways for orbit; this initial vertical climb wastes propellant but is optimal as it greatly reduces airdrag. Airbreathing engines burn propellant much more efficiently and this would permit a far flatter launch trajectory, the vehicles would typically fly approximately tangentially to Earth's surface until leaving the atmosphere then perform a rocket burn to bridge the final delta-v to orbital velocity.
Planetary arrival and landing.
When a vehicle is to enter orbit around its destination planet, or when it is to land, it must adjust its velocity. This can be done using all the methods listed above (provided they can generate a high enough thrust), but there are a few methods that can take advantage of planetary atmospheres and/or surfaces.
Hypothetical methods.
A variety of hypothetical propulsion techniques have been considered that would require entirely new principles of physics to be realized or that may not exist. To date, such methods are highly speculative and include:
A NASA assessment is found at "Assessing potential propulsion breakthroughs" (2005) and an overview of NASA research in this area is at .
Table of methods.
Below is a summary of some of the more popular, proven technologies, followed by increasingly speculative methods.
Four numbers are shown. The first is the effective exhaust velocity: the equivalent speed that the propellant leaves the vehicle. This is not necessarily the most important characteristic of the propulsion method; thrust and power consumption and other factors can be. However:
The second and third are the typical amounts of thrust and the typical burn times of the method. Outside a gravitational potential small amounts of thrust applied over a long period will give the same effect as large amounts of thrust over a short period. (This result does not apply when the object is significantly influenced by gravity.)
The fourth is the maximum delta-v this technique can give (without staging). For rocket-like propulsion systems this is a function of mass fraction and exhaust velocity. Mass fraction for rocket-like systems is usually limited by propulsion system weight and tankage weight. For a system to achieve this limit, typically the payload may need to be a negligible percentage of the vehicle, and so the practical limit on some systems can be much lower.
Testing.
Spacecraft propulsion systems are often first statically tested on Earth's surface, within the atmosphere but many systems require a vacuum chamber to test fully. Rockets are usually tested at a rocket engine test facility well away from habitation and other buildings for safety reasons. Ion drives are far less dangerous and require much less stringent safety, usually only a large-ish vacuum chamber is needed.
Famous static test locations can be found at Rocket Ground Test Facilities
Some systems cannot be adequately tested on the ground and test launches may be employed at a Rocket Launch Site.
Notes.
</dl>

</doc>
<doc id="28578" url="http://en.wikipedia.org/wiki?curid=28578" title="Steve Jackson">
Steve Jackson

Steve Jackson may refer to:

</doc>
<doc id="28582" url="http://en.wikipedia.org/wiki?curid=28582" title="Song of Songs">
Song of Songs

The Song of Songs, also known as the Song of Solomon or Canticles (Hebrew: שִׁיר הַשִּׁירִים "Šîr HašŠîrîm" ; Greek: ᾎσμα ᾈσμάτων "asma asmaton", both meaning "song of songs"), is one of the "megillot" (scrolls) of the "Ketuvim" (the "Writings", the last section of the "Tanakh" or Hebrew Bible), and the fifth of the "wisdom" books of the Christian Old Testament.
Scripturally, the Song of Songs is unique in that it makes no reference to "Law", "Covenant" or to Yahweh, the God of Israel, nor does it teach or explore "wisdom" in the manner of Proverbs or Ecclesiastes (although it does have some affinities to Wisdom literature, as the ascription to Solomon suggests). Instead, it celebrates sexual love. It gives "the voices of two lovers, praising each other, yearning for each other, proffering invitations to enjoy". The two are in harmony, each desiring the other and rejoicing in sexual intimacy; the women (or "daughters") of Jerusalem form a chorus to the lovers, functioning as an audience whose participation in the lovers' erotic encounters facilitates the participation of the reader.
In modern Judaism, the Song is read on the Sabbath during the Passover, which marks the beginning of the grain harvest as well as commemorating the Exodus from Egypt. Jewish tradition reads it as an allegory of the relationship between God and Israel. Christian tradition, in addition to appreciating the literal meaning of a romantic song between man and woman, has read the poem as an allegory of Christ (the bridegroom) and his Church (the bride).
Structure.
There is widespread consensus that, although the book has no plot, it does have what can be called a framework, as indicated by the links between its beginning and end. Beyond this, however, there appears to be little agreement: attempts to find a chiastic structure have not been compelling, and attempts to analyse it into units have used differing methods and arrived at differing results. The following must therefore be taken as indicative rather than determinative:
Summary.
The introduction calls the poem "the song of songs", a superlative construction commonly used in the Scripture to show it as the greatest and most beautiful of all songs (as in Holy of Holies). It begins with the woman's expression of desire for her lover and her self-description to the "daughters of Jerusalem". She says she is "black" because she had to work in the vineyards and got burned by the sun. A dialogue between the lovers follows: the woman asks the man to meet; he replies with a lightly teasing tone. The two compete in offering flattering compliments ("my beloved is to me as a cluster of henna blossoms in the vineyards of En Gedi", "an apple tree among the trees of the wood", "a lily among brambles", while the bed they share is like a forest canopy). The section closes with the woman telling the daughters of Jerusalem not to stir up love such as hers until it is ready.
The woman recalls a visit from her lover in the springtime. She uses imagery from a shepherd's life, and she says of her lover that "he pastures his flock among the lilies".
The woman again addresses the daughters of Jerusalem, describing her fervent and ultimately successful search for her lover through the night-time streets of the city. When she finds him she takes him almost by force into the chamber in which her mother conceived her. She reveals that this a dream, seen on her "bed at night" and ends by again warning the daughters of Jerusalem "not to stir up love until it is ready".
The next section reports a royal wedding procession. Solomon is mentioned by name, and the daughters of Jerusalem are invited to come out and see the spectacle.
The man describes his beloved: Her hair is like a flock of goats, her teeth like shorn ewes, and so on from face to breasts. Place-names feature heavily: her neck is like the Tower of David, her smell like the scent of Lebanon. He hastens to summon his beloved, saying that he is ravished by even a single glance. The section becomes a "garden poem", in which he describes her as a "locked garden" (usually taken to mean that she is chaste). The woman invites the man to enter the garden and taste the fruits. The man accepts the invitation, and a third party tells them to eat, drink, "and be drunk with love".
The woman tells the daughters of Jerusalem of another dream. She was in her chamber when her lover knocked. She was slow to open, and when she did, he was gone. She searched through the streets again, but this time she failed to find him and the watchmen, who had helped her before, now beat her. She asks the daughters of Jerusalem to help her find him, and describes his physical good looks. Eventually, she admits her lover is in his garden, safe from harm, and committed to her as she is to him.
The man describes his beloved; the woman describes a rendezvous they have shared. (The last part is unclear and possibly corrupted.)
The people praise the beauty of the woman. The images are the same as those used elsewhere in the poem, but with an unusually dense use of place-names, e.g., pools of Hebron, gate of Bath-rabbim, tower of Damascus, etc. The man states his intention to enjoy the fruits of the woman's garden. The woman invites him to a tryst in the fields. She once more warns the daughters of Jerusalem against waking love until it is ready.
The woman compares love to death and sheol: love is as relentless and jealous as these two, and cannot be quenched by any force. She summons her lover, using the language used before: he should come "like a gazelle or a young stag upon the mountain of spices".
Composition.
The Song offers no clue to its author or to the date, place or circumstances of its composition. The superscription states that it is "Solomon's", but even if this is meant to identify the author, it cannot be read as strictly as a similar modern statement. The most reliable evidence for its date is its language: Aramaic gradually replaced Hebrew after the end of the Babylonian exile in the late 6th century BCE, and the evidence of vocabulary, morphology, idiom and syntax clearly points to a late date, centuries after King Solomon to whom it is traditionally attributed.
It has long been recognised that the Song has parallels with the pastoral idylls of Theocritus, a Greek poet who wrote in the first half of the 3rd century BCE; against this, it clearly shows the influence of Mesopotamian and Egyptian love-poetry. It appears closer to Egyptian love-poetry from the first half of the 1st millennium than to Greek parallels from the last. As a result of these conflicting signs, speculation ranges from the 10th to the 2nd centuries BCE, with the cumulative evidence supporting a later rather than an earlier date.
The unity (or lack thereof) of the Song continues to be debated. Those who see it as an anthology or collection point to the abrupt shifts of scene, speaker, subject matter and mood, and the lack of obvious structure or narrative. Those who hold it to be a single poem point out that it has no internal signs of composite origins, and view the repetitions and similarities among its parts as evidence of unity. Some claim to find a conscious artistic design underlying it, but there is no agreement among them on what this might be. The question therefore remains unresolved.
The setting in which the poem arose is also debated. Some academics posit a ritual origin in the celebration of the sacred marriage of the god Tammuz and the goddess Ishtar. Whether this is so or not, (most scholars seem to doubt the idea), the poem seems to be rooted in some kind of festive performance. External evidence supports the idea that the Song was originally recited by different singers representing the different characters, accompanied by mime.
Later interpretation and influence.
Judaism.
The Song was accepted into the Jewish canon of scripture in the 2nd century CE, after a period of controversy in the 1st century. It was accepted as canonical because of its supposed authorship by Solomon and based on an allegorical reading where the subject-matter was taken to be not sexual desire but God's love for Israel.
It is one of the overtly mystical Biblical texts for the Kabbalah, which gave esoteric interpretation on all the Hebrew Bible. Following the dissemination of the Zohar in the 13th century, Jewish mysticism took on a metaphorically anthropomorphic erotic element, and Song of Songs is an example of this. In Zoharic Kabbalah, God is represented by a system of ten sephirot emanations, each symbolizing a different attribute of God, comprising both male and female. The Shechina (indwelling Divine presence) was identified with the feminine sephira Malchut, the vessel of Kingship. This symbolizes the Jewish people, and in the body, the female form, identified with the woman in Song of Songs.
Her beloved was identified with the male sephira Tiferet, the "Holy One Blessed be He", central principle in the beneficent Heavenly flow of Divine emotion. In the body, this represents the male torso, uniting through the sephira Yesod of the male sign of the covenant organ of procreation.
Through beneficent deeds and Jewish observance, the Jewish people restore cosmic harmony in the Divine realm, healing the exile of the Shechina with God's transcendence, revealing the essential Unity of God. This elevation of the World is aroused from Above on the Sabbath, a foretaste of the redeemed purpose of Creation. The text thus became a description, depending on the aspect, of the creation of the world, the passage of "Shabbat", the covenant with Israel, and the coming of the Messianic age. "Lecha Dodi", a 16th-century liturgical song with strong Kabbalistic symbolism, contains many passages, including its opening two words, taken directly from Song of Songs.
In modern Judaism, certain verses from the Song are read on "Shabbat" eve or at Passover to symbolize the love between the Jewish People and their God. Solomon B. Freehof writes of the Song:
As revealed in numerous talmudic passages, in the Targum and in the midrash, this biblical book is interpreted as referring to God's love for Israel. This interpretation (evidently the one ascribed to the "Keneset Hagdola" in Abot d'R. Nathan, Schechter, A #1) soon became official. In fact, anyone quoting verses from the Song of Songs giving them the "literal" meaning was declared a heretic who had forfeited his portion in Paradise (Tos. Sanh. XII, 10). This symbolic interpretation of the book was, with some re-interpretation, carried over into Christianity and there, too, it became official.
The famed first and second century rabbi Akiva ben Joseph (aka Rabbi Akiba) forbade the use of the Song of Songs in popular celebrations. He reportedly said, "He who sings the Song of Songs in wine taverns, treating it as if it were a vulgar song, forfeits his share in the world to come". However, Rabbi Akiba famously defended the canonicity of the Song of Songs, reportedly saying when the question came up of whether it should be considered a defiling work, "God forbid! […] For all of eternity in its entirety is not as worthy as the day on which Song of Songs was given to Israel, for all the Writings are holy, but Song of Songs is the Holy of Holies."
In modern Judaism, the Song is read on the Sabbath during the Passover, which marks the beginning of the grain harvest as well as commemorating the Exodus from Egypt. Jewish tradition reads it as an allegory of the relationship between God and Israel.
Christianity.
Christians admitted the canonicity of the Song of Songs from the beginning, but after Jewish exegetes began to read the Song allegorically, as having to do with God's love for his people, Christian exegetes followed suit, treating the love that it celebrates as an analogy for the love between God and the Church. Over the centuries the emphasis of interpretation shifted, the 11th century adding a moral element and the 12th century understanding the Bride as the Virgin Mary, each new reading absorbing rather than simply replacing earlier ones, so that the commentary became ever more complex, with multiple layers of meaning. This approach leads to conclusions not found in the more overtly theological books of the Bible, which consider the relationship between God and man as one of inequality. In contrast, reading the Song of Songs as an allegory of God's love for his Church suggests that the two partners are equals, bound in a freely consented emotional relationship.
Feminism.
In modern times, the poem has attracted the attention of feminist Biblical critics. "The Feminist Companion to the Bible" series, edited by Athalya Brenner, has two volumes (1993, 2001) devoted to the Song, the first of which was actually the first volume of the whole series. Phyllis Trible had earlier published "Depatriarchalizing in Biblical Interpretation" in 1973, offering a reading of the Song with a positive representation of sexuality and egalitarian gender relations, which was widely discussed, notably (and favourably) in Marvin Pope's major commentary for the Anchor Bible.

</doc>
<doc id="28603" url="http://en.wikipedia.org/wiki?curid=28603" title="Star cluster">
Star cluster

Star clusters or star clouds are groups of stars. Two types of star clusters can be distinguished: globular clusters are tight groups of hundreds or thousands of very old stars which are gravitationally bound, while open clusters, more loosely clustered groups of stars, generally contain fewer than a few hundred members, and are often very young. Open clusters become disrupted over time by the gravitational influence of giant molecular clouds as they move through the galaxy, but cluster members will continue to move in broadly the same direction through space even though they are no longer gravitationally bound; they are then known as a stellar association, sometimes also referred to as a "moving group".
Star clusters visible to the naked eye include Pleiades, Hyades and the Beehive Cluster.
Globular cluster.
Globular clusters, or GC, are roughly spherical groupings of from 10,000 to several million stars packed into regions of from 10 to 30 light years across. They commonly consist of very old Population II stars—just a few hundred million years younger than the universe itself—which are mostly yellow and red, with mass just less than two solar masses. Such stars predominate within clusters because hotter and more massive stars have exploded as supernovae, or evolved through planetary nebula phases to end as white dwarfs. Yet a few rare blue stars exist in globulars, thought to be formed by stellar mergers in their dense inner regions; these stars are known as blue stragglers.
In our galaxy, globular clusters are distributed roughly spherically in the galactic halo, around the galactic centre, orbiting the centre in highly elliptical orbits. In 1917, the astronomer Harlow Shapley was able to estimate the Sun's distance from the galactic centre based on the distribution of globular clusters; previously the Sun's location within the Milky Way was by no means well established.
Until recently, globular clusters were the cause of a great mystery in astronomy, as theories of stellar evolution gave ages for the oldest members of globular clusters that were greater than the estimated age of the universe. However, greatly improved distance measurements to globular clusters using the Hipparcos satellite and increasingly accurate measurements of the Hubble constant resolved the paradox, giving an age for the universe of about 13 billion years and an age for the oldest stars of a few hundred million years less.
Super star clusters, such as Westerlund 1 in the Milky Way, may be the precursors of globular clusters.
Our galaxy has about 150 globular clusters, some of which may have been captured from small galaxies disrupted by the Milky Way, as seems to be the case for the globular cluster M79. Some galaxies are much richer in globulars: the giant elliptical galaxy M87 contains over a thousand.
A few of the brightest globular clusters are visible to the naked eye, with the brightest, Omega Centauri, having been known since antiquity and catalogued as a star before the telescopic age. The best known globular cluster in the northern hemisphere is M13 (modestly called the Great Globular Cluster in Hercules).
Intermediate forms.
In 2005, astronomers discovered a completely new type of star cluster in the Andromeda Galaxy, which is, in several ways, very similar to globular clusters (although less dense). Currently, there are not any intermediate clusters (also known as "extended globular clusters") discovered in the Milky Way. The three discovered in Andromeda Galaxy are M31WFS C1 M31WFS C2, and M31WFS C3.
These new-found star clusters contain hundreds of thousands of stars, a similar number of stars that can be found in globular clusters. The clusters also share other characteristics with globular clusters, "e.g." the stellar populations and metallicity. What distinguishes them from the globular clusters is that they are much larger – several hundred light-years across – and hundreds of times less dense. The distances between the stars are, therefore, much greater within the newly discovered extended clusters. Parametrically, these clusters lie somewhere between a (low dark matter) globular cluster and a (dark matter-dominated) dwarf spheroidal galaxy.
How these clusters are formed is not yet known, but their formation might well be related to that of globular clusters. Why M31 has such clusters, while the Milky Way has not, is not yet known. It is also unknown if any other galaxy contains this kind of clusters, but it would be very unlikely that M31 is the sole galaxy with extended clusters.
Another type of cluster are "faint fuzzies" which so far have only been found in lenticular galaxies like NGC 1023 and NGC 3384. They are characterized by their large size compared to globular clusters and a ringlike distribution around the centers of their host galaxies. As the latter they seem to be old objects.
Open clusters.
Open clusters (OC) are very different from globular clusters. Unlike the spherically distributed globulars, they are confined to the galactic plane, and are almost always found within spiral arms. They are generally young objects, up to a few tens of millions of years old, with a few rare exceptions as old as a few billion years, such as Messier 67 (the closest and most observed old open cluster) for example. They form from H II regions such as the Orion Nebula.
Open clusters usually contain up to a few hundred members, within a region up to about 30 light-years across. Being much less densely populated than globular clusters, they are much less tightly gravitationally bound, and over time, are disrupted by the gravity of giant molecular clouds and other clusters. Close encounters between cluster members can also result in the ejection of evaporation'.
The most prominent open clusters are the Pleiades and Hyades in Taurus. The Double Cluster of h+Chi Persei can also be prominent under dark skies. Open clusters are often dominated by hot young blue stars, because although such stars are short-lived in stellar terms, only lasting a few tens of millions of years, open clusters tend to have dispersed before these stars die.
Establishing precise distances to open clusters enables the calibration of the famed period-luminosity relationship shown by Cepheids variable stars, which are then used as standard candles. Cepheids are luminous and can be used to establish both the distances to remote galaxies and the expansion rate of the Universe (Hubble constant). Indeed, the open cluster designated NGC 7790 hosts three classical Cepheids which are critical for such efforts.
Super star cluster.
Super star cluster (SSC) is a very large region of star formation thought to be the precursor of a globular cluster.
Embedded cluster.
Embedded clusters (EC) are stellar clusters that are partially or fully encased in an Interstellar dust or gas. The most famous example of an embedded cluster is the Trapezium cluster. In ρ Ophiuchi cloud (L1688) core region has an embedded cluster.
Stellar associations.
Once an open cluster has become gravitationally unbound, or if a newly formed group of stars fails to form a cluster, the constituent stars will continue to move on similar paths through space. The group is then known as a stellar association, or a moving group. Most of the stars in the Big Dipper are members of a former open cluster, the Ursa Major Moving Group, and have similar proper motions. Other stars across the sky, including Alphecca and Zeta Trianguli Australis, are related to this group. The Sun lies at the edge of this stream of stars at the moment, but isn't a member as is shown by its different galactic orbit, age, and chemical composition.
Another stellar association is that surrounding Mirfak (α Persei), which is very prominent in binoculars. Distant moving clusters cannot readily be detected since the proper motions of the stars need to be known.
Astronomical significance of clusters.
Stellar clusters are important in many areas of astronomy. Because the stars were all born at roughly the same time, the different properties of all the stars in a cluster are a function only of mass, and so stellar evolution theories rely on observations of open and globular clusters.
Clusters are also a crucial step in determining the distance scale of the universe. A few of the nearest clusters are close enough for their distances to be measured using parallax. A Hertzsprung–Russell Diagram can be plotted for these clusters which has absolute values known on the luminosity axis. Then, when similar diagram is plotted for a cluster whose distance is not known, the position of the main sequence can be compared to that of the first cluster and the distance estimated. This process is known as main-sequence fitting. Reddening and stellar populations must be accounted for when using this method.
Nomenclature.
In 1979, the International Astronomical Union's 17th general assembly recommended that newly discovered star clusters, open or globular, within the Galaxy have designations following the convention "Chhmm±ddd", always beginning with the prefix "C", where "h", "m", and "d" represent the approximate coordinates of the cluster center in hours and minutes of right ascension, and degrees of declination, respectively, with leading zeros. The designation, once assigned, is not to change, even if subsequent measurements improve on the location of the cluster center. The first of such designations were assigned by Gosta Lynga in 1982.

</doc>
<doc id="28607" url="http://en.wikipedia.org/wiki?curid=28607" title="Sergei Diaghilev">
Sergei Diaghilev

Sergei Pavlovich Diaghilev (; Russian: Серге́й Па́влович Дя́гилев, "Sergei Pavlovich Dyagilev"; ]; 31 March [O.S. 19 March] 1872 – 19 August 1929), usually referred to outside of Russia as Serge, was a Russian art critic, patron, ballet impresario and founder of the Ballets Russes, from which many famous dancers and choreographers would arise.
Early life and career.
Sergei Diaghilev was born to a wealthy and cultured family in Selishchi (Novgorod Governorate), Russia; his father, Pavel Pavlovich, was a cavalry colonel, but the family's money came mainly from vodka distilleries. After the death of Sergei's mother, his father married Elena Valerianovna Panaeva, an artistic young woman who was on very affectionate terms with her stepson and was a strong influence on him. The family lived in Perm but had an apartment in Saint Petersburg and a country estate in Bikbarda (near Perm). In 1890, Sergei's parents went bankrupt, having for a long time lived beyond their means, and from that time Sergei (who had a small income inherited from his mother) had to support the family. After graduating from Perm gymnasium in 1890, he went to the capital to study law at St. Petersburg University, but ended up also taking classes at the St. Petersburg Conservatory of Music, where he studied singing and music (a love of which he had picked up from his stepmother). After graduating in 1892 he abandoned his dreams of composition (his professor, Nikolai Rimsky-Korsakov, told him he had no talent for music). 
During his years at University, Diaghilev's cousin Dmitry Filosofov introduced him to a circle of art-loving friends who called themselves The Nevsky Pickwickians. They included Alexandre Benois, Walter Nouvel, Konstantin Somov, and Léon Bakst. Although not instantly received into the group, Diaghilev was aided by Benois in developing his knowledge of Russian and Western art. In two years, he had voraciously absorbed this new obsession (even travelling abroad to further his studies) and came to be respected as one of the most learned of the group. 
With financial backing from Savva Mamontov (the director of the Russian Private Opera Company) and Princess Maria Tenisheva, the group founded the journal "Mir iskusstva (World of Art)". In 1899, Diaghilev became special assistant to Prince Sergei Mikhaylovich Volkonsky, who had recently taken over directorship of all Imperial theaters. Diaghilev was soon responsible for the production of the "Annual of the Imperial Theaters" in 1900, and promptly offered assignments to his close friends: Léon Bakst would design costumes for the French play "Le Coeur de la Marquise", while Benois was given the opportunity to produce Alexander Taneyev's opera "Cupid's Revenge". 
In 1900–1901 Volkonsky entrusted Diaghilev with the staging of Léo Delibes' ballet "Sylvia", a favorite of Benois. The two collaborators concocted an elaborate production plan that startled the established personnel of the Imperial Theatres. After several increasingly antagonistic differences of opinion, Diaghilev in his demonstrative manner refused to go on editing the "Annual of the Imperial Theatres" and was discharged by Volkonsky in 1901 and left disgraced in the eyes of the nobility. At the same time, some of Diaghilev's researchers hinted at his homosexuality as the main cause for this conflict. However, his homosexuality had been well known long before he was invited into the Imperial Theatres.
Ballets Russes.
 
In 1905 he organized a huge exhibition of Russian portrait painting at the Tauride Palace in St. Petersburg, having travelled widely through Russia for a year discovering many previously unknown masterpieces of Russian portrait art. In the following year he took a major exhibition of Russian art to the Petit Palais in Paris. It was the beginning of a long involvement with France. In 1907 he presented five concerts of Russian music in Paris, and in 1908 mounted a production of Mussorgsky's "Boris Godunov", starring Feodor Chaliapin, at the Paris Opéra. 
This led to an invitation to return the following year with ballet as well as opera, and thus to the launching of his famous Ballets Russes. The company included the best young Russian dancers, among them Anna Pavlova, Adolph Bolm, Vaslav Nijinsky, Tamara Karsavina and Vera Karalli, and their first night on 19 May 1909 was a sensation.
During these years Diaghilev's stagings included several compositions by the late Nikolai Rimsky-Korsakov, such as the operas "The Maid of Pskov", "May Night", and "The Golden Cockerel". His balletic adaptation of the orchestral suite "Sheherazade", staged in 1910, drew the ire of the composer's widow, Nadezhda Rimskaya-Korsakova, who protested in open letters to Diaghilev published in the periodical "Rech." 
Diaghilev commissioned ballet music from composers such as Nikolai Tcherepnin ("Narcisse et Echo", 1911), Claude Debussy ("Jeux", 1913), Maurice Ravel ("Daphnis et Chloé", 1912), Erik Satie ("Parade", 1917), Manuel de Falla ("El Sombrero de Tres Picos", 1917), Richard Strauss ("Josephslegende", 1914), Sergei Prokofiev ("Ala and Lolli", 1915, rejected by Diaghilev and turned into the "Scythian Suite"; "Chout", 1915 revised 1920; "Le pas d'acier", 1926; and "The Prodigal Son", 1929); Ottorino Respighi ("La Boutique fantasque", 1919); Francis Poulenc ("Les biches", 1923) and others. His choreographer Michel Fokine often adapted the music for ballet. Diaghilev also worked with dancer and ballet master Léonide Massine.
The artistic director for the Ballets Russes was Léon Bakst. Together they developed a more complicated form of ballet with show-elements intended to appeal to the general public, rather than solely the aristocracy. The exotic appeal of the Ballets Russes had an effect on Fauvist painters and the nascent Art Deco style. 
Perhaps Diaghilev's most notable composer-collaborator, however, was Igor Stravinsky. Diaghilev heard Stravinsky's early orchestral works "Fireworks" and "Scherzo fantastique", and was impressed enough to ask Stravinsky to arrange some pieces by Chopin for the Ballets Russes. In 1910, he commissioned his first score from Stravinsky, "The Firebird". "Petrushka" (1911) and "The Rite of Spring" (1913) followed shortly afterwards, and the two also worked together on "Les noces" (1923) and "Pulcinella" (1920) together with Picasso, who designed the costumes and the set.
After the Russian Revolution of 1917, Diaghilev stayed abroad. The new Soviet regime, once it became obvious that he could not be lured back, condemned him in perpetuity as an especially insidious example of bourgeois decadence. Soviet art historians wrote him out of the picture for more than 60 years.
Diaghilev staged Tchaikovsky's "The Sleeping Beauty" in London in 1921; it was a production of remarkable magnificence in both settings and costumes but, despite being well received by the public, it was a financial disaster for Diaghilev and Oswald Stoll, the theatre-owner who had backed it. The first cast included the legendary ballerina Olga Spessivtseva and Lubov Egorova in the role of Aurora. Diaghilev insisted on calling the ballet "The Sleeping Princess". When asked why, he quipped, "Because I have no beauties!" The later years of the Ballets Russes were often considered too "intellectual", too "stylish" and seldom had the unconditional success of the first few seasons, although younger choreographers like George Balanchine hit their stride with the Ballet Russes.
The end of the 19th century brought a development in the handling of tonality, harmony, rhythm and meter towards more freedom. Until that time, rigid harmonic schemes had forced rhythmic patterns to stay fairly uncomplicated. Around the turn of the century, however, harmonic and metric devices became either more rigid, or much more unpredictable, and each approach had a liberating effect on rhythm, which also affected ballet. Diaghilev was a pioneer in adapting these new musical styles to modern ballet. When Ravel used a 5/4 time in the final part of his ballet "Daphnis and Chloe" (1912), dancers of the Ballets Russes sang "Ser-ge-dia-ghi-lev" during rehearsals to keep the correct rhythm. 
Members of Diaghilev's Ballets Russes later went on to found ballet traditions in the United States (George Balanchine) and England (Ninette de Valois and Marie Rambert). Ballet master Serge Lifar went on a technical revival at the Paris Opera Ballet, enhanced by Claude Bessy and Rudolf Nureyev in the 1980s. Lifar is credited for saving many Jewish and other minority dancers from the Nazi concentration camps during World War II.
Personal life.
Nijinsky's later bitter comments about Diaghilev inspired a mention in W. H. Auden's poem "September 1, 1939":
<poem>The windiest militant trash
Important Persons shout
Is not so crude as our wish:
What mad Nijinsky wrote
About Diaghilev
Is true of the normal heart;
For the error bred in the bone
Of each woman and each man
Craves what it cannot have, Not universal love
But to be loved alone.</poem>
Diaghilev was known as a hard, demanding, even frightening taskmaster. Ninette de Valois, no shrinking violet, said she was too afraid to ever look him in the face. George Balanchine said he carried around a cane during rehearsals, and banged it angrily when he was displeased. Other dancers said he would shoot them down with one look, or a cold comment. On the other hand, he was capable of great kindness, and when stranded with his bankrupt company in Spain during the 1914–18 war, gave his last bit of cash to Lydia Sokolova to buy medical care for her daughter. Alicia Markova was very young when she joined the Ballet Russes and would later say that she had called Diaghilev "Sergypops" and he had said he would take care of her like a daughter.
Diaghilev dismissed Nijinsky summarily from the Ballets Russes after the dancer's marriage in 1913. Nijinsky appeared again with the company, but the old relationship between the men was never re-established; moreover, Nijinsky's magic as a dancer was much diminished by incipient madness. Their last meeting was after Nijinsky's mind had given way, and he appeared not to recognise his former lover. Dancers such as Alicia Markova, Tamara Karsavina, Serge Lifar, and Lydia Sokolova remembered Diaghilev fondly, as a stern but kind father-figure who put the needs of his dancers and company above his own. He lived from paycheck to paycheck to finance his company, and though he spent considerable amounts of money on a splendid collection of rare books at the end of his life, many people noticed that his impeccably cut suits had frayed cuffs and trouser-ends. The film "The Red Shoes" is a thinly disguised dramatization of the Ballet Russes.
Death and legacy.
Throughout his life, Diaghilev was severely afraid of dying in water, and avoided traveling by boat. He died of diabetes in Venice on 19 August 1929, and is buried on the nearby island of San Michele.
The Ekstrom Collection of the Diaghilev and Stravinsky Foundation is held by the Department of Theatre and Performance of the Victoria and Albert Museum.

</doc>
<doc id="28622" url="http://en.wikipedia.org/wiki?curid=28622" title="Sukkot">
Sukkot

Sukkot, Succot or Sukkos (סוכות or סֻכּוֹת "sukkōt" or "sukkos", Feast of Booths, Feast of Tabernacles) is a biblical Jewish holiday celebrated on the 15th day of the month of Tishrei (varies from late September to late October). It is one of the Three Pilgrimage Festivals ("shalosh regalim") on which the Israelites would make a pilgrimage to the Temple in Jerusalem.
The holiday lasts seven days in Israel and eight in the diaspora. The first day (and second day in the diaspora) is a Shabbat-like holiday when work is forbidden, followed by intermediate days called Chol Hamoed. The festival is closed with another Shabbat-like holiday called Shemini Atzeret (two days in the diaspora, where the second day is called Simchat Torah).
The Hebrew word "sukkōt" is the plural of "sukkah", "booth" or "tabernacle", which is a walled structure covered with "s'chach" (plant material such as overgrowth or palm leaves). The sukkah is intended as a reminiscence of the type of fragile dwellings in which, according to the Torah, the Israelites dwelt during their 40 years of travel in the desert after the Exodus from slavery in Egypt. Throughout the holiday, meals are eaten inside the sukkah and many people sleep there as well. A sukkah is also the name of the temporary dwelling in which agricultural workers would live during harvesting.
On each day of the holiday it is mandatory to perform a waving ceremony with the Four Species.
Origins.
In the Book of Leviticus, God told Moses to command the people: 
The origins of Sukkot are both historical and agricultural. Historically, Sukkot commemorates the forty-year period during which the children of Israel were wandering in the desert, living in temporary shelters. Agriculturally, Sukkot is a harvest festival and is sometimes referred to as "Chag HaAsif" (חג האסיף, the "Festival of Ingathering"), as it celebrates the gathering of the harvest.
Laws and customs.
Sukkot is an eight-day holiday, with the first day celebrated as a full festival with special prayer services and holiday meals. The seventh day of Sukkot is called "Hoshana Rabbah" ("Great Hoshana", referring to the tradition that worshipers in the synagogue walk around the perimeter of the sanctuary during morning services) and has a special observance of its own. Outside Israel, the first and last two days are celebrated as full festivals. The intermediate days are known as "Chol HaMoed" ("festival weekdays"). According to Halakha, some types of work are forbidden during "Chol HaMoed". In Israel many businesses are closed during this time. 
Throughout the week of Sukkot, meals are eaten in the sukkah and the males sleep there, although the requirement is waived in case of rain. Every day, a blessing is recited over the Lulav and the Etrog.
Observance of Sukkot is detailed in the Book of Nehemiah and Leviticus 23:34-44 in the Bible, the Mishnah (Sukkah 1:1–5:8); the Tosefta (Sukkah 1:1–4:28); and the Jerusalem Talmud (Sukkah 1a–) and Babylonian Talmud (Sukkah 2a–56b).
Building a sukkah.
The sukkah walls can be constructed of any material (wood, canvas, aluminum siding, sheets). The walls can be free-standing or include the sides of a building or porch. The roof must be of organic material, known as s'chach, such as leafy tree overgrowth, schach mats or palm fronds. It is customary to decorate the interior of the sukkah with hanging decorations. the four species.
Special prayers.
Prayers during Sukkot include the reading of the Torah every day, reciting the Mussaf (additional) service after morning prayers, reciting Hallel, and adding special additions to the Amidah and Grace after Meals. In addition, the service includes rituals involving the Four Species. The lulav and etrog are not brought to the synagogue on Shabbat.
"Hoshanot".
On each day of the festival, worshippers walk around the synagogue carrying their Four species while reciting Psalm 118:25 and special prayers known as "Hoshanot". This takes place either after the morning's Torah reading or at the end of Mussaf. This ceremony commemorates the willow ceremony at the Temple in Jerusalem, in which willow branches were piled beside the altar with worshipers parading around the altar reciting prayers.
"Ushpizin".
A custom originating with Lurianic Kabbalah is to recite the "ushpizin" prayer to "invite" one of seven "exalted guests" into the sukkah. These "ushpizin" (Aramaic אושפיזין 'guests'), represent the seven shepherds of Israel: Abraham, Isaac, Jacob, Moses, Aaron, Joseph and David. According to tradition, each night a different guest enters the sukkah followed by the other six. Each of the "ushpizin" has a unique lesson which teaches the parallels of the spiritual focus of the day on which they visit.
Chol HaMoed.
The second through seventh days of Sukkot (third through seventh days outside Israel) are called Chol HaMoed (חול המועד - "festival weekdays"). These days are considered by halakha to be more than regular weekdays but less than festival days. In practice, this means that all activities that are needed for the holiday—such as buying and preparing food, cleaning the house in honor of the holiday, or traveling to visit other people's sukkot or on family outings—are permitted by Jewish law. Activities that will interfere with relaxation and enjoyment of the holiday—such as laundering, mending clothes, engaging in labor-intensive activities—are not permitted. Religious Jews typically treat Chol HaMoed as a vacation period, eating nicer than usual meals in their sukkah, entertaining guests, visiting other families in their sukkot, and taking family outings. Many synagogues and Jewish centers also offer events and meals in their sukkot during this time to foster community and goodwill.
On the Shabbat which falls during the week of Sukkot (or in the event when the first day of Sukkot is on Shabbat), the Book of Ecclesiastes is read during morning synagogue services in Israel. (Diaspora communities read it the second Shabbat {eighth day} when the first day of sukkot is on Shabbat. This Book's emphasis on the ephemeralness of life ("Vanity of vanities, all is vanity...") echoes the theme of the sukkah, while its emphasis on death reflects the time of year in which Sukkot occurs (the "autumn" of life). The penultimate verse reinforces the message that adherence to God and His Torah is the only worthwhile pursuit. (Cf. Ecclesiastes 12:13,14.)
Hakhel.
In the days of the Temple in Jerusalem, all Israelite, and later Jewish men, women, and children on pilgrimage to Jerusalem for the festival would gather in the Temple courtyard on the first day of Chol HaMoed Sukkot to hear the Jewish king read selections from the Torah. This ceremony, which was mandated in Deuteronomy 31:10-13, was held every seven years, in the year following the Shmita (Sabbatical) year. This ceremony was discontinued after the destruction of the Temple, but it has been revived in Israel on a smaller scale.
Simchat Beit HaShoevah.
During the intermediate days of Sukkot, gatherings of music and dance, known as "Simchat Beit HaShoeivah" (Celebration of the Place of Water-Drawing), take place. This commemorates the drawing of the water for the water-libation on the Altar, an offering unique to Sukkot, when water was carried up the Jerusalem pilgrim road from the Pool of Siloam to the Temple in Jerusalem.
Hoshana Rabbah.
The seventh day of Sukkot is known as "Hoshana Rabbah" (Great Supplication). This day is marked by a special synagogue service in which seven circuits are made by worshippers holding their Four Species, reciting Psalm 118:25 with additional prayers. In addition, a bundle of five willow branches is beaten on the ground.
Shemini Atzeret and Simchat Torah.
The holiday immediately following Sukkot is known as "Shemini Atzeret" ( "Eighth [Day] of Assembly"). Shemini Atzeret is usually viewed as a separate holiday. In the Diaspora a second additional holiday, "Simchat Torah" ("Joy of the Torah"), is celebrated. In the Land of Israel, Simchat Torah is celebrated on Shemini Atzeret. On Shemini Atzeret people leave their sukkah and eat their meals inside the house. Outside of Israel, many eat in the sukkah without making the blessing. The sukkah is not used on Simchat Torah.
In Christianity.
Sukkot is celebrated by a number of Christian denominations that observe holidays from the Old Testament. These groups base this on the fact that Jesus celebrated Sukkot (see the Gospel of John 7). The holiday is celebrated according to its Hebrew calendar dates. The first mention of observing the holiday by Christian groups dates to the 17th century, among the Subbotniks in Russia.
Academic views.
De Moor has suggested that there are links between Sukkot and the Ugaritic New Year festival, in particular the Ugaritic custom of erecting two rows of huts built of branches on the temple roof as temporary dwelling houses for their gods.
External links.
Jewish
Christian

</doc>
<doc id="28645" url="http://en.wikipedia.org/wiki?curid=28645" title="Smuggling">
Smuggling

Smuggling is the illegal transportation of objects or people, such as out of a house or buildings, into a prison, or across an international border, in violation of applicable laws or other regulations. 
There are various motivations to smuggle. These include the participation in illegal trade, such as in the drug trade, in illegal immigration or illegal emigration, tax evasion, providing contraband to a prison inmate, or the theft of the items being smuggled. Examples of non-financial motivations include bringing banned items past a security checkpoint (such as airline security) or the removal of classified documents from a government or corporate office.
Smuggling is a common theme in literature, from Bizet's "Carmen" to the James Bond books (and later films) "Diamonds are Forever" and "Goldfinger".
Etymology.
The verb "smuggle", from Low German "schmuggeln" or Dutch "smokkelen" (="to transport (goods) illegally"), apparently a frequentative formation of a word meaning "to sneak", most likely entered the English language during the 1600s–1700s.
History.
In germany smuggling first became a recognised problem in the 17th century, following the creation of a national customs collection system by Edward I in 127N.S.B. Gras, "The Early English Customs System" (OUP, 2015) Medieval smuggling tended to focus on the export of highly taxed export goods — notably wool and hides. Merchants also, however, sometimes smuggled other goods to circumvent prohibitions or embargoes on particular trades. Grain, for instance, was usually prohibited from export, unless prices were low, because of fears that grain exports would raise the price of food in England and thus cause food shortages and / or civil unrest. Following the loss of Gascony to the French in 1453, imports of wine were also sometimes embargoed during wars to try to deprive the French of the revenues that could be earned from their main export.
Most studies of historical smuggling have been based on official sources — such as court records, or the letters of Revenue Officers. According to (University of Bristol), the trouble with these is that 'they only detail the activities of those dumb enough to get caught'. This has led him and others, such as (University of Swansea) to use commercial records to reconstruct smuggling businesses. Jones' study focuses on smuggling in Bristol in the mid-16th century, arguing that the illicit export of goods like grain and leather represented a significant part of the city's business, with many members of the civic elite engaging in it. Grain smuggling by members of the civic elite, often working closely with corrupt customs officers, has also been shown to have been prevalent in East Anglia during the later 16th century.
In England wool was smuggled to the continent in the 17th century, under the pressure of high excise taxes. In 1724 Daniel Defoe wrote of Lymington, Hampshire, on the south coast of England
"I do not find they have any foreign commerce, except it be what we call smuggling and roguing; which I may say, is the reigning commerce of all this part of the English coast, from the mouth of the Thames to the Land's End in Cornwall."
The high rates of duty levied on tea and also wine and spirits, and other luxury goods coming in from mainland Europe at this time made the clandestine import of such goods and the evasion of the duty a highly profitable venture for impoverished fishermen and seafarers. In certain parts of the country such as the Romney Marsh, East Kent, Cornwall and East Cleveland, the smuggling industry was for many communities more economically significant than legal activities such as farming and fishing. The principal reason for the high duty was the need for the government to finance a number of extremely expensive wars with France and the United States.
Before the era of drug smuggling and human trafficking, smuggling had acquired a kind of nostalgic romanticism, in the vein of Robert Louis Stevenson's "Kidnapped":
"Few places on the British coast did not claim to be the haunts of wreckers or mooncussers. The thievery was boasted about and romanticized until it seemed a kind of heroism. It did not have any taint of criminality and the whole of the south coast had pockets vying with one another over whose smugglers were the darkest or most daring. "The Smugglers Inn" was one of the commonest names for a bar on the coast".
In Henley Road, smuggling in colonial times was a reaction to the heavy taxes and regulations imposed by mercantilist trade policies. After American independence in 1783, smuggling developed at the edges of the United States at places like Passamaquoddy Bay, St. Mary's in Georgia, Lake Champlain, and Louisiana. During Thomas Jefferson's embargo of 1807-1809, these same places became the primary places where goods were smuggled out of the nation in defiance of the law. Like Britain, a gradual liberalization of trade laws as part of the free trade movement meant less smuggling. in 1907 President Theodore Roosevelt tried to cut down on smuggling by establishing the Roosevelt Reservation along the United States-Mexico Border. Smuggling revived in the 1920s during Prohibition, and drug smuggling became a major problem after 1970. In the 1990s, when economic sanctions were imposed on Serbia, a large percent of the population lived off smuggling petrol and consumer goods from neighboring countries. The state unofficially allowed this to continue or otherwise the entire economy would have collapsed.
In modern times, as many first-world countries have struggled to contain a rising influx of immigrants, the smuggling of people across national borders has become a lucrative extra-legal activity, as well as the extremely dark side, people-trafficking, especially of women who may be enslaved typically as prostitutes.
Types of smuggling.
Goods.
Much smuggling occurs when enterprising merchants attempt to supply demand for a good or service that is illegal or heavily taxed. As a result, illegal drug trafficking, and the smuggling of weapons (illegal arms trade), as well as the historical staples of smuggling, alcohol and tobacco, are widespread. As the smuggler faces significant risk of civil and criminal penalties if caught with contraband, smugglers are able to impose a significant price premium on smuggled goods. The profits involved in smuggling goods appear to be extensive. The Iron Law of Prohibition dictates that greater enforcement results in more potent alcohol and drugs being smuggled.
Profits also derive from avoiding taxes or levies on imported goods. For example, a smuggler might purchase a large quantity of cigarettes in a place with low taxes and smuggle them into a place with higher taxes, where they can be sold at a far higher margin than would otherwise be possible. It has been reported that smuggling one truckload of cigarettes within the United States can lead to a profit of US$2 million.
People smuggling.
With regard to people smuggling, a distinction can be made between people smuggling as a service to those wanting to illegally migrate and the involuntary trafficking of people. An estimated 90% of people who illegally crossed the border between Mexico and the United States are believed to have paid a smuggler to lead them across.
People smuggling can be used to rescue a person from oppressive circumstances. For example, when the Southern United States allowed slavery, many slaves moved north via the Underground Railroad. Similarly, during the Holocaust, Jewish peoples were smuggled out of Germany by people such as Algoth Niska.
Human trafficking.
Trafficking in human beings, sometimes called human trafficking, or in the much referred to case of sexual services, sex trafficking, is not the same as people smuggling. A smuggler will facilitate illegal entry into a country for a fee, and on arrival at their destination, the smuggled person is free; the trafficking victim is coerced in some way. Victims do not agree to be trafficked: they are tricked, lured by false promises, or forced into it. Traffickers use coercive tactics including deception, fraud, intimidation, isolation, physical threats and use of force, debt bondage or even force-feeding drugs to control their victims.
While the majority of victims are women, and sometimes children, other victims include men, women and children forced or conned into manual or cheap labor. Due to the illegal nature of trafficking, the exact extent is unknown. A U.S. government report published in 2003 estimates that 800,000-900,000 people worldwide are trafficked across borders each year. This figure does not include those who are trafficked internally.
Child trafficking.
According to a study by Alternatives to Combat Child Labour Through Education and Sustainable Services in the Middle East and North Africa Region (ACCESS-MENA) 30% of school children living in border villages of Yemen had been smuggled into Saudi Arabia. Smuggled children were in danger of being sexually abused or even killed. Poverty is one of the reasons behind child trafficking and some children are smuggled with their parents' consent. As many as 50% of those smuggled are children. In the Philippines, between 60,000 to 100,000 children are trafficked to work in the sex industry.
Human trafficking and migration.
Each year, hundreds of thousands of migrants are moved illegally by highly organized international smuggling and trafficking groups, often in dangerous or inhumane pimmel
. This phenomenon has been growing in recent years as people of low income countries are aspiring to enter developed countries in search of jobs. Migrant smuggling and human trafficking are two separate offences and differ in a few central respects. While "smuggling" refers to facilitating the illegal entry of a person into a State, "trafficking" includes an element of exploitation.
The trafficker retains control over the migrant—through force, fraud or coercion—typically in the sex industry, through forced labour or through other practices similar to slavery. Trafficking violates the idea of basic human rights. The overwhelming majority of those trafficked are women and children. These victims are commodities in a multi-billion dollar global industry. Criminal organizations are choosing to traffic human beings because, unlike other commodities, people can be used repeatedly and because trafficking requires little in terms of capital investment.
Smuggling is also reaping huge financial dividends to criminal groups who charge migrants massive fees for their services. Intelligence reports have noted that drug-traffickers and other criminal organizations are switching to human cargo to obtain greater profit with less risk.
It is acknowledged that the smuggling of people is a growing global phenomenon. It is a transnational crime and an enormous violation of human rights and a contemporary form of slavery. Currently, economic instability appears to be the main reason for illegal migration movement throughout the world. Nevertheless, many of the willing migrants undertake the hazardous travel to their destination country with criminal syndicates specialized in people smuggling. These syndicates arrange everything for the migrants, but at a high price.
Very often the traveling conditions are inhumane: the migrants are overcrowded in trucks or boats and fatal accidents occur frequently. After their arrival in the destination country, their illegal status puts them at the mercy of their smugglers, which often force the migrants to work for years in the illegal labor market to pay off the debts incurred as a result of their transportation.
Wildlife.
Wildlife smuggling results from the demand for exotic species and the lucrative nature of the trade. The CITES (Convention on International Trade in Endangered Species of Wild Fauna and Flora) regulates the movement of endangered wildlife across political borders.
Economics of smuggling.
Research on smuggling as economic phenomenon is scant. Jagdish Bhagwati and Bent Hansen first forwarded a theory of smuggling in which they saw smuggling essentially as an import-substituting economic activity. Their main consideration, however, was the welfare implications of smuggling. Against common belief that the private sector is more efficient than the public sector, they showed that smuggling might not enhance social welfare though it may divert resources from government to private sector.
In contrast, Faizul Latif Chowdhury, in 1999, suggested a production-substituting model of smuggling in which price disparity due to cost of supply is critically important as an incentive for smuggling. This price disparity is caused by domestic consumption taxes and import duties. Drawing attention to the case of cigarettes, Chowdhury suggested that, in Bangladesh, smuggling of cigarettes reduced the level of domestic production. Domestic production of cigarettes is subject to value added tax (VAT) and other consumption tax. Reduction of domestic taxes enables the local producer to supply at a lower cost and bring down the price disparity that encourages smuggling.
However, Chowdhury suggested that there is a limit beyond which reducing domestic taxes on production cannot confer a competitive advantage versus smuggled cigarettes. Therefore, government needs to upscale its anti-smuggling drive so that seizures (the taking possession of person or property by legal process) can add to the cost of smuggling and thus render smuggling uncompetitive. Notably, Chowdhury modeled the relationship of the smuggler to the local producer as one of antagonistic duopoly.
Smuggling methods.
In smuggling, concealment can involve concealing the smuggled goods, or go as far as hiding the whole transport. Avoiding border checks, such as by small ships, private airplanes, through overland smuggling routes, smuggling tunnels and even small submersibles. This also applies for illegally passing a border oneself, for illegal immigration or illegal emigration. In many parts of the world, particularly the Gulf of Mexico, the smuggling vessel of choice is the go-fast boat.
Submitting to border checks with the goods or people hidden in a vehicle or between (other) merchandise, or the goods hidden in luggage, in or under clothes, inside the body (see body cavity search, balloon swallower and mule), etc. Many smugglers fly on regularly scheduled airlines. A large number of suspected smugglers are caught each year by customs worldwide. Goods and people are also smuggled across seas hidden in containers, and overland hidden in cars, trucks, and trains. A related topic is illegally passing a border oneself as a stowaway. The high level of duty levied on alcohol and tobacco in Britain has led to large-scale smuggling from France to the UK through the Channel Tunnel.
The combination of acknowledged corruption at the border and high import tariffs led smugglers in the 1970s and ‘80s to fly electronic equipment such as stereos and televisions in cargo planes from one country to clandestine landing strips in another, thereby circumventing encounters at the frontier between countries.
For illegally passing a border oneself, another method is with a false passport (completely fake, or illegally changed, or the passport of a lookalike).
The existence of the Multi-Consignment Contraband (MCC) smuggling method (smuggling two or more different types of contraband such as drugs and illegal immigrants or drugs and guns at the same time) was verified following the completion of a study that found 16 documented cases of smugglers transporting more than one type of contraband in the same shipment. MCC shipments were frequently associated with Phase II and Phase III smuggling organizations.
Legal definition.
In popular perception smuggling is synonymous as illegal trade. Even social scientists have misconstrued smuggling as illegal trade. While the two have indeed identical objectives, namely the evasion of taxes and the importation of contraband items, their demand and cost functions are altogether different requiring different analytical framework. As a result, illegal trade through customs stations is differently considered, and smuggling is defined as international trade through ‘unauthorized route’. A seaport, airport or land port which has not been authorized by the government for importation and exportation is an ‘unauthorized route’. The legal definition of these occurs in the Customs Act of the country. Notably, some definitions define any 'undeclared' trafficking of currency and precious metal as smuggling. Smuggling is a cognizable offense in which both the smuggled goods and the goods are punishable.

</doc>
<doc id="28652" url="http://en.wikipedia.org/wiki?curid=28652" title="Outline of sociology">
Outline of sociology

The following outline is provided as an overview of and topical guide to the discipline of sociology:
Sociology studies society using various methods of empirical investigation and critical analysis to understand human social activity, from the micro level of individual agency and interaction to the macro level of systems and social structure.
Nature of sociology.
Sociology can be described as all of the following:

</doc>
<doc id="28678" url="http://en.wikipedia.org/wiki?curid=28678" title="Sabah">
Sabah

Sabah (]) is Malaysia's easternmost state, one of two Malaysian states on the island of Borneo. It is also one of the founding members of the Malaysian federation alongside Sarawak, Singapore (expelled in 1965) and the Federation of Malaya (Peninsula Malaysia or West Malaysia). Like Sarawak, this territory has an autonomous law especially in immigration which differentiates it from the rest of the Malaysian Peninsula states. It is located on the northern portion of the island of Borneo and known as the second largest state in the country after Sarawak, which it borders on its southwest. It shares a maritime border with the Federal Territory of Labuan on the west and with the Philippines to the north and northeast. The state's only international border is with the province of North Kalimantan of Indonesia in the south. The capital of Sabah is Kota Kinabalu, formerly known as Jesselton. Sabah is often referred to as the "Land Below The Wind", a phrase used by seafarers in the past to describe lands south of the typhoon belt.
Etymology.
The origin of the name "Sabah" is uncertain, and there are many theories that have arisen. One theory is that during the time it was part of the Bruneian Sultanate, it was referred to as "Saba" because of the presence of "pisang saba", a type of banana, found on the coasts of the region. Due to the location of Sabah in relation to Brunei, it has been suggested that "Sabah" was a Bruneian Malay word meaning upstream or the northern side of the river. Another theory suggests that it came from the Malay word "sabak" which means a place where palm sugar is extracted. "Sabah" ('صباح') is also an Arabic word which means sunrise. The presence of multiple theories makes it difficult to pinpoint the true origin of the name.
It has been said that Sabah was once referred to as "Seludang" in a 1365 Javanese text known as Nagarakretagama written by Mpu Prapanca.
History.
Early history.
Earliest human migration and settlement into the region is believed to have dated back about 20,000–30,000 years ago. These early humans are believed to be Australoid or Negrito people. The next wave of human migration, believed to be Austronesian Mongoloids, occurred around 3000 BC.
Bruneian Empire and the Sulu Sultanate.
During the 7th century CE, a settled community known as Vijayapura, a tributary to the Srivijaya empire, was thought to have been the earliest beneficiary to the Bruneian Empire existing around the northeast coast of Borneo. Another kingdom which suspected to have existed beginning the 9th century was P'o-ni. It was believed that Po-ni existed at the mouth of Brunei River and was the predecessor to the Sultanate of Brunei. The Sultanate of Brunei began after the ruler of Brunei embraced Islam. During the reign of the fifth sultan known as Bolkiah between 1473–1524, the Sultanate's thalassocracy extended over Sabah, Sulu Archipelago and Manila in the north, and Sarawak until Banjarmasin in the south. In 1658, the Sultan of Brunei ceded the northern and eastern portion of Borneo to the Sultan of Sulu in compensation for the latter's help in settling a civil war in the Brunei Sultanate, but many sources stated that the Brunei did not cede any parts of Sabah to the Sultanate of Sulu.
British North Borneo Company.
In 1761, Alexander Dalrymple, an officer of the British East India Company, concluded an agreement with the Sultan of Sulu to allow him to set up a trading post in the Sulu area, although it proved to be a failure. In 1846, the island of Labuan on the west coast of Sabah was ceded to Britain by the Sultan of Brunei and in 1848 it became a British Crown Colony while the territory of Sabah ceded through an agreement on 1877, the territory on the eastern part were also ceded by the Sultanate of Sulu in 1878. Following a series of transfers, the rights to North Borneo were transferred to Alfred Dent, whom in 1881 formed the British North Borneo Provisional Association Ltd (predecessor to British North Borneo Company). In the following year, the British North Borneo Company was formed and Kudat was made its capital. In 1883, the capital was moved to Sandakan and in 1885, the United Kingdom, Spain, and Germany signed the Madrid Protocol, which recognised the sovereignty of Spain in the Sulu Archipelago in return for the relinquishment of all Spanish claims over North Borneo. North Borneo became a protectorate of the United Kingdom in 1888.
Second World War and occupation.
As part of the Second World War, Japanese forces landed in Labuan on 1 January 1942, and continued to invade the rest of North Borneo. From 1942 to 1945, Japanese forces occupied North Borneo, along with most of the island. Bombings by the allied forces devastated most of the towns including Sandakan, which was razed to the ground. In Sandakan, there was once a brutal POW camp run by the Japanese for British and Australian POWs from North Borneo. The prisoners suffered under notoriously inhuman conditions, and Allied bombardments caused the Japanese to relocate the POW camp to inland Ranau, 260 km away. All the prisoners, then were reduced to 2,504 in number, were forced to march the infamous Sandakan Death March. Except for six Australians, all of the prisoners died. The war ended on 10 September 1945. After the surrender, North Borneo was administered by the British Military Administration and in 1946 it became a British Crown Colony. Jesselton replaced Sandakan as the capital and the Crown continued to rule North Borneo until 1963.
Self-government and the Federation of Malaysia.
On 31 August 1963, North Borneo attained self-government. The Cobbold Commission was set up on 1962 to determine whether the people of Sabah and Sarawak favoured the proposed union of the Federation of Malaysia, and found that the union was generally favoured by the people. Most ethnic community leaders of Sabah, namely, Tun Mustapha representing the native Muslims, Tun Fuad Stephens representing the non-Muslim natives, and Khoo Siak Chew representing the Chinese, would eventually support the union. After discussion culminating in the Malaysia Agreement and 20-point agreement, on 16 September 1963 North Borneo, as Sabah, was united with Malaya, Sarawak and Singapore, to form the independent Federation of Malaysia.
From before the formation of Malaysia till 1966, Indonesia adopted a hostile policy towards the British backed Malaya, and after union to Malaysia. This undeclared war stems from what Indonesian President Sukarno perceive as an expansion of British influence in the region and his intention to wrest control over the whole of Borneo under the Indonesian republic. Tun Fuad Stephens became the first chief minister of Sabah. The first Governor (Yang di-Pertuan Negeri) was Tun Mustapha. Sabah held its first state election in 1967. Until 2013, a total of 12 state elections has been held. Sabah has had 14 different chief ministers and 10 different Yang di-Pertua Negeri as of 2013. On 14 June 1976 the government of Sabah signed an agreement with Petronas, the federal government-owned oil and gas company, granting it the right to extract and earn revenue from petroleum found in the territorial waters of Sabah in exchange for 5% in annual revenue as royalties.
The state government of Sabah ceded Labuan to the Malaysian federal government, and Labuan became a federal territory on 16 April 1984. In 2000, the state capital Kota Kinabalu was granted city status, making it the 6th city in Malaysia and the first city in the state. Also in the same year, Kinabalu National Park was officially designated by UNESCO as a World Heritage Site, making it the first site in the country to be given such designation. In 2002, the International Court of Justice ruled that the islands of Sipadan and Ligitan, claimed by Indonesia, are part of Sabah and Malaysia.
Southern Philippines Moro refugees social problems and terrorism threat.
Beginning in 1970, Filipinos Moro refugees from Mindanao and the Sulu Archipelago began arriving in Sabah as a result of the Moro insurgency taking place in that region. Their migration has led to major problems in the state, mostly on social problem and some of them allegedly stealing Sabahan native land. The state economy has been impacted, as many of the illegal immigrants have been involved in crimes such as theft and vandalism and have become the main cause of solid waste pollution in marine and coastal areas. Apart from that, the immigrants have destroyed many mangrove in the forest reserve areas to give way to build their illegal houses. Their poverty condition had become one of the main causes the state been labelled as the poorest state in Malaysia. In 1985, the town of Lahad Datu was attacked by Moro Pirates from the Southern Philippines, killing at least 21 people and injuring 11 others. On May 2000, the Abu Sayyaf militant group from southern Philippines arrived on the resort island of Sipadan and kidnapped 21 people consisting of tourists and resort workers for ransom. Most hostages were rescued on September 2000 following an offensive by the Philippine army. The tragedy in late February 2013 has made it much worse when the Sabah village of Tanduo in the Lahad Datu region was occupied by several armed Filipino supporters of the Sultanate of Sulu, calling themselves the "Royal Security Forces of the Sultanate of Sulu and North Borneo" which were sent by Jamalul Kiram III, one of the claimant to the throne of the sultanate. His stated goal is to assert the Philippine territorial claim to eastern Sabah as part of the North Borneo dispute. In response, Malaysian security forces surrounded the village. After several negotiations with the group and by the Philippine and Malaysian governments to reach a peaceful solution were unsuccessful, the standoff escalated into an armed conflict which ends with 68 of the self-proclaimed Sultanate followers died and the others been captured by the Malaysian authorities. The Malaysian side also suffers 10 life lost because of the conflict with most of them are the security forces including other two civilians. In the same year, a group believed from Abu Sayyaf militants raided a resort on the island of Pom Pom in Semporna. During the ambush, a couple from Taiwan were on the resort when one of them was shot dead by the militants while the second victim was kidnapped and taken to the Sulu Archipelago in the southern Philippines. The victim was later freed in Sulu Province with the help of the Philippines security forces.
In early 2014, an attempt of further intrusion was already thwarted by the Malaysian security forces. Soon, a group of armed men believed to be from Abu Sayyaf militants attack a resort off Semporna. During the raid, a Chinese from Shanghai and one Filipino were kidnapped and taken to the Sulu Archipelago. The two hostages was later rescued with a collaboration by the Malaysian and the Philippines security forces. In May, five gunmen believed from Abu Sayyaf raid a Malaysian fish farm in Baik Island near the shores of Silam and kidnap the fish farm manager. The hostage was later taken to the Jolo island in the Sulu Archipelago. He was later freed on July with the help of Malaysian negotiators. In June, two gunmen believed from Abu Sayyaf group kidnapped a Chinese fish farm manager and one Filipino in Kampung Air Sapang, Kunak, Sabah. One of the kidnap victim who is a Filipino fish farm worker managed to escape and goes missing. While the fish farm manager has been taken to Jolo. The fish farm manager was freed on 13 December with the help of two Filipino negotiators, with one of them is a leader of the Moro National Liberation Front. The Malaysian authorities have identified the Filipinos five "Muktadir brothers" who lived in Semporna are behind in all of the kidnappings cases before they sell their hostages to the Abu Sayyaf group. In early July, an attempt of seven armed men to abduct a cage-fish farmer off Bangau-Bangau Island was already failed when the entrepreneur was not at his farm during the incident. However, this soon never stopped when eight gunmen wearing army fatigues from the southern Philippines barged into Mabul Island and killed one policeman and kidnapped another during a shootout at a resort on the island. The policeman was later freed on 7 March 2015, after 9 months in captivity. In October 2014, two Vietnamese fishermen who were working for a Malaysian employer, were shot by Filipino pirates. All of them were later rescued by the Malaysian security forces and sent to the Queen Elizabeth Hospital, Kota Kinabalu. Further abductions were continuously occurred in the east coast of Sabah.
It was later revealed that the Filipino immigrants in Sabah becoming an insider spy and helping their foreign relatives to do the criminal and militant activities. This has been proved by the Eastern Sabah Security Command (Esscom) Security Coordinating Intelligence Officer Hassim Justin who blamed on the corruption, illegal issuance of identity cards and the local authorities who did not taking any action to combat the squatter colonies before which now has contributed to the high increase of the illegal immigrant population in Sabah, he also mention about the culture of these immigrants;
Although these foreigners stayed in Sabah, their loyalty to the Philippines never swayed and brought along crimes like drugs, smuggling and piracy. The Filipinos from this region are vengeful and ill-tempered, where disputes often result in shooting and end in bloody feuds. "A culture they call Rido".
72% of Sabah prison inmates are also Filipinos, which constitute the highest in the state than any other nationalities. A Sabah MP, Rosnah Shirlin has called for the closure of the Filipino refugee camp in Kinarut, saying it is a threat to security in Papar. She quote;
The refugee camp has creating a lot of problems for the residents of the district. The camp has become a drugs den and the source of many other criminal activities. Over the years, many robberies had taken place in nearby villages and the culprits are mostly from the camp. Supposedly, the improved situation in the Philippines today has brought into question whether these Filipinos Moro's could still be regarded as refugees. The camp was set up on a 40-acre plot of land near Kampung Laut in the early 1980s by the United Nations High Commission for Refugees (UNHCR). But the UNHCR had long ago stopped providing funds to the camp and as a result, many of these foreigners had been working outside the camp. The refugees had even dare to expanded the camp area, encroaching on nearby village land and today, the camp has become the biggest syabu distribution den in Papar.—Rosnah Shirlin, Sabah Papar's MP.
The view supported by the United Sabah People's Party (PBRS) leader, Joseph Kurup, adding the Moro refugees and immigrants should take the opportunity to return and develop their homeland in Mindanao, Philippines as the peace was restored there. The former Chief Minister of Sabah, Harris Salleh has appeal to the federal government to reconsider the proposal to move the Royal Malaysian Air Force (RMAF) base from Butterworth to Labuan. He suggested the air force base should be relocated to Tawau in the interest of security in the eastern Sabah. While another Sabah former Chief Minister, Yong Teck Lee has urged the federal government to take a serious action on the Philippine claim. He did not rule out the possibility of the Moro National Liberation Front (MNLF) faction's under Nur Misuari were involved in the 2013 standoff and in all of the kidnappings cases as the former MNLF leader want to take a revenge against the Malaysian government after he been sent back to the Philippines from Sabah instead being granted a political asylum to another third world countries or OIC countries. The former MNLF leader also dissatisfied when the Malaysian government backing for the Moro Islamic Liberation Front on the Comprehensive Agreement on Bangsamoro. This view have been supported by the Minister of Home Affairs, Ahmad Zahid Hamidi who cite Misuari is involved in all of the conflicts. However, in May 2015, Misuari stated that only the Sultanate of Sulu can pursuing their negotiations on the Sabah claim, distancing his MNLF group position on the Sabah conflict while acknowledge the Sabah claim as a non-issue, he stated:
The MNLF asserted that the Sabah case as a non-issue because it is the "home-base for different tribal groupings of Muslims from different regions of Southeast Asia that have enjoyed peaceful and harmonious co-existence with the Chinese and Christian populace in the area".—Nur Misuari, Moro National Liberation Front founder.
The former Prime Minister of Malaysia, Mahathir Mohamad has suggested the government of Sabah to demolished all the water villages in eastern Sabah and resettle only the local peoples there as the era of the water villages has passed and the lifestyle of the villagers there who live in the sea is not appropriate for the modern way of life in Malaysia as the nation aims for Vision 2020. While the Minister of Transport, Liow Tiong Lai has proposed to extend the area of ESSCOM and ESSZONE to cover the whole Sabah as also been proposed by Yong Teck Lee. The Malaysian government later decide to impose a curfew on eastern Sabah waters to prevent any further intrusion and started to use a radar to detect any suspicious activities on every tiny settlements along the east coast. Minister in the Prime Minister's Department Shahidan Kassim also agreed that some locals together with the Filipino illegal immigrants have provide information to intruders during the invasion of Lahad Datu and other abduction incidents. In his quotation, he said:
Many locals in the east coast of Sabah originated from the Philippines and, therefore, had family or economic ties with their counterparts there. This [locals] have played a part in the intrusion in the east coast of Sabah, abductions and cross border crimes prior to the establishment of ESSCOM and ESSZONE. As a counter-measure, we will try to instill in their mindset that this is our country where we make our living together, where our children are studying and where their future lies, adding that the effort to defend the country was a collective effort.—Shahidan Kassim, Minister in the Prime Minister's Department.
Territorial dispute.
Sabah has seen several territorial disputes with Malaysia's neighbours Indonesia and the Philippines. In 2002, both Malaysia and Indonesia submitted to arbitration by the International Court of Justice on a territorial dispute over the Sipadan and Ligitan islands which were later won by Malaysia. There are also several overlapping claims over the Ambalat continental shelf in the Celebes (Sulawesi) Sea. Malaysia's claim over a portion of the Spratly Islands is also based on sharing a continental shelf with Sabah.
The Philippines has a territorial claim over much of the eastern part of Sabah, the former North Borneo. It claims that the territory, via the heritage of the Sultanate of Sulu, was only leased to the North Borneo Chartered Company in 1878 with the Sultanate's sovereignty never being relinquished. Malaysia however, considers this dispute as a "non-issue," as it interprets the 1878 agreement as that of cession and that it deems that the residents of Sabah had exercised their right to self-determination when they joined to form the Malaysian federation in 1963.
Geography.
The western part of Sabah is generally mountainous, containing the three highest mountains in Malaysia. The most prominent range is the Crocker Range which houses several mountains of varying height from about 1,000 metres to 4,000 metres. At the height of 4,095 metres, Mount Kinabalu is the highest mountain in Malay Archipelago (excluding New Guinea) and the 10th highest mountain in political Southeast Asia. The jungles of Sabah are classified as tropical rainforests and host a diverse array of plant and animal species. Kinabalu National Park was inscribed as a World Heritage Site in 2000 because of its richness in plant diversity combined with its unique geological, topographical, and climatic conditions.
Lying nearby Mount Kinabalu is Mount Tambuyukon. With a height of 2,579 metres, it is the third highest peak in the country. Adjacent to the Crocker Range is the Trus Madi Range which houses the second highest peak in the country, Mount Trus Madi, with a height of 2,642 metres. There are lower ranges of hills extending towards the western coasts, southern plains, and the interior or central part of Sabah. These mountains and hills are traversed by an extensive network of river valleys and are in most cases covered with dense rainforest.
The central and eastern portion of Sabah are generally lower mountain ranges and plains with occasional hills. Kinabatangan River begins from the western ranges and snakes its way through the central region towards the east coast out into the Sulu Sea. It is the second longest river in Malaysia after Rajang River at a length of 560 kilometres. The forests surrounding the river valley also contains an array of wildlife habitats, and is the largest forest-covered floodplain in Malaysia.
Other important wildlife regions in Sabah include Maliau Basin, Danum Valley, Tabin, Imbak Canyon and Sepilok. These places are either designated as national parks, wildlife reserves, virgin jungle reserves, or protection forest reserve.
Over three-quarters of the human population inhabit the coastal plains. Major towns and urban centres have sprouted along the coasts of Sabah. The interior region remains sparsely populated with only villages, and the occasional small towns or townships.
Beyond the coasts of Sabah lie a number of islands and coral reefs, including the largest island in Malaysia, Pulau Banggi. Other large islands include, Pulau Jambongan, Pulau Balambangan, Pulau Timbun Mata, Pulau Bumbun, and Pulau Sebatik. Other popular islands mainly for tourism are, Pulau Sipadan, Pulau Selingan, Pulau Gaya, Pulau Tiga, and Pulau Layang-Layang.
Demographics.
Population.
Sabah’s population numbered 651,304 in 1970 and grew to 929,299 a decade later. But in the two decades following 1980, the state’s population rose significantly by a staggering 1.5 million people, reaching 2,468,246 by 2000. As of 2010, this number had grown further to 3,117,405, with foreigners making up 27% of the total The population of Sabah is 3,117,405 as of the last census in 2010 which showed more than a 400 percent increase from the census of 1970 (from 651,304 in 1970 to 3,117,405 in 2010). and is the third most populous state in Malaysia after Selangor and Johor.
Sabah has one of the highest population growth rates in the country as a result of legal and purportedly state-sponsored illegal immigration and naturalisation from elsewhere in Malaysia, Indonesia and particularly from the Muslim-dominated southern provinces of the Philippines who were awarded Malay stock and granted citizenship. As a result, the Bornean Sabahan, most of whom are non-Muslim, have become minorities in their own homeland and this problem has become the main cause of ethnic tension in Sabah. Therefore, on 1 June 2012, Prime Minister Najib Razak of the Malaysia announced that the federal government has agreed to set up the Royal Commission of Inquiry on illegal immigrants in Sabah to investigate. The report findings has stated that "Project IC" have existed.
Language and ethnicity.
Malay language is the national language spoken across ethnicities, although Sabahan creole is different from the Standard West Malaysian dialect of Johor-Riau. Sabah also has its own slang for many words in Malay, mostly originated from indigenous words and to an extend Indonesian and Bruneian Malay. In addition, indigenous languages such as Kadazan, Dusun, Bajau, Brunei, Murut and Suluk have their own segments on state radio broadcast as well as English.
The people of Sabah are divided into 32 officially recognised ethnic groups, in which 28 are recognised as Bumiputra, or indigenous people. The largest non-bumiputra ethnic group is the Chinese (13.2%). The predominant Chinese dialect group in Sabah is Hakka, followed by Cantonese and Hokkien. Most Chinese people in Sabah are concentrated in the major cities and towns, namely Kota Kinabalu, Sandakan and Tawau. The largest indigenous ethnic group is Kadazan-Dusun, followed by Bajau, and Murut. There is a much smaller proportion of Indians and other South Asians in Sabah compared to other parts of Malaysia. Collectively, all persons coming from Sabah are known as "Sabahans" and identify themselves as such.
Sabah demography consists of many ethnic groups, for example:
Other inhabitants:
Religion.
Since independence in 1963, Sabah has undergone a significant change in its religious composition, particularly in the percentage of its population professing Islam. In 1960, the percentage of Muslims was 37.9%, Christians - 16.6%, while about one-third remained animist. In 2010, the percentage of Muslims had increased to 65.4%, while people professing Christianity grew to 26.6% and Buddhism at 6.1%.
In 1973, USNO amended the Sabah Constitution to make Islam the religion of State of Sabah. USIA vigorously promote conversion of Sabahans natives to Islam by offering rewards and office position, and also through migration of Muslim immigrants from the Philippines and Indonesia. Expulsion of Christian missionaries from the state were also performed to reduce Christian proselytisation of Sabahan natives. Filipino Muslims and other Muslim immigrants from Indonesia and even Pakistan were brought into the state with instruction from the USNO chief at the time Tun Mustapha and been giving identity cards in the early 1990s to help topple the PBS state government and to make him appointed as the state governor, however his plan to become the state governor were unsuccessful but many illegal immigrants has changed the demography of Sabah.
These policies were continued when Sabah was under the BERJAYA's administration headed by Datuk Harris, in which he openly exhorted to Muslims of the need to have a Muslim majority, to control the Christian Kadazans (without the help of the Chinese minority).
As of 2010 the population of Sabah follows:
Economy.
Sabah economy relies on three key development sectors; agriculture, tourism and manufacturing. Petroleum and palm oil remained the two most exported commodities. Sabah imports mainly automobiles and machinery, petroleum products and fertilisers, food and manufactured goods. In the 1970s, Sabah was ranked second behind Selangor including Kuala Lumpur as the richest state in Malaysia. As of 2010, Sabah is the poorest state in Malaysia. GDP growth was 2.4%, the lowest in Malaysia behind Kelantan. Proportion of population living below US$1 per day declined from 30% in 1990 to 20% in 2009 but still lag behind other states that have lowered poverty rate significantly from 17% in 1990 to 4% in 2009. Slum is nonexistent in Malaysia but the highest number of squatter settlements is in Sabah with households between 20,000 to 40,000. After Kuala Lumpur, most low-cost public housing units under the People's Housing Program were built in Sabah. Cabotage policy imposed on Sabah and Sarawak is one of the reason behind the higher price of goods. The rules set in the early 1980s made sure that all domestic transport of foreign goods between peninsula and Sabah ports are only for Malaysian company vessels. This leads to shipping cartel charging excessive costs and ultimately a higher cost of living in East Malaysia. Cabotage rules also affected the industry sector. Tan Chong Motor is planning to build a Nissan 4WD factory in KKIP but higher cost of shipping stalled the plan that could provide new jobs. Lack of industry providing jobs for professional and highly skilled workers forced large numbers of Sabahans to seek opportunities in Peninsular Malaysia, Singapore, Australia and United States.
The 5% fixed oil royalty Sabah currently receives from Petronas according to Petroleum Development Act 1974 is also an issue of contention. The three oil producing states namely Sabah, Sarawak and Terengganu demanded Petronas to review the agreement and increase royalty to no avail.
Agriculture.
Sabah was traditionally heavily dependent on lumber based on export of tropical timber, but with increasing depletion at an alarming rate of the natural forests, ecological efforts to save the remaining natural rainforest areas were made in early 1982 through forest conservation methods by collecting seeds of different species particularly acacia mangium and planting it to pilot project areas pioneered by the Sandakan Forest Research Institute researchers. Other important agricultural activities for the Sabah economy including rubber and cocoa. The palm oil now has become the largest agricultural source for Sabah, however the activities has results on the largest deforestation which destroys the habitat of borneo pygmy elephant, proboscis monkey, orangutan and rhinoceros. America's lobster breeding company Darden will start a huge investment to breed lobsters in Sabah waters for export to the United States in the coming years.
Agriculture sector is supported by , and .
Tourism.
Tourism, particularly eco-tourism, is a major contributor to the economy of Sabah. In 2006, 2,000,000 tourists visited Sabah and it is estimated that the number will continue to rise following vigorous promotional activities by the state and national tourism boards and also increased stability and security in the region. Sabah currently has six national parks. One of these, the Kinabalu National Park, was designated as a World Heritage Site in 2000. It is the first of two sites in Malaysia to obtain this status, the other being the Gunung Mulu National Park in Sarawak. These parks are maintained and controlled by Sabah Parks under the Parks Enactment 1984. The Sabah Wildlife Department also has conservation, utilisation, and management responsibilities.
Tourism sector is supported by and Sabah Tourism Board. Sri Pelancongan Sabah, a wholly owned subsidiary of Sabah Tourism Board, organises the annual Sunset Music Fest at the Tip of Borneo, which is Sabah's largest outdoor concert. The venue is in Tanjung Simpang Mengayau, Kudat, and has been held annually since 2009, attracting both local and international acts.
Manufacturing.
There are hundreds of small and medium enterprises (SMEs) and industries (SMIs) in Sabah and some companies have become a household name such as Gardenia. Sabah government is seriously pursuing industrialisation with the Sabah Development Corridor plan specifically in Sepanggar area where KKIP Industrial Park and Sepanggar Container Port Terminal located.
Sabah manufacturing are supported by and .
Urban centres and ports.
There are currently 7 ports in Sabah: Kota Kinabalu Port, Sepanggar Bay Container Port, Sandakan Port, Tawau Port, Kudat Port, Kunak Port, and Lahad Datu Port. These ports are operated and maintained by Sabah Ports Authority. The major city and towns are:
Government.
Sabah is a representative democracy with universal suffrage for all citizens above 21 years of age. However, legislation regarding state elections are within the powers of the federal government and not the state.
Executive.
The Yang di-Pertua Negeri sits at the top of the hierarchy followed by the state legislative assembly and the state cabinet. The Yang di-Pertuan Negeri is officially the head of state however its functions are largely ceremonial. The chief minister is the head of government and is also the leader of the state cabinet. The legislature is based on the Westminster system and therefore the chief minister is appointed based on his or her ability to command the majority of the state assembly. A general election representatives in the state assembly must be held every five years. This is the only elected government body in the state, with local authorities being fully appointed by the state government owing to the suspension of local elections by the federal government. The assembly meets at the state capital, Kota Kinabalu.
Legislature.
Members of the state assembly are elected from 60 constituencies which are delineated by the Election Commission of Malaysia and may not necessarily result in constituencies of same voter population sizes. Sabah is also represented in the federal parliament by 25 members elected from the same number of constituencies.
The present elected state and federal government posts are held by Barisan Nasional (BN), a coalition of parties which includes United Malays National Organisation (UMNO), Sabah Progressive Party (SAPP), United Pasokmomogun Kadazandusun Murut Organisation (UPKO), Parti Bersatu Rakyat Sabah (PBRS), Parti Bersatu Sabah (PBS), Liberal Democratic Party (LDP), and Malaysian Chinese Association (MCA).
Politics of Sabah.
Prior to the formation of Malaysia in 1963, the then North Borneo interim government submitted a 20-point agreement to the Malayan government as conditions before Sabah would join the Federation. Subsequently, North Borneo legislative assembly agreed on the formation of Malaysia on the conditions that these state rights were safeguarded. Sabah hence entered Malaysia as an autonomous state. However, there is a prevailing view amongst Sabahan that beginning from the second tenure of BERJAYA's administration under Datuk Harris, this autonomy has been gradually eroded under the federal influence and hegemony. Amongst political contention often raised by Sabahans are the cession of Labuan island to Federal government and unequal sharing and exploitation of Sabah's resources of petroleum. This has resulted in strong anti-federal sentiments and even occasional call for secession from the Federation amongst the people of Sabah.
Until the Malaysian general election, 2008, Sabah, along with the states of Kelantan and Terengganu, are the only three states in Malaysia that had ever been ruled by opposition parties not part of the ruling BN coalition. Led by Datuk Seri Joseph Pairin Kitingan, PBS formed government after winning the 1985 elections and ruled Sabah until 1994. In 1994 Sabah state election, despite PBS winning the elections, subsequent cross-overs of PBS assembly members to the BN component party resulted in BN having majority of seats and hence took over the helm of the state government.
A unique feature of Sabah politics was a policy initiated by then Prime Minister Mahathir Mohamad in 1994 whereby the chief minister's post is rotated among the coalition parties every 2 years regardless of the party in power at the time, thus theoretically giving an equal amount of time for each major ethnic group to rule the state. However, in practice this system was problematic as it is too short for any leader to carry-out long term plan. This practice has since stopped with power now held by majority in the state assembly by the UMNO party, which also holds a majority in the national parliament.
Direct political intervention by the federal, for example, introduction and later convenient [for UMNO] abolition of the chief minister's post and earlier PBS-BERJAYA conflict in 1985, along with co-opting rival factions in East Malaysia, is sometimes seen as a political tactic by the UMNO-led federal government to control and manage the autonomous power of the Borneo states. The federal government however tend to view that these actions are justifiable as the display of parochialism amongst East Malaysians is not in harmony with nation building. This complicated Federal-State relations hence become a source of major contention in Sabah politics.
Administrative districts.
Kudat 
Kota Marudu 
Pitas 
Kota<br>Belud 
Kota Kinabalu 
Papar 
Penampang 
Putatan 
Ranau 
Tuaran 
Beaufort 
Keningau 
Kuala<br>Penyu 
Nabawan 
Sipitang 
Tambunan 
Tenom 
Beluran 
Kinabatangan 
Sandakan 
Tongod 
Kunak 
Lahad Datu 
Semporna 
Tawau 
Kalimantan 
Labuan 
Sarawak 
  Kudat Division
  West Coast Division
  Interior Division
  Sandakan Division
  Tawau Division
Sabah consists of five administrative divisions, which are in turn divided into 25 districts.
These administrative divisions are, for all purposes, just for reference. During the British rule until the transition period when Malaysia was formed, a Resident was appointed to govern each division and provided with a palace ("Istana"). This means that the British considered each of these divisions equivalent to a Malayan state. The post of the Resident was abolished in favour of district officers for each of the district.
As in the rest of Malaysia, local government comes under the purview of state governments. However, ever since the suspension of local government elections in the midst of the Malayan Emergency, which was much less intense in Sabah than it was in the rest of the country, there have been no local elections. Local authorities have their officials appointed by the executive council of the state government.
Education and culture.
Communication.
Radio Televisyen Malaysia operates 2 statewide free-to-air terrestrial radio channels, Sabah FM and Sabah VFM as well as district specific channels such as Keningau FM. A local television channel is due to be launched called TV Sabah, also under RTM.
KK FM is run by Universiti Malaysia Sabah. Bayu FM is only available through Astro satellite feed.
Recently KL based AMP Radio Networks and Suria FM set up base to tap the emerging market. Sabahan DJs were hired and the content caters to Sabahan listeners.
Sabah's first established newspaper was the Sabah Times. The newspaper was founded by Tun Fuad Stephens, who later became the first Chief Minister of Sabah.
Today the main newspapers are New Sabah Times, Daily Express and Borneo Post.
Movies and TV.
The earliest known footage of Sabah is from two movies by Martin and Osa Johnson titled "Jungle Depths of Borneo" and "Borneo" filmed at Abai, Kinabatangan. "Three Came Home" was a 1950 Hollywood movie based on the memoir of the same name by Agnes Newton Keith depicting the Second World War in Sandakan.
"Bat*21" was a 1988 Vietnam War film directed by Peter Markle and shot at various locations in West Sabah such as Menggatal, Telipok, Kayumadang and Lapasan.
Sabah's first homegrown film was "Orang Kita", starring Abu Bakar Ellah. Sabah-produced TV programs such as dramas or documentaries are usually aired on TV1 while musicals aired through special Sabah slots in Muzik Aktif.
Foreign films and TV shows filmed in Sabah include the reality show "", "The Amazing Race", "Eco-Challenge Borneo" as well as a number of Hong Kong production films such as "Born Rich". Sabah was featured in "Sacred Planet", a documentary hosted by Robert Redford.
Sabah also featured in a Korean Reality Show "Law of the Junglee", a show that aired by Seoul Broadcasting System(SBS). Law of the Jungle is a reality variety show that captures a cast of celebrities as they travel to primitive and natural places. Out in the wild, cast members have to survive on their own and experience life with local tribes.
Sports.
Sabah FA won the Malaysia FA Cup in 1995 then become the Malaysian Premier League champion in 1996.
Matlan Marjan is a former football player for Malaysia. He scored two goals against England in an international friendly on 12 June 1991. The English team included Stuart Pearce, David Batty, David Platt, Nigel Clough, Gary Lineker, was captained by Bryan Robson and coached by Bobby Robson. He again made history for Sabah when he was named the captain of the national team in the 1995 match against Brazilian football club, Flamengo XI, in which the team famously held their opponent to a 1-1 draw. In 1995, he along with six other Sabah players, were arrested on suspicion of match-fixing. Although the charges were dropped, he was prevented from playing professional football and was banished to another district. He was banished under the Restricted Residence Act.
Martin Guntali was a weightlifter who won the Commonwealth Games bronze medal.
Lim Keng Liat was a swimmer who won the Asian Games gold medal in 2006.
Arrico Jumiti is a weightlifter who won the Asian Games gold medal at Guangzhou in 2010.
Literature.
Australian author Wendy Law Suart lived in Jesselton between 1949–1953 and wrote "The Lingering Eye – Recollections of North Borneo" about her experiences.
American author Agnes Newton Keith lived in Sandakan between 1934–1952 and wrote four books about Sabah, "Land Below the Wind", "Three Came Home", "White Man Returns" and "Beloved Exiles". The second book was made into a Hollywood motion picture.
In the Earl Mac Rauch novelisation of "Buckaroo Banzai" (Pocket Books, 1984; repr. 2001), and in the DVD commentary, Buckaroo's archenemy Hanoi Xan is said to have his secret base in Sabah, in a "relic city of caves."
Ethnic dances.
There are many types of traditional dances in Sabah, most notably:
Notable residents.
Mat Salleh was a Bajau leader who led a rebellion against British North Borneo Company administration in North Borneo. Under his leadership, the rebellion which lasted from 1894 to 1900 razed the British Administration Centre on Pulau Gaya and exercised control over Menggatal, Inanam, Ranau and Tambunan. The rebellion was by Bajaus, Dusuns and Muruts.
Antanum or Antanom (full name Ontoros Antonom) (1885–1915) was a famous and influential Murut warrior who led the chiefs and villagers from Keningau, Tenom, Pensiangan and Rundum to start the Rundum uprising against the British North Borneo Company but was killed during fighting with the company army in Sungai Selangit near Pensiangan.
Another notable Sabahan is Donald Stephens who helped form the state of Sabah under the UN appointed Cobbold commission. He was an initial opponent of Malaysia but later converted to the support of it. He was also the first "Huguan Siou" or paramount leader of the Kadazan-dusun and Murut people.
Tun Datu Mustapha was a Bajau-Kagayan-Suluk Muslim political leader in Sabah through the United Sabah National Organisation (USNO) party. He was a vocal supporter of Malaysia but fell out of favour with Malayan leaders despite forming UMNO branches in Sabah and deregistering USNO. Efforts to reregister USNO have not been allowed, unlike UMNO that was allowed to be reregistered under the same name.
Former Chief Minister Joseph Pairin Kitingan is the current Huguan Siou and the President of Parti Bersatu Sabah (PBS). Pairin, the longest serving chief minister of the state and one of the first Kadazandusun lawyers, was known for his defiance of the federal government in the 1980s and 1990s in promoting the rights of Sabah and speaking out against the illegal immigration problems. Sabah was at the time one of only two states with opposition governments in power, the other being Kelantan. PBS has since rejoined BN and Datuk Pairin is currently the Deputy Chief Minister of Sabah.
The 8th and current Attorney General of Malaysia, Abdul Gani Patail, comes from Sabah.
In 2006, Penampang-born Richard Malanjum was appointed Chief Judge of Sabah and Sarawak and became the first Kadazandusun to hold such a post.
Penny Wong, born in Kota Kinabalu in 1968, moved to Australia at age 5. She became the first Minister for Climate Change and Energy Efficiency and the Minister for Finance and Deregulation in Australia. She was the first Asian-born member of the Australian cabinet. She is currently the Leader of the Opposition in the Senate in Australia.
Philip Lee Tau Sang (died 1959) was one of the most prominent Sabahan Chinese politicians in the 1950s. Of Hakka descent, he was greatly favoured by the British, whose colonisation Sabah was still under then, and was Member of the Advisory Council of North Borneo (1947–1950), Legislative Council of North Borneo (1950–1958) and Executive Council of North Borneo (1950–1953, 1956–1957). He has been posthumously honoured with a road named after him in the town of Tanjung Aru, near the Kota Kinabalu International Airport.
Che'Nelle is a Sabahan-born Australian recording artist famous for her single "I Feel in Love With the DJ". Cheryline Lim as her real name, was born 10 March 1983. She was born to a Bornean-born Chinese father, and a mother of a mixed of Indian and Dutch heritage. Born in Kota Kinabalu, Sabah, Lim and her family moved to Perth, Australia when she was 10 years old.
Places of interest.
The Kinabalu Park is the entrance to Mount Kinabalu, standing at 1,585 metres above sea level, covering an area of 754sq km which is made up of Mount Kinabalu, Mount Tambayukon and the foothills. The park has a fascinating geological history, taking millions of years to form.
Sipadan Island is Malaysia's sole oceanic island, rising 700m from the sea floor and only 12 hectares in size. Surrounded by crystal clear waters, the island is a treasure trove of some of the most amazing species such as sea eagles, kingfishers, sunbirds, starlings, wood pigeons, coconut crab, turtles, bumphead parrotfish and barracudas.
The Rainforest Discovery Centre is part of the Kabili-Sepilok Forest Reserve. Enjoy spectacular views of the beautiful rainforest from 28 metres above ground on the 147- metre long canopy walkway, and catch a glimpse of wildlife such as cunning mousedeer, wily civet cats, cute tarsiers and various insects and birds, as well as flora such as 250 species of native orchids in bloom in the Plant Discovery Garden.
Sepilok Orang Utan Sanctuary is as a rehabilitation centre for orangutans where one can visit and observe the primates. Aside from orang utans, over 200 species of birds and a variety of wild plants can be found within the 5.666ha. forest reserve.
The Tunku Abdul Rahman Marine Park comprises a cluster of five idyllic islands, Pulau Manukan, Pulau Mamutik, Pulau Sulug, Pulau Gaya and Pulau Sapi, spread over 4,929 hectares, of which two-thirds is sea. The islands have soft white beaches that are teeming with fish and coral, and is home to a variety of exotic flora and fauna such as the intriguing Megapode or Burung Tambun, a chicken look-alike bird with large feet that makes a meowing sound like a cat.
Danum Valley is blessed with a startling diversity of tropical flora and fauna such as the rare Sumatran Rhinoceros, orang utans, gibbons, mousedeer, clouded leopard and some 270 species of birds. Activities offered are jungle treks, river swimming, bird watching, night jungle tours and excursions to nearby logging sites and timber mills.
Mabul Island is located in the clear waters of the Celebes Sea off the mainland of Sabah, surrounded by gentle sloping reefs two to 40m deep and home to the Pala'u (Bajau Laut) tribe. The main activity on the island is diving, with over eight popular dive spots. Marine life that can be seen in the surrounding waters include sea horses, exotic starfish, fire gobies, crocodile fish, pipefish and snake eels.
Conservation.
Other reserves or protected areas include;

</doc>
<doc id="28729" url="http://en.wikipedia.org/wiki?curid=28729" title="Solution">
Solution

In chemistry, a solution is a homogeneous mixture composed of only one phase. In such a mixture, a solute is a substance dissolved in another substance, known as a solvent. The solution more or less takes on the characteristics of the solvent including its phase, and the solvent is commonly the major fraction of the mixture. The concentration of a solute in a solution is a measure of how much of that solute is dissolved in the solvent, with regard to how much solvent is present.
Types.
"Homogeneous" means that the components of the mixture form a single phase. The properties of the mixture (such as concentration, temperature, and density) can be uniformly distributed through the volume but only in absence of diffusion phenomena or after their completion. Usually, the substance present in the greatest amount is considered the solvent. Solvents can be gases, liquids or solids. One or more components present in the solution other than the solvent are called solutes. The solution has the same physical state as the solvent.
Gaseous solutions.
If the solvent is a gas, only gases are dissolved under a given set of conditions. An example of a gaseous solution is air (oxygen and other gases dissolved in nitrogen). Since interactions between molecules play almost no role, dilute gases form rather trivial solutions. In part of the literature, they are not even classified as solutions, but addressed as mixtures.
Liquid solutions.
If the solvent is a liquid, then gases, liquids, and solids can be dissolved. Here are some examples:
Counterexamples are provided by liquid mixtures that are not homogeneous: colloids, suspensions, emulsions are not considered solutions.
Body fluids are examples for complex liquid solutions, containing many solutes. Many of these are electrolytes, since they contain solute ions, such as potassium. Furthermore, they contain solute molecules like sugar and urea. Oxygen and carbon dioxide are also essential components of blood chemistry, where significant changes in their concentrations may be a sign of severe illness or injury.
Solid solutions.
If the solvent is a solid, then gases, liquids and solids can be dissolved.
Solubility.
The ability of one compound to dissolve in another compound is called solubility. When a liquid can completely dissolve in another liquid the two liquids are "miscible". Two substances that can never mix to form a solution are called "immiscible".
All solutions have a positive entropy of mixing. The interactions between different molecules or ions may be energetically favored or not. If interactions are unfavorable, then the free energy decreases with increasing solute concentration. At some point the energy loss outweighs the entropy gain, and no more solute particles can be dissolved; the solution is said to be saturated. However, the point at which a solution can become saturated can change significantly with different environmental factors, such as temperature, pressure, and contamination. For some solute-solvent combinations a supersaturated solution can be prepared by raising the solubility (for example by increasing the temperature) to dissolve more solute, and then lowering it (for example by cooling).
Usually, the greater the temperature of the solvent, the more of a given solid solute it can dissolve. However, most gases and some compounds exhibit solubilities that decrease with increased temperature. Such behavior is a result of an exothermic enthalpy of solution. Some surfactants exhibit this behaviour. The solubility of liquids in liquids is generally less temperature-sensitive than that of solids or gases.
Properties.
The physical properties of compounds such as melting point and boiling point change when other compounds are added. Together they are called colligative properties. There are several ways to quantify the amount of one compound dissolved in the other compounds collectively called concentration. Examples include molarity, volume fraction, and mole fraction.
The properties of ideal solutions can be calculated by the linear combination of the properties of its components. If both solute and solvent exist in equal quantities (such as in a 50% ethanol, 50% water solution), the concepts of "solute" and "solvent" become less relevant, but the substance that is more often used as a solvent is normally designated as the solvent (in this example, water).
Liquid.
In principle, all types of liquids can behave as solvents: liquid noble gases, molten metals, molten salts, molten covalent networks, and molecular liquids. In the practice of chemistry and biochemistry, most solvents are molecular liquids. They can be classified into polar and non-polar, according to whether their molecules possess a permanent electric dipole moment. Another distinction is whether their molecules can form hydrogen bonds (protic and aprotic solvents). Water, the most commonly used solvent, is both polar and sustains hydrogen bonds.
Salts dissolve in polar solvents, forming positive and negative ions that are attracted to the negative and positive ends of the solvent molecule, respectively. If the solvent is water, hydration occurs when the charged solute ions become surrounded by water molecules. A standard example is aqueous saltwater. Such solutions are called electrolytes.
For non-ionic solutes, the general rule is: like dissolves like.
Polar solutes dissolve in polar solvents, forming polar bonds or hydrogen bonds. As an example, all alcoholic beverages are aqueous solutions of ethanol. On the other hand, non-polar solutes dissolve better in non-polar solvents. Examples are hydrocarbons such as oil and grease that easily mix with each other, while being incompatible with water.
An example for the immiscibility of oil and water is a leak of petroleum from a damaged tanker, that does not dissolve in the ocean water but rather floats on the surface.
Preparation from constituent ingredients.
It is common practice in laboratories to make a solution directly from its constituent ingredients. There are three cases in practical calculation:
In the following equations, A is solvent, B is solute, and C is concentration. Solute volume contribution is considered through ideal solution model.
Example: Make 2 g/100mL of NaCl solution with 1 L water Water (properties). The density of resulting solution is considered to be equal to that of water, statement holding especially for dilute solutions, so the density information is not required.
mB = C VA =( 2 / 100 ) x 1000 =20 g

</doc>
<doc id="28751" url="http://en.wikipedia.org/wiki?curid=28751" title="Superluminal communication">
Superluminal communication

Superluminal communication is the hypothetical process by which one might send information at faster-than-light (FTL) speeds. The scientific consensus is that faster-than-light communication is not possible and to date superluminal communication has not been achieved in any experiment.
Some theories and experiments include:
According to the currently accepted theory, three of those four phenomena do not produce superluminal communication, even though they may give that appearance under some conditions. The third, tachyons, arguably do not exist as their existence is hypothetical; even if their existence were to be proven, attempts to quantize them appear to indicate that they may not be used for superluminal communication, because experiments to produce or absorb tachyons cannot be fully controlled.
If wormholes are possible, then ordinary subluminal methods of communication could be sent through them to achieve superluminal transmission speeds. Considering the immense energy that current theories suggest would be required to open a wormhole large enough to pass spacecraft through it may be that only atomic-scale wormholes would be practical to build, limiting their use solely to information transmission. Some theories of wormhole formation would prevent them from ever becoming "timeholes", allowing superluminal communication without the additional complication of allowing communication with the past.
The microscopic causality postulate of axiomatic quantum field theory implies the impossibility of superluminal communication using phenomena whose behavior can be described by orthodox quantum field theory. A special case of this is the no-communication theorem, which prevents communication using the quantum entanglement of a composite system shared between two spacelike-separated observers. Some authors have argued out that using the no-communication theorem to deduce the impossibility of superluminal communication is circular, since the no-communication theorem assumes to start with that the system is a composite system.
However, some argue that superluminal communication could be achieved "via" quantum entanglement using other methods that don't rely on cloning a quantum system. One suggested method would use an ensemble of entangled particles to transmit information, similar to a type of quantum eraser experiments where the observation of an interference pattern on half of an ensemble of entangled pairs is determined by the type of measurement performed on the other half. In these cases, though, the interference pattern only emerges with coincident measurements which requires a classical, subluminal communication channel between the two detectors. Physicist John G. Cramer at the University of Washington is attempting to perform one type of these experiment and demonstrate whether or not it can produce superluminal communication.

</doc>
<doc id="28754" url="http://en.wikipedia.org/wiki?curid=28754" title="Saul Bellow">
Saul Bellow

Saul Bellow (10 June 1915 – 5 April 2005) was a Canadian-born American writer. For his literary contributions, Bellow was awarded the Pulitzer Prize, the Nobel Prize for Literature, and the National Medal of Arts. He is the only writer to win the National Book Award for Fiction three times and he received the Foundation's lifetime Medal for Distinguished Contribution to American Letters in 1990.
In the words of the Swedish Nobel Committee, his writing exhibited "the mixture of rich picaresque novel and subtle analysis of our culture, of entertaining adventure, drastic and tragic episodes in quick succession interspersed with philosophic conversation, all developed by a commentator with a witty tongue and penetrating insight into the outer and inner complications that drive us to act, or prevent us from acting, and that can be called the dilemma of our age." His best-known works include "The Adventures of Augie March," "Henderson the Rain King", "Herzog", "Mr. Sammler's Planet", "Seize the Day", "Humboldt's Gift" and "Ravelstein". Widely regarded as one of the 20th century's greatest authors, Bellow has had a "huge literary influence."
Bellow said that of all his characters Eugene Henderson, of "Henderson the Rain King", was the one most like himself. Bellow grew up as an insolent slum kid, a "thick-necked" rowdy, and an immigrant from Quebec. As Christopher Hitchens describes it, Bellow's fiction and principal characters reflect his own yearning for transcendence, a battle "to overcome not just ghetto conditions but also ghetto psychoses." Bellow's protagonists, in one shape or another, all wrestle with what Corde (Albert Corde, the dean in "The Dean's December") called "the big-scale insanities of the 20th century." This transcendence of the "unutterably dismal" (a phrase from "Dangling Man") is achieved, if it can be achieved at all, through a "ferocious assimilation of learning" (Hitchens) and an emphasis on nobility.
Biography.
Early life.
Saul Bellow was born Solomon Bellows in Lachine, Quebec, two years after his parents, Lescha (née Gordin) and Abraham Bellows, emigrated from Saint Petersburg, Russia. Bellow's family was Lithuanian-Jewish, with father born in Vilnius. Bellow celebrated his birthday in June, although he may have been born in July (in the Jewish community, it was customary to record the Hebrew date of birth, which does not always coincide with the Gregorian calendar). Of his family's emigration, Bellow wrote: The retrospective was strong in me because of my parents. They were both full of the notion that they were falling, falling. They had been prosperous cosmopolitans in Saint Petersburg. My mother could never stop talking about the family dacha, her privileged life, and how all that was now gone. She was working in the kitchen. Cooking, washing, mending... There had been servants in Russia... But you could always transpose from your humiliating condition with the help of a sort of embittered irony.
A period of illness from a respiratory infection at age eight both taught him self-reliance (he was a very fit man despite his sedentary occupation) and provided an opportunity to satisfy his hunger for reading: reportedly, he decided to be a writer when he first read Harriet Beecher Stowe's "Uncle Tom's Cabin."
When Bellow was nine, his family moved to the Humboldt Park neighborhood on the West Side of Chicago, the city that formed the backdrop of many of his novels. Bellow's father, Abraham, was an onion importer. He also worked in a bakery, as a coal delivery man, and as a bootlegger. Bellow's mother, Liza, died when he was 17. He was left with his father and brother Maurice. His mother was deeply religious, and wanted her youngest son, Saul, to become a rabbi or a concert violinist. But he rebelled against what he later called the "suffocating orthodoxy" of his religious upbringing, and he began writing at a young age. Bellow's lifelong love for the Bible began at four when he learned Hebrew. Bellow also grew up reading William Shakespeare and the great Russian novelists of the 19th century. In Chicago, he took part in anthroposophical studies. Bellow attended Tuley High School on Chicago's west side where he befriended fellow writer Isaac Rosenfeld. In his 1959 novel "Henderson the Rain King", Bellow modeled the character King Dahfu on Rosenfeld.
Education and early career.
Bellow attended the University of Chicago but later transferred to Northwestern University. He originally wanted to study literature, but he felt the English department was anti-Jewish. Instead, he graduated with honors in anthropology and sociology. It has been suggested Bellow's study of anthropology had an influence on his literary style, and anthropological references pepper his works. Bellow later did graduate work at the University of Wisconsin–Madison.
Paraphrasing Bellow's description of his close friend Allan Bloom (see "Ravelstein"), John Podhoretz has said that both Bellow and Bloom "inhaled books and ideas the way the rest of us breathe air."
In the 1930s, Bellow was part of the Chicago branch of the Works Progress Administration Writer's Project, which included such future Chicago literary luminaries as Richard Wright and Nelson Algren. Many of the writers were radical: if they were not members of the Communist Party USA, they were sympathetic to the cause. Bellow was a Trotskyist, but because of the greater numbers of Stalinist-leaning writers he had to suffer their taunts.
In 1941 Bellow became a naturalized US citizen. In 1943, Maxim Lieber was his literary agent.
During World War II, Bellow joined the merchant marine and during his service he completed his first novel, "Dangling Man" (1944) about a young Chicago man waiting to be drafted for the war.
From 1946 through 1948 Bellow taught at the University of Minnesota, living on Commonwealth Avenue, in St. Paul, Minnesota.
In 1948, Bellow was awarded a Guggenheim Fellowship that allowed him to move to Paris, where he began writing "The Adventures of Augie March" (1953). Critics have remarked on the resemblance between Bellow's picaresque novel and the great 17th Century Spanish classic "Don Quixote". The book starts with one of American literature's most famous opening paragraphs, and it follows its titular character through a series of careers and encounters, as he lives by his wits and his resolve. Written in a colloquial yet philosophical style, "The Adventures of Augie March" established Bellow's reputation as a major author.
In the spring term of 1961 he taught creative writing at the University of Puerto Rico at Río Piedras.
One of his students was William Kennedy, who was encouraged by Bellow to write fiction.
Return to Chicago and mid-career.
Bellow lived in New York City for a number of years, but he returned to Chicago in 1962 as a professor at the Committee on Social Thought at the University of Chicago. The committee's goal was to have professors work closely with talented graduate students on a multi-disciplinary approach to learning. Bellow taught on the committee for more than 30 years, alongside his close friend, the philosopher Allan Bloom.
There were also other reasons for Bellow's return to Chicago, where he moved into the Hyde Park neighborhood with his third wife, Susan Glassman. Bellow found Chicago vulgar but vital, and more representative of America than New York. He was able to stay in contact with old high school friends and a broad cross-section of society. In a 1982 profile, Bellow's neighborhood was described as a high-crime area in the city's center, and Bellow maintained he had to live in such a place as a writer and "stick to his guns."
Bellow hit the bestseller list in 1964 with his novel "Herzog". Bellow was surprised at the commercial success of this cerebral novel about a middle-aged and troubled college professor who writes letters to friends, scholars and the dead, but never sends them. Bellow returned to his exploration of mental instability, and its relationship to genius, in his 1975 novel "Humboldt's Gift". Bellow used his late friend and rival, the brilliant but self-destructive poet Delmore Schwartz, as his model for the novel's title character, Von Humboldt Fleisher. Bellow also used Rudolf Steiner's spiritual science, anthroposophy, as a theme in the book, having attended a study group in Chicago. He was elected a Fellow of the American Academy of Arts and Sciences in 1969.
Nobel Prize and later career.
Propelled by the success of "Humboldt's Gift", Bellow won the Nobel Prize in literature in 1976. In the 70-minute address he gave to an audience in Stockholm, Sweden, Bellow called on writers to be beacons for civilization and awaken it from intellectual torpor.
The following year, the National Endowment for the Humanities selected Bellow for the Jefferson Lecture, the U.S. federal government's highest honor for achievement in the humanities. Bellow's lecture was entitled "The Writer and His Country Look Each Other Over."
Bellow traveled widely throughout his life, mainly to Europe, which he sometimes visited twice a year. As a young man, Bellow went to Mexico City to meet Leon Trotsky, but the expatriate Russian revolutionary was assassinated the day before they were to meet. Bellow's social contacts were wide and varied. He tagged along with Robert F. Kennedy for a magazine profile he never wrote, he was close friends with the author Ralph Ellison. His many friends included the journalist Sydney J. Harris and the poet John Berryman. 
While sales of Bellow's first few novels were modest, that turned around with "Herzog". Bellow continued teaching well into his old age, enjoying its human interaction and exchange of ideas. He taught at Yale University, University of Minnesota, New York University, Princeton University, University of Puerto Rico, University of Chicago, Bard College and Boston University, where he co-taught a class with James Wood ('modestly absenting himself' when it was time to discuss "Seize the Day"). In order to take up his appointment at Boston, Bellow moved in 1993 from Chicago to Brookline, Massachusetts, where he died on 5 April 2005, at age 89. He is buried at the Jewish cemetery Shir HeHarim of Brattleboro, Vermont.
Bellow was married five times, with all but his last marriage ending in divorce. His son by his first marriage, Greg Bellow, became a psychotherapist; Greg Bellow published "Saul Bellow’s Heart: A Son’s Memoir" in 2013, nearly a decade after his father's death. Bellow's son by his second marriage, Adam, published a nonfiction book "In Praise of Nepotism" in 2003. Bellow's wives were Anita Goshkin, Alexandra (Sondra) Tsachacbasov, Susan Glassman, Alexandra Ionescu Tulcea and Janis Freedman. In 1999, when he was 84, Bellow had a daughter, Rosie, his fourth child, with Freedman.
While he read voluminously, Bellow also played the violin and followed sports. Work was a constant for him, but he at times toiled at a plodding pace on his novels, frustrating the publishing company.
His early works earned him the reputation as a major novelist of the 20th century, and by his death he was widely regarded as one of the greatest living novelists. He was the first writer to win three National Book Awards in all award categories. His friend and protege Philip Roth has said of him, "The backbone of 20th-century American literature has been provided by two novelists—William Faulkner and Saul Bellow. Together they are the Melville, Hawthorne, and Twain of the 20th century." James Wood, in a eulogy of Bellow in "The New Republic", wrote:
I judged all modern prose by his. Unfair, certainly, because he made even the fleet-footed—the Updikes, the DeLillos, the Roths—seem like monopodes. Yet what else could I do? I discovered Saul Bellow's prose in my late teens, and henceforth, the relationship had the quality of a love affair about which one could not keep silent. Over the last week, much has been said about Bellow's prose, and most of the praise—perhaps because it has been overwhelmingly by men—has tended toward the robust: We hear about Bellow's mixing of high and low registers, his Melvillean cadences jostling the jivey Yiddish rhythms, the great teeming democracy of the big novels, the crooks and frauds and intellectuals who loudly people the brilliant sensorium of the fiction. All of this is true enough; John Cheever, in his journals, lamented that, alongside Bellow's fiction, his stories seemed like mere suburban splinters. Ian McEwan wisely suggested last week that British writers and critics may have been attracted to Bellow precisely because he kept alive a Dickensian amplitude now lacking in the English novel. [...] But nobody mentioned the beauty of this writing, its music, its high lyricism, its firm but luxurious pleasure in language itself. [...] [I]n truth, I could not thank him enough when he was alive, and I cannot now.
Themes and style.
The author's works speak to the disorienting nature of modern civilization, and the countervailing ability of humans to overcome their frailty and achieve greatness (or at least awareness). Bellow saw many flaws in modern civilization, and its ability to foster madness, materialism and misleading knowledge. Principal characters in Bellow's fiction have heroic potential, and many times they stand in contrast to the negative forces of society. Often these characters are Jewish and have a sense of alienation or otherness.
Jewish life and identity is a major theme in Bellow's work, although he bristled at being called a "Jewish writer." Bellow's work also shows a great appreciation of America, and a fascination with the uniqueness and vibrancy of the American experience.
Bellow's work abounds in references and quotes from the likes of Marcel Proust and Henry James, but he offsets these high-culture references with jokes. Bellow interspersed autobiographical elements into his fiction, and many of his principal characters were said to bear a resemblance to him.
Criticism, controversy and conservative cultural activism.
Martin Amis described Bellow as "The greatest American author ever, in my view".
His sentences seem to weigh more than anyone else's. He is like a force of nature... He breaks all the rules [...] [T]he people in Bellow's fiction are real people, yet the intensity of the gaze that he bathes them in, somehow through the particular, opens up into the universal.
For Linda Grant, "What Bellow had to tell us in his fiction was that it was worth it, being alive."
His vigour, vitality, humour and passion were always matched by the insistence on thought, not the predigested cliches of the mass media or of those on the left, which had begun to disgust him by the Sixties... It's easy to be a 'writer of conscience'—anyone can do it if they want to; just choose your cause. Bellow was a writer about conscience and consciousness, forever conflicted by the competing demands of the great cities, the individual's urge to survival against all odds and his equal need for love and some kind of penetrating understanding of what there was of significance beyond all the racket and racketeering.
On the other hand, Bellow's detractors considered his work conventional and old-fashioned, as if the author was trying to revive the 19th-century European novel. In a private letter, Vladimir Nabokov once referred to Bellow as a "miserable mediocrity." Journalist and author Ron Rosenbaum described Bellow's "Ravelstein" (2000) as the only book that rose above Bellow's failings as an author. Rosenbaum wrote,
My problem with the pre-Ravelstein Bellow is that he all too often strains too hard to yoke together two somewhat contradictory aspects of his being and style. There's the street-wise Windy City wiseguy and then—as if to show off that the wiseguy has Wisdom—there are the undigested chunks of arcane, not entirely impressive, philosophic thought and speculation. Just to make sure you know his novels have intellectual heft. That the world and the flesh in his prose are both figured and transfigured.
Sam Tanenhaus wrote in "New York Times Book Review" in 2007:
But what, then, of the many defects—the longueurs and digressions, the lectures on anthroposophy and religion, the arcane reading lists? What of the characters who don't change or grow but simply bristle onto the page, even the colorful lowlifes pontificating like fevered students in the seminars Bellow taught at the University of Chicago? And what of the punitively caricatured ex-wives drawn from the teeming annals of the novelist's own marital discord?
But, Tanenhaus went on to answer his question:
Shortcomings, to be sure. But so what? Nature doesn't owe us perfection. Novelists don't either. Who among us would even recognize perfection if we saw it? In any event, applying critical methods, of whatever sort, seemed futile in the case of an author who, as Randall Jarrell once wrote of Walt Whitman, 'is a world, a waste with, here and there, systems blazing at random out of the darkness'—those systems 'as beautifully and astonishingly organized as the rings and satellites of Saturn.' 
V. S. Pritchett praised Bellow, finding his shorter works to be his best. Pritchett called Bellow's novella "Seize the Day" a "small gray masterpiece."
As he grew older, Bellow moved decidedly away from leftist politics and became identified with cultural conservatism. His opponents included feminism, campus activism and postmodernism. Bellow also thrust himself into the often contentious realm of Jewish and African-American relations. Bellow has also been critical of multiculturalism and once said: "Who is the Tolstoy of the Zulus? The Proust of the Papuans? I'd be glad to read him."
Despite his identification with Chicago, he kept aloof from some of that city's more conventional writers. In a 2006 interview with "Stop Smiling" magazine, Studs Terkel said of Bellow: "I didn't know him too well. We disagreed on a number of things politically. In the protests in the beginning of Norman Mailer's "Armies of the Night", when Mailer, Robert Lowell and Paul Goodman were marching to protest the Vietnam War, Bellow was invited to a sort of counter-gathering. He said, 'Of course I'll attend'. But he made a big thing of it. Instead of just saying OK, he was proud of it. So I wrote him a letter and he didn't like it. He wrote me a letter back. He called me a Stalinist. But otherwise, we were friendly. He was a brilliant writer, of course. I love "Seize the Day"."

</doc>
<doc id="28765" url="http://en.wikipedia.org/wiki?curid=28765" title="Lawrence Alma-Tadema">
Lawrence Alma-Tadema

Sir Lawrence Alma-Tadema, OM, RA (; born Lourens Alma Tadema ]; 8 January 1836 – 25 June 1912) was a Frisian painter of special British denizenship.
Born in Dronrijp, the Netherlands, and trained at the Royal Academy of Antwerp, Belgium, he settled in England in 1870 and spent the rest of his life there. A classical-subject painter, he became famous for his depictions of the luxury and decadence of the Roman Empire, with languorous figures set in fabulous marbled interiors or against a backdrop of dazzling blue Mediterranean Sea and sky.
Though admired during his lifetime for his draftsmanship and depictions of Classical antiquity, his work fell into disrepute after his death, and only since the 1960s has it been re-evaluated for its importance within nineteenth-century English art.
Biography.
Early life.
Lourens Alma Tadema was born on 8 January 1836 in the village of Dronrijp in the province of Friesland in the north of the Netherlands. The surname "Tadema" is an old Frisian patronymic, meaning 'son of Tade', while the names "Lourens" and "Alma" came from his godfather. He was the sixth child of Pieter Jiltes Tadema (1797–1840), the village notary, and the third child of Hinke Dirks Brouwer ("c." 1800–1863). His father had three sons from a previous marriage. His parents' first child died young, and the second was Atje ("c." 1834–1876), Lourens' sister, for whom he had great affection.
The Tadema family moved in 1838 to the nearby city of Leeuwarden, where Pieter's position as a notary would be more lucrative. His father died when Lourens was four, leaving his mother with five children: Lourens, his sister, and three boys from his father’s first marriage. His mother had artistic leanings, and decided that drawing lessons should be incorporated into the children's education. He received his first art training with a local drawing master hired to teach his older half-brothers.
It was intended that the boy would become a lawyer; but in 1851 at the age of fifteen he suffered a physical and mental breakdown. Diagnosed as consumptive and given only a short time to live, he was allowed to spend his remaining days at his leisure, drawing and painting. Left to his own devices he regained his health and decided to pursue a career as an artist.
Move to Belgium.
In 1852 he entered the Royal Academy of Antwerp in Belgium where he studied early Dutch and Flemish art, under Egide Charles Gustave Wappers. During Alma-Tadema's four years as a registered student at the Academy, he won several respectable awards.
Before leaving school, towards the end of 1855, he became assistant to the painter and professor Louis (Lodewijk) Jan de Taeye, whose courses in history and historical costume he had greatly enjoyed at the Academy. Although de Taeye was not an outstanding painter, Alma-Tadema respected him and became his studio assistant, working with him for three years. De Taeye introduced him to books that influenced his desire to portray Merovingian subjects early in his career. He was encouraged to depict historical accuracy in his paintings, a trait for which the artist became known. 
Alma-Tadema left Taeye's studio in November 1858 returning to Leeuwarden before settling in Antwerp, where he began working with the painter Baron Jan August Hendrik Leys, whose studio was one of the most highly regarded in Belgium. Under his guidance Alma-Tadema painted his first major work: "The Education of the children of Clovis" (1861). This painting created a sensation among critics and artists when it was exhibited that year at the Artistic Congress in Antwerp. It is said to have laid the foundation of his fame and reputation. Alma-Tadema related that although Leys thought the completed painting better than he had expected, he was critical of the treatment of marble, which he compared to cheese. 
Alma-Tadema took this criticism very seriously, and it led him to improve his technique and to become the world's foremost painter of marble and variegated granite. Despite any reproaches from his master, "The Education of the Children of Clovis" was honorably received by critics and artists alike and was eventually purchased and subsequently given to King Leopold of Belgium.
Early works.
Merovingian themes were the painter's favourite subject up to the mid-1860s. It is perhaps in this series that we find the artist moved by the deepest feeling and the strongest spirit of romance. However Merovingian subjects did not have a wide international appeal, so he switched to themes of life in ancient Egypt that were more popular. On these scenes of Frankish and Egyptian life Alma-Tadema spent great energy and much research. In 1862 Alma-Tadema left Leys's studio and started his own career, establishing himself as a significant classical-subject European artist.
1863 was to alter the course of Alma-Tadema's personal and professional life: on 3 January his invalid mother died, and on 24 September he was married, in Antwerp City Hall, to Marie-Pauline Gressin Dumoulin, the daughter of Eugene Gressin Dumoulin, a French journalist living near Brussels. Nothing is known of their meeting and little of Pauline herself, as Alma-Tadema never spoke about her after her death in 1869. Her image appears in a number of oils, though he painted her portrait only three times, the most notable appearing in "My studio" (1867). The couple had three children. Their eldest and only son lived only a few months dying of smallpox. Their two daughters, Laurence (1864–1940) and Anna (1867–1943), both had artistic leanings: the former in literature, the latter in art. Neither would marry.
Alma-Tadema and his wife spent their honeymoon in Florence, Rome, Naples and Pompeii. This, his first visit to Italy, developed his interest in depicting the life of ancient Greece and Rome, especially the latter since he found new inspiration in the ruins of Pompeii, which fascinated him and would inspire much of his work in the coming decades.
During the summer of 1864, Tadema met Ernest Gambart, the most influential print publisher and art dealer of the period. Gambart was highly impressed with the work of Tadema, who was then painting "Egyptian chess players" (1865). The dealer, recognising at once the unusual gifts of the young painter, gave him an order for twenty-four pictures and arranged for three of Tadema's paintings to be shown in London. In 1865, Tadema relocated to Brussels where he was named a knight of the Order of Leopold.
On 28 May 1869, after years of ill health, Pauline died at Schaerbeek, in Belgium, at the age of thirty-two, of smallpox. Her death left Tadema disconsolate and depressed. He ceased painting for nearly four months. His sister Artje, who lived with the family, helped with the two daughters then aged five and two. Artje took over the role of housekeeper and remained with the family until 1873 when she married.
During the summer Tadema himself began to suffer from a medical problem which doctors in Brussels were frustratingly unable to diagnose. Gambart eventually advised him to go to England for another medical opinion. Soon after his arrival in London in December 1869, Alma-Tadema was invited to the home of the painter Ford Madox Brown. There he met Laura Theresa Epps, who was seventeen years old, and fell in love with her at first sight.
Move to England.
The outbreak of the Franco-Prussian War in July 1870 compelled Alma-Tadema to leave the continent and move to London. His infatuation with Laura Epps played a great part in his relocation to England and Gambart felt that the move would be advantageous to the artist's career. In stating his reasons for the move, Tadema simply said:
With his small daughters and sister Atje, Alma-Tadema arrived in London at the beginning of September 1870. The painter wasted no time in contacting Laura, and it was arranged that he would give her painting lessons. During one of these, he proposed marriage. As he was then thirty-four and Laura was now only eighteen, her father was initially opposed to the idea. Dr Epps finally agreed on the condition that they should wait until they knew each other better. They married in July 1871. Laura, under her married name, also won a high reputation as an artist, and appears in numerous of Alma-Tadema's canvases after their marriage ("The Women of Amphissa" (1887) being a notable example). This second marriage was enduring and happy, though childless, and Laura became stepmother to Anna and Laurence. Anna became a painter and Laurence became a novelist.
He would initially adopt the name "Laurence Alma Tadema" instead of "Lourens Alma Tadema" and later adopt the more English "Lawrence" for his forename, and incorporate "Alma" into his surname so that he appeared at the beginning of exhibition catalogues, under "A" rather than under "T". He did not actually hyphenate his last name, but it was done by others and this has since become the convention.
Victorian painter.
After his arrival in England, where he was to spend the rest of his life, Alma-Tadema's career was one of continued success. He became one of the most famous and highly paid artists of his time, acknowledged and rewarded. By 1871 he had met and befriended most of the major Pre-Raphaelite painters and it was in part due to their influence that the artist brightened his palette, varied his hues, and lightened his brushwork.
In 1872 Alma-Tadema organised his paintings into an identification system by including an opus number under his signature and assigning his earlier pictures numbers as well. "Portrait of my sister, Artje", painted in 1851, is numbered opus I, while two months before his death he completed "Preparations in the Coliseum", opus CCCCVIII. Such a system would make it difficult for fakes to be passed off as originals.
In 1873 Queen Victoria in Council by letters patent made Alma-Tadema and his wife what are now the last British Denizens (the legal process has theoretically not yet been abolished in the United Kingdom), with some limited special rights otherwise only accorded to and enjoyed by British subjects (what would now be called British citizens). The previous year he and his wife made a journey on the Continent that lasted five and a half months and took them through Brussels, Germany, and Italy. In Italy they were able to take in the ancient ruins again; this time he purchased several photographs, mostly of the ruins, which began his immense collection of folios with archival material sufficient for the documentation used in the completion of future paintings. In January 1876, he rented a studio in Rome. The family returned to London in April, visiting the Parisian Salon on their way back. In London he regularly met with fellow-artist Emil Fuchs.
Among the most important of his pictures during this period was "An Audience at Agrippa's" (1876). When an admirer of the painting offered to pay a substantial sum for a painting with a similar theme, Alma-Tadema simply turned the emperor around to show him leaving in "After the Audience".
On 19 June 1879, Alma-Tadema was made a full Academician, his most personally important award. Three years later a major retrospective of his entire oeuvre was organised at the Grosvenor Gallery in London, including 185 of his pictures.
In 1883 he returned to Rome and, most notably, Pompeii, where further excavations had taken place since his last visit. He spent a significant amount of time studying the site, going there daily. These excursions gave him an ample source of subject matter as he began to further his knowledge of daily Roman life. At times, however, he integrated so many objects into his paintings that some said they resembled museum catalogues.
One of his most famous paintings is "The Roses of Heliogabalus" (1888) – based on an episode from the life of the debauched Roman Emperor Elagabalus (Heliogabalus), the painting depicts the psychopathic Emperor suffocating his guest at an orgy under a cascade of rose petals. The blossoms depicted were sent weekly to the artist's London studio from the Riviera for four months during the winter of 1887–1888.
Among Alma-Tadema's works of this period are: "An Earthly Paradise" (1891), "Unconscious Rivals" (1893) "Spring" (1894), "The Coliseum" (1896) and "The Baths of Caracalla" (1899). Although Alma-Tadema's fame rests on his paintings set in Antiquity, he also painted portraits, landscapes and watercolours, and made some etchings himself (although many more were made of his paintings by others).
Personality.
For all the quiet charm and erudition of his paintings, Alma-Tadema himself preserved a youthful sense of mischief. He was childlike in his practical jokes and in his sudden bursts of bad temper, which could as suddenly subside into an engaging smile.
In his personal life, Alma-Tadema was an extrovert and had a remarkably warm personality. He had most of the characteristics of a child, coupled with the admirable traits of a consummate professional. A perfectionist, he remained in all respects a diligent, if somewhat obsessive and pedantic worker. He was an excellent businessman, and one of the wealthiest artists of the nineteenth century. Alma-Tadema was as firm in money matters as he was with the quality of his work.
As a man, Lawrence Alma-Tadema was a robust, fun loving and rather portly gentleman. There was not a hint of the delicate artist about him; he was a cheerful lover of wine, women and parties.
Later years.
Alma-Tadema's output decreased with time, partly on account of health, but also because of his obsession with decorating his new home, to which he moved in 1883. Nevertheless, he continued to exhibit throughout the 1880s and into the next decade, receiving a plentiful amount of accolades along the way, including the medal of Honour at the Paris Exposition Universelle of 1889, election to an honorary member of the Oxford University Dramatic Society in 1890, the Great Gold Medal at the International Exposition in Brussels of 1897. In 1899 he was Knighted in England, only the eighth artist from the Continent to receive the honour. Not only did he assist with the organisation of the British section at the 1900 Exposition Universelle in Paris, he also exhibited two works that earned him the Grand Prix Diploma. He also assisted with the St. Louis World's Fair of 1904 where he was well represented and received.
During this time, Alma-Tadema was very active with theatre design and production, designing many costumes. He also spread his artistic boundaries and began to design furniture, often modelled after Pompeian or Egyptian motifs, illustrations, textiles, and frame making. His diverse interests highlight his talents. Each of these exploits were used in his paintings, as he often incorporated some of his designed furniture into the composition, and must have used many of his own designs for the clothing of his female subjects. Through his last period of creativity Alma-Tadema continued to produce paintings, which repeat the successful formula of women in marble terraces overlooking the sea such as in "Silver Favourites" (1903). Between 1906 and his death six years later, Alma-Tadema painted less but still produced ambitious paintings like "The Finding of Moses" (1904).
On 15 August 1909 Alma-Tadema's wife, Laura, died at the age of fifty-seven. The grief-stricken widower outlived his second wife by less than three years. His last major composition was "Preparation in the Coliseum (1912)". In the summer of 1912, Alma Tadema was accompanied by his daughter Anna to Kaiserhof Spa, Wiesbaden, Germany where he was to undergo treatment for ulceration of the stomach. He died there on 28 June 1912 at the age of seventy-six. He was buried in a crypt in St Paul's Cathedral in London.
Style.
Alma-Tadema's works are remarkable for the way in which flowers, textures and hard reflecting substances, like metals, pottery, and especially marble, are painted – indeed, his realistic depiction of marble led him to be called the 'marbellous painter'. His work shows much of the fine execution and brilliant colour of the old Dutch masters. By the human interest with which he imbues all his scenes from ancient life he brings them within the scope of modern feeling, and charms us with gentle sentiment and playfulness.
From early in his career, Alma-Tadema was particularly concerned with architectural accuracy, often including objects that he would see at museums – such as the British Museum in London – in his works. He also read many books and took many images from them. He amassed an enormous number of photographs from ancient sites in Italy, which he used for the most precise accuracy in the details of his compositions.
Alma-Tadema was a perfectionist. He worked assiduously to make the most of his paintings, often repeatedly reworking parts of paintings before he found them satisfactory to his own high standards. One humorous story relates that one of his paintings was rejected and instead of keeping it, he gave the canvas to a maid who used it as her table cover. He was sensitive to every detail and architectural line of his paintings, as well as the settings he was depicting. For many of the objects in his paintings, he would depict what was in front of him, using fresh flowers imported from across the continent and even from Africa, rushing to finish the paintings before the flowers died. It was this commitment to veracity that earned him recognition but also caused many of his adversaries to take up arms against his almost encyclopaedic works.
Alma-Tadema's work has been linked with that of European Symbolist painters. As an artist of international reputation, he can be cited as an influence on European figures such as Gustav Klimt and Fernand Khnopff. Both painters incorporate classical motifs into their works and use Alma-Tadema's unconventional compositional devices such as abrupt cut-off at the edge of the canvas. They, like Alma-Tadema, also employ coded imagery to convey meaning to their paintings.
Reputation.
Alma-Tadema was among the most financially successful painters of the Victorian era, though never matching Edwin Henry Landseer. For over sixty years he gave his audience exactly what they wanted: distinctive, elaborate paintings of beautiful people in classical settings. His incredibly detailed reconstructions of ancient Rome, with languid men and women posed against white marble in dazzling sunlight provided his audience with a glimpse of a world of the kind they might one day construct for themselves at least in attitude if not in detail. As with other painters, the reproduction rights for prints were often worth more than the canvas, and a painting with its rights still attached may have been sold to Gambart for £10,000 in 1874; without rights it was sold again in 1903, when Alma-Tadema's prices were actually higher, for £2,625. Typical prices were between £2,000 and £3,000 in the 1880s, but at least three works sold for between £5,250 and £6,060 in the 1900s. Prices held well until the general collapse of Victorian prices in the early 1920s, when they fell to the hundreds, where they remained until the 1960s; by 1969 £4,600 had been reached again (the huge effect of inflation must of course be remembered for all these figures).
The last years of Alma-Tadema's life saw the rise of Post-Impressionism, Fauvism, Cubism and Futurism, of which he heartily disapproved. As his pupil John Collier wrote, 'it is impossible to reconcile the art of Alma-Tadema with that of Matisse, Gauguin and Picasso.'
His artistic legacy almost vanished. As attitudes of the public in general and the artists in particular became more sceptical of the possibilities of human achievement, his paintings were increasingly denounced. He was declared "the worst painter of the 19th century" by John Ruskin, and one critic even remarked that his paintings were "about worthy enough to adorn bourbon boxes." After this brief period of being actively derided, he was consigned to relative obscurity for many years. Only since the 1960s has Alma-Tadema's work been re-evaluated for its importance within the nineteenth century, and more specifically, within the evolution of English art.
He is now regarded as one of the principal classical-subject painters of the nineteenth century whose works demonstrate the care and exactitude of an era mesmerised by trying to visualise the past, some of which was being recovered through archaeological research.
Alma-Tadema's meticulous archaeological research, including research into Roman architecture (which was so thorough that every building featured in his canvases could have been built using Roman tools and methods) led to his paintings being used as source material by Hollywood directors in their vision of the ancient world for films such as D. W. Griffith's "Intolerance" (1916), "Ben Hur" (1926)," Cleopatra" (1934), and most notably of all, Cecil B. DeMille's epic remake of "The Ten Commandments" (1956). Indeed, Jesse Lasky Jr., the co-writer on "The Ten Commandments", described how the director would customarily spread out prints of Alma-Tadema paintings to indicate to his set designers the look he wanted to achieve. The designers of the Oscar-winning Roman epic "Gladiator" used the paintings of Alma-Tadema as a central source of inspiration. Alma-Tadema's paintings were also the inspiration for the design of the interior of Cair Paravel castle in the 2005 film "".
In 1962, New York art dealer Robert Isaacson mounted the first show of Alma-Tadema's work in fifty years; by the late 1960s, the revival of interest in Victorian painting gained impetus, and a number of well-attended exhibitions were held. Allen Funt, the creator and host of the American version of the television show "Candid Camera", was a collector of Alma-Tadema paintings at a time when the artist's reputation in the 20th century was at its nadir; in a relatively few years he bought 35 works, about ten percent of Alma-Tadema's output. After Funt was robbed by his accountant (who subsequently committed suicide), he was forced to sell his collection at Sotheby's in London in November 1973. From this sale, the interest in Alma-Tadema was re-awakened. 
In 1960, the Newman Gallery firstly tried to sell, then give away (without success) one of his most celebrated works, "The Finding of Moses" (1904). The initial purchaser had paid £5,250 for it on its completion, and subsequent sales were for £861 in 1935, £265 in 1942, and it was "bought in" at £252 in 1960 (having failed to meet its reserve), but when the same picture was auctioned at Christies in New York in May 1995, it sold for £1.75 million. On 4 November 2010 it was sold for $35,922,500 to an undisclosed bidder at Sotheby's New York, a new record for the artist and a Victorian painting. On 5 May 2011 his "The Meeting of Antony and Cleopatra: 41 BC" was sold at the same auction house for $29.2 million.
A blue plaque unveiled in 1975 commemorates Alma-Tadema at 44 Grove End Road, St John's Wood.

</doc>
<doc id="28814" url="http://en.wikipedia.org/wiki?curid=28814" title="Secure Shell">
Secure Shell

Secure Shell, or SSH, is a cryptographic (encrypted) network protocol for initiating text-based shell sessions on remote machines in a secure way.
This allows a user to run commands on a machine's command prompt without them being physically present near the machine. It also allows a user to establish a secure channel over an insecure network in a client-server architecture, connecting an SSH client application with an SSH server. Common applications include remote command-line login and remote command execution, but any network service can be secured with SSH. The protocol specification distinguishes between two major versions, referred to as SSH-1 and SSH-2.
The most visible application of the protocol is for access to shell accounts on Unix-like operating systems, but it can also be used in a similar fashion on Windows.
SSH was designed as a replacement for Telnet and other insecure remote shell protocols such as the Berkeley rsh and rexec protocols, which send information, notably passwords, in plaintext, rendering them susceptible to interception and disclosure using packet analysis. The encryption used by SSH is intended to provide confidentiality and integrity of data over an unsecured network, such as the Internet, although files leaked by Edward Snowden indicate that the National Security Agency can sometimes decrypt SSH.
Definition.
SSH uses public-key cryptography to authenticate the remote computer and allow it to authenticate the user, if necessary. There are several ways to use SSH; one is to use automatically generated public-private key pairs to simply encrypt a network connection, and then use password authentication to log on.
Another is to use a manually generated public-private key pair to perform the authentication, allowing users or programs to log in without having to specify a password. In this scenario, anyone can produce a matching pair of different keys (public and private). The public key is placed on all computers that must allow access to the owner of the matching private key (the owner keeps the private key secret). While authentication is based on the private key, the key itself is never transferred through the network during authentication. SSH only verifies whether the same person offering the public key also owns the matching private key. In all versions of SSH it is important to verify unknown public keys, i.e. associate the public keys with identities, before accepting them as valid. Accepting an attacker's public key without validation will authorize an unauthorized attacker as a valid user.
Key management.
On Unix-like systems, the list of authorized public keys is typically stored in the home directory of the user that is allowed to log in remotely, in the file ~/.ssh/authorized_keys. This file is respected by ssh only if it is not writable by anything apart from the owner and root. When the public key is present on the remote end and the matching private key is present on the local end, typing in the password is no longer required (some software like Message Passing Interface (MPI) stack may need this password-less access to run properly). However, for additional security the private key itself can be locked with a passphrase.
The private key can also be looked for in standard places, and its full path can be specified as a command line setting (the option "-i" for ssh). The ssh-keygen utility produces the public and private keys, always in pairs.
SSH also supports password-based authentication that is encrypted by automatically generated keys. In this case the attacker could imitate the legitimate server side, ask for the password, and obtain it (man-in-the-middle attack). However, this is possible only if the two sides have never authenticated before, as SSH remembers the key that the server side previously used. The SSH client raises a warning before accepting the key of a new, previously unknown server. Password authentication can be disabled.
Usage.
SSH is typically used to log into a remote machine and execute commands, but it also supports tunneling, forwarding TCP ports and X11 connections; it can transfer files using the associated SSH file transfer (SFTP) or secure copy (SCP) protocols. SSH uses the client-server model.
The standard TCP port 22 has been assigned for contacting SSH servers.
An SSH client program is typically used for establishing connections to an SSH daemon accepting remote connections. Both are commonly present on most modern operating systems, including Mac OS X, most distributions of GNU/Linux, OpenBSD, FreeBSD, NetBSD, Solaris and OpenVMS. Notably, Windows is one of the few modern desktop/server OSs that does not include SSH by default. Proprietary, freeware and open source (e.g. PuTTY, and the version of openSSH which is part of Cygwin) versions of various levels of complexity and completeness exist. Native Linux file managers (e.g. Konqueror) can use the FISH protocol to provide a split-pane GUI with drag-and-drop. The open source Windows program WinSCP provides similar file management (synchronization, copy, remote delete) capability using PuTTY as a back-end. Both WinSCP and PuTTY are available packaged to run directly off of a USB drive, without requiring installation on the client machine. Setting up an SSH server in Windows typically involves installation (e.g. via installing Cygwin, or by installing a stripped down version of Cygwin with the SSH server).
SSH is important in cloud computing to solve connectivity problems, avoiding the security issues of exposing a cloud-based virtual machine directly on the Internet. An SSH tunnel can provide a secure path over the Internet, through a firewall to a virtual machine.
History and development.
Version 1.x.
In 1995, Tatu Ylönen, a researcher at Helsinki University of Technology, Finland, designed the first version of the protocol (now called SSH-1) prompted by a password-sniffing attack at his university network. The goal of SSH was to replace the earlier rlogin, TELNET and rsh protocols, which did not provide strong authentication nor guarantee confidentiality. Ylönen released his implementation as freeware in July 1995, and the tool quickly gained in popularity. Towards the end of 1995, the SSH user base had grown to 20,000 users in fifty countries.
In December 1995, Ylönen founded SSH Communications Security to market and develop SSH. The original version of the SSH software used various pieces of free software, such as GNU libgmp, but later versions released by SSH Communications Security evolved into increasingly proprietary software.
It is estimated that, as of 2000[ [update]], there were 2 million users of SSH.
Version 1.99.
In January 2006, well after version 2.1 was established, RFC 4253 specified that an SSH server which supports both 2.0 and prior versions of SSH should identify its protoversion as 1.99. This is not an actual version but a method to identify backward compatibility.
OpenSSH and OSSH.
In 1999, developers wanting a free software version to be available went back to the older 1.2.12 release of the original SSH program, which was the last released under an open source license. Björn Grönvall's OSSH was subsequently developed from this codebase. Shortly thereafter, OpenBSD developers forked Grönvall's code and did extensive work on it, creating OpenSSH, which shipped with the 2.6 release of OpenBSD. From this version, a "portability" branch was formed to port OpenSSH to other operating systems.
s of 2005[ [update]], OpenSSH was the single most popular SSH implementation, coming by default in a large number of operating systems. OSSH meanwhile has become obsolete. OpenSSH continues to be maintained and now supports both 1.x and 2.0 versions.
Version 2.x.
"Secsh" was the official Internet Engineering Task Force's (IETF) name for the IETF working group responsible for version 2 of the SSH protocol. In 2006, a revised version of the protocol, SSH-2, was adopted as a standard. This version is incompatible with SSH-1. SSH-2 features both security and feature improvements over SSH-1. Better security, for example, comes through Diffie–Hellman key exchange and strong integrity checking via message authentication codes. New features of SSH-2 include the ability to run any number of shell sessions over a single SSH connection. Due to SSH-2's superiority and popularity over SSH-1, some implementations such as Lsh and Dropbear support only the SSH-2 protocol.
Uses.
SSH is a protocol that can be used for many applications across many platforms including most Unix variants (Linux, the BSDs including Apple's OS X, & Solaris), as well as Microsoft Windows. Some of the applications below may require features that are only available or compatible with specific SSH clients or servers. For example, using the SSH protocol to implement a VPN is possible, but presently only with the OpenSSH server and client implementation.
File transfer protocols using SSH.
There are multiple mechanisms for transferring files using the Secure Shell protocols.
Architecture.
The SSH-2 protocol has an internal architecture (defined in RFC 4251) with well-separated layers, namely:
This open architecture provides considerable flexibility, allowing the use of SSH for a variety of purposes beyond a secure shell. The functionality of the transport layer alone is comparable to Transport Layer Security (TLS); the user-authentication layer is highly extensible with custom authentication methods; and the connection layer provides the ability to multiplex many secondary sessions into a single SSH connection, a feature comparable to BEEP and not available in TLS.
Enhancements.
These are intended for performance enhancements of SSH products:
Vulnerabilities.
Vulnerabilities in 1.x.
In 1998 a vulnerability was described in SSH 1.5 which allowed the unauthorized insertion of content into an encrypted SSH stream due to insufficient data integrity protection from CRC-32 used in this version of the protocol. A fix known as SSH Compensation Attack Detector was introduced into most implementations. Many of these updated implementations contained a new integer overflow vulnerability that allowed attackers to execute arbitrary code with the privileges of the SSH daemon, typically root.
In January 2001 a vulnerability was discovered that allows attackers to modify the last block of an IDEA-encrypted session. The same month, another vulnerability was discovered that allowed a malicious server to forward a client authentication to another server.
Since SSH-1 has inherent design flaws which make it vulnerable, it is now generally considered obsolete and should be avoided by explicitly disabling fallback to SSH-1. Most modern servers and clients support SSH-2.
Vulnerabilities in 2.x.
In November 2008, a theoretical vulnerability was discovered for all versions of SSH which allowed recovery of up to 32 bits of plaintext from a block of ciphertext that was encrypted using what was then the standard default encryption mode, CBC. The most straightforward solution is to use CTR mode instead of CBC mode, since this renders SSH resistant to the attack.
Unknown vulnerabilities.
On December 28, 2014 Der Spiegel published classified information leaked by whistleblower Edward Snowden which suggests that the National Security Agency may be able to decrypt some SSH traffic. The technical details associated with this attack were not released as a part of the publication.
Standards documentation.
The following RFC publications by the IETF "secsh" working group document SSH-2 as a proposed Internet standard.
It was later modified and expanded by the following publications.

</doc>
<doc id="28824" url="http://en.wikipedia.org/wiki?curid=28824" title="Stéphane Mallarmé">
Stéphane Mallarmé

Stéphane Mallarmé (; ]; 18 March 1842 – 9 September 1898), whose real name was Étienne Mallarmé, was a French poet and critic. He was a major French symbolist poet, and his work anticipated and inspired several revolutionary artistic schools of the early 20th century, such as Cubism, Futurism, Dadaism, and Surrealism.
Biography.
Stéphane Mallarmé was born in Paris. He worked as an English teacher and spent much of his life in relative poverty but was famed for his "salons", occasional gatherings of intellectuals at his house on the rue de Rome for discussions of poetry, art and philosophy. The group became known as "les Mardistes," because they met on Tuesdays (in French, "mardi"), and through it Mallarmé exerted considerable influence on the work of a generation of writers. For many years, those sessions, where Mallarmé held court as judge, jester, and king, were considered the heart of Paris intellectual life. Regular visitors included W.B. Yeats, Rainer Maria Rilke, Paul Valéry, Stefan George, Paul Verlaine, and many others.
On 10 August 1863, he married Maria Christina Gerhard. Their daughter, (Stéphanie Françoise) Geneviève Mallarmé, was born on 19 November 1864. Mallarmé died in Valvins (present-day Vulaines-sur-Seine) September 9, 1898.
Style.
Mallarmé's earlier work owes a great deal to the style of Charles Baudelaire who was recognised as the forerunner of literary Symbolism. Mallarmé's later "fin de siècle" style, on the other hand, anticipates many of the fusions between poetry and the other arts that were to blossom in the next century. Most of this later work explored the relationship between content and form, between the text and the arrangement of words and spaces on the page. This is particularly evident in his last major poem, "Un coup de dés jamais n'abolira le hasard" ('A roll of the dice will never abolish chance') of 1897.
Some consider Mallarmé one of the French poets most difficult to translate into English. The difficulty is due in part to the complex, multilayered nature of much of his work, but also to the important role that the sound of the words, rather than their meaning, plays in his poetry. When recited in French, his poems allow alternative meanings which are not evident on reading the work on the page. For example, Mallarmé's "Sonnet en '-yx"' opens with the phrase "ses purs ongles" ('her pure nails'), whose first syllables when spoken aloud sound very similar to the words "c'est pur son" ('it's pure sound'). Indeed, the 'pure sound' aspect of his poetry has been the subject of musical analysis and has inspired musical compositions. These phonetic ambiguities are very difficult to reproduce in a translation which must be faithful to the meaning of the words.
Influence.
General poetry.
Mallarmé's poetry has been the inspiration for several musical pieces, notably Claude Debussy's "Prélude à l'après-midi d'un faune" (1894), a free interpretation of Mallarmé's poem "L'après-midi d'un faune" (1876), which creates powerful impressions by the use of striking but isolated phrases. Maurice Ravel set Mallarmé's poetry to music in "Trois poèmes de Stéphane Mallarmé" (1913). Other composers to use his poetry in song include Darius Milhaud ("Chansons bas de Stéphane Mallarmé", 1917) and Pierre Boulez ("Pli selon pli", 1957–62).
Coldplay alludes to his final poem in their song, Vida la Vida. The second verse: "I used to roll the dice - Feel the fear in my enemy's eyes - Listen as the crowd would sing - 'Now the old king is dead! Long live the king!"' "The Viva la Vida" album art confirms the subject, featuring a reproduction of "Liberty Leading the People" by Eugène Delacroix, which commemorates the July Revolution of 1830.
Man Ray's last film, entitled "Les Mystères du Château de Dé (The Mystery of the Chateau of Dice)" (1929), was greatly influenced by Mallarmé's work, prominently featuring the line "A roll of the dice will never abolish chance".
Mallarmé is referred to extensively in the latter section of Joris-Karl Huysmans' À rebours, where Des Esseintes describes his fervour-infused enthusiasm for the poet: "These were Mallarmé's masterpieces and also ranked among the masterpieces of prose poetry, for they combined a style so magnificently that in itself it was as soothing as a melancholy incantation, an intoxicating melody, with irresistibly suggestive thoughts, the soul-throbs of a sensitive artist whose quivering nerves vibrate with an intensity that fills you with a painful ecstasy." [p. 198, Robert Baldick translation]
The critic and translator Barbara Johnson has emphasized Mallarmé's influence on twentieth-century French criticism and theory: "It was largely by learning the lesson of Mallarmé that critics like Roland Barthes came to speak of 'the death of the author' in the making of literature. Rather than seeing the text as the emanation of an individual author's intentions, structuralists and deconstructors followed the paths and patterns of the linguistic signifier, paying new attention to syntax, spacing, intertextuality, sound, semantics, etymology, and even individual letters. The theoretical styles of Jacques Derrida, Julia Kristeva, Maurice Blanchot, and especially Jacques Lacan also owe a great deal to Mallarmé's 'critical poem.'"
Un Coup de Dés.
It has been suggested that much of Mallarmé's work influenced the conception of hypertext, with his purposeful use of blank space and careful placement of words on the page, allowing multiple non-linear readings of the text. This becomes very apparent in his work "Un coup de dés".
On the publishing of "Un Coup de Dés" and its mishaps after the death of Mallarmé, consult the notes and commentary of Bertrand Marchal for his edition of the complete works of Mallarmé, Volume 1, Bibliothèque de la Pléiade, Gallimard 1998. To delve more deeply, consult "Igitur, Divagations, Un Coup de Dés," edited by Bertrand Marchal with a preface by Yves Bonnefoy, nfr Poésie/Gallimard.
Prior to 2004, "Un Coup de Dés" was never published in the typography and format conceived by Mallarmé. In 2004, 90 copies on vellum of a new edition were published by Michel Pierson et Ptyx. This edition reconstructs the typography originally designed by Mallarmé for the projected Vollard edition in 1897 and which was abandoned after the sudden death of the author in 1898. All the pages are printed in the format (38 cm by 28 cm) and in the typography chosen by the author. The reconstruction has been made from the proofs which are kept in the Bibliothèque Nationale de France, taking into account the written corrections and wishes of Mallarmé and correcting certain errors on the part of the printers Firmin-Didot.
A copy of this new edition can be consulted in the Bibliothèque François-Mitterrand. Copies have been acquired by the Bibliothèque littéraire Jacques-Doucet and University of California - Irvine, as well as by private collectors. A copy has been placed in the Museum Stéphane Mallarmé at Vulaines-sur-Seine, Valvins, where Mallarmé lived and died and where, according to Paul Valéry, he made his final corrections on the proofs prior to the projected printing of the poem.
The poet and visual artist Marcel Broodthaers created a purely graphical version of "Un coup de Dés", using Mallarmé's typographical layout but with the words replaced by black bars.
In 2015, published "A Roll of the Dice Will Never Abolish Chance", a definitive dual-language edition of the poem, translated by Robert Bononno and Jeff Clark (designer).
References and sources.
Hendrik Lücke: Mallarmé - Debussy. Eine vergleichende Studie zur Kunstanschauung am Beispiel von „L'Après-midi d'un Faune“. (= Studien zur Musikwissenschaft, Bd. 4). Dr. Kovac, Hamburg 2005, ISBN 3-8300-1685-9.

</doc>
<doc id="28833" url="http://en.wikipedia.org/wiki?curid=28833" title="Sir Gawain and the Green Knight">
Sir Gawain and the Green Knight

Sir Gawain and the Green Knight (Middle English: "Sir Gawayn and þe Grene Knyȝt") is a late 14th-century Middle English chivalric romance. It is one of the best known Arthurian stories, and is of a type known as the "beheading game". The Green Knight is interpreted by some as a representation of the Green Man of folklore and by others as an allusion to Christ. Written in stanzas of alliterative verse, each of which ends in a rhyming bob and wheel, it draws on Welsh, Irish and English stories, as well as the French chivalric tradition. It is an important poem in the romance genre, which typically involves a hero who goes on a quest which tests his prowess, and it remains popular to this day in modern English renderings from J. R. R. Tolkien, Simon Armitage and others, as well as through film and stage adaptations.
It describes how Sir Gawain, a knight of King Arthur's Round Table, accepts a challenge from a mysterious "Green Knight" who challenges any knight to strike him with his axe if he will take a return blow in a year and a day. Gawain accepts and beheads him with his blow, at which the Green Knight stands up, picks up his head and reminds Gawain of the appointed time. In his struggles to keep his bargain Gawain demonstrates chivalry and loyalty until his honour is called into question by a test involving Lady Bertilak, the lady of the Green Knight's castle.
The poem survives in a single manuscript, the "Cotton Nero A.x.", which also includes three religious narrative poems: "Pearl", "Purity" and "Patience". All are thought to have been written by the same unknown author, possibly Cameron of Sutherland, dubbed the "Pearl Poet" or "Gawain Poet", since all four are written in a North West Midland dialect of Middle English.
Synopsis.
In Camelot on New Year's Day, King Arthur's court is exchanging gifts and waiting for the feasting to start when the king asks first to see or hear of an exciting adventure. At this a gigantic figure, entirely green in appearance and riding a green horse, rides unexpectedly into the hall. He wears no armour but bears an axe in one hand and a holly bough in the other. Refusing to fight anyone there on the grounds that they are all too weak to take him on, he insists he has come for a friendly "Christmas game": someone is to strike him once with his axe on condition that the Green Knight may return the blow in a year and a day. The splendid axe will belong to whoever takes him on. Arthur himself is prepared to accept the challenge when it appears no other knight will dare, but Sir Gawain, youngest of Arthur's knights and his nephew, begs for the honour instead. The giant bends and bares his neck before him and Gawain neatly beheads him in one stroke. However, the Green Knight neither falls nor falters, but instead reaches out, picks up his severed head and remounts, holding up his bleeding head to Queen Guinevere while its writhing lips remind Gawain that the two must meet again at the Green Chapel. He then rides away. Gawain and Arthur admire the axe, hang it up as a trophy and encourage Guinevere to treat the whole matter lightly.
As the date approaches, Sir Gawain sets off to find the Green Chapel and keep his side of the bargain. Many adventures and battles are alluded to (but not described) until Gawain comes across a splendid castle where he meets Bertilak de Hautdesert, the lord of the castle, and his beautiful wife, who are pleased to have such a renowned guest. Also present is an old and ugly lady, unnamed but treated with great honour by all. Gawain tells them of his New Year's appointment at the Green Chapel and that he only has a few days remaining. Bertilak laughs, explains that the Green Chapel is less than two miles away and proposes that Gawain rest at the castle till then. Relieved and grateful, Gawain agrees.
Before going hunting the next day Bertilak proposes a bargain: he will give Gawain whatever he catches on the condition that Gawain give him whatever he might gain during the day. Gawain accepts. After Bertilak leaves, Lady Bertilak visits Gawain's bedroom and behaves seductively, but despite her best efforts he yields nothing but a single kiss in his unwillingness to offend her. When Bertilak returns and gives Gawain the deer he has killed, his guest gives a kiss to Bertilak without divulging its source. The next day the lady comes again, Gawain again courteously foils her advances, and later that day there is a similar exchange of a hunted boar for two kisses. She comes once more on the third morning, this time offering Gawain a gold ring as a keepsake. He gently but steadfastly refuses but she pleads that he at least take her belt, a girdle of green and gold silk which, the lady assures him, is charmed and will keep him from all physical harm. Tempted, as he may otherwise die the next day, Gawain accepts it, and they exchange three kisses. That evening, Bertilak returns with a fox, which he exchanges with Gawain for the three kisses – but Gawain says nothing of the girdle.
The next day, Gawain leaves for the Green Chapel with the girdle wound twice around his waist. He finds the Green Knight sharpening an axe and, as promised, Gawain bends his bared neck to receive his blow. At the first swing Gawain flinches slightly and the Green Knight belittles him for it. Ashamed of himself, at the Green Knight's next swing Gawain does not flinch; but again the full force of the blow is withheld. The knight explains he was testing Gawain's nerve. Angrily Gawain tells him to deliver his blow and so the knight does, causing only a slight wound on Gawain's neck. The game is over. Gawain seizes his sword, helmet and shield, but the Green Knight, laughing, reveals himself to be the lord of the castle, Bertilak de Hautdesert, transformed by magic. He explains that the entire adventure was a trick of the 'elderly lady' Gawain saw at the castle, who is actually the sorceress Morgan le Fay, Arthur's sister, who intended to test Arthur's knights and frighten Guinevere to death. Gawain is ashamed to have behaved deceitfully but the Green Knight laughs at his scruples and the two part on cordial terms. Gawain returns to Camelot wearing the girdle as a token of his failure to keep his promise. The Knights of the Round Table absolve him of blame and decide that henceforth that they will wear a green sash in recognition of Gawain's adventure.
"Pearl Poet".
Though the real name of "The "Gawain" Poet" (or poets) is unknown, some inferences about him or her can be drawn from an informed reading of his or her works. The manuscript of "Gawain" is known in academic circles as "Cotton Nero A.x.", following a naming system used by one of its owners, Robert Cotton, a collector of Medieval English texts. Before the "Gawain" manuscript came into Cotton's possession, it was in the library of Henry Savile of Bank in Yorkshire. Little is known about its previous ownership, and until 1824, when the manuscript was introduced to the academic community in a second edition of Thomas Warton's "History" edited by Richard Price, it was almost entirely unknown. Even then, the "Gawain" poem was not published in its entirety until 1839. Now held in the British Library, it has been dated to the late 14th century, meaning the poet was a contemporary of Geoffrey Chaucer, author of "The Canterbury Tales", though it is unlikely that they ever met. The three other works found in the same manuscript as "Gawain" (commonly known as "Pearl", "Patience", and "Purity" or "Cleanliness") are often considered to be written by the same author. However, the manuscript containing these poems was transcribed by a copyist and not by the original poet. Although nothing explicitly suggests that all four poems are by the same poet, comparative analysis of dialect, verse form, and diction have pointed towards single authorship.
What is known today about the poet is largely general. As J. R. R. Tolkien and E. V. Gordon, after reviewing the text's allusions, style, and themes, concluded in 1925:
He was a man of serious and devout mind, though not without humour; he had an interest in theology, and some knowledge of it, though an amateur knowledge perhaps, rather than a professional; he had Latin and French and was well enough read in French books, both romantic and instructive; but his home was in the West Midlands of England; so much his language shows, and his metre, and his scenery.
The most commonly suggested candidate for authorship is John Massey of Cotton, Cheshire. He is known to have lived in the dialect region of the Pearl Poet and is thought to have written the poem "St. Erkenwald", which some scholars argue bears stylistic similarities to "Gawain". "St. Erkenwald", however, has been dated by some scholars to a time outside the Gawain poet's era. Thus, ascribing authorship to John Massey is still controversial and most critics consider the Gawain poet an unknown.
Verse form.
The 2,530 lines and 101 stanzas that make up "Sir Gawain and the Green Knight" are written in what linguists call the "Alliterative Revival" style typical of the 14th century. Instead of focusing on a metrical syllabic count and rhyme, the alliterative form of this period usually relied on the agreement of a pair of stressed syllables at the beginning of the line and another pair at the end. Each line always includes a pause, called a "caesura", at some point after the first two stresses, dividing it into two half-lines. Although he largely follows the form of his day, the Gawain poet was somewhat freer with convention than his or her predecessors. The poet broke the alliterative lines into variable-length groups and ended these nominal stanzas with a rhyming section of five lines known as the "bob and wheel", in which the "bob" is a very short line, sometimes of only two syllables, followed by the "wheel," longer lines with internal rhyme.
Similar stories.
The earliest known story to feature a beheading game is the 8th-century Middle Irish tale "Bricriu's Feast". This story parallels "Gawain" in that, like the Green Knight, Cú Chulainn's antagonist feints three blows with the axe before letting his target depart without injury. A beheading exchange also appears in the late 12th-century "Life of Caradoc", a Middle French narrative embedded in the anonymous First Continuation of Chrétien de Troyes' "Perceval, the Story of the Grail". A notable difference in this story is that Caradoc's challenger is his father in disguise, come to test his honour. Lancelot is given a beheading challenge in the early 13th-century "Perlesvaus", in which a knight begs him to chop off his head or else put his own in jeopardy. Lancelot reluctantly cuts it off, agreeing to come to the same place in a year to put his head in the same danger. When Lancelot arrives, the people of the town celebrate and announce that they have finally found a true knight, because many others had failed this test of chivalry.
The stories "The Girl with the Mule" (alternately titled "The Mule Without a Bridle") and "Hunbaut" feature Gawain in beheading game situations. In "Hunbaut," Gawain cuts off a man's head and, before he can replace it, removes the magic cloak keeping the man alive, thus killing him. Several stories tell of knights who struggle to stave off the advances of voluptuous women sent by their lords as a test; these stories include "Yder", the Lancelot-Grail, "Hunbaut", and "The Knight of the Sword". The last two involve Gawain specifically. Usually the temptress is the daughter or wife of a lord to whom the knight owes respect, and the knight is tested to see whether or not he will remain chaste in trying circumstances.
In the first branch of the medieval Welsh collection of tales known as the "Mabinogion", Pwyll exchanges places for a year with Arawn, the lord of Annwn (the Otherworld). Despite having his appearance changed to resemble Arawn exactly, Pwyll does not have sexual relations with Arawn's wife during this time, thus establishing a lasting friendship between the two men. This story may, then, provide a background Gawain's attempts to resist to the wife of the Green Knight; thus, the story of Sir Gawain and the Green Knight may be seen as a tale which combines elements of the Celtic beheading game and seduction test stories. Additionally, in both stories a year passes before the completion of the conclusion of the challenge or exchange is complete. Some scholars disagree with this interpretation, however, as Arawn seems to have accepted the notion that Pwyll may reciprocate with his wife, making it less of a "seduction test" per se, as seduction tests typically involve a Lord and Lady conspiring to seduce a knight, seemingly "against" the wishes of the Lord.
After the writing of "Sir Gawain and the Green Knight", several similar stories followed. "The Greene Knight" (15th–17th century) is a rhymed retelling of nearly the same tale. In it, the plot is simplified, motives are more fully explained, and some names are changed. Another story, "The Turke and Gowin" (15th century), begins with a Turk entering Arthur's court and asking, "Is there any will, as a brother, To give a buffett and take another?" At the end of this poem the Turk, rather than buffeting Gawain back, asks the knight to cut off his head, which Gawain does. The Turk then praises Gawain and showers him with gifts. "The Carle of Carlisle" (17th century) also resembles "Gawain" in a scene in which the Carle (Churl), a lord, takes Sir Gawain to a chamber where two swords are hanging and orders Gawain to cut off his head or suffer his own to be cut off. Gawain obliges and strikes, but the Carle rises, laughing and unharmed. Unlike the "Gawain" poem, no return blow is demanded or given. Other oral versions of the story suggest that Sir Gawain was the Green Knight, with more characters introduced such as Chief Eagle Eye.
Themes.
Temptation and testing.
At the heart of "Sir Gawain and the Green Knight" is the test of Gawain's adherence to the code of chivalry. The typical temptation fable of medieval literature presents a series of tribulations assembled as tests or "proofs" of moral virtue. The stories often describe several individuals' failures after which the main character is tested. Success in the proofs will often bring immunity or good fortune. Gawain's ability to pass the tests of his host are of utmost importance to his survival, though he does not know it. It is only by fortuity or "instinctive-courtesy" that Sir Gawain is able to pass his test.
In addition to the laws of chivalry, Gawain must respect another set of laws concerning courtly love. The knight's code of honour requires him to do whatever a damsel asks. Gawain must accept the girdle from the Lady, but he must also keep the promise he has made to his host that he will give whatever he gains that day. Gawain chooses to keep the girdle out of fear of death, thus breaking his promise to the host but honouring the lady. Upon learning that the Green Knight is actually his host (Bertilak), he realises that although he has completed his quest, he has failed to be virtuous. This test demonstrates the conflict between honour and knightly duties. In breaking his promise, Gawain believes he has lost his honour and failed in his duties.
Hunting and seduction.
Scholars have frequently noted the parallels between the three hunting scenes and the three seduction scenes in "Gawain". They are generally agreed that the fox chase has significant parallels to the third seduction scene, in which Gawain accepts the girdle from Bertilak's wife. Gawain, like the fox, fears for his life and is looking for a way to avoid death from the Green Knight's axe. Like his counterpart, he resorts to trickery in order to save his skin. The fox uses tactics so unlike the first two animals, and so unexpectedly, that Bertilak has the hardest time hunting it. Similarly, Gawain finds the Lady's advances in the third seduction scene more unpredictable and challenging to resist than her previous attempts. She changes her evasive language, typical of courtly love relationships, to a more assertive style. Her dress, relatively modest in earlier scenes, is suddenly voluptuous and revealing.
The deer- and boar-hunting scenes are less clearly connected, although scholars have attempted to link each animal to Gawain's reactions in the parallel seduction scene. Attempts to connect the deer hunt with the first seduction scene have unearthed a few parallels. Deer hunts of the time, like courtship, had to be done according to established rules. Women often favoured suitors who hunted well and skinned their animals, sometimes even watching while a deer was cleaned. The sequence describing the deer hunt is relatively unspecific and nonviolent, with an air of relaxation and exhilaration. The first seduction scene follows in a similar vein, with no overt physical advances and no apparent danger; the entire exchange is humorously portrayed.
The boar-hunting scene is, in contrast, laden with detail. Boars were (and are) much more difficult to hunt than deer; approaching one with only a sword was akin to challenging a knight to single combat. In the hunting sequence, the boar flees but is cornered before a ravine. He turns to face Bertilak with his back to the ravine, prepared to fight. Bertilak dismounts and in the ensuing fight kills the boar. He removes its head and displays it on a pike. In the seduction scene, Bertilak's wife, like the boar, is more forward, insisting that Gawain has a romantic reputation and that he must not disappoint her. Gawain, however, is successful in parrying her attacks, saying that surely she knows more than he about love. Both the boar hunt and the seduction scene can be seen as depictions of a moral victory: both Gawain and Bertilak face struggles alone and emerge triumphant.
Nature and chivalry.
Some argue that nature represents a chaotic, lawless order which is in direct confrontation with the civilisation of Camelot throughout "Sir Gawain and the Green Knight". The green horse and rider that first invade Arthur’s peaceful halls are iconic representations of nature's disturbance. Nature is presented throughout the poem as rough and indifferent, constantly threatening the order of men and courtly life. Nature invades and disrupts order in the major events of the narrative, both symbolically and through the inner nature of humanity. This element appears first with the disruption caused by the Green Knight, later when Gawain must fight off his natural lust for Bertilak’s wife, and again when Gawain breaks his vow to Bertilak by choosing to keep the green girdle, valuing survival over virtue. Represented by the sin-stained girdle, nature is an underlying force, forever within man and keeping him imperfect (in a chivalric sense). In this view, Gawain is part of a wider conflict between nature and chivalry, an examination of the ability of man's order to overcome the chaos of nature.
Several critics have made exactly the opposite interpretation, reading the poem as a comic critique of the Christianity of the time, particularly as embodied in the Christian chivalry of Arthur's court. In its zeal to extirpate all traces of paganism, Christianity had cut itself off from the sources of life in nature and the female. The green girdle represents all the pentangle lacks. The Arthurian enterprise is doomed unless it can acknowledge the unattainability of the ideals of the Round Table, and, for the sake of realism and wholeness, recognize and incorporate the pagan values represented by the Green Knight.
Games.
The word "gomen" (game) is found 18 times in "Gawain". Its similarity to the word "gome" (man), which appears 21 times, has led some scholars to see men and games as centrally linked. Games at this time were seen as tests of worthiness, as when the Green Knight challenges the court's right to its good name in a "Christmas game". The "game" of exchanging gifts was common in Germanic cultures. If a man received a gift, he was obliged to provide the giver with a better gift or risk losing his honour, almost like an exchange of blows in a fight (or in a "beheading game"). The poem revolves around two games: an exchange of beheading and an exchange of winnings. These appear at first to be unconnected. However, a victory in the first game will lead to a victory in the second. Elements of both games appear in other stories; however, the linkage of outcomes is unique to "Gawain".
Times and seasons.
Times, dates, seasons, and cycles within "Gawain" are often noted by scholars because of their symbolic nature. The story starts on New Year's Eve with a beheading and culminates on the next New Year's Day. Gawain leaves Camelot on All Saints Day and arrives at Bertilak's castle on Christmas Eve. Furthermore, the Green Knight tells Gawain to meet him at the Green Chapel in "a year and a day"—a period of time seen often in medieval literature. Some scholars interpret the yearly cycles, each beginning and ending in winter, as the poet's attempt to convey the inevitable fall of all things good and noble in the world. Such a theme is strengthened by the image of Troy, a powerful nation once thought to be invincible which, according to the "Aeneid", fell to the Greeks due to pride and ignorance. The Trojan connection shows itself in the presence of two virtually identical descriptions of Troy's destruction. The poem's first line reads: "Since the siege and the assault were ceased at Troy" and the final stanzaic line (before the bob and wheel) is "After the siege and the assault were ceased at Troy".
Symbolism.
Significance of the color green.
Given the varied and even contradictory interpretations of the color green, its precise meaning in the poem remains ambiguous. In English folklore and literature, green was traditionally used to symbolise nature and its associated attributes: fertility and rebirth. Stories of the medieval period also used it to allude to love and the base desires of man. Because of its connection with faeries and spirits in early English folklore, green also signified witchcraft, devilry and evil. It can also represent decay and toxicity. When combined with gold, as with the Green Knight and the girdle, green was often seen as representing youth's passing. In Celtic mythology, green was associated with misfortune and death, and therefore avoided in clothing. The green girdle, originally worn for protection, became a symbol of shame and cowardice; it is finally adopted as a symbol of honour by the knights of Camelot, signifying a transformation from good to evil and back again; this displays both the spoiling and regenerative connotations of the color green.
The Green Knight.
Scholars have puzzled over the Green Knight's symbolism since the discovery of the poem. He could be a version of the Green Man, a mythological being connected with nature in medieval art, a Christian symbol, or the Devil himself. British medieval scholar C. S. Lewis said the character was "as vivid and concrete as any image in literature" and J. R. R. Tolkien said he was the "most difficult character" to interpret in "Sir Gawain". His major role in Arthurian literature is that of a judge and tester of knights, thus he is at once terrifying, friendly, and mysterious. He appears in only two other poems: "The Greene Knight" and "King Arthur and King Cornwall". Scholars have attempted to connect him to other mythical characters, such as Jack in the green of English tradition and to Al-Khidr, but no definitive connection has yet been established.
However, there is a possibility, as Alice Buchanan has argued, that the color green is erroneously attributed to the Green Knight due to the poet's mistranslation or misunderstanding of the Irish word "glas", which could either mean grey or green. In the Death of Curoi (one of the Irish stories from Bricriu's Feast), Curoi stands in for Bercilak, and is often called "the man of the grey mantle". Though the words usually used for grey in the Death of Curoi are "lachtna" or "odar", roughly meaning milk-coloured and shadowy respectively, in later works featuring a green knight, the word "glas" is used and may have been the basis of misunderstanding.
Girdle.
The girdle's symbolic meaning, in "Sir Gawain and the Green Knight", has been construed in a variety of ways. Interpretations range from sexual in nature to spiritual. Those who argue for the sexual inference view the girdle as a "trophy". However, it is not entirely clear who the "winner" is: Sir Gawain or Lady Bertilak. The girdle is given to Gawain by Lady Bertilak in order to keep him safe when he confronts the Green Knight. When Lord Bertilak returns home from his hunting trip, Gawain does not reveal the girdle to his host but, instead, hides it. This introduces the spiritual interpretation, that Gawain’s acceptance of the girdle is a sign of his faltering faith in God, at least in the face of death. To some, the Green Knight is Christ, who overcomes death, while Gawain is the Every Christian, who in his struggles to follow Christ faithfully, chooses the easier path. In "Sir Gawain", the easier choice is the girdle, which promises what Gawain most desires. Faith in God, alternatively, requires one’s acceptance that what one most desires does not always coincide with what God has planned. It is arguably best to view the girdle not as an either–or situation, but as a complex, multi-faceted symbol that acts to test Gawain in more ways than one. While Gawain is able to resist Bertilak’s wife’s sexual advances, he is unable to resist the powers of the girdle. Gawain is operating under the laws of chivalry which, evidently, have rules that can contradict each other. In the story of "Sir Gawain", Gawain finds himself torn between doing what a damsel asks (accepting the girdle) and keeping his promise (returning anything given to him while his host is away). 
Pentangle.
The pentangle on Gawain's shield is seen by many critics as signifying Gawain's perfection and power over evil. The poem contains the only representation of such a symbol on Gawain's shield in the Gawain literature. What is more, the poet uses a total of 46 lines to describe the meaning of the pentangle; no other symbol in the poem receives as much attention or is described in such detail. The poem describes the pentangle as a symbol of faithfulness and an "endless knot". In line 625, it is described as "a sign by Solomon". Solomon, the third king of Israel, in the 10th century BC, was said to have the mark of the pentagram on his ring, which he received from the archangel Michael. The pentagram seal on this ring was said to give Solomon power over demons.
Along these lines, some academics link the Gawain pentangle to magical traditions. In Germany, the symbol was called a "Drudenfuß" and was placed on household objects to keep out evil. The symbol was also associated with magical charms that, if recited or written on a weapon, would call forth magical forces. However, concrete evidence tying the magical pentagram to Gawain's pentangle is scarce.
Gawain’s pentangle also symbolises the "phenomenon of physically endless objects signifying a temporally endless quality." Many poets use the symbol of the circle to show infinity or endlessness, but Gawain’s poet insisted on using something more complex. In medieval number theory, the number five is considered a "circular number", since it "reproduces itself in its last digit when raised to its powers". Furthermore, it replicates itself geometrically; that is, every pentangle has a smaller pentagon that allows a pentangle to be embedded in it and this "process may be repeated forever with decreasing pentangles". Thus, by reproducing the number five, which in medieval number symbolism signified incorruptibility, Gawain's pentangle represents his eternal incorruptibility.
The Lady's Ring.
Gawain's refusal of the Lady Bertilak's ring has major implications for the remainder of the story. While the modern student may tend to pay more attention to the girdle as the eminent object offered by the lady, readers in the time of Gawain would have noticed the significance of the offer of the ring as they believed that rings, and especially the embedded gems, had talismanic properties. This is especially true of the lady's ring as scholars believe it to be a ruby or carbuncle, indicated when the Gawain-Poet describes it as a "brygt sunne" (line 1819), a "fiery sun." Given the importance of magic rings in Arthurian romance, this remarkable ring would also have been believed to protect the wearer from harm.
Numbers.
The poet highlights number symbolism to add symmetry and meaning to the poem. For example, three kisses are exchanged between Gawain and Bertilak's wife; Gawain is tempted by her on three separate days; Bertilak goes hunting three times, and the Green Knight swings at Gawain three times with his axe. The number two also appears repeatedly, as in the two beheading scenes, two confession scenes, and two castles. The five points of the pentangle, the poet adds, represent Gawain's virtues, for he is "faithful five ways and five times each". The poet goes on to list the ways in which Gawain is virtuous: all five of his senses are without fault; his five fingers never fail him, and he always remembers the five wounds of Christ, as well as the five joys of the Virgin Mary. The fifth five is Gawain himself, who embodies the five moral virtues of the code of chivalry: "friendship, generosity, chastity, courtesy, and piety". All of these virtues reside, as the poet says, in the "Endless Knot" of the pentangle, which forever interlinks and is never broken. This intimate relationship between symbol and faith allows for rigorous allegorical interpretation, especially in the physical role that the shield plays in Gawain's quest. Thus, the poet makes Gawain the epitome of perfection in knighthood through number symbolism.
The number five is also found in the structure of the poem itself. "Sir Gawain" is 101 stanzas long, traditionally organised into four 'Fitts' of 21, 24, 34, and 22 stanzas. These divisions, however, have since been disputed; scholars have begun to believe that they are the work of the copyist and not of the poet. The original manuscript features a series of capital letters added after the fact by another scribe, and some scholars argue that these additions were an attempt to restore the original divisions. These letters divide the manuscript into nine parts. The first and last parts are 22 stanzas long. The second and second-to-last parts are only one stanza long, and the middle five parts are eleven stanzas long. The number eleven is associated with transgression in other medieval literature (being one more than ten, a number associated with the Ten Commandments). Thus, this set of five elevens (55 stanzas) creates the perfect mix of transgression and incorruption, suggesting that Gawain is faultless in his faults.
Wounds.
At the story's climax, Gawain is wounded superficially in the neck by the Green Knight's axe. During the medieval period, the body and the soul were believed to be so intimately connected that wounds were considered an outward sign of inward sin. The neck, specifically, was believed to correlate with the part of the soul related to will, connecting the reasoning part (the head) and the courageous part (the heart). Gawain's sin resulted from using his will to separate reasoning from courage. By accepting the girdle from the lady, he employs reason to do something less than courageous—evade death in a dishonest way. Gawain's wound is thus an outward sign of an internal wound. The Green Knight's series of tests shows Gawain the weakness that has been in him all along: the desire to use his will pridefully for personal gain, rather than submitting his will in humility to God. The Green Knight, by engaging with the greatest knight of Camelot, also reveals the moral weakness of pride in all of Camelot, and therefore all of humanity. However, the wounds of Christ, believed to offer healing to wounded souls and bodies, are mentioned throughout the poem in the hope that this sin of prideful "stiffneckedness" will be healed among fallen mortals.
Interpretations.
"Gawain" as medieval romance.
Many critics argue that "Sir Gawain and the Green Knight" should be viewed, above all, as a romance. Medieval romances typically recount the marvellous adventures of a chivalrous, heroic knight, often of super-human ability, who abides by chivalry's strict codes of honour and demeanour, embarks upon a quest and defeats monsters, thereby winning the favour of a lady. Thus, medieval romances focus not on love and sentiment (as the term "romance" implies today), but on adventure.
Gawain's function, as medieval scholar Alan Markman says, "is the function of the romance hero … to stand as the champion of the human race, and by submitting to strange and severe tests, to demonstrate human capabilities for good or bad action." Through Gawain's adventure, it becomes clear that he is merely human. The reader becomes attached to this human view in the midst of the poem's romanticism, relating to Gawain's humanity while respecting his knightly qualities. Gawain "shows us what moral conduct is. We shall probably not equal his behaviour, but we admire him for pointing out the way."
In viewing the poem as a chivalric romance, many scholars see it as intertwining chivalric and courtly love laws under the English Order of the Garter. The group's motto, 'honi soit qui mal y pense', or "Shamed be he who finds evil here," is written at the end of the poem. Some critics describe Gawain's peers wearing girdles of their own as evidence of the origin of the Order of the Garter. However, in the parallel poem "The Greene Knight", the lace is white, not green, and is considered the origin of the collar worn by the knights of the Bath, not the Order of the Garter. The motto on the poem was probably written by a copyist and not by the original author. Still, the connection made by the copyist to the Order is not extraordinary.
Christian interpretations.
The poem is in many ways deeply Christian, with frequent references to the fall of Adam and Eve and to Jesus Christ. Scholars have debated the depth of the Christian elements within the poem by looking at it in the context of the age in which it was written, coming up with varying views as to what represents a Christian element of the poem and what does not. For example, some critics compare "Sir Gawain" to the other three poems of the "Gawain" manuscript. Each has a heavily Christian theme, causing scholars to interpret "Gawain" similarly. Comparing it to the poem "Cleanness" (also known as "Purity"), for example, they see it as a story of the apocalyptic fall of a civilisation, in "Gawain's" case, Camelot. In this interpretation, Sir Gawain is like Noah, separated from his society and warned by the Green Knight (who is seen as God's representative) of the coming doom of Camelot. Gawain, judged worthy through his test, is spared the doom of the rest of Camelot. King Arthur and his knights, however, misunderstand Gawain's experience and wear garters themselves. In "Cleanness" the men who are saved are similarly helpless in warning their society of impending destruction.
One of the key points stressed in this interpretation is that salvation is an individual experience difficult to communicate to outsiders. In his depiction of Camelot, the poet reveals a concern for his society, whose inevitable fall will bring about the ultimate destruction intended by God. "Gawain" was written around the time of the Black Death and Peasants' Revolt, events which convinced many people that their world was coming to an apocalyptic end and this belief was reflected in literature and culture. However, other critics see weaknesses in this view, since the Green Knight is ultimately under the control of Morgan le Fay, usually viewed as a figure of evil in Camelot tales. This makes the knight's presence as a representative of God problematic.
While the character of the Green Knight is usually not viewed as a representation of Christ in "Sir Gawain and the Green Knight", critics do acknowledge a parallel. Lawrence Besserman, a specialist in medieval literature, explains that "the Green Knight is not a figurative representative of Christ. But the idea of Christ's divine/human nature provides a medieval conceptual framework that supports the poet's serious/comic account of the Green Knight's supernatural/human qualities and actions." This duality exemplifies the influence and importance of Christian teachings and views of Christ in the era of the Gawain Poet.
Furthermore, critics note the Christian reference to Christ's crown of thorns at the conclusion of "Sir Gawain and the Green Knight". After Gawain returns to Camelot and tells his story regarding the newly acquired green sash, the poem concludes with a brief prayer and a reference to "the thorn-crowned God". Besserman theorises that "with these final words the poet redirects our attention from the circular girdle-turned-sash (a double image of Gawain's "yntrawpe/renoun") to the circular Crown of Thorns (a double image of Christ's humiliation turned triumph)."
Throughout the poem, Gawain encounters numerous trials testing his devotion and faith in Christianity. When Gawain sets out on his journey to find the Green Chapel, he finds himself lost, and only after praying to the Virgin Mary does he find his way. As he continues his journey, Gawain once again faces anguish regarding his inevitable encounter with the Green Knight. Instead of praying to Mary, as before, Gawain places his faith in the girdle given to him by Bertilak's wife. From the Christian perspective, this leads to disastrous and embarrassing consequences for Gawain as he is forced to reevaluate his faith when the Green Knight points out his betrayal.
An analogy is also made between Gawain's trial and the Biblical test that Adam encounters in the Garden of Eden. Adam succumbs to Eve just as Gawain surrenders to Bertilak's wife by accepting the girdle. Although Gawain sins by putting his faith in the girdle and not confessing when he is caught, the Green Knight pardons him, thereby allowing him to become a better Christian by learning from his mistakes. Through the various games played and hardships endured, Gawain finds his place within the Christian world.
Feminist interpretations.
Feminist literary critics see the poem as portraying women's ultimate power over men. Morgan le Fay and Bertilak's wife, for example, are the most powerful characters in the poem—Morgan especially, as she begins the game by enchanting the Green Knight. The girdle and Gawain's scar can be seen as symbols of feminine power, each of them diminishing Gawain's masculinity. Gawain's misogynist passage, in which he blames all of his troubles on women and lists the many men who have fallen prey to women's wiles, further supports the feminist view of ultimate female power in the poem.
In contrast, others argue that the poem focuses mostly on the opinions, actions, and abilities of men. For example, on the surface, it appears that Bertilak's wife is a strong leading character. By adopting the masculine role, she appears to be an empowered individual, particularly in the bedroom scene. This is not entirely the case, however. While the Lady is being forward and outgoing, Gawain's feelings and emotions are the focus of the story, and Gawain stands to gain or lose the most. The Lady "makes the first move", so to speak, but Gawain ultimately decides what is to become of those actions. He, therefore, is in charge of the situation and even the relationship.
In the bedroom scene, both the negative and positive actions of the Lady are motivated by her desire. Her feelings cause her to step out of the typical female role and into that of the male, thus becoming more empowered. At the same time, those same actions make the Lady appear adulterous; some scholars compare her with Eve in the Bible. By forcing Gawain to take her girdle, i.e. the apple, the pact made with Bertilak—and therefore the Green Knight—is broken. In this sense, it is clear that at the hands of the Lady, Gawain is a "good man seduced".
Postcolonial interpretations.
From 1350 to 1400—the period in which the poem is thought to have been written—Wales experienced several raids at the hands of the English, who were attempting to colonise the area. The Gawain poet uses a North West Midlands dialect common on the Welsh–English border, potentially placing him in the midst of this conflict. Patricia Clare Ingham is credited with first viewing the poem through the lens of postcolonialism, and since then a great deal of dispute has emerged over the extent to which colonial differences play a role in the poem. Most critics agree that gender plays a role, but differ about whether gender supports the colonial ideals or replaces them as English and Welsh cultures interact in the poem.
A large amount of critical debate also surrounds the poem as it relates to the bi-cultural political landscape of the time. Some argue that Bertilak is an example of the hybrid Anglo-Welsh culture found on the Welsh–English border. They therefore view the poem as a reflection of a hybrid culture that plays strong cultures off one another to create a new set of cultural rules and traditions. Other scholars, however, argue that historically much Welsh blood was shed well into the 14th century, creating a situation far removed from the more friendly hybridisation suggested by Ingham. To support this argument further, it is suggested that the poem creates an "us versus them" scenario contrasting the knowledgeable civilised English with the uncivilised borderlands that are home to Bertilak and the other monsters that Gawain encounters.
In contrast to this perception of the colonial lands, others argue that the land of Hautdesert, Bertilak’s territory, has been misrepresented or ignored in modern criticism. They suggest that it is a land with its own moral agency, one that plays a central role in the story. Bonnie Lander, for example, argues that the denizens of Hautdesert are "intelligently immoral", choosing to follow certain codes and rejecting others, a position which creates a "distinction … of moral insight versus moral faith". Lander thinks that the border dwellers are more sophisticated because they do not unthinkingly embrace the chivalric codes but challenge them in a philosophical, and—in the case of Bertilak's appearance at Arthur’s court—literal sense. Lander’s argument about the superiority of the denizens of Hautdesert hinges on the lack of self-awareness present in Camelot, which leads to an unthinking populace that frowns on individualism. In this view, it is not Bertilak and his people, but Arthur and his court, who are the monsters.
Gawain's journey.
Several scholars have attempted to find a real-world correspondence for Gawain's journey to the Green Chapel. The Anglesey islands, for example, are mentioned in the poem. They exist today as a single island off the coast of Wales. In line 700, Gawain is said to pass the "Holy Head", believed by many scholars to be either Holywell or the Cistercian abbey of Poulton in Pulford. Holywell is associated with the beheading of Saint Winifred. As the story goes, Winifred was a virgin who was beheaded by a local leader after she refused his sexual advances. Her uncle, another saint, put her head back in place and healed the wound, leaving only a white scar. The parallels between this story and Gawain's make this area a likely candidate for the journey.
Gawain's trek leads him directly into the centre of the Pearl Poet's dialect region, where the candidates for the locations of the Castle at Hautdesert and the Green Chapel stand. Hautdesert is thought to be in the area of Swythamley in northwest Midland, as it lies in the writer's dialect area and matches the topographical features described in the poem. The area is also known to have housed all of the animals hunted by Bertilak (deer, boar, fox) in the 14th century. The Green Chapel is thought to be in either Lud's Church or Wetton Mill, as these areas closely match the descriptions given by the author. Ralph Elliott located the chapel ("two myle henne" v1078) from the old manor house at Swythamley Park at the bottom of a valley ("bothm of the brem valay" v2145) on a hillside ("loke a littel on the launde, on thi lyfte honde" v2147) in an enormous fissure ("an olde caue,/or a creuisse of an olde cragge" v2182-83).
Homoerotic Interpretations.
According to medieval scholar Richard Zeikowitz, the Green Knight represents a threat to homosocial friendship in his medieval world. Zeikowitz argues that the narrator of the poem seems entranced by the Knight's beauty, homoeroticising him in poetic form. The Green Knight's attractiveness challenges the homosocial rules of King Arthur's court and poses a threat to their way of life. Zeikowitz also states that Gawain seems to find Bertilak as attractive as the narrator finds the Green Knight. Bertilak, however, follows the homosocial code and develops a friendship with Gawain. Gawain's embracing and kissing Bertilak in several scenes thus represents not a homosexual but a homosocial expression. Men of the time often embraced and kissed and this was acceptable under the chivalric code. Nonetheless, the Green Knight blurs the lines between homosociality and homosexuality, representing the difficulty medieval writers sometimes had in separating the two.
Carolyn Dinshaw argues that the poem may have been a response to accusations that Richard II had a male lover—an attempt to reestablish the idea that heterosexuality was the Christian norm. Around the time the poem was written, the Catholic Church was beginning to express concerns about kissing between males. Many religious figures were trying to make the distinction between strong trust and friendship between males and homosexuality. Still, the Pearl Poet seems to have been simultaneously entranced and repulsed by homosexual desire. In his other poem "Cleanness", he points out several grievous sins, but spends lengthy passages describing them in minute detail. His obsession seems to carry into "Gawain" in his descriptions of the Green Knight.
Beyond this, Dinshaw proposes that Gawain can be read as a woman-like figure. He is the passive one in the advances of Lady Bertilak, as well as in his encounters with Lord Bertilak, where he acts the part of a woman in kissing the man. However, while the poem does have homosexual elements, these elements are brought up by the poet in order to establish heterosexuality as the normal lifestyle of Gawain's world. The poem does this by making the kisses between Lady Bertilak and Gawain sexual in nature, but rendering the kisses between Gawain and Lord Bertilak "unintelligible" to the medieval reader. In other words, the poet portrays kisses between a man and a woman as having the possibility of leading to sex, while in a heterosexual world kisses between a man and a man are portrayed as having no such possibility.
Modern adaptations.
Books.
Though the surviving manuscript dates from the fourteenth century, the first published version of the poem did not appear until as late as 1839, when Sir Frederic Madden of the British Museum recognized the poem as worth reading. Madden's scholarly, Middle English edition of the poem was followed in 1898 by the first Modern English translation – a prose version by literary scholar Jessie L. Weston. In 1925, J.R.R. Tolkien and E.V. Gordon published a scholarly edition of the Middle English text of "Sir Gawain and the Green Knight"; a revised edition of this text was prepared by Norman Davis and published in 1967. The book, featuring a text in Middle English with extensive scholarly notes, is frequently confused with the translation into Modern English that Tolkien prepared, along with translations of "Pearl" and "Sir Orfeo", late in his life. Many editions of the latter work, first published in 1975, shortly after his death, list Tolkien on the cover as author rather than translator.
For students, especially undergraduate students, the text is usually given in translation. Notable translators include Jessie Weston, whose 1898 prose translation and 1907 poetic translation took many liberties with the original; Theodore Banks, whose 1929 translation was praised for its adaptation of the language to modern usage; and Marie Borroff, whose imitative translation was first published in 1967 and "entered the academic canon" in 1968, in the second edition of the "Norton Anthology of English Literature". In 2010, her (slightly revised) translation was published as a Norton Critical Edition, with a foreword by Laura Howes. In 2007, Simon Armitage, who grew up near the Gawain poet's purported residence, published a translation which attracted attention in the US and the United Kingdom, and was published in the United States by Norton, which replaced Borroff's translation with Armitage's for the ninth edition of the "Norton Anthology of English Literature". Other modern translations include those by Brian Stone, James Winny, Helen Cooper, W. S. Merwin, Jacob Rosenberg, William Vantuono, Joseph Glaser, Bernard O'Donoghue, John Gardner, and Francis Ingledew.
In 2014, Zach Weiner published a children's book called "Augie and the Green Knight" that is a . Though the protagonist of the book is a young, modern-day girl, the events of the book largely follow the Arthurian legend. In 1997, Gerald Morris published a version of the story called The Squire, His Knight, and His Lady.
Film and television.
The poem has been adapted to film twice, on both occasions by writer-director Stephen Weeks: first as "Gawain and the Green Knight" in 1973 and again in 1984 as "", featuring Miles O'Keeffe as Gawain and Sean Connery as the Green Knight. Both films have been criticised for deviating from the plot. Gawain, for example, has an adventure in the 1973 version which is not a part of the poem between the time he leaves Camelot and the time he arrives at Bertilak's castle, in which he travels through New Earth to find his parents. Also, Bertilak and the Green Knight are never connected. French/Australian director Martin Beilby directed a short (30') film adaptation in 2014. There have been at least two television adaptations, "Gawain and the Green Knight" in 1991 and the animated "Sir Gawain and the Green Knight" in 2002. The BBC broadcast a documentary presented by Simon Armitage in which the journey depicted in the poem is traced, utilising what are believed to be the actual locations.
Theatre.
The Tyneside Theatre company presented a stage version of "Sir Gawain and the Green Knight" at the University Theatre, Newcastle at Christmas 1971. It was directed by Michael Bogdanov and adapted for the stage from the translation by Brian Stone. The music and lyrics were composed by Iwan Williams using medieval carols, such as the "Boar's Head Carol", as inspiration and folk instruments such as the Northumbrian pipes, whistles and bhodran to create a "rough" feel. Stone had referred Bogdanov to "Cuchulain and the Beheading Game", a sequence which is contained in The Grenoside Sword dance. Bogdanov found the pentangle theme to be contained in most sword dances, and so incorporated a long sword dance while Gawain lay tossing uneasily before getting up to go to the Green Chapel. The dancers made the knot of the pentangle around his drowsing head with their swords. The interlacing of the hunting and wooing scenes was achieved by frequent cutting of the action from hunt to bed-chamber and back again, while the locale of both remained on-stage.
In 1992 Simon Corble created an adaptation with medieval songs and music for The Midsommer Actors' Company. performed as walkabout productions in the summer 1992 at Thurstaston Common and Beeston Castle and in August 1995 at Brimham Rocks, North Yorkshire. Corble later wrote a substantially revised version which was produced indoors at the O'Reilly Theatre, Oxford in February 2014.
Opera.
"Sir Gawain and the Green Knight" was first adapted as an opera in 1978 by the composer Richard Blackford on commission from the village of Blewbury, Oxfordshire. The libretto was written for the adaptation by the children's novelist John Emlyn Edwards. The "Opera in Six Scenes" was subsequently recorded by Decca between March and June 1979 and released on the Argo label in November 1979.
"Sir Gawain and the Green Knight" was adapted into an opera called "Gawain" by Harrison Birtwistle, first performed in 1991. Birtwistle's opera was praised for maintaining the complexity of the poem while translating it into lyric, musical form. Another operatic adaptation is Lynne Plowman's "Gwyneth and the Green Knight", first performed in 2002. This opera uses "Sir Gawain" as the backdrop but refocuses the story on Gawain's female squire, Gwyneth, who is trying to become a knight. Plowman's version was praised for its approachability, as its target is the family audience and young children, but criticised for its use of modern language and occasional preachy nature.

</doc>
<doc id="28848" url="http://en.wikipedia.org/wiki?curid=28848" title="Scabies">
Scabies

Scabies, known as the seven-year itch, is a contagious skin infestation. When first infected, usually two to six weeks are required before symptoms occur. The most common symptoms are severe itchiness and a rash. This may affect most of the body or just certain areas such as between the fingers, at the wrist, and at the waist. While the head may be affected in young children it is typically not affected in older children or adults. The itch is often worse at night. The rash typically looks like pimples. Occasionally tiny burrows may be seen on the skin. If a person develops a second infection symptoms may start within a day. Scratching may cause skin breakdown and an additional bacterial infection of the skin.
Scabies is caused by infection with the mite "Sarcoptes scabiei". Often only between ten and fifteen mites are involved. It is most often spread during a relatively long period of direct skin contact with an infected person such as may occur during sex. Spreads of disease may occur even if the person has not developed symptoms yet. Crowded living conditions such as found in child care facilities, group homes, and prisons increase the risk of spread. Areas with a lack of access to water also have higher rates of disease. Crusted scabies is a more severe form of disease. It typically only occurs in those with a poor immune system and people may have millions of mites making them much more contagious. In these cases spread of infection may occur during brief contact or via contaminated objects. The mite is very small and usually not directly visible. The female burrows into a person's skin where they live and lay eggs. Diagnosis is based on the signs and symptoms.
A number of medications are available to treat those infected including: permethrin, crotamiton and lindane creams and ivermectin pills. Sexual contacts within the last month and people who live in the same house should also be treated at the same time. Bedding and clothing used in the last three days should be washed in hot water and dried in a hot dryer. As the mite does not live for more than three days away from human skin more washing is not needed. Symptoms may continue for two to four weeks following treatment. If after this time there continues to be symptoms retreatment may be needed.
Scabies is one of the three most common skin disorders in children, along with ringworm and bacterial skin infections. As of 2010 it affects approximately 100 million people (1.5% of the world population) and is equally common in both sexes. The young and the old are more commonly affected. It also occurs more commonly in the developing world and tropical climates. The word scabies is from Latin: "scabere", "to scratch". Other animals do not spread human scabies. Infections in other animals are typically caused by slightly different but related mites and is known as sarcoptic mange.
Signs and symptoms.
The characteristic symptoms of a scabies infection include intense itching and superficial burrows. The burrow tracks are often linear, to the point that a neat "line" of four or more closely placed and equally developed mosquito-like "bites" is almost diagnostic of the disease.
Itching.
In the classic scenario, the itch is made worse by warmth, and is usually experienced as being worse at night, possibly because there are fewer distractions. As a symptom, it is less common in the elderly.
Rash.
The superficial burrows of scabies usually occur in the area of the finger webs, feet, ventral wrists, elbows, back, buttocks, and external genitals. Except in infants and the immunosuppressed, infection generally does not occur in the skin of the face or scalp. The burrows are created by excavation of the adult mite in the epidermis.
In most people, the trails of the burrowing mites are linear or "s"-shaped tracks in the skin often accompanied by rows of small, pimple-like mosquito or insect bites. These signs are often found in crevices of the body, such as on the webs of fingers and toes, around the genital area, in stomach folds of the skin, and under the breasts of women.
Symptoms typically appear two to six weeks after infestation for individuals never before exposed to scabies. For those having been previously exposed, the symptoms can appear within several days after infestation. However, it is not unknown for symptoms to appear after several months or years. Acropustulosis, or blisters and pustules on the palms and soles of the feet, are characteristic symptoms of scabies in infants.
Crusted scabies.
The elderly and people with an impaired immune system, such as HIV, cancer, or those on immunosuppressive medications, are susceptible to crusted scabies (formerly called Norwegian scabies). On those with weaker immune systems, the host becomes a more fertile breeding ground for the mites, which spread over the host's body, except the face. Sufferers of crusted scabies exhibit scaly rashes, slight itching, and thick crusts of skin that contain thousands of mites. Such areas make eradication of mites particularly difficult, as the crusts protect the mites from topical miticides/scabicides, necessitating prolonged treatment of these areas.
Cause.
In the 18th century, Italian biologist Diacinto Cestoni (1637–1718) described the mite now called "Sarcoptes scabiei", variety "hominis", as the cause of scabies. "Sarcoptes" is a genus of skin parasites and part of the larger family of mites collectively known as scab mites. These organisms have eight legs as adults, and are placed in the same phylogenetic class (Arachnida) as spiders and ticks.
"Sarcoptes scabiei" mites are under 0.5 mm in size but are sometimes visible as pinpoints of white. Pregnant females tunnel into the dead, outermost layer (stratum corneum) of a host's skin and deposit eggs in the shallow burrows. The eggs hatch into larvae in three to ten days. These young mites move about on the skin and molt into a "nymphal" stage, before maturing as adults, which live three to four weeks in the host's skin. Males roam on top of the skin, occasionally burrowing into the skin. In general, the total number of adult mites infesting a healthy hygienic person with non-crusted scabies is small; about 11 females in burrows, on average.
The movement of mites within and on the skin produces an intense itch, which has the characteristics of a delayed cell-mediated inflammatory response to allergens. IgE antibodies are present in the serum and the site of infection, which react to multiple protein allergens in the body of the mite. Some of these cross-react to allergens from house-dust mites. Immediate antibody-mediated allergic reactions (wheals) have been elicited in infected persons, but not in healthy persons; immediate hypersensitivity of this type is thought to explain the observed far more rapid allergic skin response to reinfection seen in persons having been previously infected (especially having been infected within the previous year or two). Because the host develops the symptoms as a reaction to the mites' presence over time, there is typically a delay of four to six weeks between the onset of infestation and the onset of itching. Similarly, symptoms often persist for one to several weeks after successful eradication of the mites. As noted, those re-exposed to scabies after successful treatment may exhibit symptoms of the new infestation in a much shorter period—as little as one to four days.
Scabies is contagious and can be contracted through prolonged (as opposed to momentary) physical contact with an infested person. This includes sexual intercourse, although a majority of cases are acquired through other forms of skin-to-skin contact. Less commonly, scabies infestation can happen through the sharing of clothes, towels, and bedding, but this is not a major mode of transmission; individual mites can only survive for two to three days, at most, away from human skin. As with lice, a latex condom is ineffective against scabies transmission during intercourse, because mites typically migrate from one individual to the next at sites other than the sex organs.
Pathophysiology.
The symptoms are caused by an allergic reaction of the host's body to mite proteins, though exactly which proteins remains a topic of study. The mite proteins are also present from the gut, in mite feces, which are deposited under the skin. The allergic reaction is both of the delayed (cell-mediated) and immediate (antibody-mediated) type, and involves IgE (antibodies, it is presumed, mediate the very rapid symptoms on reinfection). The allergy-type symptoms (itching) continue for some days, and even several weeks, after all mites are killed. New lesions may appear for a few days after mites are eradicated. Nodular lesions from scabies may continue to be symptomatic for weeks after the mites have been killed.
Diagnosis.
Scabies may be diagnosed clinically in geographical areas where it is common when diffuse itching presents along with either lesions in two typical spots or there is itchiness of another household member. The classical sign of scabies is the burrows made by the mites within the skin. To detect the burrow, the suspected area is rubbed with ink from a fountain pen or a topical tetracycline solution, which glows under a special light. The skin is then wiped with an alcohol pad. If the person is infected with scabies, the characteristic zigzag or "S" pattern of the burrow will appear across the skin; however, interpreting this test may be difficult, as the burrows are scarce and may be obscured by scratch marks. A definitive diagnosis is made by finding either the scabies mites or their eggs and fecal pellets. Searches for these signs involve either scraping a suspected area, mounting the sample in potassium hydroxide and examining it under a microscope, or using dermoscopy to examine the skin directly.
Differential diagnosis.
Symptoms of early scabies infestation mirror other skin diseases, including dermatitis, syphilis, various urticaria-related syndromes, allergic reactions, and other ectoparasites such as lice and fleas.
Prevention.
Mass treatment programs that use topical permethrin or oral ivermectin have been effective in reducing the prevalence of scabies in a number of populations. No vaccine is available for scabies. The simultaneous treatment of all close contacts is recommended, even if they show no symptoms of infection (asymptomatic), to reduce rates of recurrence. Since mites can survive for only two to three days without a host, other objects in the environment pose little risk of transmission except in the case of crusted scabies, thus cleaning is of little importance. Rooms used by those with crusted scabies require thorough cleaning.
Management.
A number of medications are effective in treating scabies. Treatment should involve the entire household, and any others who have had recent, prolonged contact with the infested individual. Options to control itchiness include antihistamines and prescription anti-inflammatory agents. Bedding, clothing and towels used during the previous three days should be washed in hot water and dried in a hot dryer.
Permethrin.
Permethrin is the most effective treatment for scabies, and remains the treatment of choice. It is applied from the neck down, usually before bedtime, and left on for about eight to 14 hours, then washed off in the morning. Care should be taken to coat the entire skin surface, not just symptomatic areas; any patch of skin left untreated can provide a "safe haven" for one or more mites to survive. One application is normally sufficient, as permethrin kills eggs and hatchlings as well as adult mites, though many physicians recommend a second application three to seven days later as a precaution. Crusted scabies may require multiple applications, or supplemental treatment with oral ivermectin (below). Permethrin may cause slight irritation of the skin that is usually tolerable.
Ivermectin.
Oral Ivermectin is effective in eradicating scabies, often in a single dose. It is the treatment of choice for crusted scabies, and is sometimes prescribed in combination with a topical agent. It has not been tested on infants, and is not recommended for children under six years of age.
Topical ivermectin preparations have been shown to be effective for scabies in adults, though only one such formulation is available in the United States at present, and it is not FDA approved as a scabies treatment. It has also been useful for sarcoptic mange (the veterinary analog of human scabies).
Others.
Other treatments include lindane, benzyl benzoate, crotamiton, malathion, and sulfur preparations. Lindane is effective, but concerns over potential neurotoxicity has limited its availability in many countries. It is banned in California, but may be used in other states as a second-line treatment. Sulfur ointments or benzyl benzoate are often used in the developing world due to their low cost; 10% sulfur solutions have been shown to be effective, and sulfur ointments are typically used for at least a week, though many people find the odor of sulfur products unpleasant. Crotamiton has been found to be less effective than permethrin in limited studies. Crotamiton or sulfur preparations are sometimes recommended instead of permethrin for children, due to concerns over dermal absorption of permethrin.
Communities.
Anne Frank was infected with scabies at Auschwitz concentration camp, and scabies is endemic in many developing countries, where it tends to be particularly problematic in rural and remote areas. In such settings community wide control strategies are required to reduce the rate of disease, as treatment of only individuals is ineffective due to the high rate of reinfection. Large-scale mass drug administration strategies may be required where coordinated interventions aim to treat whole communities in one concerted effort. Although such strategies have shown to be able to reduce the burden of scabies in these kinds of communities, debate remains about the best strategy to adopt, including the choice of drug.
The resources required to implement such large-scale interventions in a cost-effective and sustainable way are significant. Furthermore, since endemic scabies is largely restricted to poor and remote areas, it is a public health issue that has not attracted much attention from policy makers and international donors.
Epidemiology.
Scabies is one of the three most common skin disorders in children, along with tinea and pyoderma. As of 2010 it affects approximately 100 million people (1.5% of the population) and is equally common in both genders. The mites are distributed around the world and equally infect all ages, races, and socioeconomic classes in different climates. Scabies is more often seen in crowded areas with unhygienic living conditions. Globally as of 2009, an estimated 300 million cases of scabies occur each year, although various parties claim the figure is either over- or underestimated. About 1–10% of the global population is estimated to be infected with scabies, but in certain populations, the infection rate may be as high as 50–80%.
History.
Scabies has been observed in humans since ancient times. Archeological evidence from Egypt and the Middle East suggests scabies was present as early as 494 BC. The first recorded reference to scabies is believed to be from the Bible – it may be a type of "leprosy" mentioned in Leviticus "circa" 1200 BC or be mentioned among the curses of Deuteronomy 28. In the fourth century BC, Aristotle reported on "lice" that "escape from little pimples if they are pricked" — a description consistent with scabies.
The Greek encyclopedist and medical writer Aulus Cornelius Celsus (c. 25 BC – c. 50 AD) is credited with naming the disease "scabies" and describing its characteristic features. The parasitic etiology of scabies was documented by the Italian physician Giovanni Cosimo Bonomo (1663–1696) in his 1687 letter, "Observations concerning the fleshworms of the human body". Bonomo's description established scabies as one of the first human diseases with a well-understood cause.
Society and culture.
The International Alliance for the Control of Scabies (IACS) was started in 2012, and brings together over 70 researchers, clinicians and public health experts from more than 15 different countries. It has managed to bring the global health implications of scabies to the attention of the World Health Organization. Consequently, the WHO has included scabies on its official list of neglected tropical diseases and other neglected conditions.
Other animals.
Scabies may occur in a number of domestic and wild animals; the mites that cause these infestations are of different subspecies from the one typically causing the human form. These subspecies can infest animals that are not their usual hosts, but such infections do not last long. Scabies-infected animals suffer severe itching and secondary skin infections. They often lose weight and become frail.
The most frequently diagnosed form of scabies in domestic animals is sarcoptic mange, caused by the subspecies "Sarcoptes scabiei canid", most commonly in dogs and cats. Sarcoptic mange is transmissible to humans who come into prolonged contact with infested animals, and is distinguished from human scabies by its distribution on skin surfaces covered by clothing. Scabies-infected domestic fowl suffer what is known as "scaly leg". Domestic animals that have gone feral and have no veterinary care are frequently afflicted with scabies and a host of other ailments. Nondomestic animals have also been observed to suffer from scabies. Gorillas, for instance, are known to be susceptible to infection via contact with items used by humans.

</doc>
<doc id="28858" url="http://en.wikipedia.org/wiki?curid=28858" title="Stone–Weierstrass theorem">
Stone–Weierstrass theorem

In mathematical analysis, the Weierstrass approximation theorem states that every continuous function defined on a closed interval ["a", "b"] can be uniformly approximated as closely as desired by a polynomial function. Because polynomials are among the simplest functions, and because computers can directly evaluate polynomials, this theorem has both practical and theoretical relevance, especially in polynomial interpolation. The original version of this result was established by Karl Weierstrass in 1885 using the Weierstrass transform.
Marshall H. Stone considerably generalized the theorem and simplified the proof . His result is known as the Stone–Weierstrass theorem. The Stone–Weierstrass theorem generalizes the Weierstrass approximation theorem in two directions: instead of the real interval ["a", "b"], an arbitrary compact Hausdorff space X is considered, and instead of the algebra of polynomial functions, approximation with elements from more general subalgebras of C("X") is investigated. The Stone–Weierstrass theorem is a vital result in the study of the algebra of continuous functions on a compact Hausdorff space.
Further, there is a generalization of the Stone–Weierstrass theorem to noncompact Tychonoff spaces, namely, any continuous function on a Tychonoff space is approximated uniformly on compact sets by algebras of the type appearing in the Stone–Weierstrass theorem and described below.
A different generalization of Weierstrass' original theorem is Mergelyan's theorem, which generalizes it to functions defined on certain subsets of the complex plane.
Weierstrass approximation theorem.
The statement of the approximation theorem as originally discovered by Weierstrass is as follows:
A constructive proof of this theorem using Bernstein polynomials is outlined on that page.
Applications.
As a consequence of the Weierstrass approximation theorem, one can show that the space C["a", "b"] is separable: the polynomial functions are dense, and each polynomial function can be uniformly approximated by one with rational coefficients; there are only countably many polynomials with rational coefficients. Since C["a", "b"] is Hausdorff and separable it follows that C["a", "b"] has cardinality equal to 2ℵ0 — the same cardinality as the cardinality of the reals. (Remark: This cardinality result also follows from the fact that a continuous function on the reals is uniquely determined by its restriction to the rationals.)
Stone–Weierstrass theorem, real version.
The set C["a", "b"] of continuous real-valued functions on ["a", "b"], together with the supremum norm , is a Banach algebra, (i.e. an associative algebra and a Banach space such that || "fg"|| ≤ || "f" ||·||"g"|| for all  "f", "g"). The set of all polynomial functions forms a subalgebra of C["a", "b"] (i.e. a vector subspace of C["a", "b"] that is closed under multiplication of functions), and the content of the Weierstrass approximation theorem is that this subalgebra is dense in C["a", "b"].
Stone starts with an arbitrary compact Hausdorff space X and considers the algebra C("X", R) of real-valued continuous functions on X, with the topology of uniform convergence. He wants to find subalgebras of C("X", R) which are dense. It turns out that the crucial property that a subalgebra must satisfy is that it "separates points": a set A of functions defined on X is said to separate points if, for every two different points x and y in X there exists a function p in A with "p"("x") ≠ "p"("y"). Now we may state:
This implies Weierstrass’ original statement since the polynomials on ["a", "b"] form a subalgebra of C["a", "b"] which contains the constants and separates points.
Locally compact version.
A version of the Stone–Weierstrass theorem is also true when X is only locally compact. Let C0("X", R) be the space of real-valued continuous functions on X which vanish at infinity; that is, a continuous function "f" is in C0("X", R) if, for every "ε" > 0, there exists a compact set "K" ⊂ "X" such that "f"  < "ε" on "X" \ "K". Again, C0("X", R) is a Banach algebra with the supremum norm. A subalgebra A of C0("X", R) is said to vanish nowhere if not all of the elements of A simultaneously vanish at a point; that is, for every x in X, there is some "f" in A such that "f" ("x") ≠ 0. The theorem generalizes as follows:
This version clearly implies the previous version in the case when X is compact, since in that case C0("X", R) = C("X", R). There are also more general versions of the Stone–Weierstrass that weaken the assumption of local compactness.
Applications.
The Stone–Weierstrass theorem can be used to prove the following two statements which go beyond Weierstrass's result.
The theorem has many other applications to analysis, including:
Stone–Weierstrass theorem, complex version.
Slightly more general is the following theorem, where we consider the algebra C("X", C) of complex-valued continuous functions on the compact space X, again with the topology of uniform convergence. This is a C*-algebra with the *-operation given by pointwise complex conjugation.
The complex unital *-algebra generated by S consists of all those functions that can be obtained from the elements of S by throwing in the constant function 1 and adding them, multiplying them, conjugating them, or multiplying them with complex scalars, and repeating finitely many times. 
This theorem implies the real version, because if a sequence of complex-valued functions uniformly approximate a given function  "f" , then the real parts of those functions uniformly approximate the real part of  "f" . As in the real case, an analog of this theorem is true for locally compact Hausdorff spaces.
Stone–Weierstrass theorem, quaternion version.
Following : consider the algebra C("X", H) of quaternion-valued continuous functions on the compact space X, again with the topology of uniform convergence. If a quaternion "q" is written in the form "q"=a+"i"b+"j"c+"k"d then the scalar part a is the real number ("q"-"iqi"-"jqj"-"kqk")/4. Likewise being the scalar part of "-qi","-qj" and "-qk" : b,c and d are respectively the real numbers ("-qi"-"iq"+"jqk"-"kqj")/4,
("-qj"-"iqk"-"jq"+"kqi")/4 and ("-qk"+"iqj"-"jqk"-"kq")/4. Then we may state :
Lattice versions.
Let X be a compact Hausdorff space. Stone's original proof of the theorem used the idea of lattices in C("X", R). A subset L of C("X", R) is called a lattice if for any two elements  "f", "g" ∈ "L", the functions max{ "f", "g"}, min{ "f", "g"}also belong to L. The lattice version of the Stone–Weierstrass theorem states:
The above versions of Stone–Weierstrass can be proven from this version once one realizes that the lattice property can also be formulated using the absolute value | "f" | which in turn can be approximated by polynomials in  "f" . A variant of the theorem applies to linear subspaces of C("X", R) closed under max :
More precise information is available:
Bishop's theorem.
Another generalization of the Stone–Weierstrass theorem is due to Errett Bishop. Bishop's theorem is as follows :
 gives a short proof of Bishop's theorem using the Krein–Milman theorem in an essential way, as well as the Hahn–Banach theorem : the process of . See also .
References.
Historical works.
The historical publication of Weierstrass (in German language) is freely available from the digital online archive of the "":
Important historical works of Stone include:
Books.
'Optimization: Insights and Applications', Jan Brinkhuis and Vladimir Tikhomirov: 2005, Princeton University Press

</doc>
<doc id="28892" url="http://en.wikipedia.org/wiki?curid=28892" title="Skara Brae">
Skara Brae

Skara Brae is a stone-built Neolithic settlement, located on the Bay of Skaill on the west coast of Mainland, the largest island in the Orkney archipelago of Scotland. It consists of eight clustered houses, and was occupied from roughly 3180 BCE–2500 BCE. Europe's most complete Neolithic village, Skara Brae gained UNESCO World Heritage Site status as one of four sites making up "The Heart of Neolithic Orkney."a Older than Stonehenge and the Great Pyramids, it has been called the "Scottish Pompeii" because of its excellent preservation.
Discovery and early exploration.
In the winter of 1850, a severe storm hit Scotland causing widespread damage and over 200 deaths. In the Bay of Skaill, the storm stripped the earth from a large irregular knoll, known as "Skerrabra". When the storm cleared, local villagers found the outline of a village, consisting of a number of small houses without roofs. William Watt of Skaill, the local laird, began an amateur excavation of the site, but after uncovering four houses the work was abandoned in 1868. The site remained undisturbed until 1913, when during a single weekend the site was plundered by a party with shovels who took away an unknown quantity of artifacts. In 1924 another storm swept away part of one of the houses and it was determined the site should be made secure and more seriously investigated. The job was given to University of Edinburgh's Professor Vere Gordon Childe who travelled to Skara Brae for the first time in mid-1927.
Neolithic lifestyle.
Skara Brae's people were makers and users of grooved ware, a distinctive style of pottery that appeared in northern Scotland not long before the establishment of the village. The houses used earth sheltering, being sunk into the ground. In fact, they were sunk into mounds of pre-existing prehistoric domestic waste known as middens. The midden provided the houses with a small degree of stability and also acted as insulation against Orkney's harsh winter climate. On average, the houses measure 40 m2 in size with a large square room containing a stone hearth used for heating and cooking. Given the number of homes, it seems likely that no more than fifty people lived in Skara Brae at any given time.
It is by no means clear what material the inhabitants burned in their hearths. Gordon Childe was sure that the fuel was peat, but a detailed analysis of vegetation patterns and trends suggests that climatic conditions conducive to the development of thick beds of peat did not develop in this part of Orkney until after Skara Brae was abandoned. Other possible fuels include driftwood and animal dung. There's evidence that dried seaweed may have been used significantly. At a number of sites in Orkney investigators have found a glassy, slag-like material called "Kelp" or "Cramp" that may be residual burnt seaweed.
The dwellings contain a number of stone-built pieces of furniture, including cupboards, dressers, seats, and storage boxes. Each dwelling was entered through a low doorway that had a stone slab door that could be closed "by a bar that slid in bar-holes cut in the stone door jambs". A sophisticated drainage system was incorporated into the village's design. It included a primitive form of toilet in each dwelling. 
Seven of the houses have similar furniture, with the beds and dresser in the same places in each house. The dresser stands against the wall opposite the door, and was the first thing seen by anyone entering the dwelling. Each of these houses had the larger bed on the right side of the doorway and the smaller on the left. Lloyd Laing noted that this pattern accorded with Hebridean custom up to the early 20th century suggesting that the husband's bed was the larger and the wife's was the smaller. The discovery of beads and paint-pots in some of the smaller beds may support this interpretation. (Central Asian yurt dwellings have an identical internal spatial gender assignment, central fire and storage chest opposite the entrance.) Additional support may come from the recognition that stone boxes lie to the left of most doorways, forcing the person entering the house to turn to the right-hand, 'male', side of the dwelling. At the front of each bed lie the stumps of stone pillars that may have supported a canopy of fur; another link with recent Hebridean style.
One house, called House 8, has no storage boxes or dresser. It has been divided into something resembling small cubicles. When this house was excavated, fragments of stone, bone and antler were found. It is possible that this building was used as a house to make simple tools such as bone needles or flint axes. The presence of heat-damaged volcanic rocks and what appears to be a flue, support this interpretation. House 8 is distinctive in other ways as well. It is a stand-alone structure not surrounded by midden, instead it is above ground and has walls over 2 m thick. It has a "porch" protecting the entrance.
The site provided the earliest known record of the human flea "Pulex irritans" in Europe.
The Grooved Ware People who built Skara Brae were primarily pastoralists who raised cattle and sheep. Childe originally believed that the inhabitants did not practice agriculture, but excavations in 1972 unearthed seed grains from a midden suggesting that barley was cultivated. Fish bones and shells are common in the middens indicating that dwellers ate seafood. Limpet shells are common and may have been fish-bait that was kept in stone boxes in the homes. The boxes were formed from thin slabs with joints carefully sealed with clay to render them waterproof.
This pastoral lifestyle is in sharp contrast to some of the more exotic interpretations of the culture of the Skara Brae people. Euan MacKie suggested that Skara Brae might be the home of a privileged theocratic class of wise men who engaged in astronomical and magical ceremonies at nearby Ring of Brodgar and the Standing Stones of Stenness. Graham and Anna Ritchie cast doubt on this interpretation noting that there is no archaeological evidence for this claim, although a Neolithic "low road" that goes from Skara Brae passes near both these sites and ends at the magnificent chambered tomb of Maeshowe, . Low roads connect Neolithic ceremonial sites throughout Britain. 
Dating and abandonment.
Originally, Childe believed that the settlement dated from around 500 BCE. This interpretation was coming under increasing challenge by the time new excavations in 1972–73 settled the question. Radiocarbon results obtained from samples collected during these excavations indicate that occupation of Skara Brae began about 3180 BCE with occupation continuing for about six hundred years. Around 2500 BCE, after the climate changed, becoming much colder and wetter, the settlement may have been abandoned by its inhabitants. There are many theories as to why the people of Skara Brae left; particularly popular interpretations involve a major storm. Evan Hadingham combined evidence from found objects with the storm scenario to imagine a dramatic end to the settlement:
As was the case at Pompeii, the inhabitants seem to have been taken by surprise and fled in haste, for many of their prized possessions, such as necklaces made from animal teeth and bone, or pins of walrus ivory, were left behind. The remains of choice meat joints were discovered in some of the beds, presumably forming part of the villagers' last supper. One woman was in such haste that her necklace broke as she squeezed through the narrow doorway of her home, scattering a stream of beads along the passageway outside as she fled the encroaching sand.
Anna Ritchie strongly disagrees with catastrophic interpretations of the village's abandonment:
A popular myth would have the village abandoned during a massive storm that threatened to bury it in sand instantly, but the truth is that its burial was gradual and that it had already been abandoned — for what reason, no one can tell.
The site was farther from the sea than it is today, and it is possible that Skara Brae was built adjacent to a freshwater lagoon protected by dunes. Although the visible buildings give an impression of an organic whole, it is certain that an unknown quantity of additional structures had already been lost to sea erosion before the site's rediscovery and subsequent protection by a seawall. Uncovered remains are known to exist immediately adjacent to the ancient monument in areas presently covered by fields, and others, of uncertain date, can be seen eroding out of the cliff edge a little to the south of the enclosed area.
Artifacts.
A number of enigmatic Carved Stone Balls have been found at the site and some are on display in the museum. Similar objects have been found throughout northern Scotland. The spiral ornamentation on some of these "balls" has been stylistically linked to objects found in the Boyne Valley in Ireland. Similar symbols have been found carved into stone lintels and bed posts. These symbols, sometimes referred to as "runic writings", have been subjected to controversial translations. For example, Castleden suggested that "colons" found punctuating vertical and diagonal symbols may represent separations between words.
Lumps of red ochre found here and at other Neolithic sites have been interpreted as evidence that body painting may have been practiced. Nodules of haematite with highly polished surfaces have been found as well; the shiny surfaces suggest that the nodules were used to finish leather.
Other artifacts excavated on site made of animal, fish, bird, and whalebone, whale and walrus ivory, and killer whale teeth included awls, needles, knives, beads, adzes, shovels, small bowls and, most remarkably, ivory pins up to 10 in long. These pins are very similar to examples found in passage graves in the Boyne Valley, another piece of evidence suggesting a linkage between the two cultures. So-called Skaill knives were commonly used tools in Skara Brae; these consist of large flakes knocked off sandstone cobbles. Skaill knives have been found throughout Orkney and Shetland.
The 1972 excavations reached layers that had remained waterlogged and had preserved items that otherwise would have been destroyed. These include a twisted skein of heather, one of a very few known examples of Neolithic rope. and a wooden handle.
Related sites in Orkney.
A comparable, though smaller, site exists at Rinyo on Rousay. Unusually, no Maeshowe-type tombs have been found on Rousay and although there are a large number of Orkney–Cromarty chambered cairns, these were built by Unstan ware people.
Knap of Howar on the Orkney island of Papa Westray, is a well preserved Neolithic farmstead. Dating from 3500 BCE to 3100 BCE, it is similar in design to Skara Brae, but from an earlier period, and it is thought to be the oldest preserved standing building in northern Europe.
There is also a site currently under excavation at Links of Noltland on Westray that appears to have similarities to Skara Brae.
World Heritage status.
"The Heart of Neolithic Orkney" was inscribed as a World Heritage site in December 1999. In addition to Skara Brae the site includes Maeshowe, the Ring of Brodgar, the Standing Stones of Stenness and other nearby sites. It is managed by Historic Scotland, whose 'Statement of Significance' for the site begins:
The monuments at the heart of Neolithic Orkney and Skara Brae proclaim the triumphs of the human spirit in early ages and isolated places. They were approximately contemporary with the mastabas of the archaic period of Egypt (first and second dynasties), the brick temples of Sumeria, and the first cities of the Harappa culture in India, and a century or two earlier than the Golden Age of China. Unusually fine for their early date, and with a remarkably rich survival of evidence, these sites stand as a visible symbol of the achievements of early peoples away from the traditional centres of civilisation.
Notes.
^a It is one of four UNESCO World Heritage Sites in Scotland, the others being the Old Town and New Town of Edinburgh; New Lanark in South Lanarkshire; and St Kilda in the Western Isles 

</doc>
<doc id="28916" url="http://en.wikipedia.org/wiki?curid=28916" title="Shetland">
Shetland

Shetland (; Scottish Gaelic: "Sealtainn" ]), also called the Shetland Islands, is a subarctic archipelago of Scotland that lies north-east of the island of Great Britain and forms part of the United Kingdom.
The islands lie some 80 km to the northeast of Orkney and 280 km southeast of the Faroe Islands and form part of the division between the Atlantic Ocean to the west and the North Sea to the east. The total area is 1468 km2 and the population totalled 23,167 in 2011. Comprising the Shetland constituency of the Scottish Parliament, Shetland is also one of the 32 council areas of Scotland; the islands' administrative centre and only burgh is Lerwick.
The largest island, known simply as "Mainland", has an area of 967 km2, making it the third-largest Scottish island and the fifth-largest of the British Isles. There are an additional 15 inhabited islands. The archipelago has an oceanic climate, a complex geology, a rugged coastline and many low, rolling hills.
Humans have lived there since the Mesolithic period, and the earliest written references to the islands date back to Roman times. The early historic period was dominated by Scandinavian influences, especially Norway, and the islands did not become part of Scotland until the 15th century. When Shetland became part of the Kingdom of Great Britain in 1707, trade with northern Europe decreased. Fishing has continued to be an important aspect of the economy up to the present day. The discovery of North Sea oil in the 1970s significantly boosted Shetland incomes, employment and public sector revenues.
The local way of life reflects the joint Norse and Scottish heritage including the Up Helly Aa fire festival, and a strong musical tradition, especially the traditional fiddle style. The islands have produced a variety of writers of prose and poetry, many of whom use the local Shetlandic dialect. There are numerous areas set aside to protect the local fauna and flora, including a number of important seabird nesting sites. The Shetland pony and Shetland Sheepdog are two well-known Shetland animal breeds.
The islands' motto, which appears on the Council's coat of arms, is "Með lögum skal land byggja". This Icelandic phrase is taken from Njáls saga and means "By law shall the land be built up".
Etymology.
The name of Shetland is derived from the Old Norse words, "hjalt" (hilt), and "land" (land).
In AD 43 and 77 the Roman authors Pomponius Mela and Pliny the Elder referred to the seven islands they call "Haemodae" and "Acmodae" respectively, both of which are assumed to be Shetland. Another possible early written reference to the islands is Tacitus' report in AD 98, after describing the discovery and conquest of Orkney, that the Roman fleet had seen "Thule, too". In early Irish literature, Shetland is referred to as "Inse Catt"—"the Isles of Cats", which may have been the pre-Norse inhabitants' name for the islands. The Cat tribe also occupied parts of the northern Scottish mainland and their name can be found in Caithness, and in the Gaelic name for Sutherland ("Cataibh", meaning "among the Cats").
The oldest version of the modern name Shetland is "Hetlandensis", the Latinised adjectival form of the Old Norse name recorded in a letter from Harald count of Shetland in 1190, becoming "Hetland" in 1431 after various intermediate transformations. It is possible that the Pictish "cat" sound forms part of this Norse name. It then became "Hjaltland" in the 16th century.
As Norn was gradually replaced by Scots, "Hjaltland" became "Ȝetland". The initial letter is the Middle Scots letter, "yogh", the pronunciation of which is almost identical to the original Norn sound, "/hj/". When the use of the letter yogh was discontinued, it was often replaced by the similar-looking letter z, hence "Zetland", the misspelt form used to describe the pre-1975 county council.
Most of the individual islands have Norse names, although the derivations of some are obscure and may represent pre-Norse, possibly Pictish or even pre-Celtic names or elements.
Geography and geology.
Shetland is around 170 km north of mainland Scotland, covers an area of 1468 km2 and has a coastline 2702 km long.
Lerwick, the capital and largest settlement, has a population of 6,958 and about half of the archipelago's total population of 23,167 people live within 16 km of the town.
Scalloway on the west coast, which was the capital until 1708, has a population of less than 1,000.
Only 16 of about 100 islands are inhabited. The main island of the group is known as Mainland, and of the next largest, Yell, Unst, and Fetlar lie to the north and Bressay and Whalsay lie to the east. East and West Burra, Muckle Roe, Papa Stour, Trondra and Vaila are smaller islands to the west of Mainland. The other inhabited islands are Foula 28 km west of Walls, Fair Isle 38 km south-west of Sumburgh Head, and the Out Skerries to the east.
The uninhabited islands include Mousa, known for the Broch of Mousa, the finest preserved example in Scotland of these Iron Age round towers, St Ninian's Isle connected to Mainland by the largest active tombolo in the UK, and Out Stack, the northernmost point of the British Isles. Shetland's location means that it provides a number of such records: Muness is the most northerly castle in the United Kingdom and Skaw the most northerly settlement.
The geology of Shetland is complex, with numerous faults and fold axes. These islands are the northern outpost of the Caledonian orogeny, and there are outcrops of Lewisian, Dalriadan and Moine metamorphic rocks with histories similar to their equivalents on the Scottish mainland. There are also Old Red Sandstone deposits and granite intrusions. The most distinctive features are the ultrabasic ophiolite, peridotite and gabbro on Unst and Fetlar, which are remnants of the Iapetus Ocean floor. Much of Shetland's economy depends on the oil-bearing sediments in the surrounding seas. Geological evidence shows that in around 6100 BC a tsunami caused by the Storegga Slides hit Shetland, as well as the rest of the east coast of Scotland, and may have created a wave of up to 25 m high in the voes where modern populations are highest.
The highest point of Shetland is Ronas Hill, which only reaches 450 m. The Pleistocene glaciations entirely covered the islands. During this period, the Stanes of Stofast, a 2000-tonne glacial erratic, came to rest on a prominent hilltop in Lunnasting.
Shetland is a National Scenic Area which, unusually, is made up of a number of discrete locations: Fair Isle, Foula, South West Mainland (including the Scalloway Islands), Muckle Roe, Esha Ness, Fethaland and Herma Ness.
Climate.
Shetland has an oceanic sub-polar climate, with long but cool winters and short mild summers. The climate all year round is moderate due to the influence of the surrounding seas, with average peak temperatures of 7 °C in March and 18 C in July and August. Temperatures over 25 °C are very rare. The highest temperature on record was 28.4 °C in July 1991 and the coldest -8.9 °C in the Januarys of 1952 and 1959. The frost-free period may be as little as three months. In contrast, inland areas of nearby Scandinavia on similar latitudes experience significantly larger temperature differences between summer and winter, with the average highs of regular July days comparable to Shetland's all-time record heat, further demonstrating the moderating effect of the Atlantic Ocean. In contrast, winters are considerably milder than those expected in nearby continental areas, even comparable to winter temperatures of many parts of England and Wales much further south.
The general character of the climate is windy and cloudy with at least 2 mm of rain falling on more than 250 days a year. Average yearly precipitation is 1003 mm, with November and December the wettest months. Snowfall is usually confined to the period November to February, and snow seldom lies on the ground for more than a day. Less rain falls from April to August although no month receives less than 50 mm. Fog is common during summer due to the cooling effect of the sea on mild southerly airflows.
Due to the islands' latitude, on clear winter nights the "northern lights" can sometimes be seen in the sky, while in summer there is almost perpetual daylight, a state of affairs known locally as the "simmer dim". Annual bright sunshine averages 1090 hours and overcast days are common.
Prehistory.
Due to the practice, dating to at least the early Neolithic, of building in stone on virtually treeless islands, Shetland is extremely rich in physical remains of the prehistoric eras and there are over 5,000 archaeological sites all told. A midden site at West Voe on the south coast of Mainland, dated to 4320–4030 BC, has provided the first evidence of Mesolithic human activity on Shetland. The same site provides dates for early Neolithic activity and finds at Scord of Brouster in Walls have been dated to 3400 BC. "Shetland knives" are stone tools that date from this period made from felsite from Northmavine.
Pottery shards found at the important site of Jarlshof also indicate that there was Neolithic activity there although the main settlement dates from the Bronze Age. This includes a smithy, a cluster of wheelhouses and a later broch. The site has provided evidence of habitation during various phases right up until Viking times. Heel-shaped cairns, are a style of chambered cairn unique to Shetland, with a particularly large example on Vementry.
Numerous brochs were erected during the Iron Age. In addition to Mousa there are significant ruins at Clickimin, Culswick, Old Scatness and West Burrafirth, although their origin and purpose is a matter of some controversy. The later Iron Age inhabitants of the Northern Isles were probably Pictish, although the historical record is sparse. Hunter (2000) states in relation to King Bridei I of the Picts in the sixth century AD: "As for Shetland, Orkney, Skye and the Western Isles, their inhabitants, most of whom appear to have been Pictish in culture and speech at this time, are likely to have regarded Bridei as a fairly distant presence.” In 2011, the collective site, "The Crucible of Iron Age Shetland", including Broch of Mousa, Old Scatness and Jarlshof, joined the UKs "Tentative List" of World Heritage Sites.
History.
Scandinavian colonisation.
The expanding population of Scandinavia led to a shortage of available resources and arable land there and led to a period of Viking expansion, the Norse gradually shifting their attention from plundering to invasion. Shetland was colonised during the late 8th and 9th centuries, the fate of the existing indigenous population being uncertain. Modern Shetlanders have almost identical proportions of Scandinavian matrilineal and patrilineal genetic ancestry, suggesting that the islands were settled by both men and women in equal measure.
Vikings then made the islands the headquarters of pirate expeditions carried out against Norway and the coasts of mainland Scotland. In response, Norwegian king Harald Hårfagre ("Harald Fair Hair") annexed the Northern Isles (comprising Orkney and Shetland) in 875. Rognvald Eysteinsson received Orkney and Shetland from Harald as an earldom as reparation for the death of his son in battle in Scotland, and then passed the earldom on to his brother Sigurd the Mighty.
The islands were Christianised in the late 10th century. King Olav Tryggvasson summoned the "jarl" Sigurd the Stout during a visit to Orkney and said, "I order you and all your subjects to be baptised. If you refuse, I'll have you killed on the spot and I swear I will ravage every island with fire and steel." Unsurprisingly, Sigurd agreed and the islands became Christian at a stroke. Unusually, from c. 1100 onwards the Norse "jarls" owed allegiance both to Norway and to the Scottish crown through their holdings as Earls of Caithness.
In 1194, when Harald Maddadsson was Earl of Orkney and Shetland, a rebellion broke out against King Sverre Sigurdsson of Norway. The "Øyskjeggs" ("Island Beardies") sailed for Norway but were beaten in the Battle of Florvåg near Bergen. After his victory King Sverre placed Shetland under direct Norwegian rule, a state of affairs that continued for nearly two centuries.
Increased Scottish interest.
From the mid-13th century onwards Scottish monarchs increasingly sought to take control of the islands surrounding the mainland. The process was begun in earnest by Alexander II and was continued by his successor Alexander III. This strategy eventually led to an invasion by Haakon Haakonsson, King of Norway. His fleet assembled in Bressay Sound before sailing for Scotland. After the stalemate of the Battle of Largs, Haakon retreated to Orkney, where he died in December 1263, entertained on his death bed by recitations of the sagas. His death halted any further Norwegian expansion in Scotland and following this ill-fated expedition, the Hebrides and Mann were yielded to the Kingdom of Scotland as a result of the 1266 Treaty of Perth, although the Scots recognised continuing Norwegian sovereignty over Orkney and Shetland.
Pawned to Scotland.
In the 14th century, Orkney and Shetland remained a Norwegian province, but Scottish influence was growing. Jon Haraldsson, who was murdered in Thurso in 1231, was the last of an unbroken line of Norse jarls, and thereafter the earls were Scots noblemen of the houses of Angus and St. Clair. On the death of Haakon VI in 1380, Norway formed a political union with Denmark after which the interest of the royal house in the islands declined. In 1469, Shetland was pledged by Christian I, in his capacity as King of Norway, as security against the payment of the dowry of his daughter Margaret, betrothed to James III of Scotland. As the money was never paid, the connection with the crown of Scotland has become perpetual. In 1470, William Sinclair, 1st Earl of Caithness ceded his title to James III and the following year the Northern Isles were directly annexed to the Crown of Scotland, a process confirmed by Parliament in 1472. Nonetheless, Shetland's connection with Norway has proven to be enduring.
From the early 15th century on the Shetlanders sold their goods through the Hanseatic League of German merchantmen. The Hansa would buy shiploads of salted fish, wool and butter and import salt, cloth, beer and other goods. The late 16th century and early 17th century was dominated by the influence of the despotic Robert Stewart, Earl of Orkney, who was granted the islands by his half-sister Mary Queen of Scots, and his son Patrick. The latter commenced the building of Scalloway Castle, but after his imprisonment in 1609 the Crown annexed Orkney and Shetland again until 1643 when Charles I granted them to William Douglas, 7th Earl of Morton. These rights were held on and off by the Mortons until 1766, when they were sold by James Douglas, 14th Earl of Morton to Laurence Dundas.
Early British rule.
The trade with the North German towns lasted until the 1707 Act of Union when high salt duties prohibited the German merchants from trading with Shetland. Shetland then went into an economic depression as the Scottish and local traders were not as skilled in trading with salted fish. However, some local merchant-lairds took up where the German merchants had left off, and fitted out their own ships to export fish from Shetland to the Continent. For the independent farmers of Shetland this had negative consequences, as they now had to fish for these merchant-lairds.
Smallpox afflicted the islands in the 17th and 18th centuries, but as vaccines became common after 1760 the population increased to a maximum of 31,670 in 1861. However, British rule came at price for many ordinary people as well as traders. The Shetlanders nautical skills were sought by the Royal Navy. Some 3,000 served during the Napoleonic wars from 1800 to 1815 and press gangs were rife. During this period 120 men were taken from Fetlar alone and only 20 of them returned home. By the late 19th century 90% of all Shetland was owned by just 32 people, and between 1861 and 1881 more than 8,000 Shetlanders emigrated. With the passing of the Crofters' Act in 1886 the Liberal prime minister William Gladstone emancipated crofters from the rule of the landlords. The Act enabled those who had effectively been landowners' serfs to become owner-occupiers of their own small farms. By this time fishermen from Holland, who had traditionally gathered each year off the coast of Shetland to fish for herring, triggered an industry in the islands that boomed from around 1880 until the 1920s when stocks of the fish began to dwindle.
20th century.
During World War I many Shetlanders served in the Gordon Highlanders, a further 3,000 served in the Merchant Navy and more than 1500 in a special local naval reserve. The 10th Cruiser Squadron was stationed at Swarbacks Minn and during a single year from March 1917 more than 4,500 ships sailed from Lerwick as part of an escorted convoy system. In total, Shetland lost more than 500 men, a higher proportion than any other part of Britain, and there were further waves of emigration in the 1920s and 1930s.
During World War II a Norwegian naval unit nicknamed the "Shetland Bus" was established by the Special Operations Executive in the autumn of 1940 with a base first at Lunna and later in Scalloway to conduct operations around the coast of Norway. About 30 fishing vessels used by Norwegian refugees were gathered and the Shetland Bus conducted covert operations, carrying intelligence agents, refugees, instructors for the resistance, and military supplies. It made over 200 trips across the sea with Leif Larsen, the most highly decorated allied naval officer of the war, making 52 of them. Several RAF bases were also established at Sullom Voe and several lighthouses suffered enemy air attacks.
Oil reserves discovered in the later 20th century in the seas both east and west of Shetland have provided a much needed alternative source of income for the islands. The East Shetland Basin is one of Europe's largest oil fields and as a result of the oil revenue and the cultural links with Norway, a small independence movement developed briefly to recast the constitutional position of Shetland. It saw as its model the Isle of Man, as well as Shetland's closest neighbour, the Faroe Islands, an autonomous dependency of Denmark.
Economy.
Today, the main revenue producers in Shetland are agriculture, aquaculture, fishing, renewable energy, the petroleum industry (crude oil and natural gas production), the creative industries and tourism.
Fishing remains central to the islands' economy today, with the total catch being 75,767 tonne in 2009, valued at over £73.2 million. Mackerel makes up more than half of the catch in Shetland by weight and value, and there are significant landings of haddock, cod, herring, whiting, monkfish and shellfish. Farming is mostly concerned with the raising of Shetland sheep, known for their unusually fine wool. Crops raised include oats and barley; however, the cold, windswept islands make for a harsh environment for most plants. Crofting, the farming of small plots of land on a legally restricted tenancy basis, is still practiced and is viewed as a key Shetland tradition as well as an important source of income.
Oil and gas were first landed in 1978 at Sullom Voe, which has subsequently become one of the largest terminals in Europe. Taxes from the oil have increased public sector spending on social welfare, art, sport, environmental measures and financial development. Three quarters of the islands' workforce is employed in the service sector, and the Shetland Islands Council alone accounted for 27.9% of output in 2003. Shetland's access to oil revenues has funded the Shetland Charitable Trust, which in turn funds a wide variety of local programmes. The balance of the fund in 2011 was £217 million, i.e., about £9,500 per head.
In January 2007, the Shetland Islands Council signed a partnership agreement with Scottish and Southern Energy for the Viking Wind Farm, a 200-turbine wind farm and subsea cable. This renewable energy project would produce about 600 megawatts and contribute about £20 million to the Shetland economy per year. The plan is meeting significant opposition within the islands, primarily resulting from the anticipated visual impact of the development. The PURE project on Unst is a research centre which uses a combination of wind power and fuel cells to create a wind hydrogen system. The project is run by the Unst Partnership, the local community's development trust.
Knitwear is important both to the economy and culture of Shetland, and the Fair Isle design is well-known. However, the industry faces challenges due to plagiarism of the word "Shetland" by manufacturers operating elsewhere, and a certification trademark, "The Shetland Lady", has been registered. 
Shetland is served by a weekly local newspaper, "The Shetland Times" and the online "Shetland News" with radio service being provided by BBC Radio Shetland and the commercial radio station SIBC.
Shetland is a popular destination for cruise ships, and in 2010 the Lonely Planet guide named Shetland as the sixth best region in the world for tourists seeking unspoilt destinations. The islands were described as "beautiful and rewarding" and the Shetlanders as "a fiercely independent and self-reliant bunch". Overall visitor expenditure was worth £16.4 million in 2006, in which year just under 26,000 cruise liner passengers arrived at Lerwick Harbour. In 2009, the most popular visitor attractions were the Shetland Museum, the RSPB reserve at Sumburgh Head, Bonhoga Gallery at Weisdale Mill and Jarlshof.
Transport.
Transport between islands is primarily by ferry, and Shetland Islands Council operates various inter-island services. Shetland is also served by a domestic connection from Lerwick to Aberdeen on mainland Scotland. This service, which takes about 12 hours, is operated by NorthLink Ferries. Some services also call at Kirkwall, Orkney, which increases the journey time between Aberdeen and Lerwick by 2 hours.
Sumburgh Airport, the main airport on Shetland, is located close to Sumburgh Head, 40 km south of Lerwick. Loganair operates flights for FlyBe to other parts of Scotland up to ten times a day, the destinations being Kirkwall, Aberdeen, Inverness, Glasgow and Edinburgh. Lerwick/Tingwall Airport is located 11 km west of Lerwick. Operated by Directflight Ltd. in partnership with Shetland Islands Council, it is devoted to inter-island flights from the Shetland Mainland to most of the inhabited islands.
Scatsta Airport near Sullom Voe allows frequent charter flights from Aberdeen to transport oilfield workers and this small terminal has the fifth largest number of international passengers in Scotland.
Public bus services are operated on Mainland, Whalsay, Burra, Unst and Yell.
The archipelago is exposed to wind and tide, and there are numerous sites of wrecked ships. Lighthouses are sited as an aid to navigation at various locations.
Public services.
The Shetland Islands Council is the Local Government authority for all the islands, based in Lerwick Town Hall.
Shetland is sub-divided into 18 community council areas and into 12 civil parishes that are used for statistical purposes.
Education.
In Shetland there are two High Schools—Anderson and Brae—seven Junior High Schools, and over thirty primary schools.
Shetland is also home to the North Atlantic Fisheries College, the Centre for Nordic Studies and Shetland College, which are all associated with the University of the Highlands and Islands.
Sport.
The islands are represented by the Shetland football team who regularly compete in the Island Games. The islands' senior football league is the G&S Flooring Premier League.
Churches and religion.
The Reformation reached the archipelago in 1560. This was an apparently peaceful transition and there is little evidence of religious intolerance in Shetland's recorded history.
A variety of different religious denominations are represented in the islands.
The Methodist Church has a relatively high membership in Shetland, which is a District of the Methodist Church (with the rest of Scotland comprising a separate District).
The Church of Scotland has a Presbytery of Shetland that includes St. Columba's Church in Lerwick.
The Catholic population is served by the church of St. Margaret and the Sacred Heart in Lerwick. The Parish is part of the Diocese of Aberdeen.
The Scottish Episcopal Church (part of the Anglican Communion) has regular worship at St Magnus' Church, Lerwick, St Colman's Church, Burravoe, and the Chapel of Christ the Encompasser, Fetlar, the last of which is maintained by the Society of Our Lady of the Isles, the most northerly and remote Anglican religious order of nuns.
The Church of Jesus Christ of Latter-day Saints has a congregation in Lerwick. The former print works and offices of the local newspaper, The Shetland Times, has been converted into a chapel.
Politics.
Shetland is represented in the House of Commons as part of the Orkney and Shetland constituency, which elects one Member of Parliament, the current incumbent being Alistair Carmichael. This seat has been held by the Liberal Democrats or their predecessors the Liberal Party since 1950, longer than any other they represent in the UK.
In the Scottish Parliament the Shetland constituency elects one Member of the Scottish Parliament (MSP) by the first past the post system. The current MSP is Tavish Scott of the Scottish Liberal Democrats. Shetland is within the Highlands and Islands electoral region.
The political composition of the Council is 22 Independents. Thus it is one of only three Councils in Scotland with a majority of elected members not representing a political party.
Roy Grönneberg, who founded the local chapter of the Scottish National Party in 1966, designed the flag of Shetland in cooperation with Bill Adams to mark the 500th anniversary of the transfer of the islands from Norway to Scotland. The colours are identical to those of the Flag of Scotland, but are shaped in the Nordic cross. After several unsuccessful attempts, including a plebiscite in 1985, the Lord Lyon King of Arms approved it as the official flag of Shetland in 2005.
Local culture and the arts.
After the islands were transferred to Scotland, thousands of Scots families emigrated to Shetland in the 16th and 17th centuries but studies of the genetic makeup of the islands' population indicate that Shetlanders are just under half Scandinavian in origin. This combination is reflected in many aspects of local life. For example, almost every place name in use can be traced back to the Vikings. The Norn language was a form of Old Norse, which continued to be spoken until the 18th century when it was replaced by an insular dialect of Scots known as Shetlandic, which is in turn being replaced by Scottish English. Although Norn was spoken for hundreds of years it is now extinct and few written sources remain. Shetlandic is used both in local radio and dialect writing, and kept alive by the Shetland Folk Society.
The Lerwick Up Helly Aa is one of a variety of fire festivals held in Shetland annually in the middle of winter, it is always started on the last Tuesday of January. The festival is just over 100 years old in its present, highly organised form. Originally, a festival held to break up the long nights of winter and mark the end of Yule, the festival has become one celebrating the isles' heritage and includes a procession of men dressed as Vikings and the burning of a replica longship.
The cuisine of Shetland is based on locally produced lamb, beef and seafood, much of it organic. Inevitably, the real ale-producing Valhalla Brewery is the most northerly in Britain. The Shetland Black is a variety of blue potato with a dark skin and indigo coloured flesh markings.
Shetland competes in the biennial International Island Games, which it hosted in 2005.
Music.
Shetland's culture and landscapes have inspired a variety of musicians, writers and film-makers. The Forty Fiddlers was formed in the 1950s to promote the traditional fiddle style, which is a vibrant part of local culture today. Notable exponents of Shetland folk music include Aly Bain, Fiddlers' Bid, and the late Tom Anderson and Peerie Willie Johnson. Thomas Fraser was a country musician who never released a commercial recording during his life, but whose work has become popular more than 20 years after his untimely death in 1978.
Writers.
Walter Scott's 1822 novel "The Pirate" is set in "a remote part of Shetland", and was inspired by his 1814 visit to the islands. The name "Jarlshof" meaning "Earl's Mansion" is a coinage of his. Hugh MacDiarmid, the Scots poet and writer lived in Whalsay from the mid-1930s through 1942, and wrote many poems there, including a number that directly address or reflect the Shetland environment such as "On A Raised Beach", which was inspired by a visit to West Linga. The 1975 novel "North Star" by Hammond Innes is largely set in Shetland and Raman Mundair's 2007 book of poetry "A Choreographer's Cartography" offers a British Asian perspective on the landscape. The "Shetland Quartet" by Ann Cleeves, who previously lived in Fair Isle, is a series of crime novels set around the islands. In 2013 her novel "Red Bones" became the basis of BBC crime drama television series "Shetland".
Vagaland, who grew up in Walls, was arguably Shetland's finest poet of the 20th century. Haldane Burgess was a Shetland historian, poet, novelist, violinist, linguist and socialist and Rhoda Bulter (1929 – 1994) is one of the best-known Shetland poets of recent times. Other 20th and 21st century poets and novelists include Christine De Luca, Robert Alan Jamieson who grew up in Sandness, the late Lollie Graham of Veensgarth, Stella Sutherland of Bressay, the late William J Tait from Yell and Laureen Johnson.
There are two monthly magazines in production: "Shetland Life" and "i'i' Shetland". The quarterly The New Shetlander, founded in 1947, is said to be Scotland's longest-running literary magazine. For much of the later 20th century it was the major vehicle for the work of local writers - and others, including early work by George Mackay Brown.
Films.
Michael Powell made "The Edge of the World" in 1937, a dramatisation based on the true story of the evacuation of the last 36 inhabitants of the remote island of St Kilda on 29 August 1930. St Kilda lies in the Atlantic Ocean, 64 km west of the Outer Hebrides but Powell was unable to get permission to film there. Undaunted, he made the film over four months during the summer of 1936 on Foula and the film transposes these events to Shetland. Forty years later, the documentary "Return To The Edge Of The World" was filmed, capturing a reunion of cast and crew of the film as they revisited the island in 1978.
A number of other films have been made on or about Shetland including "A Crofter's Life in Shetland" (1932) "A Shetland Lyric" (1934), "Devil's Gate" (2003) and "It's Nice Up North" (2006), a comedy documentary by Graham Fellows. An annual film festival takes place in the newly built Mareel, a cinema, music and education venue.
Wildlife.
Shetland has three national nature reserves, at the seabird colonies of Hermaness and Noss, and at Keen of Hamar to preserve the serpentine flora. There are a further 81 SSSIs, which cover 66% or more of the land surfaces of Fair Isle, Papa Stour, Fetlar, Noss and Foula. Mainland has 45 separate sites.
Flora.
The landscape in Shetland is marked by the grazing of sheep and the harsh conditions have limited the total number of plant species to about 400. Native trees such as rowan and crab apple are only found in a few isolated places such as cliffs and loch islands. The flora is dominated by Arctic-alpine plants, wild flowers, moss and lichen. Spring squill, buck's-horn plantain, Scots lovage, roseroot and sea campion are abundant, especially in sheltered places. Shetland mouse-ear ("Cerastium nigrescens") is an endemic flowering plant found only in Shetland. It was first recorded in 1837 by botanist Thomas Edmondston. Although reported from two other sites in the 19th century, it currently grows only on two serpentine hills on the island of Unst. The nationally scarce oysterplant is found on several islands and the British Red Listed bryophyte "Thamnobryum alopecurum" has also been recorded.
Fauna.
Shetland has numerous seabird colonies. Birds found on the islands include Atlantic puffin, storm-petrel, red-throated diver, northern gannet and bonxie. Numerous rarities have also been recorded including black-browed albatross and snow goose, and a single pair of snowy owls bred on Fetlar from 1967 to 1975. The Shetland wren, Fair Isle wren and Shetland starling are subspecies endemic to Shetland. There are also populations of various moorland birds such as curlew, snipe and golden plover.
The geographical isolation and recent glacial history of Shetland have resulted in a depleted mammalian fauna and the brown rat and house mouse are two of only three species of rodent present on the islands. The Shetland field mouse is the third and the archipelago's fourth endemic subspecies, of which there are three varieties on Yell, Foula and Fair Isle. They are variants of "Apodemus sylvaticus" and archaeological evidence suggests that this species was present during the Middle Iron Age (around 200 BC to AD 400). It is possible that "Apodemus" was introduced from Orkney where a population has existed since at the least the Bronze Age.
Domesticated animals.
There is a variety of indigenous breeds, of which the diminutive Shetland pony is probably the best known, as well as being an important part of the Shetland farming tradition. The first written record of the pony was in 1603 in the Court Books of Shetland and, for its size, it is the strongest of all the horse breeds. Others are the Shetland Sheepdog or "Sheltie", the endangered Shetland cattle and Shetland Goose and the Shetland sheep which is believed to have originated prior to 1000 AD. The Grice was a breed of semi-domesticated pig that had a habit of attacking lambs, and that became extinct in 1930.
See also.
Lists
About Shetland
Other
References.
General references.
</dl>

</doc>
<doc id="29006" url="http://en.wikipedia.org/wiki?curid=29006" title="Space observatory">
Space observatory

A space observatory is any instrument (such as a telescope) in outer space that is used for observation of distant planets, galaxies and other outer space objects. This category is distinct from other observatories located in space that are pointed toward Earth for the purpose of reconnaissance and other types of information gathering.
Introduction.
Performing astronomy from Earth's surface is limited by the filtering and distortion of electromagnetic radiation (scintillation or twinkling) due to the atmosphere. Some terrestrial telescopes (such as the Very Large Telescope) can reduce atmospheric effects with adaptive optics. A telescope orbiting Earth outside the atmosphere is subject neither to twinkling nor to light pollution from artificial light sources on Earth.
Space-based astronomy is even more important for frequency ranges which are outside the optical window and the radio window, the only two wavelength ranges of the electromagnetic spectrum that are not severely attenuated by the atmosphere. For example, X-ray astronomy is nearly impossible when done from Earth, and has reached its current importance in astronomy only due to orbiting X-ray telescopes such as the Chandra observatory and the XMM-Newton observatory. Infrared and ultraviolet are also greatly blocked.
Space observatories can generally be divided into two classes: missions which map the entire sky (surveys), and observatories which make observations of chosen parts of the sky.
Many space observatories have already completed their missions, while others continue operating, and still others are planned for the future. Satellites have been launched and operated by NASA, ISRO of India, ESA, Japanese Space Agency and the Soviet space program later succeeded by Roskosmos of Russia.
History.
In 1946, American theoretical astrophysicist Lyman Spitzer was the first to conceive the idea of a telescope in outer space, a decade before the Soviet Union launched the first satellite, "Sputnik 1".
Spitzer's proposal called for a large telescope that would not be hindered by Earth's atmosphere. After lobbying in the 1960s and 70s for such a system to be built, Spitzer's vision ultimately materialized into the Hubble Space Telescope, which was launched on April 20, 1990 by the Space Shuttle "Discovery" (STS-31).

</doc>
<doc id="29021" url="http://en.wikipedia.org/wiki?curid=29021" title="Secular humanism">
Secular humanism

The philosophy or life stance of secular humanism (alternatively known by some adherents as Humanism, specifically with a capital H to distinguish it from other forms of humanism) embraces human reason, ethics, and philosophical naturalism while specifically rejecting religious dogma, supernaturalism, pseudoscience, and superstition as the basis of morality and decision making.
Secular Humanism posits that human beings are capable of being ethical and moral without religion or a god. It does not, however, assume that humans are either inherently evil or innately good, nor does it present humans as being superior to nature. Rather, the humanist life stance emphasizes the unique responsibility facing humanity and the ethical consequences of human decisions. Fundamental to the concept of secular humanism is the strongly held viewpoint that ideology—be it religious or political—must be thoroughly examined by each individual and not simply accepted or rejected on faith. Along with this, an essential part of secular humanism is a continually adapting search for truth, primarily through science and philosophy. Many Humanists derive their moral codes from a philosophy of utilitarianism, ethical naturalism, or evolutionary ethics, and some, such as Sam Harris, advocate a science of morality.
The International Humanist and Ethical Union (IHEU) is the world union of more than one hundred Humanist, rationalist, irreligious, atheistic, Bright, secular, Ethical Culture, and freethought organizations in more than 40 countries. The "Happy Human" is the official symbol of the IHEU as well as being regarded as a universally recognised symbol for those who call themselves Humanists. Secular humanist organizations are found in all parts of the world. Those who call themselves humanists are estimated to number between four and five million people worldwide.
Terminology.
The meaning of the phrase "secular humanism" has evolved over time. The phrase has been used since at least the 1930s, and in 1943, the then Archbishop of Canterbury, William Temple, was reported as warning that the "Christian tradition... was in danger of being undermined by a 'Secular Humanism' which hoped to retain Christian values without Christian faith." During the 1960s and 1970s the term was embraced by some humanists who considered themselves anti-religious, as well as those who, although not critical of religion in its various guises, preferred a non-religious approach. The release in 1980 of "A Secular Humanist Declaration" by the newly formed Council for Democratic and Secular Humanism (CODESH, now the Council for Secular Humanism) gave secular humanism an organisational identity within the United States.
However, many adherents of the approach reject the use of the word "secular" as obfuscating and confusing, and consider that the term "secular humanism" has been "demonized by the religious right... All too often secular humanism is reduced to a sterile outlook consisting of little more than secularism slightly broadened by academic ethics. This kind of 'hyphenated humanism' easily becomes more about the adjective than its referent". Adherents of this view, including the International Humanist and Ethical Union and the American Humanist Association, consider that the unmodified but capitalised word Humanism should be used. The endorsement by the IHEU of the capitalization of the word "Humanism", and the dropping of any adjective such as "secular", is quite recent. The American Humanist Association began to adopt this view in 1973, and the IHEU formally endorsed the position in 1989. In 2002 the IHEU General Assembly unanimously adopted the Amsterdam Declaration, which represents the official defining statement of World Humanism for Humanists. This declaration makes exclusive use of capitalized "Humanist" and "Humanism", which is consistent with IHEU's general practice and recommendations for promoting a unified Humanist identity.[#endnote_] To further promote Humanist identity, these words are also free of any adjectives, as recommended by prominent members of IHEU.[#endnote_] Such usage is not universal among IHEU member organizations, though most of them do observe these conventions.
History.
Historical use of the term humanism (reflected in some current academic usage), is related to the writings of pre-Socratic philosophers. These writings were lost to European societies until Renaissance scholars rediscovered them through Muslim sources and translated them from Arabic into European languages. Thus the term humanist can mean a humanities scholar, as well as refer to The Enlightenment/ Renaissance intellectuals, and those who have agreement with the pre-Socratics, as distinct from secular humanists.
Secularism.
The term secularism was coined in 1851 by George Jacob Holyoake to describe "a form of opinion which concerns itself only with questions, the issues of which can be tested by the experience of this life."
The modern secular movement coalesced around Holyoake, Charles Bradlaugh and their intellectual circle. The first secular society was Leicester Secular Society, established in 1851. Similar regional societies came together to form the National Secular Society in 1866.
Positivism & the Church of Humanity.
Holyoake's secularism was strongly influenced by Auguste Comte, the founder of positivism and of modern sociology. Comte believed human history would progress in a "law of three stages" from a theological phase, to the "metaphysical", toward a fully rational "positivist" society. In later life, Comte had attempted to introduce a "religion of humanity" in light of growing anti-religious sentiment and social malaise in revolutionary France. This religion would necessarily fulfil the functional, cohesive role that supernatural religion once served.
Although Comte's religious movement was unsuccessful in France, the positivist philosophy of science itself played a major role in the proliferation of secular organizations in the 19th century in England. Richard Congreve visited Paris shortly after the French Revolution of 1848 where he met Auguste Comte and was heavily influenced by his positivist system. He founded the London Positivist Society in 1867, which attracted Frederic Harrison, Edward Spencer Beesly, Vernon Lushington, and James Cotter Morison amongs others.
In 1878, the Society established the Church of Humanity under Congreve's direction. There they introduced sacraments of the Religion of Humanity and published a co-operative translation of Comte's Positive Polity. When Congreve repudiated their Paris co-religionists in 1878, Beesly, Harrison, Bridges, and others formed their own positivist society, with Beesly as president, and opened a rival centre, Newton Hall, in a courtyard off Fleet Street.
The New York City version of the church was established by English immigrant Henry Edger. The American version of the "Church of Humanity." was largely modeled on the English church. Like the English version it wasn't atheistic and had sermons and sacramental rites. At times the services included readings from conventional religious works like the Book of Isaiah. It was not as significant as the church in England, but did include several educated people.
Ethical movement.
Another important precursor was the ethical movement of the 19th century. The South Place Ethical Society was founded in 1793 as the South Place Chapel on Finsbury Square, on the edge of the City of London, and in the early nineteenth century was known as "a radical gathering-place. At that point it was a Unitarian chapel, and that movement, like Quakers, supported female equality. Under the leadership of Reverend William Johnson Fox, it lent its pulpit to activists such as Anna Wheeler, one of the first women to campaign for feminism at public meetings in England, who spoke in 1829 on "Rights of Women." In later decades, the chapel changed its name to the South Place Ethical Society, now the Conway Hall Ethical Society.
In America, the ethical movement was propounded by Felix Adler, who established the New York Society for Ethical Culture in 1877. By 1886, similar societies had sprouted up in Philadelphia, Chicago and St. Louis.
These societies all adopted the same statement of principles:
In effect, the movement responded to the religious crisis of the time by replacing theology with unadulterated morality. It aimed to "disentangle moral ideas from religious doctrines, metaphysical systems, and ethical theories, and to make them an independent force in personal life and social relations." Adler was also particularly critical of the religious emphasis on creed, believing it to be the source of sectarian bigotry. He therefore attempted to provide a universal fellowship devoid of ritual and ceremony, for those who would otherwise be divided by creeds. For the same reasons the movement also adopted a neutral position on religious beliefs, advocating neither atheism nor theism, agnosticism nor deism.
The first ethical society along these lines in Britain was founded in 1886. By 1896 the four London societies formed the Union of Ethical Societies, and between 1905 and 1910 there were over fifty societies in Great Britain, seventeen of which were affiliated with the Union.
Secular humanism.
In the 1930s, "humanism" was generally used in a religious sense by the Ethical movement in the United States, and not much favoured among the non-religious in Britain. Yet "it was from the Ethical movement that the non-religious philosophical sense of "Humanism" gradually emerged in Britain, and it was from the convergence of the Ethical and Rationalist movements that this sense of "Humanism" eventually prevailed throughout the Freethought movement".
As an organized movement, Humanism itself is quite recent – born at the University of Chicago in the 1920s, and made public in 1933 with the publication of the first Humanist Manifesto. The American Humanist Association was incorporated as an Illinois non-profit organization in 1943. The International Humanist and Ethical Union was founded in 1952, when a gathering of world Humanists met under the leadership of Sir Julian Huxley. The British Humanist Association took that name in 1967, but had developed from the Union of Ethical Societies which had been founded by Stanton Coit in 1896.
Manifestos and declarations.
Humanists have put together various Humanist Manifestos, in attempts to unify the Humanist identity.
The original signers of the first Humanist Manifesto of 1933, declared themselves to be religious humanists. Because, in their view, traditional religions were failing to meet the needs of their day, the signers of 1933 declared it a necessity to establish a religion that was a dynamic force to meet the needs of the day. However, this "religion" did not profess a belief in any god. Since then two additional Manifestos were written to replace the first. In the Preface of Humanist Manifesto II, in 1973, the authors Paul Kurtz and Edwin H. Wilson assert that faith and knowledge are required for a hopeful vision for the future. Manifesto II references a section on Religion and states traditional religion renders a disservice to humanity. Manifesto II recognizes the following groups to be part of their naturalistic philosophy: "scientific", "ethical", "democratic", "religious", and "Marxist" humanism.
International Humanist and Ethical Union.
In 2002, the IHEU General Assembly unanimously adopted the Amsterdam Declaration 2002 which represents the official defining statement of World Humanism.
All member organisations of the International Humanist and Ethical Union are required by bylaw 5.1 to accept the "Minimum Statement on Humanism": 
To promote and unify "Humanist" identity, prominent members of the IHEU have endorsed the following statements on Humanist identity:[#endnote_]
Council for Secular Humanism.
According to the Council for Secular Humanism, within the United States, the term "secular humanism" describes a world view with the following elements and principles:
"A Secular Humanist Declaration" was issued in 1980 by the Council for Secular Humanism's predecessor, CODESH. It lays out ten ideals: Free inquiry as opposed to censorship and imposition of belief; separation of church and state; the ideal of freedom from religious control and from jingoistic government control; ethics based on critical intelligence rather than that deduced from religious belief; moral education; religious skepticism; reason; a belief in science and technology as the best way of understanding the world; evolution; and education as the essential method of building humane, free, and democratic societies.
American Humanist Association.
General doctrines of Humanism are also set out in the "Humanist Manifesto" prepared by the American Humanist Association.
Ethics and relationship to religious belief.
In the 20th and 21st centuries, members of Humanist organizations have disagreed as to whether Humanism is a religion. They categorize themselves in one of three ways. Religious Humanism, in the tradition of the earliest Humanist organizations in the UK and US, attempts to fulfill the traditional social role of religion. Secular humanism considers all forms of religion, including religious Humanism, to be superseded. In order to sidestep disagreements between these two factions, recent Humanist proclamations define Humanism as a "life stance"; proponents of this view making up the third faction. All three types of Humanism (and all three of the American Humanist Association's manifestos) reject deference to supernatural beliefs; promoting the practical, methodological naturalism of science, but also going further and supporting the philosophical stance of metaphysical naturalism. The result is an approach to issues in a secular way. Humanism addresses ethics without reference to the supernatural as well, attesting that ethics is a human enterprise (see naturalistic ethics).
Secular humanism does not prescribe a specific theory of morality or code of ethics. As stated by the Council for Secular Humanism,
It should be noted that Secular Humanism is not so much a specific morality as it is a method for the explanation and discovery of rational moral principles.
Secular humanism affirms that with the present state of scientific knowledge, dogmatic belief in an absolutist moral/ethical system (e.g. Kantian, Islamic, Christian) is unreasonable. However, it affirms that individuals engaging in rational moral/ethical deliberations can discover some universal "objective standards".
We are opposed to absolutist morality, yet we maintain that objective standards emerge, and ethical values and principles may be discovered, in the course of ethical deliberation.
Many Humanists adopt principles of the Golden Rule. Some believe that universal moral standards are required for the proper functioning of society. However, they believe such necessary universality can and should be achieved by developing a richer notion of morality through reason, experience and scientific inquiry rather than through faith in a supernatural realm or source.
Fundamentalists correctly perceive that universal moral standards are required for the proper functioning of society. But they erroneously believe that God is the only possible source of such standards. Philosophers as diverse as Plato, Immanuel Kant, John Stuart Mill, George Edward Moore, and John Rawls have demonstrated that it is possible to have a universal morality without God. Contrary to what the fundamentalists would have us believe, then, what our society really needs is not more religion but a richer notion of the nature of morality.
Humanism is compatible with atheism
and agnosticism,
but being atheist or agnostic does not, itself, make one a Humanist. Nevertheless, humanism is diametrically opposed to state atheism.
According to Paul Kurtz, considered by some to be the founder of the American secular humanist movement, one of the differences between Marxist–Leninist atheists and humanists is the latter's commitment to "human freedom and democracy" while stating that the militant atheism of the Soviet Union consistently violated basic human rights.
Kurtz also stated that the "defense of religious liberty is as precious to the humanist as are the rights of the believers". Greg M. Epstein states that, "modern, organized Humanism began, in the minds of its founders, as nothing more nor less than a religion without a God".
Many Humanists address ethics from the point of view of ethical naturalism, and some support an actual science of morality. Some philosophers like Peter Singer see Humanism as speciesist and lend themselves to more of a Personism.
Modern context.
Secular humanist organizations are found in all parts of the world. Those who call themselves humanists are estimated to number between four and five million people worldwide in 31 countries, but there is uncertainty because of the lack of universal definition throughout censuses. Humanism is a non-theistic belief system and, as such, it could be a sub-category of "Religion" only if that term is defined to mean "Religion and (any) belief system". This is the case in the International Covenant on Civil and Political Rights on freedom of religion "and" beliefs. Many national censuses contentiously define Humanism as a further sub-category of the sub-category "No Religion", which typically includes atheist, rationalist and agnostic thought. In England, Wales 25% of people specify that they have 'No religion' up from 15% in 2001 and in Australia, around 15% of the population specifies "No Religion" in the national census. However, in its 2006 and 2011 census Australia used Humanism as an example of "other religions". In the USA, the decennial census does not inquire about religious affiliation or its lack; surveys report the figure at roughly 13%. In the 2001 Canadian census, 16.5% of the populace reported having no religious affiliation. In Scotland, the figure is 28%. One of the largest Humanist organizations in the world (relative to population) is Norway's "Human-Etisk Forbund", which had over 86,000 members out of a population of around 4.6 million in 2013 - approximately 1.9% of the population.
The International Humanist and Ethical Union (IHEU) is the worldwide umbrella organization for those adhering to the Humanist life stance. It represents the views of over three million Humanists organized in over 100 national organizations in 30 countries. Originally based in the Netherlands, the IHEU now operates from London. Some regional groups that adhere to variants of the Humanist life stance, such as the humanist subgroup of the Unitarian Universalist Association, do not belong to the IHEU. Although the European Humanist Federation is also separate from the IHEU, the two organisations work together and share an agreed protocol.
Starting in the mid-20th century, religious fundamentalists and the religious right began using the term "secular humanism" in hostile fashion. Francis A. Schaeffer, an American theologian based in Switzerland, seizing upon the exclusion of the divine from most humanist writings, argued that rampant secular humanism would lead to moral relativism and ethical bankruptcy in his book "How Should We Then Live: The Rise and Decline of Western Thought and Culture" (1976). Schaeffer portrayed secular humanism as pernicious and diabolical, and warned it would undermine the moral and spiritual tablet of America. His themes have been very widely repeated in Fundamentalist preaching in North America. Toumey (1993) found that secular humanism is typically portrayed as a vast evil conspiracy, deceitful and immoral, responsible for feminism, pornography, abortion, homosexuality, and New Age spirituality. In certain areas of the world, Humanism finds itself in conflict with religious fundamentalism, especially over the issue of the separation of church and state. Many Humanists see religions as superstitious, repressive and closed-minded, while religious fundamentalists may see Humanists as a threat to the values set out in their sacred texts.
Humanist celebrations.
Some Humanists celebrate official religion-based public holidays, such as Christmas or Easter, but as secular holidays rather than religious ones. Many Humanists also celebrate the winter and summer solstice, the former of which (in the northern hemisphere) coincides closely with the religiously-oriented celebration of Christmas, and the equinoxes, of which the vernal equinox is associated with Christianity's Easter and indeed with all other springtime festivals of renewal, and the autumnal equinox which is related to such celebrations such as Halloween and All Souls' Day. The Society for Humanistic Judaism celebrates most Jewish holidays in a secular manner.
The IHEU endorses World Humanist Day (21 June), Darwin Day (12 February), Human Rights Day (10 December) and HumanLight (23 December) as official days of Humanist celebration, though none are yet a public holiday.
In many countries, Humanist officiants (or celebrants) perform celebrancy services for weddings, funerals, child namings, coming of age ceremonies, and other rituals.
Legal mentions in the United States.
The issue of whether and in what sense secular humanism might be considered a religion, and what the implications of this would be has become the subject of legal maneuvering and political debate in the United States. The first reference to "secular humanism" in a US legal context was in 1961, although church-state separation lawyer Leo Pfeffer had referred to it in his 1958 book, "Creeds in Competition".
Hatch amendment.
The Education for Economic Security Act of 1984 included a section, Section 20 U.S.C.A. 4059, which initially read: "Grants under this subchapter ['Magnet School Assistance'] may not be used for consultants, for transportation or for any activity which does not augment academic improvement." With no public notice, Senator Orrin Hatch tacked onto the proposed exclusionary subsection the words "or for any course of instruction the substance of which is Secular Humanism". Implementation of this provision ran into practical problems because neither the Senator's staff, nor the Senate's Committee on Labor and Human Resources, nor the Department of Justice could propose a definition of what would constitute a "course of instruction the substance of which is Secular Humanism". So, this determination was left up to local school boards. The provision provoked a storm of controversy which within a year led Senator Hatch to propose, and Congress to pass, an amendment to delete from the statute all reference to secular humanism. While this episode did not dissuade fundamentalists from continuing to object to what they regarded as the "teaching of Secular Humanism", it did point out the vagueness of the claim.
Case law.
"Torcaso v. Watkins".
The phrase "secular humanism" became prominent after it was used in the United States Supreme Court case "Torcaso v. Watkins." In the 1961 decision, Justice Hugo Black commented in a footnote, "Among religions in this country which do not teach what would generally be considered a belief in the existence of God are Buddhism, Taoism, Ethical Culture, Secular Humanism, and others."
"Fellowship of Humanity v. County of Alameda".
The footnote in "Torcaso v. Watkins" referenced "Fellowship of Humanity v. County of Alameda", a 1957 case in which an organization of humanists sought a tax exemption on the ground that they used their property "solely and exclusively for religious worship." Despite the group's non-theistic beliefs, the court determined that the activities of the "Fellowship of Humanity", which included weekly Sunday meetings, were analogous to the activities of theistic churches and thus entitled to an exemption. The "Fellowship of Humanity" case itself referred to "Humanism" but did not mention the term "secular humanism". Nonetheless, this case was cited by Justice Black to justify the inclusion of secular humanism in the list of religions in his note. Presumably Justice Black added the word "secular" to emphasize the non-theistic nature of the "Fellowship of Humanity" and distinguish their brand of humanism from that associated with, for example, Christian humanism.
"Washington Ethical Society v. District of Columbia".
Another case alluded to in the "Torcaso v. Watkins" footnote, and said by some to have established secular humanism as a religion under the law, is the 1957 tax case of "", 249 F.2d 127 (D.C. Cir. 1957). The "Washington Ethical Society" functions much like a church, but regards itself as a non-theistic religious institution, honoring the importance of ethical living without mandating a belief in a supernatural origin for ethics. The case involved denial of the Society's application for tax exemption as a religious organization. The U.S. Court of Appeals reversed the Tax Court's ruling, defined the Society as a religious organization, and granted its tax exemption. The Society terms its practice Ethical Culture. Though Ethical Culture is based on a humanist philosophy, it is regarded by some as a type of religious humanism. Hence, it would seem most accurate to say that this case affirmed that a religion need not be theistic to qualify as a religion under the law, rather than asserting that it established generic secular humanism as a religion.
In the cases of both the "Fellowship of Humanity" and the "Washington Ethical Society," the court decisions turned not so much on the particular beliefs of practitioners as on the function and form of the practice being similar to the function and form of the practices in other religious institutions.
"Peloza v. Capistrano School District".
The implication in Justice Black's footnote that secular humanism is a religion has been seized upon by religious opponents of the teaching of evolution, who have made the argument that teaching evolution amounts to teaching a religious idea. The claim that secular humanism could be considered a religion for legal purposes was examined by the United States Court of Appeals for the Ninth Circuit in "Peloza v. Capistrano School District", 37 F.3d 517 (9th Cir. 1994), "cert. denied", 515 U.S. 1173 (1995). In this case, a science teacher argued that, by requiring him to teach evolution, his school district was forcing him to teach the "religion" of secular humanism. The Court responded, "We reject this claim because neither the Supreme Court, nor this circuit, has ever held that evolutionism or Secular Humanism are 'religions' for Establishment Clause purposes." The Supreme Court refused to review the case.
The decision in a subsequent case, "Kalka v. Hawk et al.", offered this commentary:
Controversy.
Decisions about tax status have been based on whether an organization functions like a church. On the other hand, Establishment Clause cases turn on whether the ideas or symbols involved are inherently religious. An organization can function like a church while advocating beliefs that are not necessarily inherently religious. Author Marci Hamilton has pointed out: "Moreover, the debate is not between secularists and the religious. The debate is believers and non-believers on the one side debating believers and non-believers on the other side. You've got citizens who are [...] of faith who believe in the separation of church and state and you have a set of believers who do not believe in the separation of church and state."
In the 1987 case of "Smith v. Board of School Commissioners of Mobile County" a group of plaintiffs brought a case alleging that the school system was teaching the tenets of an anti-religious religion called "secular humanism" in violation of the Establishment Clause. The complainants asked that 44 different elementary through high school level textbooks (including books on home economics, social science and literature) be removed from the curriculum. Federal judge William Brevard Hand ruled for the plaintiffs agreeing that the books promoted secular humanism, which he ruled to be a religion. The Eleventh Circuit Court unanimously reversed him, with Judge Frank stating that Hand held a "misconception of the relationship between church and state mandated by the establishment clause," commenting also that the textbooks did not show "an attitude antagonistic to theistic belief. The message conveyed by these textbooks is one of neutrality: the textbooks neither endorse theistic religion as a system of belief, nor discredit it."
Manifestos.
There are numerous Humanist Manifestos and Declarations, including the following:

</doc>
<doc id="29042" url="http://en.wikipedia.org/wiki?curid=29042" title="Superfetation">
Superfetation

Superfetation (also spelled superfoetation – see fetus) is the simultaneous occurrence of more than one stage of developing offspring in the same animal. In mammals, it manifests as the formation of an embryo from a different estrous cycle while another embryo or fetus is already present in the uterus. When two separate instances of fertilisation occur during the same menstrual cycle, it is known as superfecundation.
Superfetation is claimed to be common in some species of animals, but is extremely rare in humans. In mammals, it can occur only where there are two uteri, or where the estrous cycle continues through pregnancy. The risk with superfetation in humans is that the second baby is often born prematurely, which can increase its odds of experiencing lung development problems.
In animals.
Animals that have been claimed to be subject to superfetation include rodents (mice and rats), rabbits, farm animals (horses and sheep), marsupials (kangaroos and sugar gliders), and primates (humans). Superfetation has also been clearly demonstrated and is normal for some species of poeciliid fishes.
In humans.
Reports of superfetation occurring long after the first impregnation have often been treated with suspicion, and some have been clearly discredited. Other explanations have been given (and demonstrated) for different levels of development between twins. Artificially induced superfetation has, however, been demonstrated, although only up to a short period after insemination. 
In 1960, John and Mary Tress of Baltimore MD had what the nurse called twins. She was wrong. Dr Paul C Weinberg of Mt. Sinai hospital delivered the boys, Anthony John and Mark Francis, and realized that Anthony, born five minutes before his brother Mark, looked premature. Immediately, Dr Weinberg did X-rays of the boys' thigh bones and noticed a disparity in bone age. Mark was a full-term baby born five minutes after his two-months premature brother Anthony. Anthony was conceived a full two months after his brother Mark. 
In 2007, Ame and Lia Herrity, conceived three weeks apart, were born in the United Kingdom to Amelia Spence and George Herrity.
In May 2007, Harriet and Thomas Mullineux, also conceived three weeks apart, were born in Benfleet, Essex, to Charlotte and Matt Mullineux.
In 2009, Todd and Julia Grovenburg of Fort Smith, Arkansas, received international media attention for Mrs. Grovenburg's conception of an additional child while already pregnant with a child conceived two and a half weeks earlier. If it were possible to carry both children to term, the birth of the first child would be expected in December 2009, whereas the second child would be due in January 2010. Grovenburg's obstetrician reported that cases of superfetation "can only be confirmed after delivery by chromosomal and metabolic studies on the baby." Both healthy babies were delivered through Caesarean section on 2 December 2009.

</doc>
<doc id="29043" url="http://en.wikipedia.org/wiki?curid=29043" title="Steenbeck">
Steenbeck

Steenbeck is a brand name that has become synonymous with a type of flatbed film editing suite which is usable with both 16 mm and 35 mm optical sound and magnetic sound film.
The Steenbeck company was founded in 1931 by Wilhelm Steenbeck in Hamburg, Germany. Since then, the name Steenbeck has become widely known in the film editing community and more than 25,000 machines are in operation around the world. The company relocated to Venray (The Netherlands) in September 2003, where it still manufactures editing tables.
Despite the move away from physical film stock – much editing is now based on digital media – devices such as the Lightworks non-linear film editing controller and archives still use the Steenbeck physical layout for controlling the process. The Steenbeck's lower light levels and controllable speed make it a preferred piece of equipment for film archives (such as the Library Of Congress' motion picture collection) and restoration facilities as prints can be quickly and easily inspected with less risk of damage compared to a movie projector. Because there is no intermittent movement, the image is created through a rotating prism which scans the frames. Steenbeck machines were known to be exceptionally easy on film stock, due to their use of soft-edged nylon rollers.

</doc>
<doc id="29047" url="http://en.wikipedia.org/wiki?curid=29047" title="Steelman language requirements">
Steelman language requirements

The Steelman language requirements were a set of requirements which a high-level general-purpose programming language should meet, created by the United States Department of Defense in "The Department of Defense Common High Order Language program" in 1978. The predecessors of this document were called, in order, "Strawman", "Woodenman", "Tinman" and "Ironman".
The requirements focused on the needs of embedded computer applications, and emphasised reliability, maintainability, and efficiency. Notably, they included exception handling facilities, run-time checking, and parallel computing.
It was concluded that no existing language met these criteria to a sufficient extent, so a contest was called to create a language that would be closer to fulfilling them. The design that won this contest became the Ada programming language.
The resulting language followed the Steelman requirements closely, though not exactly. 
The Ada 95 revision of the language went beyond the Steelman requirements, targeting general-purpose systems in addition to embedded ones, and adding features supporting object-oriented programming.

</doc>
<doc id="29049" url="http://en.wikipedia.org/wiki?curid=29049" title="SSB">
SSB

SSB may refer to:

</doc>
<doc id="29064" url="http://en.wikipedia.org/wiki?curid=29064" title="Slave (disambiguation)">
Slave (disambiguation)

A slave is a person owned or entrapped by another.
Slave may also refer to:

</doc>
<doc id="29068" url="http://en.wikipedia.org/wiki?curid=29068" title="Sturgeon-class submarine">
Sturgeon-class submarine

The Sturgeon"-class (known colloquially in naval circles as the 637"-class) was a class of nuclear-powered fast attack submarines (SSN) in service with the United States Navy from the 1960s until 2004. They were the "workhorses" of the Navy's attack submarine fleet throughout much of the Cold War. The boats were phased out in the 1990s and early 21st century, as their successors, the "Los Angeles", followed by the "Seawolf" and "Virginia"-class boats, entered service.
Design.
The "Sturgeon"s were essentially lengthened and improved variants of the "Thresher/Permit" class that directly preceded them. The five-compartment arrangement of the "Permit"s was retained, including the bow compartment, operations compartment, reactor compartment, auxiliary machinery room no. 2, and the engine room. The extra length was in the operations compartment, including longer torpedo racks to accommodate additional Mark 37 torpedoes, the most advanced in service at the time of the class's design in the late 1950s. The class was designed to SUBSAFE requirements, with seawater, main ballast, and other systems redesigned for improved safety. The biggest difference was the much larger sail, which permitted a second periscope and additional intelligence-gathering masts. The fairwater planes mounted on the sail could rotate 90 degrees, allowing the submarine to surface through thin ice. Because the S5W reactor was used, the same as in the "Skipjack"s and "Thresher/Permit"s, and the displacement was increased, the "Sturgeon"s' top speed was 26 kn, 2 knots slower than the "Thresher/Permit"s.
The last nine "Sturgeon"s were lengthened 10 ft to provide more space for electronic equipment and habitability. The extra space also helped facilitate the use of dry deck shelters first deployed in 1982.
The class received mid-life upgrades in the 1980s, including the BQQ-5 sonar suite with a retractable towed array, Mk 117 torpedo fire control equipment, and other electronics upgrades.
Armament.
The "Sturgeon"-class boats were equipped to carry the Harpoon missile, the Tomahawk cruise missile, the UUM-44 SUBROC, the Mark 67 SLMM and Mark 60 CAPTOR mines, and the MK-48 and ADCAP torpedoes. Torpedo tubes were located amidships to accommodate the bow-mounted sonar. The bow covering the sonar sphere was made from steel or glass reinforced plastic (GRP), both varieties having been produced both booted and not booted. Booted domes are covered with a half-inch layer of rubber. The GRP domes improved the bow sonar sphere performance; though for intelligence gathering missions, the towed-array sonar was normally used as it was a much more sensitive array.
Noise reduction.
Several "Sturgeon" boats and related submarines were modifications of the original designs to test ways to reduce noise.
Variants.
Beginning with "Archerfish", units of this class had a 10-foot (3 meter) longer hull, giving them more living and working space than previous submarines. "Parche" received an additional 100-foot (30 meter) hull extension containing cable tapping equipment that brought her total length to 401 feet (122 m). A number of the long hull "Sturgeon"-class SSNs, including "Parche", "Rivers", and "Russell" were involved in top-secret reconnaissance missions, including cable tap operations in the Barents and Okhotsk seas. "Parche" received seven Presidential Unit Citations for successful missions.
A total of seven boats were modified to carry the SEAL Dry Deck Shelter (DDS). The DDS is a submersible launch hangar with a lockout chamber attached to the ship's midships weapons shipping hatch, facilitating the use of SEAL Delivery Vehicles. DDS-equipped boats were tasked with the covert insertion of special forces troops.
Boats.
From "Register of Ships of the US Navy, 1775-1990".
Derivatives.
Two other Navy vessels, both considered one-ship classes, were based on the "Sturgeon" hull, but were modified for experimental reasons:

</doc>
<doc id="29139" url="http://en.wikipedia.org/wiki?curid=29139" title="Super Bowl XIII">
Super Bowl XIII

Super Bowl XIII was an American football game between the American Football Conference (AFC) champion Pittsburgh Steelers and the National Football Conference (NFC) champion Dallas Cowboys to decide the National Football League (NFL) champion for the 1978 season. The Steelers defeated the Cowboys by the score of 35–31. The game was played on January 21, 1979, at the Orange Bowl in Miami, Florida, the fifth and last time that the Super Bowl was played in that stadium.
This was the first Super Bowl that featured a rematch of a previous one (the Steelers had previously beaten the Cowboys, 21–17, in Super Bowl X), and both teams were attempting to be the first club to ever win a third Super Bowl. Dallas was also the defending Super Bowl XII champion, and finished the 1978 regular season with a 12–4 record, and posted playoff victories over the Atlanta Falcons and the Los Angeles Rams. Pittsburgh entered the game after posting a 14–2 regular season record and playoff wins over the Denver Broncos and the Houston Oilers.
Steelers quarterback Terry Bradshaw, who was named Super Bowl MVP, completed 17 out of 30 passes for Super Bowl records of 318 passing yards and 4 touchdown passes. Bradshaw eclipsed Bart Starr's Super Bowl record for passing yards in the first half with 253 yards in the air as the Steelers led 21–14 at intermission. His 75-yard touchdown pass in the second quarter also tied Johnny Unitas in Super Bowl V for the longest pass in a Super Bowl. The Cowboys were able to stay close, only trailing 21–17 at the end of the third quarter, but Pittsburgh scored two touchdowns in a span of 19 seconds in the fourth period. Dallas also could not overcome turnovers, drops, and a controversial penalty during the second half. The Cowboys were eventually able to score two touchdowns in the final minutes of the game, but still ended up being the first defending champion to lose in the Super Bowl and the first losing Super Bowl team to score 30 points or more.
The game came to be known as "Black Sunday" in Las Vegas. The point spread opened at Pittsburgh -3.5 points. As the Steelers backers placed bets on them the sportsbooks adjusted the line. It eventually hit Pittsburgh -4.5 and then the Dallas money poured in on the Cowboys. It eventually settled at Pittsburgh -4. The game's final outcome of Pittsburgh 35 Dallas 31 meant the Las Vegas sportsbooks lost the vast majority of wagers on the game.
Background.
The NFL awarded Super Bowl XIII to Miami on June 14, 1977 at the owners meetings held in New York.
For the 1978–79 season, the NFL extended its schedule from 14 regular season games to 16, and increased the playoffs from an 8-team tournament to 10, creating two extra playoff games. The three division winners from each conference would be ranked first through third and be given a week off, and two wild card teams from each conference, seeded fourth and fifth, would play a playoff game with the winner going on to play the first seeded team (or, if they were in the same division, the second seed).
Pittsburgh Steelers.
The Steelers joined the Cowboys in their attempt to be the first team to ever win a third Super Bowl, after wins in Super Bowl IX and Super Bowl X. Pittsburgh quarterback Terry Bradshaw had the best season of his career, completing 207 of 368 passes for 2,915 yards and 28 touchdowns, with 20 interceptions. He ranked as the second highest rated passer in the league (84.8), his 28 touchdown passes led the league, and he won the NFL Most Valuable Player Award. Wide receivers Lynn Swann and John Stallworth provided the team with a great deep threat. Swann recorded 61 receptions for 880 yards and 11 touchdowns, while Stallworth had 41 receptions for 798 yards and 9 touchdowns. Tight end Randy Grossman, who replaced injured starter Bennie Cunningham for most of the season, also was a big factor, recording 37 receptions for 448 yards and a touchdown.
In the Steelers' rushing game, fullback Franco Harris was the team's leading rusher for the 7th consecutive season, recording 1,082 yards and 8 touchdowns, while also catching 22 passes for another 144 yards. Halfback Rocky Bleier had 633 rushing yards and 5 touchdowns, while also catching 17 passes for 168 yards. The Steelers' success on offense was due in large measure to their stellar offensive line, anchored by future Hall of Fame center Mike Webster.
Although Pittsburgh's "Steel Curtain" defense had some new starters this season, such as linemen John Banaszak and Steve Furness, and defensive back Tony Dungy, they finished No. 1 in fewest points allowed (195), second in the league against the run (allowing 107.8 yards per game), and ranked third in fewest total yards allowed (4,529). Once again, defensive tackles Joe Greene and L. C. Greenwood anchored the line, while Pro Bowl linebackers Jack Ham and Jack Lambert combined for 7 interceptions. Dungy lead the team with 6 interceptions, while the rest of the secondary, defensive backs Mel Blount, Donnie Shell, and Ron Johnson, combined for 11.
Dallas Cowboys.
The Cowboys became the first team to appear in five Super Bowls (after playing in Super Bowls V, VI, X and XII). Dallas led the league in scoring (384) and was No. 2 in total yards (5,959). The defending Super Bowl champions were once again led by quarterback Roger Staubach. Staubach finished the season as the top rated passer in the NFL (84.9) by throwing 231 out of 413 completions for 3,190 yards and 25 touchdowns, with 16 interceptions. He also rushed for 182 yards and another touchdown. Wide receivers Drew Pearson and Tony Hill provided the deep passing threats, combining for 90 receptions, 1,537 yards, and 7 touchdowns. Tight end Billy Joe DuPree contributed 34 receptions for 509 yards and 9 touchdowns. Running back Tony Dorsett had another fine season, recording a total of 1,703 combined rushing and receiving yards, and scoring a total of 9 touchdowns. Fullback Robert Newhouse and halfback Preston Pearson also contributed from the offensive backfield, combining for 1,326 rushing and receiving yards, while Newhouse also scored 10 touchdowns. The Cowboys also had a superb offensive line, led by Herbert Scott and 12-time Pro Bowler Rayfield Wright.
The Cowboys' "Doomsday Defense" finished the season as the top ranked defense in the league against the run by only allowing 107.6 yards per game, 2nd in total yards allowed (4,009), and 3rd in points allowed (208). Pro Bowl linemen Ed "Too Tall" Jones, Harvey Martin, and Randy White anchored the line, leading the league with 58 sacks, while linebackers Bob Breunig, D. D. Lewis and Thomas "Hollywood" Henderson provided solid support. Their secondary, led by safeties Cliff Harris and Charlie Waters, along with cornerbacks Benny Barnes and Aaron Kyle, combined for 16 interceptions.
The Cowboys started the regular season slowly, winning only six of their first ten games. But Dallas finished strong, winning their last six regular season games to post a 12–4 record.
Playoffs.
Dallas marched through the playoffs, defeating the Atlanta Falcons, 27–20, and the Los Angeles Rams, 28–0. Meanwhile, the Steelers easily demolished the Denver Broncos, 33–10, and the Houston Oilers, 34–5.
Super Bowl pregame news and notes.
Although the Super Bowl had grown into America's biggest one-day sporting event by this point, many believe the 13th edition began the game's evolution to unofficial national holiday. It was the first Super Bowl with a true heavyweight title-fight feel, given the Steelers' and Cowboys' unquestioned status as the two best teams in the NFL, and the honor of the first three-time Super Bowl champion that would go to the winner.
Super Bowl XIII can arguably be called the greatest collection of NFL talent ever to gather for a game. In addition to Coaches Noll and Landry, 14 players would end up being voted into the Hall of Fame: Nine Pittsburgh players: Bradshaw, Harris, Swann, Stallworth, Webster, Greene, Lambert, Ham, and Blount, and five from Dallas: Staubach, Dorsett, White, Wright, and Jackie Smith, who the Cowboys lured out of retirement from the St. Louis Cardinals due to injuries to Cowboy tight ends, most notably, Jay Saldi. Other Hall of Famers who participated in the game for Dallas were: GM/President Tex Schramm, and defensive coordinator Ernie Stautner, who actually was a HOF defensive tackle for the Steelers. Additional Hall of Famers from Pittsburgh included owner Art Rooney, Sr., and son Dan Rooney.
This was the first Super Bowl in which the designated "home" team was allowed to select between their team colored jersey or their white jersey, a rule similar to that of home games in the regular season and playoffs. Previously, the designated "home" team was required to wear their team colored jersey. The Cowboys, who traditionally wear their white jerseys in home games and often only wear their blue jerseys against teams that have similar policies for themselves (most notably against the Washington Redskins and occasionally the Philadelphia Eagles), were forced to wear their blue jerseys as the "home" team in Super Bowl V, which the team lost to the Baltimore Colts and is widely believed where the "blue jersey jinx" started with America's Team. Not wanting a repeat of that being the designated "home" team in Super Bowl XIII, the Cowboys were able to persuade the NFL to change the rule to allow the "home" team to choose so that they could wear their white jerseys. The Cowboys would later repeat the option of wearing white jerseys as the "home" team in Super Bowl XXVII, while the Redskins would do so in Super Bowl XVII and, ironically, the Steelers (who always wear their black jerseys in home games) in Super Bowl XL due to the team's success on the road that season.
The Cowboys were playing their 3rd Super Bowl at the Orange Bowl, the first team to play three different Super Bowls in the same stadium. The New England Patriots have since done the same playing three Super Bowls at the Mercedes-Benz Superdome. The Cowboys are 0–3 in such games and 5–0 in their other Super Bowls.
Much of the pregame hype surrounded Super Bowl XIII centered around Cowboys linebacker Thomas "Hollywood" Henderson. Henderson caused quite a stir before the NFC Championship Game by claiming that the Rams had "no class" and the Cowboys would shut them out. His prediction turned out to be very accurate; the Cowboys did shut them out, aided by Henderson's 68-yard interception return for a touchdown. In the days leading up the Super Bowl, Henderson began talking about the Steelers in the same manner. He predicted another shutout and then made unflattering comments about several Pittsburgh players. He put down the talent and the intelligence of Bradshaw, proclaiming "Bradshaw couldn't spell 'cat' if you spotted him the 'c' and the 'a'." But the Steelers refused to get into a war of words with Henderson. Greene responded by saying the Steelers didn't need to say they were the best, they would just go out on the field and ""get the job done.'"
The matchup of quarterbacks Terry Bradshaw and Roger Staubach is still the only one in Super Bowl history to feature two quarterbacks with two Super Bowl victories. With this start, Staubach became the first quarterback to start four Super Bowls. Bradshaw joined Fran Tarkenton, Bob Griese as well as Staubach as the only quarterbacks to start at least three Super Bowls. The only quarterbacks to start more Super Bowls than Staubach and Bradshaw are John Elway and Tom Brady, who guided the Broncos and Patriots to five and six Super Bowls respectively. 49ers quarterback Joe Montana and Bills quarterback Jim Kelly later matched Staubach and Bradshaw by leading their teams to four Super Bowls.
Television and entertainment.
The game was televised in the United States by NBC, with Curt Gowdy handling play-by-play and color commentators Merlin Olsen and John Brodie. Dick Enberg served as the pregame host for the broadcast. Also taking part in NBC's coverage were Bryant Gumbel, Mike Adamle (who also covered the Vince Lombardi Trophy presentation ceremony), Donna De Varona and recently retired former Minnesota Vikings quarterback Fran Tarkenton. For this game and Super Bowl XV, NBC used a custom, synthesizer-heavy theme in place of their regular music.
This was Gowdy's seventh and final Super Bowl telecast, and his last major event for NBC before moving to CBS later in 1979. Enberg had essentially succeeded Gowdy as NBC's lead NFL play-by-play announcer in the 1978 regular season, and network producers didn't decide until nearly the last minute which man would get the Super Bowl call.
"Brothers and Sisters" premiered on NBC after the game, representing the Super Bowl lead-out program.
The game was later featured on NFL's Greatest Games as Battle of Champions.
The pregame festivities featured the Dallas Cowboys Cheerleaders and several military bands. The Colgate Thirteen performed the national anthem, while the Stetson University Army ROTC Color Guard presented the Colors. The coin toss ceremony featured Pro Football Hall of Famer and longtime Chicago Bears owner/head coach George Halas.
The halftime show was "Carnival: A Salute to the Caribbean" with various Caribbean bands.
Radio.
The national radio broadcast of Super Bowl XIII was carried by the CBS Radio Network, with Jack Buck and Hank Stram calling the action. Locally, Verne Lundquist and Brad Sham called the game for the Cowboys on KRLD-AM in Dallas, while Jack Fleming and Myron Cope called it for the Steelers on WTAE-AM in Pittsburgh. A technical glitch led to Fleming and Cope's commentary going out over NBC's television broadcast in place of the network's own audio during the coin toss ceremony.
Game summary.
Both teams entered the game with the best defenses in the league (the Cowboys only allowed 107.6 rushing yards per game while the Steelers only allowed 107.8), and each side took advantage of the other team's mistakes throughout the game. But Dallas could not overcome their miscues in the second half.
On their opening drive, the Cowboys advanced to the Pittsburgh 38-yard line, with running back Tony Dorsett gaining 38 yards off 3 running plays. But they lost the ball on a fumbled handoff while attempting to fool the Steelers defense with a reverse-pass play. Receiver Drew Pearson later explained "We practiced that play for three weeks. It is designed for me to hit Billy Joe 15 to 17 yards downfield. We practiced the play so much it was unbelievable we could fumble it. I expected the handoff a bit lower, but I should have had it. Billy Joe was in the process of breaking into the clear when the fumble occurred." The play was similar to the near-turnover by Butch Johnson in in the previous Super Bowl.
After defensive lineman John Banaszak recovered the loose ball on the Pittsburgh 47-yard line, the Steelers attempted 2 running plays with running back Franco Harris carrying the ball, but only gained 1 yard. Then on third down, wide receiver John Stallworth caught a 12-yard pass to the Cowboys' 40-yard line. Then after throwing an incomplete pass, Terry Bradshaw completed 2 consecutive passes, the second one a 28-yard touchdown completion to Stallworth to take a 7–0 lead.
On their next drive, the Cowboys responded by advancing to the Steelers 39-yard line, but were pushed back to their own 39-yard line after quarterback Roger Staubach was sacked twice, and they were forced to punt. Then on the Steelers' ensuing drive, Bradshaw threw a 22-yard pass to Harris and followed it up with a 13-yard pass to receiver Lynn Swann to move the ball to the Dallas 30-yard line. But on the next play, Dallas linebacker D. D. Lewis ended the drive by intercepting a pass intended for Stallworth.
With a little more than a minute to go in the period, Bradshaw fumbled the ball while being sacked by Cowboys lineman Harvey Martin, and defensive end Ed "Too Tall" Jones recovered it. Staubach then capitalized on Bradshaw's mistake three plays later with a 39-yard scoring strike to receiver Tony Hill, tying the game at 7 as the first quarter expired. Pittsburgh sent eight men on an all-out blitz, but Staubach got the pass away just before he was hit by Steelers' safety Mike Wagner. Hill beat Donnie Shell in single-coverage and scored the only first-quarter touchdown surrendered by Pittsburgh all season (In Super Bowl X, the Cowboys also scored a first-quarter touchdown against a Steeler team that hadn't permitted one all year). Drew Pearson ensured the play's success by distracting Steelers cornerback Mel Blount, who was oblivious of Hill as he raced past Blount and Pearson en route to the end zone.
The Steelers took possession at the start of the second quarter and advanced to their own 48-yard line. On the next play, Dallas linebackers Mike Hegman and Thomas "Hollywood" Henderson went after Bradshaw on a blitz. After taking the snap, Bradshaw collided with Franco Harris and the ball popped loose. Bradshaw scooped it up and rolled to his right, looking to pass, but Henderson wrapped him up before he could throw, while Hegman ripped the ball out of his hands and returned the fumble 37 yards for a touchdown, giving the Cowboys a 14–7 lead.
The Steelers had now turned the ball over on three consecutive possessions, but the Cowboys' lead didn’t last long. On the third play of Pittsburgh's ensuing possession, Stallworth caught a pass from Bradshaw at the Steelers 35-yard line. He then broke a tackle from defensive back Aaron Kyle, waited for Swann and blockers to cross in front of him, turned toward the inside and outraced every other defender to the end zone, making a simple 10-yard pass a 75-yard touchdown completion to tie the score, 14–14. Bradshaw later explained that Stallworth was not even the primary receiver on the play: "I was going to Lynn Swann on the post," he said, "but the Cowboys covered Swann and left Stallworth open. I laid the ball out there and it should have gone for about 15 yards, but Stallworth broke the tackle and went all the way."
Pittsburgh's "Steel Curtain" defense then dominated the Dallas offense on their ensuing drive. First, Banaszak tackled fullback Robert Newhouse for 4-yard loss. Next, linebacker Jack Ham tackled Dorsett for a 3-yard loss on an attempted sweep. On third down, defensive tackle Joe Greene sacked Staubach, forcing a fumble that bounced through the hands of Steelers' defensive lineman Steve Furness. Cowboys lineman Tom Rafferty eventually recovered at the Dallas 13-yard line. Theo Bell then returned Danny White's ensuing 38-yard punt 3 yards to the Dallas 38-yard line.
The Steelers began their ensuing drive with Bradshaw's 26-yard completion to Swann. Jones tackled Harris for an 8-yard loss on the next play, but a subsequent holding penalty on Henderson gave Pittsburgh a first down at the Dallas 25-yard line. However, after an incomplete pass and a 2-yard run by Harris, Hegman sacked Bradshaw for an 11-yard loss on third down, pushing the ball back to the 34-yard line. The Steelers then came up empty after kicker Roy Gerela's 51-yard field goal attempt hit the crossbar.
With less than two minutes remaining in the half, Dallas advanced to the Pittsburgh 32-yard line, after starting from their own 34-yard line. But Blount exacted revenge from the first quarter by intercepting a pass from Staubach and returning it 13 yards to the 29, with a personal foul on Dallas tight end Billy Joe DuPree adding another 15 yards and giving the Steelers the ball at their own 44-yard line (note: the interception happened on exactly the same play that Drew Pearson scored on in the first quarter of Super Bowl X; Mike Wagner intercepted Staubach on exactly the same play call in the 4th quarter of the same game). Following a penalty, Bradshaw completed 2 passes to Swann for gains of 29 and 21 yards, moving the ball to the 16-yard line with 40 seconds left in the half. Next, after dropping a pass intended for him, Harris ran the ball to the 7-yard line. Then with just 26 seconds left, Bradshaw completed a 7-yard touchdown pass to fullback Rocky Bleier, giving the Steelers a 21–14 lead at halftime.
The torrid scoring pace slowed during much of the third quarter, as both teams began to assert themselves on the defensive side of the ball. But late in the quarter, a 12-yard punt return by Cowboys receiver Butch Johnson gave Dallas good field position on their 42-yard line. The Cowboys subsequently drove down to the Steelers 10-yard line, mostly with Dorsett's rushing. Then on third down with less than three minutes remaining in the period, Staubach spotted 38-year old reserve tight end Jackie Smith wide open in the end zone and threw him the ball. Coach Tom Landry said Staubach tried to throw the ball soft when he saw how wide open Smith was and that it came in low, and that when Smith tried to stop, his feet seemed to come out from under him. Jackie Smith states that it was still a catchable ball and that he should have made the play. Instead, Smith dropped the pass and the Cowboys had to settle for a field goal from kicker Rafael Septien, cutting their deficit to 21–17. Though Smith played 16 years in the league and is now enshrined in the Pro Football Hall of Fame, he is perhaps best known for his embarrassing blunder on the sport's biggest stage.
Two controversial penalties early in the fourth quarter paved the way for the Steelers to score 14 unanswered points. The Steelers advanced to their own 44-yard line after a crucial 3rd down pass from Bradshaw to tight end Randy Grossman, a 13-yard pass to Swann, and a 5-yard run by Harris. Bradshaw then attempted a pass to Swann, but the receiver collided with Cowboys defensive back Benny Barnes and fell to the ground as the ball rolled incomplete. However, official Fred Swearingen called Barnes for pass interference. Replays showed that it could have been incidental contact, as Swann seemed to run into Barnes. The penalty gave Pittsburgh a first down at Dallas' 23-yard line.
Two plays later, the Steelers faced 3rd down and 4 from the Dallas 17. Henderson sacked Bradshaw for a 12-yard loss, but the play was nullified by a delay of game penalty on Pittsburgh, bringing up 3rd down and 9 instead of a fourth down. Replays clearly showed the whistle blew before the play's onset, plus most of the players pulled up and stopped playing after a whistle sounded, but Henderson claimed, "I didn't hear a whistle until after I had knocked Bradshaw down. The same guy (Swearingen) made that call too. Who is that guy?" " Franco Harris confronted Henderson for taunting Bradshaw after the whistle, and on the next play, Bradshaw handed the ball off to Harris, who raced untouched, with help from the Umpire Art Demmas impeding Cowboys safety Charlie Waters' attempt to tackle him, up the middle for a 22-yard touchdown run. The next day Waters was quoted as saying, "I don't know what I could do – maybe knock him [Umpire Demmas] flat and maybe he'd knock Franco flat? Our safeties play a vital role in the run. That official gets in the way a lot. He screened me off." This score increased Pittsburgh's lead to 28–17. The run would be the Steelers' longest touchdown run in Super Bowl competition until Willie Parker scampered 75 yards for a score against the Seattle Seahawks in Super Bowl XL.
On the ensuing kickoff, video shows that Gerela slipped when trying to plant his foot, causing him to squib the ball, which bounced to Cowboys lineman Randy White at the 24-yard line. White, who was playing the game with a cast on his broken left hand, fumbled the ball before being hit by Tony Dungy, and Pittsburgh linebacker Dennis Winston recovered the ball at the Dallas 18-yard line. Remarkably, Winston wasn't even in the middle of the scrum when the fumble first occurred; he was standing by several teammates and decided to join the battle for the ball before referees intervened. On the next play, Bradshaw threw an 18-yard touchdown pass to Swann, increasing the Steelers' lead to 35–17 with less than 7 minutes left in the game. The touchdown was Bradshaw's last pass of the game.
Some of the Steelers were already celebrating victory on the sidelines, but the Cowboys refused to give up. On their next drive, Dallas drove 89 yards in 8 plays, including an 18-yard scramble by Staubach on 3rd and 11 and a 29-yard run by Dorsett, to score on Staubach's 7-yard touchdown pass to DuPree. Then after Dallas' Dennis Thurman recovered an onside kick at 2:19, Drew Pearson caught 2 passes for gains of 22 and 25 yards (the second catch on 4th down and 18) as the Cowboys drove 52 yards in 9 plays to score on Staubach's 4-yard touchdown pass to Butch Johnson. With the ensuing extra point, the Steelers' lead was cut to 35–31 with just 0:22 left in the game.
But the Cowboys' second onside kick attempt was unsuccessful. Bleier recovered the ball, and the Steelers were able to run out the clock to win the game.
Swann was the leading receiver in the game with 7 receptions for 124 yards and a touchdown. Stallworth recorded 115 yards and two touchdowns off just 3 receptions. Stallworth and Swann became the first pair of teammates to each have 100 yards receiving in a Super Bowl and first time two receivers did it in the same game. Dorsett was the top rusher of the game with 96 rushing yards, and also caught 5 passes for 44 yards. Harris was Pittsburgh's leading rusher with 68 yards, and he caught a pass for 22 yards. Staubach finished the game with exactly as many passing attempts (30) and completions (17) as Bradshaw, good for 228 passing yards, 3 touchdowns, and 1 interception. Butch Johnson caught 2 passes for 30 yards and a touchdown, returned 3 kickoffs for 63 yards, and gained 33 yards on 2 punt returns, giving him 126 total yards. Drew Pearson hauled in 4 passes for 73 yards, all in the fourth quarter.
Aftermath
The teams would face in yet another re-match in Three Rivers Stadium during Week 9 of the 1979 regular season, with the Steelers prevailing in a defensive slug fest, 14–3. Unlike Super Bowl XIII, Harris was able to break free of Doomsday, gaining 48 of his 102 yards on the game-clinching touchdown. The game was mostly remembered for L.C. Greenwood's hit of a sliding Staubach, causing a head injury that later influenced the quarterback to retire following the season.
Pittsburgh would cement their legacy as "The Team of the 70's" by winning Super Bowl XIV over the Los Angeles Rams, 31–19. The Cowboys would fall to the Rams in the Divisional Round in 1979 in Staubach's final game. Led by Danny White, Dallas would appear in three straight NFC Championship games from 1980–82 but wouldn't reach another Super Bowl until their 52–17 victory over the Buffalo Bills in Super Bowl XXVII. Super Bowl XIII is still widely regarded as one of the greatest Super Bowl games ever played. 
Box score.
"at Orange Bowl, Miami, Florida
! Game information
Second Quarter
Third Quarter
Fourth Quarter
Final statistics.
Source: 
Individual leaders.
1Completions/attempts
2Carries
3Longest gain
4Receptions

</doc>
<doc id="29153" url="http://en.wikipedia.org/wiki?curid=29153" title="Super Bowl XXVIII">
Super Bowl XXVIII

Super Bowl XXVIII was an American football game between the National Football Conference (NFC) champion Dallas Cowboys and the American Football Conference (AFC) champion Buffalo Bills to decide the National Football League (NFL) champion for the 1993 season. The Cowboys defeated the Bills by the score of 30–13, winning their fourth Super Bowl in team history, tying the Pittsburgh Steelers and the San Francisco 49ers for most Super Bowl wins. The game was played on January 30, 1994, at the Georgia Dome in Atlanta, Georgia.
This was the first time that the same two teams met in consecutive Super Bowls. The defending Super Bowl XXVII champion Cowboys finished with a 12–4 regular season record, despite key players missing games due to injuries. The Bills were making their fourth consecutive Super Bowl appearance, but still seeking their first title, after also finishing with a 12–4 regular season record, largely through the strength of their no-huddle offense.
After trailing 13–6 at halftime, the Cowboys scored 24 unanswered points in the second half. The Bills had built their lead off of running back Thurman Thomas' 4-yard touchdown run. But just 45 seconds into the 3rd quarter, Thomas was stripped of the ball, and Dallas safety James Washington returned the fumble 46 yards for a touchdown to tie the game. From there, Cowboys running back Emmitt Smith, who was named the Super Bowl MVP, largely took over the game. On Dallas' next possession, Smith was handed the ball seven times on an eight-play, 64-yard drive that was capped with his 15-yard touchdown run. He later scored on a 1-yard touchdown in the 4th quarter. Overall, Smith had 30 carries for 132 yards and 2 touchdowns, while also catching 4 passes for 26 yards.
Background.
NFL owners voted to award Super Bowl XXVIII to Atlanta, Georgia, during their May 23, 1990, meeting in Dallas. The Georgia Dome was under construction at the time of the vote.
Dallas Cowboys.
The Cowboys' journey to Super Bowl XXVIII proved more difficult than the previous season. Pro Bowl running back Emmitt Smith held out the first two regular season games over a contract dispute, and Dallas lost both of those contests, including a 13-10 loss at home to the Bills. Pro Bowl quarterback Troy Aikman, along with a few other key players, missed games due to injuries. Dallas still managed to finish with an NFC-best 12–4 record after defeating the New York Giants in their final regular season game. 
Though not as dynamic as the previous year, Dallas' offense remained incredibly efficient, led by Aikman, who finished the regular season completing 271 out of 392 passes for 3,100 yards, 15 touchdowns, and 6 interceptions. Smith recorded 1,486 rushing yards and 9 touchdowns, while catching 57 passes for 414 yards and another touchdown, earning him the NFL Most Valuable Player Award. Fullback Daryl Johnston was also a reliable backfield threat, scoring four touchdowns and contributing a career high 50 receptions for 371 yards. Pro Bowler Michael Irvin was once again the team's leading wide receiver with 88 catches for 1,330 yards and 7 touchdowns. Wide receiver Alvin Harper had 36 catches for 777 yards and 5 touchdowns, while Pro Bowl tight end Jay Novacek had 44 receptions for 445 yards and 1 touchdown. Pro Bowlers Mark Stepnoski, Erik Williams, and Nate Newton anchored the offensive line. On special teams, rookie receiver Kevin Williamsranked 7th in the NFL with 381 yards on 36 punt returns, while also gaining 689 kickoff return yards and catching 20 passes for 151 yards. 
The Cowboys' defense was anchored by such Pro Bowlers as lineman Russell Maryland, linebacker Ken Norton Jr., and defensive backs Thomas Everett and Kevin Smith, who picked off 6 passes during the season. Defensive end Tony Tolbert led the team with 7.5 sacks.
Buffalo Bills.
The Bills finished at the top of the AFC by clinching the conference's best regular season record at 12–4. Quarterback Jim Kelly once again led Buffalo's no-huddle offense by passing for 288 out of 470 regular season completions for 3,382 yards, 18 touchdowns, with 18 interceptions. Kelly was joining an elite class by starting his fourth Super Bowl. The only other quarterbacks to start four were Roger Staubach, Terry Bradshaw and Joe Montana, with John Elway and Tom Brady later doing so. Kelly is the only one to start four consecutive Super Bowls.
Running back Thurman Thomas gained 1,315 rushing yards and 6 touchdowns, while also catching 48 passes for 387 yards. Running back Kenneth Davis rushed for 391 yards and 6 touchdowns, while also recording 21 receptions for 95 yards. Pro Bowl wide receiver Andre Reed led the team with 52 receptions for 854 yards and 6 touchdowns; wide receiver Bill Brooks had 60 receptions for 714 yards and 5 touchdowns; and wide receiver Don Beebe recorded 31 receptions for 504 yards and 3 touchdowns. Also, Pete Metzelaars led the Bills tight ends with 68 receptions for 609 yards and 4 touchdowns. Pro Bowl offensive lineman Howard Ballard anchored the line.
Buffalo's defense was the team's weakness, ranking 28th (then-last) in the league, giving up 5,810 total yards. The defense did have a few good contributors, such as Hall of Fame lineman Bruce Smith (14 sacks, 1 fumble recovery), Pro Bowl linebacker Cornelius Bennett (5 sacks, 2 fumble recoveries), linebacker Darryl Talley (101 tackles, 2 sacks, 2 fumble recoveries, 3 interceptions) and cornerback Nate Odomes, who led the NFL with 9 interceptions and 1 fumble recovery. Linebacker Marvcus Patton, who had moved up to the starting lineup to replace departed Pro Bowler Shane Conlan, was also an impact player, intercepting two passes and recovering three fumbles
Playoffs.
Buffalo's first opponent was the Los Angeles Raiders, led by quarterback Jeff Hostetler, who had led the New York Giants to victory over the Bills in Super Bowl XXV 3 years earlier. In this game, the Raiders built up a 17–13 halftime lead, but Buffalo stormed back with 16 second half points. First, they scored on Kelly's 25-yard touchdown pass to Brooks. Then on their next drive, kicker Steve Christie made a 29-yard field goal to give the Bills a 23–17 lead. Los Angeles managed to respond with an 86-yard scoring strike from Hostetler to receiver Tim Brown, but Buffalo stormed right back with Brooks' 22-yard touchdown reception in the fourth quarter. The Bills ended up winning the game 29–23, having scored 16 points in a span of 6:18 in the second half. Kelly threw for 287 yards and 2 touchdowns with no interceptions.
One week later, the Bills advanced to their fourth consecutive Super Bowl by blowing away the Kansas City Chiefs 30–13 in the AFC Championship Game. Thomas rushed for 186 yards and three touchdowns, and caught two passes for 22 yards. On defense, the Bills limited Chiefs future Hall of Fame quarterback Joe Montana to just 9 of 23 completions for 125 yards and no touchdowns, with 1 interception. In addition, Kansas City's future Hall of Fame running back, Marcus Allen, was held to just 50 rushing yards on 18 carries.
In the NFC, Dallas' first opponent in the playoffs was the Green Bay Packers, who were coming off a thrilling 28–24 win over the Detroit Lions in the Wild Card Game, in which quarterback Brett Favre had thrown the winning touchdown pass to Sterling Sharpe with only 55 seconds left in the game. In this game, the Packers scored first with a field goal, but Dallas stormed back with 17 consecutive points in the second quarter. First, Aikman threw a 6-yard touchdown pass to Harper. Then with time running out the period, Dallas scored again on an Eddie Murray field goal. Green Bay then fumbled the ensuing kickoff, allowing the Cowboys to score again with Aikman's 6-yard pass to Novacek. The Cowboys went on to stave off an attempted Packers comeback in the second half and win the game, 27–17. Aikman finished the game with 28 of 37 completions for 302 yards and 3 touchdowns, with 2 interceptions. Irvin recorded 9 catches for 126 yards and 2 touchdowns.
One week later, Dallas faced the San Francisco 49ers in the NFC Championship Game for the second year in a row in what was, at the time, the last NFL game to air on CBS. The last time the two teams played, Dallas won when Aikman thwarted an attempted 49ers comeback with a touchdown drive late in the fourth quarter. But this time, the game was extremely one-sided. The Cowboys scored touchdowns on four of their five first-half possessions. By the end of the half, Dallas had a commanding 28–7 lead and were on their way to a 38–21 win. Although he missed most of the second half due to injury, Aikman completed 14 of 18 passes for 177 yards and three touchdowns with no interceptions, while also rushing for 25 yards. Smith rushed for 88 yards, caught seven passes for 85 yards, and scored two touchdowns.
Both Dallas and Buffalo were the top seeds in their respective conferences, earning home field advantage throughout the playoffs. Until the New Orleans Saints and Indianapolis Colts qualified for Super Bowl XLIV, this was the last time that both number one seeds advanced to the Super Bowl.
Pregame news.
Many sports writers and fans were a bit upset that the Bills advanced to their fourth consecutive Super Bowl. They were distressed with Buffalo having lost the three previous Super Bowl games and did not want to see them lose again. Some Bills fans appeared to be defensive about their team's presence in the game; during Buffalo's victory in the AFC Championship Game a week earlier, one fan displayed a banner defiantly proclaiming, "We're back; deal with it, America!"
Therefore, the Super Bowl hype was more focused onto Cowboys owner/general manager Jerry Jones and head coach Jimmy Johnson. Although the two rebuilt the team with young talent that eventually won the previous year's Super Bowl, both men had huge egos that conflicted with each other. Both had different ideas on the future personnel plans for the Cowboys, and both wanted equal credit for the team's recent success (eventually, Johnson would leave the team after the season).
This was the fourth rematch in Super Bowl history, and the first time that both teams met in consecutive years.
Television and entertainment.
The game was broadcast in the United States by NBC with play-by-play announcer Dick Enberg and color commentator Bob Trumpy. Jim Lampley hosted all the events with the help of analysts Mike Ditka and Joe Gibbs and sideline reporters O.J. Simpson (on Buffalo's sideline) and Will McDonough (on Dallas' sideline). While Lampley was busy covering the trophy presentation, Bob Costas (who also interviewed Dallas head coach Jimmy Johnson and Dallas owner/general manager Jerry Jones together prior to the game) covered for Lampley at the host and anaylsts' desk (and signed off the broadcast for NBC).
It was the first time a network had held consecutive Super Bowls outright. The five-year NFL contract signed in 1989 had a provision where the last Super Bowl in the contract (XXVIII) would not be rotated, but would go to the highest bidder. NBC, which had held XXVII (according the original rotation, NBC would have had XXVI and CBS XXVII, but the NFL allowed the networks to switch the two games in order to allow CBS a significant lead-in to its coverage of the 1992 Winter Olympics), was the only network to bid on XXVIII. Less than two weeks before the game was aired, NBC had shown a Peanuts special, "You're In the Super Bowl, Charlie Brown", in which the character Melody-Melody wins the Punt, Pass & Kick contest wearing a Dallas Cowboys uniform. For this game, NBC introduced a new theme for NFL broadcasts by composer John Colby that would be retained for the 1994 season.
Previously, the league alternated the Super Bowl broadcast among its television networks, except for Super Bowl I in which both NBC and CBS televised it simultaneously. CBS broadcast Super Bowl II, then the league rotated the broadcast between CBS and NBC until 1985 when ABC entered the rotation when they broadcast Super Bowl XIX.
NBC aired the premiere of "The John Larroquette Show" following the game.
Pregame ceremonies.
The pregame show held before the game was titled "Georgia Music Makers" and featured performances by the rap music duo Kris Kross, the rock band The Georgia Satellites, country musician Charlie Daniels, and the Morehouse College Marching Band.
The United States Trampoline Association (USTA) performed on 4 trampolines during "Jump-Jump" performed by Kris Kross.
Later, singer Natalie Cole, accompanied by the Atlanta University Center Chorus, sang the national anthem.
To honor the 25th anniversary of the New York Jets' upset win in Super Bowl III, that game's MVP, former Jets quarterback Joe Namath joined the coin toss ceremony.
Halftime show.
The halftime show was titled "Rockin' Country Sunday" and featured country music stars Clint Black, Tanya Tucker, Travis Tritt, Brooks & Dunn, and Wynonna Judd. The show's finale included a special appearance by Naomi Judd, who joined Wynonna in performing The Judds' single "Love Can Build a Bridge", to which everyone eventually joined in.
This was the first Super Bowl halftime show in which the main stadium lights were turned off for the performance. The show included dancers with yard-long light sticks.
Game summary.
Though the Bills had a lead at halftime, Super Bowl XXVIII would have an identical outcome to the three preceding Super Bowls and end with a Buffalo loss.
Dallas kick returner Kevin Williams returned the opening kickoff 50 yards to the Buffalo 48-yard line. Then the Cowboys began the drive with quarterback Troy Aikman's 20-yard pass to wide receiver Michael Irvin. But with third down and six from 24-yard line, Aikman threw an incomplete pass, and the Cowboys had to settle for kicker Eddie Murray's 41-yard field goal.
The Bills then responded with a 7-play, 43-yard scoring drive. Quarterback Jim Kelly's 24-yard pass to running back Thurman Thomas advanced the ball across the Dallas 40-yard line. After a 3-yard run by running back Kenneth Davis, however, Kelly threw two straight incompletions. The Bills then tied the game, 3–3, with Steve Christie's 54-yard field goal, the longest field goal in Super Bowl history.
Buffalo then forced Dallas to punt, but on the first play of the Bills' ensuing possession, Dallas safety James Washington forced Thomas to fumble, and safety Darren Woodson recovered the ball at midfield. Aided by receiver Alvin Harper's 24-yard reception, the Cowboys drove to the Bills' 7-yard line, but once again were forced to settle for a field goal; a 24-yarder by Murray to regain the lead, 6–3.
After receiving Murray's kickoff, the Bills could only reach their own 41-yard line before being forced to punt. However, Dallas cornerback Dave Thomas was penalized for running into punter Chris Mohr on the play, giving Buffalo a first down. Taking advantage of their second chance, the Bills marched down the field with runs by Thomas and short completions by Kelly. Thomas eventually finished off the 17-play, 80-yard drive with a 4-yard touchdown run, giving the Bills a 10–6 lead early in the 2nd quarter.
Dallas started out their ensuing drive with a 15-yard reception by Irvin and a 13-yard run by running back Emmitt Smith to get to midfield. They were eventually forced to punt, but Cowboys defensive end Matt Vanderbeek downed John Jett's 43-yard punt at the Bills' 1-yard line. A 19-yard completion from Kelly to receiver Andre Reed moved Buffalo out from the shadow of their own end zone, and they eventually reached the Cowboys 46-yard line, but they too were forced to punt. However, Mohr matched Jett's feat with a 45-yard punt that was downed at the Dallas 1-yard line by Buffalo special teams expert Steve Tasker.
As the Bills had done, Dallas managed to get out of their own territory and advance to the Buffalo 47-yard line. However, Bills defensive back Nate Odomes intercepted a pass intended for Irvin, and returned it 41 yards to the Dallas 47-yard line with 1:03 left in the half. After a 1-yard run by Thomas, Kelly completed a pair of passes to Thomas and Reed for gains of 12 and 22 yards, respectively, to move the ball to the Cowboys 12-yard line. But the Dallas defense tightened up on the next three plays, as Kelly threw a 3-yard completion to Thomas, an incomplete pass, and a completion to Thomas for no gain. Christie then kicked his second field goal as time expired in the half, increasing Buffalo's lead to 13–6.
Buffalo's command over the game proved short-lived, as the Cowboys dominated the second half. 45 seconds into the 3rd quarter, Leon Lett forced a Thomas fumble, which Washington returned 46 yards for a touchdown to tie the game, 13-13.
Bills receiver Russell Copeland then returned the ensuing kickoff 22 yards to the Buffalo 37-yard line, but on third down, Cowboys linemen Jim Jeffcoat and Charles Haley shared a 13-yard sack on Kelly to force the Bills to punt. The Cowboys then scored on an 8-play, 64-yard drive in which Smith carried the ball on seven of the eight plays, gaining all but 3 of the 64 yards himself, and finished the drive with a 15-yard touchdown run to give Dallas a 20–13 lead.
Meanwhile, Dallas' defense continued to stop Buffalo's offense throughout the second half. Washington intercepted a pass from Kelly on the first play of the 4th quarter and returned it 12 yards to the Bills 34-yard line. A false start penalty on the next play moved the ball back to the 39, but on the next three plays, Smith ran twice for 10 yards and caught a screen pass for 9. Aikman then completed a 16-yard pass to Harper, giving Dallas a first and goal at the 6-yard line. The Bills managed to prevent a touchdown on the next three plays, but on fourth and goal from the 1-yard line, Smith ran into the end zone for the score, giving the Cowboys a 27–13 lead.
The Bills started their ensuing drive from their own 22-yard line and managed to reach their own 36. Cowboy defensive lineman Jimmie Jones made two key plays, however; a second-down tackle on Thomas for a one-yard loss and a 13-yard sack on third down to push the ball back to the 22-yard line and force Buffalo to punt; a poor, 29-yard kick which the Cowboys recovered at their own 49-yard line. Dallas then put the game away with a 9-play, 49-yard scoring drive that took 4:10 off the clock. On the sixth play of the drive, Aikman completed a 35-yard pass to Harper to the Bills 1-yard line. After a false start penalty pushed them back to the 6-yard line, the Cowboys ran the ball on their next three plays to force Buffalo to use up all of their timeouts. Murray then kicked a 20-yard field goal with 2:50 left in the game, increasing the Cowboys' lead to 30–13, and effectively ending any chance of a Bills comeback.
"This one is the worst," Reed said after the game, referring to the Bills' streak of four consecutive Super Bowl losses. "We should have won. Then they come up with 24 unanswered points. That last fumble was once in a million. These things always happen to the Bills. It rips the heart out of you." "Dallas didn't wear us down in the second half," added Thomas. "I fumbled. I cost us the game." However, center Kent Hull managed to find some consolation. "In the immediate future we'll be thought of as losers," he said. "But one day down the road, when I'm no longer playing, they'll say, 'Wow, they won four straight AFC championships. They must have been good.' "
For the Cowboys, Troy Aikman was 19 out of 27 for 207 yards with 1 interception, while Alvin Harper was the team's top receiver with three catches for 75 yards. Emmitt Smith, still suffering the effects of a shoulder injury during the regular-season finale, became just the second player in Super Bowl history to run for 100 yards in back-to-back Super Bowls (the other being Larry Csonka, who did it in Super Bowls VII and VIII). He also became the fourth player to rush for touchdowns in back-to-back Super Bowls (joining Franco Harris, John Riggins and Thomas). Smith also became the first player to lead the league in rushing yards, win the NFL Most Valuable Player Award, and win Super Bowl MVP all in the same season. He was also the fourth player, after Bart Starr (1966), Terry Bradshaw (1978), and Joe Montana (1989) to win both the NFL MVP and Super Bowl MVP during the same season. Defensively, James Washington, who began as the nickel-back to counter Buffalo's "no-huddle" and frequent use of three wide receivers, had a phenomenal game with his 46-yard fumble return touchdown, an interception, forcing a Thurman Thomas fumble that Darren Woodson recovered, and collecting 11 tackles.
For the Bills, wide receiver Andre Reed finished the game with 6 receptions for 75 yards to lead Buffalo, with Don Beebe catching 6 passes for 60 yards and returning 2 kickoffs for 63 yards. Thomas was limited to just 37 rushing yards, but he also caught 7 passes for 52 yards (Thomas became the first player in Super Bowl history to score touchdowns in four Super Bowls: he scored one TD in each of the Bills' four straight appearances, XXV-XXVIII). Kenneth Davis was the Bills' top rusher with 38 yards. Kelly finished the game 31-of-50 for 260 yards and 1 interception. His 31 completions was a Super Bowl record. Kelly became the only player ever to throw 50 passes in two Super Bowls. In addition to his 50 passes in this game, he threw a Super Bowl-record 58 passes in Super Bowl XXVI.
Box score.
"at Georgia Dome, Atlanta, Georgia
! Game information
Final statistics.
Source: 
Individual leaders.
1Completions/attempts
2Carries
3Long gain
4Receptions
Starting lineups.
Source:

</doc>
<doc id="29166" url="http://en.wikipedia.org/wiki?curid=29166" title="Galgo Español">
Galgo Español

The Galgo Español ("Spanish galgo") or Spanish greyhound is an ancient breed of dog, specifically a member of the sighthound family. 
The English greyhound is possibly a descendant of the Spanish greyhound and, for several years in the 20th century, some breeders did cross-breed Galgos and Greyhounds in order to produce faster and more powerful Galgos, specifically for track racing purposes.
Description.
Appearance.
Galgos are similar in appearance to Greyhounds, but are distinctly different in their conformation. Galgos are higher in the rear than in the front, and have flatter muscling than a Greyhound, which is characteristic of endurance runners. They also tend to be smaller, lighter in build, have longer tails and have a very long, streamlined head that gives the impression of larger ears. Their chests are not as deep as a Greyhound's and should not reach the point of the elbow 
Unlike Greyhounds, Galgos come in two coat types: smooth and rough. The rough coat can provide extra protection from skin injuries while running in the field. They come in a variety of colors and coat patterns. Main colors are "barcino" or "atigrado" (brindle), "negro" (black), "barquillo"(golden), "tostado"(toasted), "canela" (cinnamon), "amarillo"(yellow), "rojo"(red), "blanco" (white), "berrendo" (white with patches) or "pío" (any colour with white muzzle and forehead).
Temperament.
Galgos have a very similar nature to Greyhounds. They are calm, quiet, gentle and laid back; happy to sleep their day away on their backs on a sofa. More than 90% of Galgos can be considered cat-friendly and are therefore an ideal choice for the hound lover who also owns cats. Almost all Galgos are also friendly towards other dogs and small dogs. Galgos are also very good with children, being calm in the house so there is less risk of a child being knocked over or jumped on than with a more excitable breed. They are very gentle and tolerate the often over-enthusiastic attentions of children with little risk of retaliation from the dog.
Galgos have a very reserved personality and they have a tendency towards shyness, so it is very important that they be socialized early in life so that they grow up to be comfortable around strange people, dogs and locations.
Health.
Like many other sighthounds, Galgos are a fairly healthy breed although they are sensitive to anaesthesia. As such, proper care should be taken by the owner to ensure that the attending veterinarian is aware of this issue.
Although Galgos are big dogs, their history of selection as a working sighthound, their light weight, and their anatomy keep them safe from hip dysplasia.
These dogs must run regularly to keep in perfect health, combined with their characteristic tendency to sleep all the rest of the day.
History.
The Galgo is not only "the Spanish greyhound" but also "the Spanish dog".
Its name is probably derived from the Latin "Canis Gallicus" or "Dog from Gaul". The Spanish word for all kinds of Greyhounds - including the Galgo - is "lebrel", which means "harrier" or "dog for chasing hares", since "liebre" is Spanish for hare. We can see the same derivative in the Italian "levriero" and the French "lévrier". 
The first written references to an ancient Celtic sighthound, the "vertragus", in the "Cynegeticus" of Flavius Arrianus (Arrian), Roman proconsul of Baetica in the second century, may refer to the Galgo, or more likely to its antecedent.
The author Arrian, during his personal experience in Spain, describes hare hunting with Galgos in a manner almost identical to that used nowadays in Spain, adding that it was a general Celtic tradition not related to a social class. 
He indicates that there were not only smooth haired types of the vertragus but also coated ones.
There is little evidence on the Galgo or its antecedent in the first centuries of the Middle Age but it appeared to survive and flourish in the second half of this period.
In the 9th and 10th centuries great spaces in Castile were colonized, coinciding with the Reconquista, resulting in the Christian military repossession of the Iberian Peninsula from the Muslims. This open land introduces a new character to hunting with dogs: while the North of Spain is mountainous, the regions progressively recovered from the Muslims were flat, open areas full of small animals like hares, which provided the Galgo a useful opportunity to hunt. At this time, it is considered a noble dog, and kept mainly by aristocracy, both in the Christian and the Muslim Kingdoms in which the Spanish territory was still divided at the time. It is likely that the Galgo and Sloughi were interbred at this period.
The great esteem in which the Galgo was held is visible in the many laws of the time designed to punish the killing or theft of this dog: Fuero of Salamanca (9th century); Fuero of Cuenca; Fuero of Zorita de los Canes; Fuero of Molina de Aragón (12th century); Fuero of Usagre (12th century). 
In the Cartuario of Slonza we can read a will written in Villacantol, in which, using an odd mixture of Latin and Spanish, the Mayor Gutiérrez bequeaths a Galgo to Diego Citid in the year 1081:
The fact that this dog was a significant item in a noble's will, demonstrates the great value that it was given at the time.
The mural paintings at the Hermitage of San Baudelio de Berlanga, in Soria, dating from the 12th century show a hunting scene with three Galgos apparently identical to the ones that we can see today.
In the Renaissance Martínez del Espinar writes in his book "Arte de Ballestería y Montería" ("The Art of Hunting and Archery"):
The Galgo appears to have developed first in the Castillian plains, both in the north (Valladolid, Zamora, Ávila Salamanca, Segovia, Soria, Burgos and Palencia) and the south (Toledo, Cuenca, Guadalajara, Madrid and Ciudad Real) of Castilla. And, afterwards, in more southern territories: La Mancha and Andalusia.
It became the typical dog type of the Spanish interior, while the bloodhound plays the same role in the coast regions.
The Galgo appears not only in hunting books but also in common Spanish expressions, as well as in Literature. Maybe the most famous reference is the one contained in the opening sentence of "Don Quixote de La Mancha":
There are plenty of common expressions in Spain that name the Galgo. For example
""A galgo viejo, echadle liebre, no conejo" which means " use old Galgos for chasing hares instead of rabbits"" suggests that it is best to use experienced people for hard tasks and challenges.
"Galgo que va tras dos liebres, sin ninguna vuelve" meaning "if a Galgo tries to chase two hares, it will return with none" recommends focussing on a single effort, otherwise by distraction, failing.
Although the breed did not apparently experience any significant change in the 18th and 19th centuries, and was kept in its vocation as a swift hunting dog, maybe the most telling proverb which mentions the Galgo, is the one dating from the first years of the nineteenth Century:
Meaning
Which was used at first to satirize the corrupt Government of Fernando VII, considered to cheat in everything it did.
In the first years of the 20th century, large scale crossbreeding occurred between the Galgo and the English Greyhound in order to create faster dogs for professional track racing. This certainly affected the purity of the breed, the resulting dogs were just a bit faster, but did lose their long-distance-running abilities. Finally breeders came to the conclusion that it was not worth crossbreeding. 
The pure bred Galgo kept its major presence in the Spanish villages as an excellent hunting type.
Despite its antiquity and importance, the Spanish Galgo has only recently been acknowledged by the cynological associations. The English Greyhound has tended to outshine the Galgo. Spain has suffered catastrophic events during the last century, such as the Spanish Civil War and the 40-year-long Francisco Franco fascist dictatorship, which allowed this breed to be kept relatively unknown both inside and outside of its native country, at least until democracy led to greater social and cultural equality and development.
The breed faces the 21st century being progressively more appreciated at home and abroad, as contemporary Spain becomes more conscious of the uniqueness and heritage of this splendid animal.
Roles.
Galgos as pets.
Due to their primary role as hunting dogs in the Spanish countryside, the Spanish Galgos are sometimes treated a little better than commodities. However, some people argue that Galgos are mistreated and abused in their native Spain. Galgueros (breeders), as they are normally called, will often select puppies from a litter that show the most propensity for hunting or racing, while abandoning the rest in the streets. The puppies that do get selected often do not live very long lives, as the galgueros often consider the dog too old to hunt once it has achieved two or three years of age-often after hunting season ends, they are either abandoned, shot, or hanged. For all these reasons, many associations in defense of the Galgo have appeared with the aim to save these dogs from a terrible fate, provide much needed rehabilitation, and adoptive homes, usually in the cities. Some associations will adopt them to other locations in Europe, including France, the UK, Germany, Belgium and the Netherlands.
Because they tend to be quiet and docile, Galgos make very nice house pets. In Spain they have a well earned reputation as gentle dogs, with sweet temperaments and solid health. They tend to get along well with people and other dogs, and they can be well-behaved around cats if properly socialized. Outside of sunny Spain, they require a warm coat to keep them warm in cold winter weather: like all greyhound type breeds, they have little body fat and short coats, so extra warmth is preferred for colder climates
Galgos excel at performance activities like lure coursing and racing. They are eligible to compete in lure coursing events sanctioned by the American Sighthound Field Association, entered in the Limited class. They also make very nice show dogs and have enjoyed success in the European show ring, although they are not as well known in the American show world due to their rarity outside Europe.

</doc>
<doc id="29318" url="http://en.wikipedia.org/wiki?curid=29318" title="Streptococcus">
Streptococcus

Streptococcus is a genus of coccus (spherical) Gram-positive bacteria belonging to the phylum Firmicutes and the Lactobacillales (lactic acid bacteria) order. Cellular division occurs along a single axis in these bacteria, and thus they grow in chains or pairs, hence the name—from Greek στρεπτός "streptos", meaning easily bent or twisted, like a chain (twisted chain).
Contrast this with "Staphylococci", which divide along multiple axes and generate grape-like clusters of cells. Most "Streptococci" are oxidase- and catalase-negative, and many are facultative anaerobes.
In 1984, many organisms formerly considered "Streptococcus" were separated out into the genera "Enterococcus" and "Lactococcus". Currently, over 50 species are recognised in this genus.
Pathogenesis and classification.
In addition to streptococcal pharyngitis (strep throat), certain "Streptococcus" species are responsible for many cases of pink eye, meningitis, bacterial pneumonia, endocarditis, erysipelas, and necrotizing fasciitis (the 'flesh-eating' bacterial infections). However, many streptococcal species are not pathogenic, and form part of the commensal human microbiota of the mouth, skin, intestine, and upper respiratory tract. Furthermore, streptococci are a necessary ingredient in producing Emmentaler ("Swiss") cheese.
Species of "Streptococcus" are classified based on their hemolytic properties. Alpha-hemolytic species cause oxidization of iron in hemoglobin molecules within red blood cells, giving it a greenish color on blood agar. Beta-hemolytic species cause complete rupture of red blood cells. On blood agar, this appears as wide areas clear of blood cells surrounding bacterial colonies. Gamma-hemolytic species cause no hemolysis.
Beta-hemolytic streptococci are further classified by Lancefield grouping, a serotype classification (that is, describing specific carbohydrates present on the bacterial cell wall). The 20 described serotypes are named Lancefield groups A to V (excluding I and J).
In the medical setting, the most important groups are the alpha-hemolytic streptococci "S. pneumoniae" and "Streptococcus" "viridans "group, and the beta-hemolytic streptococci of Lancefield groups A and B (also known as “group A strep” and “group B strep”).
Alpha-hemolytic.
When alpha hemolysis (α-hemolysis) is present, the agar under the colony is dark and greenish. Streptococcus pneumoniae and a group of oral streptococci (Streptococcus viridans or viridans streptococci) display alpha hemolysis. This is sometimes called green hemolysis because of the color change in the agar. Other synonymous terms are incomplete hemolysis and partial hemolysis. Alpha hemolysis is caused by hydrogen peroxide produced by the bacterium, oxidizing hemoglobin to green methemoglobin.
The viridans group: alpha-hemolytic.
Table: Medically relevant streptococci
Beta-hemolytic.
Beta hemolysis (β-hemolysis), sometimes called complete hemolysis, is a complete lysis of red cells in the media around and under the colonies: the area appears lightened (yellow) and transparent. Streptolysin, an exotoxin, is the enzyme produced by the bacteria which causes the complete lysis of red blood cells. There are two types of streptolysin: Streptolysin O (SLO) and streptolysin S (SLS). Streptolysin O is an oxygen-sensitive cytotoxin, secreted by most Group A streptococcus (GAS), and interacts with cholesterol in the membrane of eukaryotic cells (mainly red and white blood cells, macrophages, and platelets), and usually results in β-hemolysis under the surface of blood agar. Streptolysin S is an oxygen-stable cytotoxin also produced by most GAS strains which results in clearing on the surface of blood agar. SLS affects immune cells, including polymorphonuclear leukocytes and lymphocytes, and is thought to prevent the host immune system from clearing infection. Streptococcus pyogenes, or Group A beta-hemolytic Strep (GAS), displays beta hemolysis.
Some weakly beta-hemolytic species cause intense beta hemolysis when grown together with a strain of Staphylococcus. This is called the CAMP test.[1] Streptococcus agalactiae displays this property. Clostridium perfringens can be identified presumptively with this test. Listeria monocytogenes is also positive on sheep's blood agar.
Group A.
"S. pyogenes", also known as group A "Streptococcus" (GAS), is the causative agent in a wide range of group A streptococcal infections. These infections may be noninvasive or invasive. The noninvasive infections tend to be more common and less severe. The most common of these infections include streptococcal pharyngitis (strep throat) and impetigo. Scarlet fever is also a noninvasive infection, but has not been as common in recent years. 
The invasive infections caused by group A β-hemolytic streptococci tend to be more severe and less common. This occurs when the bacterium is able to infect areas where it is not usually found, such as the blood and the organs. The diseases that may be caused include streptococcal toxic shock syndrome, necrotizing fasciitis, pneumonia, and bacteremia.
Additional complications may be caused by GAS, namely acute rheumatic fever and acute glomerulonephritis. Rheumatic fever, a disease that affects the joints, kidneys, and heart valves, is a consequence of untreated strep A infection caused not by the bacterium itself. Rheumatic fever is caused by the antibodies created by the immune system to fight off the infection cross-reacting with other proteins in the body. This "cross-reaction" causes the body to essentially attack itself and leads to the damage above. Globally, GAS has been estimated to cause more than 500,000 deaths every year, making it one of the world's leading pathogens. Group A "Streptococcus" infection is generally diagnosed with a rapid strep test or by culture.
Group B.
"S. agalactiae", or group B "Streptococcus", GBS, causes pneumonia and meningitis in neonates and the elderly, with occasional systemic bacteremia. They can also colonize the intestines and the female reproductive tract, increasing the risk for premature rupture of membranes during pregnancy, and transmission of the organism to the infant. The American Congress of Obstetricians and Gynecologists (formerly the American College of Obstetricians and Gynecologists), American Academy of Pediatrics, and the Centers for Disease Control recommend all pregnant women between 35 and 37 weeks gestation to be tested for GBS. Women who test positive should be given prophylactic antibiotics during labor, which will usually prevent transmission to the infant.
The United Kingdom has chosen to adopt a risk factor-based protocol, rather than the culture-based protocol followed in the US. Current guidelines state that if one or more of the following risk factors are present, then women should be treated with intrapartum antibiotics:
This protocol results in treatment of 15–20% of pregnant women and prevention of 65–70% of cases of early onset GBS sepsis.
Group C.
This group includes "S. equi", which causes strangles in horses, and "S. zooepidemicus"—"S. equi" is a clonal descendent or biovar of the ancestral "S. zooepidemicus"—which causes infections in several species of mammals, including cattle and horses. "S. dysgalactiae" is also a member of group C, β-haemolytic streptococci that can cause pharyngitis and other pyogenic infections similar to group A streptococci.
Group D (enterococci).
Many former group D streptococci have been reclassified and placed in the genus "Enterococcus" (including "E. faecalis", "E. faecium", "E. durans", and "E. avium"). For example, "Streptococcus faecalis" is now "Enterococcus faecalis". "E. faecalis" is sometimes alpha hemolytic and "E. faecium" is sometimes beta hemolytic.
The remaining nonenterococcal group D strains include "Streptococcus bovis" and "Streptococcus equinus".
Nonhemolytic streptococci rarely cause illness. However, weakly hemolytic group D beta-hemolytic streptococci and "Listeria monocytogenes" (which is actually a Gram-positive bacillus) should not be confused with nonhemolytic streptococci.
Group F streptococci.
Group F streptococci were first described in 1934 by Long and Bliss amongst the "minute haemolytic streptococci". They are also known as "Streptococcus anginosus" (according to the Lancefield classification system) or as members of the "S. milleri" group (according to the European system).
Group G streptococci.
These streptococci are usually, but not exclusively, beta-hemolytic. "S. canis" is an example of a GGS which is typically found on animals, but can cause infection in humans.
Group H streptococci.
Group H streptococci cause infections in medium-sized canines. Group H streptococci rarely cause illness unless a human has direct contact with the mouth of a canine. One of the most common ways this can be spread is human-to-canine, mouth-to-mouth contact. However, the canine may lick the human's hand and infection can be spread, as well.
Molecular taxonomy and phylogenetics.
Streptococci have been divided into six groups on the basis of their 16S rDNA sequences: "S. anginosus, S.bovis, S. mitis, S. mutans, S. pyogenes" and "S. salivarius". The 16S groups have been confirmed by whole genome sequencing (see figure). The important pathogens "S. pneumoniae" and "S. pyogenes" belong to the "S. mitis" and "S. pyogenes" groups, respectively, while the causative agent of dental caries, "Streptococcus mutans", is basal to the "Streptococcus" group.
Genomics.
The genomes of hundreds of species have been sequenced. Most "Streptococcus" genomes are 1.8 to 2.3 Mb in size and encode 1,700 to 2,300 proteins. Some important genomes are listed in the table. The four species shown in the table ("S. pyogenes, S. agalactiae, S. pneumoniae", and "S. mutans") have an average pairwise protein sequence identity of about 70%.

</doc>
<doc id="29346" url="http://en.wikipedia.org/wiki?curid=29346" title="Sambuca">
Sambuca

Sambuca (]) is an Italian anise-flavoured, usually colourless, liqueur. Its most common variety is often referred to as "white sambuca" to differentiate it from other varieties that are deep blue in colour ("black sambuca") or bright red ("red sambuca"). Like other anise-flavoured liqueurs, the ouzo effect is sometimes observed when combined with water.
Ingredients.
Sambuca is flavoured with essential oils obtained from anise, star anise, liquorice and other spices. It also contains elderflowers. The oils are added to pure alcohol, a concentrated solution of sugar, and other flavouring. It is commonly bottled at 42% alcohol by volume.
History.
The etymology is disputed: the Molinari company states that the name "Sambuca" comes from an Arabic word: "Zammut". This was the name of an anise-flavoured drink that arrived to the port of Civitavecchia by ships coming from the East. The "Oxford English Dictionary" states, however, that the term comes from the Latin word "sambucus", meaning "elderberry".
The Greek word "Sambuca" was first used as the name of another elderberry liquor that was created in Civitavecchia about 130 years ago.
The first commercial version of such a drink started at the end of 1800 in Civitavecchia, where Luigi Manzi sold "Sambuca Manzi". In 1945, soon after the end of Second World War, commendatore Angelo Molinari started producing "Sambuca Extra Molinari", which helped popularise Sambuca throughout Italy.
Serving.
Sambuca may be served neat. It may also be served on the rocks or with water, resulting in the ouzo effect from the anethole in the anise.
Sambuca is considered to go particularly well with coffee. Like other anise liqueurs, it may be drunk after coffee as a ammazzacaffè or added directly to coffee in place of sugar.
The most iconic serving of sambuca is a shot with three coffee beans, called "con la mosca", which means "with the fly". The three coffee beans are said to represent health, happiness and prosperity, or the Holy Trinity. The shot may be ignited to toast the coffee beans with the flame extinguished immediately before drinking.

</doc>
<doc id="29356" url="http://en.wikipedia.org/wiki?curid=29356" title="International Society of Cryptozoology">
International Society of Cryptozoology

The International Society of Cryptozoology (ISC) was a former professional organization founded in 1982 in Washington, D.C. It ceased to exist in 1998.
It was founded to serve as a scholarly center for documenting and evaluating evidence of unverified animals; that is, animal species or forms which have been reported in some manner but which have not been scientifically proven to exist. The study of such animals is known as cryptozoology, and "Cryptozoology" was also the title of its journal. The President was Bernard Heuvelmans, and the Vice-President Roy Mackal. The Secretary was J. Richard Greenwell (died 2005), of the University of Arizona. Loren Coleman, John Willison Green, and several other prominent cryptozoologists were either Life Members, Honorary Members, or Board Members.
The official emblem of the society was the Okapi, which was chosen because, although it was well known to the inhabitants of its region, it was unknown to the European scientific community until the English explorer Harry Johnston sent to London an Okapi skin which received international attention in 1901.
The journal "Cryptozoology" was published from 1982 to 1996. The Society also published a newsletter "ISC News".
The ISC ended its activities in 1998 due to financial problems, though a website continued until 2005.
According to the journal "Cryptozoology", the ISC served "as a focal point for the investigation, analysis, publication, and discussion of all matters related to animals of unexpected form or size, or unexpected occurrence in time or space."

</doc>
<doc id="29357" url="http://en.wikipedia.org/wiki?curid=29357" title="Send in the Clowns">
Send in the Clowns

"Send in the Clowns" is a song written by Stephen Sondheim for the 1973 musical "A Little Night Music", an adaptation of Ingmar Bergman's film "Smiles of a Summer Night". It is a ballad from Act II in which the character Desirée reflects on the ironies and disappointments of her life. Among other things, she looks back on an affair years earlier with the lawyer Fredrik. Meeting him after so long, she finds that he is now in an unconsummated marriage with a much younger woman. Desirée proposes marriage to rescue him from this situation, but he declines, citing his dedication to his bride. Reacting to his rejection, Desirée sings this song. The song is later reprised as a coda after Fredrik's young wife runs away with his son, and Fredrik is finally free to accept Desirée's offer.
Sondheim wrote the song specifically for the actress Glynis Johns, who created the role of Desirée on Broadway. The song is structured with four verses and a bridge, and uses a complex compound meter. It became Sondheim's most popular song after Frank Sinatra recorded it in 1973 and Judy Collins' version charted in 1975 and 1977. Subsequently, Sarah Vaughan, Shirley Bassey, Judi Dench, Grace Jones, Barbra Streisand, Zarah Leander, Tiger Lillies, Joyce Castle, Ray Conniff, Glenn Close, Cher, Bryn Terfel, Plácido Domingo and many other artists recorded the song and it has become a jazz standard.
Meaning of title.
The "clowns" in the title do not refer to circus clowns. Instead, they symbolize fools, as Sondheim explained in a 1990 interview:
I get a lot of letters over the years asking what the title means and what the song's about; I never thought it would be in any way esoteric. I wanted to use theatrical imagery in the song, because she's an "actress," but it's not supposed to be a circus [...] [I]t's a theater reference meaning "if the show isn't going well, let's send in the clowns"; in other words, "let's do the jokes." I always want to know, when I'm writing a song, what the end is going to be, so "Send in the Clowns" didn't settle in until I got the notion, "Don't bother, they're here", which means that "We are the fools."
In a 2008 interview, Sondheim further clarified:
As I think of it now, the song could have been called "Send in the Fools". I knew I was writing a song in which Desirée is saying, "aren't we foolish" or "aren't we fools?" Well, a synonym for fools is clowns, but "Send in the Fools" doesn't have the same ring to it.
Context.
In an interview with Alan Titchmarsh, Judi Dench, who performed the role of Desirée in London, commented on the context of the song. The play is "a dark play about people who, at the beginning, are with wrong partners and in the end it is hopefully going to become right, and she (Desiree) mistimes her life in a way and realizes when she re-meets the man she had an affair with and had a child by (though he does not know that), that she loves him and he is the man she wants."
Some years before the play begins, Desirée was a young, attractive actress, whose passions were the theater and men. She lived her life dramatically, flitting from man to man. Fredrik was one of her many lovers and fell deeply in love with Desirée, but she declined to marry him. The play implies that when they parted Desirée may have been pregnant with his child.
A few months before the play begins, Fredrik married a beautiful woman who at 18 years old was much younger than he. In Act One, Fredrik meets Desirée again, and is introduced to her daughter, a precocious adolescent suggestively named Fredrika. Fredrik explains to Desirée that he is now married to the young woman, whom he loves, but who is still a virgin and refuses to have sex with him. Desirée and Fredrik then make love.
Act Two begins days later, and Desirée realizes that she truly loves Fredrik. She tells Fredrik that he needs to be rescued from his marriage, and she proposes to him. Fredrik explains to Desirée that he has been swept off the ground and is "in the air" in love with his beautiful, young wife, and apologizes for having misled her. Fredrik walks across the room, while Desirée remains sitting on the bed; as she feels both intense sadness and anger, at herself, her life and her choices, she sings, "Send in the Clowns." Not long thereafter, Fredrik's young wife runs away with his son, and he is free to accept Desirée's proposal, and the song is reprised as a coda.
Score.
History.
Sondheim wrote the lyrics and music over a two-day period during rehearsals for the play's Broadway debut, specifically for the actress Glynis Johns, who created the role of Desirée. According to Sondheim, "Glynis had a lovely, crystal voice, but sustaining notes was not her thing. I wanted to write short phrases, so I wrote a song full of questions" and the song's melody is within a small music range:
Lyrics.
The lyrics of the song are written in four verses and a bridge and sung by Desirée. As Sondheim explains, Desirée experiences both deep regret and furious anger:
"Send in the Clowns" was never meant to be a soaring ballad; it's a song of regret. And it's a song of a lady who is too upset and too angry to speak– meaning to sing for a very long time. She is furious, but she doesn't want to make a scene in front of Fredrik because she recognizes that his obsession with his 18-year-old wife is unbreakable. So she gives up; so it's a song of regret and anger, and therefore fits in with short-breathed phrases.
Meter and key.
The song was originally performed in the key of D flat major.
The song uses an unusual and complex meter, which alternates between 12/8 and 9/8. These are two complex compound meters that evoke the sense of a waltz used throughout the score of the show. Sondheim tells the story:
Styles.
"Send in the Clowns" is performed in two completely different styles: dramatic and lyric. The dramatic style is the theatrical performance by Desirée, and this style emphasizes Desirée's feelings of anger and regret, and the dramatic style acts as a cohesive part of the play. The lyric style is the concert performance, and this style emphasizes the sweetness of the melody and the poetry of the lyrics. Most performances are in concert, so they emphasize the beauty of the melody and lyrics.
Sondheim teaches both dramatic and lyric performers several important elements for an accurate rendition:
The dramatic performer must take on the character of Desirée: a woman who finally realizes that she has misspent her youth on the shallow life. She is both angry and sad, and both must be seen in the performance. Two important examples are the contrast between the lines, "Quick, send in the clowns" and "Well, maybe next year." Sondheim teaches that the former should be steeped in self-loathing, while the latter should emphasize regret. Thus, the former is clipped, with a break between "quick" and "send," while the latter "well" is held pensively.
Sondheim himself apologizes for flaws in his composition. For example, in the line, "Well, maybe next year," the melodic emphasis is on the word "year" but the dramatic emphasis must be on the word "next":
The word "next" is important: "Maybe "next" year" as opposed to ""this" year". [Desirée means,] "All right, I've screwed it up this year. Maybe next year I'll do something right in my life." So [it's] "well, maybe "next" year" even though it isn't accented in the music. This is a place where the lyric and the music aren't as apposite as they might be, because the important word is "next", and yet the accented word is "year". That's my fault, but [something the performer must] overcome.
Another example arises from Sondheim's roots as a speaker of American rather than British English: The line "Don't you love farce?" features two juxtaposed labiodental fricative sounds (the former ["v"] voiced, the latter ["f"] devoiced). American concert and stage performers will often fail to "breathe" and/or "voice" between the two fricatives, leading audiences familiar with British slang to hear "Don't you love arse?," misinterpreting the lyric or at the least perceiving an unintended double entendre. Sondheim agrees that "[i]t's an awkward moment in the lyric, but that "v" and that "f" should be separated."
In the line of the fourth verse, "I thought that you'd want what I want. Sorry, my dear," the performer must communicate the connection between the "want" and the "sorry". Similarly, Sondheim insists that performers separately enunciate the adjacent "t"s in the line, "There ought to be clowns."
Popular success.
In 1973, the musical, with the song, debuted on Broadway. The song become popular with theater audiences but had not become a pop hit. Sondheim explained how the song became a hit:
First of all, it wasn't a hit for two years. I mean, the first person to sing it was Bobby Short, who happened to see the show in Boston, and it was exactly his kind of song: He's a cabaret entertainer. And then my memory is that Judy Collins picked it up, but she recorded it in England; Sinatra heard it and recorded it. And between the two of them, they made it a hit.
In 1973, Frank Sinatra recorded "Send in the Clowns" on his comeback album, "Ol' Blue Eyes Is Back", which hit gold status. Gordon Jenkins arranged the song. It was also released as a single, with "Let Me Try Again" on side B.
Two years later in 1975, Judy Collins recorded "Send In the Clowns" and included it in her album, "Judith".
The song was released as a single, which soon became a major pop hit. It remained on the "Billboard" Hot 100 for 11 weeks in 1975, reaching Number 36.
Then, in 1977, the song again reached the Billboard Hot 100, where it remained for 16 weeks and reached Number 19.
At the Grammy Awards of 1976, the Judy Collins performance of the song was named "Song of the Year".
After Sinatra and Collins recorded the song, it was recorded by Kenny Rogers, Lou Rawls and many others.
In 1985, Sondheim added a verse for Barbra Streisand to use in her concert performances
and recording, which was featured on "The Broadway Album". In 1986, her version became a Number 25 Billboard Hot Adult Contemporary hit.
The song has become a jazz standard with performances by Count Basie, Sarah Vaughan, the Stan Kenton Orchestra and many others.
Recordings.
The song occurs on over 900 records by hundreds of performers in a wide variety of arrangements. Among these are:

</doc>
<doc id="29379" url="http://en.wikipedia.org/wiki?curid=29379" title="Shiva (Judaism)">
Shiva (Judaism)

Shiva (Hebrew: שבעה‎) (literally "seven") is the week-long mourning period in Judaism for first-degree relatives: father, mother, son, daughter, brother, sister, and spouse. The ritual is referred to as "sitting shiva." Immediately after burial, people assume the "halakhic" status of "avel" (Hebrew: אבל ; "mourner"). This state lasts for seven days, during which family members traditionally gather in one home (preferably the home of the deceased) and receive visitors. At the funeral, mourners traditionally tear an outer garment, a ritual known as "keriah". This garment is worn throughout shiva.
Etymology.
The word Shiva comes from the Hebrew word "shiv'ah", which literally means "seven". The tradition was developed in response to the story in Genesis 50:1-14 in which Joseph mourns the death of his father Jacob (Israel) for seven days.
Length of shiva.
The Hebrew word "shiva" means "seven", and the official shiva period is seven days. The day of the funeral is counted as the first day of shiva, even though the practice does not begin until after the mourner(s) arrive at the designated location following the funeral. On day seven, shiva generally ends in the morning, following services. On Shabbat during the week of shiva, no formal mourning takes place, but the day is counted as one of the seven. Sometimes, a minyan with a Torah reading will take place at the mourner's house.
If the first day of a "Yom Tov" (holy days which includes Rosh Hashanah, Yom Kippur, Sukkot, Passover, and Shavuot) occurs during shiva, the shiva ends, regardless of the number of days that have already been observed. Even if a Yom Tov begins at nightfall on the day of the funeral, the remainder of shiva is cancelled.
If the death occurs during "Yom Tov", shiva does not begin until the burial is completed. Burial may not take place on "Yom Tov", but can on "Chol HaMoed" (the intermediate days of Sukkot or Passover). Burial can also take place on the second day of "Yom Tov" in the Diaspora. In addition, it is also permitted to delegate the burial to gentiles even on the first day, though such is not usually done.
If a burial occurs on "Chol HaMoed" of Passover, shiva does not begin until after the Yom Tov is completed. In the Diaspora, where most "Yom Tovim" are observed for two days, mourning does not take place on the second day, but the day is still counted as one of the days of shiva.
Shiva customs.
Traditionally, the first meal after the funeral, the "seudat havra'ah" (Hebrew: סעודת הבראה ; "meal of comforting"), is supplied by neighbors and friends. The mourners do not bathe or shower for pleasure, they do not wear leather shoes or jewelry, men do not shave, and in many communities household mirrors are covered. The prohibition of bathing includes bathing or showering the whole body, or using hot water. It is permitted to wash separately various parts of the body in cool water. Marital relations and Torah study are not permitted. (It is permitted to study the laws of mourning, as well as that material which may be studied on "Tisha B'Av", including Job, Lamentations, portions of Jeremiah and the third chapter of Talmud tractate "Moed Katan".) No public mourning may occur on Shabbat, nor may the burial take place on Shabbat; "private" mourning restrictions continue during the Shabbat. It is customary for the mourners to sit on low stools, or even the floor, symbolic of the emotional reality of being "brought low" by the grief. Typically, mourners do not return to work until the end of the week of mourning.
Many communities have an arrangement where members of the "chevra kadisha" (local Jewish burial society) organise the meals for the mourners, and serve refreshments for visitors. If prayer services are organized in the house of mourning, it is customary for an adult mourner to lead the prayers.
Visiting a shiva home.
It is considered a great "mitzvah" (literally "commandment" but usually interpreted as "good deed") of kindness and compassion to pay a home visit ("make" or "pay a shiva call") to the mourners, a practice known as "Nichum Aveilim". Traditionally, no greetings are exchanged and visitors wait for the mourners to initiate conversation, or remain silent if the mourners do not do so, out of respect for their bereavement. Once engaged in conversation by the mourners, it is appropriate for visitors to talk about the deceased, sharing stories of their life. Some mourners use the "shiva" as a distraction from their loss, other mourners prefer to openly experience their grief together with friends and family.
Upon leaving an Ashkenazic shiva house, visitors recite a traditional blessing: "May God comfort you among the other mourners of Zion and Jerusalem" (המקום ינחם אתכם בתוך שאר אבלי ציון וירושלים, transliterated "HaMakom yenachem etchem betoch sha'ar aveylei Tziyon viYerushalayim"). At a Sephardic shiva house, visitors say, "May Heaven comfort you" (מן השמים תנוחמו – "min haShamayim tenuchamu").
It is considered a mitzvah for visitors to bring prepared food for the mourners. Among Sephardic Jews, food is served to the visitors and it is considered a mitzvah to make a blessing on the food in the merit of the deceased. Sephardim believe that every beracha (blessing) said elevates the "neshama" (soul) of the deceased, and so one should eat a variety of foods to be able to say more than one beracha. In an Ashkenazic home of mourning, food is not served except for the possibility of a light breakfast as a courtesy to those attending "Shaharit" (morning prayer), since they generally go straight to work after the service. The mourner is not allowed to serve food to the visitors and it is family and friends who take care of the guests and everyday issues.
Leaving the shiva house.
Leaving the shiva house is permitted when traveling between two locations where shiva is being observed by different members of the family, in cases of pikuach nefesh, e.g., a human life is in danger, whether that of the mourner or someone else; when something must be done to prevent another person from suffering and no one else can do it, such as caring for a child or an elderly or sick person; to feed or care for one's animals if there is no one else to do so; if another relative for whom the mourner is required to sit shiva dies, the mourner may attend the funeral. Leaving the house is also permitted on Shabbat.
Generally, one does not work or conduct business during shiva, although an exception may be made for those whose duties involve pikuach nefesh (doctors, nurses and emergency medical technicians). The same is true for mourners who are liable to suffer serious economic loss. If a mourner shares a business with a partner, and the partner can operate the business alone, the partner shall run the business. The partner is entitled to keep all profits made during the time, but if the partner does not exert additional effort, and the mourner will suffer economic loss, the partner is encouraged to donate the profits to the mourner, considering it "tzedaka". A mourner may do the minimal amount of work necessary in order to assure the survival of a business, or if his position is important in meeting the needs of the public and no substitute can be found. This includes elected officials whose work is necessary for the citizens. During the shiva period, the mourner is permitted to give instructions on how to handle business in his absence.
Shiva minyan.
During shiva, a "minyan" (a quorum of ten or more adult Jews) traditionally gather at the shiva home for services. The services held are like those at a synagogue, except that certain prayers or verses are either added or omitted. On days that the Torah is read in a synagogue, it is likewise read at the shiva home. An effort is made by the community to lend a Torah scroll to the mourner for this purpose. Kaddish is recited during the services; the mourner, if eligible, may recite kaddish.
Keriah.
The torn garment, usually a shirt, jacket or vest that "covers the heart," is worn throughout the shiva period (a practice known as "keriah"; alternative spellings "keriyah", "kria"), except on Shabbat. Conservative and Reform Jews will usually wear a torn piece of black ribbon instead of a torn garment. The torn garment symbolizes and expresses the grief of the mourner.

</doc>
<doc id="29383" url="http://en.wikipedia.org/wiki?curid=29383" title="Stonewall riots">
Stonewall riots

The Stonewall riots were a series of spontaneous, violent demonstrations by members of the gay community against a police raid that took place in the early morning hours of June 28, 1969, at the Stonewall Inn, located in the Greenwich Village neighborhood of Manhattan, New York City. They are widely considered to constitute the single most important event leading to the gay liberation movement and the modern fight for LGBT rights in the United States.
Gay Americans in the 1950s and 1960s faced an anti-homosexual legal system. Early homophile groups in the U.S. sought to prove that gay people could be assimilated into society, and they favored non-confrontational education for homosexuals and heterosexuals alike. The last years of the 1960s, however, were very contentious, as many social movements were active, including the African American Civil Rights Movement, the Counterculture of the 1960s, and antiwar demonstrations. These influences, along with the liberal environment of Greenwich Village, served as catalysts for the Stonewall riots.
Very few establishments welcomed openly gay people in the 1950s and 1960s. Those that did were often bars, although bar owners and managers were rarely gay. At the time, the Stonewall Inn was owned by the Mafia. It catered to an assortment of patrons and was known to be popular among the poorest and most marginalized people in the gay community: drag queens, representatives of the transgender community, effeminate young men, male prostitutes, and homeless youth. Police raids on gay bars were routine in the 1960s, but officers quickly lost control of the situation at the Stonewall Inn. They attracted a crowd that was incited to riot. Tensions between New York City police and gay residents of Greenwich Village erupted into more protests the next evening, and again several nights later. Within weeks, Village residents quickly organized into activist groups to concentrate efforts on establishing places for gays and lesbians to be open about their sexual orientation without fear of being arrested.
After the Stonewall riots, gays and lesbians in New York City faced gender, race, class, and generational obstacles to becoming a cohesive community. Within six months, two gay activist organizations were formed in New York, concentrating on confrontational tactics, and three newspapers were established to promote rights for gays and lesbians. Within a few years, gay rights organizations were founded across the U.S. and the world. On June 28, 1970, the first Gay Pride marches took place in New York, Los Angeles, San Francisco and Chicago commemorating the anniversary of the riots. Similar marches were organized in other cities. Today, Gay Pride events are held annually throughout the world toward the end of June to mark the Stonewall riots.
Background.
Homosexuality in 20th-century United States.
Following the social upheaval of World War II, many people in the United States felt a fervent desire to "restore the prewar social order and hold off the forces of change", according to historian Barry Adam. Spurred by the national emphasis on anti-communism, Senator Joseph McCarthy conducted hearings searching for communists in the U.S. government, the U.S. Army, and other government-funded agencies and institutions, leading to a national paranoia. Anarchists, communists, and other people deemed un-American and subversive were considered security risks. Homosexuals were included in this list by the U.S. State Department in 1950, on the theory that they were susceptible to blackmail. Under Secretary of State James E. Webb noted in a report, "It is generally believed that those who engage in overt acts of perversion lack the emotional stability of normal persons." Between 1947 and 1950, 1,700 federal job applications were denied, 4,380 people were discharged from the military, and 420 were fired from their government jobs for being suspected homosexuals.
Throughout the 1950s and 1960s, the Federal Bureau of Investigation (FBI) and police departments kept lists of known homosexuals, their favored establishments, and friends; the U.S. Post Office kept track of addresses where material pertaining to homosexuality was mailed. State and local governments followed suit: bars catering to homosexuals were shut down, and their customers were arrested and exposed in newspapers. Cities performed "sweeps" to rid neighborhoods, parks, bars, and beaches of gay people. They outlawed the wearing of opposite gender clothes, and universities expelled instructors suspected of being homosexual. Thousands of gay men and women were publicly humiliated, physically harassed, fired, jailed, or institutionalized in mental hospitals. Many lived double lives, keeping their private lives secret from their professional ones.
In 1952, the American Psychiatric Association listed homosexuality in the "Diagnostic and Statistical Manual" (DSM) as a mental disorder. A large-scale study of homosexuality in 1962 was used to justify inclusion of the disorder as a supposed pathological hidden fear of the opposite sex caused by traumatic parent–child relationships. This view was widely influential in the medical profession. In 1956, however, the psychologist Evelyn Hooker performed a study that compared the happiness and well-adjusted nature of self-identified homosexual men with heterosexual men and found no difference. Her study stunned the medical community and made her a hero to many gay men and lesbians, but homosexuality remained in the "DSM" until 1973.
Homophile activism.
In response to this trend, two organizations formed independently of each other to advance the cause of homosexuals and provide social opportunities where gays and lesbians could socialize without fear of being arrested. Los Angeles area homosexuals created the Mattachine Society in 1950, in the home of communist activist Harry Hay. Their objectives were to unify homosexuals, educate them, provide leadership, and assist "sexual deviants" with legal troubles. Facing enormous opposition to its radical approach, in 1953 the Mattachine shifted their focus to assimilation and respectability. They reasoned that they would change more minds about homosexuality by proving that gays and lesbians were normal people, no different from heterosexuals. Soon after, several women in San Francisco met in their living rooms to form the Daughters of Bilitis (DOB) for lesbians. Although the eight women who created the DOB initially came together to be able to have a safe place to dance, as the DOB grew they developed similar goals to the Mattachine, and urged their members to assimilate into general society.
One of the first challenges to government repression came in 1953. An organization named ONE, Inc. published a magazine called "ONE". The U.S. Postal Service refused to mail its August issue, which concerned homosexuals in heterosexual marriages, on the grounds that the material was obscene despite it being covered in brown paper wrapping. The case eventually went to the Supreme Court, which in 1958 ruled that ONE, Inc. could mail its materials through the Postal Service.
Homophile organizations—as homosexual groups were called—grew in number and spread to the East Coast. Gradually, members of these organizations grew bolder. Frank Kameny founded the Mattachine of Washington, D.C. He had been fired from the U.S. Army Map Service for being a homosexual, and sued unsuccessfully to be reinstated. Kameny wrote that homosexuals were no different from heterosexuals, often aiming his efforts at mental health professionals, some of whom attended Mattachine and DOB meetings telling members they were abnormal. In 1965, Kameny, inspired by the Civil Rights Movement, organized a picket of the White House and other government buildings to protest employment discrimination. The pickets shocked many gay people, and upset some of the leadership of Mattachine and the DOB. At the same time, demonstrations in the Civil Rights movement and opposition to the Vietnam War all grew in prominence, frequency, and severity throughout the 1960s, as did their confrontations with police forces.
Compton's Cafeteria riot.
On the outer fringes of the few small gay communities were people who challenged gender expectations. They were effeminate men and masculine women, or people assigned male at birth who dressed and lived as women and people assigned female at birth who dressed as men, respectively, either part or full-time. Contemporary nomenclature classified them as transvestites, and they were the most visible representatives of sexual minorities. They belied the carefully crafted image portrayed by the Mattachine Society and DOB that asserted homosexuals were respectable, normal people. The Mattachine and DOB considered the trials of being arrested for wearing clothing of the opposite gender as a parallel to the struggles of homophile organizations: similar but distinctly separate. Gay and transgender people staged a small riot in Los Angeles in 1959 in response to police harassment.
In a larger event in 1966 in San Francisco, drag queens, hustlers, and transvestites were sitting in Compton's Cafeteria when the police arrived to arrest men dressed as women. A riot ensued, with the patrons of the cafeteria slinging cups, plates, and saucers, and breaking the plexiglass windows in the front of the restaurant, and returning several days later to smash the windows again after they were replaced. Professor Susan Stryker classifies the Compton's Cafeteria riot as an "act of anti-transgender discrimination, rather than an act of discrimination against sexual orientation" and connects the uprising to the issues of gender, race, and class that were being downplayed by homophile organizations. It marked the beginning of transgender activism in San Francisco.
Greenwich Village.
The Manhattan neighborhoods of Greenwich Village and Harlem were home to a sizable homosexual population after World War I, when men and women who had served in the military took advantage of the opportunity to settle in larger cities. The enclaves of gays and lesbians, described by a newspaper story as "short-haired women and long-haired men", developed a distinct subculture through the following two decades. Prohibition inadvertently benefited gay establishments, as drinking alcohol was pushed underground along with other behaviors considered immoral. New York City passed laws against homosexuality in public and private businesses, but because alcohol was in high demand, speakeasies and impromptu drinking establishments were so numerous and temporary that authorities were unable to police them all.
The social repression of the 1950s resulted in a cultural revolution in Greenwich Village. A cohort of poets, later named the Beat poets, wrote about the evils of the social organization at the time, glorifying anarchy, drugs, and hedonistic pleasures over unquestioning social compliance, consumerism, and closed mindedness. Of them, Allen Ginsberg and William S. Burroughs—both Greenwich Village residents—also wrote bluntly and honestly about homosexuality. Their writings attracted sympathetic liberal-minded people, as well as homosexuals looking for a community.
By the early 1960s, a campaign to rid New York City of gay bars was in full effect by order of Mayor Robert F. Wagner, Jr., who was concerned about the image of the city in preparation for the 1964 World's Fair. The city revoked the liquor licenses of the bars, and undercover police officers worked to entrap as many homosexual men as possible. Entrapment usually consisted of an undercover officer who found a man in a bar or public park, engaged him in conversation; if the conversation headed toward the possibility that they might leave together—or the officer bought the man a drink—he was arrested for solicitation. One story in the "New York Post" described an arrest in a gym locker room, where the officer grabbed his crotch, moaning, and a man who asked him if he was all right was arrested. Few lawyers would defend cases as undesirable as these, and some of those lawyers kicked back their fees to the arresting officer.
The Mattachine Society succeeded in getting newly elected Mayor John Lindsay to end the campaign of police entrapment in New York City. They had a more difficult time with the New York State Liquor Authority (SLA). While no laws prohibited serving homosexuals, courts allowed the SLA discretion in approving and revoking liquor licenses for businesses that might become "disorderly". Despite the high population of gays and lesbians who called Greenwich Village home, very few places existed, other than bars, where they were able to congregate openly without being harassed or arrested. In 1966 the New York Mattachine held a "sip-in" at a Greenwich Village bar named Julius, which was frequented by gay men, to illustrate the discrimination homosexuals faced.
None of the bars frequented by gays and lesbians were owned by gay people. Almost all of them were owned and controlled by organized crime, who treated the regulars poorly, watered down the liquor, and overcharged for drinks. However, they also paid off police to prevent frequent raids.
Stonewall Inn.
The Stonewall Inn, located at 51 and 53 Christopher Street, along with several other establishments in the city, was owned by the Genovese family. In 1966, three members of the Mafia invested $3,500 to turn the Stonewall Inn into a gay bar, after it had been a restaurant and a nightclub for heterosexuals. Once a week a police officer would collect envelopes of cash as a payoff; the Stonewall Inn had no liquor license. It had no running water behind the bar—used glasses were run through tubs of water and immediately reused. There were no fire exits, and the toilets overran consistently. Though the bar was not used for prostitution, drug sales and other "cash transactions" took place. It was the only bar for gay men in New York City where dancing was allowed; dancing was its main draw since its re-opening as a gay club.
Visitors to the Stonewall Inn in 1969 were greeted by a bouncer who inspected them through a peephole in the door. The legal drinking age was 18, and to avoid unwittingly letting in undercover police (who were called "Lily Law", "Alice Blue Gown", or "Betty Badge"), visitors would have to be known by the doorman, or look gay. The entrance fee on weekends was $3, for which the customer received two tickets that could be exchanged for two drinks. Patrons were required to sign their names in a book to prove that the bar was a private "bottle club", but rarely signed their real names. There were two dance floors in the Stonewall; the interior was painted black, making it very dark inside, with pulsing gel lights or black lights. If police were spotted, regular white lights were turned on, signaling that everyone should stop dancing or touching. In the rear of the bar was a smaller room frequented by "queens;" it was one of two bars where effeminate men who wore makeup and teased their hair (though dressed in men's clothing) could go. Only a few transvestites, or men in full drag, were allowed in by the bouncers. The customers were "98 percent male" but a few lesbians sometimes came to the bar. Younger homeless adolescent males, who slept in nearby Christopher Park, would often try to get in so customers would buy them drinks. The age clientele ranged between the upper teens and early thirties, and the racial mix was evenly distributed among white, black, and Hispanic patrons. Because of its even mix of people, its location, and the attraction of dancing, the Stonewall Inn was known by many as ""the" gay bar in the city".
Police raids on gay bars were frequent—occurring on average once a month for each bar. Many bars kept extra liquor in a secret panel behind the bar, or in a car down the block, to facilitate resuming business as quickly as possible if alcohol was seized. Bar management usually knew about raids beforehand due to police tip-offs, and raids occurred early enough in the evening that business could commence after the police had finished. During a typical raid, the lights were turned on, and customers were lined up and their identification cards checked. Those without identification or dressed in full drag were arrested; others were allowed to leave. Some of the men, including those in drag, used their draft cards as identification. Women were required to wear three pieces of feminine clothing, and would be arrested if found not wearing them. Employees and management of the bars were also typically arrested. The period immediately before June 28, 1969, was marked by frequent raids of local bars—including a raid at the Stonewall Inn on the Tuesday before the riots—and the closing of the Checkerboard, the Tele-Star, and two other clubs in Greenwich Village.
Riots.
Police raid.
At 1:20 AM on Saturday, June 28, 1969, four plainclothes policemen in dark suits, two patrol officers in uniform, and Detective Charles Smythe and Deputy Inspector Seymour Pine arrived at the Stonewall Inn's double doors and announced "Police! We're taking the place!" Stonewall employees do not recall being tipped off that a raid was to occur that night, as was the custom. According to Duberman (p. 194), there was a rumor that one might happen, but since it was much later than raids generally took place, Stonewall management thought the tip was inaccurate. Days after the raid, one of the bar owners complained that the tipoff had never come, and that the raid was ordered by the Bureau of Alcohol, Tobacco, and Firearms, who objected that there were no stamps on the liquor bottles, indicating the alcohol was bootlegged.
David Carter presents information indicating that the Mafia owners of the Stonewall and the manager were blackmailing wealthier customers, particularly those who worked in the Financial District. They appeared to be making more money from extortion than they were from liquor sales in the bar. Carter deduces that when the police were unable to receive kickbacks from blackmail and the theft of negotiable bonds (facilitated by pressuring gay Wall Street customers), they decided to close the Stonewall Inn permanently. Two undercover policewomen and two undercover policemen had entered the bar earlier that evening to gather visual evidence, as the Public Morals Squad waited outside for the signal. Once inside, they called for backup from the Sixth Precinct using the bar's pay telephone. The music was turned off and the main lights were turned on. Approximately 205 people were in the bar that night. Patrons who had never experienced a police raid were confused. A few who realized what was happening began to run for doors and windows in the bathrooms, but police barred the doors. Michael Fader remembered, Things happened so fast you kind of got caught not knowing. All of a sudden there were police there and we were told to all get in lines and to have our identification ready to be led out of the bar."
The raid did not go as planned. Standard procedure was to line up the patrons, check their identification, and have female police officers take customers dressed as women to the bathroom to verify their sex, upon which any men dressed as women would be arrested. Those dressed as women that night refused to go with the officers. Men in line began to refuse to produce their identification. The police decided to take everyone present to the police station, after separating those cross-dressing in a room in the back of the bar. Maria Ritter, then known as Steve to her family, recalled, "My biggest fear was that I would get arrested. My second biggest fear was that my picture would be in a newspaper or on a television report in my mother's dress!" Both patrons and police recalled that a sense of discomfort spread very quickly, spurred by police who began to assault some of the lesbians by "feeling some of them up inappropriately" while frisking them.
"When did you ever see a fag fight back?... Now, times were a-changin'. Tuesday night was the last night for bullshit... Predominantly, the theme [w]as, "this shit has got to stop!""
—anonymous Stonewall riots participant
The police were to transport the bar's alcohol in patrol wagons. Twenty-eight cases of beer and nineteen bottles of hard liquor were seized, but the patrol wagons had not yet arrived, so patrons were required to wait in line for about 15 minutes. Those who were not arrested were released from the front door, but they did not leave quickly as usual. Instead, they stopped outside and a crowd began to grow and watch. Within minutes, between 100 and 150 people had congregated outside, some after they were released from inside the Stonewall, and some after noticing the police cars and the crowd. Although the police forcefully pushed or kicked some patrons out of the bar, some customers released by the police performed for the crowd by posing and saluting the police in an exaggerated fashion. The crowd's applause encouraged them further: "Wrists were limp, hair was primped, and reactions to the applause were classic."
When the first patrol wagon arrived, Inspector Pine recalled that the crowd—most of whom were homosexual—had grown to at least ten times the number of people who were arrested, and they all became very quiet. Confusion over radio communication delayed the arrival of a second wagon. The police began escorting Mafia members into the first wagon, to the cheers of the bystanders. Next, regular employees were loaded into the wagon. A bystander shouted, "Gay power!", someone began singing "We Shall Overcome", and the crowd reacted with amusement and general good humor mixed with "growing and intensive hostility". An officer shoved a transvestite, who responded by hitting him on the head with her purse as the crowd began to boo. Author Edmund White, who had been passing by, recalled, "Everyone's restless, angry, and high-spirited. No one has a slogan, no one even has an attitude, but something's brewing." Pennies, then beer bottles, were thrown at the wagon as a rumor spread through the crowd that patrons still inside the bar were being beaten.
A scuffle broke out when a woman in handcuffs was escorted from the door of the bar to the waiting police wagon several times. She escaped repeatedly and fought with four of the police, swearing and shouting, for about ten minutes. Described as "a typical New York butch" and "a dyke–stone butch", she had been hit on the head by an officer with a baton for, as one witness claimed, complaining that her handcuffs were too tight. Bystanders recalled that the woman, whose identity remains unknown (Stormé DeLarverie has been identified by some, including herself, as the woman, but accounts vary ), sparked the crowd to fight when she looked at bystanders and shouted, "Why don't you guys do something?" After an officer picked her up and heaved her into the back of the wagon, the crowd became a mob and went "berserk": "It was at that moment that the scene became explosive."
The last straw.
The police tried to restrain some of the crowd, and knocked a few people down, which incited bystanders even more. Some of those handcuffed in the wagon escaped when police left them unattended (deliberately, according to some witnesses). As the crowd tried to overturn the police wagon, two police cars and the wagon—with a few slashed tires—left immediately, with Inspector Pine urging them to return as soon as possible. The commotion attracted more people who learned what was happening. Someone in the crowd declared that the bar had been raided because "they didn't pay off the cops", to which someone else yelled "Let's pay them off!" Coins sailed through the air towards the police as the crowd shouted "Pigs!" and "Faggot cops!" Beer cans were thrown and the police lashed out, dispersing some of the crowd, who found a construction site nearby with stacks of bricks. The police, outnumbered by between 500 and 600 people, grabbed several people, including folk singer Dave Van Ronk—who had been attracted to the revolt from a bar two doors away from the Stonewall. Though Van Ronk was not gay, he had experienced police violence when he participated in antiwar demonstrations: "As far as I was concerned, anybody who'd stand against the cops was all right with me, and that's why I stayed in... Every time you turned around the cops were pulling some outrage or another." Ten police officers—including two policewomen—barricaded themselves, Van Ronk, Howard Smith (a writer for "The Village Voice"), and several handcuffed detainees inside the Stonewall Inn for their own safety.
Multiple accounts of the riot assert that there was no pre-existing organization or apparent cause for the demonstration; what ensued was spontaneous. Michael Fader explained,
We all had a collective feeling like we'd had enough of this kind of shit. It wasn't anything tangible anybody said to anyone else, it was just kind of like everything over the years had come to a head on that one particular night in the one particular place, and it was not an organized demonstration... Everyone in the crowd felt that we were never going to go back. It was like the last straw. It was time to reclaim something that had always been taken from us... All kinds of people, all different reasons, but mostly it was total outrage, anger, sorrow, everything combined, and everything just kind of ran its course. It was the police who were doing most of the destruction. We were really trying to get back in and break free. And we felt that we had freedom at last, or freedom to at least show that we demanded freedom. We weren't going to be walking meekly in the night and letting them shove us around—it's like standing your ground for the first time and in a really strong way, and that's what caught the police by surprise. There was something in the air, freedom a long time overdue, and we're going to fight for it. It took different forms, but the bottom line was, we weren't going to go away. And we didn't.
The only photograph taken during the first night of the riots shows the homeless youth that slept in nearby Christopher Park, scuffling with police. The Mattachine Society newsletter a month later offered its explanation of why the riots occurred: "It catered largely to a group of people who are not welcome in, or cannot afford, other places of homosexual social gathering... The Stonewall became home to these kids. When it was raided, they fought for it. That, and the fact that they had nothing to lose other than the most tolerant and broadminded gay place in town, explains why."
Garbage cans, garbage, bottles, rocks, and bricks were hurled at the building, breaking the windows. Witnesses attest that "flame queens", hustlers, and gay "street kids"—the most outcast people in the gay community—were responsible for the first volley of projectiles, as well as the uprooting of a parking meter used as a battering ram on the doors of the Stonewall Inn. Sylvia Rivera, who was in full drag and had been in the Stonewall during the raid, remembered: 
You've been treating us like shit all these years? Uh-uh. Now it's our turn!... It was one of the greatest moments in my life. 
The mob lit garbage on fire and stuffed it through the broken windows as the police grabbed a fire hose. Because it had no water pressure, the hose was ineffective in dispersing the crowd, and seemed only to encourage them. When demonstrators broke through the windows—which had been covered by plywood by the bar owners to deter the police from raiding the bar—the police inside unholstered their pistols. The doors flew open and officers pointed their weapons at the angry crowd, threatening to shoot. "The Village Voice" writer Howard Smith, in the bar with the police, took a wrench from the bar and stuffed it in his pants, unsure if he might have to use it against the mob or the police. He watched someone squirt lighter fluid into the bar; as it was lit and the police took aim, sirens were heard and fire trucks arrived. The onslaught had lasted 45 minutes.
Escalation.
The Tactical Police Force (TPF) of the New York City Police Department arrived to free the police trapped inside the Stonewall. One officer's eye was cut, and a few others were bruised from being struck by flying debris. Bob Kohler, who was walking his dog by the Stonewall that night, saw the TPF arrive: "I had been in enough riots to know the fun was over... The cops were totally humiliated. This never, ever happened. They were angrier than I guess they had ever been, because everybody else had rioted... but the fairies were not supposed to riot... no group had ever forced cops to retreat before, so the anger was just enormous. I mean, they wanted to kill." With larger numbers, police detained anyone they could and put them in patrol wagons to go to jail, though Inspector Pine recalled, "Fights erupted with the transvestites, who wouldn't go into the patrol wagon." His recollection was corroborated by another witness across the street who said, "All I could see about who was fighting was that it was transvestites and they were fighting furiously."
The TPF formed a phalanx and attempted to clear the streets by marching slowly and pushing the crowd back. The mob openly mocked the police. The crowd cheered, started impromptu kick lines, and sang to the tune of "The Howdy Doody Show" theme song: "We are the Stonewall girls/ We wear our hair in curls/ We don't wear underwear/ We show our pubic hairs." Lucian Truscott reported in "The Village Voice": "A stagnant situation there brought on some gay tomfoolery in the form of a chorus line facing the line of helmeted and club-carrying cops. Just as the line got into a full kick routine, the TPF advanced again and cleared the crowd of screaming gay power[-]ites down Christopher to Seventh Avenue." One participant who had been in the Stonewall during the raid recalled, "The police rushed us, and that's when I realized this is not a good thing to do, because they got me in the back with a nightstick." Another account stated, "I just can't ever get that one sight out of my mind. The cops with the [nightsticks] and the kick line on the other side. It was the most amazing thing... And all the sudden that kick line, which I guess was a spoof on the machismo... I think that's when I felt rage. Because people were getting smashed with bats. And for what? A kick line."
Craig Rodwell, owner of the Oscar Wilde Memorial Bookshop, reported watching police chase participants through the crooked streets, only to see them appear around the next corner behind the police. Members of the mob stopped cars, overturning one of them to block Christopher Street. Jack Nichols and Lige Clarke, in their column printed in "Screw", declared that "massive crowds of angry protesters chased [the police] for blocks screaming, 'Catch them!' "
By 4:00 in the morning the streets had nearly been cleared. Many people sat on stoops or gathered nearby in Christopher Park throughout the morning, dazed in disbelief at what had transpired. Many witnesses remembered the surreal and eerie quiet that descended upon Christopher Street, though there continued to be "electricity in the air". One commented: "There was a certain beauty in the aftermath of the riot... It was obvious, at least to me, that a lot of people really were gay and, you know, this was our street." Thirteen people had been arrested. Some in the crowd were hospitalized, and four police officers were injured. Almost everything in the Stonewall Inn was broken. Inspector Pine had intended to close and dismantle the Stonewall Inn that night. Pay phones, toilets, mirrors, jukeboxes, and cigarette machines were all smashed, possibly in the riot and possibly by the police.
Open rebellion.
During the siege of the Stonewall, Craig Rodwell called "The New York Times", the "New York Post", and the "Daily News" to inform them what was happening. All three papers covered the riots; "The New York Daily News" placed coverage on the front page. News of the riot spread quickly throughout Greenwich Village, fueled by rumors that it had been organized by the Students for a Democratic Society, the Black Panthers, or triggered by "a homosexual police officer whose roommate went dancing at the Stonewall against the officer's wishes". All day Saturday, June 28, people came to stare at the burned and blackened Stonewall Inn. Graffiti appeared on the walls of the bar, declaring "Drag power", "They invaded our rights", "Support gay power", and "Legalize gay bars", along with accusations of police looting, and—regarding the status of the bar—"We are open."
The next night, rioting again surrounded Christopher Street; participants remember differently which night was more frantic or violent. Many of the same people returned from the previous evening—hustlers, street youths, and "queens"—but they were joined by "police provocateurs", curious bystanders, and even tourists. Remarkable to many was the sudden exhibition of homosexual affection in public, as described by one witness: "From going to places where you had to knock on a door and speak to someone through a peephole in order to get in. We were just out. We were in the streets."
"You know, the guys there were so beautiful—they've lost that wounded look that fags all had 10 years ago " – Allen Ginsberg
 
Thousands of people had gathered in front of the Stonewall, which had opened again, choking Christopher Street until the crowd spilled into adjoining blocks. The throng surrounded buses and cars, harassing the occupants unless they either admitted they were gay or indicated their support for the demonstrators. Sylvia Rivera saw a friend of hers jump on a nearby car trying to drive through; the crowd rocked the car back and forth, terrifying its occupants. Another of Rivera's friends, Marsha P. Johnson, climbed a lamppost and dropped a heavy bag onto the hood of a police car, shattering the windshield. As on the previous evening, fires were started in garbage cans throughout the neighborhood. More than a hundred police were present from the Fourth, Fifth, Sixth, and Ninth Precincts, but after 2:00 a.m. the TPF arrived again. Kick lines and police chases waxed and waned; when police captured demonstrators, whom the majority of witnesses described as "sissies" or "swishes", the crowd surged to recapture them. Street battling ensued again until 4:00 a.m.
Beat poet and longtime Greenwich Village resident Allen Ginsberg lived on Christopher Street, and happened upon the jubilant chaos. After he learned of the riot that had occurred the previous evening, he stated, "Gay power! Isn't that great!... It's about time we did something to assert ourselves", and visited the open Stonewall Inn for the first time. While walking home, he declared to Lucian Truscott, "You know, the guys there were so beautiful—they've lost that wounded look that fags all had 10 years ago."
"Intolerable situation".
Activity in Greenwich Village was sporadic on Monday and Tuesday, partly due to rain. Police and Village residents had a few altercations, as both groups antagonized each other. Craig Rodwell and his partner Fred Sargeant took the opportunity the morning after the first riot to print and distribute 5,000 leaflets, one of them reading: "Get the Mafia and the Cops out of Gay Bars." The leaflets called for gays to own their own establishments, for a boycott of the Stonewall and other Mafia-owned bars, and for public pressure on the mayor's office to investigate the "intolerable situation".
Not everyone in the gay community considered the revolt a positive development. To many older homosexuals and many members of the Mattachine Society who had worked throughout the 1960s to promote homosexuals as no different from heterosexuals, the display of violence and effeminate behavior was embarrassing. Randy Wicker, who had marched in the first gay picket lines before the White House in 1965, said the "screaming queens forming chorus lines and kicking went against everything that I wanted people to think about homosexuals... that we were a bunch of drag queens in the Village acting disorderly and tacky and cheap." Others found the closing of the Stonewall Inn, termed a "sleaze joint", as advantageous to the Village.
On Wednesday, however, "The Village Voice" ran reports of the riots, written by Howard Smith and Lucian Truscott, that included unflattering descriptions of the events and its participants: "forces of faggotry", "limp wrists", and "Sunday fag follies". A mob descended upon Christopher Street once again and threatened to burn down the offices of "The Village Voice". Also in the mob of between 500 and 1,000 were other groups that had had unsuccessful confrontations with the police, and were curious how the police were defeated in this situation. Another explosive street battle took place, with injuries to demonstrators and police alike, looting in local shops, and arrests of five people. The incidents on Wednesday night lasted about an hour, and were summarized by one witness: "The word is out. Christopher Street shall be liberated. The fags have had it with oppression."
Aftermath.
The feeling of urgency spread throughout Greenwich Village, even to people who had not witnessed the riots. Many who were moved by the rebellion attended organizational meetings, sensing an opportunity to take action. On July 4, 1969, the Mattachine Society performed its annual picketing in front of Independence Hall in Philadelphia, called the Annual Reminder. Organizers Craig Rodwell, Frank Kameny, Randy Wicker, Barbara Gittings, and Kay Lahusen, who had all participated for several years, took a bus along with other picketers from New York City to Philadelphia. Since 1965, the pickets had been very controlled: women wore skirts and men wore suits and ties, and all marched quietly in organized lines. This year Rodwell remembered feeling restricted by the rules Kameny had set. When two women spontaneously held hands, Kameny broke them apart, saying, "None of that! None of that!" Rodwell, however, convinced about ten couples to hold hands. The hand-holding couples made Kameny furious, but they earned more press attention than all of the previous marches. Participant Lilli Vincenz remembered, "It was clear that things were changing. People who had felt oppressed now felt empowered." Rodwell returned to New York City determined to change the established quiet, meek ways of trying to get attention. One of his first priorities was planning Christopher Street Liberation Day.
Gay Liberation Front.
Although the Mattachine Society had existed since the 1950s, many of their methods now seemed too mild for people who had witnessed or been inspired by the riots. Mattachine recognized the shift in attitudes in a story from their newsletter entitled, "The Hairpin Drop Heard Around the World." When a Mattachine officer suggested an "amicable and sweet" candlelight vigil demonstration, a man in the audience fumed and shouted, "Sweet! "Bullshit!" That's the role society has been forcing these queens to play." With a flyer announcing: "Do You Think Homosexuals Are Revolting? You Bet Your Sweet Ass We Are!", the Gay Liberation Front (GLF) was soon formed, the first gay organization to use "gay" in its name. Previous organizations such as the Mattachine Society, the Daughters of Bilitis, and various homophile groups had masked their purpose by deliberately choosing obscure names.
The rise of militancy became apparent to Frank Kameny and Barbara Gittings—who had worked in homophile organizations for years and were both very public about their roles—when they attended a GLF meeting to see the new group. A young GLF member demanded to know who they were and what their credentials were. Gittings, nonplussed, stammered, "I'm gay. That's why I'm here." The GLF borrowed tactics from and aligned themselves with black and antiwar demonstrators with the ideal that they "could work to restructure American society". They took on causes of the Black Panthers, marching to the Women's House of Detention in support of Afeni Shakur, and other radical New Left causes. Four months after they formed, however, the group disbanded when members were unable to agree on operating procedure.
Gay Activists Alliance.
Within six months of the Stonewall riots, activists started a city-wide newspaper called "Gay"; they considered it necessary because the most liberal publication in the city—"The Village Voice"—refused to print the word "gay" in GLF advertisements seeking new members and volunteers. Two other newspapers were initiated within a six-week period: "Come Out!" and "Gay Power"; the readership of these three periodicals quickly climbed to between 20,000 and 25,000.
GLF members organized several same-sex dances, but GLF meetings were chaotic. When Bob Kohler asked for clothes and money to help the homeless youth who had participated in the riots, many of whom slept in Christopher Park or Sheridan Square, the response was a discussion on the downfall of capitalism. In late December 1969, several people who had visited GLF meetings and left out of frustration formed the Gay Activists Alliance (GAA). The GAA was to be entirely focused on gay issues, and more orderly. Their constitution started, "We as liberated homosexual activists demand the freedom for expression of our dignity and value as human beings." The GAA developed and perfected a confrontational tactic called a zap, where they would catch a politician off guard during a public relations opportunity, and force him or her to acknowledge gay and lesbian rights. City councilmen were zapped, and Mayor John Lindsay was zapped several times—once on television when GAA members made up the majority of the audience.
Raids on gay bars had not stopped after the Stonewall riots. In March 1970, Deputy Inspector Seymour Pine raided the Zodiac and 17 Barrow Street. An after-hours gay club with no liquor or occupancy licenses called The Snake Pit was soon raided, and 167 people were arrested. One of them was Diego Vinales, an Argentinian national so frightened that he might be deported as a homosexual that he tried to escape the police precinct by jumping out of a two-story window, impaling himself on a 14 in spike fence. "The New York Daily News" printed a graphic photo of the young man's impalement on the front page. GAA members organized a march from Christopher Park to the Sixth Precinct in which hundreds of gays, lesbians, and liberal sympathizers peacefully confronted the TPF. They also sponsored a letter-writing campaign to Mayor Lindsay in which the Greenwich Village Democratic Party and Congressman Ed Koch sent pleas to end raids on gay bars in the city.
The Stonewall Inn lasted only a few weeks after the riot. By October 1969 it was up for rent. Village residents surmised it was too notorious a location, and Rodwell's boycott discouraged business.
Gay Pride.
Christopher Street Liberation Day on June 28, 1970 marked the first anniversary of the Stonewall riots with an assembly on Christopher Street; with simultaneous Gay Pride marches in Los Angeles and Chicago, these mark the first Gay Pride marches in U.S. history. The next year, Gay Pride marches took place in Boston, Dallas, Milwaukee, London, Paris, West Berlin, and Stockholm. The New York Gay Pride march covered 51 blocks, from Christopher Street to Central Park. The march took less than half the scheduled time due to excitement, but also due to wariness about walking through the city with gay banners and signs. Although the parade permit was delivered only two hours before the start of the march, the marchers encountered little resistance from onlookers. "The New York Times" reported (on the front page) that the marchers took up the entire street for about 15 city blocks. Reporting by "The Village Voice" was positive, describing "the out-front resistance that grew out of the police raid on the Stonewall Inn one year ago".
"There was little open animosity, and some bystanders applauded when a tall, pretty girl carrying a sign "I am a Lesbian" walked by." – "The New York Times" coverage of Gay Liberation Day, 1970
 
By 1972 the participating cities included Atlanta, Buffalo, Detroit, Washington D.C., Miami, and Philadelphia, as well as San Francisco.
Frank Kameny soon realized the pivotal change brought by the Stonewall riots. An organizer of gay activism in the 1950s, he was used to persuasion, trying to convince heterosexuals that gay people were no different than they were. When he and other people marched in front of the White House, the State Department, and Independence Hall only five years earlier, their objective was to look as if they could work for the U.S. government. Ten people marched with Kameny then, and they alerted no press to their intentions. Although he was stunned by the upheaval by participants in the Annual Reminder in 1969, he later observed, "By the time of Stonewall, we had fifty to sixty gay groups in the country. A year later there was at least fifteen hundred. By two years later, to the extent that a count could be made, it was twenty-five hundred."
Similar to Kameny's regret at his own reaction to the shift in attitudes after the riots, Randy Wicker came to describe his embarrassment as "one of the greatest mistakes of his life". The image of gays retaliating against police, after so many years of allowing such treatment to go unchallenged, "stirred an unexpected spirit among many homosexuals". Kay Lahusen, who photographed the marches in 1965, stated, "Up to 1969, this movement was generally called the homosexual or homophile movement... Many new activists consider the Stonewall uprising the birth of the gay liberation movement. Certainly it was the birth of gay pride on a massive scale." David Carter, in his article "What made Stonewall different", explained that even though there were several uprisings before Stonewall, the reason Stonewall was so historical was that thousands of people were involved, the riot lasted a long time (six days), it was the first to get major media coverage, and it sparked the formation of many gay rights groups.
Legacy.
Unlikely community.
Within two years of the Stonewall riots there were gay rights groups in every major American city, as well as Canada, Australia, and Western Europe. People who joined activist organizations after the riots had very little in common other than their same-sex attraction. Many who arrived at GLF or GAA meetings were taken aback by the number of gay people in one place. Race, class, ideology, and gender became frequent obstacles in the years after the riots. This was illustrated during the 1973 Stonewall rally when, moments after Barbara Gittings exuberantly praised the diversity of the crowd, feminist activist Jean O'Leary protested what she perceived as the mocking of women by cross-dressers and drag queens in attendance. During a speech by O'Leary, in which she claimed that drag queens made fun of women for entertainment value and profit, Sylvia Rivera and Lee Brewster jumped on the stage and shouted "You go to bars because of what drag queens did for you, and "these bitches" tell us to quit being ourselves!" Both the drag queens and lesbian feminists in attendance left in disgust.
O'Leary also worked in the early 1970s to exclude trans people from gay rights issues because she felt that rights for trans people would be too difficult to attain. Sylvia Rivera left gay activism in the 1970s to work on issues for transgender people and cross-dressers. The initial disagreements between participants in the movements, however, often evolved after further reflection. O'Leary later regretted her stance against the drag queens attending in 1973: "Looking back, I find this so embarrassing because my views have changed so much since then. I would never pick on a transvestite now." "It was horrible. How could I work to exclude transvestites and at the same time criticize the feminists who were doing their best back in those days to exclude lesbians?"
O'Leary was referring to the Lavender Menace, a description by second wave feminist Betty Friedan for attempts by members of the National Organization for Women (NOW) to distance themselves from the perception of NOW as a haven for lesbians. As part of this process, Rita Mae Brown and other lesbians who had been active in NOW were forced out. They staged a protest in 1970 at the Second Congress to Unite Women, and earned the support of many NOW members, finally gaining full acceptance in 1971.
The growth of lesbian feminism in the 1970s at times so conflicted with the gay liberation movement that some lesbians refused to work with gay men. Many lesbians found men's attitudes patriarchal and chauvinistic, and saw in gay men the same misguided notions about women as they saw in heterosexual men. The issues most important to gay men—entrapment and public solicitation—were not shared by lesbians. In 1977 a Lesbian Pride Rally was organized as an alternative to sharing gay men's issues, especially what Adrienne Rich termed "the violent, self-destructive world of the gay bars". Veteran gay activist Barbara Gittings chose to work in the gay rights movement, rationalizing "It's a matter of where does it hurt the most? For me it hurts the most not in the female arena, but the gay arena."
Throughout the 1970s gay activism had significant successes. One of the first and most important was the "zap" in May 1970 by the Los Angeles GLF at a convention of the American Psychiatric Association (APA). At a conference on behavior modification, during a film demonstrating the use of electroshock therapy to decrease same-sex attraction, Morris Kight and GLF members in the audience interrupted the film with shouts of "Torture!" and "Barbarism!" They took over the microphone to announce that medical professionals who prescribed such therapy for their homosexual patients were complicit in torturing them. Although 20 psychiatrists in attendance left, the GLF spent the hour following the zap with those remaining, trying to convince them that homosexuals were not mentally ill. When the APA invited gay activists to speak to the group in 1972, activists brought John E. Fryer, a gay psychiatrist who wore a mask, because he felt his practice was in danger. In December 1973—in large part due to the efforts of gay activists—the APA voted unanimously to remove homosexuality from the "Diagnostic and Statistical Manual".
Gay men and lesbians came together to work in grassroots political organizations responding to organized resistance in 1977. A coalition of conservatives named Save Our Children staged a campaign to repeal a civil rights ordinance in Dade County, Florida. Save Our Children was successful enough to influence similar repeals in several American cities in 1978. However, the same year a campaign in California called the Briggs Initiative, designed to force the dismissal of homosexual public school employees, was defeated. Reaction to the influence of Save Our Children and the Briggs Initiative in the gay community was so significant that it has been called the second Stonewall for many activists, marking their initiation into political participation.
Rejection of gay subculture.
The Stonewall riots marked such a significant turning point that many aspects of prior gay and lesbian culture, such as bar culture formed from decades of shame and secrecy, were forcefully ignored and denied. Historian Martin Duberman writes, "The decades preceding Stonewall... continue to be regarded by most gays and lesbians as some vast neolithic wasteland." Historian Barry Adam notes, "Every social movement must choose at some point what to retain and what to reject out of its past. What traits are the results of oppression and what are healthy and authentic?" In conjunction with the growing feminist movement of the early 1970s, roles of butch and femme that developed in lesbian bars in the 1950s and 1960s were rejected, because as one writer put it: "all role playing is sick." Lesbian feminists considered the butch roles as archaic imitations of masculine behavior. Some women, according to Lillian Faderman, were eager to shed the roles they felt forced into playing. The roles returned for some women in the 1980s, although they allowed for more flexibility than before Stonewall.
Author Michael Bronski highlights the "attack on pre-Stonewall culture", particularly gay pulp fiction for men, where the themes often reflected self-hatred or ambivalence about being gay. Many books ended unsatisfactorily and drastically, often with suicide, and writers portrayed their gay characters as alcoholics or deeply unhappy. These books, which he describes as "an enormous and cohesive literature by and for gay men", have not been reissued and are lost to later generations. Dismissing the reason simply as political correctness, Bronski writes, "gay liberation was a youth movement whose sense of history was defined to a large degree by rejection of the past."
Lasting impact.
The riots spawned from a bar raid became a literal example of gays and lesbians fighting back, and a symbolic call to arms for many people. Historian David Carter remarks in his book about the Stonewall riots that the bar itself was a complex business that represented a community center, an opportunity for the Mafia to blackmail its own customers, a home, and a place of "exploitation and degradation". The true legacy of the Stonewall riots, Carter insists, is the "ongoing struggle for lesbian, gay, bisexual, and transgender equality". Historian Nicholas Edsall writes, Stonewall has been compared to any number of acts of radical protest and defiance in American history from the Boston Tea Party on. But the best and certainly a more nearly contemporary analogy is with Rosa Parks' refusal to move to the back of the bus in Montgomery, Alabama, in December 1955, which sparked the modern civil rights movement. Within months after Stonewall radical gay liberation groups and newsletters sprang up in cities and on college campuses across America and then across all of northern Europe as well.
Before the rebellion at the Stonewall Inn, homosexuals were, as historians Dudley Clendinen and Adam Nagourney write, a secret legion of people, known of but discounted, ignored, laughed at or despised. And like the holders of a secret, they had an advantage which was a disadvantage, too, and which was true of no other minority group in the United States. They were invisible. Unlike African Americans, women, Native Americans, Jews, the Irish, Italians, Asians, Hispanics, or any other cultural group which struggled for respect and equal rights, homosexuals had no physical or cultural markings, no language or dialect which could identify them to each other, or to anyone else... But that night, for the first time, the usual acquiescence turned into violent resistance... From that night the lives of millions of gay men and lesbians, and the attitude toward them of the larger culture in which they lived, began to change rapidly. People began to appear in public as homosexuals, demanding respect.
Historian Lillian Faderman calls the riots the "shot heard round the world", explaining, "The Stonewall Rebellion was crucial because it sounded the rally for that movement. It became an emblem of gay and lesbian power. By calling on the dramatic tactic of violent protest that was being used by other oppressed groups, the events at the Stonewall implied that homosexuals had as much reason to be disaffected as they."
Joan Nestle co-founded the Lesbian Herstory Archives in 1974, and credits "its creation to that night and the courage that found its voice in the streets." Cautious, however, not to attribute the start of gay activism to the Stonewall riots, Nestle writes,
I certainly don't see gay and lesbian history starting with Stonewall... and I don't see resistance starting with Stonewall. What I do see is a historical coming together of forces, and the sixties changed how human beings endured things in this society and what they refused to endure... Certainly something special happened on that night in 1969, and we've made it more special in our need to have what I call a point of origin... it's more complex than saying that it all started with Stonewall.
The events of the early morning of June 28, 1969 were not the first instances of homosexuals fighting back against police in New York City and elsewhere. Not only had the Mattachine Society been active in major cities such as Los Angeles and Chicago, but similarly marginalized people started the riot at Compton's Cafeteria in 1966, and another riot responded to a raid on Los Angeles' Black Cat Tavern in 1967. However, several circumstances were in place that made the Stonewall riots memorable. The location of the raid was a factor: it was across the street from "The Village Voice" offices, and the narrow crooked streets gave the rioters advantage over the police. Many of the participants and residents of Greenwich Village were involved in political organizations that were effectively able to mobilize a large and cohesive gay community in the weeks and months after the rebellion. The most significant facet of the Stonewall riots, however, was the commemoration of them in Christopher Street Liberation Day, which grew into the annual Gay Pride events around the world.
The middle of the 1990s was marked by the inclusion of bisexuals as a represented group within the gay community, when they successfully sought to be included on the platform of the 1993 March on Washington for Lesbian, Gay and Bi Equal Rights and Liberation. Transgender people also asked to be included, but were not, though trans-inclusive language was added to the march's list of demands. The transgender community continued to find itself simultaneously welcome and at odds with the gay community as attitudes about binary and fluid sexual orientation and gender developed and came increasingly into conflict. In 1994, New York City celebrated "Stonewall 25" with a march that went past the United Nations Headquarters and into Central Park. Estimates put the attendance at 1.1 million people. Sylvia Rivera led an alternate march in New York City in 1994 to protest the exclusion of transgender people from the events. Attendance at Gay Pride events has grown substantially over the decades. Most large cities around the world now have some kind of Pride demonstration. Pride events in some cities mark the largest annual celebration of any kind. The growing trend towards commercializing marches into parades—with events receiving corporate sponsorship—has caused concern about taking away the autonomy of the original grassroots demonstrations that put inexpensive activism in the hands of individuals.
In June 1999 the U.S. Department of the Interior designated 51 and 53 Christopher Street, the street itself, and the surrounding streets as a National Historic Landmark, the first of significance to the lesbian, gay, bisexual and transgender community. In a dedication ceremony, Assistant Secretary of the Department of the Interior John Berry stated, "Let it forever be remembered that here—on this spot—men and women stood proud, they stood fast, so that we may be who we are, we may work where we will, live where we choose and love whom our hearts desire."
On June 1, 2009, President Barack Obama declared June 2009 Lesbian, Gay, Bisexual, and Transgender Pride Month, citing the riots as a reason to "commit to achieving equal justice under law for LGBT Americans". The year marked the 40th anniversary of the riots, giving journalists and activists cause to reflect on progress made since 1969. Frank Rich in "The New York Times" noted that no federal legislation exists to protect the rights of gay Americans. An editorial in the "Washington Blade" compared the scruffy, violent activism during and following the Stonewall riots to the lackluster response to failed promises given by President Obama; for being ignored, wealthy LGBT activists reacted by promising to give less money to Democratic causes. Two years later, the Stonewall Inn served as a rallying point for celebrations after the New York Senate voted to pass same-sex marriage. The act was signed into law by Governor Andrew Cuomo on June 24, 2011.
Obama also referenced the Stonewall riots in a call for full equality during his second inaugural address on January 21, 2013:"We, the people, declare today that the most evident of truths—that all of us are created equal—is the star that guides us still; just as it guided our forebears through Seneca Falls, and Selma, and Stonewall... Our journey is not complete until our gay brothers and sisters are treated like anyone else under the law—for if we are truly created equal, then surely the love we commit to one another must be equal as well."This was a historic moment, being the first time that a president mentioned gay rights or the word "gay" in an inaugural address.
In 2014 a marker dedicated to the Stonewall riots was included in the Legacy Walk, an outdoor public display in Chicago celebrating LGBT history and people.
On May 29, 2015, the New York City Landmarks Preservation Commission announced it would officially consider designating Stonewall as a landmark, the first city location to be considered based on its LGBT cultural significance alone.
See also.
Films
References.
Bibliography.
</dl>

</doc>
<doc id="29426" url="http://en.wikipedia.org/wiki?curid=29426" title="Sino-Indian War">
Sino-Indian War

The Sino-Indian War (Hindi: भारत-चीन युद्ध "Bhārat-Chīn Yuddh"), also known as the Sino-Indian Border Conflict (), was a war between China and India that occurred in 1962. A disputed Himalayan border was the main pretext for war, but other issues played a role. There had been a series of violent border incidents after the 1959 Tibetan uprising, when India had granted asylum to the Dalai Lama. India initiated a Forward Policy in which it placed outposts along the border, including several north of the McMahon Line, the eastern portion of a Line of Actual Control proclaimed by Chinese Premier Zhou Enlai in 1959.
Unable to reach political accommodation on disputed territory along the 3,225-kilometre-long Himalayan border, the Chinese launched simultaneous offensives in Ladakh and across the McMahon Line on 20 October 1962. Chinese troops advanced over Indian forces in both theatres, capturing Rezang la in Chushul in the western theatre, as well as Tawang in the eastern theatre. The war ended when the Chinese declared a ceasefire on 20 November 1962, and simultaneously announced its withdrawal from the disputed area.
The Sino-Indian War is notable for the harsh mountain conditions under which much of the fighting took place, entailing large-scale combat at altitudes of over 4,000 metres (14,000 feet). The Sino-Indian War was also noted for the non-deployment of the navy or air force by either the Chinese or Indian side.
Location.
China and India shared a long border, sectioned into three stretches by Nepal, Sikkim (then an Indian protectorate), and Bhutan, which follows the Himalayas between Burma and what was then West Pakistan. A number of disputed regions lie along this border. At its western end is the Aksai Chin region, an area the size of Switzerland, that sits between the Chinese autonomous region of Xinjiang and Tibet (which China declared as an autonomous region in 1965). The eastern border, between Burma and Bhutan, comprises the present Indian state of Arunachal Pradesh (formerly the North East Frontier Agency). Both of these regions were overrun by China in the 1962 conflict.
Most combat took place at high altitudes. The Aksai Chin region is a desert of salt flats around 5,000 metres above sea level, and Arunachal Pradesh is mountainous with a number of peaks exceeding 7000 metres. The Chinese Army had possession of one of the highest ridges in the regions. The high altitude and freezing conditions also cause logistical and welfare difficulties; in past similar conflicts (such as the Italian Campaign of World War I) more casualties have been caused by the harsh conditions than enemy action. The Sino-Indian War was no different, with many troops on both sides dying in the freezing cold.
Background.
The cause of the war was a dispute over the sovereignty of the widely separated Aksai Chin and Arunachal Pradesh border regions. Aksai Chin, claimed by India to belong to Kashmir and by China to be part of Xinjiang, contains an important road link that connects the Chinese regions of Tibet and Xinjiang. China's construction of this road was one of the triggers of the conflict.
Aksai Chin.
The western portion of the Sino-Indian boundary originated in 1834, with the Sikh Confederacy's conquest of Ladakh. In 1842, the Sikh Confederacy, which at the time ruled over much of Northern India (including the frontier regions of Jammu and Kashmir), signed a treaty which guaranteed the integrity of its existing borders with its neighbours. The British defeat of the Sikhs in 1846 resulted in transfer of sovereignty over Ladakh, part of the Jammu and Kashmir region, to the British, and British commissioners contacted Chinese officials to negotiate the border. The boundaries at its two extremities, Pangong Lake and Karakoram Pass, were well defined, but the Aksai Chin area in between lay undefined.
W. H. Johnson, a civil servant with the Survey of India, proposed the "Johnson Line" in 1865, which put Aksai Chin in Kashmir. Johnson presented this line to the Maharaja of Kashmir, who then claimed the 18,000 square kilometres contained within. Johnson's work was severely criticized as inaccurate. His boundary line was described as "patently absurd", and extending further north than the Indian claim. Johnson was reprimanded by the British Government for crossing into Khotan without permission and resigned from the Survey. According to Francis Younghusband, who explored the region in the late 1880s, there was only an abandoned fort and not one inhabited house at Shahidulla when he was there - it was just a convenient staging post for the nomadic Kirghiz. The abandoned fort had apparently been built a few years earlier by the Kashmiris. In 1878 the Chinese had reconquered Xinjiang, and by 1890 they already had Shahidulla before the issue was decided. By 1892, China had erected boundary markers at Karakoram Pass.
In 1893, Hung Ta-chen, a senior Chinese official at Kashgar, handed a map of the boundary proposed by China to George Macartney, the British consul-general at Kashgar. This boundary placed the Lingzi Tang plains, which are south of the Laktsang range, in India, and Aksai Chin proper, which is north of the Laktsang range, in China. Macartney agreed with the proposal and forwarded it to the British Indian government. The British presented this line, known as the Macartney-MacDonald line, to the Chinese in 1899 in a note by Sir Claude MacDonald. In 1911 the Xinhai Revolution resulted in power shifts in China, and by the end of World War I, the British officially used the Johnson Line. However they took no steps to establish outposts or assert actual control on the ground. According to Neville Maxwell, the British had used as many as 11 different boundary lines in the region, as their claims shifted with the political situation. From 1917 to 1933, the "Postal Atlas of China", published by the Government of China in Peking had shown the boundary in Aksai Chin as per the Johnson line, which runs along the Kunlun mountains. The "Peking University Atlas", published in 1925, also put the Aksai Chin in India.:101 Upon independence in 1947, the government of India used the Johnson Line as the basis for its official boundary in the west, which included the Aksai Chin. On 1 July 1954, India's first Prime Minister Jawaharlal Nehru definitively stated the Indian position, claiming that Aksai Chin had been part of the Indian Ladakh region for centuries, and that the border (as defined by the Johnson Line) was non-negotiable. According to George N. Patterson, when the Indian government finally produced a report detailing the alleged proof of India's claims to the disputed area, "the quality of the Indian evidence was very poor, including some very dubious sources indeed".:275
In 1956–57, China constructed a road through Aksai Chin, connecting Xinjiang and Tibet, which ran south of the Johnson Line in many places. Aksai Chin was easily accessible to the Chinese, but access from India, which meant negotiating the Karakoram mountains, was much more difficult. The road came on Chinese maps published in 1958.
The McMahon Line.
In 1826, British India gained a common border with China after the British wrested control of Manipur and Assam from the Burmese, following the First Anglo-Burmese War of 1824–1826. In 1847, Major J. Jenkins, agent for the North East Frontier, reported that the Tawang was part of Tibet. In 1872, four monastic officials from Tibet arrived in Tawang and supervised a boundary settlement with Major R. Graham, NEFA official, which included the Tawang Tract as part of Tibet. Thus, in the last half of the 19th century, it was clear that the British treated the Tawang Tract as part of Tibet. This boundary was confirmed in a 1 June 1912 note from the British General Staff in India, stating that the "present boundary (demarcated) is south of Tawang, running westwards along the foothills from near Ugalguri to the southern Bhutanese border." A 1908 map of The Province of Eastern Bengal and Assam prepared for the Foreign Department of the Government of India, showed the international boundary from Bhutan continuing to the Baroi River, following the Himalayas foothill alignment. In 1913, representatives of Great Britain, China and Tibet attended a conference in Simla regarding the borders between Tibet, China and British India. Whilst all three representatives initialed the agreement, Beijing later objected to the proposed boundary between the regions of Outer Tibet and Inner Tibet, and did not ratify it. The details of the Indo-Tibetan boundary was not revealed to China at the time. The foreign secretary of the British Indian government, Henry McMahon, who had drawn up the proposal, decided to bypass the Chinese (although instructed not to by his superiors) and settle the border bilaterally by negotiating directly with Tibet. According to later Indian claims, this border was intended to run through the highest ridges of the Himalayas, as the areas south of the Himalayas were traditionally Indian. However, the McMahon Line lay south of the boundary India claims. India's government held the view that the Himalayas were the ancient boundaries of the Indian subcontinent, and thus should be the modern boundaries of India, while it is the position of the Chinese government that the disputed area in the Himalayas have been geographically and culturally part of Tibet since ancient times.
Months after the Simla agreement, China set up boundary markers south of the McMahon Line. T. O'Callaghan, an official in the Eastern Sector of the North East Frontier, relocated all these markers to a location slightly south of the McMahon Line, and then visited Rima to confirm with Tibetan officials that there was no Chinese influence in the area. The British-run Government of India initially rejected the Simla Agreement as incompatible with the Anglo-Russian Convention of 1907, which stipulated that neither party was to negotiate with Tibet "except through the intermediary of the Chinese government". The British and Russians cancelled the 1907 agreement by joint consent in 1921. It was not until the late 1930s that the British started to use the McMahon Line on official maps of the region.
China took the position that the Tibetan government should not have been allowed to make a such a treaty, rejecting Tibet's claims of independent rule. For its part, Tibet did not object to any section of the McMahon Line excepting the demarcation of the trading town of Tawang, which the Line placed under British-Indian jurisdiction. However, up until World War II, Tibetan officials were allowed to administer Tawang with complete authority. Due to the increased threat of Japanese and Chinese expansion during this period, British Indian troops secured the town as part of the defence of India's eastern border.
In the 1950s, India began actively patrolling the region. It found that, at multiple locations, the highest ridges actually fell north of the McMahon Line. Given India's historic position that the original intent of the line was to separate the two nations by the highest mountains in the world, in these locations India extended its forward posts northward to the ridges, regarding this move as compliant with the original border proposal, although the Simla Convention did not explicitly state this intention.
Events leading up to war.
Tibet and the border dispute.
The 1940s saw huge change in South Asia with the Partition of India in 1947 (resulting in the establishment of the two new states of India and Pakistan), and the establishment of the People's Republic of China (PRC) in 1949. One of the most basic policies for the new Indian government was that of maintaining cordial relations with China, reviving its ancient friendly ties. India was among the first nations to grant diplomatic recognition to the newly created PRC.
At the time, Chinese officials issued no condemnation of Nehru's claims or made any opposition to Nehru's open declarations of control over Aksai Chin. In 1956, Chinese Premier Zhou Enlai stated that he had no claims over Indian controlled territory. He later argued that Aksai Chin was already under Chinese jurisdiction and that the McCartney MacDonald Line was the line China could accept. Zhou later argued that as the boundary was undemarcated and had never been defined by treaty between any Chinese or Indian government, the Indian government could not unilaterally define Aksai Chin's borders.
In 1950, the Chinese People's Liberation Army annexed Tibet and later the Chinese extended their influence by building a road in 1956–67 and placing border posts in Aksai Chin. India found out after the road was completed, protested against these moves and decided to look for a diplomatic solution to ensure a stable Sino-Indian border. To resolve any doubts about the Indian position, Prime Minister Jawaharlal Nehru declared in parliament that India regarded the McMahon Line as its official border. The Chinese expressed no concern at this statement, and in 1951 and 1952, the government of China asserted that there were no frontier issues to be taken up with India.
In 1954, Prime Minister Nehru wrote a memo calling for India's borders to be clearly defined and demarcated; in line with previous Indian philosophy, Indian maps showed a border that, in some places, lay north of the McMahon Line. Chinese Premier Zhou Enlai, in November 1956, again repeated Chinese assurances that the People's Republic had no claims on Indian territory, although official Chinese maps showed 120000 km2 of territory claimed by India as Chinese. CIA documents created at the time revealed that Nehru had ignored Burmese premier Ba Swe when he warned Nehru to be cautious when dealing with Zhou. They also allege that Zhou purposefully told Nehru that there were no border issues with India.
In 1954, China and India negotiated the Five Principles of Peaceful Coexistence, by which the two nations agreed to abide in settling their disputes. India presented a frontier map which was accepted by China, and the slogan "Hindi-Chini bhai-bhai" (Indians and Chinese are brothers) was popular then. However, Nehru in 1958, had privately told G. Parthasarathi, the Indian envoy to China not to trust the Chinese at all and send all communications directly to him, bypassing the Defence Minister VK Krishna Menon since his communist background clouded his thinking about China. According to Georgia Tech political analyst John W Garver, Nehru's policy on Tibet was to create a strong Sino-Indian partnership which would be catalysed through agreement and compromise on Tibet. Garver believes that Nehru's previous actions had given him confidence that China would be ready to form an "Asian Axis" with India.
This apparent progress in relations suffered a major setback when, in 1959, Nehru accommodated the Tibetan religious leader at the time, the 14th Dalai Lama, who fled Lhasa after a failed Tibetan uprising against Chinese rule. The Chairman of the Chinese Communist Party, Mao Zedong, was enraged and asked the Xinhua News Agency to produce reports on Indian expansionists operating in Tibet.
Border incidents continued through this period. In August 1959, the People's Liberation Army took an Indian prisoner at Longju, which had an ambiguous position in the McMahon Line, and two months later in Aksai Chin, a clash led to the death of nine Indian frontier policemen.
On 2 October, Soviet Premier Nikita Khrushchev defended Nehru in a meeting with Mao. This action reinforced China's impression that the Soviet Union, the United States and India all had expansionist designs on China. The People's Liberation Army went so far as to prepare a self-defence counterattack plan. Negotiations were restarted between the nations, but no progress was made.
As a consequence of their non-recognition of the McMahon Line, China's maps showed both the North East Frontier Area (NEFA) and Aksai Chin to be Chinese territory. In 1960, Zhou Enlai unofficially suggested that India drop its claims to Aksai Chin in return for a Chinese withdrawal of claims over NEFA. Adhering to his stated position, Nehru believed that China did not have a legitimate claim over either of these territories, and thus was not ready to concede them. This adamant stance was perceived in China as Indian opposition to Chinese rule in Tibet. Nehru declined to conduct any negotiations on the boundary until Chinese troops withdrew from Aksai Chin, a position supported by the international community. India produced numerous reports on the negotiations, and translated Chinese reports into English to help inform the international debate. China believed that India was simply securing its claim lines in order to continue its "grand plans in Tibet". India's stance that China withdraw from Aksai Chin caused continual deterioration of the diplomatic situation to the point that internal forces were pressuring Nehru to take a military stance against China.
1960 meetings to resolve the boundary question.
In 1960, based on an agreement between Nehru and Chou En-Lai, officials from India and China held discussions in order to settle the boundary dispute.:91 China and India disagreed on the major watershed that defined the boundary in the western sector.:96 The Chinese statements with respect to their border claims often misrepresented the cited sources.:99
The Forward Policy.
At the beginning of 1961, Nehru appointed General B. M. Kaul as army Chief of General Staff, but he refused to increase military spending and prepare for a possible war. According to James Barnard Calvin of the U.S. Navy, in 1959, India started sending Indian troops and border patrols into disputed areas. This program created both skirmishes and deteriorating relations between India and China. The aim of this policy was to create outposts behind advancing Chinese troops to interdict their supplies, forcing them north of the disputed line. There were eventually 60 such outposts, including 43 north of the McMahon Line, to which India claimed sovereignty. China viewed this as further confirmation of Indian expansionist plans directed towards Tibet. According to the Indian official history, implementation of the Forward Policy was intended to provide evidence of Indian occupation in the previously unoccupied region through which Chinese troops had been advancing. Kaul was confident, through contact with Indian Intelligence and CIA information, that China would not react with force. Indeed, at first the PLA simply withdrew, but eventually Chinese forces began to counter-encircle the Indian positions which clearly encroached into the north of McMahon Line. This led to a tit-for-tat Indian reaction, with each force attempting to outmanoeuver the other. However, despite the escalating nature of the dispute, the two forces withheld from engaging each other directly.
Chinese attention was diverted for a time by the military activity of the Nationalists on Taiwan, but on 23 June the U.S. assured China that a Nationalist invasion would not be permitted. China's heavy artillery facing Taiwan could then be moved to Tibet. It took China six to eight months to gather the resources needed for the war, according to Anil Athale, author of the official Indian history. The Chinese sent a large quantity of non-military supplies to Tibet through the Indian port of Calcutta.
Early incidents.
Various border conflicts and "military incidents" between India and China flared up throughout the summer and autumn of 1962. In May, the Indian Air Force was told not to plan for close air support, although it was assessed as being a feasible way to counter the unfavourable ratio of Chinese to Indian troops. In June, a skirmish caused the deaths of dozens of Chinese troops. The Indian Intelligence Bureau received information about a Chinese buildup along the border which could be a precursor to war.
During June–July 1962, Indian military planners began advocating "probing actions" against the Chinese, and accordingly, moved mountain troops forward to cut off Chinese supply lines. According to Patterson, the Indian motives were threefold:
On 10 July 1962, 350 Chinese troops surrounded an Indian occupied post in Chushul (north of the McMahon Line) but withdrew after a heated argument via loudspeaker. On 22 July, the Forward Policy was extended to allow Indian troops to push back Chinese troops already established in disputed territory. Whereas Indian troops were previously ordered to fire only in self-defence, all post commanders were now given discretion to open fire upon Chinese forces if threatened. In August, the Chinese military improved its combat readiness along the McMahon Line and began stockpiling ammunition, weapons and gasoline.
Given his foreknowledge of the coming Cuban Missile Crisis, Mao Zedong was able to persuade Nikita Khrushchev to reverse the Russian policy of backing India, at least temporarily. In mid-October, the Communist organ "Pravda" encouraged peace between India and China. When the Cuban Missile Crisis ended and Mao's rhetoric changed, however, Russia reversed course.
Confrontation at Thag La.
In June 1962, Indian forces established an outpost at Dhola, on the southern slopes of the Thag La Ridge. Dhola lay north of the McMahon Line but south of the ridges along which India interpreted the McMahon Line to run. In August, China issued diplomatic protests and began occupying positions at the top of Thag La. On 8 September, a 60-strong PLA unit descended to the south side of the ridge and occupied positions that dominated one of the Indian posts at Dhola. Fire was not exchanged, but Nehru said to the media that the Indian Army had instructions to "free our territory" and the troops had been given discretion to use force. On 11 September, it was decided that "all forward posts and patrols were given permission to fire on any armed Chinese who entered Indian territory".
However, the operation to occupy Thag La was flawed in that Nehru's directives were unclear and it got underway very slowly because of this. In addition to this, each man had to carry 35 kg over the long trek and this severely slowed down the reaction. By the time the Indian battalion reached the point of conflict, Chinese units controlled both banks of the Namka Chu River. On 20 September, Chinese troops threw grenades at Indian troops and a firefight developed, triggering a long series of skirmishes for the rest of September.
Some Indian troops, including Brigadier Dalvi who commanded the forces at Thag La, were also concerned that the territory they were fighting for was not strictly territory that "we should have been convinced was ours". According to Neville Maxwell, even members of the Indian defence ministry were categorically concerned with the validity of the fighting in Thag La.
On 3 October, a week before the start of the war, Zhou Enlai visited Nehru in New Delhi promising there would be no war. On 4 October, Kaul assigned some troops to secure regions south of the Thag La Ridge. Kaul decided to first secure Yumtso La, a strategically important position, before re-entering the lost Dhola post. Kaul had then realised that the attack would be desperate and the Indian government tried to stop an escalation into all-out war. Indian troops marching to Thag La had suffered in the previously unexperienced conditions; two Gurkha soldiers died of pulmonary edema.
On 10 October, an Indian Punjabi patrol of 50 troops to Yumtso La were met by an emplaced Chinese position of some 1,000 soldiers. Indian troops were in no position for battle, as Yumtso La was 16,000 feet (4,900 m) above sea level and Kaul did not plan on having artillery support for the troops. The Chinese troops opened fire on the Indians under their belief that they were north of the McMahon Line. The Indians were surrounded by Chinese positions which used mortar fire. However, they managed to hold off the first Chinese assault, inflicting heavy casualties.
At this point, the Indian troops were in a position to push the Chinese back with mortar and machine gun fire. However, Brigadier Dalvi opted not to fire, as it would mean decimating the Rajput who were still in the area of the Chinese regrouping. They helplessly watched the Chinese ready themselves for a second assault. In the second Chinese assault, the Indians began their retreat, realising the situation was hopeless. The Indian patrol suffered 25 casualties, and the Chinese 33. The Chinese troops held their fire as the Indians retreated, and then buried the Indian dead with military honours, as witnessed by the retreating soldiers. This was the first occurrence of heavy fighting in the war.
This attack had grave implications for India and Nehru tried to solve the issue, but by 18 October, it was clear that the Chinese were preparing for an attack on India, with massive troop buildups on the border. A long line of mules and porters had also been observed supporting the buildup and reinforcement of positions south of the Thag La Ridge.
Chinese and Indian preparations.
Motives.
Two of the major factors leading up to China's eventual conflicts with Indian troops were India's stance on the disputed borders and perceived Indian subversion in Tibet. There was "a perceived need to punish and end perceived Indian efforts to undermine Chinese control of Tibet, Indian efforts which were perceived as having the objective of restoring the pre-1949 status quo ante of Tibet". The other was "a perceived need to punish and end perceived Indian aggression against Chinese territory along the border". John W. Garver argues that the first perception was incorrect based on the state of the Indian military and polity in the 1960s. It was, nevertheless a major reason for China's going to war. However, he argues the Chinese perception of Indian aggression to be "substantially accurate".
The CIA's recently declassified POLO documents reveal contemporary American analysis of Chinese motives during the war. According to this document, "Chinese apparently were motivated to attack by one primary consideration — their determination to retain the ground on which PLA forces stood in 1962 and to punish the Indians for trying to take that ground". In general terms, they tried to show the Indians once and for all that China would not acquiesce in a military "reoccupation" policy. The secondary reasons for the attack, which had made it desirable but not necessary, included a desire :
Another factor which might have affected China's decision for war with India was a perceived need to stop a Soviet-U.S.-India encirclement and isolation of China. India's relations with the Soviet Union and United States were both strong at this time, but the Soviets (and Americans) were preoccupied by the Cuban Missile Crisis and would not interfere with the Sino-Indian War. P. B. Sinha suggests that China waited until October to attack because the timing of the war was exactly in parallel with American actions so as to avoid any chance of American or Soviet involvement. Although American buildup of forces around Cuba occurred on the same day as the first major clash at Dhola, and China's buildup between 10 and 20 October appeared to coincide exactly with the United States establishment of a blockade against Cuba which began 20 October, the Chinese probably prepared for this before they could anticipate what would happen in Cuba. Another explanation is that the confrontation in the Taiwan Strait had eased by then.
Garver argues that the Chinese correctly assessed Indian border policies, particularly the Forward Policy, as attempts for incremental seizure of Chinese-controlled territory. On Tibet, Garver argues that one of the major factors leading to China's decision for war with India was a common tendency of humans "to attribute others behavior to interior motivations, while attributing their own behavior to situational factors". Studies from China published in the 1990s confirmed that the root cause for China going to war with India was the perceived Indian aggression in Tibet, with the forward policy simply catalysing the Chinese reaction.
Neville Maxwell and Allen Whiting argue that the Chinese leadership believed they were defending territory that was legitimately Chinese, and which was already under de facto Chinese occupation prior to Indian advances, and regarded the Forward Policy as an Indian attempt at creeping annexation. Mao Zedong himself compared the Forward Policy to a strategic advance in Chinese chess:
Their [India's] continually pushing forward is like crossing the Chu Han boundary. What should we do? We can also set out a few pawns, on our side of the river. If they don't then cross over, that’s great. If they do cross, we'll eat them up [chess metaphor meaning to take the opponent's pieces]. Of course, we cannot blindly eat them. Lack of forbearance in small matters upsets great plans. We must pay attention to the situation.
India claims that the motive for the Forward Policy was to cut off the supply routes for Chinese troops posted in NEFA and Aksai Chin. According to the official Indian history, the forward policy was continued because of its initial success, as it claimed that Chinese troops withdrew when they encountered areas already occupied by Indian troops. It also claimed that the Forward Policy was having success in cutting out supply lines of Chinese troops who had advanced South of the McMahon Line, though there was no evidence of such advance before the 1962 war. However, the Forward Policy rested on the assumption that Chinese forces "were not likely to use force against any of our posts, even if they were in a position to do so". No serious re-appraisal of this policy took place even when Chinese forces ceased withdrawing. Nehru's confidence was probably justified given the difficulty for China to supply the area over the high altitude terrain over 5000 km from the more populated areas of China.
The Chinese leadership initially held a sympathetic view towards India as the latter had been ruled by British colonial masters for centuries. However, Nehru's forward policy convinced PRC leadership that the independent Indian leadership was a reincarnation of British imperialism. Mao Zedong stated: "Rather than being constantly accused of aggression, it's better to show the world what really happens when China indeed moves its muscles."
Chinese policy toward India, therefore, operated on two contradictory assumptions in the first half of 1961. On the one hand, the Chinese leaders continued to entertain a hope, although a shrinking one, that some opening for talks.would appear. On, the other hand, they read Indian statements and actions as clear signs that Nehru wanted to talk only about a Chinese withdrawal. Regarding the hope, they were willing to negotiate and tried to prod Nehru into a similar attitude. Regarding Indian intentions, they began to act politically and to build a rationale based on the assumption that Nehru already had become a lackey of imperialism; for this reason he opposed border talks.
Krishna Menon is reported to have said that when he arrived in Geneva on 6 June 1961 for an international conference in Laos, Chinese officials in Chen Yi's delegation indicated that Chen might be interested in discussing the border dispute with him. At several private
meetings with Menon, Chen avoided any discussion of the dispute and Menon surmised that the Chinese wanted him to broach the matter first. He did not, as he was under instructions from Nehru to avoid taking the initiative, leaving the Chinese with the impression
that Nehru was unwilling to show any flexibility.
In September, the Chinese took a step toward criticizing Nehru openly in their commentary. After citing Indonesian and Burmese press criticism of Nehru by name, the Chinese critiqued his moderate remarks on colonialism (People's Daily Editorial, 9 September) - Somebody at the Non-Aligned Nations Conference advanced the argument that the era of classical colonialism is gone and dead...contrary to facts." This was a distortion of Nehru's remarks but appeared close enough to be credible. On the same day, Chen Yi referred to Nehru by implication at the Bulgarian embassy reception: 'Those who attempted to' deny history, ignore reality, and distort the truth and who attempted to divert the Conference from its important object have failed to gain support and were isolated." On 10 September, they dropped all circumlocutions and criticized him by name in a China Youth article and NCNA report—the first time in almost two years that they had commented extensively on the Prime Minister.
By early 1962, the Chinese leadership began to believe that India's intentions were to launch a massive attack against Chinese troops, and that the Indian leadership wanted a war. In 1961, the Indian army had been sent into Goa, a small region without any other international borders apart from the Indian one, after Portugal refused to surrender the exclave colony to the Indian Union. Although this action met little to no international protest or opposition, China saw it as an example of India's expansionist nature, especially in light of heated rhetoric from Indian politicians. India's Home Minister declared, "If the Chinese will not vacate the areas occupied by it, India will have to repeat what it did in Goa. India will certainly drive out the Chinese forces", while another member of the Indian Congress Party pronounced, "India will take steps to end [Chinese] aggression on Indian soil just as it ended Portuguese aggression in Goa". By mid-1962, it was apparent to the Chinese leadership that negotiations had failed to make any progress, and the Forward Policy was increasingly perceived as a grave threat as Delhi increasingly sent probes deeper into border areas and cut off Chinese supply lines. Foreign Minister Marshal Chen Yi commented at one high-level meeting, "Nehru's forward policy is a knife. He wants to put it in our heart. We cannot close our eyes and await death." The Chinese leadership believed that their restraint on the issue was being perceived by India as weakness, leading to continued provocations, and that a major counterblow was needed to stop perceived Indian aggression.
Xu Yan, prominent Chinese military historian and professor at the PLA's National Defense University, gives an account of the Chinese leadership's decision to go to war. By late September 1962, the Chinese leadership had begun to reconsider their policy of "armed coexistence", which had failed to address their concerns with the forward policy and Tibet, and consider a large, decisive strike. On 22 September 1962, the "People's Daily" published an article which claimed that "the Chinese people were burning with 'great indignation' over the Indian actions on the border and that New Delhi could not 'now say that warning was not served in advance'."
Military planning.
The Indian side was confident war would not be triggered and made little preparations. India had only two divisions of troops in the region of the conflict. In August 1962, Brigadier D. K. Palit claimed that a war with China in the near future could be ruled out. Even in September 1962, when Indian troops were ordered to "expel the Chinese" from Thag La, Maj. General J. S. Dhillon expressed the opinion that "experience in Ladakh had shown that a few rounds fired at the Chinese would cause them to run away." Because of this, the Indian army was completely unprepared when the attack at Yumtso La occurred.
Recently declassified CIA documents which were compiled at the time reveal that India's estimates of Chinese capabilities made them neglect their military in favour of economic growth. It is claimed that if a more military-minded man had been in place instead of Nehru, India would have been more likely to have been ready for the threat of a counter-attack from China.
On 6 October 1962, the Chinese leadership convened. Lin Biao reported that PLA intelligence units had determined that Indian units might assault Chinese positions at Thag La on 10 October (Operation Leghorn). The Chinese leadership and the Central Military Council decided upon war to launch a large-scale attack to punish perceived military aggression from India. In Beijing, a larger meeting of Chinese military was convened in order to plan for the coming conflict.
Mao and the Chinese leadership issued a directive laying out the objectives for the war. A main assault would be launched in the eastern sector, which would be coordinated with a smaller assault in the western sector. All Indian troops within China's claimed territories in the eastern sector would be expelled, and the war would be ended with a unilateral Chinese ceasefire and withdrawal to prewar positions, followed by a return to the negotiating table. India led the Non-Aligned Movement, Nehru enjoyed international prestige, and China, with a larger military, would be portrayed as an aggressor. However, he said that a well-fought war "will guarantee at least thirty years of peace" with India, and determined the benefits to offset the costs.
China also reportedly bought significant amount of Indian Rupee currency notes from Hong Kong, supposedly to distribute amongst its soldiers in preparation for the war.
On 8 October, additional veteran and elite divisions were ordered to prepare to move into Tibet from the Chengdu and Lanzhou military regions.
On 12 October, Nehru declared that he had ordered the Indian army to "clear Indian territory in the NEFA of Chinese invaders" and personally met with Kaul, issuing instructions to him.
On 14 October, an editorial on "People's Daily" issued China's final warning to India: "So it seems that Mr. Nehru has made up his mind to attack the Chinese frontier guards on an even bigger scale.  ... It is high time to shout to Mr. Nehru that the heroic Chinese troops, with the glorious tradition of resisting foreign aggression, can never be cleared by anyone from their own territory ... If there are still some maniacs who are reckless enough to ignore our well-intentioned advice and insist on having another try, well, let them do so. History will pronounce its inexorable verdict ... At this critical moment ... we still want to appeal once more to Mr. Nehru: better rein in at the edge of the precipice and do not use the lives of Indian troops as stakes in your gamble."
Marshal Liu Bocheng headed a group to determine the strategy for the war. He concluded that the opposing Indian troops were among India's best, and to achieve victory would require deploying crack troops and relying on force concentration to achieve decisive victory. On 16 October, this war plan was approved, and on the 18th, the final approval was given by the Politburo for a "self-defensive counter-attack", scheduled for 20 October.
Chinese offensive.
On 20 October 1962, the Chinese People's Liberation Army launched two attacks, 1000 kilometres apart. In the western theatre, the PLA sought to expel Indian forces from the Chip Chap valley in Aksai Chin while in the eastern theatre, the PLA sought to capture both banks of the Namka Chu river. Some skirmishes also took place at the Nathula Pass, which is in the Indian state of Sikkim (an Indian protectorate at that time). Gurkha rifles travelling north were targeted by Chinese artillery fire. After four days of fierce fighting, the three regiments of Chinese troops succeeded in securing a substantial portion of the disputed territory.
Eastern theatre.
Chinese troops launched an attack on the southern banks of the Namka Chu River on 20 October. The Indian forces were undermanned, with only an understrength battalion to support them, while the Chinese troops had three regiments positioned on the north side of the river. The Indians expected Chinese forces to cross via one of five bridges over the river and defended those crossings. However, the PLA bypassed the defenders by crossing the shallow October river instead. They formed up into battalions on the Indian-held south side of the river under cover of darkness, with each battalion assigned against a separate group of Rajputs.
At 5:14 am, Chinese mortar fire began attacking the Indian positions. Simultaneously, the Chinese cut the Indian telephone lines, preventing the defenders from making contact with their headquarters. At about 6:30 am, the Chinese infantry launched a surprise attack from the rear and forced the Indians to leave their trenches.
The Chinese troops overwhelmed the Indians in a series of flanking manoeuvres south of the McMahon Line and prompted their withdrawal from Namka Chu. Fearful of continued losses, Indian troops escaped into Bhutan. Chinese forces respected the border and did not pursue. Chinese forces now held all of the territory that was under dispute at the time of the Thag La confrontation, but they continued to advance into the rest of NEFA.
On 22 October, at 12:15 am, PLA mortars fired on Walong, on the McMahon line. Flares launched by Indian troops the next day revealed numerous Chinese milling around the valley. The Indians tried to use their mortars against the Chinese but the PLA responded by lighting a bushfire, causing confusion amongst the Indians. Some 400 Chinese troops attacked the Indian position. The initial Chinese assault was halted by accurate Indian mortar fire. The Chinese were then reinforced and launched a second assault. The Indians managed to hold them back for four hours, but the Chinese used sheer weight of numbers to break through. Most Indian forces to withdraw to established positions in Walong, while a company supported by mortars and medium machine guns remained to cover the retreat.
On the morning 23 October, the Indians discovered a Chinese force gathered in a cramped pass and opened fire with mortars and machine guns, leading to heavy fighting. About 200 Chinese soldiers were killed and wounded in this action. Nine Indian soldiers were also killed. The fighting continued well into the afternoon, until the company was ordered to withdraw. Meanwhile, the 4th Sikhs made contact with the Chinese and subjected them to withering mortar and machine gun fire as the Chinese set off a brushfire and attempted to sneak forward. Sepoy Piara Singh tried to douse the fire while fighting the enemy, but died after he was wounded and refused to be evacuated.
Elsewhere, Chinese troops were launched a three-pronged attack on Tawang, which the Indians evacuated without any resistance.
Over the following days, there were clashes between Indian and Chinese patrols at Walong as the Chinese rushed in reinforcements. On 25 October, the Chinese made a probe, which was met with resistance from the 4th Sikhs. As some Chinese soldiers began to close in, Sepoy Kewal Singh charged them with his bayonet and killed a few of them in hand-to-hand combat, but he himself was killed. The following day, a patrol from the 4th Sikhs was encircled, and after being unable to break the encirclement, an Indian unit sneaked in and attacked the Chinese flank, allowing the Sikhs to break free.
Western theatre.
On the Aksai Chin front, China already controlled most of the disputed territory. Chinese forces quickly swept the region of any remaining Indian troops. Late on 19 October, Chinese troops launched a number of attacks throughout the western theatre. By 22 October, all posts north of Chushul had been cleared.
On 20 October, the Chinese easily took the Chip Chap Valley, Galwan Valley, and Pangong Lake. Many outposts and garrisons along the Western front were unable to defend against the surrounding Chinese troops. Most Indian troops positioned in these posts offered resistance but were either killed or taken prisoner. Indian support for these outposts was not forthcoming, as evidenced by the Galwan post, which had been surrounded by enemy forces in August, but no attempt made to relieve the besieged garrison. Following the 20 October attack, nothing was heard from Galwan.
On 24 October, Indian forces fought hard hold the Rezang La Ridge, in order to prevent a nearby airstrip from falling to the Chinese.
After realising the magnitude of the attack, the Indian Western Command withdrew many of the isolated outposts to the south-east. Daulet Beg Oldi was also evacuated, but it was south of the Chinese claim line and was not approached by Chinese forces. Indian troops were withdrawn in order to consolidate and regroup in the event that China probed south of their claim line.
Lull in the fighting.
By 24 October, the PLA had entered territory previously administered by India to give the PRC a diplomatically strong position over India. The majority of Chinese forces had advanced sixteen kilometres south of the control line prior to the conflict. Four days of fighting were followed by a three-week lull. Zhou ordered the troops to stop advancing as he attempted to negotiate with Nehru. The Indian forces had retreated into more heavily fortified positions around Se La and Bombdi La which would be difficult to assault. Zhou sent Nehru a letter, proposing
Nehru's 27 October reply expressed interest in the restoration of peace and friendly relations and suggested a return to the "boundary prior to 8 September 1962". He was categorically concerned about a mutual twenty kilometre withdrawal after "40 or 60 kilometres of blatant military aggression". He wanted the creation of a larger immediate buffer zone and thus resist the possibility of a repeat offensive. Zhou's 4 November reply repeated his 1959 offer to return to the McMahon Line in NEFA and the Chinese traditionally claimed MacDonald Line in Aksai Chin. Facing Chinese forces maintaining themselves on Indian soil and trying to avoid political pressure, the Indian parliament announced a national emergency and passed a resolution which stated their intent to "drive out the aggressors from the sacred soil of India". The United States and the United Kingdom supported India's response. However, the Soviet Union was preoccupied with the Cuban Missile Crisis and did not offer the support it had provided in previous years. With the backing of other great powers, a 14 November letter by Nehru to Zhou once again rejected his proposal.
Neither side declared war, used their air force, or fully broke off diplomatic relations; however, the conflict is commonly referred to as a war. This war coincided with the Cuban Missile Crisis and was viewed by the western nations at the time as another act of aggression by the Communist bloc.
According to Calvin, the Chinese side evidently wanted a diplomatic resolution and discontinuation of the conflict.
Continuation of war.
After Zhou received Nehru's letter (rejecting Zhou's proposal), the fighting resumed on the eastern theatre on 14 November (Nehru's birthday), with an Indian attack on Walong, claimed by China, launched from the defensive position of Se La and inflicting heavy casualties on the Chinese. The Chinese resumed military activity on Aksai Chin and NEFA hours after the Walong battle.
Eastern theatre.
On the eastern theatre, the PLA attacked Indian forces near Se La and Bomdi La on 17 November. These positions were defended by the Indian 4th Infantry Division. Instead of attacking by road as expected, PLA forces approached via a mountain trail, and their attack cut off a main road and isolated 10,000 Indian troops.
Se La occupied high ground, and rather than assault this commanding position, the Chinese captured Thembang, which was a supply route to Se La.
Western theatre.
On the western theatre, PLA forces launched a heavy infantry attack on 18 November near Chushul. Their attack started at 4:35 am, despite a mist surrounding most of the areas in the region. At 5:45 the Chinese troops advanced to attack 2 platoons of Indian troops at Gurung Hill.
The Indians did not know what was happening, as communications were dead. As a patrol was sent, China attacked with greater numbers. Indian artillery could not hold off against superior Chinese forces. By 9:00 am, Chinese forces attacked Gurung Hill directly and Indian commanders withdrew from the area and also from the connecting Spangur Gap.
The Chinese had been simultaneously attacking Rezang La which was held by 123 Indian troops. At 5:05 am, Chinese troops launched their attack audaciously. Chinese medium machine gun fire pierced through the Indian tactical defences.
At 6:55 am the sun rose and the Chinese attack on the 8th platoon began in waves. Fighting continued for the next hour, until the Chinese signaled that they had destroyed the 7th platoon. Indians tried to use light machine guns on the medium machine guns from the Chinese but after 10 minutes the battle was over. Logistical inadequacy once again hurt the Indian troops. The Chinese gave the Indian troops a respectful military funeral. The battles also saw the death of Major Shaitan Singh of the Kumaon Regiment, who had been instrumental in the first battle of Rezang La. The Indian troops were forced to withdraw to high mountain positions. Indian sources believed that their troops were just coming to grips with the mountain combat and finally called for more troops. However, the Chinese declared a ceasefire, ending the bloodshed.
Indians suffered heavy casualties, with dead Indian troops' bodies being found in the ice, frozen with weapons in hand. Chinese forces also suffered heavy casualties, especially at Rezang La. This signalled the end of the war in Aksai Chin as China had reached their claim line – many Indian troops were ordered to withdraw from the area. China claimed that the Indian troops wanted to fight on until the bitter end. However, the war ended with their withdrawal, so as to limit the amount of casualties.
The PLA penetrated close to the outskirts of Tezpur, Assam, a major frontier town nearly fifty kilometres from the Assam-North-East Frontier Agency border. The local government ordered the evacuation of the civilians in Tezpur to the south of the Brahmaputra River, all prisons were thrown open, and government officials who stayed behind destroyed Tezpur's currency reserves in anticipation of a Chinese advance.
Ceasefire.
China had reached its claim lines so the PLA did not advance farther, and on 19 November, it declared a unilateral cease-fire. Zhou Enlai declared a unilateral ceasefire to start on midnight, 21 November. Zhou's ceasefire declaration stated,
Beginning from 21 November 1962, the Chinese frontier guards will cease fire along the entire Sino-Indian border. Beginning from 1 December 1962, the Chinese frontier guards will withdraw to positions 20 kilometres behind the line of actual control which existed between China and India on 7 November 1959. In the eastern sector, although the Chinese frontier guards have so far been fighting on Chinese territory north of the traditional customary line, they are prepared to withdraw from their present positions to the north of the illegal McMahon Line, and to withdraw twenty kilometres back from that line. In the middle and western sectors, the Chinese frontier guards will withdraw twenty kilometres from the line of actual control.
Zhou had first given the ceasefire announcement to Indian chargé d'affaires on 19 November (before India's request for United States air support), but New Delhi did not receive it until 24 hours later. The aircraft carrier was ordered back after the ceasefire, and thus, American intervention on India's side in the war was avoided. Retreating Indian troops, who hadn't come into contact with anyone knowing of the ceasefire, and Chinese troops in NEFA and Aksai Chin, were involved in some minor battles, but for the most part, the ceasefire signalled an end to the fighting. The United States Air Force flew in supplies to India in November 1962, but neither side wished to continue hostilities.
Toward the end of the war India increased its support for Tibetan refugees and revolutionaries, some of them having settled in India, as they were fighting the same common enemy in the region. The Nehru administration ordered the raising of an elite Indian-trained "Tibetan Armed Force" composed of Tibetan refugees. The CIA had already begun operations in bringing about change in Tibet.
World opinion.
The Chinese military action has been viewed by the United States as part of the PRC's policy of making use of aggressive wars to settle its border disputes and to distract from its internal issues. According to James Calvin from the United States Marine Corps, western nations at the time viewed China as an aggressor during the China–India border war, and the war was part of a monolithic communist objective for a world dictatorship of the proletariat. This was further triggered by Mao Zedong's views that: "The way to world conquest lies through Havana, Accra, and Calcutta". Calvin believes that Chinese actions show a "pattern of conservative aims and limited objectives, rather than expansionism" and blames this particular conflict on India's provocations towards China. However, Calvin also expresses that China, in the past, has been adamant to gain control over regions to which it has a "traditional claim", which triggered the dispute over NEFA and Aksai Chin and indeed Tibet. Calvin's assumption, based on the history of the Cold War and the Domino Effect, assumed that China might ultimately try to regain control of everything that it considers as "traditionally Chinese" which in its view includes the entirety of South East Asia.
The Kennedy administration was disturbed by what they considered "blatant Chinese communist aggression against India". In a May 1963 National Security Council meeting, contingency planning on the part of the United States in the event of another Chinese attack on India was discussed. Defense Secretary Robert McNamara and General Maxwell Taylor advised the president to use nuclear weapons should the Americans intervene in such a situation. McNamara stated "Before any substantial commitment to defend India against China is given, we should recognize that in order to carry out that commitment against any substantial Chinese attack, we would have to use nuclear weapons. Any large Chinese Communist attack on any part of that area would require the use of nuclear weapons by the U.S., and this is to be preferred over the introduction of large numbers of U.S. soldiers." After hearing this and listening to two other advisers, Kennedy stated "We should defend India, and therefore we will defend India." It remains unclear if his aides were trying to dissuade the President of considering any measure with regard to India by immediately raising the stakes to an unacceptable level, nor is it clear if Kennedy was thinking of conventional or nuclear means when he gave his reply. By 1964 China had developed its own nuclear weapon which would have likely caused any American nuclear policy in defense of India to be reviewed. The Johnson Administration considered and then rejected giving nuclear weapons technology to the Indians. However India developed its own nuclear weapon by 1974, within 10 years of the Chinese.
The non-aligned nations remained mostly uninvolved, and only the United Arab Republic openly supported India. Of the non-aligned nations, six, Egypt, Burma, Cambodia, Sri Lanka, Ghana and Indonesia, met in Colombo on 10 December 1962. The proposals stipulated a Chinese withdrawal of 20 km from the customary lines without any reciprocal withdrawal on India's behalf. The failure of these six nations to unequivocally condemn China deeply disappointed India.
In 1972, Chinese Premier Zhou explained the Chinese point of view to President Nixon of the US. As for the causes of the war, Zhou asserted that China did not try to expel Indian troops from south of the McMahon line and that three open warning telegrams were sent to Nehru before the war. However, Indian patrols south of the McMahon line were expelled and suffered casualties in the Chinese attack. Zhou also told Nixon that Chairman Mao ordered the troops to return to show good faith. The Indian government maintains that the Chinese military could not advance further south due to logistical problems and the cut-off of resource supplies.
While Western nations did not view Chinese actions favourably because of fear of the Chinese and competitiveness, Pakistan, which had had a turbulent relationship with India ever since the Indian partition, improved its relations with China after the war. Prior to the war, Pakistan also shared a disputed boundary with China, and had proposed to India that the two countries adopt a common defence against "northern" enemies (i.e. China), which was rejected by India. However, China and Pakistan took steps to peacefully negotiate their shared boundaries, beginning on 13 October 1962, and concluding in December of that year. Pakistan also expressed fear that the huge amounts of western military aid directed to India would allow it to threaten Pakistan's security in future conflicts. Mohammed Ali, External Affairs Minister of Pakistan, declared that massive Western aid to India in the Sino-Indian dispute would be considered an unfriendly act towards Pakistan. As a result Pakistan made efforts to improve its relations with China. The following year, China and Pakistan peacefully settled disputes on their shared border, and negotiated the China-Pakistan Border Treaty in 1963, as well as trade, commercial, and barter treaties. On 2 March 1963, Pakistan conceded its northern claim line in Pakistani-controlled Kashmir to China in favor of a more southerly boundary along the Karakoram Range. The border treaty largely set the border along the MacCartney-Macdonald Line. India's military failure against China would embolden Pakistan to initiate the Second Kashmir War with India. However, it effectively ended in a stalemate as Calvin states that the Sino-Indian War had caused the previously passive government to take a stand on actively modernising India's military. China offered diplomatic support to Pakistan in this war but did not offer military support. In January 1966, China condemned the Tashkent Agreement between India and Pakistan as a Soviet-US plot in the region. In the Indo-Pakistani War of 1971, Pakistan expected China to provide military support, but it was left alone as India successfully helped the rebels in East Pakistan to found the new nation-state of Bangladesh.
Involvement of other nations.
During the conflict, Nehru wrote two desperate letters to U.S. President John F. Kennedy, requesting 12 squadrons of fighter jets and a modern radar system. These jets were seen as necessary to beef up Indian air strength so that air-to-air combat could be initiated safely from the Indian perspective (bombing troops was seen as unwise for fear of Chinese retaliatory action). Nehru also asked that these aircraft be manned by American pilots until Indian airmen were trained to replace them. These requests were rejected by the Kennedy Administration (which was involved in the Cuban Missile Crisis during most of the Sino-Indian War). According to former Indian diplomat G Parthasarathy, "only after we got nothing from the US did arms supplies from the Soviet Union to India commence." In 1962, President of Pakistan Ayub Khan made clear to India that Indian troops could safely be transferred from the Pakistan frontier to the Himalayas.
Aftermath.
China.
According to the China's official military history, the war achieved China's policy objectives of securing borders in its western sector, as China retained de facto control of the Aksai Chin. After the war, India abandoned the Forward Policy, and the de facto borders stabilised along the Line of Actual Control.
According to James Calvin of Marine Corps Command and Staff College, even though China won a military victory it lost in terms of its international image. China's first nuclear weapon test in October 1964 and it support of Pakistan in the 1965 India Pakistan War tended to confirm the American view of communist world objectives, including Chinese influence over Pakistan.
Lora Saalman opined in a study of Chinese military publications, that while the war led to much blame, debates and ultimately acted as causation of military modernization of India but the war is now treated as basic reportage of facts with relatively diminished interest by Chinese analysts.
India.
The aftermath of the war saw sweeping changes in the Indian military to prepare it for similar conflicts in the future, and placed pressure on Indian prime minister Jawaharlal Nehru, who was seen as responsible for failing to anticipate the Chinese attack on India. Indians reacted with a surge in patriotism and memorials were erected for many of the Indian troops who died in the war. Arguably, the main lesson India learned from the war was the need to strengthen its own defences and a shift from Nehru's foreign policy with China based on his stated concept of "brotherhood". Because of India's inability to anticipate Chinese aggression, Prime Minister Nehru faced harsh criticism from government officials, for having promoted pacifist relations with China. Indian President Radhakrishnan said that Nehru's government was naive and negligent about preparations, and Nehru admitted his failings. According to Inder Malhotra, a former editor of "The Times of India" and a commentator for "The Indian Express", Indian politicians invested more effort in removing Defence Minister Krishna Menon than in actually waging war. Krishna Menon's favoritism weakened the Indian Army, and national morale dimmed. The public saw the war as political and military debacle. Under American advice (by American envoy John Kenneth Galbraith who made and ran American policy on the war as all other top policy makers in USA were absorbed in coincident Cuban Missile Crisis) Indians refrained, not according to the best choices available, from using the Indian air force to beat back the Chinese advances. The CIA later revealed that at that time the Chinese had neither the fuel nor runways long enough for using their air force effectively in Tibet. Indians in general became highly sceptical of China and its military. Many Indians view the war as a betrayal of India's attempts at establishing a long-standing peace with China and started to question the once popular "Hindi-Chini bhai-bhai" (meaning "Indians and Chinese are brothers"). The war also put an end to Nehru's earlier hopes that India and China would form a strong Asian Axis to counteract the increasing influence of the Cold War bloc superpowers.
The unpreparedness of the army was blamed on Defence Minister Menon, who resigned his government post to allow for someone who might modernise India's military further. India's policy of weaponisation via indigenous sources and self-sufficiency was thus cemented. Sensing a weakened army, Pakistan, a close ally of China, began a policy of provocation against India by infiltrating Jammu and Kashmir and ultimately triggering the Second Kashmir War with India in 1965 and Indo-Pakistani war of 1971. The Attack of 1965 was successfully stopped and ceasefire was negotiated under international pressure. In the Indo-Pakistani war of 1971 India won a clear victory, resulting in liberation of Bangladesh (formerly East-Pakistan).
As a result of the war, the Indian government commissioned an investigation, resulting in the classified Henderson-Brooks-Bhagat Report on the causes of the war and the reasons for failure. India's performance in high-altitude combat in 1962 led to an overhaul of the Indian Army in terms of doctrine, training, organisation and equipment. Neville Maxwell claimed that the Indian role in international affairs after the border war was also greatly reduced after the war and India's standing in the non-aligned movement suffered. The Indian government has attempted to keep the Hendersen-Brooks-Bhagat Report secret for decades, although portions of it have recently been leaked by Neville Maxwell.
According to James Calvin, an analyst from the U.S. Navy, India gained many benefits from the 1962 conflict. This war united the country as never before. India got 32,000 square miles (8.3 million hectares, 83,000 km2) of disputed territory even if it felt that NEFA was hers all along. The new Indian republic had avoided international alignments; by asking for help during the war, India demonstrated its willingness to accept military aid from several sectors. And, finally, India recognised the serious weaknesses in its army. It would more than double its military manpower in the next two years and it would work hard to resolve the military's training and logistic problems to later become the third-largest army in the world. India's efforts to improve its military posture significantly enhanced its army's capabilities and preparedness. This played a role in subsequent wars against Pakistan.
Internment and deportation of Chinese Indians.
Soon after the end of the war, the Indian government passed the Defence of India Act in December 1962, permitting the "apprehension and detention in custody of any person [suspected] of being of hostile origin." The broad language of the act allowed for the arrest of any person simply for having a Chinese surname, Chinese ancestry or a Chinese spouse. The Indian government incarcerated thousands of Chinese-Indians in an internment camp in Deoli, Rajasthan, where they were held for years without trial. The last internees were not released until 1967. Thousands more Chinese-Indians were forcibly deported or coerced to leave India. Nearly all internees had their properties sold off or looted. Even after their release, the Chinese Indians faced many restrictions in their freedom. They could not travel freely until the mid-1990s.
Later conflicts.
India also reported a series of military conflicts after the 1962 war. One report provided by India shows that in late 1967, there were two incidents in which both countries exchanged fire in Sikkim. The first one was dubbed the "Nathu La incident", and the other being "Chola incident".
Diplomatic process.
In 1993 and 1996, the two sides signed the Sino-Indian Bilateral Peace and Tranquility Accords, agreements to maintain peace and tranquility along the Line of Actual Control (LoAC). Ten meetings of a Sino-Indian Joint Working Group (SIJWG) and five of an expert group have taken place to determine where the LoAC lies, but little progress has occurred.
On 20 November 2006 Indian politicians from Arunachal Pradesh expressed their concern over Chinese military modernization and appealed to parliament to take a harder stance on the PRC following a military buildup on the border similar to that in 1962. Additionally, China's military aid to Pakistan as well is a matter of concern to the Indian public, as the two sides have engaged in various wars.
On 6 July 2006, the historic Silk Road passing through this territory via the Nathu La pass was reopened. Both sides have agreed to resolve the issues by peaceful means.
In Oct 2011, it was stated that India and China will formulate a border mechanism to handle different perceptions as to the LAC and resume the bilateral army exercises between Indian and Chinese army from early 2012.

</doc>
<doc id="29460" url="http://en.wikipedia.org/wiki?curid=29460" title="List of mayors of Sacramento, California">
List of mayors of Sacramento, California

This is a list of mayors of Sacramento, California. The Sacramento City Council met for the first time on August 1, 1849 and the citizens approved the city charter on October 13, 1849. The City Charter was recognized by the State of California on February 27, 1850 and Sacramento was incorporated on March 18, 1850.
"See also :" Lists of incumbents
References.
http://ohp.parks.ca.gov/?page_id=21454

</doc>
<doc id="29462" url="http://en.wikipedia.org/wiki?curid=29462" title="Sabotage">
Sabotage

Sabotage is a deliberate action aimed at weakening a polity or corporation through subversion, obstruction, disruption, or destruction. In a workplace setting, sabotage is the conscious withdrawal of efficiency generally directed at causing some change in workplace conditions. One who engages in sabotage is a saboteur. Saboteurs typically try to conceal their identities because of the consequences of their actions.
Any unexplained adverse condition might be sabotage. Sabotage is sometimes called tampering, meddling, tinkering, malicious pranks, malicious hacking, a practical joke or the like to avoid needing to invoke legal and organizational requirements for addressing sabotage.
Etymology.
Claimed explanations include:
As industrial action.
At the inception of the Industrial Revolution, skilled workers such as the Luddites (1811-1812) used sabotage as a means of negotiation in labor disputes.
Labor unions such as the Industrial Workers of the World (IWW) have advocated sabotage as a means of self-defense and direct action against unfair working conditions.
The IWW was shaped in part by the industrial unionism philosophy of Big Bill Haywood, and in 1910 Haywood was exposed to sabotage while touring Europe:
The experience that had the most lasting impact on Haywood was witnessing a general strike on the French railroads. Tired of waiting for parliament to act on their demands, railroad workers walked off their jobs all across the country. The French government responded by drafting the strikers into the army and then ordering them back to work. Undaunted, the workers carried their strike to the job. Suddenly, they could not seem to do anything right. Perishables sat for weeks, sidetracked and forgotten. Freight bound for Paris was misdirected to Lyon or Marseille instead. This tactic — the French called it "sabotage" — won the strikers their demands and impressed Bill Haywood.
For the IWW, sabotage came to mean any withdrawal of efficiency, including the slowdown, the strike, working to rule, or creative bungling of job assignments.
One of the most severe examples was at the construction site of the Robert-Bourassa Generating Station in 1974, in Québec, Canada, when workers used bulldozers to topple electric generators, damaged fuel tanks, and set buildings on fire. The project was delayed a year, and the direct cost of the damage estimated at $2 million CAD. The causes were not clear, but three possible factors have been cited: inter-union rivalry, poor working conditions, and the perceived arrogance of American executives of the contractor, Bechtel Corporation.
As environmental action.
Certain groups turn to destruction of property to stop environmental destruction or to make visible arguments against forms of modern technology they consider detrimental to the environment. The U.S. Federal Bureau of Investigation (FBI) and other law enforcement agencies use the term eco-terrorist when applied to damage of property. Proponents argue that since property cannot feel terror, damage to property is more accurately described as sabotage. Opponents, by contrast, point out that property owners and operators can indeed feel terror. The image of the monkey wrench thrown into the moving parts of a machine to stop it from working was popularized by Edward Abbey in the novel "The Monkey Wrench Gang" and has been adopted by eco-activists to describe destruction of earth damaging machinery.
As war tactic.
In war, the word is used to describe the activity of an individual or group not associated with the military of the parties at war, such as a foreign agent or an indigenous supporter, in particular when actions result in the destruction or damaging of a productive or vital facility, such as equipment, factories, dams, public services, storage plants or logistic routes. Prime examples of such sabotage are the events of Black Tom and the Kingsland Explosion. Like spies, saboteurs who conduct a military operation in civilian clothes or enemy uniforms behind enemy lines are subject to prosecution and criminal penalties instead of detention as prisoners of war. It is common for a government in power during war or supporters of the war policy to use the term loosely against opponents of the war. Similarly, German nationalists spoke of a stab in the back having cost them the loss of World War I.
A modern form of sabotage is the distribution of software intended to damage specific industrial systems. For example, the U.S. Central Intelligence Agency (CIA) is alleged to have sabotaged a Siberian pipeline during the Cold War, using information from the Farewell Dossier. A more recent case may be the Stuxnet computer worm, which was designed to subtly infect and damage specific types of industrial equipment. Based on the equipment targeted and the location of infected machines, security experts believe it was an attack on the Iranian nuclear program by the United States, Israel or, according to the latest news, even Russia.
Sabotage, done well, is inherently difficult to detect and difficult to trace to its origin. During World War II, the U.S. Federal Bureau of Investigation (FBI) investigated 19,649 cases of sabotage and concluded the enemy had not caused any of them.
Sabotage in warfare, according to the Office of Strategic Services (OSS) manual, varies from highly technical "coup de main" acts that require detailed planning and specially trained operatives, to innumerable simple acts that ordinary citizen-saboteurs can perform. Simple sabotage is carried out in such a way as to involve a minimum danger of injury, detection, and reprisal. There are two main methods of sabotage; physical destruction and the "human element." While physical destruction as a method is self-explanatory, its targets are nuanced, reflecting objects to which the saboteur has normal and inconspicuous access in everyday life. The "human element" is based on universal opportunities to make faulty decisions, to adopt a non-cooperative attitude, and to induce others to follow suit.
There are many examples of physical sabotage in wartime. However, one of the most effective uses of sabotage is against organizations. The OSS manual provides numerous techniques under the title "General Interference with Organizations and Production":
From the section entitled, "General Devices for Lowering Morale and Creating Confusion" comes the following quintessential simple sabotage advice: "Act stupid."
Value of simple sabotage in wartime.
The United States Office of Strategic Services, later renamed the CIA, noted specific value in committing simple sabotage against the enemy during wartime: "... slashing tires, draining fuel tanks, starting fires, starting arguments, acting stupidly, short-circuiting electric systems, abrading machine parts will waste materials, manpower, and time." To underline the importance of simple sabotage on a widespread scale, they wrote, "Widespread practice of simple sabotage will harass and demoralize enemy administrators and police." The OSS was also focused on the battle for hearts and minds during wartime; "the very practice of simple sabotage by natives in enemy or occupied territory may make these individuals identify themselves actively with the United Nations War effort, and encourage them to assist openly in periods of Allied invasion and occupation."
In World War I.
On 30 July 1916, the Black Tom explosion occurred when German agents set fire to a complex of warehouses and ships in Jersey City, New Jersey that held munitions, fuel, and explosives bound to aid the Allies in their fight.
On 11 January 1917, Fiodore Wozniak, using a rag saturated with phosphorus or an incendiary pencil supplied by German sabotage agents, set fire to his workbench at an ammunition assembly plant near Lyndhurst, New Jersey, causing a four-hour fire that destroyed half a million 3-inch explosive shells and destroyed the plant for an estimated at $17 million in damages. Wozniak's involvement was not discovered until 1927. 
On 12 February 1917, Bedouins allied with the British destroyed a Turkish railroad near the port of Wajh, derailing a Turkish locomotive. The Bedouins traveled by camel and used explosives to demolish a portion of track.
Post World War I.
In Ireland, the Irish Republican Army (IRA) used sabotage against the British following the Easter 1916 uprising. The IRA compromised communication lines and lines of transportation and fuel supplies. The IRA also employed passive sabotage, refusing dock and train workers to work on ships and rail cars used by the government. In 1920, agents of the IRA committed arson against at least fifteen British warehouses in Liverpool. The following year, the IRA set fire to numerous British targets again, including the Dublin Customs House, this time sabotaging most of Liverpool's firetrucks in the firehouses before lighting the matches.
In World War II.
Sabotage training for the Allies consisted of teaching would-be saboteurs key components of working machinery to destroy.
"Saboteurs learned hundreds of small tricks to cause the Germans big trouble. The cables in a telephone junction box ... could be jumbled to make the wrong connections when numbers were dialed. A few ounces of plastique, properly placed, could bring down a bridge, cave in a mine shaft, or collapse the roof of a railroad tunnel."
The French Resistance ran an extremely effective sabotage campaign against the Germans during World War II. Receiving their sabotage orders through messages over the BBC radio or by aircraft, the French used both passive and active forms of sabotage. Passive forms included losing German shipments and allowing poor quality material to pass factory inspections. Many active sabotage attempts were against critical rail lines of transportation. German records count 1,429 instances of sabotage from French Resistance forces between January 1942 and February 1943. From January through March 1944, sabotage accounted for three times the number of locomotives damaged by Allied airpower. See also Normandy Landings for more information about sabotage on D-Day.
During World War II, the Allies committed sabotage against the Peugeot truck factory. After repeated failures in Allied bombing attempts to hit the factory, a team of French Resistance fighters and Special Operations Executive (SOE) agents distracted the German guards with a game of soccer while part of their team entered the plant and destroyed machinery.
In December 1944, the Germans ran a false flag sabotage infiltration, Operation Greif, which was commanded by Waffen-SS commando Otto Skorzeny during the Battle of the Bulge. German commandos, wearing US Army uniforms, carrying US Army weapons, and using US Army vehicles, penetrated US lines to spread panic and confusion among US troops and to blow up bridges, ammunition dumps, and fuel stores and to disrupt the lines of communication. Many of the commandos were captured by the Americans. Because they were wearing US uniforms, a number of the Germans were executed as spies, either summarily or after military commissions.
After World War II.
From 1948 to 1960, the Malayan Communists committed numerous effective acts of sabotage against the Malaysian Government, first targeting railway bridges, then hitting larger targets such as military camps. Most of their efforts were centered around crippling Malaysia's economy and involved sabotage against trains, rubber trees, water pipes, and electric lines. The Communist's sabotage efforts were so successful that they caused backlash amongst the Malaysian population, who gradually withdrew support for the Communist movement as their livelihoods became threatened.
In Mandatory Palestine from 1945 to 1948, Jewish groups opposed British control. Though that control was to end according to the Balfour Declaration in 1948, the groups used sabotage as an opposition tactic. The Haganah focused their efforts on camps used by the British to hold refugees and radar installations that could be used to detect illegal immigrant ships. The Stern Gang and the Irgun used terrorism and sabotage against the British government and against lines of communications. In November 1946, the Irgun and Stern Gang attacked a railroad twenty-one times in a three-week period, eventually causing shell-shocked Arab railway workers to strike. The 6th Airborne Division was called in to provide security as a means of ending the strike.
In Vietnam.
The Viet Cong used swimmer saboteurs often and effectively during the Vietnam War. Between 1969 and 1970, swimmer saboteurs sunk, destroyed, or damaged 77 assets of the U.S. and its allies. Viet Cong swimmers were poorly equipped but well-trained and resourceful. The swimmers provided a low-cost/low-risk option with high payoff; possible loss to the country for failure compared to the possible gains from a successful mission led to the obvious conclusion the swimmer saboteurs were a good idea.
During the Cold War.
On 1 January 1984, the Cuscatlan bridge over Lempa river in El Salvador, critical to flow of commercial and military traffic, was destroyed by guerrilla forces using explosives after using mortar fire to "scatter" the bridge's guards, causing an estimated $3.7 million in required repairs, and considerably impacting on El Salvadoran business and security. 
In 1982 in Honduras, a group of nine Salvadorans and Nicaraguans destroyed a main electrical power station, leaving Tegucigalpa, the capital city, for three days without power.
As crime.
Some criminals have engaged in acts of sabotage for reasons of extortion. For example, Klaus-Peter Sabotta sabotaged German railway lines in the late 1990s in an attempt to extort DM10 million from the German railway operator Deutsche Bahn. He is now serving a sentence of life imprisonment.
As political action.
The term political sabotage is sometimes used to define the acts of one political camp to disrupt, harass or damage the reputation of a political opponent, usually during an electoral campaign. See Watergate. The term could also describe the actions and expenditures of private entities, corporations and organizations against democratically approved or enacted laws, policies and programs.
In a coup d'etat.
Sabotage is a crucial tool of the successful coup d'etat, which requires control of communications before, during, and after the coup is staged. Simple sabotage against physical communications platforms using semi-skilled technicians, or even those trained only for this task, could effectively silence the target government of the coup, leaving the information battle space open to the dominance of the coup's leaders. To underscore the effectiveness of sabotage, "A single cooperative technician will be able temporarily to put out of action a radio station which would otherwise require a full-scale assault."
Railroads, where strategically important to the regime the coup is against, are prime targets for sabotage- if a section of the track is damaged entire portions of the transportation network can be stopped until it is fixed.
Derivative usages.
Sabotage radio.
A sabotage radio was a small two-way radio designed for use by resistance movements in World War II, and after the war often used by expeditions and similar parties.
Cybotage.
Arquilla and Rondfeldt, in their work entitled "Networks and Netwars", differentiate their definition of "netwar" from a list of "trendy synonyms," including "cybotage," a portmanteau from the words "sabotage" and "cyber." They dub the practitioners of cybotage "cyboteurs" and note while all cybotage is not netwar, some netwar is cybotage.
Counter-sabotage.
Counter-sabotage, defined by Webster's dictionary, is "counterintelligence designed to detect and counteract sabotage." The United States Department of Defense definition, found in the Dictionary of Military and Associated Terms, is "Action designed to detect and counteract sabotage. See also counterintelligence".
In World War II.
During World War II, British subject Eddie Chapman, trained by the Germans in sabotage, became a double agent for the British. The German Abwehr entrusted Chapman to destroy the British de Havilland Company's main plant which manufactured the outstanding Mosquito light bomber, but required photographic proof from their agent to verify the mission's completion. A special unit of the Royal Engineers known as the Magic Gang covered the de Havilland plant with canvas panels and scattered papier-mâché furniture and chunks of masonry around three broken and burnt giant generators. Photos of the plant taken from the air reflected devastation for the factory and a successful sabotage mission, and Chapman, as a British sabotage double-agent, fooled the Germans for the duration of the war.
Borrowed into Japanese.
In Japanese, the verb saboru (サボる) means to skip school or loaf on the job.

</doc>
<doc id="29468" url="http://en.wikipedia.org/wiki?curid=29468" title="Speech recognition">
Speech recognition

In computer science and electrical engineering, speech recognition (SR) is the translation of spoken words into text. It is also known as "automatic speech recognition" (ASR), "computer speech recognition", or just "speech to text" (STT).
Some SR systems use "speaker-independent speech recognition" while others use "training" where an individual speaker reads sections of text into the SR system. These systems analyze the person's specific voice and use it to fine-tune the recognition of that person's speech, resulting in more accurate transcription. Systems that do not use training are called "speaker-independent" systems. Systems that use training are called "speaker-dependent" systems.
Speech recognition applications include voice user interfaces such as voice dialling (e.g. "Call home"), call routing (e.g. "I would like to make a collect call"), domotic appliance control, search (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed Direct Voice Input).
The term "voice recognition" or "speaker identification" refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.
From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the world-wide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems. These speech industry players include Microsoft, Google, IBM, Baidu (China), Apple, Amazon, Nuance, IflyTek (China), many of which have publicized the core technology in their speech recognition systems being based on deep learning. 
History.
As early as 1932, Bell Labs researchers like Harvey Fletcher were investigating the science of speech perception. In 1952 three Bell Labs researchers built a system for single-speaker digit recognition. Their system worked by locating the formants in the power spectrum of each utterance. The 1950s era technology was limited to single-speaker systems with vocabularies of around ten words.
Unfortunately, funding at Bell Labs dried up for several years when, in 1969, the influential John Pierce wrote an open letter that was critical of speech recognition research. Pierce's letter compared speech recognition to "schemes for turning water into gasoline, extracting gold from the sea, curing cancer, or going to the moon." Pierce defunded speech recognition research at Bell Labs.
Raj Reddy was the first person to take on continuous speech recognition as a graduate student at Stanford University in the late 1960s. Previous systems required the users to make a pause after each word. Reddy's system was designed to issue spoken commands for the game of chess. Also around this time Soviet researchers invented the dynamic time warping algorithm and used it to create a recognizer capable of operating on a 200-word vocabulary. Achieving speaker independence was a major unsolved goal of researchers during this time period.
In 1971, DARPA funded five years of speech recognition research through its Speech Understanding Research program with ambitious end goals including a minimum vocabulary size of 1,000 words. BBN. IBM., Carnegie Mellon and Stanford Research Institute all participated in the program. The government funding revived speech recognition research that had been largely abandoned in the United States after John Pierce's letter. Despite the fact that CMU's Harpy system met the goals established at the outset of the program, many of the predictions turned out to be nothing more than hype disappointing DARPA administrators. This disappointment led to DARPA not continuing the funding. Several innovations happened during this time, such as the invention of beam search for use in CMU's Harpy system. The field also benefited from the discovery of several algorithms in other fields such as linear predictive coding and cepstral analysis.
During the late 1960's Leonard Baum developed the mathematics of Markov chains at the Institute for Defense Analysis. At CMU, Raj Reddy's student James Baker and his wife Janet Baker began using the Hidden Markov Model (HMM) for speech recognition. James Baker had learned about HMMs from a summer job at the Institute of Defense Analysis during his undergraduate education. The use of HMMs allowed researchers to combine different sources of knowledge, such as acoustics, language, and syntax, in a unified probabilistic model.
Under Fred Jelinek's lead, IBM created a voice activated typewriter called Tangora, which could handle a 20,000 word vocabulary by the mid 1980s. Jelinek's statistical approach put less emphasis on emulating the way the human brain processes and understands speech in favor of using statistical modeling techniques like HMMs. (Jelinek's group independently discovered the application of HMMs to speech.) This was controversial with linguists since HMMs are too simplistic to account for many common features of human languages. However, the HMM proved to be a highly useful way for modeling speech and replaced dynamic time warping to become the dominate speech recognition algorithm in the 1980s.
IBM had a few competitors including Dragon Systems founded by James and Janet Baker in 1982. The 1980s also saw the introduction of the n-gram language model.
Much of the progress in the field is owed to the rapidly increasing capabilities of computers. At the end of the DARPA program in 1976, the best computer available to researchers was the PDP-10 with 4 MB ram. A few decades later, researchers had access to tens of thousands of times as much computing power. As the technology advanced and computers got faster, researchers began tackling harder problems such as larger vocabularies, speaker independence, noisy environments and conversational speech. In particular, this shifting to more difficult tasks has characterized DARPA funding of speech recognition since the 1980s. For example, progress was made on speaker independence first by training on a larger variety of speakers and then later by doing explicit speaker adaptation during decoding. Further reductions in word error rate came as researchers shifted acoustic models to be discriminative instead of using maximum likelihood models.
Another one of Raj Reddy's former students, Xuedong Huang, developed the Sphinx-II system at CMU. The Sphinx-II system was the first to do speaker-independent, large vocabulary, continuous speech recognition and it had the best performance in DARPA's 1992 evaluation. Huang went on to found the speech recognition group at Microsoft in 1993.
The 1990s saw the first introduction of commercially successful speech recognition technologies. By this point, the vocabulary of the typical commercial speech recognition system was larger than the average human vocabulary. In 2000, Lernout & Hauspie acquired Dragon Systems and was an industry leader until an accounting scandal brought an end to the company in 2001. The L&H speech technology was bought by ScanSoft which became Nuance in 2005. Apple originally licensed software from Nuance to provide speech recognition capability to its digital assistant Siri.
In the 2000s DARPA sponsored two speech recognition programs: Effective Affordable Reusable Speech-to-Text (EARS) in 2002 and Global Autonomous Language Exploitation (GALE). Four teams participated in the EARS program: IBM, BBN,
Cambridge University and a team composed of ISCI, SRI and University of Washington. The GALE program focused on Mandarin broadcast news speech. Google's first effort at speech recognition came in 2007 with the launch of GOOG-411, a telephone based directory service. The recordings from GOOG-411 produced valuable data that helped Google improve their recognition systems. Google voice search is now supported in over 30 languages.
The use of deep learning for acoustic modeling was introduced during later part of 2009 by Geoffrey Hinton and his students at University of Toronto and by Li Deng and colleagues at Microsoft Research, initially in the collaborative work between Microsoft and University of Toronto which was subsequently expanded to include IBM and Google (hence "The shared views of four research groups" subtitle in their 2012 review paper). A Microsoft research executive called this innovation "the most dramatic change in accuracy since 1979." In contrast to the steady incremental improvements of the past few decades, the application of deep learning decreased word error rate by 30%. This innovation was quickly adopted across the field. Researchers have began to use deep learning techniques for language modeling as well.
In the long history of speech recognition, both shallow form and deep form (e.g. recurrent nets) of artificial neural networks had been explored for many years during 80's, 90's and a few years into 2000.
But these methods never won over the non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.
A number of key difficulties had been methodologically analyzed in 1990's, including gradient diminishing and weak temporal correlation structure in the neural predictive models.
All these difficulties were in addition to the lack of big training data and big computing power in these early days. Most speech recognition researchers who understood such barriers hence subsequently moved away from neural nets to pursue generative modeling approaches until the recent resurgence of deep learning starting around 2009-2010 that had overcome all these difficulties. Hinton et al. and Deng et al. reviewed part of this recent history about how their collaboration with each other and then with colleagues across four groups (University of Toronto, Microsoft, Google, and IBM) ignited the renaissance of neural networks and initiated deep learning research and applications in speech recognition.
Models, methods, and algorithms.
Both acoustic modeling and language modeling are important parts of modern statistically-based speech recognition algorithms. Hidden Markov models (HMMs) are widely used in many systems. Language modeling is also used in many other natural language processing applications such as document classification or statistical machine translation.
Hidden Markov models.
Modern general-purpose speech recognition systems are based on Hidden Markov Models. These are statistical models that output a sequence of symbols or quantities. HMMs are used in speech recognition because a speech signal can be viewed as a piecewise stationary signal or a short-time stationary signal. In a short time-scale (e.g., 10 milliseconds), speech can be approximated as a stationary process. Speech can be thought of as a Markov model for many stochastic purposes.
Another reason why HMMs are popular is because they can be trained automatically and are simple and computationally feasible to use. In speech recognition, the hidden Markov model would output a sequence of "n"-dimensional real-valued vectors (with "n" being a small integer, such as 10), outputting one of these every 10 milliseconds. The vectors would consist of cepstral coefficients, which are obtained by taking a Fourier transform of a short time window of speech and decorrelating the spectrum using a cosine transform, then taking the first (most significant) coefficients. The hidden Markov model will tend to have in each state a statistical distribution that is a mixture of diagonal covariance Gaussians, which will give a likelihood for each observed vector. Each word, or (for more general speech recognition systems), each phoneme, will have a different output distribution; a hidden Markov model for a sequence of words or phonemes is made by concatenating the individual trained hidden Markov models for the separate words and phonemes.
Described above are the core elements of the most common, HMM-based approach to speech recognition. Modern speech recognition systems use various combinations of a number of standard techniques in order to improve results over the basic approach described above. A typical large-vocabulary system would need context dependency for the phonemes (so phonemes with different left and right context have different realizations as HMM states); it would use cepstral normalization to normalize for different speaker and recording conditions; for further speaker normalization it might use vocal tract length normalization (VTLN) for male-female normalization and maximum likelihood linear regression (MLLR) for more general speaker adaptation. The features would have so-called delta and delta-delta coefficients to capture speech dynamics and in addition might use heteroscedastic linear discriminant analysis (HLDA); or might skip the delta and delta-delta coefficients and use splicing and an LDA-based projection followed perhaps by heteroscedastic linear discriminant analysis or a global semi-tied co variance transform (also known as maximum likelihood linear transform, or MLLT). Many systems use so-called discriminative training techniques that dispense with a purely statistical approach to HMM parameter estimation and instead optimize some classification-related measure of the training data. Examples are maximum mutual information (MMI), minimum classification error (MCE) and minimum phone error (MPE).
Decoding of the speech (the term for what happens when the system is presented with a new utterance and must compute the most likely source sentence) would probably use the Viterbi algorithm to find the best path, and here there is a choice between dynamically creating a combination hidden Markov model, which includes both the acoustic and language model information, and combining it statically beforehand (the finite state transducer, or FST, approach).
A possible improvement to decoding is to keep a set of good candidates instead of just keeping the best candidate, and to use a better scoring function (re scoring) to rate these good candidates so that we may pick the best one according to this refined score. The set of candidates can be kept either as a list (the N-best list approach) or as a subset of the models (a lattice). Re scoring is usually done by trying to minimize the Bayes risk (or an approximation thereof): Instead of taking the source sentence with maximal probability, we try to take the sentence that minimizes the expectancy of a given loss function with regards to all possible transcriptions (i.e., we take the sentence that minimizes the average distance to other possible sentences weighted by their estimated probability). The loss function is usually the Levenshtein distance, though it can be different distances for specific tasks; the set of possible transcriptions is, of course, pruned to maintain tractability. Efficient algorithms have been devised to re score lattices represented as weighted finite state transducers with edit distances represented themselves as a finite state transducer verifying certain assumptions.
Dynamic time warping (DTW)-based speech recognition.
Dynamic time warping is an approach that was historically used for speech recognition but has now largely been displaced by the more successful HMM-based approach.
Dynamic time warping is an algorithm for measuring similarity between two sequences that may vary in time or speed. For instance, similarities in walking patterns would be detected, even if in one video the person was walking slowly and if in another he or she were walking more quickly, or even if there were accelerations and deceleration during the course of one observation. DTW has been applied to video, audio, and graphics – indeed, any data that can be turned into a linear representation can be analyzed with DTW.
A well-known application has been automatic speech recognition, to cope with different speaking speeds. In general, it is a method that allows a computer to find an optimal match between two given sequences (e.g., time series) with certain restrictions. That is, the sequences are "warped" non-linearly to match each other. This sequence alignment method is often used in the context of hidden Markov models.
Neural networks.
Neural networks emerged as an attractive acoustic modeling approach in ASR in the late 1980s. Since then, neural networks have been used in many aspects of speech recognition such as phoneme classification, isolated word recognition, and speaker adaptation.
In contrast to HMMs, neural networks make no assumptions about feature statistical properties and have several qualities making them attractive recognition models for speech recognition. When used to estimate the probabilities of a speech feature segment, neural networks allow discriminative training in a natural and efficient manner. Few assumptions on the statistics of input features are made with neural networks. However, in spite of their effectiveness in classifying short-time units such as individual phones and isolated words, neural networks are rarely successful for continuous recognition tasks, largely because of their lack of ability to model temporal dependencies.
However, recently Recurrent Neural Networks(RNN's) and Time Delay Neural Networks(TDNN's) have been used which have been shown to be able to identify latent temporal dependencies and use this information to perform the task of speech recognition. This however enormously increases the computational cost involved and hence makes the process of speech recognition slower. A lot of research is still going on in this field to ensure that TDNN's and RNN's can be used in a more computationally affordable way to improve the Speech Recognition Accuracy immensely.
Deep Neural Networks and Denoising Autoencoders are also being experimented with to tackle this problem in an effective manner.
Due to the inability of traditional Neural Networks to model temporal dependencies, an alternative approach is to use neural networks as a pre-processing e.g. feature transformation, dimensionality reduction, for the HMM based recognition.
Deep Neural Networks and Other Deep Learning Models.
A deep neural network (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers. Similar to shallow neural networks, DNNs can model complex non-linear relationships. DNN architectures generate compositional models, where extra layers enable composition of features from lower layers, giving a huge learning capacity and thus the potential of modeling complex patterns of speech data. The DNN is the most popular type of deep learning architectures successfully used as an acoustic model for speech recognition since 2010.
The success of DNNs in large vocabulary speech recognition occurred in 2010 by industrial researchers, in collaboration with academic researchers, where large output layers of the DNN based on context dependent HMM states constructed by decision trees were adopted.
 See comprehensive reviews of this development and of the state of the art as of October 2014 in the recent . See also the related background of automatic speech recognition and the impact of various machine learning paradigms including notably deep learning in
a recent overview article.
One fundamental principle of deep learning is to do away with hand-crafted feature engineering and to use raw features. This principle was first explored successfully in the architecture of deep autoencoder on the "raw" spectrogram or linear filter-bank features, showing its superiority over the Mel-Cepstral features which contain a few stages of fixed transformation from spectrograms.
The true "raw" features of speech, waveforms, have more recently been shown to produce excellent larger-scale speech recognition results.
Since the initial successful debut of DNNs for speech recognition around 2009-2011, there have been huge new progresses made. This progress (as well as future directions) has been summarized into the following eight major areas: 
Large-scale automatic speech recognition is the first and the most convincing successful case of deep learning in the recent history, embraced by both industry and academic across the board. Between 2010 and 2014, the two major conferences on signal processing and speech recognition, IEEE-ICASSP and Interspeech, have seen near exponential growth in the numbers of accepted papers in their respective annual conference papers on the topic of deep learning for speech recognition. More importantly, all major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) nowadays are based on deep learning methods. See also the recent media interview with the CTO of Nuance Communications.
Applications.
In-car systems.
Typically a manual control input, for example by means of a finger control on the steering-wheel, enables the speech recognition system and this is signalled to the driver by an audio prompt. Following the audio prompt, the system has a "listening window" during which it may accept a speech input for recognition.
Simple voice commands may be used to initiate phone calls, select radio stations or play music from a compatible smartphone, MP3 player or music-loaded flash drive. Voice recognition capabilities vary between car make and model. Some of the most recent car models offer natural-language speech recognition in place of a fixed set of commands. allowing the driver to use full sentences and common phrases. With such systems there is, therefore, no need for the user to memorize a set of fixed command words.
Health care.
Medical documentation.
In the health care sector, speech recognition can be implemented in front-end or back-end of the medical documentation process. Front-end speech recognition is where the provider dictates into a speech-recognition engine, the recognized words are displayed as they are spoken, and the dictator is responsible for editing and signing off on the document. Back-end or deferred speech recognition is where the provider dictates into a digital dictation system, the voice is routed through a speech-recognition machine and the recognized draft document is routed along with the original voice file to the editor, where the draft is edited and report finalised. Deferred speech recognition is widely used in the industry currently.
One of the major issues relating to the use of speech recognition in healthcare is that the American Recovery and Reinvestment Act of 2009 (ARRA) provides for substantial financial benefits to physicians who utilize an EMR according to "Meaningful Use" standards. These standards require that a substantial amount of data be maintained by the EMR (now more commonly referred to as an Electronic Health Record or EHR). The use of speech recognition is more naturally suited to the generation of narrative text, as part of a radiology/pathology interpretation, progress note or discharge summary: the ergonomic gains of using speech recognition to enter structured discrete data (e.g., numeric values or codes from a list or a controlled vocabulary) are relatively minimal for people who are sighted and who can operate a keyboard and mouse.
A more significant issue is that most EHRs have not been expressly tailored to take advantage of voice-recognition capabilities. A large part of the clinician's interaction with the EHR involves navigation through the user interface using menus, and tab/button clicks, and is heavily dependent on keyboard and mouse: voice-based navigation provides only modest ergonomic benefits. By contrast, many highly customized systems for radiology or pathology dictation implement voice "macros", where the use of certain phrases - e.g., "normal report", will automatically fill in a large number of default values and/or generate boilerplate, which will vary with the type of the exam - e.g., a chest X-ray vs. a gastrointestinal contrast series for a radiology system.
As an alternative to this navigation by hand, cascaded use of speech recognition and information extraction has been studied as a way to fill out a handover form for clinical proofing and sign-off. The results are encouraging, and the paper also opens data, together with the related performance benchmarks and some processing software, to the research and development community for studying clinical documentation and language-processing.
Therapeutic use.
Prolonged use of speech recognition software in conjunction with word processors has shown benefits to short-term-memory restrengthening in brain AVM patients who have been treated with resection. Further research needs to be conducted to determine cognitive benefits for individuals whose AVMs have been treated using radiologic techniques.
Military.
High-performance fighter aircraft.
Substantial efforts have been devoted in the last decade to the test and evaluation of speech recognition in fighter aircraft. Of particular note is the U.S. program in speech recognition for the Advanced Fighter Technology Integration (AFTI)/F-16 aircraft (F-16 VISTA), and a program in France installing speech recognition systems on Mirage aircraft, and also programs in the UK dealing with a variety of aircraft platforms. In these programs, speech recognizers have been operated successfully in fighter aircraft, with applications including: setting radio frequencies, commanding an autopilot system, setting steer-point coordinates and weapons release parameters, and controlling flight display.
Working with Swedish pilots flying in the JAS-39 Gripen cockpit, Englund (2004) found recognition deteriorated with increasing G-loads. It was also concluded that adaptation greatly improved the results in all cases and introducing models for breathing was shown to improve recognition scores significantly. Contrary to what might be expected, no effects of the broken English of the speakers were found. It was evident that spontaneous speech caused problems for the recognizer, as could be expected. A restricted vocabulary, and above all, a proper syntax, could thus be expected to improve recognition accuracy substantially.
The Eurofighter Typhoon currently in service with the UK RAF employs a speaker-dependent system, i.e. it requires each pilot to create a template. The system is not used for any safety critical or weapon critical tasks, such as weapon release or lowering of the undercarriage, but is used for a wide range of other cockpit functions. Voice commands are confirmed by visual and/or aural feedback. The system is seen as a major design feature in the reduction of pilot workload, and even allows the pilot to assign targets to himself with two simple voice commands or to any of his wingmen with only five commands.
Speaker-independent systems are also being developed and are in testing for the F35 Lightning II (JSF) and the Alenia Aermacchi M-346 Master lead-in fighter trainer. These systems have produced word accuracies in excess of 98%.
Helicopters.
The problems of achieving high recognition accuracy under stress and noise pertain strongly to the helicopter environment as well as to the jet fighter environment. The acoustic noise problem is actually more severe in the helicopter environment, not only because of the high noise levels but also because the helicopter pilot, in general, does not wear a facemask, which would reduce acoustic noise in the microphone. Substantial test and evaluation programs have been carried out in the past decade in speech recognition systems applications in helicopters, notably by the U.S. Army Avionics Research and Development Activity (AVRADA) and by the Royal Aerospace Establishment (RAE) in the UK. Work in France has included speech recognition in the Puma helicopter. There has also been much useful work in Canada. Results have been encouraging, and voice applications have included: control of communication radios, setting of navigation systems, and control of an automated target handover system.
As in fighter applications, the overriding issue for voice in helicopters is the impact on pilot effectiveness. Encouraging results are reported for the AVRADA tests, although these represent only a feasibility demonstration in a test environment. Much remains to be done both in speech recognition and in overall speech technology in order to consistently achieve performance improvements in operational settings.
Training air traffic controllers.
Training for air traffic controllers (ATC) represents an excellent application for speech recognition systems. Many ATC training systems currently require a person to act as a "pseudo-pilot", engaging in a voice dialog with the trainee controller, which simulates the dialog that the controller would have to conduct with pilots in a real ATC situation.
Speech recognition and synthesis techniques offer the potential to eliminate the need for a person to act as pseudo-pilot, thus reducing training and support personnel. In theory, Air controller tasks are also characterized by highly structured speech as the primary output of the controller, hence reducing the difficulty of the speech recognition task should be possible. In practice, this is rarely the case. The FAA document 7110.65 details the phrases that should be used by air traffic controllers. While this document gives less than 150 examples of such phrases, the number of phrases supported by one of the simulation vendors speech recognition systems is in excess of 500,000.
The USAF, USMC, US Army, US Navy, and FAA as well as a number of international ATC training organizations such as the Royal Australian Air Force and Civil Aviation Authorities in Italy, Brazil, and Canada are currently using ATC simulators with speech recognition from a number of different vendors.
Telephony and other domains.
ASR in the field of telephony is now commonplace and in the field of computer gaming and simulation is becoming more widespread. Despite the high level of integration with word processing in general personal computing. However, ASR in the field of document production has not seen the expected increases in use.
The improvement of mobile processor speeds made feasible the speech-enabled Symbian and Windows Mobile smartphones. Speech is used mostly as a part of a user interface, for creating predefined or custom speech commands. Leading software vendors in this field are: Google, Microsoft Corporation (Microsoft Voice Command), Digital Syphon (Sonic Extractor), LumenVox, Nuance Communications (Nuance Voice Control), VoiceBox Technology, Speech Technology Center, Vito Technologies (VITO Voice2Go), Speereo Software (Speereo Voice Translator), and SVOX.
Usage in education and daily life.
For language learning, speech recognition can be useful for learning a second language. It can teach proper pronunciation, in addition to helping a person develop fluency with their speaking skills.
Students who are blind (see Blindness and education) or have very low vision can benefit from using the technology to convey words and then hear the computer recite them, as well as use a computer by commanding with their voice, instead of having to look at the screen and keyboard.
Students who are physically disabled or suffer from Repetitive strain injury/other injuries to the upper extremities can be relieved from having to worry about handwriting, typing, or working with scribe on school assignments by using speech-to-text programs. They can also utilize speech recognition technology to freely enjoy searching the Internet or using a computer at home without having to physically operate a mouse and keyboard.
Speech recognition can allow students with learning disabilities to become better writers. By saying the words aloud, they can increase the fluidity of their writing, and be alleviated of concerns regarding spelling, punctuation, and other mechanics of writing. Also, see Learning disability.
Voice Recognition Software's use, in conjunction with a digital audio recorder, a personal computer and Microsoft Word has proven to be positive for restoring damaged short-term-memory capacity, in stroke and craniotomy individuals.
People with disabilities.
People with disabilities can benefit from speech recognition programs. For individuals that are Deaf or Hard of Hearing, speech recognition software is used to automatically generate a closed-captioning of conversations such as discussions in conference rooms, classroom lectures, and/or religious services.
Speech recognition is also very useful for people who have difficulty using their hands, ranging from mild repetitive stress injuries to involved disabilities that preclude using conventional computer input devices. In fact, people who used the keyboard a lot and developed RSI became an urgent early market for speech recognition. Speech recognition is used in deaf telephony, such as voicemail to text, relay services, and captioned telephone. Individuals with learning disabilities who have problems with thought-to-paper communication (essentially they think of an idea but it is processed incorrectly causing it to end up differently on paper) can possibly benefit from the software but the technology is not bug proof. Also the whole idea of speak to text can be hard for intellectually disabled person's due to the fact that it is rare that anyone tries to learn the technology to teach the person with the disability.
This type of technology can help those with dyslexia but other disabilities are still in question. The effectiveness of the product is the problem that is hindering it being effective. Although a kid may be able to say a word depending on how clear they say it the technology may think they are saying another word and input the wrong one. Giving them more work to fix, causing them to have to take more time with fixing the wrong word.
Performance.
The performance of speech recognition systems is usually evaluated in terms of accuracy and speed. Accuracy is usually rated with word error rate (WER), whereas speed is measured with the real time factor. Other measures of accuracy include Single Word Error Rate (SWER) and Command Success Rate (CSR).
However, speech recognition (by a machine) is a very complex problem. Vocalizations vary in terms of accent, pronunciation, articulation, roughness, nasality, pitch, volume, and speed. Speech is distorted by a background noise and echoes, electrical characteristics. Accuracy of speech recognition vary with the following:
Accuracy.
As mentioned earlier in this article, accuracy of speech recognition varies in the following:
e.g. The 10 digits "zero" to "nine" can be recognized essentially perfectly, but vocabulary sizes of 200, 5000 or 100000 may have error rates of 3%, 7% or 45% respectively.
e.g. The 26 letters of the English alphabet are difficult to discriminate because they are confusable words (most notoriously, the E-set: "B, C, D, E, G, P, T, V, Z");
an 8% error rate is considered good for this vocabulary.
A speaker-dependent system is intended for use by a single speaker.
A speaker-independent system is intended for use by any speaker, more difficult.
With isolated speech single words are used, therefore it becomes easier to recognize the speech.
With discontinuous speech full sentences separated by silence are used, therefore it becomes easier to recognize the speech as well as with isolated speech. 
With continuous speech naturally spoken sentences are used, therefore it becomes harder to recognize the speech, different from both isolated and discontinuous speech.
e.g. Querying application may dismiss the hypothesis "The apple is red." 
e.g. Constraints may be semantic; rejecting "The apple is angry." 
e.g. Syntactic; rejecting "Red is apple the." 
Constraints are often represented by a grammar. 
When a person reads it's usually in a context that has been previously prepared, but when a person uses spontaneous speech, it is difficult to recognize the speech because of the disfluencies (like "uh" and "um", false starts, incomplete sentences, stuttering, coughing, and laughter) and limited vocabulary. 
Environmental noise (e.g. Noise in a car or a factory) 
Acoustical distortions (e.g. echoes, room acoustics)
Speech recognition is a multi-levelled pattern recognition task.
e.g. Phonemes, Words, Phrases, and Sentences;
e.g. Known word pronunciations or legal word sequences, which can compensate for errors or uncertainties at lower level;
By combining decisions probabilistically at all lower levels, and making more deterministic decisions only at the highest level;
Speech recognition by a machine is a process broken into several phases. Computationally, it is a problem in which a sound pattern has to be recognized or classified into a category that represents a meaning to a human. Every acoustic signal can be broken in smaller more basic sub-signals. As the more complex sound signal is broken into the smaller sub-sounds, different levels are created, where at the top level we have complex sounds, which are made of simpler sounds on lower level, and going to lower levels even more, we create more basic and shorter and simpler sounds. The lowest level, where the sounds are the most fundamental, a machine would check for simple and more probabilistic rules of what sound should represent. Once these sounds are put together into more complex sound on upper level, a new set of more deterministic rules should predict what new complex sound should represent. The most upper level of a deterministic rule should figure out the meaning of complex expressions. In order to expand our knowledge about speech recognition we need to take into a consideration neural networks. There are four steps of neural network approaches: 
For telephone speech the sampling rate is 8000 samples per second; 
computed every 10 ms, with one 10 ms section called a frame;
Analysis of four-step neural network approaches can be explained by further information. Sound is produced by air (or some other medium) vibration, which we register by ears, but machines by receivers. Basic sound creates a wave which has 2 descriptions; Amplitude (how strong is it), and frequency (how often it vibrates per second).
The sound waves can be digitized: Sample a strength at short intervals like in picture above to get bunch of numbers that approximate at each time step the strength of a wave. Collection of these numbers represent analog wave. This new wave is digital. Sound waves are complicated because they superimpose one on top of each other. Like the waves would. This way they create odd-looking waves. For example, if there are two waves that interact with each other we can add them which creates new odd-looking wave.
Given basic sound blocks that a machine digitized, one has a bunch of numbers which describe a wave and waves describe words. Each frame has a unit block of sound, which are broken into basic sound waves and represented by numbers which, after Fourier Transform, can be statistically evaluated to set to which class of sounds it belongs. The nodes in the figure on a slide represent a feature of a sound in which a feature of a wave from the first layer of nodes to the second layer of nodes based on statistical analysis. This analysis depends on programmer's instructions. At this point, a second layer of nodes represents higher level features of a sound input which is again statistically evaluated to see what class they belong to. Last level of nodes should be output nodes that tell us with high probability what original sound really was.
In 1982, Kurzweil Applied Intelligence and Dragon Systems released speech recognition products. By 1985, Kurzweil’s software had a vocabulary of 1,000 words—if uttered one word at a time. Two years later, in 1987, its lexicon reached 20,000 words, entering the realm of human vocabularies, which range from 10,000 to 150,000 words. But recognition accuracy was only 10% in 1993. Two years later, the error rate crossed below 50%. Dragon Systems released "Naturally Speaking" in 1997, which recognized normal human speech. Progress mainly came from improved computer performance and larger source text databases. The Brown Corpus was the first major database available, containing several million words. Carnegie Mellon University researchers found no significant increase in recognition accuracy.
Further information.
Conferences and Journals.
Popular speech recognition conferences held each year or two include SpeechTEK and SpeechTEK Europe, ICASSP, Interspeech/Eurospeech, and the IEEE ASRU. Conferences in the field of natural language processing, such as ACL, NAACL, EMNLP, and HLT, are beginning to include papers on speech processing. Important journals include the IEEE Transactions on Speech and Audio Processing (later renamed IEEE Transactions on Audio, Speech and Language Processing and since Sept 2014 renamed IEEE/ACM Transactions on Audio, Speech and Language Processing --- after merging with an ACM publication), Computer Speech and Language, and Speech Communication.
Books.
Books like "Fundamentals of Speech Recognition" by Lawrence Rabiner can be useful to acquire basic knowledge but may not be fully up to date (1993). Another good source can be "Statistical Methods for Speech Recognition" by Frederick Jelinek and "Spoken Language Processing (2001)" by Xuedong Huang etc. More up to date are "Computer Speech", by Manfred R. Schroeder, second edition published in 2004, and "Speech Processing: A Dynamic and Optimization-Oriented Approach" published in 2003 by Li Deng and Doug O'Shaughnessey. The recently updated textbook of "Speech and Language Processing (2008)" by Jurafsky and Martin presents the basics and the state of the art for ASR. Speaker recognition also uses the same features, most of the same front-end processing, and classification techniuqes as is done in speech recognition. A most recent comprehensive textbook, "Fundamentals of Speaker Recognition" by Homayoon Beigi, is an in depth source for up to date details on the theory and practice. A good insight into the techniques used in the best modern systems can be gained by paying attention to government sponsored evaluations such as those organised by DARPA (the largest speech recognition-related project ongoing as of 2007 is the GALE project, which involves both speech recognition and translation components).
A good and accessible introduction to speech recognition technology and its history is provided by the general audience book "The Voice in the Machine. Building Computers That Understand Speech" by Roberto Pieraccini (2012).
The most recent book on speech recognition is "Automatic Speech Recognition: A Deep Learning Approach" (Publisher: Springer) written by D. Yu and L. Deng published near the end of 2014, with highly mathematically-oriented technical detail on how deep learning methods are derived and implemented in modern speech recognition systems based on DNNs and related deep learning methods. A related book, published earlier in 2014, "Deep Learning: Methods and Applications" by L. Deng and D. Yu provides a less technical but more methodology-focused overview of DNN-based speech recognition during 2009-2014, placed within the more general context of deep learning applications including not only speech recognition but also image recognition, natural language processing, information retrieval, multimodal processing, and multitask learning.
Software.
In terms of freely available resources, Carnegie Mellon University's Sphinx toolkit is one place to start to both learn about speech recognition and to start experimenting. Another resource (free but copyrighted) is the HTK book (and the accompanying HTK toolkit). The AT&T libraries GRM and DCD are also general software libraries for large-vocabulary speech recognition. For more recent and state-of-the-art techniques, Kaldi toolkit can be used.
For more software resources, see List of speech recognition software.

</doc>
<doc id="29476" url="http://en.wikipedia.org/wiki?curid=29476" title="Kaman SH-2 Seasprite">
Kaman SH-2 Seasprite

The Kaman SH-2 Seasprite is a ship-based helicopter, originally developed in the late 1950s as a fast utility helicopter for the United States Navy. In the 1970s, anti-submarine, anti-surface threat capabilities were added to the design, including over-the-horizon targeting, resulting in modifying most existing UH-2 models to the "SH-2 Seasprite".
This aircraft extends and increases shipboard sensor and weapon capabilities against several types of enemy threats, including submarines of all types, surface ships and patrol craft that may be armed with anti-ship missiles. It served with the U.S. Navy from the 1960s until the last SH-2G helicopters were retired in 2001.
Design and development.
Origins.
In 1956, the U.S. Navy launched a competition to meet its requirement for a compact, all-weather multipurpose naval helicopter. Kaman's K-20 model was selected as the winner. Kaman was awarded a contract for four prototype and 12 production "HU2K-1" helicopters in late 1957. Kaman's design was for a conventional helicopter powered by a single General Electric T58-8F turboshaft engine, driving a 44-foot four-bladed main rotor and a four-bladed tail rotor.
In 1960, the Royal Canadian Navy announced that the HU2K was the frontrunner for a large anti-submarine warfare contract; the Canadian Treasury Board had approved an initial procurement of 12 units for $14.5 million. Abruptly, Kaman raised the estimated price to $23 million, and there was concern that the manufacturer's weight and performance projections were overly optimistic. The Naval Board decided to wait until after the US Navy had conducted sea trials before approving the purchase. These trials revealed the HU2K to be overweight and underpowered, and thus incapable of meeting Canadian requirements. Hence, in late 1961, the Sikorsky Sea King was selected.
With no follow-on orders, Kaman ended production in the late 1960s after delivering 184 SH-2s to the U.S. Navy; production was later restarted in 1971 to manufacture an improved variant of the helicopter, the "SH-2F". A significant factor in the reopening of the production line was that the Navy's Sikorsky SH-60 Sea Hawk, which was newer and more capable in anti-submarine operations, was too large to be operated from the small flight decks of older frigates.
Further development.
Upon enactment of the 1962 United States Tri-Service aircraft designation system, the HU2K-1 was redesignated "UH-2A" and the "HU2K-1U" was redesignated "UH-2B". In service, the UH-2 Seasprite would see several modifications and improvements, such as the addition of fixtures for mounting external stores. Beginning in 1968, the Navy's remaining UH-2s were extensively remanufactured, their single engines being replaced by a twin-engine arrangement.
The UH-2 was selected to be the airframe for the interim Light Airborne Multi-Purpose System (LAMPS) helicopter in October 1970. LAMPS evolved in the late 1960s from an urgent requirement to develop a manned helicopter that would support a non-aviation ship and serve as its tactical Anti-Submarine Warfare arm. Known as LAMPS Mark I, the advanced sensors, processors, and display capabilities aboard the helicopter enabled ships to extend their situational awareness beyond the line-of-sight limitations that hamper shipboard radars and the short distances for acoustic detection and prosecution of underwater threats associated with hull-mounted sonars. H-2s reconfigured for the LAMPS mission were redesignated "SH-2D". On 16 March 1971, the first SH-2D LAMPS prototype first flew.
The full LAMPS I system was equipped on the "SH-2F". The SH-2F was delivered to the Navy beginning in 1973. This variant had upgraded engines, longer life rotor, and higher take-off weight. In 1981, the Navy ordered 60 production SH-2Fs. Beginning in 1987, 16 SH-2Fs were upgraded with chin mounted Forward Looking Infrared Sensors (FLIR), Chaff (AIRBOC)/Flares, dual rear mounted IR scramblers, and Missile/Mine detecting equipment.
Eventually all but two H-2s then in Navy inventory were remanufactured into SH-2Fs. The final production procurement of the SH-2F was in Fiscal Year 1986. The last six orders for production SH-2Fs were switched to the SH-2G Super Seasprite variant.
Operational history.
United States.
The UH-2 began entering operational service in 1962. The Navy soon found the helicopter's capabilities to be restricted by its single engine, and ordered Kaman to retrofit all of its Seasprites with a twin-engine arrangement instead; with two engines the Seasprite was capable of reaching an airspeed of 130 knots and operating at a range of up to 411 nautical miles. The Navy would operate a total fleet of nearly 200 Seasprites for various duties, such as anti-submarine warfare (ASW), search and rescue (SAR) and transportation. Typically, several UH-2s would be deployed upon an aircraft carrier to perform plane guard and SAR missions.
The UH-2 was introduced in time to see action in the Tonkin Gulf incident in August 1964; the Seasprite's principal contribution to what would become the Vietnam War was the retrieval of downed aircrews, both from the sea and from inside enemy territory, and was increasingly relied upon in this mission as the war intensified, such as during Operation Rolling Thunder in 1965. In October 1966 alone, out of 269 downed pilots, helicopter-based SAR teams were able to recover 103 men.
In the 1970s, the conversion of UH-2s to the SH-2 anti-submarine configuration provided the US Navy with its first ASW helicopter capable of operating from vessels other than its aircraft carriers. The small size of the SH-2 allowed it to be operated from flight decks that were too small for most helicopters, this being a factor in the Navy's decision to acquire the improved SH-2F in the early 1980s.
SH-2Fs were utilized to enforce and support Operation Earnest Will in July 1987, Operation Praying Mantis in April 1988, and Operation Desert Storm during January 1991 in the Persian Gulf region. The countermeasures and additional equipment on the SH-2F allowed it to conduct combat support and surface warfare missions in these hostile environments, which had an often-minimal threat from submarines. The SH-2F was retired from active service in October 1993, at roughly the same time that the Navy retired the last of its Vietnam-era Knox Class Frigates that were unable to accommodate the larger SH-60 Sea Hawk.
In 1991, the US Navy began to receive deliveries of the new SH-2G Super Seasprite; a total of 18 converted SH-2Fs and 6 new-built SH-2Gs were produced. These were assigned to Naval Reserve squadrons, the SH-2G entered service with HSL-84 in 1993. The SH-2 served in some 600 deployments and flew 1.5 million flight hours before the last of the type were finally retired in mid-2001.
New Zealand.
The Royal New Zealand Navy (RNZN) replaced its Westland Wasps with four interim SH-2F Seasprites (ex-US Navy), operated and maintained by a mix of Navy and Air Force personnel known as No. 3 Squadron RNZAF Naval Support Flight, to operate with ANZAC class frigates until the fleet of five new SH-2G Super Seasprites were delivered. The Navy air element was transferred to No. 6 Squadron RNZAF at RNZAF Base Auckland in Whenuapai in October 2005. RNZN Seasprites have seen service in East Timor. Six additional SH-2Fs rejected by the Royal Australian Navy were purchased and are now stationed at the RNZAF Ground Training Wing (GTW) at Woodbourne near Blenheim as training helicopters. An SH-2F (ex-RNZN, NZ3442) is preserved in the Royal New Zealand Air Force Museum, donated to the museum by Kaman Aircraft Corporation after an accident while in service with the RNZN.
Exports.
In the late 1990s the United States offered surplus U.S. Navy SH-2Fs as foreign aid to a number of countries, Greece which had been offered six and Turkey which had been offered 14 rejected the offer. Egypt acquired four SH-2F aircraft under the aid program mainly for spares to support a fleet of ten SH-2Gs. Poland acquired the later SH-2G.
Specifications.
UH-2A.
"Data from" "Carrier Aviation Air Power Directory"General characteristics
Performance
SH-2F.
"Data from" "The Encyclopedia of World Military Aircraft"General characteristics* Crew: 3 (Pilot, Co-pilot/Tactical Coordinator (TACCO), Sensor Operator (SENSO))
Performance
</ul>Armament
 </ul>
References.
</dl>

</doc>
<doc id="29507" url="http://en.wikipedia.org/wiki?curid=29507" title="Scientific American">
Scientific American

Scientific American (informally abbreviated SciAm) is an American popular science magazine. It has a long history of presenting scientific information on a monthly basis to the general educated public, with careful attention to the clarity of its text and the quality of its specially commissioned color graphics. Many famous scientists, including Albert Einstein, have contributed articles in the past 170 years. It is the oldest continuously published monthly magazine in the United States.
History.
"Scientific American" was founded by inventor and publisher Rufus M. Porter in 1845 as a four-page weekly newspaper. Throughout its early years, much emphasis was placed on reports of what was going on at the U.S. Patent Office. It also reported on a broad range of inventions including perpetual motion machines, an 1860 device for buoying vessels by Abraham Lincoln, and the universal joint which now can be found in nearly every automobile manufactured. Current issues include a "this date in history" section, featuring excerpts from articles originally published 50, 100, and 150 years earlier. Topics include humorous incidents, wrong-headed theories, and noteworthy advances in the history of science and technology.
Porter sold the publication to Alfred Ely Beach and Orson Desaix Munn I a mere ten months after founding it. Until 1948, it remained owned by Munn & Company. Under Orson Desaix Munn III, grandson of Orson I, it had evolved into something of a "workbench" publication, similar to the twentieth century incarnation of "Popular Science".
In the years after World War II, the magazine fell into decline. In 1948, three partners who were planning on starting a new popular science magazine, to be called "The Sciences", purchased the assets of the old "Scientific American" instead and put its name on the designs they had created for their new magazine. Thus the partners—publisher Gerard Piel, editor Dennis Flanagan, and general manager Donald H. Miller, Jr.—essentially created a new magazine. Miller retired in 1979, Flanagan and Piel in 1984, when Gerard Piel's son Jonathan became president and editor; circulation had grown fifteen-fold since 1948. In 1986, it was sold to the Holtzbrinck group of Germany, which has owned it since.
In the fall of 2008, "Scientific American" was put under the control of Nature Publishing Group, a division of Holtzbrinck.
Donald Miller died in December 1998, Gerard Piel in September 2004 and Dennis Flanagan in January 2005. Mariette DiChristina is the current editor-in-chief, after John Rennie stepped down in June 2009.
International editions.
"Scientific American" published its first foreign edition in 1890, the Spanish-language "La America Cientifica". Publication was suspended in 1905, and another 63 years would pass before another foreign-language edition appeared: In 1968, an Italian edition, "Le Scienze", was launched, and a Japanese edition, "Nikkei Science" (), followed three years later. A new Spanish edition, "Investigación y Ciencia" was launched in Spain in 1976, followed by a French edition, ', in France in 1977, and a German edition, ', in Germany in 1978. A Russian edition "V Mire Nauki" was launched in the Soviet Union in 1983, and continues in the present-day Russian Federation. "Kexue" (科学, "Science" in Chinese), a simplified Chinese edition launched in 1979, was the first Western magazine published in the People's Republic of China. Founded in Chongqing, the simplified Chinese magazine was transferred to Beijing in 2001. Later in 2005, a newer edition, "Global Science" (环球科学), was published instead of "Kexue", which shut down due to financial problems. A traditional Chinese edition, known as ("Scientist" in Chinese), was introduced to Taiwan in 2002, and has been developed to the best popular science magazine in Taiwan. The Hungarian edition "Tudomány" existed between 1984 and 1992. In 1986, an Arabic-edition, "Oloom magazine" (), was published. In 2002, a Portuguese edition was launched in Brazil.
Today, "Scientific American" publishes 18 foreign-language editions around the globe: Arabic, Brazilian Portuguese, Simplified Chinese, Traditional Chinese, Czech, Dutch, French, German, Greek, Hebrew, Italian, Japanese, Korean, Lithuanian (discontinued after 15 issues), Polish, Romanian, Russian, and Spanish.
From 1902 to 1911, "Scientific American" supervised the publication of the "Encyclopedia Americana", which during some of that period was known as "The Americana".
First issue.
It originally styled itself "The Advocate of Industry and Enterprise" and "Journal of Mechanical and other Improvements". On the front page of the first issue was the engraving of "Improved Rail-Road Cars". The masthead had a commentary as follows:
"Scientific American" published every Thursday morning at No. 11 Spruce Street, New York, No. 16 State Street, Boston, and No. 2l Arcade Philadelphia, (The principal office being in New York) by Rufus Porter. Each number will be furnished with from two to five original Engravings, many of them elegant, and illustrative of New Inventions, Scientific Principles, and Curious Works; and will contain, in high addition to the most interesting news of passing events, general notices of progress of Mechanical and other Scientific Improvements; American and Foreign. Improvements and Inventions; Catalogues of American Patents; Scientific Essays, illustrative of the principles of the sciences of Mechanics, Chemistry, and Architecture: useful information and instruction in various Arts and Trades; Curious Philosophical Experiments; Miscellaneous Intelligence, Music and Poetry. This paper is especially entitled to the patronage of Mechanics and Manufactures, being the only paper in America, devoted to the interest of those classes; but is particularly useful to farmers, as it will not only appraise them of improvements in agriculture implements, But instruct them in various mechanical trades, and guard them against impositions. As a family newspaper, it will convey more useful intelligence to children and young people, than five times its cost in school instruction. Another important argument in favor of this paper, is that it will be worth two dollars at the end of the year when the volume is complete, (Old volumes of the "New York Mechanic", being now worth double the original cost, in cash.) Terms: The "Scientific American" will be furnished to subscribers at $2.00 per annum, - one dollar in advance, and the balance in six months. Five copies will be sent to one address six months for four dollars in advance. Any person procuring two or more subscribers, will be entitled to a commission of 25 cents each.
The commentary under the illustration gives the flavor of its style at the time:
There is perhaps no mechanical subject, in which improvement has advanced so rapidly, within the last ten years, as that of railroad passenger cars. Let any person contrast the awkward and uncouth cars of '35 with the superbly splendid long cars now running on several of the eastern roads, and he will find it difficult to convey to a third party, a correct idea of the vast extent of improvement. Some of the most elegant cars of this class, and which are of a capacity to accommodate from sixty to eighty passengers, and run with a steadiness hardly equaled by a steamboat in still water, are manufactured by Davenport & Bridges, at their establishment in Cambridgeport, Mass. The manufacturers have recently introduced a variety of excellent improvements in the construction of trucks, springs, and connections, which are calculated to avoid atmospheric resistance, secure safety and convenience, and contribute ease and comfort to passengers, while flying at the rate of 30 or 40 miles per hour.
Also in the first issue is commentary on Signor Muzio Muzzi's proposed device for aerial navigation.
Scientific American 50 award.
The Scientific American 50 award was started in 2002 to recognize contributions to science and technology during the magazine's previous year. The magazine's 50 awards cover many categories including agriculture, communications, defence, environment, and medical diagnostics. The complete list of each year's winners appear in the December issue of the magazine, as well as on the magazine's web site.
Website.
In March 1996, "Scientific American" launched its own website that includes articles from current and past issues, online-only features, daily news, weird science, special reports, trivia, "Scidoku" and more.
Columns.
Notable features have included:
Television.
From 1990 to 2005 "Scientific American" also produced a television program on PBS called "Scientific American Frontiers".
Books.
From 1983 to 1997, "Scientific American" has produced an encyclopedia set of volumes from their publishing division, the Scientific American Library. These books were not sold in retail stores, but as a Book of the Month club selection priced from $24.95 to $32.95. Topics covered dozens of areas of scientific knowledge and included in-depth essays on: The Animal Mind; Atmosphere, Climate, and Change; Beyond the Third Dimension; Cosmic Clouds; Cycles of Life • Civilization and the Biosphere; The Discovery Of Subatomic Particles; Diversity and the Tropical Rain Forest; Earthquakes and Geological Discovery; Exploring Planetary Worlds; Gravity’s Fatal Attraction; Fire; Fossils And The History Of Life; From Quarks to the Cosmos; A Guided Tour Of The Living Cell; Human Diversity; Perception; The Solar System; Sun and Earth; The Science of Words (Linguistics); The Science Of Musical Sound; The Second Law (of Thermodynamics); Stars; Supercomputing and the Transformation of Science.
Scientific and political debate.
In April 1950, the U.S. Atomic Energy Commission ordered "Scientific American" to cease publication of an issue containing an article by Hans Bethe that appeared to reveal classified information about the thermonuclear hydrogen bomb. Subsequent review of the material determined that the AEC had overreacted. The incident was important for the "new" "Scientific American"'s history, as the AEC's decision to burn 3000 copies of an early press-run of the magazine containing the offending material appeared to be "book burning in a free society" when publisher Gerard Piel leaked the incident to the press.
In its January 2002 issue, "Scientific American" published a series of criticisms of the Bjørn Lomborg book "The Skeptical Environmentalist". Cato Institute fellow Patrick J. Michaels said the attacks came because the book "threatens billions of taxpayer dollars that go into the global change kitty every year." Journalist Ronald Bailey called the criticism "disturbing" and "dishonest", writing, "The subhead of the review section, 'Science defends itself against "The Skeptical Environmentalist",' gives the show away: Religious and political views need to defend themselves against criticism, but science is supposed to be a process for determining the facts."
The May 2007 issue featured a column by Michael Shermer calling for a United States pullout from the Iraq War. In response, "Wall Street Journal" online columnist James Taranto jokingly called "Scientific American" "a liberal political magazine".
The publisher was criticized in 2009 when it notified collegiate libraries that subscribe to the journal that yearly subscription prices would increase by nearly 500% for print and 50% for online access to $1500 yearly.
Controversy.
In 2013, Dr. Danielle Lee, a female scientist who blogs at "Scientific American", was called a "whore" in an email by an editor at the science website "Biology Online" after refusing to write professional content without compensation. When Lee, outraged about the email, wrote a rebuttal on her "Scientific American" blog, the editor-in-chief of "Scientific American", Mariette DiChristina, removed the post, sparking an outrage by supporters of Lee. While DiChristina cited legal reasons for removing the blog, others criticized her for censoring Lee. The editor at Biology Online was fired after the incident.
The controversy widened in the ensuing days. The magazine's blog editor, Bora Zivkovic, was the subject of allegations of sexual harassment by another blogger, Monica Byrne. Although the alleged incident had occurred about a year earlier, editor Mariette DiChristina informed readers that the incident had been investigated and resolved to Ms. Byrne's satisfaction. However, the incident involving Dr. Lee had prompted Ms. Byrne to reveal the identity of Zivkovic, following the latter's support of Dr. Lee. Zivkovic responded on Twitter and his own blog, admitting the incident with Ms. Byrne had taken place. His blog post apologized to Ms. Byrne, and referred to the incident as "singular", stating that his behavior was not "engaged in before or since."
Due to the allegations, Zivkovic resigned from the board of Science Online, the popular science blogging conference that he helped establish. Following Zivkovic's admission, several prominent female bloggers, including other bloggers for the magazine, wrote their own accounts that contradicted Zivkovic's assertions, alleging additional incidents of sexual harassment. A day after these new revelations, Zivkovic resigned his position at "Scientific American", according to a press release from the magazine.
References.
</dl>

</doc>
<doc id="29519" url="http://en.wikipedia.org/wiki?curid=29519" title="Side effect (computer science)">
Side effect (computer science)

In computer science, a function or expression is said to have a side effect if, in addition to returning a value, it also modifies some state or has an "observable" interaction with calling functions or the outside world. For example, a function might modify a global variable or static variable, modify one of its arguments, raise an exception, write data to a display or file, read data, or call other side-effecting functions. In the presence of side effects, a program's behavior may depend on history; that is, the order of evaluation matters. Understanding and debugging a function with side effects requires knowledge about the context and its possible histories.
Side effects are the most common way that a program interacts with the outside world (people, filesystems, other computers on networks). But the degree to which side effects are used depends on the programming paradigm. Imperative programming is known for its frequent utilization of side effects. In functional programming, side effects are rarely used. Functional languages such as Standard ML, Scheme and Scala do not restrict side effects, but it is customary for programmers to avoid them. The functional language Haskell expresses side effects such as I/O and other stateful computations using monadic actions.
Assembly language programmers must be aware of "hidden" side effects — instructions that modify parts of the processor state which are not mentioned in the instruction's mnemonic. A classic example of a hidden side effect is an arithmetic instruction that implicitly modifies condition codes (a hidden side effect) while it explicitly modifies a register (the overt effect). One potential drawback of an instruction set with hidden side effects is that, if many instructions have side effects on a single piece of state, like condition codes, then the logic required to update that state sequentially may become a performance bottleneck. The problem is particularly acute on some processors designed with pipelining (since 1990) or with out-of-order execution. Such a processor may require additional control circuitry to detect hidden side effects and stall the pipeline if the next instruction depends on the results of those effects.
Referential transparency.
Absence of side effects is a necessary, but not sufficient, condition for referential transparency. Referential transparency means that an expression (such as a function call) can be replaced with its value; this requires that the expression has no side effects and is pure (always returns the same results on the same input).
Temporal side effects.
Side effects caused by the time taken for an operation to execute are usually ignored when discussing side effects and referential transparency. There are some cases, such as with hardware timing or testing, where operations are inserted specifically for their temporal side effects e.g. codice_1 or codice_2. These instructions do not change state other than taking an amount of time to complete.
Idempotence.
A function with side effects is said to be idempotent under sequential composition (codice_3) if, when called with the same argument twice, the second call returns the same value and has no side effects which can distinguish it from the first call. For instance, consider:
Here, codice_4 is idempotent because the second call to codice_4 (with the same argument) returns the same value and does not change the visible program state. Note that this is distinct from idempotence under function composition (codice_6): codice_4 is not idempotent under function composition because codice_8 and codice_9 set codice_10 to different values where return value of xSetter is x; in this example return value is always 5.
Example.
One common demonstration of side effect behavior is that of the assignment operator in C++. For example, assignment returns the right operand and has the side effect of assigning that value to a variable. This allows for syntactically clean multiple assignment:
Because the operator right associates, this equates to
Where the result of assigning 3 into "j" then gets assigned into "i". This presents a potential hangup for novice programmers who may confuse
with

</doc>
<doc id="29532" url="http://en.wikipedia.org/wiki?curid=29532" title="Sebastian Shaw">
Sebastian Shaw

Sebastian Shaw is the name of:

</doc>
<doc id="29536" url="http://en.wikipedia.org/wiki?curid=29536" title="Stephen Schneider">
Stephen Schneider

Stephen Henry Schneider (February 11, 1945 – July 19, 2010) was Professor of Environmental Biology and Global Change at Stanford University, a Co-Director at the Center for Environment Science and Policy of the Freeman Spogli Institute for International Studies and a Senior Fellow in the Stanford Woods Institute for the Environment. Schneider served as a consultant to federal agencies and White House staff in the Richard Nixon, Jimmy Carter, Ronald Reagan, George H. W. Bush, Bill Clinton, George W. Bush and Barack Obama administrations.
His research included modeling of the atmosphere, climate change, and the effect of global climate change on biological systems. Schneider was the founder and editor of the journal "Climatic Change" and authored or co-authored over 450 scientific papers and other publications. He was a Coordinating Lead Author in Working Group II IPCC TAR and was engaged as a co-anchor of the Key Vulnerabilities Cross-Cutting Theme for the Fourth Assessment Report (AR4) at the time of his death. During the 1980s, Schneider emerged as a leading public advocate of sharp reductions of greenhouse gas emissions to combat global warming. In 2006 Professor Schneider was an Adelaide Thinker in Residence advising the South Australian Government of Premier Mike Rann on climate change and renewable energy policies. In ten years South Australia went from zero to 31% of its electricity generation coming from renewables.
An annual award for outstanding climate science communication was created in Schneider's honor after his death.
Early work.
Schneider grew up in Long Island, New York. He studied engineering at Columbia University, receiving his bachelor's degree in mechanical engineering in 1966. In 1971, he earned a Ph.D. in mechanical engineering and plasma physics. Schneider studied the role of greenhouse gases and suspended particulate material on climate as a postdoctoral fellow at NASA's Goddard Institute for Space Studies.
In 1971, Schneider was second author on a "Science" paper with S. I. Rasool titled "Atmospheric Carbon Dioxide and Aerosols: Effects of Large Increases on Global Climate" ("Science" 173, 138–141). This paper used a one-dimensional radiative transfer model to examine the competing effects of cooling from aerosols and warming from CO2. The paper concluded:
However, it is projected that man's potential to pollute will increase 6 to 8-fold in the next 50 years. If this increased rate of injection... should raise the present background opacity by a factor of 4, our calculations suggest a decrease in global temperature by as much as 3.5 "°C". Such a large decrease in the average temperature of Earth, sustained over a period of few years, is believed to be sufficient to trigger an ice age. However, by that time, nuclear power may have largely replaced fossil fuels as a means of energy production.
Carbon dioxide was predicted to have only a minor role. However, the model was very simple and the calculation of the CO2 effect was lower than other estimates by a factor of about three, as noted in a footnote to the paper.
The story made headlines in the "New York Times". Shortly afterwards, Schneider became aware that he had overestimated the cooling effect of aerosols, and underestimated the warming effect of CO2 by a factor of about three. He had mistakenly assumed that measurements of air particles he had taken near the source of pollution applied worldwide. He also found that much of the effect was due to natural aerosols which would not be affected by human activities, so the cooling effect of changes in industrial pollution would be much less than he had calculated. Having found that recalculation showed that global warming was the more likely outcome, he published a retraction of his earlier findings in 1974.
In a 1976 book "The Genesis Strategy" he discusses both long-term warming due to carbon dioxide and short-term cooling due to aerosols, and advocated for adopting policies that are resilient to future changes in climate.
Media contributions.
He was a frequent contributor to commercial and noncommercial print and broadcast media on climate and environmental issues, e.g., NOVA, Planet Earth, Nightline, Today Show, Tonight Show, Bill Maher Show, Good Morning America, Dateline, Discovery Channel, British, Canadian and Australian Broadcasting Corporations.
Schneider has commented about the frustrations and difficulties involved with assessing and communicating scientific ideas.
In a January 2002 "Scientific American" article Schneider wrote:
In 1989, Schneider addressed the challenge scientists face trying to communicate complex, important issues without adequate time during media interviews. This citation sometimes was used by his critics to accuse him of supporting misuse of science for political goals:
Schneider has accused people, including Julian Simon, of deliberately taking this quote out of context in order to misrepresent his views.
Personal.
Schneider was married to the biologist Terry Root. Schneider was a survivor of an aggressive cancer, mantle cell lymphoma. He documented his struggle to conquer the condition, including applying his own knowledge of science to design his own treatment regime, in a self-published 2005 book, "The Patient from Hell". He died unexpectedly on July 19, 2010 after suffering a pulmonary embolism while returning from a scientific meeting in Käringön, Sweden.

</doc>
<doc id="29555" url="http://en.wikipedia.org/wiki?curid=29555" title="List of tourist attractions in Sardinia">
List of tourist attractions in Sardinia

This is a list of the most famous tourist destinations of Sardinia.
Minor islands are included from Olbia, clockwise — industrial sites are not included.

</doc>
<doc id="29556" url="http://en.wikipedia.org/wiki?curid=29556" title="List of Sardinians">
List of Sardinians

This is a list of famous people from or with origins in the island of Sardinia.

</doc>
<doc id="29559" url="http://en.wikipedia.org/wiki?curid=29559" title="Sienna">
Sienna

Sienna, Italian: Terra di Siena, "Siena ground", is an earth pigment containing iron oxide and manganese oxide. In its natural state, it is yellow-brown and is called raw sienna. When heated, it becomes a reddish brown and is called burnt sienna. It takes its name from the city-state of Siena, where it was produced during the Renaissance. Along with ochre and umber, it was one of the first pigments to be used by humans, and is found in many cave paintings. Since the Renaissance, it has been one of the brown pigments most widely used by artists.
The first recorded use of "sienna" as a colour name in English was in 1760.
Sienna and the earth colours.
Like the other earth colours, such as yellow ochre and umber, sienna is a clay containing iron oxide, called limonite, which in its natural state has a yellowish colour. In addition to iron oxide, natural or raw sienna also contains about five percent of manganese oxide, which makes it darker than ochre. When heated, the iron oxide is dehydrated and turns partially to haematite, which gives it a reddish-brown colour. 
Sienna is lighter in shade than raw umber, which is also clay with iron oxide. but which has a higher content of manganese (5 to 20 percent) which makes it greenish brown or dark brown. When heated, raw umber becomes burnt umber, a very dark brown.
History.
The pigment sienna was known and used, in its natural form, by the ancient Romans. It was mined near Arcidosso, formerly under Sienese control, now in the province of Grosseto, on Monte Amiata in southern Tuscany. It was called "terra rossa" (red earth), "terra gialla", or terra di Siena"." During the Renaissance, it was noted by the most widely read author about painting techniques, Giorgio Vasari, under the name terra rossa. It became, along with umber and yellow ochre, one of the standard browns used by artists from the 16th to 19th centuries, including Caravaggio (1571-1610) and Rembrandt (1606-1669), who used all the earth colours, including ochre, sienna and umber, in his palette.
By the 1940s, the traditional sources in Italy were nearly exhausted. Much of today's sienna production is carried out in the Italian islands of Sardinia and Sicily, while other major deposits are found in the Appalachian Mountains, where it is often found alongside the region's iron deposits. It is also still produced in the French Ardennes, in the small town of Bonne Fontaine near Ecordal.
In the 20th century, pigments began to be produced using synthetic iron oxide rather than the natural earth. The labels on paint tubes indicate whether they contain natural or synthetic ingredients. PY-43 indicates natural raw sienna, PR-102 indicates natural burnt sienna.
Variations of sienna.
There is no single agreed standard for the colour of Sienna, and the name is used today for a wide variety of hues and shades. They vary by country and colour list, and there are many proprietary variations offered by paint companies. The colour box at the top of the article shows one variation from the ISCC-NBS colour list.
Raw Sienna.
Raw sienna is a yellowish brown natural earth pigment, composed primarily of iron oxide hydroxide. The box shows the colour of the pigment in its natural, or raw state. It contains a large quantity of iron oxide and a small quantity (about five percent) of manganese oxide.
This kind of pigment is known as yellow ochre, yellow earth, limonite, or terra gialla. The pigment name for natural raw sienna from the Colour Index International, shown on the labels of oil paints, is PY-43.
This box at rights shows a variation of raw sienna from the Italian Ferrario 1919 colour list. 
Burnt sienna.
Burnt sienna contains a large proportion of anhydrous iron oxide. It is made by heating raw sienna, which dehydrates the iron oxide, changing it partially to haematite, giving it rich reddish brown colour.
The pigment is also known as red earth, red ochre, and terra rossa. On the Colour Index International, the pigment is known as PR-102.
This version is from the Italian Ferrario 1919 colour list.
The first recorded use of "burnt sienna" as a colour name in English was in 1853.
Burnt sienna pigment (Maerz and Paul).
This variation of burnt sienna is from the Maerz and Paul "A Dictionary of Color" from 1930. It is considerably lighter than most other versions of burnt sienna. 
Dark sienna (ISCC-NBS).
This infobox shows the colour dark sienna. This variation is from the ISCC-NBS colour list. 
Sienna (X11 colour).
The web colour Sienna is defined by the list of X11 colours used in web browsers and web design.

</doc>
<doc id="29638" url="http://en.wikipedia.org/wiki?curid=29638" title="Sierpinski triangle">
Sierpinski triangle

The Sierpinski triangle (also with the original orthography "Sierpiński"), also called the Sierpinski gasket or the Sierpinski Sieve, is a fractal and attractive fixed set with the overall shape of an equilateral triangle, subdivided recursively into smaller equilateral triangles. Originally constructed as a curve, this is one of the basic examples of self-similar sets, i.e. it is a mathematically generated pattern that can be reproducible at any magnification or reduction. It is named after the Polish mathematician Wacław Sierpiński, but appeared as a decorative pattern many centuries prior to the work of Sierpiński.
Constructions.
There are many different ways of constructing the Sierpinski triangle.
Removing triangles.
The Sierpinski triangle may be constructed from an equilateral triangle by repeated removal of triangular subsets:
Each removed triangle (a "trema") is topologically an open set.
This process of recursively removing triangles is an example of a finite subdivision rule.
Shrinking and duplication.
The same sequence of shapes, converging to the Sierpinski triangle, can alternatively be generated by the following steps:
Note that this infinite process is not dependent upon the starting shape being a triangle—it is just clearer that way. The first few steps starting, for example, from a square also tend towards a Sierpinski triangle. Michael Barnsley used an image of a fish to illustrate this in his paper "V-variable fractals and superfractals."
The actual fractal is what would be obtained after an infinite number of iterations. More formally, one describes it in terms of functions on closed sets of points. If we let formula_1 note the dilation by a factor of ½ about a point a, then the Sierpinski triangle with corners a, b, and c is the fixed set of the transformation formula_1 U formula_3 U formula_4.
This is an attractive fixed set, so that when the operation is applied to any other set repeatedly, the images converge on the Sierpinski triangle. This is what is happening with the triangle above, but any other set would suffice.
Chaos game.
If one takes a point and applies each of the transformations formula_1, formula_3, and formula_4 to it randomly, the resulting points will be dense in the Sierpinski triangle, so the following algorithm will again generate arbitrarily close approximations to it:
Start by labeling p1, p2 and p3 as the corners of the Sierpinski triangle, and a random point v1. Set vn+1 = ½ ( vn + prn ), where rn is a random number 1, 2 or 3. Draw the points v1 to v∞. If the first point v1 was a point on the Sierpiński triangle, then all the points vn lie on the Sierpinski triangle. If the first point v1 to lie within the perimeter of the triangle is not a point on the Sierpinski triangle, none of the points vn will lie on the Sierpinski triangle, however they will converge on the triangle. If v1 is outside the triangle, the only way vn will land on the actual triangle, is if vn is on what would be part of the triangle, if the triangle was infinitely large.
Or more simply:
"Note: This method is also called the chaos game, and is an example of an iterated function system. You can start from any point outside or inside the triangle, and it would eventually form the Sierpinski Gasket with a few leftover points. It is interesting to do this with pencil and paper. A brief outline is formed after placing approximately one hundred points, and detail begins to appear after a few hundred.
Arrowhead curve.
Another construction for the Sierpinski triangle shows that it can be constructed as a curve in the plane. It is formed by a process of repeated modification of simpler curves, analogous to the construction of the Koch snowflake:
The resulting fractal curve is called the Sierpiński arrowhead curve, and its limiting shape is the Sierpinski triangle.
Cellular automata.
The Sierpinski triangle also appears in certain cellular automata (such as Rule 90), including those relating to Conway's Game of Life. For instance, the life-like cellular automaton B1/S12 when applied to a single cell will generate four approximations of the Sierpinski triangle. The time-space diagram of a replicator pattern in a cellular automaton also often resembles a Sierpinski triangle.
Pascal's triangle.
If one takes Pascal's triangle with 2"n" rows and colors the even numbers white, and the odd numbers black, the result is an approximation to the Sierpinski triangle. More precisely, the limit as "n" approaches infinity of this parity-colored 2"n"-row Pascal triangle is the Sierpinski triangle.
Towers of Hanoi.
The Towers of Hanoi puzzle involves moving disks of different sizes between three pegs, maintaining the property that no disk is ever placed on top of a smaller disk. The states of an "n"-disk puzzle, and the allowable moves from one state to another, form an undirected graph that can be represented geometrically as the intersection graph of the set of triangles remaining after the "n"th step in the construction of the Sierpinski triangle. Thus, in the limit as "n" goes to infinity, this sequence of graphs can be interpreted as a discrete analogue of the Sierpinski triangle.
Properties.
For integer number of dimensions "d", when doubling a side of an object, "2" "d" copies of it are created, i.e. 2 copies for 1-dimensional object, 4 copies for 2-dimensional object and 8 copies for 3-dimensional object. For Sierpinski triangle doubling its side creates 3 copies of itself. Thus Sierpinski triangle has Hausdorff dimension log(3)/log(2) ≈ 1.585, which follows from solving "2" "d" = "3" for "d".
The area of a Sierpinski triangle is zero (in Lebesgue measure). The area remaining after each iteration is clearly 3/4 of the area from the previous iteration, and an infinite number of iterations results in zero.
The points of a Sierpinski triangle have a simple characterization in Barycentric coordinates. If a point has coordinates (0."u"1"u"2"u"3…,0."v"1"v"2"v"3…,0."w"1"w"2"w"3…), expressed as Binary numbers, then the point is in Sierpinski's triangle if and only if "u""i"+"v""i"+"w""i"=1 for all "i".
Generalization to other Moduli.
A generalization of the Sierpinski triangle can also be generated using Pascal's Triangle if a different Modulo is used. Iteration n can be generated by taking a Pascal's triangle with P"n" rows and coloring numbers by their value for x mod P. As n approaches infinity, a fractal is generated.
The same fractal can be achieved by dividing a triangle into a tessellation of P2 similar triangles and removing the triangles that are upside-down from the original, then iterating this step with each smaller triangle.
Conversely, the fractal can also be generated by beginning with a triangle and duplicating it and arranging n(n+1)/2 of the new figures in the same orientation into a larger similar triangle with the vertices of the previous figures touching, then iterating that step. 
Analogues in higher dimensions.
The tetrix is the three-dimensional analogue of the Sierpinski triangle, formed by repeatedly shrinking a regular tetrahedron to one half its original height, putting together four copies of this tetrahedron with corners touching, and then repeating the process. This can also be done with a square pyramid and five copies instead.
A tetrix constructed from an initial tetrahedron of side-length L has the property that the total surface area remains constant with each iteration.
The initial surface area of the (iteration-0) tetrahedron of side-length L is formula_8. At the next iteration, the side-length is halved
and there are 4 such smaller tetrahedra. Therefore, the total surface area after the first iteration is:
This remains the case after each iteration. Though the surface area of each subsequent tetrahedron is 1/4 that of the tetrahedron in the previous iteration, there are 4 times as many—thus maintaining a constant total surface area.
The total enclosed volume, however, is geometrically decreasing (factor of 0.5) with each iteration and asymptotically approaches 0 as the number of iterations increases. In fact, it can be shown that, while having fixed area, it has no 3-dimensional character. The Hausdorff dimension of such a construction is formula_11 which agrees with the finite area of the figure. (A Hausdorff dimension strictly between 2 and 3 would indicate 0 volume and infinite area.)
History.
Wacław Sierpiński described the Sierpinski triangle in 1915. However, similar patterns appear already in the 13th-century Cosmati mosaics in the cathedral of Anagni, Italy, and other places of central Italy, for carpets in many places such as the nave of the Roman Basilica of Santa Maria in Cosmedin, and for isolated triangles positioned in rotae in several churches and Basiliche. In the case of the isolated triangle, it is interesting to notice that the iteration is at least of three levels.

</doc>
<doc id="29648" url="http://en.wikipedia.org/wiki?curid=29648" title="Single-lens reflex camera">
Single-lens reflex camera

A single-lens reflex camera (SLR) typically uses a mirror and prism system (hence "reflex", from the mirror's reflection) that permits the photographer to view through the lens and see exactly what will be captured, contrary to viewfinder cameras where the image could be significantly different from what will be captured.
History.
Prior to the development of SLR, all cameras with viewfinders had two optical light paths: one path through the lens to the film, and another path positioned above (TLR or "twin-lens reflex") or to the side (rangefinder). Because the viewfinder and the film lens cannot share the same optical path, the viewing lens is aimed to intersect with the film lens at a fixed point somewhere in front of the camera. This is not problematic for pictures taken at a middle or longer distance, but parallax causes framing errors in close-up shots. Moreover, focusing the lens of a fast reflex camera when it is opened to wider apertures (such as in low light or while using low-speed film) is not easy.
Most SLR cameras permit upright and laterally correct viewing through use of a roof pentaprism situated in the optical path between the reflex mirror and viewfinder. Light, which comes both horizontally and vertically inverted after passing through the lens, is reflected upwards by the reflex mirror, into the pentaprism where it is reflected several times to correct the inversions caused by the lens, and align the image with the viewfinder. When the shutter is released, the mirror moves out of the light path, and the light shines directly onto the film (or in the case of a DSLR, the CCD or CMOS imaging sensor). The Canon Pellix film camera was an exception to the moving mirror system, wherein the mirror was a fixed beamsplitting pellicle.
Focus can be adjusted manually by the photographer or automatically by an autofocus system. The viewfinder can include a matte focusing screen located just above the mirror system to diffuse the light. This permits accurate viewing, composing and focusing, especially useful with interchangeable lenses.
Up until the 1990s, SLR was the most advanced photographic preview system available, but the recent development and refinement of digital imaging technology with an on-camera live LCD preview screen has overshadowed SLR's popularity. Nearly all inexpensive compact digital cameras now include an LCD preview screen allowing the photographer to see what the CCD is capturing. However, SLR is still popular in high-end and professional cameras because they are system cameras with interchangeable parts, allowing customization. They also have far less shutter lag, allowing photographs to be timed more precisely. Also the pixel resolution, contrast ratio, refresh rate, and color gamut of an LCD preview screen cannot compete with the clarity and shadow detail of a direct-viewed optical SLR viewfinder.
Large format SLR cameras were probably first marketed with the introduction of C.R. Smith's "Monocular Duplex" (USA, 1884). SLRs for smaller exposure formats were launched in the 1920s by several camera makers. The first 35mm SLR available to the mass market, Leica's PLOOT reflex housing along with a 200mm f4.5 lens paired to a 35mm rangefinder camera body, debuted in 1935. The Soviet "Спорт" (“Sport”), also a 24mm by 36mm image size, was prototyped in 1934 and went to market in 1937. K. Nüchterlein's Kine Exakta (Germany, 1936) was the first integrated 35mm SLR to enter the market. Additional Exakta models, all with waist-level finders, were produced up to and during World War II. Another ancestor of the modern SLR camera was the Swiss-made Alpa, which was innovative, and influenced the later Japanese cameras. The first eye-level SLR viewfinder was patented in Hungary on August 23, 1943 by Jenő Dulovits, who then designed the first 35 mm camera with one, the Duflex, which used a system of mirrors to provide a laterally correct, upright image in the eye-level viewfinder. The Duflex, which went into serial production in 1948, was also the world's first SLR with an instant-return (a.k.a. autoreturn) mirror.
The first commercially produced SLR that employed a roof pentaprism was the Italian Rectaflex A.1000, shown in full working condition on Milan fair April 1948 and produced from September the same year, thus being on the market one year before the east German Zeiss Ikon VEB Contax S, announced on May 20, 1949, produced from September.
The Japanese adopted and further developed the SLR. In 1952, Asahi developed the Asahiflex and in 1954, the Asahiflex IIB. In 1957, the Asahi Pentax combined the fixed pentaprism and the right-hand thumb wind lever. Nikon, Canon and Yashica introduced their first SLRs in 1959 (the F, Canonflex, and Pentamatic, respectively).
Through-the-lens light metering.
As a small matter of history, the first 35 mm camera (non-SLR) to feature through the lens light metering may have been Nikon, with a prototype rangefinder camera, the SPX. According to the website below, the camera used Nikon 'S' type rangefinder lenses.
Through-the-lens light metering is also known as "behind-the-lens metering". In the SLR design scheme, there were various placements made for the metering cells, all of which used CdS (Cadmium sulfide) photocells. The cells were either located in the pentaprism housing, where they metered light transmitted through the focusing screen; underneath the reflex mirror glass itself, which was Topcon's design; or in front of the shutter mechanism, which was the design used by Canon with their Canon Pellix.
Pentax was the first manufacturer to show an early prototype 35 mm behind-the-lens metering SLR camera, which was named the "Pentax Spotmatic". The camera was shown at the 1960 Photokina show. However, the first Through-the-lens (TTL) light metering SLR on the market was the 1963 Topcon RE Super, which had the CdS metering cell placed behind the reflex mirror. The mirror had narrow slits cut into the surface to let the light reach the cell providing average metering. Late in the following year, a production model of the Pentax Spotmatic was shown whose CdS light meter cells were on the pentaprism, reading the light off the focusing screen providing average reading, yet keeping the Spotmatic name, but now written in one word. Another clever design appeared in 1965, the Canon Pellix employing a pellicle mirror that is semi-transparent, placing the meter cell on an arm swinging into the lightpass behind the mirror for meter reading.
Mamiya Sekor came out with cameras such as the Mamiya Sekor TL and various other versions. Yashica introduced the TL Super. Both of these cameras used M42 screw thread lenses as did the Pentax Spotmatic. Later on Fujica introduced their ST-701, then ST-801 and ST-901 cameras. The ST-701 was the first SLR to use a silicon cell photodiode, which was more sensitive than CdS and was immune to the memory effect that the CdS cell suffered from in bright sunlight. Gradually, other 35 mm SLR camera manufacturers changed their behind-the-lens meters from CdS cells to Silicon Diode photocells.
Other manufacturers responded and introduced their own behind-the-lens metering cameras. Nikon and Miranda, at first, simply upgraded their interchangeable pentaprisms to include behind-the-lens metering (for Nikon F, and Miranda D, F, Fv and G models) and these manufacturers also bought out other camera models with built-in behind-the-lens metering capability, such as the Nikkormat FT and the Miranda Sensorex (which used an external coupling diaphragm). Minolta introduced the SRT-101, which used Minolta's proprietary system they referred to as "CLC", which was an acronym for "contrast light compensation", which metered differently from an average metering behind-the-lens camera.
Some German manufacturers also introduced cameras such as the Zeiss Ikon Contarex family, which was one of very few 35 mm SLR to use interchangeable film backs.
Inexpensive leaf-shutter cameras also benefited from behind-the-lens metering as, Topcon introduced the Auto 100 with front-mount interchangeable lenses designed only for that camera, and one of the Zeiss Ikon Contaflex leaf shutter cameras. Kowa manufactured their SET-R, which had similar specifications.
Within months, manufacturers decided to bring out models that provided limited area metering, such as Nikon's Photomic Tn finder, which concentrated 60% of the CdS cells sensitivity on the inner circle of the focusing screen and 30% on the surrounding area. Canon used spot metering in the unusual Canon Pellix camera, which also had a stationary mirror system that allowed approximately 70% of the light to travel to the film plane and 30% to the photographer's eye. This system, unfortunately, degraded the native resolution of the attached lens and provided less illumination to the eyepiece. It did have the advantage of having less vibration than other SLR cameras but this was not sufficient to attract professionals to the camera in numbers.
Semi-automatic exposure capabilities.
While auto-exposure was commonly used in the early 1960s with various 35 mm fixed lens rangefinder cameras such as the Konica Auto 'S', and other cameras such as the Polaroid Land cameras whose early models used selenium cell meters, auto-exposure for interchangeable lens SLRs was a feature that was largely absent, except for a few early leaf-shutter SLRs such as the Kowa SE-R and Topcon Auto 100.
The types of automation found in some of these cameras consisted of the simple programmed shutter, whereby the camera's metering system would select a mechanically set series of apertures with shutter speeds, one setting of which would be sufficient for the correct exposure. In the case of the above-mentioned Kowa and Topcon, automation was semi-automatic, where the camera's CDs meter would select the correct aperture only.
Autoexposure, technically known as semi-automatic exposure, where the camera's metering system chooses either the shutter speed or the aperture, was finally introduced by the Savoyflex and popularized by Konishiroku in the 1965 Konica Auto-Reflex. This camera was of the 'shutter-priority' type automation, which meant that the camera selected the correct aperture automatically. This model also had the interesting ability to photograph in 35 mm full-frames or half-frames, all selected by a lever.
Other SLRs soon followed, but because of limitations with their lens mounts, the manufacturers of these cameras had to choose 'aperture-priority' automation, where the camera's metering system selects the correct shutter speed. As one example, Pentax introduced the Electro Spotmatic, which was able to use the then considerable bulk of 42 mm screw-mount lenses produced by various manufacturers. Yashica, another screw-mount camera manufacturer, soon followed.
Canon, which produced the FD lens mount (known as the breech-mount; a unique lens mounting system that combines the advantages of screw-mount and bayonet-mount) introduced their shutter priority 35 mm SLR, the Canon EF in 1976 or so. This camera's build quality was almost the equal of their flagship camera, the Canon F1, and featured a copal-square vertically travelling focal plane shutter that could synchronize electronic flash at shutter speeds up to and including 1/125 of a second, thus making this a good second-body camera for the professional photographer.
Nikon at first produced an aperture-priority camera, but later made subtle changes on the inside of their bayonet mount, which allowed for shutter-priority automation without obsoleting the photographers lenses.
Full-program auto-exposure.
Full-program auto-exposure soon followed with the advent of the Canon A-1 in 1978. This SLR had a 'P' mode on the shutter speed dial, and a lock on the aperture ring to allow the lens to be put on 'Auto' mode. Other manufacturers soon followed with Nikon introducing the FA, Minolta introducing the X-700 in 1981, and Pentax introducing the Super Program. Olympus, however, continued with 'aperture-priority' automation in their OM system line.
The 1970s and 1980s saw steadily increasing use of electronics, automation, and miniaturization, including integrated motor driven film advance with the Konica FS-1 in 1979, and motor rewind functions.
Autofocus.
The first autofocus 35 mm SLR was the Pentax ME-F released in 1981.
The Minolta Maxxum 7000, released in 1985, was the first 35 mm SLR with integrated autofocus and motorized film-advance winder, which became the standard configuration for SLR cameras from then on. This development had significant impact on the photographic industry.
Some manufacturers discarded their existing lens systems to compete with other manufacturer's autofocus capability in their new cameras. This was the case for Canon, with its new EOS lens line. Other manufacturers chose to adapt their existing lens systems for autofocus capability, as was the case with Nikon and Pentax. This allowed photographers to continue using their existing lenses, which greatly reduced the cost of upgrading. For example, almost all Nikon lenses from the 1960s and later still function on the current Nikon bodies, only lacking autofocus. Still some manufacturers, notably Leica with its R-system lenses, and Contax with its Zeiss lenses, decided to keep their lens mounts non-autofocus.
From the late 1980s competition and technical innovations made 35 mm camera systems more versatile and sophisticated by adding more advanced light metering capabilities such as spot-metering; limited area metering such as used by Canon with the F1 series; matrix metering as used by Nikon, exposure communication with dedicated electronic flash units. The user interface also changed on many cameras, replacing meter needle displays that were galvanometer-based and thereby fragile, with light-emitting diodes (LEDs) and then with more comprehensive liquid crystal displays (LCDs) both in the SLR viewfinder and externally on the cameras' top plate using an LCD screen. Wheels and buttons replaced the shutter dial on the camera and the aperture ring on the lens on many models, although some photographers still prefer shutter dials and aperture rings. Some manufacturers introduced image stabilization on certain lenses to combat camera shake and to allow longer hand-held exposures without using a tripod. This feature is especially useful with long telephoto lenses.
Digital SLRs.
Canon, Nikon and Pentax have all developed digital SLR cameras (DSLRs) using the same lens mounts as on their respective film SLR cameras. Konica Minolta did the same, but in 2006 sold their camera technology to Sony, who now builds DSLRs based on the Minolta lens mount. Samsung builds DSLRs based on the Pentax lens mount. Olympus, on the other hand, chose to create a new digital-only Four Thirds System SLR standard, adopted later by Panasonic and Leica.
Contax came out with a DSLR model, the Contax N-Digital. This model was too late and too expensive to be competitive with other camera manufacturers. The Contax N-digital was the last Contax to use that maker's lens system, and the camera, while having impressive features such as a full-frame sensor, was expensive and lacked sufficient write-speed to the memory card for it to be seriously considered by some professional photographers.
The digital single-lens reflex camera have largely replaced film SLR's design in convenience, sales and popularity at the start of 21st century.
Optical components.
A cross-section ("or" 'side-view') of the optical components of a typical SLR camera shows how the light passes through the lens assembly (1), is reflected by the mirror (2) placed at a 45-degree angle, and is projected on the matte focusing screen (5). Via a condensing lens (6) and internal reflections in the roof pentaprism (7) the image appears in the eyepiece (8). When an image is taken, the mirror moves upwards from its resting position in the direction of the arrow, the focal plane shutter (3) opens, and the image is projected onto the film or sensor (4) in exactly the same manner as on the focusing screen.
This feature distinguishes SLRs from other cameras as the photographer sees the image composed exactly as it will be captured on the film or sensor (see Advantages below).
Pentaprisms and penta-mirrors.
Most 35 mm SLRs use a roof pentaprism or penta-mirror to direct the light to the eyepiece, first used on the 1948 Duflex constructed by Jenő Dulovits and patented August 1943 (Hungary). With this camera also appeared the first Instant-return mirror.
The first Japanese pentaprism SLR was the 1955 Miranda T, followed by the Asahi Pentax, Minolta SR-2, Zunow, Nikon F and the Yashica Pentamatic. Some SLRs offered removable pentaprisms with optional viewfinder capabilities, such as the waist-level finder, the interchangeable sports finders used on the Canon F1 and F1n; the Nikon F, F2, F3, F4 and F5; and the Pentax LX.
Another prism design was the porro prism system used in the Olympus Pen F, the Pen FT, the Pen FV half-frame 35 mm SLR cameras. This was later used on the Olympus EVOLT E-3x0 series, the Leica Digilux 3 and the Panasonic DMC-L1.
A right-angle finder is available that slips onto the eyepiece of most SLRs and D-SLRs and allows viewing through a waist-level viewfinder. There is also a finder that provides EVF remote capability.
Shutter mechanisms.
Focal-plane shutters.
Almost all contemporary SLRs use a focal-plane shutter located in front of the film plane, which prevents the light from reaching the film even if the lens is removed, except when the shutter is actually released during the exposure. There are various designs for focal plane shutters. Early focal-plane shutters designed from the 1930s onwards usually consisted of two curtains that travelled horizontally across the film gate: an opening shutter curtain followed by a closing shutter curtain. During fast shutter speeds, the focal-plane shutter would form a 'slit' whereby the second shutter curtain was closely following the first opening shutter curtain to produce a narrow, vertical opening, with the shutter slit moving horizontally. The slit would get narrower as shutter speeds were increased. Initially these shutters were made from a cloth material (which was in later years often rubberised), but some manufacturers used other materials instead. Nippon Kōgaku (now Nikon Corporation), for example, used titanium foil shutters for several of their flagship SLR cameras, including the Nikon F, F2, and F3.
Other focal-plane shutter designs, such as the Copal Square, travelled vertically — the shorter travelling distance of 24 millimetres (as opposed to 36 mm horizontally) meant that minimum exposure and flash synchronisation times could be reduced. These shutters are usually manufactured from metal, and use the same moving-slit principle as horizontally travelling shutters. They differ, though, in usually being formed of several slats or blades, rather than single curtains as with horizontal designs, as there is rarely enough room above and below the frame for a one-piece shutter. Vertical shutters became very common in the 1980s (though Konica, Mamiya, and Copal first pioneered their use in the 1950s and 1960s, and are almost exclusively used for new cameras. Nikon used Copal-made vertical plane shutters in their Nikomat/Nikkormat -range, enabling x-sync speeds from 1⁄30 to 1⁄125 while the only choice for focal plane shutters at that time was 1⁄60. Later, Nikon again pioneered the use of titanium for vertical shutters, using a special honeycomb pattern on the blades to reduce their weight and achieve world-record speeds in 1982 of 1⁄4000 second for non-sync shooting, and 1⁄250 with x-sync. Nowadays most such shutters are manufactured from cheaper aluminium (though some high-end cameras use materials such as carbon-fibre and Kevlar).
Rotary focal-plane shutter.
One unusual design, the Olympus Pen half-frame 35 mm SLR system, manufactured by Olympus in Japan, used a rotary focal-plane shutter mechanism that was extremely simple and elegant in design. This shutter used titanium foil but consisted of one piece of metal with a fixed opening, which allowed electronic flash synchronisation up to and including its maximum speed of 1/500 of a second – rivalling the capabilities of leaf-shutter systems
Another 35 mm camera system that used a rotary shutter, was the Robot Royal cameras, most of which were rangefinder 35 mm cameras. Some of these cameras were full-frame; some were half-frame, and at least one Robot camera produced an unusual square-sized image on the 35 mm frame.
The Mercury II, produced in 1946, also used a rotary shutter. This was a half-frame 35mm camera.
Leaf shutters.
Another shutter system is the leaf shutter, whereby the shutter is constructed of diaphragm-like blades and can be situated either between the lens or behind the lens. If the shutter is part of a lens assembly some other mechanism is required to ensure that no light reaches the film between exposures.
An example of a behind-the-lens leaf shutter is found in the 35 mm SLRs produced by Kodak, with their Retina Reflex camera line; Topcon, with their Auto 100; and Kowa with their SE-R and SET-R reflexes.
A primary example of a medium-format SLR with a between-the-lens leaf shutter system would be Hasselblad, with their 500C, 500CM, 500 EL-M (a motorized Hasselblad) and other models (producing a 6 cm square negative). Hasselblads use an auxiliary shutter blind situated behind the lens mount and the mirror system to prevent the fogging of film.
Other medium-format SLRs also using leaf shutters include the now discontinued Zenza-Bronica camera system lines such as the Bronica ETRs, the ETRs'i (both producing a 6 × 4.5 cm. image), the SQ and the SQ-AI (producing a 6 × 6 cm image like the Hasselblad), and the Zenza-Bronica G system (6 × 7 cm). Certain Mamiya medium-format SLRs, discontinued camera systems such as the Kowa 6 and a few other camera models also used between-the-lens leaf shutters in their lens systems.
Thus, any time a photographer purchased one of these lenses, that lens included a leaf shutter in its lens mount.
Because leaf shutters synchronized electronic flash at all shutter speeds especially at fast shutter speeds of 1⁄500 of a second or faster, cameras using leaf shutters were more desirable to studio photographers who used sophisticated studio electronic flash systems.
Some manufacturers of medium-format 120 film SLR cameras also made leaf-shutter lenses for their focal-plane-shutter models. Rollei made at least two such lenses for their Rolleiflex SL-66 medium format which was a focal-plane shutter SLR. Rollei later switched to a camera system of leaf-shutter design (e.g., the 6006 and 6008 reflexes) and their current medium-format SLRs are now all of the between-the-lens shutter design.
Further developments.
Since the technology became widespread in the 1970s, SLRs have become the main photographic instrument used by dedicated amateur photographers and professionals. Some photographers of static subjects (such as architecture, landscape, and some commercial subjects), however, prefer view cameras because of the capability to control perspective. With a triple-extension bellows 4" × 5" camera such as the Linhof SuperTechnika V, the photographer can correct certain distortions such as "keystoning", where the image 'lines' converge (i.e., photographing a building by pointing a typical camera upward to include the top of the building). Perspective correction lenses are available in the 35 mm and medium formats to correct this distortion with film cameras, and it can also be corrected after the fact with photo software when using digital cameras. The photographer can also extend the bellows to its full length, tilt the front standard and perform photomacrography (commonly known as 'macro photography'), producing a sharp image with depth-of-field without stopping down the lens diaphram.
Film formats.
Early SLRs were built for large format photography, but this film format has largely lost favor among professional photographers. SLR film-based cameras have been produced for most film formats as well as for digital formats. These film-based SLRs use the 35 mm format as, this film format offers a variety of emulsions and film sensitivity speeds, usable image quality and a good market cost. 35 mm film comes in a variety of exposure lengths: 20 exposure, 24 exposure and 36 exposure rolls. Medium format SLRs provide a higher-quality image with a negative that can be more easily retouched than the smaller 35 mm negative, when this capability is required.
A small number of SLRs were built for APS such as the Canon IX series and the Nikon Pronea cameras. SLRs were also introduced for film formats as small as Kodak's 110, such as the Pentax Auto 110, which had interchangeable lenses.
Common features.
Other features found on many SLR cameras include through-the-lens (TTL) metering and sophisticated flash control referred to as "dedicated electronic flash". In a dedicated system, once the dedicated electronic flash is inserted into the camera's hot shoe and turned on, there is then communication between camera and flash. The camera's synchronization speed is set, along with the aperture. Many camera models measure the light that reflects off of the film plane, which controls the flash duration of the electronic flash. This is denoted TTL flash metering.
Some electronic flash units can send out several short bursts of light to aid the autofocus system or for wireless communication with off-camera flash units. A pre-flash is often used to determine the amount of light that is reflected from the subject, which sets the duration of the main flash at time of exposure. Some cameras also employ automatic fill-flash, where the flash light and the available light are balanced. While these capabilities are not unique to the SLR, manufacturers included them early on in the top models, whereas the best rangefinder cameras adopted such features later.
Advantages.
Many of the advantages of SLR cameras derive from viewing and focusing the image through the attached lens. Most other types of cameras do not have this function; subjects are seen through a viewfinder that is near the lens, making the photographer's view different from that of the lens. SLR cameras provide photographers with precision; they provide a viewing image that will be exposed onto the negative exactly as it is seen through the lens. There is no parallax error, and exact focus can be confirmed by eye—especially in macro photography and when photographing using long focus lenses. The depth of field may be seen by stopping down to the attached lens aperture, which is possible on most SLR cameras except for the least expensive models. Because of the SLR's versatility, most manufacturers have a vast range of lenses and accessories available for them.
Compared to most fixed-lens compact cameras, the most commonly used and inexpensive SLR lenses offer a wider aperture range and larger maximum aperture (typically f/1.4 to f/1.8 for a 50 mm lens). This allows photographs to be taken in lower light conditions without flash, and allows a narrower depth of field, which is useful for blurring the background behind the subject, making the subject more prominent. "Fast" lenses are commonly used in theater photography, portrait photography, surveillance photography, and all other photography requiring a large maximum aperture.
The variety of lenses also allows for the camera to be used and adapted in many different situations. This provides the photographer with considerably more control (i.e., how the image is viewed and framed) than would be the case with a view camera. In addition, some SLR lenses are manufactured with extremely long focal lengths, allowing a photographer to be a considerable distance away from the subject and yet still expose a sharp, focused image. This is particularly useful if the subject includes dangerous animals (e.g., wildlife); the subject prefers anonymity to being photographed; or else, the photographer's presence is unwanted (e.g., celebrity photography or surveillance photography). Practically all SLR and DSLR camera bodies can also be attached to telescopes and microscopes via an adapter tube to further enhance their imaging capabilities.
Disadvantages.
In most cases, single-lens reflex cameras cannot be made as small or as light as other camera designs—such as rangefinder cameras, autofocus compact cameras and digital cameras with electronic viewfinders (EVF)—owing to the mirror box and pentaprism/pentamirror. The mirror box also prevents lenses with deeply recessed rear elements from being mounted close to the film or sensor unless the camera has a mirror lockup feature; this means that simple designs for wide angle lenses cannot be used. Instead, larger and more complex retrofocus designs are required.
The SLR mirror 'blacks-out' the viewfinder image during the exposure. In addition, the movement of the reflex mirror takes time, limiting the maximum shooting speed. The mirror system can also cause noise and vibration. Partially reflective (pellicle) fixed mirrors avoid these problems and have been used in a very few designs including the Canon Pellix and the Canon EOS-1N RS, but these designs introduce their own problems. These pellicle mirrors reduce the amount of light travelling to the film plane or sensor and also can distort the light passing through them, resulting in a less-sharp image. To avoid the noise and vibration, many professional cameras offer a mirror lock-up feature, however, this feature totally disables the SLR's automatic focusing ability. Electronic viewfinders have the potential to give the 'viewing-experience' of a DSLR (through-the-lens viewing) without many of the disadvantages. More recently, Sony have resurrected the pellicle mirror concept in their "single-lens translucent" (SLT) range of cameras.
Reliability.
SLRs vary widely in their construction and typically have bodies made of plastic or magnesium. Most manufacturers do not cite durability specifications, but some report shutter life expectancies for professional models. For instance, the Canon EOS 1Ds MkII is rated for 200,000 shutter cycles and the newer Nikon D3 is rated for 300,000 with its exotic carbon fiber/kevlar shutter. Because many SLRs have interchangeable lenses, there is a tendency for dust, sand and dirt to get into the main body of the camera through the mirror box when the lens is removed, thus dirtying or even jamming the mirror movement mechanism or the shutter curtain mechanism itself. In addition, these particles can also jam or otherwise hinder the focusing feature of a lens if they enter into the focusing helicoid. The problem of sensor cleaning has been somewhat reduced in DSLRs as some cameras have a built-in sensor cleaning unit.
Price and affordability.
The price of SLRs in general also tends to be somewhat higher than that of other types of cameras, owing to their internal complexity. This is compounded by the expense of additional components, such as flashes or lenses. The initial investment in equipment can be prohibitive enough to keep some casual photographers away from SLRs, although the market for used SLRs has become larger particularly as photographers migrate to digital systems.
Future of SLRs.
The digital single-lens reflex camera have largely replaced film SLR's design in convenience, sales and popularity at the start of 21st century. These cameras are currently the marketing 'favorite' among advanced amateur and professional photographers. Film based SLR's are still used by a niche market of enthusiasts and format lovers.

</doc>
<doc id="29660" url="http://en.wikipedia.org/wiki?curid=29660" title="State terrorism">
State terrorism

State terrorism refers to acts of terrorism conducted by a state against a foreign state or people, or against its own people.
Definition.
There is neither an academic nor an international legal consensus regarding the proper definition of the word "terrorism". Many scholars believe that the actions of governments can be labeled "terrorism". For example, using the term 'terrorism' to mean violent action used with the predominant intention of causing terror, Paul James and Jonathan Friedman distinguish between state terrorism against non-combatants and state terrorism against combatants, including 'Shock and Awe' tactics:
However, others, including governments, international organizations, private institutions and scholars, believe that the term is only applicable to the actions of violent non-state actors. Historically, the term terrorism was used to refer to actions taken by governments against their own citizens whereas now it is more often perceived as targeting of non-combatants as part of a strategy directed "against" governments.
Historian Henry Commager wrote that "Even when definitions of terrorism allow for "state terrorism", state actions in this area tend to be seen through the prism of war or national self-defense, not terror.” While states may accuse other states of state-sponsored terrorism when they support insurgencies, individuals who accuse their governments of terrorism are seen as radicals, because actions by legitimate governments are not generally seen as illegitimate. Academic writing tends to follow the definitions accepted by states. Most states use the term "terrorism" for non-state actors only.
The Encyclopædia Britannica Online defines terrorism generally as "the systematic use of violence to create a general climate of fear in a population and thereby to bring about a particular political objective", and states that "terrorism is not legally defined in all jurisdictions." The encyclopedia adds that "[e]stablishment terrorism, often called state or state-sponsored terrorism, is employed by governments -- or more often by factions within governments -- against that government's citizens, against factions within the government, or against foreign governments or groups."
While the most common modern usage of the word terrorism refers to civilian-victimizing political violence by insurgents or conspirators, several scholars make a broader interpretation of the nature of terrorism that encompasses the concepts of state terrorism and state-sponsored terrorism. Michael Stohl argues, "The use of terror tactics is common in international relations and the state has been and remains a more likely employer of terrorism within the international system than insurgents. Stohl clarifies, however, that "[n]ot all acts of state violence are terrorism. It is important to understand that in terrorism the violence threatened or perpetrated, has purposes broader than simple physical harm to a victim. The audience of the act or threat of violence is more important than the immediate victim."
Scholar Gus Martin describes state terrorism as terrorism "committed by governments and quasi-governmental agencies and personnel against perceived threats", which can be directed against both domestic and foreign targets. Noam Chomsky defines state terrorism as "terrorism practised by states (or governments) and their agents and allies". Jeffrey A. Sluka has described Noam Chomsky and Edward S. Herman as pioneers in academic studies about state terrorism.
Stohl and George A. Lopez have designated three categories of state terrorism, based on the openness/secrecy with which the alleged terrorist acts are performed, and whether states directly perform the acts, support them, or acquiesce to them.
History.
Aristotle wrote critically of terror employed by tyrants against their subjects. The earliest use of the word "terrorism" identified by the "Oxford English Dictionary" is a 1795 reference to tyrannical state behavior, the "reign of terrorism" in France. In that same year, Edmund Burke famously decried the "thousands of those hell-hounds called terrorists" who he believed threatened Europe. During the Reign of Terror, the Jacobin government and other factions of the French Revolution used the apparatus of the state to kill and intimidate political opponents, and the Oxford English Dictionary includes as one definition of terrorism "Government by intimidation carried out by the party in power in France between 1789-1794". The original general meaning of terrorism was of terrorism by the state, as reflected in the 1798 supplement of the Dictionnaire of the Académie française, which described terrorism as "systeme", "regime de la terreur". Dr. Myra Williamson wrote: "The meaning of “terrorism” has undergone a transformation. During the reign of terror a regime or system of terrorism was used as an instrument of governance, wielded by a recently established revolutionary state against the enemies of the people. Now the term “terrorism" is commonly used to describe terrorist acts committed by "non-state or subnational entities" against a state." (italics in original)
Later examples of state terrorism include the police state measures employed by the Soviet Union beginning in the 1930s, and by Germany's Nazi regime in the 1930s and 1940s. According to Igor Primoratz, "Both [the Nazis and Soviets] sought to impose total political control on society. Such a radical aim could only be pursued by a similarly radical method: by terrorism directed by an extremely powerful political police at an atomized and defenseless population. Its success was due largely to its arbitrary character — to the unpredictability of its choice of victims. In both countries, the regime first suppressed all opposition; when it no longer had any opposition to speak of, political police took to persecuting “potential” and “objective opponents”. In the Soviet Union, it was eventually unleashed on victims chosen at random." 
Military actions primarily directed against non-combatant targets have also been referred to as state terrorism. For example, the bombing of Guernica has been called an act of terrorism, as well as the surprise attack on Pearl Harbor. Other examples of state terrorism may include the World War II bombings of London, Dresden, Chongqing, and Hiroshima.
Sometimes regarded as an act of terrorism was the peace-time sinking of the Rainbow Warrior, a ship owned by Greenpeace, which occurred while in port at Auckland, New Zealand on July 10, 1985. The bomb detonation killed Fernando Pereira, a Portuguese photographer. The organization who committed the attack, the DGSE, is a branch of France's intelligence services. Ironically, France and New Zealand had been allies since French missionaries settled in Akaroa, in 1835. The agents responsible pled guilty to manslaughter as part of a plea deal and were sentenced to ten years in prison, but were secretly released early to France under an agreement between the two countries' governments.
Another example is the British Military Reaction Force in Northern Ireland during the 1970s which murdered innocent civilians from the Catholic community in order to stir up ethnic hatred and "take the heat off the army". In 2012–13, a former MRF member using the covername 'Simon Cursey' gave a number of interviews and published the book MRF Shadow Troop, about his time in the unit: "We opened fire at any small group in hard areas [...] armed or not – it didn't matter. We targeted specific groups that were always up to no good. These types were sympathisers and supporters, assisting the IRA movement. As far as we were concerned they were guilty by association and party to terrorist activities, leaving themselves wide open to the ultimate punishment from us".
In November 2013, a BBC Panorama documentary was aired about the MRF. It drew on information from seven former members, as well as a number of other sources: Soldier H said "We operated initially with them thinking that we were the UVF", to which Soldier F added: "We wanted to cause confusion". In June 1972, he was succeeded as commander by Captain James 'Hamish' McGregor.
In June 2014, the wake of the Panorama program the Police Service of Northern Ireland (PSNI) opened an investigation into the matter. In an earlier review of the program the position of the PSNI was that none of the statements by soldiers in the programme could be taken as an admission of criminality.
Arguments that terrorism is not committed by states.
Discussions of terrorism in social sciences and philosophy tend to apply to violent non-state actors.
The Chairman of the United Nations Counter-Terrorism Committee has stated that the twelve previous international conventions on terrorism had never referred to state terrorism, which was not an international legal concept, and that when states abuse their powers they should be judged against international conventions dealing with war crimes, international human rights law, and international humanitarian law, rather than against international anti-terrorism statutes. In a similar vein, Kofi Annan, at the time United Nations Secretary-General, stated that it is "time to set aside debates on so-called 'state terrorism'. The use of force by states is already regulated under international law". Annan added, "...regardless of the differences between governments on the question of definition of terrorism, what is clear and what we can all agree on is any deliberate attack on innocent civilians [or non-combatants], regardless of one's cause, is unacceptable and fits into the definition of terrorism."
Dr. Bruce Hoffman has argued that failing to differentiate between state and non-state violence ignores the fact that there is a “fundamental qualitative difference between the two types of violence.” Hoffman argues that even in war, there are rules and accepted norms of behavior that prohibit certain types of weapons and tactics and outlaw attacks on specific categories of targets. For instance, rules codified in the Geneva and Hague Conventions on warfare prohibit taking civilians as hostages, outlaw reprisals against either civilians or POWs, recognize neutral territory, etc. Hoffman states that “even the most cursory review of terrorist tactics and targets over the past quarter century reveals that terrorists have violated all these rules.” Hoffman also states that when states transgress these rules of war “the term “war crime” is used to describe such acts.”
Walter Laqueur has stated that those who argue that state terrorism should be included in studies of terrorism ignore the fact that “The very existence of a state is based on its monopoly of power. If it were different, states would not have the right, nor be in a position, to maintain that minimum of order on which all civilized life rests.” Calling the concept a “red herring” he stated: “This argument has been used by the terrorists themselves, arguing that there is no difference between their activities and those by governments and states. It has also been employed by some sympathizers, and rests on the deliberate obfuscation between all kinds of violence...”
External links.
Prevention of terrorism

</doc>
<doc id="29666" url="http://en.wikipedia.org/wiki?curid=29666" title="Syringomyelia">
Syringomyelia

Syringomyelia is a generic term referring to a disorder in which a cyst or cavity forms within the spinal cord. This cyst, called a syrinx, can expand and elongate over time, destroying the spinal cord. The damage may result in pain, paralysis, weakness, and stiffness in the back, shoulders, and extremities. Syringomyelia may also cause a loss of the ability to feel extremes of hot or cold, especially in the hands. The disorder generally leads to a cape-like loss of pain and temperature sensation along the back and arms. Each patient experiences a different combination of symptoms. These symptoms typically vary depending on the extent and, often more critically, to the location of the syrinx within the spinal cord.
Syringomyelia has a prevalence estimated at 8.4 cases per 100,000 people, with symptoms usually beginning in young adulthood. Signs of the disorder tend to develop slowly, although sudden onset may occur with coughing, straining, or myelopathy.
Pathogenesis.
The pathogenesis of syringomyelia is debated.The cerebrospinal fluid also serves to cushion the brain. Excess cerebrospinal fluid in the central canal of the spinal cord is called hydromyelia. This term refers to increased cerebrospinal fluid that is contained within the ependyma of the central canal. When fluid dissects into the surrounding white matter forming a cystic cavity or syrinx, the term syringomyelia is applied. As these conditions coexist in the majority of cases, the term syringohydromyelia is applied. The terms are used interchangeably.
It has been observed that obstruction of the cerebrospinal fluid spaces in the subarachnoid space can result in syrinx formation, and alleviation of the obstruction may improve symptoms. A number of pathological conditions can cause an obstruction of the normal cerebrospinal fluid spaces. These include Chiari malformation, spinal arachnoiditis, scoliosis, spinal vertebrae misalignment, spinal tumors, spina bifida, and others. The reasons that blockage of the cerebrospinal fluid space within the subarachnoid space can result in syrinx formation are not fully understood although a small posterior fossa is one known cause. It is unclear if syrinx fluid originates from bulk movement of cerebrospinal fluid into the spinal cord, from bulk transmural movement of blood fluids through the spinal vasculature into the syrinx, or from a combination of both. Once a syrinx has formed, pressure differences along the spine have been proposed to be one mechanism causing fluid movement within the cyst, possibly resulting in damage to the spinal cord. Recent work suggests that central nervous system compliance is the underlying problem for the central nervous system, and also that hydrocephalus and syringomyelia have related causes.
Different origins.
Generally, there are two forms of syringomyelia: congenital and acquired. (In addition, one form of the disorder involves the brainstem. The brainstem controls many of our vital functions, such as respiration and heartbeat. When syrinxes affect the brainstem, the condition is called syringobulbia.)
Congenital.
The first major form relates to an abnormality of the brain called an Arnold–Chiari malformation. This is the most common cause of syringomyelia, where the anatomic abnormality which may be due to a small posterior fossa causes the lower part of the cerebellum to protrude from its normal location in the back of the head into the cervical or neck portion of the spinal canal. A syrinx may then develop in the cervical region of the spinal cord. Here, symptoms usually begin between the ages of 25 and 40 and may worsen with straining or any activity that causes cerebrospinal fluid pressure to fluctuate suddenly. Some patients, however, may have long periods of stability. Some patients with this form of the disorder also have hydrocephalus, in which cerebrospinal fluid accumulates in the skull, or a condition called arachnoiditis, in which a covering of the spinal cord—the arachnoid membrane—is inflamed.
Some cases of syringomyelia are familial, although this is rare.
Acquired.
The second major form of syringomyelia occurs as a complication of trauma, meningitis, hemorrhage, a tumor, or arachnoiditis. Here, the syrinx or cyst develops in a segment of the spinal cord damaged by one of these conditions. The syrinx then starts to expand. This is sometimes referred to as noncommunicating syringomyelia. Symptoms may appear months or even years after the initial injury, starting with pain, weakness, and sensory impairment originating at the site of trauma.
The primary symptom of post-traumatic syringomyelia (often referred to using the abbreviation of PTS) is pain, which may spread upward from the site of injury. Symptoms, such as pain, numbness, weakness, and disruption in temperature sensation, may be limited to one side of the body. Syringomyelia can also adversely affect sweating, sexual function, and, later, bladder and bowel control. A typical cause of PTS would be a car accident or similar trauma involving a whip-lash injury.
What can make PTS difficult to diagnose is the fact that symptoms can often first appear long after the actual cause of the syrinx occurred, e.g. a car accident occurring and then the patient first experiencing PTS symptoms such as pain, loss of sensation, reduced ability on the skin to feel varying degrees of hot and cold, a number of months after the car accident.
Symptoms.
Syringomyelia causes a wide variety of neuropathic symptoms due to damage of the spinal cord. Patients may experience severe chronic pain, abnormal sensations and loss of sensation particularly in the hands. Some patients experience paralysis or paresis temporarily or permanently. A syrinx may also cause disruptions in the parasympathetic and sympathetic nervous systems, leading to abnormal body temperature or sweating, bowel control issues, or other problems. If the syrinx is higher up in the spinal cord or affecting the brainstem as in syringobulbia, vocal cord paralysis, ipsilateral tongue wasting, trigeminal nerve sensory loss, and other signs may occur. Rarely, bladder stones can occur in the onset of weakness in the lower extremities.
Classically, syringomyelia spares the dorsal column/medial lemniscus of the spinal cord, leaving pressure, vibration, touch and proprioception intact in the upper extremities. Neuropathic arthropathy, also known as a Charcot joint, can occur, particularly in the shoulders, in patients with syringomyelia. The loss of sensory fibers to the joint is theorized to lead to damage of the joint over time.
Diagnosis.
Physicians now use magnetic resonance imaging (MRI) to diagnose syringomyelia. The MRI radiographer takes images of body anatomy, such as the brain and spinal cord, in vivid detail. This test will show the syrinx in the spine or any other conditions, such as the presence of a tumor. MRI is safe, painless, and informative and has greatly improved the diagnosis of syringomyelia.
The physician may order additional tests to help confirm the diagnosis. One of these is called electromyography (EMG), which measures muscle weakness. In addition, computed axial tomography (CT) scans of a patient's head may reveal the presence of tumors and other abnormalities such as hydrocephalus.
Like MRI and CT scans, another test, called a , uses radiographs and requires a contrast medium to be injected into the subarachnoid space. Since the introduction of MRI this test is rarely necessary to diagnose syringomyelia.
The possible causes are trauma, tumors and congenital defects. It is most usually observed in the part of the spinal cord corresponding to the neck area. Symptoms are due to spinal cord damage and are: pain, decreased sensation of touch, weakness and loss of muscle tissue. The diagnosis is confirmed with a spinal CT, myelogram or MRI of the spinal cord. The cavity may be reduced by surgical decompression.
Furthermore, evidence also suggests that impact injuries to the thorax area highly correlate with the occurrence of a cervical-located syrinx.
Treatment.
Surgery.
The first step after diagnosis is finding a neurosurgeon who is experienced in the treatment of syringomyelia. Surgery is the treatment for syringomyelia. Evaluation of the condition is necessary because syringomyelia can remain stationary for long periods of time, and in some cases progress rapidly.
Surgery of the spinal cord has certain, characteristic risks associated with it and the benefits of a surgical procedure on the spine have to be weighed against the possible complications associated with any procedure. Surgical treatment is aimed at correcting the condition that allowed the syrinx to form. It is vital to bear in mind that the drainage of a syrinx does not necessarily mean the elimination of the syrinx-related symptoms, but rather is aimed at stopping progression. In cases involving an Arnold-Chiari malformation, the main goal of surgery is to provide more space for the cerebellum at the base of the skull and upper cervical spine without entering the brain or spinal cord. This often results in flattening or disappearance of the primary syrinx or cavity, over time, as the normal flow of cerebrospinal fluid is restored. If a tumor is causing syringomyelia, removal of the tumor is the treatment of choice and almost always eliminates the syrinx.
Surgery results in stabilization or modest improvement in symptoms for most patients. Delay in treatment may result in irreversible spinal cord injury. Recurrence of syringomyelia after surgery may make additional operations necessary; these may not be completely successful over the long term.
In some patients it may also be necessary to drain the syrinx, which can be accomplished using a catheter, drainage tubes, and valves. This system is also known as a shunt. Shunts are used in both the communicating and noncommunicating forms of the disorder. First, the surgeon must locate the syrinx. Then, the shunt is placed into it with the other end draining cerebrospinal fluid (CSF) into a cavity, usually the abdomen. This type of shunt is called a ventriculoperitoneal shunt and is particularly useful in cases involving hydrocephalus. By draining syrinx fluid, a shunt can arrest the progression of symptoms and relieve pain, headache, and tightness. Syringomyelia shunts are not always successful and can become blocked as with other central nervous system shunts.
The decision to use a shunt requires extensive discussion between doctor and patient, as this procedure carries with it greater risk of injury to the spinal cord, infection, blockage, or hemorrhage and may not necessarily work for all patients. Draining the syrinx more quickly does not produce better outcomes, but a shunt may be required if the fluid in the syrinx is otherwise unable to drain.
In the case of trauma-related syringomyelia, the surgeon operates at the level of the initial injury. The syrinx collapses at surgery but a tube or shunt is usually necessary to prevent re-expansion.
Other.
Surgery is not always recommended for syringomyelia patients. For many patients, the main treatment is analgesia. Physicians specializing in pain management can develop a medication and treatment plan to ameliorate pain. Medications to combat any neuropathic pain symptoms such as shooting and stabbing pains (e.g. gabapentin or Lyricapregabalin) would be first-line choices. Opiates are usually prescribed for pain for management of this condition. Facet injections are not indicated for treatment of syringomyelia.
Drugs have no curative value as a treatment for syringomyelia. Radiation is used rarely and is of little benefit except in the presence of a tumor. In these cases, it can halt the extension of a cavity and may help to alleviate pain.
In the absence of symptoms, syringomyelia is usually not treated. In addition, a physician may recommend not treating the condition in patients of advanced age or in cases where there is no progression of symptoms. Whether treated or not, many patients will be told to avoid activities that involve straining.
Since the natural history of syringomyelia is poorly understood, a conservative approach may be recommended. When surgery is not yet advised, patients should be carefully monitored by a neurologist or neurosurgeon. Periodic MRI's and physical evaluations should be scheduled at the recommendation of a qualified physician.
Research.
The precise causes of syringomyelia are still unknown although blockage to the flow of cerebrospinal fluid has been known to be an important factor since the 1970s. Scientists in the UK and America continue to explore the mechanisms that lead to the formation of syrinxes in the spinal cord. It has been demonstrated a block to the free flow of cerebrospinal fluid is a contributory factor in the pathogenesis of the disease. Duke University in America and Warwick University are conducting research to explore genetic features of syringomyelia. 
Surgical techniques are also being refined by the neurosurgical research community. Successful procedures expand the area around the cerebellum and spinal cord, thus improving the flow of cerebrospinal fluid thereby reducing the syrinx.
It is also important to understand the role of birth defects in the development of hindbrain malformations that can lead to syringomyelia as syringomyelia is a feature of intrauterine life and is also associated with spina bifida. Learning when these defects occur during the development of the fetus can help us understand this and similar disorders, and may lead to preventive treatment that can stop the formation of some birth abnormalities. Dietary supplements of folic acid prior to pregnancy have been found to reduce the number of cases of spina bifida and are also implicated in prevention of cleft palate and some cardiac defects.
Diagnostic technology is another area for continued research. MRI has enabled scientists to see conditions in the spine, including syringomyelia before symptoms appear. A new technology, known as dynamic MRI, allows investigators to view spinal fluid flow within the syrinx. CT scans allow physicians to see abnormalities in the brain, and other diagnostic tests have also improved greatly with the availability of new, non-toxic, contrast dyes.

</doc>
<doc id="29778" url="http://en.wikipedia.org/wiki?curid=29778" title="Thomas Edison">
Thomas Edison

Thomas Alva Edison (February 11, 1847 – October 18, 1931) was an American inventor and businessman. He developed many devices that greatly influenced life around the world, including the phonograph, the motion picture camera, and a long-lasting, practical electric light bulb. Dubbed "The Wizard of Menlo Park", he was one of the first inventors to apply the principles of mass production and large-scale teamwork to the process of invention, and because of that, he is often credited with the creation of the first industrial research laboratory.
Edison was a prolific inventor, holding 1,093 US patents in his name, as well as many patents in the United Kingdom, France, and Germany. More significant than the number of Edison's patents was the widespread impact of his inventions: electric light and power utilities, sound recording, and motion pictures all established major new industries world-wide. Edison's inventions contributed to mass communication and, in particular, telecommunications. These included a stock ticker, a mechanical vote recorder, a battery for an electric car, electrical power, recorded music and motion pictures.
His advanced work in these fields was an outgrowth of his early career as a telegraph operator. Edison developed a system of electric-power generation and distribution to homes, businesses, and factories – a crucial development in the modern industrialized world. His first power station was on Pearl Street in Manhattan, New York.
Early life.
Thomas Edison was born in Milan, Ohio, and grew up in Port Huron, Michigan. He was the seventh and last child of Samuel Ogden Edison, Jr. (1804–1896, born in Marshalltown, Nova Scotia, Canada) and Nancy Matthews Elliott (1810–1871, born in Chenango County, New York). His father had to escape from Canada because he took part in the unsuccessful Mackenzie Rebellion of 1837. His patrilineal family line was Dutch.
In school, the young Edison's mind often wandered, and his teacher, the Reverend Engle, was overheard calling him "addled". This ended Edison's three months of official schooling. Edison recalled later, "My mother was the making of me. She was so true, so sure of me; and I felt I had something to live for, someone I must not disappoint." His mother taught him at home. Much of his education came from reading R.G. Parker's "School of Natural Philosophy" and The Cooper Union.
Edison developed hearing problems at an early age. The cause of his deafness has been attributed to a bout of scarlet fever during childhood and recurring untreated middle-ear infections. Around the middle of his career, Edison attributed the hearing impairment to being struck on the ears by a train conductor when his chemical laboratory in a boxcar caught fire and he was thrown off the train in Smiths Creek, Michigan, along with his apparatus and chemicals. In his later years, he modified the story to say the injury occurred when the conductor, in helping him onto a moving train, lifted him by the ears.
Edison's family moved to Port Huron, Michigan, after the railroad bypassed Milan in 1854 and business declined; his life there was bittersweet. Edison sold candy and newspapers on trains running from Port Huron to Detroit, and sold vegetables to supplement his income. He also studied qualitative analysis, and conducted chemical experiments on the train until an accident prohibited further work of the kind.
Edison obtained the exclusive right to sell newspapers on the road, and, with the aid of four assistants, he set in type and printed the "Grand Trunk Herald", which he sold with his other papers. This began Edison's long streak of entrepreneurial ventures, as he discovered his talents as a businessman. These talents eventually led him to found 14 companies, including General Electric, which is still one of the largest publicly traded companies in the world.
Telegrapher.
Edison became a telegraph operator after he saved three-year-old Jimmie MacKenzie from being struck by a runaway train. Jimmie's father, station agent J.U. MacKenzie of Mount Clemens, Michigan, was so grateful that he trained Edison as a telegraph operator. Edison's first telegraphy job away from Port Huron was at Stratford Junction, Ontario, on the Grand Trunk Railway.
In 1866, at the age of 19, Edison moved to Louisville, Kentucky, where, as an employee of Western Union, he worked the Associated Press bureau news wire. Edison requested the night shift, which allowed him plenty of time to spend at his two favorite pastimes—reading and experimenting. Eventually, the latter pre-occupation cost him his job. One night in 1867, he was working with a lead–acid battery when he spilled sulfuric acid onto the floor. It ran between the floorboards and onto his boss's desk below. The next morning Edison was fired.
One of his mentors during those early years was a fellow telegrapher and inventor named Franklin Leonard Pope, who allowed the impoverished youth to live and work in the basement of his Elizabeth, New Jersey, home. Some of Edison's earliest inventions were related to telegraphy, including a stock ticker. His first patent was for the electric vote recorder, (U.S. Patent 90,646), which was granted on June 1, 1869.
Marriages and children.
On December 25, 1871, Edison married 16-year-old Mary Stilwell (1855–1884), whom he had met two months earlier; she was an employee at one of his shops. They had three children:
Mary Edison died at age 29 on August 9, 1884, of unknown causes: possibly from a brain tumor or a morphine overdose. Doctors frequently prescribed morphine to women in those years to treat a variety of causes, and researchers believe that some of her symptoms sounded as if they were associated with morphine poisoning.
On February 24, 1886, at the age of thirty-nine, Edison married the 20-year-old Mina Miller (1866–1947) in Akron, Ohio. She was the daughter of the inventor Lewis Miller, co-founder of the Chautauqua Institution and a benefactor of Methodist charities. They also had three children together:
Mina outlived Thomas Edison, dying on August 24, 1947.
Beginning his career.
Edison began his career as an inventor in Newark, New Jersey, with the automatic repeater and his other improved telegraphic devices, but the invention that first gained him notice was the phonograph in 1877. This accomplishment was so unexpected by the public at large as to appear almost magical. Edison became known as "The Wizard of Menlo Park," New Jersey.
His first phonograph recorded on tinfoil around a grooved cylinder. Despite its limited sound quality and that the recordings could be played only a few times, the phonograph made Edison a celebrity. Joseph Henry, president of the National Academy of Sciences and one of the most renowned electrical scientists in the US, described Edison as "the most ingenious inventor in this country... or in any other". In April 1878, Edison travelled to Washington to demonstrate the phonograph before the National Academy of Sciences, Congressmen, Senators and US President Hayes. The "Washington Post" described Edison as a "genius" and his presentation as "a scene... that will live in history". Although Edison obtained a patent for the phonograph in 1878, he did little to develop it until Alexander Graham Bell, Chichester Bell, and Charles Tainter produced a phonograph-like device in the 1880s that used wax-coated cardboard cylinders.
Menlo Park.
Edison's major innovation was the first industrial research lab, which was built in Menlo Park, a part of Raritan Township, Middlesex County, New Jersey (today named Edison in his honor). It was built with the funds from the sale of Edison's quadruplex telegraph. After his demonstration of the telegraph, Edison was not sure that his original plan to sell it for $4,000 to $5,000 was right, so he asked Western Union to make a bid. He was surprised to hear them offer $10,000 ($<br>{Inflation} - Amount must not have "" prefix: 10,000.   in today's dollars.), which he gratefully accepted. The quadruplex telegraph was Edison's first big financial success, and Menlo Park became the first institution set up with the specific purpose of producing constant technological innovation and improvement. Edison was legally attributed with most of the inventions produced there, though many employees carried out research and development under his direction. His staff was generally told to carry out his directions in conducting research, and he drove them hard to produce results.
William Joseph Hammer, a consulting electrical engineer, began his duties as a laboratory assistant to Edison in December 1879. He assisted in experiments on the telephone, phonograph, electric railway, iron ore separator, electric lighting, and other developing inventions. However, Hammer worked primarily on the incandescent electric lamp and was put in charge of tests and records on that device. In 1880, he was appointed chief engineer of the Edison Lamp Works. In his first year, the plant under General Manager Francis Robbins Upton turned out 50,000 lamps. According to Edison, Hammer was "a pioneer of incandescent electric lighting". Frank J. Sprague, a competent mathematician and former naval officer, was recruited by Edward H. Johnson and joined the Edison organization in 1883. One of Sprague's contributions to the Edison Laboratory at Menlo Park was to expand Edison's mathematical methods. Despite the common belief that Edison did not use mathematics, analysis of his notebooks reveal that he was an astute user of mathematical analysis conducted by his assistants such as Francis Robbins Upton, for example, determining the critical parameters of his electric lighting system including lamp resistance by an analysis of Ohm's Law, Joule's Law and economics.
Nearly all of Edison's patents were utility patents, which were protected for a 17-year period and included inventions or processes that are electrical, mechanical, or chemical in nature. About a dozen were design patents, which protect an ornamental design for up to a 14-year period. As in most patents, the inventions he described were improvements over prior art. The phonograph patent, in contrast, was unprecedented as describing the first device to record and reproduce sounds.
In just over a decade, Edison's Menlo Park laboratory had expanded to occupy two city blocks. Edison said he wanted the lab to have "a stock of almost every conceivable material". A newspaper article printed in 1887 reveals the seriousness of his claim, stating the lab contained "eight thousand kinds of chemicals, every kind of screw made, every size of needle, every kind of cord or wire, hair of humans, horses, hogs, cows, rabbits, goats, minx, camels ... silk in every texture, cocoons, various kinds of hoofs, shark's teeth, deer horns, tortoise shell ... cork, resin, varnish and oil, ostrich feathers, a peacock's tail, jet, amber, rubber, all ores ..." and the list goes on.
Over his desk, Edison displayed a placard with Sir Joshua Reynolds' famous quotation: "There is no expedient to which a man will not resort to avoid the real labor of thinking." This slogan was reputedly posted at several other locations throughout the facility.
With Menlo Park, Edison had created the first industrial laboratory concerned with creating knowledge and then controlling its application.
Carbon telephone transmitter.
In 1877–78, Edison invented and developed the carbon microphone used in all telephones along with the Bell receiver until the 1980s. After protracted patent litigation, in 1892 a federal court ruled that Edison and not Emile Berliner was the inventor of the carbon microphone. The carbon microphone was also used in radio broadcasting and public address work through the 1920s.
Electric light.
In 1878 Edison began working on a system of electrical illumination, something he hoped could compete with gas and oil based lighting. He began by tackling the problem of creating a long lasting incandescent lamp, something that would be needed for indoor use. Many earlier inventors had previously devised incandescent lamps, including Alessandro Volta's demonstration of a glowing wire in 1800 and inventions by Henry Woodward and Mathew Evans. Others who developed early and commercially impractical incandescent electric lamps included Humphry Davy, James Bowman Lindsay, Moses G. Farmer, William E. Sawyer, Joseph Swan and Heinrich Göbel. Some of these early bulbs had such flaws as an extremely short life, high expense to produce, and high electric current drawn, making them difficult to apply on a large scale commercially.:217–218 Edison realized that in order to keep the thickness of the copper wire needed to connect a series of electric lights to an economically manageable size he would have to come up with a lamp that would draw a low amount of current. This meant the lamp would have to have a high resistance and run at a low voltage (around 110 volts).
After many experiments, first with carbon filaments and then with platinum and other metals, in the end Edison returned to a carbon filament. The first successful test was on October 22, 1879;:186 it lasted 13.5 hours. Edison continued to improve this design and by November 4, 1879, filed for U.S. patent 223,898 (granted on January 27, 1880) for an electric lamp using "a carbon filament or strip coiled and connected to platina contact wires". This was the first commercially practical incandescent light.
Although the patent described several ways of creating the carbon filament including "cotton and linen thread, wood splints, papers coiled in various ways", it was not until several months after the patent was granted that Edison and his team discovered a carbonized bamboo filament that could last over 1,200 hours. The idea of using this particular raw material originated from Edison's recalling his examination of a few threads from a bamboo fishing pole while relaxing on the shore of Battle Lake in the present-day state of Wyoming, where he and other members of a scientific team had traveled so that they could clearly observe a total eclipse of the sun on July 29, 1878, from the Continental Divide.
In 1878, Edison formed the Edison Electric Light Company in New York City with several financiers, including J. P. Morgan and the members of the Vanderbilt family. Edison made the first public demonstration of his incandescent light bulb on December 31, 1879, in Menlo Park. It was during this time that he said: "We will make electricity so cheap that only the rich will burn candles."
Henry Villard, president of the Oregon Railroad and Navigation Company, had attended Edison's 1879 demonstration. Villard quickly became impressed and requested Edison install his electric lighting system aboard his company's new steamer, the "Columbia". Although hesitant at first, Edison relented and agreed to Villard's request. Following most of its completion in May 1880, the "Columbia" was sent to New York City, where Edison and his personnel installed "Columbia"'s new lighting system. Due to this, the 
"Columbia" became Edison's first commercial application for his incandescent light bulb. The Edison equipment was eventually removed from "Columbia" in 1895.
Lewis Latimer joined the Edison Electric Light Company in 1884. Latimer had received a patent in January 1881 for the "Process of Manufacturing Carbons", an improved method for the production of carbon filaments for lightbulbs. Latimer worked as an engineer, a draftsman and an expert witness in patent litigation on electric lights.
George Westinghouse's company bought Philip Diehl's competing induction lamp patent rights (1882) for $25,000, forcing the holders of the Edison patent to charge a more reasonable rate for the use of the Edison patent rights and lowering the price of the electric lamp.
On October 8, 1883, the US patent office ruled that Edison's patent was based on the work of William Sawyer and was therefore invalid. Litigation continued for nearly six years, until October 6, 1889, when a judge ruled that Edison's electric-light improvement claim for "a filament of carbon of high resistance" was valid. To avoid a possible court battle with Joseph Swan, whose British patent had been awarded a year before Edison's, he and Swan formed a joint company called Ediswan to manufacture and market the invention in Britain.
Mahen Theatre in Brno (in what is now the Czech Republic), which opened in 1882, was the first public building in the world to use Edison's electric lamps, with the installation supervised by Edison's assistant in the invention of the lamp, Francis Jehl. In September 2010, a sculpture of three giant light bulbs was erected in Brno, in front of the theatre.
Electric power distribution.
After devising a commercially viable electric light bulb on October 21, 1879, Edison went on to develop and electric "utility" designed to compete with the then existent gas lighting utilities. In 1889 he patented a system for electricity distribution and on December 17, 1880, he founded the Edison Illuminating Company. The company established the first investor-owned electric utility in 1882 on Pearl Street Station, New York City. It was on September 4, 1882, that Edison switched on his Pearl Street generating station's electrical power distribution system, which provided 110 volts direct current (DC) to 59 customers in lower Manhattan.
Earlier in the year, in January 1882, he had switched on the first steam-generating power station at Holborn Viaduct in London. The DC supply system provided electricity supplies to street lamps and several private dwellings within a short distance of the station. On January 19, 1883, the first standardized incandescent electric lighting system employing overhead wires began service in Roselle, New Jersey.
War of currents.
As Edison was expanding his direct current (DC) power delivery system he began receiving stiff competition from companies installing alternating current (AC) systems. From the early 1880s on AC arc lighting systems for streets and large spaces had been an expanding business in the US. With development of transformers in Europe and by Westinghouse Electric in the US in 1885-1886 it became possible to transmit AC very long distances over thinner and cheaper wires, and "step down" the voltage at the destination for distribution to users. This allowed AC to be used not only in street lighting but also in lighting for small business and domestic customers, the market Edison's patented low voltage DC incandescent lamp system had been designed to supply. Edison's DC empire began suffering from one of its chief drawbacks: it was suitable only for the high density of customers found in large cities. Edison's DC plants could not deliver electricity to customers who were more than one mile from the plant and the short range left a patchwork of un-supplied customers in-between plants. Small cities and rural areas could not afford an Edison style system at all. This left a large part of market without electrical service and AC companies were expanding into this gap.
Edison expressed views that AC was unworkable and the high voltages used were dangerous. As George Westinghouse was installing his first AC systems in 1886, Thomas Edison began a pattern of striking out personally against his chief rival stating, "Just as certain as death, Westinghouse will kill a customer within six months after he puts in a system of any size. He has got a new thing and it will require a great deal of experimenting to get it working practically." Many reasons have been put forward for Edison's anti-AC stance. One notion is that the inventor may not have been able to grasp the more abstract theories behind AC and was trying to avoid developing a system he did not understand. Edison also appeared to have been worried about the high voltage from some competitor's misinstalled AC system killing customers and hurting the sales of electric power systems in general. On top of all that was the simple fact that Edison Electric had based their entire design on low voltage DC and switching a standard after they had installed over 100 systems was, in Edison's mind, out of the question. By the end of 1887 Edison Electric was beginning to lose market share with Westinghouse, who had built 68 AC-based power stations to Edison's 121 DC-based stations. To make matters worse for Edison, the Thomson-Houston Electric Company of Lynn, Massachusetts (another AC-based competitor) had built 22 power stations.
Parallel to the expanding competition between Edison and the AC companies was a rising public furor over a series of deaths in the spring of 1888 caused by pole mounted high voltage alternating current lines that turned into a media frenzy against the current and the seemingly greedy and callous lighting companies that used it. Edison took advantage of the public perception that AC was dangerous and teamed up with the self-styled New York anti-AC crusader Harold P. Brown in a propaganda campaign, aiding Brown in the public electrocution of animals with AC as well as supported legislation to control and severely limit AC installations and voltages (to the point of making it an ineffective power delivery system) in what was now being referred to as a "battle of currents". The development of the electric chair was used in an attempt to portray AC as having a greater lethal potential than DC and smear Westinghouse at the same time via Edison colluding with Brown and Westinghouse's chief AC rival, the Thomson-Houston Electric Company, to make sure the first electric chair was powered by a Westinghouse AC generator.
Thomas Edison's staunch anti-AC tactics were not sitting well with his own stock holders. By the early 1890s Edison's company was generating much smaller profits than its AC rivals, and the War of Currents would come to an end in 1892 with Edison being forced out of controlling his own company. That year the financier J P Morgan engineered a merger of Edison General Electric with Thomson-Houston that basically put the board of Thomson-Houston in charge of the new company called General Electric (dropping "Edison" from its name). General Electric now controlled three quarters of the US electrical business and would go on to compete with Westinghouse for the AC market.
Other inventions and projects.
Fluoroscopy.
Edison is credited with designing and producing the first commercially available fluoroscope, a machine that uses X-rays to take radiographs. Until Edison discovered that calcium tungstate fluoroscopy screens produced brighter images than the barium platinocyanide screens originally used by Wilhelm Röntgen, the technology was capable of producing only very faint images.
The fundamental design of Edison's fluoroscope is still in use today, although Edison himself abandoned the project after nearly losing his own eyesight and seriously injuring his assistant, Clarence Dally. Dally had made himself an enthusiastic human guinea pig for the fluoroscopy project and in the process been exposed to a poisonous dose of radiation. He later died of injuries related to the exposure. In 1903, a shaken Edison said "Don't talk to me about X-rays, I am afraid of them."
Telegraph improvements.
The key to Edison's fortunes was telegraphy. With knowledge gained from years of working as a telegraph operator, he learned the basics of electricity. This allowed him to make his early fortune with the stock ticker, the first electricity-based broadcast system. On August 9, 1892, Edison received a patent for a two-way telegraph.
Motion pictures.
Edison was also granted a patent for the motion picture camera or "Kinetograph". He did the electromechanical design, while his employee W.K.L. Dickson, a photographer, worked on the photographic and optical development. Much of the credit for the invention belongs to Dickson. In 1891, Thomas Edison built a Kinetoscope, or peep-hole viewer. This device was installed in penny arcades, where people could watch short, simple films. The kinetograph and kinetoscope were both first publicly exhibited May 20, 1891.
In April 1896, Thomas Armat's Vitascope, manufactured by the Edison factory and marketed in Edison's name, was used to project motion pictures in public screenings in New York City. Later he exhibited motion pictures with voice soundtrack on cylinder recordings, mechanically synchronized with the film.
Officially the kinetoscope entered Europe when the rich American Businessman Irving T. Bush (1869–1948) bought from the Continental Commerce Company of Frank Z. Maguire and Joseph D. Baucus a dozen machines. Bush placed from October 17, 1894, the first kinetoscopes in London. At the same time the French company Kinétoscope Edison Michel et Alexis Werner bought these machines for the market in France. In the last three months of 1894, The Continental Commerce Company sold hundreds of kinetoscopes in Europe (i.e. the Netherlands and Italy). In Germany and in Austria-Hungary the kinetoscope was introduced by the Deutsche-österreichische-Edison-Kinetoscop Gesellschaft, founded by the Ludwig Stollwerck of the Schokoladen-Süsswarenfabrik Stollwerck & Co of Cologne.
The first kinetoscopes arrived in Belgium at the Fairs in early 1895. The Edison's Kinétoscope Français, a Belgian company, was founded in Brussels on January 15, 1895, with the rights to sell the kinetoscopes in Monaco, France and the French colonies. The main investors in this company were Belgian industrialists.
On May 14, 1895, the Edison's Kinétoscope Belge was founded in Brussels. The businessman Ladislas-Victor Lewitzki, living in London but active in Belgium and France, took the initiative in starting this business. He had contacts with Leon Gaumont and the American Mutoscope and Biograph Co. In 1898 he also became a shareholder of the Biograph and Mutoscope Company for France.
Edison's film studio made close to 1,200 films. The majority of the productions were short films showing everything from acrobats to parades to fire calls including titles such as "Fred Ott's Sneeze" (1894), "The Kiss" (1896), "The Great Train Robbery" (1903), "Alice's Adventures in Wonderland" (1910), and the first "Frankenstein" film in 1910. In 1903, when the owners of Luna Park, Coney Island announced they would execute Topsy the elephant by strangulation, poisoning, and electrocution (with the electrocution part ultimately killing the elephant), Edison Manufacturing sent a crew to film it, releasing it that same year with the title "Electrocuting an Elephant".
As the film business expanded competing exhibitors routinely copied and exhibited each other's films. To better protect the copyrights on his films, Edison deposited prints of them on long strips of photographic paper with the U.S. copyright office. Many of these paper prints survived longer and in better condition than the actual films of that era.
In 1908, Edison started the Motion Picture Patents Company, which was a conglomerate of nine major film studios (commonly known as the Edison Trust). Thomas Edison was the first honorary fellow of the Acoustical Society of America, which was founded in 1929.
Edison said his favorite movie was "The Birth of a Nation". He thought that talkies had "spoiled everything" for him. "There isn't any good acting on the screen. They concentrate on the voice now and have forgotten how to act. I can sense it more than you because I am deaf." His favorite stars were Mary Pickford and Clara Bow.
Mining.
In 1901, Edison visited an industrial exhibition in the Sudbury area in Ontario, Canada and thought nickel and cobalt deposits there could be used in his production of electrical equipment. He returned as a mining prospector, and is credited with the original discovery of the Falconbridge ore body. His attempts to mine the ore body were not successful, however, and he abandoned his mining claim in 1903. A street in Falconbridge, as well as the Edison Building, which served as the head office of Falconbridge Mines, are named for him.
West Orange and Fort Myers (1886–1931).
Edison moved from Menlo Park after the death of his first wife, Mary, in 1884, and purchased a home known as "Glenmont" in 1886 as a wedding gift for his second wife, Mina, in Llewellyn Park in West Orange, New Jersey. In 1885, Thomas Edison had bought property in Fort Myers, Florida, and built what was later called Seminole Lodge as a winter retreat. Edison and Mina spent many winters at their home in Fort Myers, and Edison tried to find a domestic source of natural rubber.
In 1928, Edison joined the Fort Myers Civitan Club. He believed strongly in the organization, writing that "The Civitan Club is doing things—big things—for the community, state, and nation, and I certainly consider it an honor to be numbered in its ranks." He was an active member in the club until his death, sometimes bringing Henry Ford to the club's meetings.
Final years and death.
Henry Ford, the automobile magnate, later lived a few hundred feet away from Edison at his winter retreat in Fort Myers, Florida. Edison even contributed technology to the automobile. They were friends until Edison's death.
Edison was active in business right up to the end. Just months before his death, the Lackawanna Railroad inaugurated suburban electric train service from Hoboken to Montclair, Dover, and Gladstone, New Jersey. Electrical transmission for this service was by means of an overhead catenary system using direct current, which Edison had championed. Despite his frail condition, Edison was at the throttle of the first electric MU (Multiple-Unit) train to depart Lackawanna Terminal in Hoboken in September 1930, driving the train the first mile through Hoboken yard on its way to South Orange.
This fleet of cars would serve commuters in northern New Jersey for the next 54 years until their retirement in 1984. A plaque commemorating Edison's inaugural ride can be seen today in the waiting room of Lackawanna Terminal in Hoboken, which is presently operated by New Jersey Transit.
Edison was said to have been influenced by a popular fad diet in his last few years; "the only liquid he consumed was a pint of milk every three hours". He is reported to have believed this diet would restore his health. However, this tale is doubtful. In 1930, the year before Edison died, Mina said in an interview about him, "correct eating is one of his greatest hobbies." She also said that during one of his periodic "great scientific adventures", Edison would be up at 7:00, have breakfast at 8:00, and be rarely home for lunch or dinner, implying that he continued to have all three.
Edison became the owner of his Milan, Ohio, birthplace in 1906. On his last visit, in 1923, he was reportedly shocked to find his old home still lit by lamps and candles.
Death.
Edison died of complications of diabetes on October 18, 1931, in his home, "Glenmont" in Llewellyn Park in West Orange, New Jersey, which he had purchased in 1886 as a wedding gift for Mina. He is buried behind the home.
Edison's last breath is reportedly contained in a test tube at the Henry Ford Museum. Ford reportedly convinced Charles Edison to seal a test tube of air in the inventor's room shortly after his death, as a memento. A plaster death mask was also made. Mina died in 1947.
Views on politics, religion and metaphysics.
Historian Paul Israel has characterized Edison as a "freethinker". Edison was heavily influenced by Thomas Paine's "The Age of Reason". Edison defended Paine's "scientific deism", saying, "He has been called an atheist, but atheist he was not. Paine believed in a supreme intelligence, as representing the idea which other men often express by the name of deity." In an October 2, 1910, interview in the "New York Times Magazine," Edison stated:
Nature is what we know. We do not know the gods of religions. And nature is not kind, or merciful, or loving. If God made me — the fabled God of the three qualities of which I spoke: mercy, kindness, love — He also made the fish I catch and eat. And where do His mercy, kindness, and love for that fish come in? No; nature made us — nature did it all — not the gods of the religions.
Edison was accused of being an atheist for those remarks, and although he did not allow himself to be drawn into the controversy publicly, he clarified himself in a private letter: 
You have misunderstood the whole article, because you jumped to the conclusion that it denies the existence of God. There is no such denial, what you call God I call Nature, the Supreme intelligence that rules matter. All the article states is that it is doubtful in my opinion if our intelligence or soul or whatever one may call it lives hereafter as an entity or disperses back again from whence it came, scattered amongst the cells of which we are made.
He also stated, "I do not believe in the God of the theologians; but that there is a Supreme Intelligence I do not doubt."
Nonviolence was key to Edison's moral views, and when asked to serve as a naval consultant for World War I, he specified he would work only on defensive weapons and later noted, "I am proud of the fact that I never invented weapons to kill." Edison's philosophy of nonviolence extended to animals as well, about which he stated: "Nonviolence leads to the highest ethics, which is the goal of all evolution. Until we stop harming all other living beings, we are still savages." However, he is also notorious for having electrocuted a number of dogs in 1888, both by direct and alternating current, in an attempt to argue that the former (which he had a vested business interest in promoting) was safer than the latter (favored by his rival George Westinghouse).
Edison's success in promoting direct current as less lethal also led to alternating current being used in the electric chair adopted by New York in 1889 as a supposedly humane execution method. Because Westinghouse was angered by the decision, he funded Eighth Amendment-based appeals for inmates set to die in the electric chair, ultimately resulting in Edison providing the generators which powered early electrocutions and testifying successfully on behalf of the state that electrocution was a painless method of execution.
In 1920, Edison set off a media sensation when he told B. C. Forbes of American Magazine that he was working on a "spirit phone" to allow communication with the dead, a story which other newspapers and magazines repeated. Edison later disclaimed the idea, telling the New York Times in 1926 that "I really had nothing to tell him, but I hated to disappoint him so I thought up this story about communicating with spirits, but it was all a joke."
Views on money.
Thomas Edison was an advocate for monetary reform in the United States. He was ardently opposed to the gold standard and debt-based money. Famously, he was quoted in the New York Times stating "Gold is a relic of Julius Caesar, and interest is an invention of Satan."
In the same article, he expounded upon the absurdity of a monetary system in which the taxpayer of the United States, in need of a loan, be compelled to pay in return perhaps double the principal, or even greater sums, due to interest. His basic point was that if the Government can produce debt-based money, it could equally as well produce money that was a credit to the taxpayer.
He thought at length about the subject of money over 1921 and 1922. In May 1922, he published a proposal, entitled "A Proposed Amendment to the Federal Reserve Banking System". In it, he detailed an explanation of a commodity-backed currency, in which the Federal Reserve would issue interest-free currency to farmers, based on the value of commodities they produced. During a publicity tour that he took with friend and fellow inventor, Henry Ford, he spoke publicly about his desire for monetary reform. For insight, he corresponded with prominent academic and banking professionals. In the end, however, Edison's proposals failed to find support, and were eventually abandoned.
Awards.
The President of the Third French Republic, Jules Grévy, on the recommendation of his Minister of Foreign Affairs Jules Barthélemy-Saint-Hilaire and with the presentations of the Minister of Posts and Telegraphs Louis Cochery, designated Edison with the "distinction" of an" 'Officer of the Legion of Honour' "(Légion d'honneur) by decree on November 10, 1881; He also named a Chevalier in 1879, and a Commander in 1889.
In 1887, Edison won the Matteucci Medal. In 1890, he was elected a member of the Royal Swedish Academy of Sciences.
The Philadelphia City Council named Edison the recipient of the John Scott Medal in 1889.
In 1899, Edison was awarded the Edward Longstreth Medal of The Franklin Institute.
He was named an "Honorable Consulting Engineer" at the Louisiana Purchase Exposition World's fair in 1904.
In 1908, Edison received the American Association of Engineering Societies John Fritz Medal.
In 1915, Edison was awarded Franklin Medal of The Franklin Institute for discoveries contributing to the foundation of industries and the well-being of the human race.
In 1920, The United States Navy department awarded him the Navy Distinguished Service Medal.
In 1923, The American Institute of Electrical Engineers created the Edison Medal and he was its first recipient.
In 1927, he was granted membership in the National Academy of Sciences.
On May 29, 1928, Edison received the Congressional Gold Medal.
In 1983, the United States Congress, pursuant to Senate Joint Resolution 140 (Public Law 97—198), designated February 11, Edison's birthday, as National Inventor's Day.
"Life" magazine (USA), in a special double issue in 1997, placed Edison first in the list of the "100 Most Important People in the Last 1000 Years", noting that the light bulb he promoted "lit up the world". In the 2005 television series "The Greatest American", he was voted by viewers as the fifteenth-greatest.
In 2008, Edison was inducted in the New Jersey Hall of Fame.
In 2010, Edison was honored with a Technical Grammy Award.
In 2011, Edison was inducted into the Entrepreneur Walk of Fame, and named a Great Floridian by the Florida Governor and Cabinet.
Tributes.
Places and people named for Edison.
Several places have been named after Edison, most notably the town of Edison, New Jersey. Thomas Edison State College, a nationally known college for adult learners, is in Trenton, New Jersey. Two community colleges are named for him: Edison State College in Fort Myers, Florida, and
Edison Community College in Piqua, Ohio. There are numerous high schools named after Edison (see Edison High School) and other schools including Thomas A. Edison Middle School.
In 1883, the City Hotel in Sunbury, Pennsylvania was the first building to be lit with Edison's three-wire system. The hotel was renamed The Hotel Edison upon Edison's return to the City on 1922.
Lake Thomas A Edison in California was named after Edison to mark the 75th anniversary of the incandescent light bulb.
Edison was on hand to turn on the lights at the Hotel Edison in New York City when it opened in 1931.
Three bridges around the United States have been named in Edison's honor: the Edison Bridge in New Jersey, the Edison Bridge in Florida, and the Edison Bridge in Ohio.
In space, his name is commemorated in asteroid 742 Edisona.
Museums and memorials.
In West Orange, New Jersey, the 13.5 acre Glenmont estate is maintained and operated by the National Park Service as the Edison National Historic Site, as is his nearby laboratory and workshops including the reconstructed Black Maria- the world's first movie studio. The Thomas Alva Edison Memorial Tower and Museum is in the town of Edison, New Jersey. In Beaumont, Texas, there is an Edison Museum, though Edison never visited there.
The Port Huron Museum, in Port Huron, Michigan, restored the original depot that Thomas Edison worked out of as a young newsbutcher. The depot has been named the Thomas Edison Depot Museum. The town has many Edison historical landmarks, including the graves of Edison's parents, and a monument along the St. Clair River. Edison's influence can be seen throughout this city of 32,000.
In Detroit, the Edison Memorial Fountain in Grand Circus Park was created to honor his achievements. The limestone fountain was dedicated October 21, 1929, the fiftieth anniversary of the creation of the lightbulb. On the same night, The Edison Institute was dedicated in nearby Dearborn.
Awards named in honor of Edison.
The Edison Medal was created on February 11, 1904, by a group of Edison's friends and associates. Four years later the American Institute of Electrical Engineers (AIEE), later IEEE, entered into an agreement with the group to present the medal as its highest award. The first medal was presented in 1909 to Elihu Thomson. It is the oldest award in the area of electrical and electronics engineering, and is presented annually "for a career of meritorious achievement in electrical science, electrical engineering or the electrical arts."
In the Netherlands, the major music awards are named the Edison Award after him. The award is an annual Dutch music prize, awarded for outstanding achievements in the music industry, and is one of the oldest music awards in the world, having been presented since 1960.
The American Society of Mechanical Engineers concedes the Thomas A. Edison Patent Award to individual patents since 2000.
Other items named after Edison.
The United States Navy named the USS "Edison" (DD-439), a Gleaves class destroyer, in his honor in 1940. The ship was decommissioned a few months after the end of World War II. In 1962, the Navy commissioned USS "Thomas A. Edison" (SSBN-610), a fleet ballistic missile nuclear-powered submarine.
In popular culture.
Thomas Edison has appeared in popular culture as a character in novels, films, comics and video games. His prolific inventing helped make him an icon and he has made appearances in popular culture during his lifetime down to the present day. Edison is also portrayed in popular culture as an adversary of Nikola Tesla.
On February 11, 2011, on Thomas Edison's 164th birthday, Google's homepage featured an animated Google Doodle commemorating his many inventions. When the cursor was hovered over the doodle, a series of mechanisms seemed to move, causing a lightbulb to glow.
See also.
Other inventors and businessmen
Bibliography.
</dl>
External links.
Locations
Information and media

</doc>
<doc id="29947" url="http://en.wikipedia.org/wiki?curid=29947" title="Trick-taking game">
Trick-taking game

A trick-taking game is a card game or tile-based game in which play of a "hand" centers on a series of finite rounds or units of play, called "tricks", which are each evaluated to determine a winner or "taker" of that trick. The object of such games then may be closely tied to the number of tricks taken, as in plain-trick games such as Whist, Contract Bridge, Spades, Napoleon, Rowboat, and Spoil Five, or on the value of the cards contained in taken tricks, as in point-trick games such as Pinochle, the Tarot family, Rook, All Fours, Manille, Briscola, and most "evasion" games like Hearts. The domino game Texas 42 is an example of a trick-taking game that is not a card game.
Trick-and-draw games are trick-taking games in which the players can fill up their hands after each trick. Typically players are free to play any card into a trick in the first phase of the game, but must follow suit as soon as the score is depleted.
Basic structure.
Certain actions in trick-taking games with three or more players always proceed in the same direction. In games originating in North and West Europe including England, Russia, and the United States and Canada, the rotation is typically clockwise, i.e. play proceeds to the left; in South and East Europe and Asia it is typically counterclockwise, so that play proceeds to the right. When games move from one region to another, they tend to initially preserve their original sense of rotation, but a region with a dominant sense of rotation may adapt a migrated game to its own sensibilities. For two-player games the order of play is moot as either direction would result in exactly the same turn order.
In each "hand" or "deal", one player is the "dealer". This function moves from deal to deal in the normal direction of play. The dealer usually shuffles the deck (some games use "soft shuffling", where the dealer does not explicitly shuffle the deck), and after giving the player one seat from the dealer opposite the normal direction of play an opportunity to cut, hands out the same (prescribed) number of cards to each player, usually in an order following the normal direction of play. Most games deal cards one at a time in rotation; a few games require dealing multiple cards at one time in a "packet". The cards apportioned to each player are collectively known as that player's "hand" and are only known to the player. Some games involve a set of cards that are not dealt to a player's hand; these cards form the "stock" (see below). It is generally good manners to leave one's cards on the table until the deal is complete. 
The player sitting one seat after the declarer (one with the highest bid and not the dealer) in normal rotation is known as the "eldest hand". The eldest hand "leads" to the first "trick", i.e. places the first card of the trick face up in the middle of all players. The other players each follow with a single card, in the direction of play. When every player has played a card to the trick, the trick is evaluated to determine the winner, who takes the cards, places them face down on a pile, and leads to the next trick. The winner or taker of a trick is usually the player who played the highest-value card of the suit that was led, unless the game uses one or more "trump" cards (see below).
The player who leads to a trick is usually allowed to play an arbitrary card from their hand. Some games have restrictions on the first card played in the hand, or may disallow leading a card of a particular suit until that suit has been played "off-suit" in a prior trick (called "breaking" the suit, usually seen in cases of a trump or penalty suit). Other games have special restrictions on the card that must be led to the first trick; usually this is a specific card (e.g. 2♣) and the holder of that card is the eldest hand instead of the person one seat after the dealer.
The following players must "follow suit" if they can, i.e. they must play a card of the same suit if possible. A player who cannot follow suit may "sluff" a card, i.e. play a card of a different suit. A trick is won by the player who has played the highest-ranked card of the "suit led", i.e. of the suit of the first card in the trick (unless the game uses a trump suit; see below).
It can be an advantage to lead to a trick, because the player who leads controls the suit that is led and which others must follow; playing a suit that the leading player has many of decreases the chance that anyone else would be able to follow suit, while conversely playing a suit the player has few of allows the player to rid their hand of that suit (known as "voiding" the suit), freeing them from the restriction to follow suit when that suit is led by another player. On the other hand it can also be an advantage to be the last player who plays to the trick, because at that point one has full information about the other cards played to the trick; the last player to a trick can play a card just slightly higher or lower than the current winning card, guaranteeing they will win or lose it by the minimum amount necessary, saving more valuable high or low value cards for situations where they must guarantee that a card played early to a trick will win or lose.
When all cards have been played, the number or contents of the tricks won by each player is tallied and used to update the score. Scoring based on the play of tricks varies widely between games, but in most games either the number of tricks a player or partnership has won ("plain-trick" games), or the value of certain cards that the player has won by taking tricks ("point-trick" games) is important.
Trumps.
"Trump cards" are a set of one or more cards in the deck that, when played, are of higher value than the suit led. If a trick contains any trump cards, it is won by the highest-value trump card played, not the highest-value card of the suit led.
In most games with trumps, one of the four suits is identified as the "trump suit". In the simplest case, there is a static trump suit such as the Spade suit in the game Spades, or a dedicated symbol-less trump suit in the Tarot family. More often, a dynamic trump suit is determined by some means, either randomly by selection of a card, or decided by the winner or winning bid of an auction. In certain games, such as Rowboat and Rage, the trump suit may change during the course of the hand, even from trick to trick.
In most modern games with trump suits, the rules for following suit do not distinguish between the trump suit and the "plain suits". If a trick begins with a plain suit card and a later player cannot follow suit, the player may choose freely to either "sluff" (discard a card of another plain suit), or "ruff" ("trump" the trick by playing a trump card). Subsequent players to the trick must still follow the original suit, and may only discard or trump if they do not hold a card of the suit led. Certain games are "play to beat" or "must-trump"; if a player cannot follow suit but can play trump, they must play trump, and additionally if they are able they must beat any trump card already played to the trick. Pinochle and several of the Tarot card game variants have this rule.
In some games, in addition to or separately from a trump suit, certain fixed cards are always the highest trumps, e.g. the Jacks in Skat, the Jacks or Jokers in Euchre, the Rook Bird card in Rook, or the Fool in certain Tarot games.
Revoking.
If a player who can follow suit does not do so, or in games with additional restrictions on card play, not following these restrictions is known as a revoke, or 'renege'. A revoke typically cannot be discovered at the time when it is committed, but when a player plays off-suit to a trick, competent opponents will make a mental note that the player does not hold the suit led, and will notice later if the player later plays a card of the suit they were thought to be void in. The situation is similar for other types of revoke. Most game rules prescribe a severe penalty for a revoke and may also result in the hand being voided (a "misdeal"). Decks of cards have been marketed for trick-taking games with the traditional Anglo-French suit symbols, but in four colors; these are often called "no-revoke" decks, as the color contrast between each suit makes a potential revoking play easier to spot and harder to do accidentally.
Winning and scoring a deal.
When all tricks have been played, the winner of the deal and the players' scores can be determined. The determining factor in plain-trick games (the most popular form of trick-taking games in English-speaking countries) is simply how many tricks each player or partnership has taken. In point-trick games, certain card values are worth varying points, and the players sum the points from cards in their "scoring piles" that were accumulated by taking tricks. Points for cards, and the method of counting points, vary by game; in Rook, for example, the 5-card of each color is worth 5 points, the 10 and 14 (or Ace) is worth 10, and the Rook Bird (or Joker) is worth 20, while all other cards are worth nothing. Pinochle has many popular scoring variants usually based on point values for face cards and Aces, while pip cards score no points. In French Tarot, all cards have a value including a half-point, and are traditionally scored in pairs of a high-value and a low-value card which results in a whole-point value for the pair.
In the most common "positive" or "race" games, players seek to win as many tricks or card points as possible. To win a deal, a player typically needs to win a minimal number of tricks or card points; this minimal threshold is usually called the "contract", and may be defined by the game's rules (a simple majority of total available points or tricks, or tiered thresholds depending on which player or side has captured certain cards), or the result of an "auction" or "bidding" process. A player who wins more than the number of tricks or card points necessary for winning the deal may be rewarded with a higher score, or conversely (in exact-prediction games) they may be penalized.
There are also "negative" or "evasion" games, in which the object is to avoid tricks or card points. E.g. in Hearts each card point won in a trick contributes negatively to the score. A special type is "misère" games, which are usually variants of positive games which can only be won by not winning a single trick.
Other criteria also occur. Sometimes the last trick has special significance. In "marriage games" such as Pinochle the winner of the last trick receives 10 points in addition to the card points, while in "final-trick" games such as Cắt Tê only the winner of the last trick can win a deal. 
There are also blends between positive and negative games, e.g. the aim may be to win a certain prescribed number of tricks. Many card games, regardless of their normal scoring mechanism, give bonuses to players or partnerships who win all tricks or possible points in a hand, or conversely lose all tricks or points.
Contracts and auctions.
In a "contract" game the winning and scoring conditions are not fixed but are chosen by one of the players after seeing their hand. In such games, players make "bids" depending on the number of tricks or card points they believe they can win during play of the hand. One or more of these bids stands as the "contract", and the player who made that bid is rewarded for meeting it or penalized for not meeting it.
In "auction" games, bidding players are competing against each other for the right to attempt to make the contract. In a few games, the contract is fixed (normally a simple majority, less often based on certain cards captured during play) and players' bids are a wager of game points to be won or lost. In others, the bid is a number of tricks or card points the bidder is confident that they or their partnership will take. Either of these can also include the suit to be used as trumps during the hand. The highest bid becomes the contract and the highest bidder is the "contractor", known in some games as the "declarer" or "taker", who then plays either with or without a partner. The other players become "opponents", whose main goal is to prevent the contract being met. Popular examples include Contract bridge, Pinochle, tarot games, Skat, Belote and Twenty-Eight. In many auction games the eldest hand leads to the first trick, regardless of who won the auction, but in some, such as Contract Bridge, the first lead is made by the player next in rotation after the contractor, so that the contractor plays last to that trick.
In "precision" or "exact-prediction" games, all players choose their winning condition independently: to win precisely a predicted number of tricks (Oh Hell) or card points (Differenzler). Each player's bid stands (in partnership games the partners' bids are often combined), and each player or partnership then tries to take exactly the number of tricks or points they bid, and are rewarded or penalized for doing so independently of anyone else's success or failure in meeting their bid. This type of game began to mature in the 20th century. Other games generally falling into the exact-prediction category are Spades and Ninety-Nine.
Stock.
In some games not all cards are distributed to the players, and a "stock" remains. This stock can be referred to by different names, depending on the game; "supply", "talon", "nest", "kitty", and "dog" are common game-specific and/or regional names.
In some games the stock remains untouched throughout play of the hand; it is simply a pile of "extra" cards that will never be played and whose values are unknown, which will reduce the effectiveness of "counting cards" (a common strategy of keeping track of the cards that have been played or are yet to be played). In other games, the winner of an auction-bidding process (the taker or declarer) may get to exchange cards from his hand with the stock, either by integrating the stock into his hand and then discarding equal cards as in Skat, Rook and French Tarot, or in a "blind" fashion by discarding and drawing as in Ombre. The stock, either in its original or discarded form, may additionally form part of one or more players' "scoring piles" of tricks taken; it may be kept by the declarer, may be won by the player of the first trick, or may go to an opposing player or partnership.
In some games, especially two-player games, after each trick every player draws a new card. This continues while the stock lasts. Since this drawing mechanism would normally make it difficult or impossible to detect a revoke (for instance, the player may not be able to follow suit, so they play off-suit and then immediately draw a card of the suit led), in the first phase of trick-play (before the stock is empty) players generally need not follow suit. A widespread game of this type is Schnapsen/66/Marriage.
Partnerships.
In many games such as Hearts and Oh Hell, all players play individually against each other, but in other games there are fixed or varying partnerships.
Some games such as Pinochle are commonly played with or without partnerships, depending on the number of players.
Special variations.
Numerous further variations to the basic rules may occur, and only a few examples can be mentioned here:
History.
According to card game researcher David Parlett, the oldest known trick-taking game, Karnöffel, was mentioned in 1426 in the Bavarian town Nördlingen – roughly half a century after the introduction of playing cards to Europe, which were first mentioned in Spain in 1371. The oldest known trumps appear in Karnöffel, where specific ranks of one suit were named "Karnöffel, Devil, Pope" etc. and subject to an elaborate system of trumping powers. Around 1440 in Italy special cards called "trionfi" were introduced with a similar function. These special cards are now known as "tarots", and a deck augmented by tarots as a tarot deck. The trionfi/tarots formed essentially a fifth suit without the ordinary ranks but consisting of trumps in a fixed hierarchy. But one can get a similar effect by declaring all cards of a fixed or randomly determined suit to be trumps. This method is still followed by a number of modern trick-taking games that do not involve an auction. Parlett notes that while trumps were retroactively added to some games, such as Trappola, no example is known of trumps being removed from a game. 
The trick-taking genre includes some of the most historically popular games ever played such as Bridge, Spades, Hearts, Rook, Belote, Skat, Euchre, and Pinochle.
Most trick-taking games popular in the English-speaking world descended from the game Ruff and Honours, a simple "race"-type game where the object is to take as many tricks as possible. This game evolved into Whist, from which the majority of current plain-trick games was derived.
It is possible that the origin of the practice of counting tricks (in plain-trick games) was the counting of cards won in tricks. It was therefore a logical development to accord some cards a higher counting-value, and some cards no value at all, leading to point-trick games. Point-trick games are at least as old as tarot decks and may even predate the invention of trumps. All point-trick games are played with tarot decks or stripped decks, which in many countries became standard before 1600, and neither point-trick games nor stripped decks have a tradition in England.
While there is a number of games with unusual card-point values, such as Trappola and All Fours, most point-trick games are in the huge family of Ace–Ten card games. Pinochle is a representative of this family that is popular in the United States. Other examples include Belote and Skat.

</doc>
<doc id="29954" url="http://en.wikipedia.org/wiki?curid=29954" title="Topology">
Topology

In mathematics, topology (from the Greek τόπος, "place", and λόγος, "study"), the study of topological spaces, is an area of mathematics concerned with the properties of space that are preserved under continuous deformations, such as stretching and bending, but not tearing or gluing. Important topological properties include connectedness and compactness.
Topology developed as a field of study out of geometry and set theory, through analysis of such concepts as space, dimension, and transformation. Such ideas go back to Gottfried Leibniz, who in the 17th century envisioned the "geometria situs" (Greek-Latin for "geometry of place") and "analysis situs" (Greek-Latin for "picking apart of place"). The term "topology" was introduced by Johann Benedict Listing in the 19th century, although it was not until the first decades of the 20th century that the idea of a topological space was developed. By the middle of the 20th century, topology had become a major branch of mathematics.
Topology has many subfields:
See also: topology glossary for definitions of some of the terms used in topology, and topological space for a more technical treatment of the subject.
History.
Topology began with the investigation of certain questions in geometry. Leonhard Euler's 1736 paper on the Seven Bridges of Königsberg is regarded as one of the first academic treatises in modern topology.
The term "Topologie" was introduced in German in 1847 by Johann Benedict Listing in "Vorstudien zur Topologie", who had used the word for ten years in correspondence before its first appearance in print. The English form topology was first used in 1883 in Listing's obituary in the journal "Nature" to distinguish "qualitative geometry from the ordinary geometry in which quantitative relations chiefly are treated". The term topologist in the sense of a specialist in topology was used in 1905 in the magazine "Spectator". However, none of these uses corresponds exactly to the modern definition of topology.
Modern topology depends strongly on the ideas of set theory, developed by Georg Cantor in the later part of the 19th century. In addition to establishing the basic ideas of set theory, Cantor considered point sets in Euclidean space as part of his study of Fourier series.
Henri Poincaré published "Analysis Situs" in 1895, introducing the concepts of homotopy and homology, which are now considered part of algebraic topology.
Unifying the work on function spaces of Georg Cantor, Vito Volterra, Cesare Arzelà, Jacques Hadamard, Giulio Ascoli and others, Maurice Fréchet introduced the metric space in 1906. A metric space is now considered a special case of a general topological space. In 1914, Felix Hausdorff coined the term "topological space" and gave the definition for what is now called a Hausdorff space. Currently, a topological space is a slight generalization of Hausdorff spaces, given in 1922 by Kazimierz Kuratowski.
For further developments, see point-set topology and algebraic topology.
Introduction.
Topology can be formally defined as "the study of qualitative properties of certain objects (called topological spaces) that are invariant under a certain kind of transformation (called a continuous map), especially those properties that are invariant under a certain kind of transformation (called homeomorphism)."
Topology is also used to refer to a structure imposed upon a set "X", a structure that essentially 'characterizes' the set "X" as a topological space by taking proper care of properties such as convergence, connectedness and continuity, upon transformation.
Topological spaces show up naturally in almost every branch of mathematics. This has made topology one of the great unifying ideas of mathematics.
The motivating insight behind topology is that some geometric problems depend not on the exact shape of the objects involved, but rather on the way they are put together. For example, the square and the circle have many properties in common: they are both one dimensional objects (from a topological point of view) and both separate the plane into two parts, the part inside and the part outside.
One of the first papers in topology was the demonstration, by Leonhard Euler, that it was impossible to find a route through the town of Königsberg (now Kaliningrad) that would cross each of its seven bridges exactly once. This result did not depend on the lengths of the bridges, nor on their distance from one another, but only on connectivity properties: which bridges are connected to which islands or riverbanks. This problem in introductory mathematics called "Seven Bridges of Königsberg" led to the branch of mathematics known as graph theory.
Similarly, the hairy ball theorem of algebraic topology says that "one cannot comb the hair flat on a hairy ball without creating a cowlick." This fact is immediately convincing to most people, even though they might not recognize the more formal statement of the theorem, that there is no nonvanishing continuous tangent vector field on the sphere. As with the "Bridges of Königsberg", the result does not depend on the shape of the sphere; it applies to any kind of smooth blob, as long as it has no holes.
To deal with these problems that do not rely on the exact shape of the objects, one must be clear about just what properties these problems "do" rely on. From this need arises the notion of homeomorphism. The impossibility of crossing each bridge just once applies to any arrangement of bridges homeomorphic to those in Königsberg, and the hairy ball theorem applies to any space homeomorphic to a sphere.
Intuitively, two spaces are homeomorphic if one can be deformed into the other without cutting or gluing. A traditional joke is that a topologist cannot distinguish a coffee mug from a doughnut, since a sufficiently pliable doughnut could be reshaped to a coffee cup by creating a dimple and progressively enlarging it, while shrinking the hole into a handle.
Homeomorphism can be considered the most basic "topological equivalence". Another is homotopy equivalence. This is harder to describe without getting technical, but the essential notion is that two objects are homotopy equivalent if they both result from "squishing" some larger object.
An introductory exercise is to classify the uppercase letters of the English alphabet according to homeomorphism and homotopy equivalence. The result depends partially on the font used. The figures use the sans-serif Myriad font. Homotopy equivalence is a rougher relationship than homeomorphism; a homotopy equivalence class can contain several homeomorphism classes. The simple case of homotopy equivalence described above can be used here to show two letters are homotopy equivalent. For example, O fits inside P and the tail of the P can be squished to the "hole" part.
Homeomorphism classes are:
Homotopy classes are larger, because the tails can be squished down to a point. They are:
To be sure that the letters are classified correctly, we need to show that two letters in the same class are equivalent and two letters in different classes are not equivalent. In the case of homeomorphism, this can be done by selecting points and showing their removal disconnects the letters differently. For example, X and Y are not homeomorphic because removing the center point of the X leaves four pieces; whatever point in Y corresponds to this point, its removal can leave at most three pieces. The case of homotopy equivalence is harder and requires a more elaborate argument showing an algebraic invariant, such as the fundamental group, is different on the supposedly differing classes.
Letter topology has practical relevance in stencil typography. For instance, Braggadocio font stencils are made of one connected piece of material.
Concepts.
Topologies on Sets.
The term topology also refers to a specific mathematical idea which is central to the area of mathematics called topology. Informally, a topology is used to tell how elements of a set are related spatially to each other. The same set can have different topologies. For instance, the real line, the complex plane, and the Cantor set can be thought of as the same set with different topologies.
Formally, let "X" be a set and let "τ" be a family of subsets of "X". Then "τ" is called a "topology on X" if:
If "τ" is a topology on "X", then the pair ("X", "τ") is called a "topological space". The notation "Xτ" may be used to denote a set "X" endowed with the particular topology "τ".
The members of "τ" are called "open sets" in "X". A subset of "X" is said to be closed if its complement is in "τ" (i.e., its complement is open). A subset of "X" may be open, closed, both (clopen set), or neither. The empty set and "X" itself are always both closed and open. An open set containing a point "x" is called a 'neighborhood' of "x".
A set with a topology is called a topological space.
Continuous functions and homeomorphisms.
A function or map from one topological space to another is called "continuous" if the inverse image of any open set is open. If the function maps the real numbers to the real numbers (both spaces with the Standard Topology), then this definition of continuous is equivalent to the definition of continuous in calculus. If a continuous function is one-to-one and onto, and if the inverse of the function is also continuous, then the function is called a homeomorphism and the domain of the function is said to be homeomorphic to the range. Another way of saying this is that the function has a natural extension to the topology. If two spaces are homeomorphic, they have identical topological properties, and are considered topologically the same. The cube and the sphere are homeomorphic, as are the coffee cup and the doughnut. But the circle is not homeomorphic to the doughnut.
Manifolds.
While topological spaces can be extremely varied and exotic, many areas of topology focus on the more familiar class of spaces known as manifolds. A manifold is a topological space that resembles Euclidean space near each point. More precisely, each point of an "n"-dimensional manifold has a neighbourhood that is homeomorphic to the Euclidean space of dimension "n". Lines and circles, but not figure eights, are one-dimensional manifolds. Two-dimensional manifolds are also called surfaces. Examples include the plane, the sphere, and the torus, which can all be realized in three dimensions, but also the Klein bottle and real projective plane which cannot.
Topics.
General topology.
General topology is the branch of topology dealing with the basic set-theoretic definitions and constructions used in topology. It is the foundation of most other branches of topology, including differential topology, geometric topology, and algebraic topology. Another name for general topology is point-set topology.
The fundamental concepts in point-set topology are "continuity", "compactness", and "connectedness". Intuitively, continuous functions take nearby points to nearby points; compact sets are those which can be covered by finitely many sets of arbitrarily small size; and connected sets are sets which cannot be divided into two pieces which are far apart. The words 'nearby', 'arbitrarily small', and 'far apart' can all be made precise by using open sets, as described below. If we change the definition of 'open set', we change what continuous functions, compact sets, and connected sets are. Each choice of definition for 'open set' is called a "topology". A set with a topology is called a "topological space".
"Metric spaces" are an important class of topological spaces where distances can be assigned a number called a "metric". Having a metric simplifies many proofs, and many of the most common topological spaces are metric spaces.
Algebraic topology.
Algebraic topology is a branch of mathematics that uses tools from abstract algebra to study topological spaces. The basic goal is to find algebraic invariants that classify topological spaces up to homeomorphism, though usually most classify up to homotopy equivalence.
The most important of these invariants are homotopy groups, homology, and cohomology.
Although algebraic topology primarily uses algebra to study topological problems, using topology to solve algebraic problems is sometimes also possible. Algebraic topology, for example, allows for a convenient proof that any subgroup of a free group is again a free group.
Differential topology.
Differential topology is the field dealing with differentiable functions on differentiable manifolds. It is closely related to differential geometry and together they make up the geometric theory of differentiable manifolds.
More specifically, differential topology considers the properties and structures that require only a smooth structure on a manifold to be defined. Smooth manifolds are 'softer' than manifolds with extra geometric structures, which can act as obstructions to certain types of equivalences and deformations that exist in differential topology. For instance, volume and Riemannian curvature are invariants that can distinguish different geometric structures on the same smooth manifold—that is, one can smoothly "flatten out" certain manifolds, but it might require distorting the space and affecting the curvature or volume.
Geometric topology.
Geometric topology is a branch of topology that primarily focuses on low-dimensional manifolds (i.e. dimensions 2,3 and 4) and their interaction with geometry, but it also includes some higher-dimensional topology.
 Some examples of topics in geometric topology are orientability, handle decompositions, local flatness, and the planar and higher-dimensional Schönflies theorem.
In high-dimensional topology, characteristic classes are a basic invariant, and surgery theory is a key theory.
Low-dimensional topology is strongly geometric, as reflected in the uniformization theorem in 2 dimensions – every surface admits a constant curvature metric; geometrically, it has one of 3 possible geometries: positive curvature/spherical, zero curvature/flat, negative curvature/hyperbolic – and the geometrization conjecture (now theorem) in 3 dimensions – every 3-manifold can be cut into pieces, each of which has one of 8 possible geometries.
2-dimensional topology can be studied as complex geometry in one variable (Riemann surfaces are complex curves) – by the uniformization theorem every conformal class of metrics is equivalent to a unique complex one, and 4-dimensional topology can be studied from the point of view of complex geometry in two variables (complex surfaces), though not every 4-manifold admits a complex structure.
Generalizations.
Occasionally, one needs to use the tools of topology but a "set of points" is not available. In pointless topology one considers instead the lattice of open sets as the basic notion of the theory, while Grothendieck topologies are structures defined on arbitrary categories that allow the definition of sheaves on those categories, and with that the definition of general cohomology theories.
Applications.
Biology.
Knot theory, a branch of topology, is used in biology to study the effects of certain enzymes on DNA. These enzymes cut, twist, and reconnect the DNA, causing knotting with observable effects such as slower electrophoresis. Topology is also used in evolutionary biology to represent the relationship between phenotype and genotype. Phenotypic forms which appear quite different can be separated by only a few mutations depending on how genetic changes map to phenotypic changes during development.
Computer science.
Topological data analysis uses techniques from algebraic topology to determine the large scale structure of a set (for instance, determining if a cloud of points is spherical or toroidal). The main method used by topological data analysis is:
Physics.
In physics, topology is used in several areas such as quantum field theory and cosmology.
A topological quantum field theory (or topological field theory or TQFT) is a quantum field theory which computes topological invariants.
Although TQFTs were invented by physicists, they are also of mathematical interest, being related to, among other things, knot theory and the theory of four-manifolds in algebraic topology, and to the theory of moduli spaces in algebraic geometry. Donaldson, Jones, Witten, and Kontsevich have all won Fields Medals for work related to topological field theory.
In cosmology, topology can be used to describe the overall shape of the universe. This area is known as spacetime topology.
Robotics.
The various possible positions of a robot can be described by a manifold called configuration space. In the area of motion planning, one finds paths between two points in configuration space. These paths represent a motion of the robot's joints and other parts into the desired location and pose.

</doc>
<doc id="29957" url="http://en.wikipedia.org/wiki?curid=29957" title="Troll">
Troll

A troll is a supernatural being in Norse mythology and Scandinavian folklore. In origin, "troll" may have been a negative synonym for a "jötunn" (plural "jötnar"). In Old Norse sources, beings described as trolls dwell in isolated rocks, mountains, or caves, live together in small family units, and are rarely helpful to human beings.
Later, in Scandinavian folklore, trolls became beings in their own right, where they live far from human habitation, are not Christianized, and are considered dangerous to human beings. Depending on the region from which accounts of trolls stem, their appearance varies greatly; trolls may be ugly and slow-witted, or look and behave exactly like human beings, with no particularly grotesque characteristic about them.
Trolls are sometimes associated with particular landmarks, which at times may be explained as formed from a troll exposed to sunlight (e.g., Risin og Kellingin). One of the most famous elements of Scandinavian folklore, trolls are depicted in a variety of media in modern popular culture.
Norse mythology.
In Norse mythology, "troll", like "thurs", is a term applied to "jötnar", and are mentioned throughout the Old Norse corpus. In Old Norse sources, trolls are said to dwell in isolated mountains, rocks, and caves, sometimes live together (usually as father-and-daughter or mother-and-son), and are rarely described as helpful or friendly. In the "Prose Edda" book "Skáldskaparmál", a scenario describing an encounter between an unnamed troll woman and the 9th century skald Bragi Boddason is provided. According to the section, once, late in the evening, Bragi was driving through "a certain forest" when a troll woman aggressively asked him who he was, in the process describing herself:
Bragi responds in turn, describing himself and his abilities as a skillful skald, before the scenario ends.
There is much confusion and overlap in the use of Old Norse terms "jötunn", "troll", "þurs" and "risi", which describe various beings. Lotte Motz theorized that these were originally four distinct classes of beings; lords of nature ("jötunn"), mythical magicians ("troll"), hostile monsters ("þurs") and heroic and courtly beings ("risi")—the last class being the youngest addition. Ármann Jakobsson calls this theory "unsupported by any convincing evidence". He has gone on to study the Old Norse examples of the term "troll" and has concluded that in the Middle Ages, the term is used to denote various beings such as a giant or mountain-dweller, a witch, an abnormally strong or large or ugly person, an evil spirit, a ghost, a blámaðr, a magical boar, a heathen demi-god, a demon, a brunnmigi or a berserk.
Scandinavian folklore.
Later, in Scandinavian folklore, trolls become defined as a particular type of being. Numerous tales about trolls are recorded, in which they are frequently described as being extremely old, very strong, but slow and dim-witted, and are at times described as man-eaters and as turning to stone upon contact with sunlight. However, trolls are also attested as looking much the same as human beings, without any particularly hideous appearance about them, but where they differ is in that they live far away from human habitation, and, unlike the "rå" and "näck"—who are attested as "solitary beings", trolls generally have "some form of social organization". Where they differ, Lindow adds, is that they are not Christian, and those that encounter them do not know them. Therefore trolls were in the end dangerous, regardless of how well they may get along with Christian society, and trolls display a habit of "bergtagning" ('kidnapping'; literally "mountain-taking") and overrunning a farm or estate.
While noting that the etymology of the word "troll" remains uncertain, John Lindow defines trolls in later Swedish folklore as "nature beings" and as "all-purpose otherworldly being[s], equivalent, for example, to fairies in Anglo-Celtic traditions" and that they "therefore appear in various migratory legends where collective nature-beings are called for". Lindow notes that trolls are sometimes swapped out for cats and "little people" in the folklore record.
A Scandinavian folk belief that lightning frightens away trolls and jötnar appears in numerous Scandinavian folktales, and may be a late reflection of the god Thor's role in fighting such beings. In connection, the lack of trolls and jötnar in modern Scandinavia is sometimes explained as a result of the "accuracy and efficiency of the lightning strokes". Additionally, the absence of trolls in regions of Scandinavia are described in folklore as being a "consequence of the constant din of the church-bells". This ring caused the trolls to leave for other lands, although not without some resistance; numerous traditions relate how trolls destroyed a church under construction or hurled boulders and stones at completed churches. Large local stones are sometimes described as the product of a troll's toss. Additionally, into the 20th century, the origins of particular Scandinavian landmarks, such as particular stones, are ascribed to trolls who may, for example, have turned to stone upon exposure to sunlight.
Lindow compares the trolls of the Swedish folk tradition to Grendel, the supernatural mead hall invader in the Old English poem "Beowulf", and notes that "just as the poem "Beowulf" emphasizes not the harrying of Grendel but the cleansing of the hall of Beowulf, so the modern tales stress the moment when the trolls are driven off."
Smaller trolls are attested as living in burial mounds and in mountains in Scandinavian folk tradition. In Denmark, these creatures are recorded as "troldfolk" ("troll-folk"), "bjergtrolde" ("mountain-trolls"), or "bjergfolk" ("mountain-folk") and in Norway also as "troldfolk" ("troll-folk") and "tusser". Trolls may be described as small, human-like beings or as tall as men depending on the region of origin of the story.
In Norwegian tradition, similar tales may be told about the larger trolls and the Huldrefolk ("hidden-folk") yet a distinction is made between the two. The use of the word "trow" in Orkney and Shetland, to mean beings which are very like the Huldrefolk in Norway may suggest a common origin for the terms. The word "troll" may have been used by pagan Norse settlers in Orkney and Shetland as a collective term for supernatural beings who should be respected and avoided rather than worshiped. "Troll" could later have become specialized as a description of the larger, more menacing Jötunn-kind whereas "Huldrefolk" may have developed as the general term applied to smaller trolls.
It has been remarked that stories about trolls were exploited by national romantics in the nineteenth and early twentieth centuries who sought to construct a national past and thus a national image from apparently ancient and unsullied rural traditions. James MacCulloch posits a connection between the Old Norse vættir and trolls, suggesting that both concepts may derive from spirits of the dead.
References.
</dl>

</doc>
<doc id="29967" url="http://en.wikipedia.org/wiki?curid=29967" title="Tarragon">
Tarragon

Tarragon (Artemisia dracunculus) is a species of perennial herb in the sunflower family. It is widespread in the wild across much of Eurasia and North America, and is cultivated for culinary and medicinal purposes in many lands.
One sub-species, "Artemisia dracunculus var. sativa", is cultivated for use of the leaves as an aromatic culinary herb. In some other sub-species, the characteristic aroma is largely absent. The species is polymorphic. Informal names for distinguishing the variations include "French tarragon" (best for culinary use), "Russian tarragon" (typically better than wild tarragon but not as good as so-called French tarragon for culinary use), and "wild tarragon" (covers various states).
Tarragon grows to 120–150 cm tall, with slender branched stems. The leaves are lanceolate, 2–8 cm long and 2–10 mm broad, glossy green, with an entire margin. The flowers are produced in small capitulae 2–4 mm diameter, each capitulum containing up to 40 yellow or greenish-yellow florets. French tarragon, however, seldom produces any flowers (or seeds). Some tarragon plants produce seeds that are generally only sterile. Others produce viable seeds. Tarragon has rhizomatous roots and it readily reproduces from the rhizomes.
The name "tarragon" is believed to have been borrowed from the Persian name for tarragon which is ترخون "tarkhūn".
Cultivation.
French tarragon is the variety generally considered best for the kitchen, but is never grown from seed as the flowers are sterile; instead it is propagated by root division. It is normally purchased as a plant, and some care must be taken to ensure that true French tarragon is purchased. A perennial, it normally goes dormant in winter. It likes a hot, sunny spot, without excessive watering.
Russian tarragon ("A. dracunculoides" L.) can be grown from seed but is much weaker in flavor when compared to the French variety. However, Russian tarragon is a far more hardy and vigorous plant, spreading at the roots and growing over a meter tall. This tarragon actually prefers poor soils and happily tolerates drought and neglect. It is not as strongly aromatic and flavorsome as its French cousin, but it produces many more leaves from early spring onwards that are mild and good in salads and cooked food. Russian tarragon loses what flavor it has as it ages and is widely considered useless as a culinary herb, though it is sometimes used in crafts. The young stems in early spring can be cooked as an asparagus substitute. Horticulturists recommend that Russian tarragon be grown indoors from seed and planted out in the summer. The spreading plants can be divided easily.
A better substitute for French tarragon is Spanish tarragon ("Tagetes lucida"), also known as Mexican mint marigold, Mexican tarragon, Texas tarragon, or winter tarragon. It is much more reminiscent of French tarragon, with a hint of anise. Although not in the same genus as the other tarragons, Spanish tarragon has a stronger flavor than Russian tarragon that does not diminish significantly with age.
Health.
Tarragon has an aromatic property reminiscent of anise, due to the presence of estragole, a known carcinogen and teratogen in mice. The European Union investigation revealed that the danger of estragole is minimal even at 100–1,000 times the typical consumption seen in humans. Estragole concentration in fresh tarragon leaves is about 2900 mg/kg.
Uses.
Culinary use.
Tarragon is one of the four "fines herbes" of French cooking, and is particularly suitable for chicken, fish and egg dishes. Tarragon is the main flavoring component of Béarnaise sauce. Fresh, lightly bruised sprigs of tarragon are steeped in vinegar to produce tarragon vinegar.
Tarragon is used to flavor a popular carbonated soft drink in the countries of Armenia, Azerbaijan, Georgia and, by extension, Russia, Ukraine and Kazakhstan. The drink, named Tarhun (] Թարխուն), is made out of sugary tarragon concentrate and colored bright green.
In Iran, tarragon is used as a side dish in sabzi khordan (fresh herbs), or in stews and in Persian style pickles, particularly 'khiar shoor'.
In Slovenia, tarragon is used in a variation of the traditional nut roll sweet cake, called "potica". In Hungary a popular kind of chicken soup is flavored with tarragon.
"cis"-Pellitorin, an isobutyramide eliciting a pungent taste, has been isolated from the tarragon plant.
Chemistry.
"A. dracunculus" oil contained predominantly phenylpropanoids such as methyl chavicol (16.2%) and methyl eugenol (35.8%). Gas chromatography/mass spectrometry analysis of the essential oil revealed the presence of "trans"-anethole (21.1%), α-"trans"-ocimene (20.6%), limonene (12.4%), α-pinene (5.1%), "allo"-ocimene (4.8%), methyl eugenol (2.2%), β-pinene (0.8%), α-terpinolene (0.5%), bornyl acetate (0.5%) and bicyclogermacrene (0.5%) as the main components.
References.
Notes

</doc>
<doc id="29987" url="http://en.wikipedia.org/wiki?curid=29987" title="The Penguins">
The Penguins

The Penguins were an American doo-wop group of the 1950s and early 1960s, best remembered for their only Top 40 hit, "Earth Angel (Will You Be Mine)", which was one of the first rhythm and blues hits to cross over to the pop charts. The song peaked at #8 on the "Billboard" Hot 100 chart, but had a three-week run at #1 on the R&B chart.
Early career.
The original members of The Penguins were Cleveland Duncan (July 23, 1935 – November 7, 2012), Curtis Williams (December 11, 1934 – August 10, 1979), Dexter Tisby (March 10, 1935) and Bruce Tate (January 27, 1937 – June 20, 1973). Duncan and Williams were former classmates at Fremont High School in Los Angeles, California, and Williams had become a member of The Hollywood Flames. In late 1953, they decided to form a new vocal group, and added Tisby and Tate. Their midtempo performance style was a cross between rhythm and blues and rock and roll. Williams brought with him a song, "Earth Angel", on which he had worked with Gaynel Hodge, another member of the Hollywood Flames.
The Penguins were one of a number of doo-wop groups of the period named after birds (such as The Orioles, The Flamingos, and The Crows). One of the members smoked Kool cigarettes, which, at the time, had "Willie the Penguin" as its cartoon advertising character. They considered themselves "cool", and accordingly decided to call themselves "The Penguins".
Dootone Records released The Penguins' single "Hey Senorita" in late 1954 as the intended A-side, but a radio DJ flipped the record over to the B-side: "Earth Angel" worked its way up to #1 on the Billboard charts (the only Penguins song ever to chart that high), and held that place for three weeks early in 1955. The Penguins followed up this hit with a Christmas release "A Christmas prayer" with "Jingle jangle."
Duncan sang lead on "Earth Angel". He reprised his performance a decade later on Frank Zappa's "Memories of El Monte", an elegiac 1963 song in which he suddenly breaks into "Earth Angel" as one of the various songs remembered. El Monte, a city near Los Angeles, had spawned such popular performers as Tony Allan, Marvin & Johnny, The Shields, as well as the Penguins. Those groups were also emulated as part of Zappa's tribute to early days of rock and roll.
In a common practice of the time, radio stations frequently featured segregated playlists. Thus, "Earth Angel" was simultaneously recorded by the white group, The Crew-Cuts in 1955. The Crew-Cuts cover peaked at #3 on the Hot 100 chart, five spots higher than the Penguins version. The single's success contributed to the Crew-Cuts' own successful career of recording crossover-friendly covers of R&B hits.
The songwriting genesis for "Earth Angel" was a matter of some dispute, eventually ending up in a split credit between Penguins baritone Curtis Williams, Gaynel Hodge, and Jesse Belvin. The song had evolved through several Los Angeles area groups, and was based on the "Blue Moon" chord changes that were so popular with many doo-wop groups. The song was influenced by Jesse and Marvin's #2 R&B hit "Dream Girl", which contained many of the same vocal inflections used to great effect in "Earth Angel". The "Will you be mine?" hook in "Earth Angel", which was also the song's subtitle, was borrowed from the #9 R&B hit of the same name by The Swallows. The Hollywood Flames had also recorded "I Know" in 1953, a song which has been called a chord-for-chord blueprint for "Earth Angel", and which featured the same Curtis Williams piano intro that Williams himself reused on the Penguins hit. The coda of "Earth Angel", with the repeatedly harmonized word "You-oo... you-oo... you-oo... you-oo", had previously been heard in the Dominoes' #5 R&B cover of "These Foolish Things Remind Me Of You".
After "Earth Angel".
Coming off the success of "Earth Angel", the Penguins approached Buck Ram to manage them. Ram's primary interest was in managing the Platters, who at that point had no hit singles, but were a profitable touring group. With the Penguins in hand, Ram was able to swing a 2-for-1 deal with Mercury Records, in which the company agreed to take on the Platters as a condition for getting the Penguins (the group that Mercury really wanted). Ironically, the Platters became the label's more successful act, the Penguins never scoring a second hit single.
In 1955, Bruce Tate left the group. He was replaced by Randy Jones (who would later sing with the Cadets). During the summer of 1956, Jones and Tisby were briefly out of the group, and were replaced by Ray Brewster and Teddy Harper, respectively. Jones and Tisby returned shortly afterwards. Curtis Williams left in December 1957, with Harper rejoining as his permanent replacement. The Penguins never had another national hit, but their 1957 cover of "Pledge of Love" reached #15 on the R&B chart.
Later years.
The group broke up in 1962. Cleveland Duncan continued recording as "The Penguins", with new member Walter Saulsberry and a backing group, the Viceroys. Later, the group was Duncan, Saulsberry, Vesta and Evelyn King, and Vera Walker. (Duncan and the King sisters had recorded a record as "Cleve Duncan and the Radiants" in 1959.) By the late 1960s, the group was being billed as the "Fabulous Penguins", and featured Duncan, Walker, and new member Rudy Wilson. By the 1970s, the members were Duncan, the returning Walter Saulsberry, and new member Glenn Madison, formerly of the Delco's (Indiana). This was the current line up of the group until 2012.
The group performed on the PBS television special, "Doo Wop 50". Duncan, Madison, and Saulsberry also performed with Randy Jones as guest, in 2001. It was planned for Jones to appear with the Penguins the following year but he suffered a stroke while rehearsing with the group and died shortly thereafter. Jones also performed with the reunited Jacks/Cadets in the 1990s. Duncan died on November 7, 2012, in Los Angeles at the age of 77.
The group is mentioned in the Paul Simon song "Rene and Georgette Magritte with Their Dog after the War".
Award.
The Penguins were inducted into The Vocal Group Hall of Fame in 2004.

</doc>
<doc id="29990" url="http://en.wikipedia.org/wiki?curid=29990" title="Titanic Thompson">
Titanic Thompson

Alvin Clarence Thomas (November 30, 1893 – May 19, 1974) was an American gambler, golfer and hustler better known as Titanic Thompson. He was a major witness at the 1929 Arnold Rothstein murder trial in New York City.
Overview.
He traveled the country wagering at cards, dice games, golf, shooting, billiards, horseshoes and proposition bets of his own devising. As an ambidextrous golfer, card player, marksman and pool shark, his skills and reputation were compared to "Merlin himself". Writer Damon Runyon allegedly based the character Sky Masterson, the gambler-hero of "The Idyll of Miss Sarah Brown" (on which the musical "Guys and Dolls" is based), on Thompson. In 1928, Thompson was involved in a high-stakes poker game that led to the shooting death of New York City crime boss Arnold "the Brain" Rothstein, then called the "crime of the century". The following year he testified in the trial of George "Hump" McManus, who was charged with Rothstein's murder.
Early life.
Thomas was born in southwestern Missouri but raised mainly on a farm in the Ozark Mountains a few miles from Rogers, Arkansas, 50 miles further south. His mother remarried (following desertion by his father Lee Thomas, a gambler himself). Thomas began conducting his nomadic, lucrative career of hustling in the rural south-central United States about 1908, leaving home at age 16 with less than a dollar in his pocket. Unable to read or write effectively, he had attended school only sporadically, and felt unwelcome in the home of his stepfather. Thomas spent most of his youth developing skills he would use later, such as shooting and understanding odds at card games through marathon dealing of hands.
Military service.
Thomas was drafted in early 1918, several months after the United States entered World War I. Following basic training, where he excelled, he was promoted to the rank of sergeant. Thomas remained stateside, trained younger draftees, and did not see overseas service or combat before the war ended in November 1918, when he was discharged. Thomas also taught gambling skills to many of his trainees, and then proceeded to win substantial money from them. He ended the war with more than $50,000 in cash, and used much of this money to buy his mother a house in Monett, Missouri, his birthplace.
Gambling style, and favorite bets.
Later, when he had honed his skills, he became a "road gambler", a traveling hustler who became an underground legend by winning at all manner of propositions, many of them tricky if not outright fraudulent. Among his favorites were: betting he could throw a Walnut over a building (he had weighted the hollowed shell with lead beforehand), throwing a large room key into its lock, and moving a road mileage sign before betting that the listed distance to the town was in error. He once bet that he could drive a golf ball 500 yards, using a hickory-shafted club, at a time when an expert player's drive was just over 200 yards. He won by waiting until winter and driving the ball onto a frozen lake, where it bounced past the required distance on the ice.
His partners in "the hustling game" included pool player Minnesota Fats, who considered Titanic a genius, "the greatest action man of all time".
Thompson's one weakness, as he admitted, was betting on horse racing, where he lost millions of dollars during his life in failed bets.
Becomes expert golfer.
Blessed with extraordinary eyesight and hand-eye coordination, he was a skilled athlete, crack shot and self-taught golfer good enough to turn professional. Raised in a poor environment far from exclusive golf courses, Thomas did not take up golf seriously until he was in his early thirties, but improved very quickly during an extended stint in San Francisco, where he took lessons from club professionals and honed his skills. From then on he played several times per week for the next 20 years. In an era when the top pro golfers would be fortunate to make $30,000 a year, Thomas (who, after a misprint in a New York newspaper, let people think his name was Thompson) could make that much in a week hustling rich country club players. Asked whether he would ever turn professional, he replied, "I could not afford the cut in pay". Hall of Fame golfer Ben Hogan, who traveled with him in the early 1930s for money games, later called Titanic the best shotmaker he ever saw. "He can play right- or left-handed, you can't beat him", said Hogan. One hustle of his was to beat a golfer playing right-handed, and then offer double or nothing to play the course again left-handed as an apparent concession. One thing his opponent usually did not know was that Thomas was naturally left-handed. Thomas' genius was in figuring out the odds on almost any proposition and heavily betting that way. He also had to perform under pressure, and most often did.
As he aged, Thompson liked to pick promising young players as his golf partners. Several of these who went on to later PGA Tour stardom included young and unknown Ben Hogan, Ky Laffoon, Herman Keiser and Lee Elder. Other well-known golfers who left behind first-hand documented accounts of their dealings and matches with Thompson included Harvey Penick, Paul Runyan, Byron Nelson and Sam Snead, all of whom were inducted into the World Golf Hall of Fame.
Marriages and family.
Married five times, Thompson fathered three children, all boys, with three different wives. He was also romantically linked with many women. Among his alleged trysts were actresses Myrna Loy and Jean Harlow. He typically married a young woman, lived with her for a few months, then returned to his road hustling, while leaving comfortable housing and financial support for his newly divorced wife.
Killings.
In his life Thompson killed five men. The first was in 1910, in rural Arkansas, when a man called Jim Johnson accused him of cheating at dice and threw him off the boat on which they were traveling (and which Thompson had recently won when gambling with its previous owner—a friend of Johnson's). When Thompson climbed back on board, Johnson drew a knife and threatened Thompson's girlfriend, who was also on board. Thompson seized a hammer and struck Johnson several times on the head before throwing him overboard. The unconscious Johnson drowned. Thompson showed no remorse, stating that it was Johnson's fault for not being able to swim. The sheriff gave Thompson the choice of standing trial or handing over the deed to the boat and leaving town, which he chose.
The other four men Thompson killed were shot by him in self-defense when they tried to rob him of gambling winnings. Two were killed in one incident in St. Louis in 1919 (the local police chief thanked him for killing two wanted bank robbers). The fourth came in St. Joseph, Missouri where Thompson and his hired bodyguard between them shot two men attempting to rob a poker game (again, the victims were known criminals and no charges were pressed). Thompson's last killing came near a country club in Texas in 1932 when he shot a masked figure who was holding him at gunpoint. This turned out to be sixteen-year-old Jimmy Frederick, who had caddied for Thompson earlier that day in a winning match. The dying Frederick confirmed to witnesses that he had been trying to rob Thompson.
Arnold Rothstein case.
On November 4, 1928, Arnold Rothstein was murdered, allegedly because he refused to pay his debts from a poker game, held several months earlier, that he believed to have been fixed. This game had been organized by George McManus, who stood trial for the murder the next year, in a proceeding heavily covered by the media. McManus was eventually acquitted, and no one else was ever tried for Rothstein's death. Thompson had been present at the game, and an active participant in it; and it was he who, in association with one Nate Raymond, allegedly fixed the game, leaving Rothstein with total debts estimated at $500,000. Thompson, who was not present at the shooting, gave evidence at McManus' trial, without revealing his own role in the poker game. Ironically Rothstein had stood to recoup his losses by successful heavy bets on the 1928 elections of Herbert Hoover (new president) and Franklin Delano Roosevelt (new governor of New York), which actually did take place, shortly after Rothstein's death. Thompson later told close friends that he knew the real killer had been Rothstein's bodyguard.
Origin of the nickname.
In his own story, published in "Sports Illustrated" in 1972, Alvin Thomas, listed as a co-author, said:
In the spring of 1912 I went to Joplin, Missouri, just about the time the Titanic liner hit an iceberg and sank with more than 1,500 people on board. I was in a pool room there and beat a fellow named Snow Clark out of $500. To give him a chance to get even, I bet $200 I could jump across his pool table without touching it. If you think that’s easy, try it. But I could jump farther than a herd of bullfrogs in those days. I put down an old mattress on the other side of the table. Then I took a run and dived headfirst across the pool table. While I was counting my money, somebody asked Clark what my name was "It must be Titanic," said Clark. "He sinks everybody." so I was Titanic from then on.
Trevino vs. Floyd match.
In the 1960s, Thompson settled in Dallas and, although approaching 70 years of age, kept up a good standard of golf, and frequently hustled games at Tenison Park, a municipal golf course, and at posh Glen Lakes Country Club. Mid-decade, Thompson sponsored a young Raymond Floyd, then early in his PGA Tour career but already a winner, in a big money stakes match against Lee Trevino, then a completely unknown assistant pro, in El Paso, at Trevino's home course. After three days of play, honors and bets wound up all even, with both players well under par each round. Trevino gained confidence from the match, and within a few years became a Tour superstar himself, while Floyd's career also ascended.
Later years.
Thompson was honored at the first World Series of Poker in Las Vegas, Nevada in 1970. He lived out his final years in a nursing home near Dallas. Thompson had made gambling trips with eldest son Tommy for many years, but after his father died, Tommy, who had become a skilled, successful gambler in his own right, gave up gambling for a church ministry, with success counseling prisoners, preaching to convince others to stay away from gambling.

</doc>
<doc id="30000" url="http://en.wikipedia.org/wiki?curid=30000" title="Taxi Driver">
Taxi Driver

Taxi Driver is a 1976 American vigilante film with neo-noir and psychological thriller elements, directed by Martin Scorsese and written by Paul Schrader. Set in New York City soon after the end of the Vietnam War, the film stars Robert De Niro and features Jodie Foster, Harvey Keitel, Cybill Shepherd, Peter Boyle, and Albert Brooks.
The film is regularly cited by critics, film directors, and audiences alike as one of the greatest films of all time. Nominated for four Academy Awards, including Best Picture, it won the Palme d'Or at the 1976 Cannes Film Festival. The American Film Institute ranked "Taxi Driver" as the 52nd-greatest American film on its AFI's 100 Years…100 Movies (10th Anniversary Edition) list. In 2012, "Sight & Sound" named it the 31st-best film ever in its decennial critics' poll, ranked with "The Godfather Part II", and the fifth-greatest film of all time on its directors' poll. The film was considered "culturally, historically or aesthetically" significant by the US Library of Congress and was selected for preservation in the National Film Registry in 1994.
Plot.
Travis Bickle, an honorably discharged U.S. Marine, is a lonely and depressed man in New York City. He becomes a taxi driver to cope with chronic insomnia, driving passengers every night around the boroughs of New York City. He also spends time in seedy porn theaters and keeps a diary. Travis becomes infatuated with Betsy, a campaign volunteer for Senator and presidential candidate Charles Palantine. After watching her interact with fellow worker Tom through her window, Travis enters to volunteer as a pretext to talk to her, and takes her out for coffee. On a later date, he takes her to see a Swedish sex education film, which offends her, and she goes home alone. His attempts at reconciliation by sending flowers are rebuffed, so he berates her at the campaign office, before being kicked out by Tom.
Travis confides in fellow taxi driver Wizard about his thoughts, which are beginning to turn violent, but Wizard assures him that he will be fine. Disgusted by the street crime and prostitution that he witnesses throughout the city, Travis finds an outlet for his frustration and begins a program of intense physical training. He buys guns from dealer Easy Andy and constructs a sleeve gun to attach on his arm, with which he practices drawing his weapons. One night, Travis enters a convenience store moments before a man attempts to rob it, and he shoots the robber. The shop owner takes responsibility and Travis leaves. On another night, teenage prostitute Iris enters Travis's cab, attempting to escape her pimp Matthew "Sport" Higgins. Sport drags Iris from the cab and throws Travis a crumpled twenty-dollar bill, which continually reminds him of her. Some time later, Travis hires Iris, but instead of having sex with her, attempts to dissuade her from continuing in prostitution. He fails to completely turn her from her course, but she does agree to meet with him for breakfast the next day, and Travis becomes obsessed with helping her return to her parents' home. Travis leaves a letter to Iris at his apartment saying he will soon be dead, and inside the letter, money for her to return home.
After shaving his head into a mohawk, Travis attends a public rally, where he attempts to assassinate Senator Palantine, but Secret Service agents notice him and he flees without taking a shot. He returns to his apartment and then drives to the East Village, where he confronts Sport. Travis shoots Sport, then walks into Iris's brothel and shoots off the bouncer's fingers. After a wounded Sport shoots Travis, grazing his neck, Travis shoots and kills him. Iris's client, a mobster, appears and shoots Travis in the arm, but Travis reveals his sleeve gun and kills the gangster. The bouncer continues to harass Travis, causing Travis to stab him in the hand and shoot him in the head to kill him. As a horrified Iris cries, Travis attempts suicide but, out of ammunition, resigns himself to a sofa until police arrive. When they do, he places his index finger against his temple gesturing the act of shooting himself. While recuperating, Travis receives a letter from Iris's parents, who thank him for saving her, and the media hail him as a hero. Travis then returns to his job and encounters Betsy as a fare. She discusses his newfound fame, but he denies being a hero and drops her at her place free of charge. He glances at her in his rear view mirror as he drives away.
Production.
According to Scorsese, it was Brian De Palma who introduced him to Schrader. In "Scorsese on Scorsese", the director talks about how much of the film arose from his feeling that movies are like dreams or drug-induced reveries. He admits attempting to incubate within the viewer the feeling of being in a limbo state somewhere between sleeping and waking. He calls Travis an "avenging angel" floating through the streets of a New York City intended to represent all cities everywhere. Scorsese calls attention to improvisation in the film, such as in the scene between De Niro and Cybill Shepherd in the coffee shop. The director also cites Alfred Hitchcock's "The Wrong Man" and Jack Hazan's "A Bigger Splash" as inspirations for his camerawork in the movie.
In "Scorsese on Scorsese", the director mentions the religious symbolism in the story, comparing Bickle to a saint who wants to cleanse or purge both his mind and his body of weakness. Bickle attempts to kill himself near the end of the movie as a tribute to the samurai’s "death with honour" principle.
When Travis meets Betsy to join him for coffee and pie, she is reminded of a line in Kris Kristofferson's song "The Pilgrim, Chapter 33": "He's a prophet and a pusher, partly truth, partly fiction—a walking contradiction." On their date, Bickle takes her to see a Swedish sex education film, which is in fact the American sexplotation film "Sexual Freedom in Denmark" with added Swedish sound.
Shot during a New York summer heat wave and garbage strike, "Taxi Driver" came into conflict with the MPAA for its violence (Scorsese de-saturated the color in the final shoot-out, and the film got an R rating). To achieve the atmospheric scenes in Bickle's cab, the sound men would get in the trunk and Scorsese and his cinematographer, Michael Chapman, would ensconce themselves on the back seat floor and use available light to shoot.
In writing the script, Schrader was inspired by the diaries of Arthur Bremer (who shot presidential candidate George Wallace in 1972) and Fyodor Dostoyevsky’s "Notes from Underground". The writer also used himself as inspiration; prior to writing the screenplay, Schrader was in a lonely and alienated position, much like Bickle is. Following a divorce and a breakup with a live-in girlfriend, he spent a few weeks living in his car. He wrote the script in under a month while staying in his former girlfriend's apartment while she was away.
Schrader decided to make Bickle a Vietnam vet because the national trauma of the war seemed to blend perfectly with Bickle's paranoid psychosis, making his experiences after the war more intense and threatening. Thus, Bickle chooses to drive his taxi anywhere in the city as a way to feed his hate.
While preparing for his role as Bickle, De Niro was filming Bernardo Bertolucci's "1900" in Italy. According to Boyle, he would "finish shooting on a Friday in Rome ... get on a plane ... [and] fly to New York." De Niro obtained a cab driver's license, and when on break would pick up a cab and drive around New York for a couple of weeks, before returning to Rome to resume filming "1900". De Niro apparently lost 35 pounds and listened repeatedly to a taped reading of the diaries of Arthur Bremer. When he had time off from shooting "1900", De Niro visited an army base in Northern Italy and tape-recorded soldiers from the Midwestern United States, whose accents he thought might be appropriate for Travis's character.
When Bickle decides to assassinate Senator Palantine, he cuts his hair into a Mohawk. This detail was suggested by actor Victor Magnotta, a friend of Scorsese's who had a small role as a Secret Service agent and who had served in Vietnam. Scorsese later noted, "Magnotta had talked about certain types of soldiers going into the jungle. They cut their hair in a certain way; looked like a Mohawk ... and you knew that was a special situation, a commando kind of situation, and people gave them wide berths ... we thought it was a good idea."
Jodie Foster was not the first choice to play Iris. Scorsese considered Melanie Griffith, Linda Blair, Bo Derek, and Carrie Fisher for the role. A newcomer, Mariel Hemingway, auditioned for the role but turned it down due to pressure from her family. After the other actresses turned down the role as well, Foster, an experienced child actor, was chosen.
In the original draft, Schrader had written the role of Sport as a black man. There were also additions of other negative black roles. Scorsese believed that this would give the film an overly racist subtext, so they were changed to white roles. 
The Terminal Bar was featured in a scene in the film.
Schrader originally set the film in Los Angeles, but it was moved to New York City because taxicabs were much more prevalent there than in Los Angeles.
Taking place in an actual apartment, the tracking shot over the murder scene at the end took three months of preparation just because the production team had to cut through the ceiling in order to get it right.
Music.
The music by Bernard Herrmann was his final score before his death on December 24, 1975, and the film is dedicated to his memory. Robert Barnett of MusicWeb International has said that it contrasts deep, sleazy noises, representing the "scum" that Travis sees all over the city, with the saxophone, a musical counterpart to Travis, creating a mellifluously disenchanted troubadour. Barnett also observes that the opposing noises in the soundtrack—gritty little harp figures, hard as shards of steel, as well as a jazz drum kit placing the drama in the city—are indicative of loneliness in the midst of mobs of people. Deep brass and woodwinds are also evident. Barnett heard in the drumbeat a wild-eyed martial air charting the pressure on Bickle, who is increasingly oppressed by the corruption around him, and that the harp, drum, and saxophone play significant roles in the music.
Also featured in the film is Jackson Browne's "Late for the Sky", appearing in a scene where couples are dancing on the program "American Bandstand" to the song as Travis watches on his small TV.
The soundtrack for the film, re-released in 1998 on CD, includes an expanded version of the score as well as the tracks from the original 1976 LP. It also features album notes by director Martin Scorsese, as well as full documentation for the tracks, linking them in great detail to individual takes. Track 12, "Diary of a Taxi Driver", features Herrmann's music with De Niro's voice-over taken directly from the soundtrack.
Some of the tracks feature relatively long titles, representative of the fact that similar reprises are heard in many scenes.
Only the A-side of the 1976 soundtrack LP featured the Herrmann score. The B-side (tracks 14 through 18 on the CD) was dedicated to jazz "interpretations" of the score, arranged and conducted by Dave Blume. None of these recordings appeared in the film.
Controversies.
The climactic shoot-out was considered intensely graphic by few critics at the film's premiere. To attain an "R" rating, Scorsese had the colors de-saturated, making the brightly colored blood less prominent. In later interviews, Scorsese commented that he was actually pleased by the color change and considered it an improvement over the originally filmed scene, which has been lost. In the special-edition DVD, Michael Chapman, the film's cinematographer, regrets the decision and the fact that no print with the unmuted colors exists anymore, as the originals had long since deteriorated.
Some critics showed concern over 13-year-old Foster's presence during the climactic shoot-out. Foster said that she was present during the setup and staging of the special effects used during the scene; the entire process was explained and demonstrated for her, step by step. Moreover, Foster said, she was fascinated and entertained by the behind-the-scenes preparation that went into the scene. In addition, before being given the part, Foster was subjected to psychological testing to ensure that she would not be emotionally scarred by her role, in accordance with California Labor Board requirements.
Copies of the film distributed for TV broadcast had an unexplained disclaimer added during the closing credits:
To our Television Audience: In the aftermath of violence, the distinction between hero and villain is sometimes a matter of interpretation or misinterpretation of facts. "Taxi Driver" suggests that tragic errors can be made. The Filmmakers.
John Hinckley, Jr..
"Taxi Driver" formed part of the delusional fantasy of John Hinckley, Jr. that triggered his attempted assassination of President Ronald Reagan in 1981, an act for which he was found not guilty by reason of insanity. Hinckley stated that his actions were an attempt to impress actress Jodie Foster, on whom Hinckley was fixated, by mimicking Travis's mohawked appearance at the Palantine rally. His attorney concluded his defense by playing the movie for the jury.
Interpretations.
Sabine Haenni, a professor at Cornell University, commented on the film in her article "Geographies of Desire: Postsocial Urban Space and Historical Revision in the Films of Martin Scorsese" pg. 67: "While Taxi Driver chronicles Travis's excessive response to the perceived decline of the city, perhaps more fundamentally, the decline of the city seems to engender the decline of the male hero—Travis's inability to function in individual, collective, and heteronormative terms." 
Roger Ebert has written of the film's ending:
"There has been much discussion about the ending, in which we see newspaper clippings about Travis's 'heroism' of saving Iris, and then Betsy gets into his cab and seems to give him admiration instead of her earlier disgust. Is this a fantasy scene? Did Travis survive the shoot-out? Are we experiencing his dying thoughts? Can the sequence be accepted as literally true? ... I am not sure there can be an answer to these questions. The end sequence plays like music, not drama: It completes the story on an emotional, not a literal, level. We end not on carnage but on redemption, which is the goal of so many of Scorsese's characters."
James Berardinelli, in his review of the film, argues against the dream or fantasy interpretation, stating:"Scorsese and writer Paul Schrader append the perfect conclusion to "Taxi Driver". Steeped in irony, the five-minute epilogue underscores the vagaries of fate. The media builds Bickle into a hero, when, had he been a little quicker drawing his gun against Senator Palantine, he would have been reviled as an assassin. As the film closes, the misanthrope has been embraced as the model citizen—someone who takes on pimps, drug dealers, and mobsters to save one little girl."
On the Laserdisc audio commentary, Scorsese acknowledged several critics' interpretation of the film's ending as being Bickle's dying dream. He admits that the last scene of Bickle glancing at an unseen object implies that Bickle might fall into rage and recklessness in the future, and he is like "a ticking time bomb." Writer Paul Schrader confirms this in his commentary on the 30th-anniversary DVD, stating that Travis "is not cured by the movie's end," and that "he's not going to be a hero next time." When asked on the website Reddit about the film's ending, Schrader said that it was not to be taken as a dream sequence, but that he envisioned it as returning to the beginning of the film—as if the last frame "could be spliced to the first frame, and the movie started all over again."
Reaction.
Critical and box office reception.
Filmed on a budget of $1.3 million, "Taxi Driver" was a financial success, grossing $28,262,574 in the United States, making it the 17th-highest-grossing film of 1976.
Roger Ebert instantly praised it as one of the greatest films he had ever seen, claiming: "Taxi Driver" is a hell, from the opening shot of a cab emerging from stygian clouds of steam to the climactic killing scene in which the camera finally looks straight down. Scorsese wanted to look away from Travis's rejection; we almost want to look away from his life. But he's there, all right, and he's suffering.
It was also nominated for four Academy Awards, including Best Picture and Best Actor (De Niro), and received the Palme d'Or at the 1976 Cannes Film Festival. It has been selected for preservation in the United States National Film Registry. The film was chosen by "Time" as one of the 100 best films of all time.
As of 2014, Rotten Tomatoes gives the film a score of 98% based on reviews from 63 critics; the site's consensus states: "A must-see film for movie lovers, this Martin Scorsese masterpiece is as hard-hitting as it is compelling, with Robert De Niro at his best."
The July/August 2009 issue of "Film Comment" polled several critics on the best films to win the Palme d'Or at the Cannes Film Festival. "Taxi Driver" placed first, above films such as "Il Gattopardo", "Viridiana", "Blowup", "The Conversation", "Apocalypse Now", "La Dolce Vita", and "Pulp Fiction".
In the American Film Institute's top 50 movie villains of all time, Bickle was named the 30th greatest film villain. "Empire" also ranked him 18th in its "The 100 Greatest Movie Characters" poll.
Legacy.
"Taxi Driver", "American Gigolo", "Light Sleeper", and "The Walker" make up a series referred to variously as the "Man in a Room" or "Night Worker" films. Screenwriter Paul Schrader (who directed the latter three films) has said that he considers the central characters of the four films to be one character, who has changed as he has aged. The film also influenced the Charles Winkler film "You Talkin' to Me?"
In the 2012 film "Seven Psychopaths", psychotic Los Angeles actor Billy Bickle (Sam Rockwell) believes himself to be the illegitimate son of Travis Bickle.
Travis also appears as a minor supporting character in the 2012 graphic novel "".
In the Canadian television series "Trailer Park Boys", a man dressed as Travis makes an appearance in the episode "Jim Lahey is a Drunk Bastard", during the trailer park supervisor election.
"You talkin' to me?".
The catchphrase "You talkin' to me?" has become a pop culture mainstay. In 2005, it was ranked number 10 on the American Film Institute's AFI's 100 Years... 100 Movie Quotes.
In the corresponding scene, Bickle is looking into a mirror at himself, imagining a confrontation that would give him a chance to draw his gun. He says:
Roger Ebert called it "the truest line in the film... Travis Bickle's desperate need to make some kind of contact somehow—to share or mimic the effortless social interaction he sees all around him, but does not participate in."
Schrader does not take credit for the line, saying that his script only read, "Travis speaks to himself in the mirror", and that De Niro improvised the dialogue. However, Schrader went on to say that De Niro's performance was inspired by a routine by "an underground New York comedian" whom he had once seen, possibly including his signature line.
In his 2009 memoir, saxophonist Clarence Clemons said De Niro explained the line's origins when Clemons coached De Niro to play the saxophone for the 1977 film "New York, New York". Clemons said De Niro had seen Bruce Springsteen say the line onstage at a concert as fans were screaming his name, and decided to make the line his own.
Home media.
The first collector's edition (DVD), released in 1999, was packaged as a single-disc edition release. It contained special features, such as behind-the-scenes and several trailers, including one for "Taxi Driver".
In 2006, a 30th-anniversary 2-disc collector's edition was released. The first disc contains the film itself, two audio commentaries (one by writer Schrader and the other by Professor Robert Kolker), and trailers. This edition also retains some of the special features from the earlier release on the second disc, as well as some newly produced documentary material.
A Blu-ray was released on April 5, 2011 to commemorate the film's 35th anniversary. It includes the special features from the previous 2-disc collector's edition, plus an audio commentary by Scorsese released in 1991 for The Criterion Collection, previously released on Laserdisc.
As part of the Blu-ray production, Sony gave the film a full 4K digital restoration, which included scanning and cleaning the original negative (removing emulsion dirt and scratches). Colors were matched to director-approved prints under guidance from Scorsese and director of photography Michael Chapman. An all-new lossless DTS-HD Master Audio 5.1 soundtrack was also made from the original stereo recordings by Scorsese's personal sound team. The restored print premiered in February 2011 at the Berlin Film Festival, and to promote the Blu-ray, Sony also had the print screened at AMC Theatres across the United States on March 19 and 22.
Sequel/remake.
In late January 2005, a sequel was announced by De Niro and Scorsese. At a 25th-anniversary screening of "Raging Bull", De Niro talked about the story of an older Travis Bickle being in development. Also in 2000, De Niro mentioned interest in bringing back the character in conversation with Actors Studio host James Lipton. In November 2013, he revealed Schrader did a first draft but both him and Scorsese thought it wasn't good enough to go beyond.
At the Berlinale 2010, De Niro, Scorsese, and Lars von Trier announced plans to work on a remake of "Taxi Driver". The film will be produced in a similar manner to von Trier's "The Five Obstructions".
In December 2011, Scorsese was interviewed about combining his passion for 3D as a new medium with the legacy of older films, and said, "If I could go back in time, I'd shoot "Taxi Driver" in 3D. Bob De Niro in the mirror as Travis Bickle. Imagine how intimidating. 'You talking to me? You talking to me?' Amazing possibilities."

</doc>
<doc id="30042" url="http://en.wikipedia.org/wiki?curid=30042" title="Tin">
Tin

Tin is a chemical element with the symbol Sn (for Latin: "stannum") and atomic number 50. It is a main group metal in group 14 of the periodic table. Tin shows a chemical similarity to both neighboring group-14 elements, germanium and lead, and has two possible oxidation states, +2 and the slightly more stable +4. Tin is the 49th most abundant element and has, with 10 stable isotopes, the largest number of stable isotopes in the periodic table. It is a silvery, malleable other metal that is not easily oxidized in air, obtained chiefly from the mineral cassiterite where it occurs as tin dioxide, SnO2.
The first alloy used on a large scale since 3000 BC was bronze, an alloy of tin and copper. After 600 BC, pure metallic tin was produced. Pewter, which is an alloy of 85–90% tin with the remainder commonly consisting of copper, antimony and lead, was used for flatware from the Bronze Age until the 20th century. In modern times, tin is used in many alloys, most notably tin/lead soft solders, which are typically 60% or more tin. Another large application for tin is corrosion-resistant tin plating of steel. Because of its low toxicity, tin-plated metal is commonly used for food packaging as tin cans, which are made mostly of steel.
Characteristics.
Physical properties.
Tin is a malleable, ductile and highly crystalline silvery-white metal. When a bar of tin is bent, a crackling sound known as the tin cry can be heard due to the twinning of the crystals. Tin melts at a low temperature of about 232 C, which is further reduced to 177.3 C for 11-nm particles.
β-tin (the metallic form, or white tin), which is stable at and above room temperature, is malleable. In contrast, α-tin (nonmetallic form, or gray tin), which is stable below 13.2 C, is brittle. α-tin has a diamond cubic crystal structure, similar to diamond, silicon or germanium. α-tin has no metallic properties at all because its atoms form a covalent structure where electrons cannot move freely. It is a dull-gray powdery material with no common uses, other than a few specialized semiconductor applications. These two allotropes, α-tin and β-tin, are more commonly known as "gray tin" and "white tin", respectively. Two more allotropes, γ and σ, exist at temperatures above 161 C  and pressures above several GPa. In cold conditions, β-tin tends to transform spontaneously into α-tin, a phenomenon known as "tin pest". Although the α-β transformation temperature is nominally 13.2 C, impurities (e.g. Al, Zn, etc.) lower the transition temperature well below 0 C and, on the addition of Sb or Bi, the transformation may not occur at all, increasing the durability of the tin.
Commercial grades of tin (99.8%) resist transformation because of the inhibiting effect of the small amounts of bismuth, antimony, lead and silver present as impurities. Alloying elements such as copper, antimony, bismuth, cadmium and silver increase its hardness. Tin tends rather easily to form hard, brittle intermetallic phases, which are often undesirable. It does not form wide solid solution ranges in other metals in general, and there are few elements that have appreciable solid solubility in tin. Simple eutectic systems, however, occur with bismuth, gallium, lead, thallium and zinc.
Tin becomes a superconductor below 3.72 K. In fact, tin was one of the first superconductors to be studied; the Meissner effect, one of the characteristic features of superconductors, was first discovered in superconducting tin crystals.
Chemical properties.
Tin resists corrosion from water, but can be attacked by acids and alkalis. Tin can be highly polished and is used as a protective coat for other metals. In this case, the formation of a protective oxide layer is used to prevent further oxidation. This oxide layer forms on pewter and other tin alloys. Tin acts as a catalyst when oxygen is in solution and helps accelerate chemical attack.
Isotopes.
Tin has ten stable isotopes, with atomic masses of 112, 114 through 120, 122 and 124, the greatest number of any element. Of these, the most abundant ones are 120Sn (at almost a third of all tin), 118Sn, and 116Sn, while the least abundant one is 115Sn. The isotopes possessing even mass numbers have no nuclear spin, while the odd ones have a spin of +1/2. Tin, with its three common isotopes 116Sn, 118Sn and 120Sn, is among the easiest elements to detect and analyze by NMR spectroscopy, and its chemical shifts are referenced against SnMe4.
This large number of stable isotopes is thought to be a direct result of tin possessing an atomic number of 50, which is a "magic number" in nuclear physics. In addition, there are 29 known unstable isotopes, encompassing all the remaining ones with atomic masses between 99 and 137. Aside from 126Sn, which has a half-life of 230,000 years, all the radioactive isotopes have a half-life of less than a year. The radioactive 100Sn is one of the few nuclides possessing a "doubly magic" nucleus and was discovered in 1994. Another 30 metastable isomers have been characterized for isotopes between 111 and 131, the most stable being 121mSn, with a half-life of 43.9 years.
Etymology.
The word "tin" is shared among Germanic languages and can be traced back to reconstructed Proto-Germanic "*tin-om"; cognates include German "Zinn", Swedish "tenn" and Dutch "tin". It is not found in other branches of Indo-European, except by borrowing from Germanic (e.g. Irish "tinne" from English).
The Latin name "stannum" originally meant an alloy of silver and lead, and came to mean 'tin' in the 4th century BCE—the earlier Latin word for it was "plumbum candidum", or "white lead". "Stannum" apparently came from an earlier "stāgnum" (meaning the same substance), the origin of the Romance and Celtic terms for 'tin'. The origin of "stannum"/"stāgnum" is unknown; it may be pre-Indo-European. The "Meyers Konversationslexikon" speculates on the contrary that "stannum" is derived from (the ancestor of) Cornish "stean", and is proof that Cornwall in the first centuries AD was the main source of tin.
History.
Tin extraction and use can be dated to the beginnings of the Bronze Age around 3000 BC, when it was observed that copper objects formed of polymetallic ores with different metal contents had different physical properties. The earliest bronze objects had a tin or arsenic content of less than 2% and are therefore believed to be the result of unintentional alloying due to trace metal content in the copper ore. The addition of a second metal to copper increases its hardness, lowers the melting temperature, and improves the casting process by producing a more fluid melt that cools to a denser, less spongy metal. This was an important innovation that allowed for the much more complex shapes cast in closed moulds of the Bronze Age. Arsenical bronze objects appear first in the Near East where arsenic is commonly found in association with copper ore, but the health risks were quickly realized and the quest for sources of the much less hazardous tin ores began early in the Bronze Age. This created the demand for rare tin metal and formed a trade network that linked the distant sources of tin to the markets of Bronze Age cultures.
Cassiterite (SnO2), the tin oxide form of tin, was most likely the original source of tin in ancient times. Other forms of tin ores are less abundant sulfides such as stannite that require a more involved smelting process. Cassiterite often accumulates in alluvial channels as placer deposits due to the fact that it is harder, heavier, and more chemically resistant than the granite in which it typically forms. These deposits can be easily seen in river banks as cassiterite is usually black, purple or otherwise dark in color, a feature exploited by early Bronze Age prospectors. It is likely that the earliest deposits were alluvial in nature, and perhaps exploited by the same methods used for panning gold in placer deposits.
Compounds and chemistry.
See also: .
In the great majority of its compounds, tin has the oxidation state II or IV.
Inorganic compounds.
Halide compounds are known for both oxidation states. For Sn(IV), all four halides are well known: SnF4, SnCl4, SnBr4, and SnI4. The three heavier members are volatile molecular compounds, whereas the tetrafluoride is polymeric. All four halides are known for Sn(II) also: SnF2, SnCl2, SnBr2, and SnI2. All are polymeric solids. Of these eight compounds, only the iodides are colored.
Tin(II) chloride (also known as stannous chloride) is the most important tin halide in a commercial sense. Illustrating the routes to such compounds, chlorine reacts with tin metal to give SnCl4 whereas the reaction of hydrochloric acid and tin gives SnCl2 and hydrogen gas. Alternatively SnCl4 and Sn combine to stannous chloride via a process called comproportionation:
Tin can form many oxides, sulfides, and other chalcogenide derivatives. The dioxide SnO2 (cassiterite) forms when tin is heated in the presence of air. SnO2 is amphoteric, which means that it dissolves in both acidic and basic solutions. There are also stannates with the structure [Sn(OH)6]2−, like K2[Sn(OH)6], although the free stannic acid H2[Sn(OH)6] is unknown. The sulfides of tin exist in both the +2 and +4 oxidation states: tin(II) sulfide and tin(IV) sulfide (mosaic gold).
Hydrides.
Stannane (SnH4), where tin is in the +4 oxidation state, is unstable. Organotin hydrides are however well known, e.g. tributyltin hydride (Sn(C4H9)3H). These compound release transient tributyl tin radicals, rare examples of compounds of tin(III).
Organotin compounds.
Organotin compounds, sometimes called stannanes, are chemical compounds with tin–carbon bonds. Of the compounds of tin, the organic derivatives are the most useful commercially. Some organotin compounds are highly toxic and have been used as biocides. The first organotin compound to be reported was diethyltin diiodide ((C2H5)2SnI2), reported by Edward Frankland in 1849.
Most organotin compounds are colorless liquids or solids that are stable to air and water. They adopt tetrahedral geometry. Tetraalkyl- and tetraaryltin compounds can be prepared using Grignard reagents:
The mixed halide-alkyls, which are more common and more important commercially than the tetraorgano derivatives, are prepared by redistribution reactions:
Divalent organotin compounds are uncommon, although more common than related divalent organogermanium and organosilicon compounds. The greater stabilization enjoyed by Sn(II) is attributed to the "inert pair effect". Organotin(II) compounds include both stannylenes (formula: R2Sn, as seen for singlet carbenes) and distannylenes (R4Sn2), which are roughly equivalent to alkenes. Both classes exhibit unusual reactions.
Occurrence.
See also: .
Tin is generated via the long S-process in low-to-medium mass stars (with masses of 0.6 to 10 times that of Sun). It arises via beta decay of heavy isotopes of indium.
Tin is the 49th most abundant element in Earth's crust, representing 2 ppm compared with 75 ppm for zinc, 50 ppm for copper, and 14 ppm for lead.
Tin does not occur as the native element but must be extracted from various ores. Cassiterite (SnO2) is the only commercially important source of tin, although small quantities of tin are recovered from complex sulfides such as stannite, cylindrite, franckeite, canfieldite, and teallite. Minerals with tin are almost always associated with granite rock, usually at a level of 1% tin oxide content.
Because of the higher specific gravity of tin dioxide, about 80% of mined tin is from secondary deposits found downstream from the primary lodes. Tin is often recovered from granules washed downstream in the past and deposited in valleys or under sea. The most economical ways of mining tin are through dredging, hydraulic methods or open cast mining. Most of the world's tin is produced from placer deposits, which may contain as little as 0.015% tin.
About 253,000 tonnes of tin have been mined in 2011, mostly in China (110,000 t), Indonesia (51,000 t), Peru (34,600 t), Bolivia (20,700 t) and Brazil (12,000 t). Estimates of tin production have historically varied with the dynamics of economic feasibility and the development of mining technologies, but it is estimated that, at current consumption rates and technologies, the Earth will run out of tin that can be mined in 40 years. However Lester Brown has suggested tin could run out within 20 years based on an extremely conservative extrapolation of 2% growth per year.
Secondary, or scrap, tin is also an important source of the metal. The recovery of tin through secondary production, or recycling of scrap tin, is increasing rapidly. Whereas the United States has neither mined since 1993 nor smelted tin since 1989, it was the largest secondary producer, recycling nearly 14,000 tonnes in 2006.
New deposits are reported to be in southern Mongolia, and in 2009, new deposits of tin were discovered in Colombia, by the Seminole Group Colombia CI, SAS.
Production.
Tin is produced by carbothermic reduction of the oxide ore with carbon or coke. Both reverberatory furnace and electric furnace can be used.
Industry.
The ten largest companies produced most of the world's tin in 2007. It is not clear which of these companies include tin smelted from the mine at Bisie, Democratic Republic of the Congo, which is controlled by a renegade militia and produces 15,000 tonnes.
Most of the world's tin is traded on the London Metal Exchange (LME), from 8 countries, under 17 brands.
Price and exchanges.
Tin is unique among other mineral commodities by the complex "agreements" between producer countries and consumer countries dating back to 1921. The earlier agreements tended to be somewhat informal and sporadic; they led to the "First International Tin Agreement" in 1956, the first of a
continuously numbered series that essentially collapsed in 1985. Through this series of agreements, the International Tin Council (ITC) had a considerable effect on tin prices. The ITC supported the price of tin during periods of low prices by buying tin for its buffer stockpile and was able to restrain the price during periods of high prices by selling tin from the stockpile. This was an anti-free-market approach, designed to assure a sufficient flow of tin to consumer countries and a decent profit for producer countries. However, the buffer stockpile was not sufficiently large, and during most of those 29 years tin prices rose, sometimes sharply, especially from 1973 through 1980 when rampant inflation plagued many world economies.
During the late 1970s and early 1980s, the U.S. Government tin stockpile was in an aggressive selling mode, partly to take advantage of the historically high tin prices. The sharp recession of 1981–82 proved to be quite harsh on the tin industry. Tin consumption declined dramatically. The ITC was able to avoid truly steep declines through accelerated buying for its buffer stockpile; this activity required the ITC to borrow extensively from banks and metal trading firms to augment its resources. The ITC continued to borrow until late 1985, when it reached its credit limit. Immediately, a major "tin crisis" followed — tin was delisted from trading on the London Metal Exchange for about three years, the ITC dissolved soon afterward, and the price of tin, now in a free-market environment, plummeted sharply to $4 per pound and remained around this level through the 1990s. It increased again by 2010 due to the rebound in consumption following the 2008–09 world economic crisis, restocking and continued growth in consumption by the world's developing economies.
London Metal Exchange (LME) is the principal trading site for tin. Other tin contract markets are Kuala Lumpur Tin Market (KLTM) and Indonesia Tin Exchange (INATIN).
Price of tin in USD cents per kg.
Source:Helgi Library 
Applications.
In 2006, about half of tin produced was used in solder. The rest was divided between tin plating, tin chemicals, brass and bronze, and niche uses.
Solder.
Tin has long been used as a solder in the form of an alloy with lead, tin accounting for 5 to 70% w/w. Tin forms a eutectic mixture with lead containing 63% tin and 37% lead. Such solders are primarily used for joining pipes or electric circuits. Since the European Union Waste Electrical and Electronic Equipment Directive (WEEE Directive) and Restriction of Hazardous Substances Directive came into effect on 1 July 2006, the use of lead in such alloys has decreased. Replacing lead has many problems, including a higher melting point, and the formation of tin whiskers causing electrical problems. Tin pest can occur in lead-free solders, leading to loss of the soldered joint. Replacement alloys are rapidly being found, although problems of joint integrity remain.
Tin plating.
Tin bonds readily to iron and is used for coating lead, zinc and steel to prevent corrosion. Tin-plated steel containers are widely used for food preservation, and this forms a large part of the market for metallic tin. A tinplate canister for preserving food was first manufactured in London in 1812. Speakers of British English call them "tins", while speakers of American English call them "cans" or "tin cans". One thus-derived use of the slang term "tinnie" or "tinny" means "can of beer". The tin whistle is so called because it was first mass-produced in tin-plated steel.
Specialized alloys.
Tin in combination with other elements forms a wide variety of useful alloys. Tin is most commonly alloyed with copper. Pewter is 85–99% tin; Bearing metal has a high percentage of tin as well. Bronze is mostly copper (12% tin), while addition of phosphorus gives phosphor bronze. Bell metal is also a copper-tin alloy, containing 22% tin. Tin has also sometimes been used in coinage; for example, it once formed a single-digit figure percentage (usually five percent or less) of the American and Canadian pennies. Because copper is often the major metal in such coins, and zinc is sometimes present as well, these could technically be called bronze and/or brass alloys.
The niobium-tin compound Nb3Sn is commercially used as wires for superconducting magnets, due to the material's high critical temperature (18 K) and critical magnetic field (25 T). A superconducting magnet weighing as little as two kilograms is capable of producing magnetic fields comparable to a conventional electromagnet weighing tons.
The addition of a few percent of tin is commonly used in zirconium alloys for the cladding of nuclear fuel.
Most metal pipes in a pipe organ are made of varying amounts of a tin/lead alloy, with 50%/50% being the most common. The amount of tin in the pipe defines the pipe's tone, since tin is the most tonally resonant of all metals. When a tin/lead alloy cools, the lead cools slightly faster and produces a mottled or spotted effect. This metal alloy is referred to as spotted metal. Major advantages of using tin for pipes include its appearance, its workability, and resistance to corrosion.
Other applications.
Punched tin-plated steel, also called pierced tin, is an artisan technique originating in central Europe for creating housewares that are both functional and decorative. Decorative piercing designs exist in a wide variety, based on geography or the artisan's personal creations. Punched tin lanterns are the most common application of this artisan technique. The light of a candle shining through the pierced design creates a decorative light pattern in the room where it sits. Punched tin lanterns and other punched tin articles were created in the New World from the earliest European settlement. A well-known example is the Revere type lantern, named after Paul Revere.
Before the modern era, in some areas of the Alps, a goat or sheep's horn would be sharpened and a tin panel would be punched out using the alphabet and numbers from one to nine. This learning tool was known appropriately as "the horn". Modern reproductions are decorated with such motifs as hearts and tulips.
In America, pie safes and food safes came into use in the days before refrigeration. These were wooden cupboards of various styles and sizes – either floor standing or hanging cupboards meant to discourage vermin and insects and to keep dust from perishable foodstuffs. These cabinets had tinplate inserts in the doors and sometimes in the sides, punched out by the homeowner, cabinetmaker or a tinsmith in varying designs to allow for air circulation. Modern reproductions of these articles remain popular in North America.
Window glass is most often made by floating molten glass on top of molten tin (creating float glass) in order to produce a flat surface. This is called the "Pilkington process".
Tin is also used as a negative electrode in advanced Li-ion batteries. Its application is somewhat limited by the fact that some tin surfaces catalyze decomposition of carbonate-based electrolytes used in Li-ion batteries.
Tin(II) fluoride is added to some dental care products as stannous fluoride (SnF2). Tin(II) fluoride can be mixed with calcium abrasives while the more common sodium fluoride gradually becomes biologically inactive combined with calcium compounds. It has also been shown to be more effective than sodium fluoride in controlling gingivitis.
Organotin compounds.
Of all the chemical compounds of tin, the organotin compounds are most heavily used. Worldwide industrial production probably exceeds 50,000 tonnes.
PVC stabilizers.
The major commercial application of organotin compounds is in the stabilization of PVC plastics. In the absence of such stabilizers, PVC would otherwise rapidly degrade under heat, light, and atmospheric oxygen, to give discolored, brittle products. Tin scavenges labile chloride ions (Cl−), which would otherwise initiate loss of HCl from the plastic material. Typical tin compounds are carboxylic acid derivatives of dibutyltin dichloride, such as the dilaurate.
Biocides.
Organotin compounds can have a relatively high toxicity, which is both advantageous and problematic. They have been used for their biocidal effects in/as fungicides, pesticides, algaecides, wood preservatives, and antifouling agents. Tributyltin oxide is used as a wood preservative. Tributyltin was used as additive for ship paint to prevent growth of marine organisms on ships, with use declining after organotin compounds were recognized as persistent organic pollutants with an extremely high toxicity for some marine organisms, for example the dog whelk. The EU banned the use of organotin compounds in 2003, while concerns over the toxicity of these compounds to marine life and their effects on the reproduction and growth of some marine species, (some reports describe biological effects to marine life at a concentration of 1 nanogram per liter) have led to a worldwide ban by the International Maritime Organization. Many nations now restrict the use of organotin compounds to vessels over 25 meters long.
Organic chemistry.
Some tin reagents are useful in organic chemistry. In the largest application, stannous chloride is a common reducing agent for the conversion of nitro and oxime groups to amines. The Stille reaction couples organotin compounds with organic halides or pseudohalides.
Li-ion batteries.
Tin forms several inter-metallic phases with lithium metal and it makes it a potentially attractive material. Large volumetric expansion of tin upon alloying with lithium and instability of the tin-organic electrolyte interface at low electrochemical potentials are the greatest challenges in employing it in commercial cells. The problem was partially solved by Sony. Tin inter-metallic compound with cobalt, mixed with carbon, has been implemented by Sony in its Nexelion cells released in late 2000's. The composition of the active materials is close to Sn0.3Co0.4C0.3. Recent research showed that only some crystalline facets of tetragonal (beta) Sn are responsible for undesirable electrochemical activity.
Precautions.
Cases of poisoning from tin metal, its oxides, and its salts are "almost unknown". On the other hand, certain organotin compounds are almost as toxic as cyanide.

</doc>
<doc id="30098" url="http://en.wikipedia.org/wiki?curid=30098" title="Thump Records">
Thump Records

Thump Records is a record label specialized in freestyle music, Latin music and hip hop music. They are probably best known for their rap and R&B Old School compilations, as well as their "Lowrider" magazine compilations and videos.

</doc>
<doc id="30104" url="http://en.wikipedia.org/wiki?curid=30104" title="Problem of evil">
Problem of evil

In the philosophy of religion, the problem of evil is the question of how to reconcile the existence of evil with that of a deity who is, in either absolute or relative terms, omnipotent, omniscient, and omnibenevolent (see theism). An argument from evil attempts to show that the co-existence of evil and such a deity is unlikely or impossible if placed in absolute terms. Attempts to show the contrary have traditionally been discussed under the heading of theodicy.
A wide range of responses have been given to the problem of evil in theology. There are also many discussions of evil and associated problems in other philosophical fields, such as secular ethics, and scientific disciplines such as evolutionary ethics. But as usually understood, the "problem of evil" is posed in a theological context.
Detailed arguments.
Together, John Joseph Haldane's Wittgenstinian-Thomistic account of concept formation and Martin Heidegger's observation of temporality's thrown nature imply that God's act of creation and God's act of judgment are the same act. God's condemnation of evil is subsequently believed to be executed and expressed in his created world; a judgement that is unstoppable due to God's all powerful will; a constant and eternal judgement that becomes announced and communicated to other people on Judgment Day. In this explanation, God's condemnation of evil is declared to be a good judgement.
The above argument is set against numerous versions of the problem of evil that have been formulated. These versions have included philosophical and theological formulations.
Logical problem of evil.
The originator of the logical problem of evil has been cited as the Greek philosopher Epicurus, and this argument may be schematized as follows:
This argument is of the form modus tollens, and is logically valid if its premises are true, the conclusion follows of necessity. To show that the first premise is plausible, subsequent versions tend to expand on it, such as this modern example:
Both of these arguments are understood to be presenting two forms of the "logical" problem of evil. They attempt to show that the assumed propositions lead to a logical contradiction and therefore cannot all be correct. Most philosophical debate has focused on the propositions stating that God cannot exist with, or would want to prevent, all evils (premises 3 and 6), with defenders of theism (for example, Leibniz) arguing that God could very well exist with and allow evil in order to achieve a greater good.
One greater good that has been proposed is that of free will, famously argued for by Alvin Plantinga in his free will defense. The first part of this defense accounts for moral evil as the result of free human action. The second part of this defense argues for the logical possibility of "a mighty nonhuman spirit" such as Satan who is responsible for so-called "natural evils", including earthquakes, tidal waves, and virulent diseases. Some philosophers agree that Plantinga successfully solves the logical problem of evil, by showing that God and evil are logically compatible though others explicitly dissent. The second part of Plantinga's defense, though, concedes God's omnipotence by claiming the possibility of "a mighty nonhuman spirit" capable of causing evil in spite of God's desire for evil not to exist (a necessary consequence of His benevolence), effectively "overpowering" God.
Evidential problem of evil.
The "evidential" version of the problem of evil (also referred to as the probabilistic or inductive version), seeks to show that the existence of evil, although logically consistent with the existence of God, counts against or lowers the probability of the truth of theism. As an example, a critic of Plantinga's idea of "a mighty nonhuman spirit" causing natural evils may concede that the existence of such a being is not logically impossible but argue that due to lacking scientific evidence for its existence this is very unlikely and thus it is an unconvincing explanation for the presence of natural evils. Both absolute versions and relative versions of the evidential problems of evil are presented below.
A version by William L. Rowe:
Another by Paul Draper:
These arguments are probability judgments since they rest on the claim that, even after careful reflection, one can see no good reason for God’s permission of evil. The inference from this claim to the general statement that there exists unnecessary evil is inductive in nature and it is this inductive step that sets the evidential argument apart from the logical argument.
The logical possibility of hidden or unknown reasons for the existence of evil still exists. However, the existence of God is viewed as any large-scale hypothesis or explanatory theory that aims to make sense of some pertinent facts. The extent to which it fails to do so has not been confirmed. According to Occam's razor, one should make as few assumptions as possible. Hidden reasons are assumptions, as is the assumption that all pertinent facts can be observed, or that facts and theories humans have not discerned are indeed hidden. Thus, as per Paul Draper's argument above, the theory that there is an omniscient and omnipotent being who is indifferent requires no hidden reasons in order to explain evil. It is thus a simpler theory than one that also requires hidden reasons regarding evil in order to include omnibenevolence. Similarly, for every hidden argument that completely or partially justifies observed evils it is equally likely that there is a hidden argument that actually makes the observed evils worse than they appear without hidden arguments. As such, from an inductive viewpoint hidden arguments will neutralize one another.
Author and researcher Gregory S. Paul offers what he considers to be a particularly strong problem of evil. Paul introduces his own estimates that at least 100 billion people have been born throughout human history (starting roughly 50 000 years ago, when Homo Sapiens—humans—first appeared). He then performed what he calls "simple" calculations to estimate the historical death rate of children throughout this time. He found that the historical death rate was over 50%, and that the deaths of these children were mostly due to diseases (like malaria).
Paul thus sees it as a problem of evil, because this means that within the bounds of his estimates, that throughout human history, over 50 billion people died naturally before they were old enough to give mature consent. He adds that as many as 300 billion humans may never have reached birth, instead dying naturally but prenatally (the prenatal death rate being about 3/4 historically). Paul says that these figures could have implications for calculating the population of a heaven (which could include the aforementioned 50 billion children, 50 billion adults, and roughly 300 billion fetuses—excluding any living today).
A common response to instances of the evidential problem is that there are plausible (and not hidden) justifications for God’s permission of evil. These theodicies are discussed below.
Biblical examples.
Though the purpose of the Bible from the church's perspective is to teach that God is all powerful and good, arguments from evil have examined biblical passages and have argued otherwise. By using examples from the Bible to claim that God is actually evil, the argument is made that this fact lowers the chance of actual theism or the existence of an all powerful and loving God.
Genesis chapter 2 reads, "Out of the ground the Lord God made to grow every tree that is pleasant to the sight and good for food, the tree of life also in the midst of the garden, and the tree of the knowledge of good and evil."
This section of Genesis introduces the "All Knowing Tree" that Adam and Eve eat from. Arguments from evil suggest that this is an example of God's evil nature or an example that shows that God isn't all powerful. The reasoning being that if God was all loving and powerful then the evil in the tree would not exist. Or it's possible that God isn't all powerful therefore He was not able to prohibit the evil in the tree.
Regarding the story of Noah's Ark, Genesis chapter 6 reads, "I have determined to make an end of all flesh, for the earth is filled with violence because of them; now I am going to destroy them along with the earth."
In this passage of the Bible one can see how God destroyed the earth, killing everyone in it, except Noah and his family. Arguments from evil often use this passage to demonstrate how God's wrath makes it impossible for him to be all loving and good.
Related arguments.
Doctrines of hell, particularly those involving eternal suffering, pose a particularly strong form of the problem of evil (see problem of hell). If the problem of unbelief, incorrect beliefs, or poor design are considered evils, then the argument from nonbelief, the argument from inconsistent revelations, and the argument from poor design may be seen as particular instances of the argument that the co-existence of evil with such a deity is unlikely or impossible.
Responses, defences and theodicies.
Responses to the problem of evil have occasionally been classified as "defences" or "theodicies;" however, authors disagree on the exact definitions. Generally, a "defense" against the problem of evil may refer to attempts to defuse the logical problem of evil by showing that there is no logical incompatibility between the existence of evil and the existence of God. This task does not require the identification of a plausible explanation of evil, and is successful if the explanation provided shows that the existence of God and the existence of evil are logically compatible. It need not even be true, since a false though coherent explanation would be sufficient to show logical compatibility.
A "theodicy", on the other hand, is more ambitious, since it attempts to provide a plausible justification—a morally or philosophically sufficient reason—for the existence of evil and thereby rebut the "evidential" argument from evil. Richard Swinburne maintains that it does not make sense to assume there are greater goods that justify the evil's presence in the world unless we know what they are—without knowledge of what the greater goods could be, one cannot have a successful theodicy. Thus, some authors see arguments appealing to demons or the fall of man as indeed logically possible, but not very "plausible" given our knowledge about the world, and so see those arguments as providing defences but not good theodicies.
Denial of absolute omniscience, omnipotence, omnibenevolence.
If God lacks any one of these qualities, the existence of evil is explicable, and so the problem of evil would be treated instead under the heading of some alternate formulation or doctrine of theology.
In polytheism the individual deities are usually not omnipotent or omnibenevolent as the powers which they share are distributed among the diverse gods; however, if one of the deities has these properties the problem of evil applies. Belief systems where several deities are omnipotent would lead to logical contradictions and conflict.
Ditheistic belief systems (a kind of dualism) explain the problem of evil from the existence of two rival great, but not omnipotent, deities that work in polar opposition to each other. Examples of such belief systems include Zoroastrianism, Manichaeism, Catharism, and possibly Gnosticism. The Devil in Islam and in Christianity is not seen as equal in power to God who is omnipotent. Thus the Devil could only exist if so allowed by God. The Devil, if so limited in power, can therefore by himself not explain the problem of evil without recourse to theism of some alternate version of theology.
Process theology and open theism are other positions that limit God's omnipotence and/or omniscience (as defined in traditional Christian theology).
Denial of omnibenevolence.
Dystheism is the belief that God is not wholly good. Pantheists and panentheists who are dystheistic may provide alternate versions for describing the disposition of evil.
"Greater good" responses.
The omnipotence paradoxes, where evil persists in the presence of an all powerful God, raise questions as to the nature of God's omnipotence. Although that is from excluding the idea of how an interference would negate and subjugate the concept of free will, or in other words result in a totalitarian system that creates a lack of freedom. Some solutions propose that omnipotence does not require the ability to actualize the logically impossible. "Greater good" responses to the problem make use of this insight by arguing for the existence of goods of great value which God cannot actualize without also permitting evil, and thus that there are evils he cannot be expected to prevent despite being omnipotent. Among the most popular versions of the "greater good" response are appeals to the apologetics of free will. Theologians will argue that since no one can fully understand God's ultimate plan, no one can assume that evil actions do not have some sort of greater purpose. Therefore the nature of evil has a necessary role to play in God's plan for a better world.
Free will.
Use of the term "free will" creates confusion unless its definition is stated. In order to reduce confusion, Mortimer Adler found that a delineation of three kinds of freedom is necessary for clarity on the subject. ("Free will" and "freedom" are often used as synonyms.) These three kinds of freedom follow:
For Greg Boyd, open theist and exponent of libertarian freedom, the free will response asserts that the existence of free beings is something of very high value, because with free will comes the ability to make morally significant choices (which include the expression of love and affection). Boyd also maintains that God does not plan or will evil in people's lives, but that evil is a result of a combination of free choices and the interconnectedness and complexity of life in a sinful and fallen world. With free will also comes the potential for ethical abuse, as when individuals fail to act morally. But the evil result created by such abuse of free will is easily outweighed by the great value of free will and the good that comes of it, and so God is justified in creating a world which offers the existence of free will, and with it the potential for evil. A world with free beings and no evil would be still better. However, this would require the cooperation of free beings with God, as it would be logically inconsistent for God to prevent abuses of freedom without thereby curtailing that freedom.
However, critics of the free will response have questioned whether it accounts for the degree of evil seen in this world. One point in this regard is that while the value of free will may be thought sufficient to counterbalance minor evils, it is less obvious that it outweighs the negative attributes of evils such as rape and murder. Particularly egregious cases known as horrendous evils, which "[constitute] prima facie reason to doubt whether the participant’s life could (given their inclusion in it) be a great good to him/her on the whole," have been the focus of recent work in the problem of evil. Another point is that those actions of free beings which bring about evil very often diminish the freedom of those who suffer the evil; for example the murder of a young child may prevent the child from ever exercising their free will. In such a case the freedom of an innocent child is pitted against the freedom of the evil-doer, it is not clear why God would remain unresponsive and passive.
A second criticism is that the potential for evil inherent in free will may be limited by means which do not impinge on that free will. God could accomplish this by making moral actions especially pleasurable, so that they would be irresistible to us; he could also punish immoral actions immediately, and make it obvious that moral rectitude is in our self-interest; or he could allow bad moral decisions to be made, but intervene to prevent the harmful consequences from actually happening. A reply is that such a "toy world" would mean that free will has less or no real value. Critics may respond that this view seems to imply it would be similarly wrong for humans to try to reduce suffering in these ways, a position which few would advocate. It may nevertheless be advocated that the widescale prevention of suffering, such as the with indiscriminate use of analgesics, would lead individuals to become unresponsive to the rectifying feedback such suffering serves to provide. The debate depends on the definitions of free will and determinism, which are , as well as their relation to one another. See also compatibilism, incompatibilism, and predestination. In general terms, compatibilism and incompatibilism refer to whether free-will in individuals is in conflict with a God who may or may not have knowledge of the outcome of the choices which individuals make based on this free-will before the choices are made.
A third reply is that though the free will defence has the potential to explain moral evil, it fails to address natural evil. By definition, moral evil results from human action, but natural evil results from natural processes that cause natural disasters such as volcanic eruptions or earthquakes. Advocates of the free will response to evil propose various explanations of natural evils. Alvin Plantinga, following Augustine of Hippo, and others have argued that natural evils are caused by the free choices of supernatural beings such as demons. Others have argued 
Advocates of the free will response can also point to the fact that "the line between moral and natural evil is not always clear." Natural evils are often caused or exacerbated by humans in their exercise of free will.
Finally, because the free will response assumes a libertarian account of free will, the debate over its adequacy naturally widens into a debate concerning the nature and existence of free will. Compatibilists deny that a being who is determined to act morally lacks free will, and so also believe that God cannot ensure the moral behavior of the free beings he creates. Hard determinists deny the existence of free will, and therefore they deny that the existence of free will justifies the evil in our world. There is also debate regarding the compatibility of moral free will (to select good or evil action) with the absence of evil from heaven, with God's omniscience (see the argument from free will), and with his omnibenevolence.
Soul-making or Irenaean theodicy.
Distinctive of the soul-making theodicy is the claim that evil and suffering are necessary for spiritual growth. Theology consistent with this type of theodicy was developed by the second-century Christian theologian, Irenaeus of Lyons, and its most recent advocate has been the influential philosopher of religion, John Hick. A perceived inadequacy with the theodicy is that many evils do not seem to promote such growth, and can be positively destructive of the human spirit. Hick acknowledges that this process often fails in our world. A second issue concerns the distribution of evils suffered: were it true that God permitted evil in order to facilitate spiritual growth, then we would expect evil to disproportionately befall those in poor spiritual health. This does not seem to be the case, as the decadent enjoy lives of luxury which insulate them from evil, whereas many of the pious are poor, and are well acquainted with worldly evils. A third problem attending this theodicy is that the qualities developed through experience with evil seem to be useful precisely because they are useful in overcoming evil. But if there were no evil, then there would seem to be no value in such qualities, and consequently no need for God to permit evil in the first place. Against this it may be asserted that the qualities developed are intrinsically valuable, but this view would need further justification.
Afterlife.
The afterlife has also been cited as justifying evil. Christian author Randy Alcorn argues that the joys of heaven will compensate for the sufferings on earth, and writes:
Without this eternal perspective, we assume that people who die young, who have handicaps, who suffer poor health, who don't get married or have children, or who don't do this or that will miss out on the best life has to offer. But the theology underlying these assumptions have a fatal flaw. It presumes that our present Earth, bodies, culture, relationships and lives are all there is... [but] Heaven will bring far more than compensation for our present sufferings.
Philosopher Stephen Maitzen has called this the "Heaven Swamps Everything" theodicy, and argues that it is false because it conflates compensation and justification. He observes that this reasoning:
... may stem from imagining an ecstatic or forgiving state of mind on the part of the blissful: in heaven no one bears grudges, even the most horrific earthly suffering is as nothing compared to infinite bliss, all past wrongs are forgiven. But “are forgiven” doesn’t mean “were justified”; the blissful person’s disinclination to dwell on his or her earthly suffering doesn’t imply that a perfect being was justified in permitting the suffering all along. By the same token, our ordinary moral practice recognizes a legitimate complaint about child abuse even if, as adults, its victims should happen to be on drugs that make them uninterested in complaining. Even if heaven swamps everything, it doesn’t thereby justify everything.
Previous lives and karma.
The theory of karma holds that good acts result in pleasure and bad acts with suffering. Thus it accepts that there is suffering in the world, but maintains that there is no "undeserved" suffering, and in that sense, no evil. The obvious objection that people sometimes suffer misfortune that was undeserved is met with by coupling karma with reincarnation, so that such suffering is the result of actions in previous lifetimes. The real problem of evil is the desire to invert the law of karma by way of causing suffering to the innocent, and rewarding pleasure to the guilty as superimposed rule.
Skeptical theism.
Skeptical theists argue that due to humanity's limited knowledge, we cannot expect to understand God or his ultimate plan. When a parent takes an infant to the doctor for a regular vaccination to prevent childhood disease, it's because the parent cares for and loves that child. The infant however will be unable to appreciate this. It is argued that just as an infant cannot possibly understand the motives of its parent due to its cognitive limitations, so too are humans unable to comprehend God's will in their current physical and earthly state. Given this view, the difficulty or impossibility of finding a plausible explanation for evil in a world created by God is to be expected, and so the argument from evil is assumed to fail unless it can be proven that God's reasons would be comprehensible to us.
A related response is that good and evil are strictly beyond human comprehension. Since our concepts of good and evil as instilled in us by God are only intended to facilitate ethical behaviour in our relations with other humans, we should have no expectation that our concepts are accurate beyond what is needed to fulfill this function, and therefore cannot presume that they are sufficient to determine whether what we call evil really is evil. Such a view may be independently attractive to the theist, as it permits an agreeable interpretation of certain biblical passages, such as "...Who makes peace and creates evil; I am the Lord, Who makes all these."
A counterpoint to the above is that while these considerations harmonize belief in God with our inability to identify his reasons for permitting evil, there remains a question as to why we have not been given a clear and unambiguous assurance by God that he has good reasons for allowing evil, which would be within our ability to understand. Here discussion of the problem of evil shades into discussion of the argument from nonbelief.
Denial of the existence of evil.
Evil as the absence of good.
The fifth century theologian Augustine of Hippo maintained that evil exists only as a privation or absence of the good. Ignorance is an evil, but is merely the absence of knowledge, which is good; disease is the absence of health; callousness an absence of compassion. Since evil has no positive reality of its own, it cannot be caused to exist, and so God cannot be held responsible for causing it to exist. In its strongest form, this view may identify evil as an absence of God, who is the sole source of that which is good.
A related view, which draws on the Taoist concept of yin-yang, allows that both evil and good have positive reality, but maintains that they are complementary opposites, where the existence of each is dependent on the existence of the other. Compassion, a valuable virtue, can only exist if there is suffering; bravery only exists if we sometimes face danger; self-sacrifice is called for only where others are in need. This is sometimes called the "contrast" argument.
Perhaps the most important criticism of this view is that, even granting its success against the argument from evil, it does nothing to undermine an 'argument from the absence of goodness' which may be pushed instead, and so the response is only superficially successful.
Evil as illusory.
It is possible to hold that evils such as suffering and disease are mere illusions, and that we are mistaken about the existence of evil. This approach is favored by some Eastern religious philosophies such as Hinduism and Buddhism, and by Christian Science. It is most plausible when considering our knowledge of evils which are geographically or temporally distant, for these might not be real after all. However, when considering our own sensations of pain and mental anguish, there does not seem to be a difference in apprehending that we are afflicted by such sensations and suffering under their influence. If that is the case, it seems that not all evils can be dismissed as illusory.
Turning the tables.
"Evil" suggests an ethical law.
A different approach to the problem of evil is to turn the tables by suggesting that any argument from evil is self-refuting, in that its conclusion would necessitate the falsity of one of its premises. One response then is to point out that the assertion "evil exists" implies an ethical standard against which moral value is determined, and then to argue that this standard implies the existence of God (see argument from morality). C. S. Lewis writes:
My argument against God was that the universe seemed so cruel and unjust. But how had I got this idea of just and unjust? A man does not call a line crooked unless he has some idea of a straight line. What was I comparing this universe with when I called it unjust?... Of course I could have given up my idea of justice by saying it was nothing but a private idea of my own. But if I did that, then my argument against God collapsed too—for the argument depended on saying the world was really unjust, not simply that it did not happen to please my fancies.
The standard criticism of this view is that an argument from evil is not necessarily a presentation of the views of its proponent, but is instead intended to show how premises which the theist is inclined to believe lead him or her to the conclusion that God does not exist (i.e. as a reductio of the theist's worldview). Another tact is to reformulate the argument from evil so that this criticism does not apply—for example, by replacing the term "evil" with "suffering", or what is more cumbersome, state of affairs that orthodox theists would agree are properly called "evil".
General criticisms of defenses and theodicies.
Several philosophers have argued that just as there exists a problem of evil for theists who believe in an omniscient, omnipotent and omnibenevolent being, so too is there a problem of good for anyone who believes in an omniscient, omnipotent, and omnimalevolent (or perfectly evil) being. As it appears that the defenses and theodicies which might allow the theist to resist the problem of evil can be inverted and used to defend belief in the omnimalevolent being, this suggests that we should draw similar conclusions about the success of these defensive strategies. In that case, the theist appears to face a dilemma: either to accept that both sets of responses are equally bad, and so that the theist does not have an adequate response to the problem of evil; or to accept that both sets of responses are equally good, and so to commit to the existence of an omnipotent, omniscient, and omnimalevolent being as plausible.
Critics have noted that theodicies and defenses are often addressed to the logical problem of evil. As such, they are intended only to demonstrate that it is "possible" that evil can co-exist with an omniscient, omnipotent and omnibenevolent being. Since the relevant parallel commitment is only that good can co-exist with an omniscient, omnipotent and omnimalevolent being, not that it is plausible that they should do so, the theist who is responding to the problem of evil need not be committing themselves to something they are likely to think is false. This reply, however, leaves the evidential problem of evil untouched.
Another general criticism is that though a theodicy may harmonize God with the existence of evil, it does so at the cost of nullifying morality. This is because most theodicies assume that whatever evil there is exists because it is required for the sake of some greater good. But if an evil is necessary because it secures a greater good, then it appears we humans have no duty to prevent it, for in doing so we would also prevent the greater good for which the evil is required. Even worse, it seems that any action can be rationalized, as if one succeeds in performing it, then God has permitted it, and so it must be for the greater good. From this line of thought one may conclude that, as these conclusions violate our basic moral intuitions, no greater good theodicy is true, and God does not exist. Alternatively, one may point out that greater good theodicies lead us to see every conceivable state of affairs as compatible with the existence of God, and in that case the notion of God's goodness is rendered meaningless.
See also Christian alternatives to theodicy.
By religion.
Ancient Mesopotamia and Egypt.
The problem of evil takes at least four formulations in ancient Mesopotamian religious thought, as in the extant manuscripts of Ludlul bēl nēmeqi ("I Will Praise the Lord of Wisdom"), "Erra and Ishum", "The Babylonian Theodicy", and "The Dialogue of Pessimism".
In this type of polytheistic context, the chaotic nature of the world implies multiple gods battling for control.
In ancient Egypt, it was thought the problem takes at least two formulations, as in the extant manuscripts of "Dialogue of a Man with His Ba" and "The Eloquent Peasant". Due to the conception of Egyptian gods as being far removed, these two formulations of the problem focus heavily on the relation between evil and people; that is, moral evil.
Judaism.
An oral tradition exists in Judaism that God determined the time of the Messiah's coming by erecting a great set of scales. On one side, God placed the captive Messiah with the souls of dead laymen. On the other side, God placed sorrow, tears, and the souls of righteous martyrs. God then declared that the Messiah would appear on earth when the scale was balanced. According to this tradition, then, evil is necessary in the bringing of the world's redemption, as sufferings reside on the scale.
The Talmud states that every bad thing is for the ultimate good, and a person should praise God for bad things like he praises God for the good things.
Tzimtzum in Kabbalistic thought holds that God has withdrawn himself so that creation could exist, but that this withdrawal means that creation lacks full exposure to God's all-good nature.
Christianity.
The Bible.
The Bible "has been, both in theory and in fact, the dominant influence upon ideas about God and evil in the Western world." The word "evil" occurs 613 times in the King James Version of the Bible: 481 times in the Old Testament and 132 times in the New Testament.
In the biblical view, evil is all that is "opposed to God and His purposes" (i.e., sin) or that which, from the human perspective, is "harmful and nonproductive" (i.e., suffering).
The fact of evil creates, not only a problem for existence, it creates a problem for belief in an all-good and all-powerful God. For if God was all-good and all-powerful then in theory such a God would be able to prohibit such evils from happening. In response, theologians have argued that though the problem of evil is present, it alone is not strong enough evidence to suggest that God is not all powerful and loving. The simplest biblical response to the problem of evil is that God has good reasons to permit such evils. Meaning each act of evil has a message that is meant to teach the world a moral value. This is an example of a greater good response. Greater good responses justify evil acts in the world by claiming that they are necessary for God's plan, which will ultimately benefit everyone. Another answer that the bible uses is the free will response. Theologians argue that if God prohibited one evil, then he would have to prohibit them all, therefore hindering freewill and the natural laws of the world.
For more on evil in the Bible and the problem of belief it creates, see the "Greater good responses" section at the top of this page or Theodicy and the Bible.
Gnosticism.
Gnosticism refers to several beliefs seeing evil as due to the world being created by an imperfect God, the demiurge, which is contrasted with a superior entity. However, this by itself does not answer the problem of evil if the superior entity is omnipotent and omnibenevolent. Different gnostic beliefs may give varying answers, like Manichaeism, which adopts dualism, in opposition to the doctrine of omnipotence.
Irenaean theodicy.
Irenaean theodicy, posited by Irenaeus (2nd century AD–c. 202), has been reformulated by John Hick. It holds that one cannot achieve moral goodness or love for God if there is no evil and suffering in the world. Evil is soul-making and leads one to be truly moral and close to God. God created an epistemic distance (such that God is not immediately knowable) so that we may strive to know him and by doing so become truly good. Evil is a means to good for 3 main reasons:
Pelagianism.
The consequences of the original sin were debated by Pelagius and Augustine of Hippo. Pelagius argues on behalf of original innocence, while Augustine indicts Eve and Adam for original sin. Pelagianism is the belief that original sin did not taint all of humanity and that mortal free will is capable of choosing good or evil without divine aid. Augustine's position, and subsequently that of much of Christianity, was that Adam and Eve had the power to topple God's perfect order, thus changing nature by bringing sin into the world, but that the advent of sin then limited mankind's power thereafter to evade the consequences without divine aid. Eastern Orthodox theology holds that one inherits the nature of sinfulness but not Adam and Eve's guilt for their sin which resulted in the fall.
Augustinian theodicy.
St Augustine of Hippo (AD 354–430) in his Augustinian theodicy, as presented in John Hick's book "Evil and the God of Love", focuses on the Genesis story that essentially dictates that God created the world and that it was good; evil is merely a consequence of the fall of man (The story of the Garden of Eden where Adam and Eve disobeyed God and caused inherent sin for man). Augustine stated that natural evil (evil present in the natural world such as natural disasters etc.) is caused by fallen angels, whereas moral evil (evil caused by the will of human beings) is as a result of man having become estranged from God and choosing to deviate from his chosen path. Augustine argued that God could not have created evil in the world, as it was created good, and that all notions of evil are simply a deviation or privation of goodness. Evil cannot be a separate and unique substance. For example, Blindness is not a separate entity, but is merely a lack or privation of sight. Thus the Augustinian theodicist would argue that the problem of evil and suffering is void because God did not create evil; it was man who chose to deviate from the path of perfect goodness.
St. Thomas Aquinas.
Saint Thomas systematized the Augustinian conception of evil, supplementing it with his own musings. Evil, according to St. Thomas, is a privation, or the absence of some good which belongs properly to the nature of the creature. There is therefore no positive source of evil, corresponding to the greater good, which is God; evil being not real but rational—i.e. it exists not as an objective fact, but as a subjective conception; things are evil not in themselves, but by reason of their relation to other things or persons. All realities are in themselves good; they produce bad results only incidentally; and consequently the ultimate cause of evil is fundamentally good, as well as the objects in which evil is found.
Catholic Encyclopedia.
Evil is threefold, viz., metaphysical evil, moral, and physical, the retributive consequence of moral guilt. Its existence subserves the perfection of the whole; the universe would be less perfect if it contained no evil. Thus fire could not exist without the corruption of what it consumes; the lion must slay the ass in order to live, and if there were no wrong doing, there would be no sphere for patience and justice. God is said (as in Isaiah 45) to be the author of evil in the sense that the corruption of material objects in nature is ordained by Him, as a means for carrying out the design of the universe; and on the other hand, the evil which exists as a consequence of the breach of Divine laws is in the same sense due to Divine appointment; the universe would be less perfect if its laws could be broken with impunity. Thus evil, in one aspect, i.e. as counter-balancing the deordination of sin, has the nature of good. But the evil of sin, though permitted by God, is in no sense due to him; denying the Divine omnipotence, that another equally perfect universe could not be created in which evil would have no place.
Luther and Calvin.
Both Luther and Calvin explained evil as a consequence of the fall of man and the original sin. However, due to the belief in predestination and omnipotence, the fall is part of God's plan. Ultimately humans may not be able to understand and explain this plan.
Christian Science.
Christian Science views evil as having no ultimate reality and as being due to false beliefs, consciously or unconsciously held. Evils such as illness and death may be banished by correct understanding. This view has been questioned, aside from the general criticisms of the concept of evil as an illusion discussed earlier, since the presumably correct understanding by Christian Science members, including the founder, has not prevented illness and death. However, Christian Scientists believe that the many instances of spiritual healing (as recounted e.g. in the Christian Science periodicals and in the textbook Science and Health with Key to the Scriptures by Mary Baker Eddy) are anecdotal evidence of the correctness of the teaching of the unreality of evil. According to one author, the denial by Christian Scientists that evil ultimately exists neatly solves the problem of evil; however, most people cannot accept that solution.
Jehovah's Witnesses.
Jehovah's Witnesses believe that Satan is the original cause of evil. Though once a perfect angel, Satan developed feelings of self-importance and craved worship, and eventually challenged God's right to rule. Satan caused Adam and Eve to disobey God, and humanity subsequently became participants in a challenge involving the competing claims of Jehovah and Satan to universal sovereignty. Other angels who sided with Satan became demons.
God's subsequent tolerance of evil is explained in part by the value of free will. But Jehovah's Witnesses also hold that this period of suffering is one of non-interference from God, which serves to demonstrate that Jehovah's "right to rule" is both correct and in the best interests of all intelligent beings, settling the "issue of universal sovereignty". Further, it gives individual humans the opportunity to show their willingness to submit to God's rulership.
At some future time known to him, God will consider his right to universal sovereignty to have been settled for all time. The reconciliation of "faithful" humankind will have been accomplished through Christ, and nonconforming humans and demons will have been destroyed. Thereafter, evil (any failure to submit to God's rulership) will be summarily executed.
The Church of Jesus Christ of Latter-day Saints.
The Church of Jesus Christ of Latter-day Saints (LDS Church) introduces a concept similar to Irenaean theodicy, that experiencing evil is a necessary part of the development of the soul. Specifically, the laws of nature prevent an individual from fully comprehending or experiencing good without experiencing its opposite. In this respect, Mormons don't regard the fall of Adam and Eve as a tragic, unplanned cancellation of an eternal paradise; rather it was an essential element of God's plan, because it is through the opposition of mortality that humans "taste the bitter, that they may learn to prize the good."
This necessarily acknowledges a departure from the mainstream Christian definition of omnipotence and omniscience, which Mormons believe was changed by post-apostolic theologians in the centuries after Christ. The writings of Justin Martyr, Origen, Augustine, and others indicate a merging of Christian principles with Greek metaphysical philosophies such as Neoplatonism, which described divinity as an utterly simple, immaterial, formless substance/essence (ousia) that was the absolute causality and creative source of all that existed. Mormons teach that through modern day revelation, God restored the truth about his nature, which eliminated the speculative metaphysical elements that had been incorporated after the Apostolic era. As such, God's omniscience/omnipotence is not to be understood as metaphysically transcending all limits of nature, but as a perfect comprehension of all things within nature—which gives God the power to bring about any state or condition within those bounds. This restoration also clarified that God does not create Ex nihilo (out of nothing), but uses existing materials to organize order out of chaos. Because opposition is inherent in nature, and God operates within nature’s bounds, God is therefore not considered the author of evil—nor can He simply eradicate all evil from the universe. His primary purpose, however, is to help His children to learn for themselves to both appreciate and choose the right, and thus achieve eternal joy.
Islam.
Islamic scholar Sherman Jackson states that the Mu'tazila school emphasized God's omnibenevolence. Evil arises not from God but from the actions of his creations who create their own actions independent of God. The Ash'ari school instead emphasized God's omnipotence. God is not restricted to follow some objective moral system centered on humans but has the power do whatever he wants with his world. The Maturidi school argued that evil arises from God but that evil in the end has a wiser purpose as a whole and for the future. Some theologians have viewed God as all-powerful and human life as being between the hope that God will be merciful and the fear that he will not.
Hinduism.
Hinduism is a complex religion with many different currents or schools. As such the problem of evil in Hinduism is answered in several different ways such as by the concept of karma.
Buddhism.
In Buddhism, the problem of evil, or the related problem of dukkha, is one argument against a benevolent, omnipotent creator god, identifying such a notion as attachment to a false concept.
Pandeism.
While acknowledging that William L. Rowe had raised "a powerful, evidential argument against ethical theism," theologian William C. Lane contends that the theological theory of pandeism escapes the problem of evil. Lane writes of Rowe's proof:
However, it does not count against pandeism. In pandeism, God is no superintending, heavenly power, capable of hourly intervention into earthly affairs. No longer existing "above," God "cannot" intervene from above and cannot be blamed for failing to do so. Instead God "bears" all suffering, whether the fawn's or anyone else's.
Even so, a skeptic might ask, "Why must there be "so" much suffering,? Why could not the world's design omit or modify the events that cause it?" In pandeism, the reason is clear: to remain unified, a world must convey information through transactions. Reliable conveyance requires relatively simple, uniform laws. Laws designed to skip around suffering-causing events or to alter their natural consequences (i.e., their consequences under simple laws) would need to be vastly complicated or (equivalently) to contain numerous exceptions. Such laws would not be discernable from within the world. From that standpoint, the only one that matters, they would not be laws at all. Absent laws, transactions would not reliably convey information, and the world could not be one. God could not consistently become such a world.:76–77
Greek mythology.
One of the most unique relationships between God and humans can be viewed through ancient Greek mythology. Unlike most religions, the Gods in Greek mythology were seen as superior, but shared similar traits with humans and often interacted with them.
Though the Greeks didn't believe in any "evil" gods, the Greeks still acknowledged the fact that evil was present in the world.<ref name=Greek/Gods></ref> Gods often meddled in the affairs of men, and sometimes their actions consisted of bringing misery to people, for example Gods would sometimes be a direct cause of death for people. However, the Greeks did not consider the Gods to be evil, as a result of their actions, instead the answer for most situations in Greek mythology was the power of fate. Fate is considered to be more powerful than the Gods themselves and for this reason no one can escape it. For this reason the Greeks recognized that unfortunate events were justifiable by the idea of fate.
For more information see Greek Mythology.
By philosophers.
Epicurus.
Epicurus is generally credited with first expounding the problem of evil, and it is sometimes called "the Epicurean paradox" or "the riddle of Epicurus":
"Is God willing to prevent evil, but not able? Then he is not omnipotent. Is he able, but not willing? Then he is malevolent. Is he both able and willing? Then whence cometh evil? Is he neither able nor willing? Then why call him God?" — 'the Epicurean paradox'.
Epicurus himself did not leave any written form of this argument. It can be found in Christian theologian Lactantius's "Treatise on the Anger of God" where Lactantius critiques the argument. Epicurus's argument as presented by Lactantius actually argues that a god that is all-powerful and all-good does not exist and that the gods are distant and uninvolved with man's concerns. The gods are neither our friends nor enemies.
David Hume.
David Hume's formulation of the problem of evil in "Dialogues Concerning Natural Religion":
"Is he [God] willing to prevent evil, but not able? then is he impotent. Is he able, but not willing? then is he malevolent. Is he both able and willing? whence then is evil?"
"[God's] power we allow [is] infinite: Whatever he wills is executed: But neither man nor any other animal are happy: Therefore he does not will their happiness. His wisdom is infinite: He is never mistaken in choosing the means to any end: But the course of nature tends not to human or animal felicity: Therefore it is not established for that purpose. Through the whole compass of human knowledge, there are no inferences more certain and infallible than these. In what respect, then, do his benevolence and mercy resemble the benevolence and mercy of men?"
Gottfried Leibniz.
In his "Dictionnaire Historique et Critique", the sceptic Pierre Bayle denied the goodness and omnipotence of God on account of the sufferings experienced in this earthly life. Gottfried Leibniz introduced the term theodicy in his 1710 work "Essais de Théodicée sur la bonté de Dieu, la liberté de l'homme et l'origine du mal" ("Theodicic Essays on the Benevolence of God, the Free will of man, and the Origin of Evil") which was directed mainly against Bayle. He argued that this is the best of all possible worlds that God could have created.
Imitating the example of Leibniz, other philosophers also called their treatises on the problem of evil theodicies. Voltaire's popular novel "Candide" mocked Leibnizian optimism through the fictional tale of a naive youth.
Thomas Robert Malthus.
The population and economic theorist Thomas Malthus argued in a 1798 essay that evil exists to spur human creativity and production. Without evil or the necessity of strife mankind would have remained in a savage state since all amenities would be provided for.
Immanuel Kant.
Immanuel Kant argued for sceptical theism. He claimed there is a reason all possible theodicies must fail: evil is a personal challenge to every human being and can be overcome only by faith. He wrote:
We can understand the necessary limits of our reflections on the subjects which are beyond our reach. This can easily be demonstrated and will put an end once and for all to the trial.
Victor Cousin.
Victor Cousin (1856) stridently argued that different competing philosophical ideologies all had some claim on truth, as they all had arisen in defense of some truth. He however argued that there was a theodicy which united them, and that one should be free in quoting competing and sometimes contradictory ideologies in order to gain a greater understanding of truth through their reconciliation.
Peter Kreeft.
Christian philosopher Peter Kreeft provides several answers to the problem of evil and suffering, including that a) God may use short-term evils for long-range goods, b) God created the possibility of evil, but not the evil itself, and that free will was necessary for the highest good of real love. Kreeft says that being all-powerful doesn't mean being able to do what is logically contradictory, e.g., giving freedom with no potentiality for sin, c) God's own suffering and death on the cross brought about his supreme triumph over the devil, d) God uses suffering to bring about moral character, quoting apostle Paul in Romans 5, e) Suffering can bring people closer to God, and f) The ultimate "answer" to suffering is Jesus himself, who, more than any explanation, is our real need.
William Hatcher.
Mathematical logician William Hatcher (a member of the Baha'i Faith) made use of relational logic to claim that very simple models of moral value cannot be consistent with the premise of evil as an absolute, whereas goodness as an absolute is entirely consistent with the other postulates concerning moral value. In Hatcher's view, one can only validly say that if an act A is "less good" than an act B, one cannot logically commit to saying that A is absolutely evil, unless one is prepared to abandon other more reasonable principles.
See also.
</dl>

</doc>
<doc id="30113" url="http://en.wikipedia.org/wiki?curid=30113" title="Economy of Tajikistan">
Economy of Tajikistan

Since independence, Tajikistan gradually followed the path of transition economy, reforming its economic policies. With foreign revenue precariously dependent upon exports of cotton and aluminium, the economy is highly vulnerable to external shocks. In fiscal year (FY) 2000, international assistance remained an essential source of support for rehabilitation programs that reintegrated former civil war combatants into the civilian economy, thus helping keep the peace. International assistance also was necessary to address the second year of severe drought that resulted in a continued shortfall of food production. Tajikistan's economy grew substantially after the war. The gross domestic product (GDP) of Tajikistan expanded at an average rate of 9.6% over the period of 2000-2007 according to the World Bank data. This improved Tajikistan's position among other Central Asian countries (namely Turkmenistan and Uzbekistan), which have degraded economically ever since. As of August 2009, an estimated 60% of Tajikistani citizens live below the poverty line. The 2008 global financial crisis has hit Tajikistan hard, both domestically and internationally. Tajikistan has been hit harder than many countries because it already has a high poverty rate and because many of its citizens depend on remittances from expatriate Tajikistanis.
Economic history.
This is a chart of trend of gross domestic product of Tajikistan at market prices by the International Monetary Fund with figures in millions of ruling currency.
For purchasing power parity comparisons, the US Dollar is exchanged at 0.82 Somoni only.
The Tajikistani economy has been gravely weakened by six years of civil conflict and loss of markets for its products. Tajikistan thus depends on international humanitarian assistance for much of its basic subsistence needs. Even if the peace agreement of June 1997 is honored, the country faces major problems in integrating refugees and former combatants into the economy. The future of Tajikistan's economy and the potential for attracting foreign investment depend upon stability and continued progress in the peace process.
In 2006 GDP per capita of Tajikistan was 85% of 1990s level. While population has increased from 5.3 million in 1991 to 7.3 million in 2009.
Despite resistance from vested interests, the Government of Tajikistan continued to pursue macroeconomic stabilization and structural reform in FY 2000. In December 1999, the government announced that small-enterprise privatization had been successfully completed, and the privatization of medium-sized and large-owned enterprises (SOEs) continued incrementally. The continued privatization of medium-sized and large SOEs, land reform, and banking reform and restructuring remain top priorities. Shortly after the end of FY 2000, the Board of the International Monetary Fund gave its vote of confidence to the government's recent performance by approving the third annual Poverty Reduction and Growth Facility Loan for Tajikistan. Improved fiscal discipline by the Government of Tajikistan has supported the return to positive economic growth. The government budget was nearly in balance in 2001 and the government’s 2002 budget targets a fiscal deficit of 0.3% of GDP, including recent increases in social sector spending.
Gross domestic product.
In 2005 Tajikistan’s GDP grew by 6.7%, to about US$1.89 billion, and growth for 2006 was about 8%, marking the fifth consecutive year of annual growth exceeding 6%. The official forecast for GDP growth in 2007 is 7.5%. Per capita GDP in 2005 was US$258, lowest among the 15 countries of the former Soviet Union. In 2005 services contributed 48%, agriculture 23.4%, and industry 28.6% to GDP. The recent global recession has reduced Tajikistan's GDP growth rate to 2.8% in the first half of 2009. Remittances from expatriate Tajikistanis is estimated to account for 30-50% of Tajikistan's GDP.
Industries.
Agriculture.
Although the government has announced an expedited land reform program, many Soviet-era state farms still existed in 2006, and the state retains control of production and harvesting on privatized farms. Privatization of cotton farms has been especially slow, and unresolved debts of cotton farmers remained a problem in 2006. In the early 2000s, the major crops were cotton (which occupied one-third of arable land in 2004 but decreased after that date), cereals (mainly wheat), potatoes, vegetables (mainly onions and tomatoes), fruits, and rice. Cotton makes an important contribution to both the agricultural sector and the national economy. Cotton accounts for 60 percent of agricultural output, supports 75 percent of the rural population, and uses 45 percent of irrigated arable land. More than 80% of the 8,800 square kilometers of land in use for agriculture depends on irrigation. Tajikistan must import grain from Kazakhstan and Uzbekistan.
Forestry.
About 5% of Tajikistan is wooded, mainly at elevations between 1,000 and 3,000 meters. No forest region is classified as commercially usable; most are under state protection. Wood production is negligible, but local inhabitants harvest non-wood forest products.
Fishing.
Streams and lakes produce a limited amount of fish, and some fish is produced by aquaculture. In 2003 some 158 tons of fish were caught and 167 tons raised on fish farms.
Mining and Minerals.
Tajikistan has rich deposits of gold, silver, and antimony. The largest silver deposits are in Sughd Province, where Tajikistan’s largest gold mining operation also is located. Russia’s Norilsk nickel company has explored a large new silver deposit at Bolshoy Kanimansur. Tajikistan also produces strontium, salt, lead, zinc, fluorspar, and mercury. Uranium, an important mineral in the Soviet era, remains in some quantity but no longer is extracted. Fossil fuel deposits are limited to coal, of which about 30,000 tons are mined annually. Tajikistan’s extensive aluminium processing industry depends entirely on imported ore.
Industry and manufacturing.
The output of most industries declined sharply during the mid-1990s; despite widespread privatization, in the early 2000s industry rallied very slowly. In 2006 an estimated one-third of Tajikistan’s 700 major industrial enterprises were completely idle, and the remainder were operating at 20 or 25% of capacity. The causes are outmoded equipment, low investment levels, and lack of markets. To revitalize the sector, in 2006 the government was considering renationalizing some enterprises. Tajikistan’s only major heavy industries are aluminum processing and chemical production. The former, which provided 40% of industrial production in 2005, is centered at the Tursunzoda processing plant, the latter in Dushanbe, Qurghonteppa, and Yavan. Aluminum production increased by 6% in 2005. Some small light industrial plants produce textiles and processed foods, using mainly domestic agricultural products. The textile industry processes about 20% of domestically grown cotton. The expansion of light industry output contributed significantly to GDP growth in 2005. The construction industry, about half of which is state-owned, has suffered from low investment in capital projects and from shoddy workmanship that has discouraged international contracts. However, new infrastructure projects and increased housing construction brought a 60% increase in output from 2004 to 2005. As of 2009, one third of industrial plants and factories are inactive, according to Tajikistan's Institute of Economic Studies. Industrial output has fallen by 13% in the first six months of 2009, leading to a fall in export revenues of 48%.
Energy.
The rivers of Tajikistan, such as the Vakhsh and the Panj, have great hydropower potential, and the government has focused on attracting investment for projects for internal use and electricity exports. Tajikistan is home to the hydroelectric power station Nurek, the highest dam in the world. Sangtuda 1 Hydroelectric Power Plant of 670 megawatts (MW) capacity, operated by Russian Inter RAO UES, commenced operations on 18 January 2008 and was officially commissioned on 31 July 2009. Other projects at the development stage include Sangduta 2 by Iran, Zerafshan by Chinese SinoHydro and Rogun power plant, which, at 335 m, is projected to supersede the Nurek Dam as tallest in the world if completed. The Rogun Dam was originally planned to be built by Russia's Inter RAO UES, but following disagreements, Russia pulled out. In 2010, production resumed with Iranian investment and Chinese assistance. Besides hydropower, other energy resources include sizable coal deposits and smaller reserves of natural gas and petroleum. In December 2010, Russian Gazprom announced discovery of significant natural gas reserves in Sarykamish field with 60 bcm of natural gas, enough for 50 years of Tajikistan's domestic consumption. The national power company is Barqi Tojik.
Tajikistan is a partner country of the EU INOGATE energy programme, which has four key topics: enhancing energy security, convergence of member state energy markets on the basis of EU internal energy market principles, supporting sustainable energy development, and attracting investment for energy projects of common and regional interest.
Services.
Throughout the early 2000s, the overall output of the services sector has increased steadily. The banking system has improved significantly because of strengthened oversight by the National Bank of Tajikistan, relaxed restrictions on participation by foreign institutions, and regulatory reform. The system includes 16 commercial banks and the central bank, or National Bank. The state controls the system, although in principle most banks have been privatized. An internationally assisted restructuring program was completed in 2003. Banks provide a narrow range of services, concentrating on providing credit to state-owned enterprises. Only an estimated 10% of the capital in Tajikistan moves through the banking system, and small businesses rarely borrow from banks. Despite substantial potential, the tourism industry, which was eliminated by the civil war, has not re-established itself since the war because of poor infrastructure, lack of promotion, and security concerns. Some small insurance companies began operations in the early 2000s.
Abdujabbor Shirinov, Chairman of the National Bank of Tajikistan announced 142 credit organizations, including 16 banks and 299 their branches, two non-bank financial institutions and 124 microfinance organizations functioned in Tajikistan at the first of 2013.
Labor.
In 2003 Tajikistan’s active labor force was estimated at 3.4 million, of whom 64% were employed in agriculture, 24% in services, and 10% in industry and construction. After declining in the early 2000s, the real wages of state employees were raised in 2004 and 2005. Because of the continued dominance of state farms, the majority of workers are government employees, although only a small number rely completely on wages. Driven by high unemployment, in 2006 an estimated 700,000 workers found seasonal or permanent employment in Russia and other countries. Their remittances, estimated at US$600 million in 2005, are an important economic resource in Tajikistan; in 2004 an estimated 15% of households depended mainly on those payments. In May 2009 remittances to Tajiks had fallen to $525 million, a 34% decline from the previous year. Immediately before the 2008 financial crisis there were an estimated 1.5 million foreign workers sending remittances back to Tajikistan. In 2006 the average wage was US$27 per month. The national unemployment rate was estimated unofficially as high as 40% in 2006, but in rural areas unemployment has exceeded 60%. Unemployment has been higher in the southern Khatlon Province than in the northern Soghd Province. Mean wages were $0.66 per manhour in 2009.
Tajikistan's informal employment sector has been reported to use both child labor and forced labor in the country's cotton industry according to the U.S. Department of Labor's "List of Goods Produced by Child Labor or Forced Labor".
Currency, exchange rate, and inflation.
The somoni was introduced in 2000 to replace the Tajikistani ruble, which had been the currency since 1991. Since 2001 the exchange rate has remained relatively stable. In January 2007, some 3.21 somoni equaled US$1. In April 2011 1 US-$ is about 4,7 somoni.
Throughout the post-Soviet era, inflation has been a serious obstacle to economic growth and improvement of the standard of living. For the years 2001–3, Tajikistan’s inflation rates were 33%, 12.2%, and 16.3%, respectively, but in 2004 the rate fell to 6.8%, and the rate for 2005 was 7.1%. In late 2006, inflation approached the 10% level. The official forecast for 2007 is 7%.
Government budget.
The year 2004 was the first year of budget deficit after three consecutive years of budget surpluses, which in turn had followed four years of deficits between 1997 and 2000. In 2005 revenues totaled US$442 million (aided by improvements in tax collection), and expenditures were US$542 million, a deficit of US$100 million. The approved 2007 state budget calls for revenues of US$926 million and expenditures of US$954 million, leaving a deficit of US$28 million.
Foreign economic relations.
In the post-Soviet era, Tajikistan has substantially shifted its markets away from the former Soviet republics; in 2005 more than 80% of total exports went to customers outside the Commonwealth of Independent States (CIS), including more than 70% to countries of the European Union (EU) and Turkey. However, because most of Tajikistan’s food and energy are imported from CIS countries, in 2005 only about 53% of total trade activity was outside the CIS. In 2005 the top overall buyers of Tajikistan’s exports, in order of value, were the Netherlands, Turkey, Russia, Uzbekistan, Latvia, and Iran. Besides aluminum, which accounts for more than half of export value, the main export commodities are cotton, electric power, fruits, vegetable oils, and textiles. In 2005 the largest suppliers of Tajikistan’s imports, in order of value, were Russia, Kazakhstan, Uzbekistan, Azerbaijan, China, and Ukraine. Those import rankings are determined largely by the high value of fuels and electric power that Tajikistan buys from its neighbors. Another significant import is alumina (aluminum oxide) to supply the aluminum industry. The major suppliers of alumina are Azerbaijan, Kazakhstan, and Ukraine.
Tajikistan has suffered trade deficits throughout the post-Soviet era. In 2003 the deficit was US$97 million, based on exports of US$705 million and imports of US$802 million. In 2004 exports were worth US$736 million and imports, US$958 billion, creating a trade deficit of US$222 million. The deficit increased again in 2005, to US$339 million, mainly because cotton exports decreased and domestic demand for goods increased.
In 2005 the current account deficit was US$86 million, having shown a general downward trend since the late 1990s. The estimated current account deficit for both 2006 and 2007 is 4.5% of GDP, or about US$90 million in 2006. In 2005 the overall balance of payments was US$14 million. The estimated overall balance of payments for 2006 is US$8 million.
At the end of 2006, Tajikistan’s external debt was estimated at US$830 million, most of which was long-term international debt. This amount grew steadily through the 1990s and early 2000s because of state borrowing policy. In 2004 Tajikistan eliminated about 20% of its external debt by exchanging debt to Russia for Russian ownership of the Nurek space tracking station, and by 2006 rescheduling negotiations had reduced the debt by about two-thirds as a percentage of gross domestic product.
In the early 2000s, foreign direct investment has remained low because of political and economic instability, corruption, the poor domestic financial system, and Tajikistan’s geographic isolation. The establishment of businesses nearly always requires bribing officials and often encounters resistance from entrepreneurs with government connections. To attract foreign investment and technology, Tajikistan has offered to establish free economic zones in which firms receive advantages on taxes, fees, and customs. In 2004, the parliament passed a law on free economic zones and in 2008 passed a decree creating two zones: the Panj Free Economic Zone and the Sughd Free Economic Zone. In 2003 foreign direct investment totaled US$41 million; it increased to US$272 million in 2004 because of the debt-reduction transaction with Russia. In the first half of 2005, the figure was US$16 million. Beginning in 2005, the Russian Rusal aluminum company resumed operations to complete the hydroelectric station at Rogun on the Vakhsh River and expand aluminum production at the Tursunzade plant. That plant was scheduled for possible sale to Rusal in 2007. Also in 2005, Russia and Iran resumed work on the Vakhsh River Sangtuda hydroelectric project. Gazprom, the Russian natural gas monopoly, allocated US$12 million for oil and gas exploration in Tajikistan in 2007 after spending US$7 million in 2006. In 2005 the Russian telecommunications company VimpelCom bought a controlling share of Tajikistan’s Tacom mobile telephone company. As of 2006, Turkey tentatively planned to invest in a luxury hotel and a cotton processing plant.
WTO.
Tajikistan joined the World Trade Organization (WTO)on 2 March 2013, becoming the 159th country to join the organization. The Working Party on the accession of Tajikistan was established by the General Council on 18 July 2001. Tajikistan completed its membership negotiations on 26 October 2012, when the Working Party adopted the accession package. The General Council approved the accession on 10 December 2012. The Working Party held its sixth meeting in July 2011 to continue the examination of Tajikistan’s foreign trade regime. As part of bilateral market access negotiations, Tajikistan agreed to lower tariffs on cooking equipment, refrigerators, ovens and water heaters in discussions to gain Thailand's backing. Earlier, the government of Tajikistan confirmed that it had concluded negotiations with Japan, and had received support from the nation for its accession in an agreement signed on July 31, 2012. 
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="30128" url="http://en.wikipedia.org/wiki?curid=30128" title="Thailand">
Thailand

Thailand ( or ; Thai: ประเทศไทย, rtgs: "Prathet Thai"), officially the Kingdom of Thailand (Thai: ราชอาณาจักรไทย, rtgs: "Ratcha Anachak Thai";  ]), formerly known as Siam (Thai: สยาม; rtgs: Sayam), is a country at the centre of the Indochina peninsula in Southeast Asia. It is bordered to the north by Burma and Laos, to the east by Laos and Cambodia, to the south by the Gulf of Thailand and Malaysia, and to the west by the Andaman Sea and the southern extremity of Burma. Its maritime boundaries include Vietnam in the Gulf of Thailand to the southeast, and Indonesia and India on the Andaman Sea to the southwest. 
Thailand is a monarchy headed by King Bhumibol Adulyadej, Rama IX, and governed by a military junta that took power in May 2014. The king is the ninth of the House of Chakri, and has reigned since 1946 as the world's longest-serving current head of state and the country's longest-reigning monarch. The King of Thailand's titles include Head of State, Head of the Armed Forces, Adherent of Buddhism, and Upholder of religions.
With a total area of approximately 513000 km2, Thailand is the world's 51st-largest country. It is the 20th-most-populous country in the world, with around 66 million people. The capital and largest city is Bangkok, which is Thailand's political, commercial, industrial, and cultural hub. About 75–95% of the population is ethnically Tai, which includes four major regional groups: central Thai, northeastern Thai (Khon [Lao] Isan), northern Thai (Khon Mueang); and southern Thai. Thai Chinese, those of significant Chinese heritage, are 14% of the population, while Thais with partial Chinese ancestry comprise up to 40% of the population. Thai Malays represent 3% of the population, with the remainder consisting of Mons, Khmers and various "hill tribes". The country's official language is Thai and the primary religion is Buddhism, which is practised by around 95% of the population.
Thailand experienced rapid economic growth between 1985 and 1996, becoming a newly industrialised country and a major exporter. Manufacturing, agriculture, and tourism are leading sectors of the economy. Among the ten ASEAN countries, Thailand ranks second in quality of life and the country's HDI had been rated as "high". Its large population and growing economic influence have made it a middle power in the region and around the world.
Etymology.
The country has always been called "Mueang Thai" by its citizens; but by others, by the exonym Siam (Thai: สยาม rtgs: Sayam,  ]). Also spelled "Siem", "Syâm", or "Syâma", it has been identified with the Sanskrit "Śyāma" (श्याम, meaning "dark" or "brown"). The names Shan and A-hom seem to be variants of the same word, and Śyâma is possibly not its origin, but a learned and artificial distortion.
The signature of King Mongkut (r. 1851 – 1868) reads "SPPM" (Somdet Phra Poramenthra Maha) "Mongkut King of Siam", giving it official status until 23 June 1939 when it was changed to Thailand. Thailand was renamed Siam from 1945 to 11 May 1949, after which it again reverted to Thailand.
The word "Thai" (ไทย) is not, as commonly believed, derived from the word "Thai" (ไท) meaning "independence" in the Thai language; it is, however, the name of an ethnic group from the central plains (the Thai people). A famous Thai scholar argued that Thai (ไท) simply means "people" or "human being" since his investigation shows that in some rural areas the word "Thai" was used instead of the usual Thai word "khon" (คน) for people.
The Thai use of the phrase "land of the freedom" expresses pride in the fact that Thailand is the only country in Southeast Asia never colonised by a European power.
While Thai people will often refer to their country using the polite form "prathet Thai" (Thai: ประเทศไทย), they most commonly use the more colloquial word "mueang Thai" (Thai: เมืองไทย) or simply "Thai" (Thai: ไทย), the word "mueang" (Thai: เมือง) meaning "nation" but most commonly used to refer to a city or town. "Ratcha Anachak Thai" (Thai: ราชอาณาจักรไทย) means "kingdom of Thailand" or "kingdom of Thai". Etymologically, its components are: "-Ratcha-" (from Sanskrit "raja", meaning "king, royal, realm") ; "-ana-" (from Pāli "āṇā", "authority, command, power", itself from Sanskrit "ājñā", same meaning) "-chak" (from Sanskrit "cakra" or "cakraṃ" meaning "wheel", a symbol of power and rule). The Thai National Anthem (Thai: เพลงชาติ), written by Luang Saranupraphan during the extremely patriotic 1930s, refers to the Thai nation as: "prathet Thai" (Thai: ประเทศไทย). The first line of the national anthem is: "prathet thai ruam lueat nuea chat chuea thai" (Thai: ประเทศไทยรวมเลือดเนื้อชาติเชื้อไทย), "Thailand is the unity of Thai flesh and blood."
History.
There is evidence of human habitation in Thailand that has been dated at 40,000 years before the present, with stone artefacts dated to this period at Tham Lod Rockshelter in Mae Hong Son. Similar to other regions in Southeast Asia, Thailand was heavily influenced by the culture and religions of India, starting with the Kingdom of Funan around the 1st century CE to the Khmer Empire.
Ayutthaya was burned and sacked in 1767 by a Burmese army under King Hsinbyushin. Indian influence on Siamese culture was partly the result of direct contact with Indian settlers, but mainly it was brought about indirectly via the Indianized kingdoms of Dvaravati, Srivijaya, and Cambodia. E:A Voretzsch believes that Buddhism must have been flowing into Siam from India in the time of the Indian Emperor Ashoka of the Maurya Empire and far on into the first millennium after Christ. Later Thailand was influenced by the south Indian Pallava Dynasty and north Indian Gupta Empire.
After the fall of the Khmer Empire in the 13th century, various states thrived there, such as the various Tai, Mon, Khmer, and Malay Kingdoms, as seen through the numerous archaeological sites and artefacts that are scattered throughout the Siamese landscape. Prior to the 12th century however, the first Thai or Siamese state is traditionally considered to be the Buddhist kingdom of Sukhothai, which was founded in 1238.
Following the decline and fall of the Khmer empire in the 13th–15th century, the Buddhist Tai kingdoms of Sukhothai, Lanna, and Lan Xang (now Laos) were on the rise. However, a century later, the power of Sukhothai was overshadowed by the new Kingdom of Ayutthaya, established in the mid-14th century in the lower Chao Phraya River or Menam area.
Ayutthaya's expansion centred along the Menam while in the northern valleys the Lanna Kingdom and other small Tai city-states ruled the area. In 1431, the Khmer abandoned Angkor after Ayutthaya forces invaded the city. Thailand retained a tradition of trade with its neighbouring states, from China to India, Persia, and Arab lands. Ayutthaya became one of the most vibrant trading centres in Asia. European traders arrived in the 16th century, beginning with the Portuguese, followed by the French, Dutch, and English.
After the fall of Ayutthaya in 1767 to the Burmese, King Taksin the Great moved the capital of Thailand to Thonburi for approximately 15 years. The current Rattanakosin era of Thai history began in 1782, following the establishment of Bangkok as capital of the Chakri dynasty under King Rama I the Great. According to the Encyclopædia Britannica, "A quarter to a third of the population of some areas of Thailand and Burma were slaves in the 17th through the 19th centuries."
Despite European pressure, Thailand is the only Southeast Asian nation to never have been colonised. This has been ascribed to the long succession of able rulers in the past four centuries who exploited the rivalry and tension between French Indochina and the British Empire. As a result, the country remained a buffer state between parts of Southeast Asia that were colonised by the two colonial powers, Great Britain and France. Western influence nevertheless led to many reforms in the 19th century and major concessions, most notably the loss of a large territory on the east side of the Mekong to the French and the step-by-step absorption by Britain of the Shan and Karen people areas and Malay Peninsula.
20th century.
The losses initially included Penang and eventually culminated in the loss of four predominantly ethnic-Malay southern provinces, which later became Malaysia's four northern states, under the Anglo-Siamese Treaty of 1909.
In 1932, a bloodless revolution carried out by the Khana Ratsadon group of military and civilian officials resulted in a transition of power, when King Prajadhipok was forced to grant the people of Siam their first constitution, thereby ending centuries of absolute monarchy.
In 1939, the name of the kingdom, "Siam", was changed to "Thailand".
World War II.
During World War II, the Empire of Japan demanded the right to move troops across Thailand to the Malayan frontier. Japan invaded Thailand on 8 December 1941, in co-ordination with attacks throughout Asia, and engaged the Thai Army for six to eight hours before Plaek Pibulsonggram ordered an armistice. Shortly thereafter, Japan was granted free passage, and on 21 December 1941, Thailand and Japan signed a military alliance with a secret protocol, wherein Tokyo agreed to help Thailand regain territories lost to the British and French.
Subsequently, Thailand declared war on the United States and the United Kingdom on 25 January 1942, and undertook to "assist" Japan in its war against the Allies, while at the same time maintaining an active anti-Japanese resistance movement known as the Seri Thai. Approximately 200,000 Asian labourers (mainly romusha) and 60,000 Allied prisoners of war (POWs) worked on the Thailand–Burma Death Railway.
After the war, Thailand emerged as an ally of the United States. As with many of the developing nations during the Cold War, Thailand then went through decades of political instability characterised by a number of coups d'état, as one military regime replaced another, but eventually progressed towards a stable, prosperous democracy in the 1980s.
Politics and government.
The politics of Thailand is currently conducted within the framework of a constitutional monarchy, whereby the Prime Minister is the head of government and a hereditary monarch is head of state. The judiciary is supposed to be independent of the executive and the legislative branches, although judicial rulings are suspected of being based on political considerations rather than on existing law.
Constitutional history.
Since the political reform of the absolute monarchy in 1932, Thailand has had 19 constitutions and charters. Throughout this time, the form of government has ranged from military dictatorship to electoral democracy, but all governments have acknowledged a hereditary monarch as the head of state.
28 June 1932.
Prior to 1932, the Kingdom of Siam did not possess a legislature, as all legislative powers were vested in the person of the monarch. This had been the case since the foundation of the Sukhothai Kingdom in the 12th century as the king was seen as a "Dharmaraja" or "king who rules in accordance with Dharma", (the Buddhist law of righteousness). However, on 24 June 1932 a group of civilians and military officers, calling themselves the Khana Ratsadon (or People's Party) carried out a bloodless revolution, in which the 150 years of absolute rule of the House of Chakri was ended. In its stead the group advocated a constitutional form of monarchy with an elected legislature.
The "Draft Constitution" of 1932 signed by King Prajadhipok created Thailand's first legislature, a People's Assembly with 70 appointed members. The assembly met for the first time on 28 June 1932, in the Ananda Samakhom Throne Hall. The Khana Ratsadon decided that the people were not yet ready for an elected assembly. They later changed their minds. By the time the "permanent" constitution came into force in December of that year, elections were scheduled for 15 November 1933. The new constitution changed the composition of the assembly to 78 directly elected and 78 appointed (by the Khana Ratsadon), together totalling 156 members.
1932 to 1972.
The history of Thailand from 1932 to 1973 was dominated by military dictatorships which were in power for much of the period. The main personalities of the period were the dictator Luang Phibunsongkhram (better known as Phibun), who allied the country with Japan during the Second World War, and the civilian politician Pridi Phanomyong, who founded Thammasat University and was briefly the prime minister after the war. The Japanese invasion of Thailand occurred on 8 December 1941.
A succession of military dictators followed Pridi's ousting — Phibun again, Sarit Dhanarajata and Thanom Kittikachorn — under whom traditional, authoritarian rule was combined with increasing modernisation and westernisation under the influence of the US. The end of the period was marked by Thanom's resignation, following a massacre of pro-democracy protesters led by Thammasat students. Thanom misread the situation as a coup d'état, and fled, leaving the country leaderless. HM appointed Thammasat University chancellor Sanya Dharmasakti PM by royal command. For events subsequent to the abdication of the king, including the name change of 1939, up to the coup d'état of 1957, see Plaek Pibulsonggram.
Thailand helped the USA and South Vietnam in the Vietnam War between 1965–1971. The USAF based F-4 Phantom fighters at Udon and Ubon Air Base, and stationed B-52s at U-Tapao. Thai forces also saw heavy action in the covert war in Laos that occurred from 1964 to 1972.
1997 to 2001.
The 1997 Constitution was the first constitution to be drafted by popularly elected Constitutional Drafting Assembly, and was popularly called the "people's constitution". The 1997 Constitution created a bicameral legislature consisting of a 500-seat House of Representatives (สภาผู้แทนราษฎร, "sapha phu thaen ratsadon") and a 200-seat Senate (วุฒิสภา, "wutthisapha"). For the first time in Thai history, both houses were directly elected.
Many human rights were explicitly acknowledged, and measures were established to increase the stability of elected governments. The House was elected by the first past the post system, where only one candidate with a simple majority could be elected in one constituency. The Senate was elected based on the provincial system, where one province could return more than one senator depending on its population size.
The two houses of the National Assembly have two different terms. In accordance with the constitution the Senate is elected to a six-year term, while the House is elected to a four-year term. Overall the term of the National Assembly is based on that of the House. The National Assembly each year will sit in two sessions: an "ordinary session" and a "legislative session". The first session of the National Assembly must take place within thirty days after the general election of the House of Representatives. The first session must be opened by the king in person by reading a Speech from the Throne; this ceremony is held in the Ananta Samakhom Throne Hall. He may also appoint the crown prince or a representative to carry out this duty. It is also the duty of the king to prorogue sessions through a royal decree when the House term expires. The king also has the prerogative to call extraordinary sessions and prolong sessions upon advice of the House of Representatives.
The National Assembly may host a "joint-sitting" of both Houses under several circumstances. These include: The appointment of a regent, any alteration to the 1924 Palace Law of Succession, the opening of the first session, the announcement of policies by the Cabinet of Thailand, the approval of the declaration of war, the hearing of explanations and approval of a treaty and the amendment of the Constitution.
Members of the House of Representatives served four-year terms, while senators served six-year terms. The 1997 People's Constitution also promoted human rights more than any other constitution. The court system (ศาล, san) included a constitutional court with jurisdiction over the constitutionality of parliamentary acts, royal decrees, and political matters.
2001 to 2008.
The January 2001 general election, the first election under the 1997 Constitution, was called the most open, corruption-free election in Thai history.
Thai Rak Thai Party, led by Thaksin Shinawatra won the election. The Thaksin government was the first in Thai history to complete a four-year term. The 2005 election had the highest voter turnout in Thai history, and Thai Rak Thai Party won an absolute majority. However, despite efforts to clean up the system, vote buying and electoral violence remained electoral problems in 2005.
The PollWatch Foundation, Thailand's most prominent election watchdog, declared that vote buying in this election, specifically in the north and the northeast, was more serious than in the 2001 election. The organisation also accused the government of violating the election law by abusing state power in presenting new projects in a bid to seek votes.
2006 coup d'état.
Without meeting much resistance, a military junta overthrew the interim government of Thaksin Shinawatra on 19 September 2006. The junta abrogated the constitution, dissolved Parliament and the Constitutional Court, detained and later removed several members of the government, declared martial law, and appointed one of the king's Privy Counselors, General Surayud Chulanont, as the Prime Minister. The junta later wrote a highly abbreviated interim constitution and appointed a panel to draft a new permanent constitution. The junta also appointed a 250-member legislature, called by some critics a "chamber of generals" while others claimed that it lacks representatives from the poor majority.
In this interim constitution draft, the head of the junta was allowed to remove the prime minister at any time. The legislature was not allowed to hold a vote of confidence against the cabinet and the public was not allowed to file comments on bills. This interim constitution was later surpassed by the permanent constitution on 24 August 2007. Martial law was partially revoked in January 2007. The ban on political activities was lifted in July 2007, following the 30 May dissolution of the Thai Rak Thai party. The new constitution was approved by referendum on 19 August, which led to a return to a democratic general election on 23 December 2007.
2008–2010 political crisis.
The People's Power Party (Thailand), led by Samak Sundaravej formed a government with five smaller parties. Following several court rulings against him in a variety of scandals, and surviving a vote of no confidence, and protesters blockading government buildings and airports, in September 2008, Sundaravej was found guilty of conflict of interest by the Constitutional Court of Thailand (due to being a host in a TV cooking program), and thus, ended his term in office.
He was replaced by PPP member Somchai Wongsawat. As of October 2008, Wongsawat was unable to gain access to his offices, which were occupied by protesters from the People's Alliance for Democracy. On 2 December 2008, Thailand's Constitutional Court in a highly controversial ruling found the Peoples Power Party guilty of electoral fraud, which led to the dissolution of the party according to the law. It was later alleged in media reports that at least one member of the judiciary had a telephone conversation with officials working for the Office of the Privy Council and one other. The phone call was taped and has since circulated on the Internet. In it, the callers discuss finding a way to ensure the ruling PPP party would be disbanded. Accusations of judicial interference were levelled in the media but the recorded call was dismissed as a hoax. However, in June 2010, supporters of the eventually disbanded PPP were charged with tapping a judge's phone.
Immediately following what many media described as a "judicial coup", a senior member of the Armed Forces met with factions of the governing coalition to get their members to join the opposition and the Democrat Party was able to form a government, a first for the party since 2001. The leader of the Democrat party, and former leader of the opposition, Abhisit Vejjajiva was appointed and sworn-in as the 27th Prime Minister, together with the new cabinet on 17 December 2008.
In April 2009, protests by the National United Front of Democracy Against Dictatorship (UDD, or "Red Shirts") forced the cancellation of the Fourth East Asia Summit after protesters stormed the Royal Cliff hotel venue in Pattaya, smashing the glass doors of the venue to gain entry, and a blockade prevented the Chinese premier at the time, Wen Jiabao, from attending. The summit was eventually held in Thailand in October 2009.
About a year later, a set of new "Red Shirts" protests resulted in 87 deaths (mostly civilian and some military) and 1,378 injured. When the army tried to disperse the protesters on 10 April 2010, the army was met with automatic gunfire, grenades, and fire bombs from the opposition faction in the army, known as the "watermelon". This resulted in the army returning fire with rubber bullets and some live ammunition. During the time of the "red shirt" protests against the government, there have been numerous grenade and bomb attacks against government offices and the homes of government officials. Gas grenades were fired at "yellow-shirt" protesters, that were protesting against the "red-shirts" and in favour of the government, by unknown gunmen killing one pro-government protester, the government stated that the Red Shirts were firing the weapons at civilians. Red-shirts continued to hold a position in the business district of Bangkok and it was shut down for several weeks.
On 3 July 2011, the oppositional Pheu Thai Party, led by Yingluck Shinawatra (the youngest sister of Thaksin Shinawatra), won the general election by a landslide (265 seats in the House of Representatives, out of 500). She had never previously been involved in politics, Pheu Thai campaigning for her with the slogan 'Thaksin thinks, Pheu Thai acts'. Yingluck is the nation's first female prime minister and her role was officially endorsed in a ceremony presided over by King Bhumibol Adulyadej. The Pheu Thai Party is a continuation of Thaksin's Thai Rak Thai party.
2013–2014 political crisis.
Protests recommenced in late 2013, as a broad alliance of protestors, led by former opposition deputy leader Suthep Thaugsuban, demanded an end to the so-called Thaksin regime. A blanket amnesty for people involved in the 2010 protests, altered at the last minute to include all political crimes – including all convictions against Thaksin – triggered a mass show of discontent, with numbers variously estimated between 98,500 (the police) and 400,000 (an aerial photo survey done by the Bangkok Post), taking to the streets. The Senate was urged to reject the bill to quell the reaction, but the measure failed. A newly named group, the People's Democratic Reform Committee (PDRC) along with allied groups, escalated the pressure, with the opposition Democrat party resigning en masse to create a parliamentary vacuum. Protesters demands variously evolved as the movement's numbers grew, extending a number of deadlines and demands that became increasingly unreasonable or unrealistic, yet attracting a groundswell of support. They called for the establishment of an indirectly elected “people’s council”—in place of Yingluck's government—that will cleanse Thai politics and eradicate the Thaksin regime.
In response to the intensive protests, Yingluck dissolved parliament on 9 December 2013 and proposed a new election for 2 February 2014, a date that was later approved by the election commission. The PDRC insisted that the prime minister stand down within 24 hours, regardless of her actions, with 160,000 protesters in attendance at Government House on 9 December. Yingluck insisted that she would continue her duties until the scheduled election in February 2014, urging the protesters to accept her proposal: "Now that the government has dissolved parliament, I ask that you stop protesting and that all sides work towards elections. I have backed down to the point where I don't know how to back down any further."
In response to the Electoral Commission (EC)'s registration process for party-list candidates—for the scheduled election in February 2014—anti-government protesters marched to the Thai-Japanese sports stadium, the venue of the registration process, on 22 December 2013. Suthep and the PDRC led the protest, of which security forces claimed that approximately 270,000 protesters joined. Yingluck and the Pheu Thai Party reiterated their election plan and anticipate presenting a list of 125 party-list candidates to the EC.
On 7 May 2014, the Constitutional Court ruled that Yingluck would have to step down as the Prime Minister as she was deemed to have abused her power in transferring a high-level government official. On 21 August 2014 she was replaced by army chief General Prayut Chan-o-cha.
2014 coup d'état.
On 20 May 2014 the Thai army declared martial law and began to deploy troops in the capital. They denied that it was a coup attempt. On 22 May, the army announced that it was a coup and that it was taking control of the country and suspending the country's constitution. On the same day, the military announced imposed a curfew between the hours of 22:00–05:00, ordering citizens and visitors to remain indoors during this period. On 21 August 2014 the National Assembly of Thailand elected the army chief, General Prayut Chan-o-cha, as prime minister. Martial law was declared formally ended on 1 April 2015
Administrative divisions.
Thailand is divided into 76 provinces (จังหวัด, changwat), which are gathered into 5 groups of provinces by location. There are also 2 specially-governed districts: the capital Bangkok (Krung Thep Maha Nakhon) and Pattaya. Bangkok is at provincial level and thus often counted as a province.
Each province is divided into districts and the districts are further divided into sub-districts (tambons). As of 2006 there were 877 districts (อำเภอ, amphoe) and the 50 districts of Bangkok (เขต, khet). Some parts of the provinces bordering Bangkok are also referred to as Greater Bangkok (ปริมณฑล, pari monthon). These provinces include Nonthaburi, Pathum Thani, Samut Prakan, Nakhon Pathom and Samut Sakhon. The name of each province's capital city (เมือง, mueang) is the same as that of the province. For example, the capital of Chiang Mai Province ("Changwat Chiang Mai") is "Mueang Chiang Mai" or "Chiang Mai".
Southern region.
Thailand controlled the Malay Peninsula as far south as Malacca in the 1400s and held much of the peninsula, including Temasek (Singapore), some of the Andaman Islands, and a colony on Java, but eventually contracted when the British used force to guarantee their suzerainty over the sultanate.
Mostly the northern states of the Malay Sultanate presented annual gifts to the Thai king in the form of a golden flower—a gesture of tribute and an acknowledgement of vassalage. The British intervened in the Malay State and with the Anglo-Siamese Treaty tried to build a railway from the south to Bangkok. Thailand relinquished sovereignty over what are now the northern Malay provinces of Kedah, Perlis, Kelantan, and Terengganu to the British. Satun and Pattani Provinces were given to Thailand.
The Malay peninsular provinces were occupied by the Japanese during World War II, and infiltrated by the Malayan Communist Party (CPM) from 1942 to 2008, when they sued for peace with the Malaysian and Thai governments after the CPM lost its support from Vietnam and China subsequent to the Cultural Revolution. Recent insurgent uprisings may be a continuation of separatist fighting which started after World War II with Sukarno's support for the PULO. Most victims since the uprisings have been Buddhist and Muslim bystanders.
Foreign relations.
The foreign relations of Thailand are handled by the Minister of Foreign Affairs.
Thailand participates fully in international and regional organisations. It is a major non-NATO ally and Priority Watch List Special 301 Report of the United States. The country remains an active member of ASEAN (Association of South East Asian Nations). Thailand has developed increasingly close ties with other ASEAN members: Indonesia, Malaysia, the Philippines, Singapore, Brunei, Laos, Cambodia, Burma, and Vietnam, whose foreign and economic ministers hold annual meetings. Regional co-operation is progressing in economic, trade, banking, political, and cultural matters. In 2003, Thailand served as APEC (Asia Pacific Economic Cooperation) host. Dr. Supachai Panitchpakdi, the former Deputy Prime Minister of Thailand, currently serves as Secretary-General of the United Nations Conference on Trade and Development (UNCTAD). In 2005 Thailand attended the inaugural East Asia Summit.
In recent years, Thailand has taken an increasingly active role on the international stage. When East Timor gained independence from Indonesia, Thailand, for the first time in its history, contributed troops to the international peacekeeping effort. Its troops remain there today as part of a UN peacekeeping force. As part of its effort to increase international ties, Thailand has reached out to such regional organisations as the Organization of American States (OAS) and the Organisation for Security and Cooperation in Europe (OSCE). Thailand has contributed troops to reconstruction efforts in Afghanistan and Iraq.
Thaksin initiated negotiations for several free trade agreements with China, Australia, Bahrain, India, and the US. The latter especially was criticised, with claims that uncompetitive Thai industries could be wiped out.
Thaksin also announced that Thailand would forsake foreign aid, and work with donor countries to assist in the development of neighbours in the Greater Mekong Sub-region. Thaksin sought to position Thailand as a regional leader, initiating various development projects in poorer neighbouring countries like Laos. More controversially, he established close, friendly ties with the Burmese dictatorship.
Thailand joined the US-led invasion of Iraq, sending a 423-strong humanitarian contingent. It withdrew its troops on 10 September 2004. Two Thai soldiers died in Iraq in an insurgent attack.
Abhisit appointed Peoples Alliance for Democracy leader Kasit Piromya as foreign minister. In April 2009, fighting broke out between Thai and Cambodian troops on territory immediately adjacent to the 900-year-old ruins of Cambodia's Preah Vihear Hindu temple near the border. The Cambodian government claimed its army had killed at least four Thais and captured 10 more, although the Thai government denied that any Thai soldiers were killed or injured. Two Cambodian and three Thai soldiers were killed. Both armies blamed the other for firing first and denied entering the other's territory.
Armed forces.
The Royal Thai Armed Forces (Thai: กองทัพไทย, "Kong Thap Thai") constitute the military of the Kingdom of Thailand. It consists of the Royal Thai Army (กองทัพบกไทย), the Royal Thai Navy (กองทัพเรือไทย), and the Royal Thai Air Force (กองทัพอากาศไทย). It also incorporates various paramilitary forces.
The Thai Armed Forces have a combined manpower of 306,000 active duty personnel and another 245,000 active reserve personnel. The head of the Thai Armed Forces (จอมทัพไทย, "Chom Thap Thai") is King Bhumibol Adulyadej (Rama IX), although this position is only nominal. The armed forces are managed by the Ministry of Defence of Thailand, which is headed by the Minister of Defence (a member of the cabinet of Thailand) and commanded by the Royal Thai Armed Forces Headquarters, which in turn is headed by the Chief of Defence Forces of Thailand. In 2011, Thailand's known military expenditure totalled approximately US$5.1 billion.
According to the constitution, serving in the armed forces is a duty of all Thai citizens. However, only males over the age of 21, who have not gone through reserve training of the Army Reserve Force Students, are given the option of volunteering for the armed forces, or participating in the random draft. The candidates are subjected to varying lengths of training, from six months to two years of full-time service, depending on their education, whether they have partially completed the reserve training course, and whether they volunteered prior to the draft date (usually 1 April every year).
Candidates with a recognised bachelor's degree serve one year of full-time service if they are conscripted, or six months if they volunteer at their district office (สัสดี, "satsadi"). Likewise, the training length is also reduced for those who have partially completed the three-year reserve training course (ร.ด., "ro do"). A person who completed one year out of three will only have to serve full-time for one year. Those who completed two years of reserve training will only have to do six months of full-time training, while those who complete three years or more of reserve training will be exempted entirely.
Royal Thai Armed Forces Day is celebrated on 18 January, commemorating the victory of King Naresuan the Great in battle against the Crown Prince of Burma in 1593.
Geography.
Totalling 513120 km2, Thailand is the world's 51st-largest country by total area. It is slightly smaller than Yemen and slightly larger than Spain.
Thailand is home to several distinct geographic regions, partly corresponding to the provincial groups. The north of the country is the mountainous area of the Thai highlands, with the highest point being Doi Inthanon in the Thanon Thong Chai Range at 2565 m above sea level. The northeast, Isan, consists of the Khorat Plateau, bordered to the east by the Mekong River. The centre of the country is dominated by the predominantly flat Chao Phraya river valley, which runs into the Gulf of Thailand.
Southern Thailand consists of the narrow Kra Isthmus that widens into the Malay Peninsula. Politically, there are six geographical regions which differ from the others in population, basic resources, natural features, and level of social and economic development. The diversity of the regions is the most pronounced attribute of Thailand's physical setting.
The Chao Phraya and the Mekong River are the indispensable water courses of rural Thailand. Industrial scale production of crops use both rivers and their tributaries. The Gulf of Thailand covers 320000 km2 and is fed by the Chao Phraya, Mae Klong, Bang Pakong, and Tapi Rivers. It contributes to the tourism sector owing to its clear shallow waters along the coasts in the southern region and the Kra Isthmus. The eastern shore of the Gulf of Thailand is an industrial centre of Thailand with the kingdom's premier deepwater port in Sattahip and its busiest commercial port, Laem Chabang.
The Andaman Sea is a precious natural resource as it hosts the most popular and luxurious resorts in Asia. Phuket, Krabi, Ranong, Phang Nga, and Trang and their islands all lay along the coasts of the Andaman Sea and despite the 2004 tsunami, they are a tourist magnet for visitors from around the world.
Plans have resurfaced for a canal which would connect the Andaman Sea to the Gulf of Thailand, analogous to the Suez and the Panama Canals. The idea has been greeted positively by Thai politicians as it would cut fees charged by the Ports of Singapore, improve ties with China and India, lower shipping times, and eliminate pirate attacks in the Strait of Malacca, and support the Thai government's policy of being the logistical hub for Southeast Asia. The canal, it is claimed, would improve economic conditions in the south of Thailand, which relies heavily on tourism income, and it would also change the structure of the Thai economy by making it an Asia logistical hub. The canal would be a major engineering project and has an expected cost of US$20–30 billion.
Climate.
Most of Thailand has a "tropical wet and dry or savanna climate" type (Köppen's Tropical savanna climate). The south and the eastern tip of the east have a tropical monsoon climate.
Countrywide, temperatures normally range from an average annual high of 38 °C to a low of 19 °C. During the dry season, the temperature rises dramatically in the second half of March, spiking to well over 40 °C in some areas by mid-April when the sun passes its zenith.
Southwest monsoons that arrive between May and July (except in the south) signal the advent of the rainy season ("ruedu fon"). This lasts into October and the cloud covering reduces the temperature again, with the high humidity experienced as 'hot and sticky'. November and December mark the onset of the dry season and night temperatures on high ground can occasionally drop to a light frost. Temperatures begin to climb again in January.
Wildlife.
The elephant is Thailand's national symbol. Although there were 100,000 domesticated elephants in Thailand in 1850, the population of elephants has dropped to an estimated 2,000. Poachers have long hunted elephants for ivory, meat, and hides. Young elephants are often captured for use in tourist attractions or as work animals, although their use has declined since the government banned logging in 1989. There are now more elephants in captivity than in the wild, and environmental activists claim that elephants in captivity are often mistreated.
Poaching of protected species remains a major problem. Hunters have decimated the populations of tigers, leopards, and other large cats for their valuable pelts. Many animals (including tigers, bears, crocodiles, and king cobras) are farmed or hunted for their meat, which is considered a delicacy, and for their supposed medicinal properties. Although such trade is illegal, the famous Bangkok market Chatuchak is still known for the sale of endangered species.
The practice of keeping wild animals as pets threatens several species. Baby animals are typically captured and sold, which often requires killing the mother. Once in captivity and out of their natural habitat, many pets die or fail to reproduce. Affected populations include the Asiatic black bear, Malayan sun bear, white-handed lar, pileated gibbon and binturong.
Education.
In 2014 the literacy rate was 93.5%. Education is provided by a well-organized school system of kindergartens, primary, lower secondary and upper secondary schools, numerous vocational colleges, and universities. The private sector of education is well developed and significantly contributes to the overall provision of education which the government would not be able to meet with public establishments. Education is compulsory up to and including age 14, with the government providing free education through to age 17.
Teaching relies heavily on rote learning rather than on student-centred methodology. The establishment of reliable and coherent curricula for its primary and secondary schools is subject to such rapid changes that schools and their teachers are not always sure what they are supposed to be teaching, and authors and publishers of textbooks are unable to write and print new editions quickly enough to keep up with the volatility. Issues concerning university entrance has been in constant upheaval for a number of years. Nevertheless, Thai education has seen its greatest progress in the years since 2001. Most of the present generation of students are computer literate. Thailand was ranked 54th out of 56 countries globally for English proficiency, the second-lowest in Asia.
Students in ethnic minority areas score consistently lower in standardised national and international tests.
This is likely due to unequal allocation of educational resources, weak teacher training, poverty, and low Thai language skill, the language of the tests.
Extensive nationwide IQ tests were administered to 72,780 Thai students from December 2010 to January 2011. The average IQ was found to be 98.59, which is higher than previous studies have found. IQ levels were found to be inconsistent throughout the country, with the lowest average of 88.07 found in the southern region of Narathiwat Province and the highest average of 108.91 reported in Nonthaburi Province. The Ministry of Public Health blames the discrepancies on iodine deficiency and steps are being taken to require that iodine be added to table salt, a practice common in many Western countries.
In 2013, the Ministry of Information and Communication Technology announced that 27,231 schools would receive classroom-level access to high-speed internet.#Redirect 
Science and technology.
The National Science and Technology Development Agency is an agency of the government of Thailand which supports research in science and technology and its application in the Thai economy.
The Synchrotron Light Research Institute (SLRI) is a Thai synchrotron light source for physics, chemistry, material science, and life sciences. It is at the Suranaree University of Technology (SUT), in Nakhon Ratchasima, about 300 km northeast of Bangkok. The institute, financed by the Ministry of Science and Technology (MOST), houses the only large scale synchrotron in Southeast Asia. It was originally built as the SORTEC synchrotron in Japan and later moved to Thailand and modified for 1.2 GeV operation. It provides users with regularly scheduled light.
Internet.
In Bangkok, there are 23,000 free public Wi-Fi Internet hotspots. The Internet in Thailand includes 10Gbit/s high speed fibre-optic lines that can be leased and ISPs such as KIRZ that provide residential Internet services.
The Internet is censored by the Thai government, making some sites unreachable. The organisations responsible are the Royal Thai Police, the Communications Authority of Thailand, and the Ministry of Information and Communication Technology (MICT).
Economy.
Thailand is an emerging economy and is considered a newly industrialised country. Thailand had a 2013 GDP of US$673 billion (on a purchasing power parity [PPP] basis). Thailand is the 2nd largest economy in Southeast Asia after Indonesia. Thailand ranks midway in the wealth spread in Southeast Asia as it is the 4th richest nation according to GDP per capita, after Singapore, Brunei, and Malaysia.
Thailand functions as an anchor economy for the neighbouring developing economies of Laos, Burma, and Cambodia. In the third quarter of 2014, the unemployment rate in Thailand stood at 0.84% according to Thailand's National Economic and Social Development Board (NESDB).
Recent economic history.
Thailand experienced the world's highest economic growth rate from 1985 to 1996 – averaging 12.4% annually. In 1997 increased pressure on the baht, a year in which the economy contracted by 1.9%, led to a crisis that uncovered financial sector weaknesses and forced the Chavalit Yongchaiyudh administration to float the currency. Prime Minister Chavalit Yongchaiyudh was forced to resign after his cabinet came under fire for its slow response to the economic crisis. The baht was pegged at 25 to the US dollar from 1978 to 1997. The baht reached its lowest point of 56 to the US dollar in January 1998 and the economy contracted by 10.8% that year, triggering the Asian financial crisis.
Thailand's economy started to recover in 1999, expanding 4.2–4.4% in 2000, thanks largely to strong exports. Growth (2.2%) was dampened by the softening of the global economy in 2001, but picked up in the subsequent years owing to strong growth in Asia, a relatively weak baht encouraging exports, and increased domestic spending as a result of several mega projects and incentives of Prime Minister Thaksin Shinawatra, known as Thaksinomics. Growth in 2002, 2003, and 2004 was 5–7% annually.
Growth in 2005, 2006, and 2007 hovered around 4–5%. Due both to the weakening of the US dollar and an increasingly strong Thai currency, by March 2008 the dollar was hovering around the 33 baht mark. While Thaksinomics has received criticism, official economic data reveals that between 2001 and 2011, Isan's GDP per capita more than doubled to US$1,475, while, over the same period, GDP in the Bangkok area increased from US$7,900 to nearly US$13,000.
With the instability surrounding major 2010 protests, the GDP growth of Thailand settled at around 4–5%, from highs of 5–7% under the previous civilian administration. Political uncertainty was identified as the primary cause of a decline in investor and consumer confidence. The IMF predicted that the Thai economy would rebound strongly from the low 0.1% GDP growth in 2011, to 5.5% in 2012 and then 7.5% in 2013, due to the monetary policy of the Bank of Thailand, as well as a package of fiscal stimulus measures introduced by the incumbent Yingluck Shinawatra government.
Following the Thai military coup of 22 May 2014, the AFP global news agency published an article that claimed that the nation was on the verge of recession. The article focused on the departure of nearly 180,000 Cambodians from Thailand due to fears of an immigration clampdown, but concluded with information on the Thai economy's contraction of 2.1% quarter-on-quarter, from January to the end of March 2014.
Exports and manufacturing.
The economy of Thailand is heavily export-dependent, with exports accounting for more than two-thirds of gross domestic product (GDP). Thailand exports over US$105 billion worth of goods and services annually. Major exports include rice, textiles and footwear, fishery products, rubber, jewellery, cars, computers, and electrical appliances.
Substantial industries include electric appliances, components, computer components, and vehicles. Thailand's recovery from the 1997–1998 Asian financial crisis depended mainly on exports, among various other factors. s of 2012[ [update]], the Thai automotive industry was the largest in Southeast Asia and the 9th largest in the world. The Thailand industry has an annual output of near 1.5 million vehicles, mostly commercial vehicles.
Most of the vehicles built in Thailand are developed and licensed by foreign producers, mainly Japanese and South Korean. The Thai car industry takes advantage of the ASEAN Free Trade Area (AFTA) to find a market for many of its products. Eight manufacturers, five Japanese, two US, and Tata of India, produce pick-up trucks in Thailand. Thailand is the second largest consumer of pick-up trucks in the world, after the US. In 2014, pick-ups accounted for 42% of all new vehicle sales in Thailand.
Tourism.
Tourism in Thailand makes up about 6% of the economy. Thailand was the most visited country in Southeast Asia in 2013, according to the World Tourism Organisation. Prostitution in Thailand and sex tourism also form a "de facto" part of the economy. Cultural milieu combined with poverty and the lure of money have caused prostitution and sex tourism in particular to flourish in Thailand. One estimate published in 2003 placed the trade at US$4.3 billion per year or about 3% of the Thai economy. According to research by Chulalongkorn University on the Thai illegal economy, prostitution in Thailand in the period between 1993 and 1995, made up around 2.7% of the GDP. It is believed that at least 10% of tourist dollars are spent on the sex trade.
Agriculture.
Forty-nine per cent of Thailand's labour force is employed in agriculture. This is down from 70% in 1980. Rice is the most important crop in the country and Thailand had long been the world's leading exporter of rice, until recently falling behind both India and Vietnam. Thailand has the highest percentage of arable land, 27.25%, of any nation in the Greater Mekong Subregion. About 55% of the arable land area is used for rice production.
Agriculture has been experiencing a transition from labour-intensive and transitional methods to a more industrialised and competitive sector. Between 1962 and 1983, the agricultural sector grew by 4.1% per year on average and continued to grow at 2.2% between 1983 and 2007. The relative contribution of agriculture to GDP has declined while exports of goods and services have increased.
Energy.
75% of Thailand's electrical generation is powered by natural gas in 2014. Coal-fired power plants produce an additional 20% of electricity, with the remainder coming from biomass, hydro, and biogas.
Thailand produces roughly one-third of the oil it consumes. It is the second largest importer of oil in SE Asia. Thailand is a large producer of natural gas, with reserves of at least 10 trillion cubic feet. After Indonesia, it is the largest coal producer in SE Asia, but must import additional coal to meet domestic demand.
Demographics.
Thailand had a population of 66,720,153 as of 2013. Thailand's population is largely rural, concentrated in the rice-growing areas of the central, northeastern, and northern regions. Thailand had an urban population of 45.7% as of 2010, concentrated mostly in and around the Bangkok Metropolitan Area.
Thailand's government-sponsored family planning program resulted in a dramatic decline in population growth from 3.1% in 1960 to around 0.4% today. In 1970, an average of 5.7 people lived in a Thai household. At the time of the 2010 census, the average Thai household size was 3.2 people.
Ethnic groups.
Ethnic Thais make up the majority of Thailand's population, 95.9% in 2010. This number includes Thai Chinese, a historically and economically important minority. The remaining 4.1% of the population are Burmese (2.0%), others 1.3%, and unspecified 0.9%.
Thailand is home to a large expatriate community of around 200,000 foreigners. Some 41,000 Britons alone live in Thailand. Increasing numbers of migrants from neighbouring Burma, Laos, and Cambodia, as well as from Nepal and India, have pushed the total number of non-national residents to around 3.5 million as of 2009, up from an estimated 2 million in 2008, and about 1.3 million in the year 2000.
Language.
The official language of Thailand is Thai, a Tai–Kadai language closely related to Lao, Shan in Burma, and numerous smaller languages spoken in an arc from Hainan and Yunnan south to the Chinese border. It is the principal language of education and government and spoken throughout the country. The standard is based on the dialect of the central Thai people, and it is written in the Thai alphabet, an abugida script that evolved from the Khmer script. Several other dialects exist, and coincide with the regional designations. Southern Thai is spoken in the southern provinces, and Northern Thai is spoken in the provinces that were formerly part of the independent kingdom of Lannathai.
Thailand is also host to several other minority languages, the largest of which is the Lao dialect of Isan spoken in the northeastern provinces. Although sometimes considered a Thai dialect, it is a Lao dialect, and the region in where it is traditionally spoken was historically part of the Lao kingdom of Lan Xang. In the far south, Yawi, a dialect of Malay, is the primary language of the Malay Muslims. Varieties of Chinese are also spoken by the large Chinese population, with Teochew being best represented.
Numerous tribal languages are also spoken, including those belonging to the Mon–Khmer family, such as Mon, Khmer, Viet, Mlabri and Orang Asli; Austronesian family, such as Cham and Moken; Sino-Tibetan family such as Lawa, Akhan, and Karen; and other Tai languages such as Nyaw, Phu Thai, and Saek. Hmong is a member of the Hmong–Mien languages, which is now regarded as a language family of its own.
English is a mandatory school subject, but the number of fluent speakers remains low, especially outside cities.
Religion.
Thailand's prevalent religion is Theravada Buddhism, which is an integral part of Thai identity and culture. Active participation in Buddhism is among the highest in the world. According to the 2000 census, 94.6% of the country's population self-identified as Buddhists of the Theravada tradition. Muslims constitute the second largest religious group in Thailand, comprising 4.6% of the population.
Islam is concentrated mostly in the country's southernmost provinces: Pattani, Yala, Satun, Narathiwat, and part of Songkhla Chumphon, which are predominantly Malay, most of whom are Sunni Muslims. Christians represent 0.7% of the population, with the remaining population consisting of Sikhs and Hindus, who live mostly in the country's cities. There is also a small but historically significant Jewish community in Thailand dating back to the 17th century.
Culture.
Thai culture has been shaped by many influences, including Indian, Lao, Burmese, Cambodian, and Chinese.
Its traditions incorporate a great deal of influence from India, China, Cambodia, and the rest of Southeast Asia. Thailand's national religion, Theravada Buddhism, is central to modern Thai identity. Thai Buddhism has evolved over time to include many regional beliefs originating from Hinduism, animism, as well as ancestor worship. The official calendar in Thailand is based on the Eastern version of the Buddhist Era (BE), which is 543 years ahead of the Gregorian (Western) calendar. Thus the year 2015 is 2558 BE in Thailand.
Several different ethnic groups, many of which are marginalised, populate Thailand. Some of these groups spill over into Burma, Laos, Cambodia, and Malaysia and have mediated change between their traditional local culture, national Thai, and global cultural influences. Overseas Chinese also form a significant part of Thai society, particularly in and around Bangkok. Their successful integration into Thai society has allowed for this group to hold positions of economic and political power. Thai Chinese businesses prosper as part of the larger bamboo network, a network of overseas Chinese businesses operating in the markets of Southeast Asia that share common family and cultural ties.
The traditional Thai greeting, the "wai", is generally offered first by the younger of the two people meeting, with their hands pressed together, fingertips pointing upwards as the head is bowed to touch face to fingertips, usually coinciding with the spoken words "sawatdi khrap" for male speakers, and "sawatdi kha" for females. The elder may then respond in the same way. Social status and position, such as in government, will also have an influence on who performs the "wai" first. For example, although one may be considerably older than a provincial governor, when meeting it is usually the visitor who pays respect first. When children leave to go to school, they are taught to "wai" their parents to indicate their respect. The wai is a sign of respect and reverence for another, similar to the namaste greeting of India and Nepal.
As with other Asian cultures, respect towards ancestors is an essential part of Thai spiritual practice. Thais have a strong sense of hospitality and generosity, but also a strong sense of social hierarchy. Seniority is paramount in Thai culture. Elders have by tradition ruled in family decisions or ceremonies. Older siblings have duties to younger ones.
Taboos in Thailand include touching someone's head or pointing with the feet, as the head is considered the most sacred and the foot the lowest part of the body.
Cuisine.
Thai cuisine blends five fundamental tastes: sweet, spicy, sour, bitter, and salty. Thai cuisine is heavily inspired by Chinese cuisine, especially Thai street food, soups and stir fry dishes. Some common ingredients used in Thai cuisine include garlic, chillies, lime juice, lemon grass, coriander, galangal, palm sugar, and fish sauce (nam pla). The staple food in Thailand is rice, particularly jasmine variety rice (also known as "hom Mali" rice) which is included at almost every meal. Thailand was long the world's largest exporter of rice, and Thais domestically consume over 100 kg of milled rice per person per year. Over 5,000 varieties of rice from Thailand are preserved in the rice gene bank of the International Rice Research Institute (IRRI), based in the Philippines. The king of Thailand is the official patron of IRRI.
Media.
Thai society has been influenced in recent years by its widely available multi-language press and media. There are some English and numerous Thai and Chinese newspapers in circulation. Most Thai popular magazines use English headlines as a chic glamour factor. Many large businesses in Bangkok operate in English as well as other languages.
Thailand is the largest newspaper market in Southeast Asia with an estimated circulation of over 13 million copies daily in 2003. Even upcountry, out of Bangkok, the media flourish. For example, according to Thailand's Public Relations Department Media Directory 2003–2004, the nineteen provinces of Isan, Thailand's northeastern region, hosted 116 newspapers along with radio, TV, and cable.
Units of measurement.
Thailand generally uses the metric system, but traditional units of measurement for land area are used, and imperial units of measurement are occasionally used for building materials, such as wood and plumbing fixtures. Years are numbered as B.E. (Buddhist Era) in educational settings, the civil service, government, and on contracts and newspaper datelines. In banking, and increasingly in industry and commerce, standard Western year (Christian or Common Era) counting is the standard practice.
Sports.
Muay Thai (Thai: มวยไทย, RTGS: Muai Thai,  ], lit. "Thai boxing") is a native form of kickboxing and Thailand's signature sport. It incorporates kicks, punches, knees and elbow strikes in a ring with gloves similar to those used in Western boxing and this has led to Thailand gaining medals at the Olympic Games in boxing.
Football has possibly overtaken muay Thai as the most widely followed sport in contemporary Thai society. Thailand national football team has played the AFC Asian Cup six times and reached the semifinals in 1972. The country has hosted the Asian Cup twice, in 1972 and in 2007. The 2007 edition was co-hosted together with Indonesia, Malaysia and Vietnam. It is not uncommon to see Thais cheering their favourite English Premier League teams on television and walking around in replica kit. Another widely enjoyed pastime, and once a competitive sport, is kite flying.
Takraw (Thai: ตะกร้อ) is a sport native to Thailand, in which the players hit a rattan ball and are only allowed to use their feet, knees, chest, and head to touch the ball. Sepak takraw is a form of this sport which is similar to volleyball. The players must volley a ball over a net and force it to hit the ground on the opponent's side. It is also a popular sport in other countries in Southeast Asia. A rather similar game but played only with the feet is Buka ball.
Snooker has enjoyed increasing popularity in Thailand in recent years, with interest in the game being stimulated by the success of Thai snooker player James Wattana in the 1990s. Other notable players produced by the country include Ratchayothin Yotharuck, Noppon Saengkham and Dechawat Poomjaeng.
Rugby is also a growing sport in Thailand with the Thailand national rugby union team rising to be ranked 61st in the world. Thailand became the first country in the world to host an international 80 kg welterweight rugby tournament in 2005. The national domestic Thailand Rugby Union (TRU) competition includes several universities and services teams such as Chulalongkorn University, Mahasarakham University, Kasetsart University, Prince of Songkla University, Thammasat University, Rangsit University, the Thai Police, the Thai Army, the Thai Navy and the Royal Thai Air Force. Local sports clubs which also compete in the TRU include the British Club of Bangkok, the Southerners Sports Club (Bangkok) and the Royal Bangkok Sports Club.
Thailand has been called the golf capital of Asia as it is a popular destination for golf. The country attracts a large number of golfers from Japan, Korea, Singapore, South Africa, and Western countries who come to play golf in Thailand every year. The growing popularity of golf, especially among the middle classes and expats, is evident as there are more than 200 world-class golf courses nationwide, and some of them are chosen to host PGA and LPGA tournaments, such as Amata Spring Country Club, Alpine Golf and Sports Club, Thai Country Club, and Black Mountain Golf Club.
Basketball is a growing sport in Thailand, especially on the professional sports club level. The Chang Thailand Slammers won the 2011 ASEAN Basketball League Championship. The Thailand national basketball team had its most successful year at the 1966 Asian Games where it won the silver medal.
Other sports in Thailand are slowly growing as the country develops its sporting infrastructure. The success in sports like weightlifting and taekwondo at the last two summer Olympic Games has demonstrated that boxing is no longer the only medal option for Thailand.
Sporting venues.
Thammasat Stadium is a multi-purpose stadium in Bangkok. It is currently used mostly for football matches. The stadium holds 25,000. It is on Thammasat University's Rangsit campus. It was built for the 1998 Asian Games by construction firm Christiani and Nielsen, the same company that constructed the Democracy Monument in Bangkok.
Rajamangala National Stadium is the biggest sporting arena in Thailand. It currently has a capacity of 65,000. It is in Bang Kapi, Bangkok. The stadium was built in 1998 for the 1998 Asian Games and is the home stadium of the Thailand national football team.
The well-known Lumpini Boxing Stadium will host its final Muay Thai boxing matches on 7 February 2014 after the venue first opened in December 1956. Managed by the Royal Thai Army, the stadium was officially selected for the purpose of muay Thai bouts following a competition that was staged on on 15 March 1956. From 11 February 2014, the stadium will relocate to Ram Intra Road, due to the new venue's capacity to accommodate audiences of up to 3,500. Foreigners typically pay between 1,000–2,000 baht to view a match, with prices depending on the location of the seating.

</doc>
<doc id="30139" url="http://en.wikipedia.org/wiki?curid=30139" title="History of Togo">
History of Togo

Little is known about the history of Togo before the late fifteenth century, when Portuguese explorers arrived, although there are signs of Ewe settlement for several centuries before their arrival.
Pre-colonial.
Various tribes moved into the country from all sides - the Ewe from Benin, and the Mina and the Guin from Ghana. These three groups settled along the coast. The Portuguese built forts in neighboring Ghana (at Elmina) and Benin (at Ouidah). Although the coast of Togo had no natural harbors, the Portuguese did trade at a small fort at Porto Seguro. For the next 200 years, the coastal region was a major trading center for Europeans in search of slaves, earning Togo and the surrounding region the name "The Slave Coast".
Colonial rule.
German Togoland.
The German Empire established the protectorate of Togoland (in what is now the nation of Togo and most of what is now the Volta Region of Ghana) in 1884 during the period generally known as the "Scramble for Africa". The colony was established in part of what was then the Slave Coast and German control was gradually extended inland. Because it became Germany's only self-supporting colony and because of its extensive rail and road infrastructure, Togoland was known as its model possession. At the outbreak of the First World War in 1914 the colony was drawn into the conflict. It was invaded and quickly overrun by British and French forces during the Togoland campaign and placed under military rule. In 1916 the territory was divided into separate British and French administrative zones, and this was formalized in 1922 with the creation of British Togoland and French Togoland.
League of Nations mandates.
On August 8, 1914, French and British forces invaded Togoland and the German forces there surrendered on 26 August. In 1916, Togoland was divided into French and British administrative zones. Following the war, Togoland formally became a League of Nations mandate divided for administrative purposes between France and the United Kingdom. After World War I, newly founded Czechoslovakia was also interested in this colony but this idea did not succeed.
Lome was initially allocated to the British zone but after negotiations transferred to France 1 October 1920.
After World War II, the mandate became a UN trust territory administered by the United Kingdom and France. During the mandate and trusteeship periods, western Togo was administered as part of the British Gold Coast. In December 1956, the residents of British Togoland voted to join the Gold Coast as part of the new independent nation of Ghana. 
In the General Council elections in 1946, there were two parties, the Committee of Togolese Unity (Comité de l'Unité Togolaise—CUT) and the Togolese Party for Progress (Parti Togolais du Progrès—PTP). The CUT was overwhelmingly successful, and Sylvanus Olympio, the CUT leader became Council leader. In the 1952 Territorial Assembly elections, however, the CUT was defeated, and it refused to participate in further French supervised elections because it claimed that the PTP was receiving French support. By statute in 1955, French Togoland became an autonomous republic within the French union, although it retained its UN trusteeship status. Following elections to the Territorial Assembly on 12 June 1955, which were boycotted by CUT, considerable power over internal affairs was granted, with an elected executive body headed by a prime minister responsible to the legislature. These changes were embodied in a constitution approved in a 1956 referendum. On 10 September 1956, Nicolas Grunitzky became prime minister of the Republic of Togo. However, due to irregularities in the plebiscite, a UN supervised general election was held on 27 April 1958, the first held in Togo with universal suffrage, which was won by the opposition pro-independence CUT and its leader Sylvanus Olympio, who became Prime Minister. On 13 October 1958 the French government announced that full independence would be granted. On 14 November 1958 the United Nations’ General Assembly took note of the French government’s declaration according to which Togo which was under French administration would gain independence in 1960, thus marking an end to the trusteeship period (Resolution 1253. XIII). On 5 December 1959 the United Nations’ General Assembly resolved that the UN Trusteeship Agreement with France for Cameroon would end when Togo became independent on 27 April 1960 (Resolution 1416. XIV).
On 27 April 1960, in a smooth transition, Togo severed its constitutional ties with France, shed its UN trusteeship status, and became fully independent under a provisional constitution with Olympio as president.
Independence and turmoil.
A new constitution in 1961 established an executive president, elected for 7 years by universal suffrage and a weak National Assembly. The president was empowered to appoint ministers and dissolve the assembly, holding a monopoly of executive power. In elections that year, from which Grunitzky's party was disqualified, Olympio's party won 90% of the vote and all 51 National Assembly seats, and he became Togo's first elected president. 
During this period, four principal political parties existed in Togo: the leftist Juvento (Togolese youth movement); the Union Démocratique des Populations Togolaises (IDPT); the Parti Togolais du Progrès (PTP), founded by Grunitzky but having limited support; and the Unité Togolaise (UT), the party of President Olympio. Rivalries between elements of these parties had begun as early as the 1940s, and they came to a head with Olympio dissolving the opposition parties in January 1962 because of alleged plots against the majority party government. The reign of Olympio was marked by the terror of his militia, the Ablode Sodjas. Many opposition members, including Grunitzky and Meatchi, were jailed or fled to avoid arrest. 
On January 13, 1963, President Olympio was overthrown and killed in a coup d'état led by army non-commissioned officers dissatisfied with conditions following their discharge from the French army. Grunitzky returned from exile 2 days later to head a provisional government with the title of prime minister. On May 5, 1963, the Togolese adopted a new constitution which reinstated a multi-party system, chose deputies from all political parties for the National Assembly, and elected Grunitzky as president and Antoine Meatchi as vice president. Nine days later, President Grunitzky formed a government in which all parties were represented. 
During the next several years, the Grunitzky government's power became insecure. On 21 November 1966, an attempt to overthrow Grunitzky, inspired principally by civilian political opponents in the UT party, was unsuccessful. Grunitzky then tried to lessen his reliance on the army, but on January 13, 1967, a coup led by Lt. Col. Étienne Eyadéma (later Gen. Gnassingbé Eyadéma) and Kléber Dadjo ousted President Grunitzky without bloodshed. Following the coup, political parties were banned, and all constitutional processes were suspended. Dadjo became the chairman of the "committee of national reconciliation", which ruled the country until April 14, when Eyadéma assumed the presidency. In late 1969, a single national political party, the Rally of the Togolese People (RPT), was created, and President Eyadéma was elected party president on November 29, 1969. In 1972, a national referendum, in which Eyadéma ran unopposed, confirmed his role as the country's president.
Eyadéma's rule.
The third republic.
In late 1979, Eyadéma declared a third republic and a transition to greater civilian rule with a mixed civilian and military cabinet. He garnered 99.97% of the vote in uncontested presidential elections held in late 1979 and early 1980. A new constitution also provided for a national assembly to serve primarily as a consultative body. Eyadéma was reelected to a third consecutive 7-year term in December 1986 with 99.5% of the vote in an uncontested election. On 23 September 1986, a group of some 70 armed Togolese dissidents crossed into Lomé from Ghana in an unsuccessful attempt to overthrow the Eyadéma government.
Opposition.
In 1989 and 1990, Togo, like many other countries, was affected by the winds of democratic change sweeping Eastern Europe and the Soviet Union. On 5 October 1990, the trial of students who handed out antigovernment tracts sparked riots in Lomé. Antigovernment demonstrations and violent clashes with the security forces marked the months that followed. In April 1991, the government began negotiations with newly formed opposition groups and agreed to a general amnesty that permitted exiled political opponents to return to Togo. After a general strike and further demonstrations, the government and opposition signed an agreement to hold a "national forum" on 12 June 1991. 
The national forum, dominated by opponents of President Eyadéma, opened in July 1991 and immediately declared itself to be a sovereign "National Conference." Although subjected to severe harassment from the government, the conference drafted an interim constitution calling for a 1-year transitional regime tasked with organizing free elections for a new government. The conference selected Joseph Kokou Koffigoh, a lawyer and human rights group head, as transitional prime minister but kept President Eyadéma as chief of state for the transition, although with limited powers. 
A test of wills between the president and his opponents followed over the next 3 years during which President Eyadéma gradually gained the upper hand. Frequent political paralysis and intermittent violence marked this period. Following a vote by the transitional legislature (High Council of the Republic) to dissolve the President's political party—the RPT—in November 1991, the army attacked the prime minister's office on 3 December and captured the prime minister. Koffigoh then formed a second transition government in January 1992 with substantial participation by ministers from the President's party. Opposition leader Gilchrist Olympio, son of the slain president Sylvanus Olympio, was ambushed and seriously wounded apparently by soldiers on 5 May 1992. 
In July and August 1992, a commission composed of presidential and opposition representatives negotiated a new political agreement. On 27 September, the public overwhelmingly approved the text of a new, democratic constitution, formally initiating Togo's fourth republic.
Powerless legislature and political violence.
The democratic process was set back in October 1991, when elements of the army held the interim legislature hostage for 24 hours. This effectively put an end to the interim legislature. In retaliation, on 16 November, opposition political parties and labor unions declared a general strike intended to force President Eyadéma to agree to satisfactory conditions for elections. The general strike largely shut down Lomé for months and resulted in severe damage to the economy. 
In January 1993, President Eyadéma declared the transition at an end and reappointed Koffigoh as prime minister under Eyadéma's authority. This set off public demonstrations, and, on 25 January, members of the security forces fired on peaceful demonstrators, killing at least 19. In the ensuing days, several security force members were waylaid and injured or killed by civilian oppositionists. On 30 January 1993, elements of the military went on an 8-hour rampage throughout Lomé, firing indiscriminately and killing at least 12 people. This incident provoked more than 300,000 Togolese to flee Lomé for Benin, Ghana, or the interior of Togo. Although most had returned by early 1996, some still remain abroad. 
On 25 March 1993, armed Togolese dissident commandos based in Ghana attacked Lomé's main military camp and tried unsuccessfully to kill President Eyadéma. They inflicted significant casualties, however, which set off lethal reprisals by the military against soldiers thought to be associated with the attackers.
Negotiating with the opposition.
Under substantial domestic and foreign pressure and the burden of the general strike, the presidential faction entered negotiations with the opposition in early 1993. Four rounds of talks led to the 11 July Ouagadougou agreement setting forth conditions for upcoming presidential and legislative elections and ending the general strike as of 3 August 1993. The presidential elections were set for 25 August, but hasty and inadequate technical preparations, concerns about fraud, and the lack of effective campaign organization by the opposition led the chief opposition candidates—former minister and Organization of African Unity Secretary General Edem Kodjo and lawyer Yawovi Agboyibo—to drop out of the race before election day and to call for a boycott. President Eyadéma won the elections by a 96.42% vote against token opposition. About 36% of the voters went to the polls; the others boycotted. 
Ghana-based armed dissidents launched a new commando attack on military sites in Lomé in January 1994. President Eyadéma was unhurt, and the attack and subsequent reaction by the Togolese armed forces resulted in hundreds of deaths, mostly civilian. The government went ahead with legislative elections on 6 February and 20 February 1994. In generally free and fair polls as witnessed by international observers, the allied opposition parties UTD and CAR together won a narrow majority in the National Assembly.
Edem Kodjo named as Prime Minister.
On April 22, President Eyadéma named Edem Kodjo, the head of the smaller opposition party, the UTD, as prime minister instead of Yawovi Agboyibo, whose CAR party had far more seats. Kodjo's acceptance of the post of prime minister provoked the CAR to break the opposition alliance and refuse to join the Kodjo government. 
Kodjo was then forced to form a governing coalition with the RPT. Kodjo's government emphasized economic recovery, building democratic institutions and the rule of law and the return of Togolese refugees abroad. In early 1995, the government made slow progress toward its goals, aided by the CAR's August 1995 decision to end a 9-month boycott of the National Assembly. However, Kodjo was forced to reshuffle his government in late 1995, strengthening the representation by Eyadéma's RPT party, and he resigned in August 1996. Since then, Eyadéma has reemerged with a sure grip on power, controlling most aspects of government. 
In the June 1998 presidential election, the government prevented citizens from effectively exercising the right to vote. The Interior Ministry declared Eyadéma the winner with 52% of the vote in the 1998 election; however, serious irregularities in the government's conduct of the election strongly favored the incumbent and appear to have affected the outcome materially. Although the government did not obstruct the functioning of political opponents openly, the President used the strength of the military and his government allies to intimidate and harass citizens and opposition groups. The government and the state remained highly centralized: President Eyadéma's national government appointed the officials and controlled the budgets of all subnational government entities, including prefectures and municipalities, and influenced the selection of traditional chiefs.
National Assembly elections.
The second multi-party legislative elections of Eyadéma's 33-year rule were held on 21 March 1999. However, the opposition boycotted the election, in which the ruling party won 79 of the 81 seats in the National Assembly. Those two seats went to candidates from little-known independent parties. Procedural problems and significant fraud, particularly misrepresentation of voter turnout marred the legislative elections. 
After the legislative election, the government announced that it would continue to pursue dialog with the opposition. In June 1999, the RPT and opposition parties met in Paris, in the presence of facilitators representing France, Germany, the European Union, and La Francophonie (an international organization of French-speaking countries), to agree on security measures for formal negotiations in Lomé. In July 1999, the government and the opposition began discussions, and on 29 July 1999, all sides signed an accord called the "Lomé Framework Agreement", which included a pledge by President Eyadéma that he would respect the constitution and not seek another term as president after his current one expires in 2003. The accord also called for the negotiation of a legal status for opposition leaders, as well as for former heads of state (such as their immunity from prosecution for acts in office). In addition, the accord addressed the rights and duties of political parties and the media, the safe return of refugees, and the security of all citizens. The accord also contained a provision for compensating victims of political violence. The President also agreed to dissolve the National Assembly in March and hold new legislative elections, which would be supervised by an independent national election commission (CENI) and which would use the single-ballot method to protect against some of the abuses of past elections. However, the March 2000 date passed without presidential action, and new legislative elections were ultimately rescheduled for October 2001. Because of funding problems and disagreements between the government and opposition, the elections were again delayed, this time until March 2002. 
In May 2002 the government scrapped CENI, blaming the opposition for its inability to function. In its stead, the government appointed seven magistrates to oversee preparations for legislative elections. Not surprisingly, the opposition announced it would boycott them. Held in October, as a result of the opposition’s boycott the government party won more than two-thirds of the seats in the National Assembly. In December 2002, Eyadéma's government used this rubber-stamp parliament to amend Togo’s constitution, allowing President Eyadéma to run for an “unlimited” number of terms. A further amendment stated that candidates must reside in the country for at least 12 months before an election, a provision that barred the participation in the upcoming presidential election of popular Union des Forces du Progrès (UFC) candidate, Gilchrist Olympio, who had been in exile since 1992. The presidential election was held 1 June. President Eyadéma was re-elected with 57% of the votes, amid allegations of widespread vote rigging.
Death of Eyadéma and Gnassingbé's rise.
President Eyadéma died on 5 February 2005 while on board an airplane en route to France for treatment for a heart attack. Papa Gnassingbé is said to have killed more than fifteen thousand people during his dictatorship. His son Faure Gnassingbé, the country's former minister of public works, mines, and telecommunications, was named President by Togo's military following the announcement of his father's death. Under international pressure from the African Union and the United Nations however, who both denounced the transfer of power from father to son as a coup, Gnassingbé was forced to step down on 25 February 2005, shortly after accepting the nomination to run for elections in April. Deputy Speaker Bonfoh Abbass was appointed interim president until the inauguration of the 24 April election winner. As to official results, the winner of the election was Gnassingbé who garnered 60% of the vote. Opposition leader Emmanuel Bob-Akitani however disputed the election and declared himself to be the winner with 70% of the vote. After the announcement of the results, tensions flared up and to date, 100 people have been killed. On 3 May 2005, Gnassingbé was sworn in and vowed to concentrate on "the promotion of development, the common good, peace and national unity".
In August 2006 President Gnassingbe and members of the opposition signed the Global Political Agreement (GPA), bringing an end to the political crisis trigged by Gnassingbe Eyadema's death in February 2005 and the flawed and violent electoral process that followed. The GPA provided for a transitional unity government whose primary purpose would be to prepare for benchmark legislative elections, originally scheduled for June 24, 2007. CAR opposition party leader and human rights lawyer Yawovi Agboyibo was appointed Prime Minister of the transitional government in September 2006. Leopold Gnininvi, president of the CDPA party, was appointed minister of state for mines and energy. The third opposition party, UFC, headed by Gilchrist Olympio, declined to join the government, but agreed to participate in the national electoral commission and the National Dialogue follow-up committee, chaired by Burkina Faso President Blaise Compaore.
Parliamentary elections took place on October 14, 2007. Mr Olympio, who returned from exile to campaign, took part for the first time in 17 years. The ruling party, Rally of the Togolese People(RPT), won a majority of the parliamentary seats in the election which international observers declared the poll "largely" free and fair. Despite these assurances, the secretary-general of the opposition party Union of Forces for Change(UFC) initially state that his party would not accept the election results.
Mr Olympio stated that the election results did not properly represent the voters' will, pointing out that the UFC received nearly as many votes as the RPT, but that due to the way the electoral system was designed the UFC won far fewer seats.

</doc>
<doc id="30141" url="http://en.wikipedia.org/wiki?curid=30141" title="Demographics of Togo">
Demographics of Togo

The demographics of Togo include ethnicity, population density, age, education level, health, economic status and religious affiliation.
Language and ethnicity.
Togo's population of 5.86 million people (2008 est.) is composed of about 21 ethnic groups, the two biggest being the Ewe in the South (about 21% of the population) and the Kabye in the North (12% of the population). Dagomba is the second most common language in the north, where other Gur languages such as Mossi and Gourma are also found.
The ethnic groups of the coastal region, particularly Ewe and Gen speakers (the two major African languages in the south), constitute the bulk of the civil servants, professionals, and merchants, due in part to the former colonial administrations which provided greater infrastructure development in the south. Most of the southern peoples use these two closely related languages, which are spoken in commercial sectors throughout Togo. 
The Kabye live on marginal land and traditionally have emigrated south from their home area in the Kara region to seek employment. Their historical means of social advancement has been through the military and law enforcement forces, and they continue to dominate these services.
Other groups include the Akposso on the Central Plateau, the Bassar in the Centre-West, the Cotocoli, the Tchamba and the Komkombas around Sokodé, the Lambas in the Kandé region, the Hausa, the Tamberma, the Losso and the Ouachi. 
White African settlers descended from the original French and German colonials make up less than 1% of the total population along with Togo's minute Lebanese community. The remaining 99% are indigenous: most people in this category hail from one of thirty-seven different tribes.
Population.
Population distribution is very uneven due to soil and terrain variations. The population is generally concentrated in the south and along the major north-south highway connecting the coast to the Sahel. Age distribution is also uneven; nearly one-half of Togolese are less than fifteen years old. 
French, the official language, is used in administration and documentation. The public primary schools combine French with Ewe or Kabye as languages of instruction, depending on the region. English is spoken in neighboring Ghana and is taught in Togolese secondary schools. As a result, many Togolese, especially in the south and along the Ghana border, speak some English.
According to the 2010 revison of the World Population Prospects the total population was 6 028 000 in 2010, compared to only 1 395 000 in 1950. The proportion of children below the age of 15 in 2010 was 39.6%, 56.9% was between 15 and 65 years of age, while 3.4% was 65 years or older
Vital statistics.
Registration of vital events is in Togo not complete. The Population Departement of the United Nations prepared the following estimates.
Fertility and Births.
Total Fertility Rate (TFR) and Crude Birth Rate (CBR): 
CIA World Factbook demographic statistics.
The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.
Sex ratio.
<br>"at birth:"
1.03 male(s)/female
<br>"under 15 years:"
1.01 male(s)/female
<br>"15-64 years:"
0.95 male(s)/female
<br>"65 years and over:"
0.78 male(s)/female
<br>"total population:"
0.97 male(s)/female (2000 est.)
Life expectancy at birth.
<br>"total population:"
54.69 years
<br>"male:"
52.75 years
<br>"female:"
56.7 years (2000 est.)
HIV Prevalence.
Adult infection rate 4.2% (2003)
People living with HIV/Aids 110,000 (2003)
Death rate 10,000 (2003)
Religions.
According to the CIA Factbook, approximately 29% of the population is Christian, 20% are Muslim, and 51% hold traditional beliefs.
Literacy.
<br>"definition:"
age 15 and over can read and write
<br>"total population:"
51.7%
<br>"male:"
67%
<br>"female:"
37% (1995 est.)

</doc>
<doc id="30147" url="http://en.wikipedia.org/wiki?curid=30147" title="Foreign relations of Togo">
Foreign relations of Togo

Although Togo's foreign policy is nonaligned, it has strong historical and cultural ties with western Europe, especially France and Germany. Togo recognizes the People's Republic of China, North Korea, and Cuba. It re-established relations with Israel in 1987.
Togo pursues an active foreign policy and participates in many international organizations. It is particularly active in West African regional affairs and in the African Union. Relations between Togo and neighboring states are generally good.
Bilateral relations.
China.
Togo and China established diplomatic relations on September 19 1972.
France.
Togo gained its independence from France in 1960.
Ghana.
After 1918, following the defeat of Germany, the League of Nations divided the German colony of Togoland from north to south, a decision that divided the Ewe people among the Gold Coast, British Togoland, and French Togoland. After 1945, the United Nations (UN) took over the Togoland mandates. During the 1950s, when the independence of Ghana was in sight, demands grew for a separate Ewe state, an idea that Kwame Nkrumah, leader of the Gold Coast independence movement, opposed. Following a UN plebiscite in May 1956, in which a majority of the Ewe voted for union with Ghana, British Togoland became part of the Gold Coast. After Togolese independence in 1960, relations between Togo and Ghana deteriorated, aggravated by political differences and incidents such as smuggling across their common border. At times, relations have verged on open aggression.
The result of the transfer of Togoland to Ghana has meant that many Togolese keep one foot on either side of the border, living in Ghana by night and working in the markets of the capital, Lomé, by day.
Kosovo.
Togo recognized the Republic of Kosovo on July 2, 2014. Togolese foreign minister Robert Dussey visited Pristina on July 21, 2014, and signed three agreements with his Kosovar counterpart, Enver Hoxhaj, which established diplomatic relations and initiated economic, trade, and educational cooperation. 
Palestine.
Togo recognized the State of Palestine on November 29, 1988. During the votes to admit Palestine to UNESCO as a member state in October 2011 and as Non-Member Observer State, in November 2012 Togo abstained from the voting.
United States.
The United States and Togo have had generally good relations since its independence, although the United States has never been one of Togo's major trading partners. The largest share of U.S. exports to Togo generally has been used clothing and scrap textiles. Other important U.S. exports include rice, wheat, shoes, and tobacco products, and U.S. personal computers and other office electronics are becoming more widely used.
The U.S. maintains an embassy in Lomé.

</doc>
<doc id="30159" url="http://en.wikipedia.org/wiki?curid=30159" title="History of Tonga">
History of Tonga

Seafarers associated with the Lapita diaspora first settled the islands making up the Kingdom of Tonga in the century after 900 BC.
The area served (along with Fiji and Samoa) as a gateway into the rest of the Pacific region known as Polynesia. Ancient Tongan mythologies recorded by early European explorers report the islands of 'Ata and Tongatapu as the first islands being hauled to the surface from the deep ocean by Maui.
Pre-contact.
The dates of the initial settlement of Tonga are still subject to debate, nonetheless one of the oldest occupied sites is found in the village of Pea on Tongatapu. Based on radiocarbon dating of a shell found at the site dates the occupation at 3180 ± 100 BP (Before Present). Some of the oldest sites pertaining to the first occupants of the Tongan Islands are found on Tongatapu which is also where the first Lapita ceramics were found by WC McKern in 1921. Nonetheless, reaching the Tongan islands (without Western navigational tools and techniques) was a remarkable feat accomplished by the Lapita peoples. Not much is known about Tonga before European contact because of the lack of a writing system during prehistoric times other than the oral history told to the early European explorers. The first time the Tongan people encountered Europeans was in April 1616 when Jacob Le Maire and Willem Schouten made a short visit to the islands to trade.
Early culture.
Centuries before Westerners arrived, Tongans created large monumental stoneworks, most notably, the Haʻamonga ʻa Maui and the Langi (terraced tombs). The Haʻamonga is 5 meters high and made of three coral-lime stones that weigh more than 40 tons each. The Langi are low, very flat, two or three tier pyramids that mark the graves of former kings.
Tongan Maritime empire.
By the 12th century, Tongans, and the Tongan kings, the Tu'i Tonga, were known across the Pacific, from Niuē, Samoa to Tikopia they ruled these nations for over 400 years, sparking some historians to refer to a "Tongan Empire," although it was more so a network of interacting navigators, chiefs, and adventurers. It is unclear whether chiefs of the other islands actually came to Tonga regularly to acknowledge their sovereign. Distinctive pottery and Tapa cloth designs also show that the Tongans have travelled from the far reaches of Micronesia, all the way to Fiji and even Hawaii.
European arrival and Christianisation.
In the 15th century and again in the 17th, civil war erupted. It was in this context that the first Europeans arrived, beginning with Dutch explorers Willem Schouten and Jacob Le Maire. Between April 21 to 23, 1616 they moored at the Northern Tongan islands "Cocos Island" (Tafahi) and "Traitors Island" (Niuatoputapu), respectively. The kings of both of these islands boarded the ships and Le Maire drew up a list of Niuatoputapu words, a language now extinct. On April 24, 1616 they tried to moor at the "Island of Good Hope" (Niuafo'ou), but a less welcoming reception there made them decide to sail on.
On January 21, 1643, the Dutch explorer Abel Tasman was the first European to visit the main island (Tongatapu) and Haʻapai after rounding Australia and New Zealand. The most significant impact had the visits of Captain Cook visits in 1773, 1774, and 1777, followed by the first London missionaries in 1797, and the Wesleyan Methodist Walter Lawry in 1822. Around that time most Tongans converted en masse to the Wesleyan (Methodist) or Catholic faiths. Later other denominations followed like Pentecostals, Mormons, Seventh-day Adventists and most recently the Bahá'í faith.
Unification.
In 1799 the 14th q Tuʻi Kanokupolu, Tukuʻaho was murdered, which sent Tonga into a civil war for fifty years. Finally the islands were united into a Polynesian kingdom in 1845 by the ambitious young warrior, strategist, and orator Tāufaʻāhau. He held the chiefly title of Tu'i Kanokupolu, but was baptised with the name King George Tupou I. In 1875, with the help of missionary Shirley Baker, he declared Tonga a constitutional monarchy, at which time he emancipated the 'serfs', enshrined a code of law, land tenure, and freedom of the press, and limited the power of the chiefs. Tonga became a British protected state under a Treaty of Friendship on May 18, 1900, when European settlers and rival Tongan chiefs tried to oust the second king. The Treaty of Friendship and protected state status ended in 1970 under arrangements established prior to her death by the third monarch, Queen Sālote. Tonga joined the Commonwealth of Nations in 1970, and the United Nations in 1999. While exposed to colonial forces, Tonga has never lost indigenous governance, a fact that makes Tonga unique in the Pacific and gives Tongans much pride, as well as confidence in the monarchical system. The British High Commission in Tonga closed in March 2006.
Tonga's current king, Tupou VI, traces his line directly back through six generations of monarchs. The previous king, George Tupou V born in 1948, continued to have ultimate control of the government until July 2008. At that point, concerns over financial irregularities and calls for democracy led to his relinquishing most of his day-to-day powers over the government.

</doc>
<doc id="30162" url="http://en.wikipedia.org/wiki?curid=30162" title="Politics of Tonga">
Politics of Tonga

Politics of Tonga takes place in a framework of a constitutional monarchy, whereby the King is the Head of State and the Commander-in-Chief of the Armed Forces. Tonga's Prime Minister is currently appointed by the King from among the members of Parliament after having won the support of a majority of its members. Executive power is vested in the Cabinet of Ministers. Legislative power is vested in the King in Parliament, and judicial power is vested in the supreme court.
Political conditions.
For most of the 20th century Tonga was quiet, inward-looking, and somewhat isolated from developments elsewhere in the world. Tonga's complex social structure is essentially broken into three tiers: the king, the nobles, and the commoners. Between the nobles and commoners are Matapule, sometimes called "talking chiefs," who are associated with the king or a noble and who may or may not hold estates. Obligations and responsibilities are reciprocal, and although the nobility are able to extract favors from people living on their estates, they likewise must extend favors to their people. Status and rank play a powerful role in personal relationships, even within families.
Tongans are beginning to confront the problem of how to preserve their cultural identity and traditions in the wake of the increasing impact of Western technology and culture. Migration and the gradual monetization of the economy have led to the breakdown of the traditional extended family. Some of the poor, once supported by the extended family, are now being left without visible means of support.
Educational opportunities for young commoners have advanced, and their increasing political awareness has stimulated some dissent against the nobility system. In addition, the rapidly increasing population is already too great to provide the constitutionally mandated 8.25 acre (33,000 m²) api for each male at age 16. In mid-1982, population density was 134 persons per square kilometer. Because of these factors, there is considerable pressure to move to the Kingdom's only urban center.
In the March 2002 election, seven of nine popularly elected representatives were chosen under the pro-democratic banner with the remaining two representing "traditionalist" values. The nine nobles and all the cabinet ministers that sit in the Legislative Assembly generally support the government. Tonga does not rate as an "electoral democracy" under the criteria of Freedom House's Freedom in the World 2006 report. This is likely because while elections exist, they can only elect nine of 30 Legislative Assembly seats, the remainder being selected either by the nobility or the government; as such the people have a voice in but no control over the government.
In 2003, the Taimi 'o Tonga (Tongan Times), a newspaper published in New Zealand in the Tongan language that had been critical of the government was prohibited from distribution in Tonga due to government objections to its political content. After the newspaper obtained two court orders, it was again distributed freely. A Media Operators Bill and constitutional amendment, intended to restrict media freedom in Tonga, was hotly debated in 2003. The legislation allowed the government to exert control over coverage of "cultural" and "moral" issues, ban publications it deemed offensive, and ban foreign ownership of the media. In October 2003, thousands of Tongans marched peacefully through the streets of the capital city Nukuʻalofa in an unprecedented demonstration against the government's plans to limit media freedom. Despite the protests, the Media Operators Bill and constitutional amendment passed the Legislature and as of December 2003 needed only the King's signature to become law.
By February 2004, the amendment was passed and licensure of news media was required. Those papers denied licenses under the new act included the Taimi 'o Tonga (Tongan Times), the Kele'a and the Matangi Tonga, while those permitted licenses were uniformly church based or pro-government. Further opposition to government action included calls by the Tu'i Pelehake (a prince, nephew of the King and elected member of parliament) for Australia and other nations to pressure the Tongan government to democratize the electoral system, and a legal writ calling for a judicial investigation of the bill. The latter was supported by some 160 people, including 7 of the 9 elected "People's Representatives".
In 2005 the government spent several weeks negotiating with striking civil service workers before reaching a settlement. A constitutional commission met in 2005-2006 to study proposals to update the constitution. A copy of the commission's report was presented to King Taufa'ahau Tupou IV, shortly before his death in September 2006 and is currently under study by the present king, George Tupou V, and members of parliament.
The Tongan Speaker of the House was found to be guilty of bribery.
Prime Minister Prince Lavaka Ata 'Ulukalala resigned suddenly on 11 February 2006, and also gave up his other cabinet portfolios. He was replaced by the elected Minister of Labour, Dr. Feleti Sevele.
The public expected democratic changes from the new monarch. On November 16, 2006, rioting broke out in the capital city of Nukuʻalofa when it seemed that the parliament would adjourn for the year without having made any advances in increasing democracy in government. Government buildings, offices, and shops were looted and burned. Eight people died in the riots. The government agreed that elections would be held in 2008 in which a majority of the parliament would be elected by popular vote. A state of emergency was declared on November 17, with emergency laws giving security forces the right to stop and search people without a warrant.
On 29 May 2008, in the speech from the throne at the opening of Parliament, Princess Regent, Salote Mafile'o Pilolevu Tuita announced that the government would introduce a political reform bill by June 2008, and that the current term of Parliament would be the last one under the current constitution
Relinquishing the monarch's powers.
In July 2008, three days before his coronation, King George Tupou V announced that he would relinquish most of his power and be guided by his Prime Minister's recommendations on most matters.
In November 2009, a constitutional review panel recommended a ceremonial monarchy stripped of real political power and to invest political power in a completely elected Legislative Assembly which, up to this point was largely hereditary due to the fact that most of the seats where designated for the nobles.
Executive.
Its executive includes the prime minister and the cabinet, which becomes the Privy Council when presided over by the monarch. In intervals between legislative sessions, the Privy Council makes ordinances, which become law if confirmed by the legislature. The monarch is hereditary, the prime minister and deputy prime minister are appointed for life by the monarch, the Cabinet is appointed by the monarch.
Legislature.
The Legislative Assembly is composed of the representatives of the Nobles, the representatives of the people and the members of Cabinet. This composition is established by Article 59 of the Constitution as amended by the " Constitution of Tonga amendment Act 2010 " Article 51 of the same Act allows the PM to nominate and the King to appoint up to 4 extra cabinet members from outside the Assembly.
The current composition is:
Political parties and elections.
The electoral system was changed in April 2010, with 17 of 26 representatives now directly elected.
Byelections.
Below is a list of recent by-elections:
Courts.
Tonga's court system consists of the Court of Appeal (Privy Council), the Supreme Court, the Magistrates' Court, and the Land Court. Judges are appointed by the monarch.
The judiciary is headed by a Chief Justice. The current Chief Justice is Michael Dishington Scott.
Administrative divisions.
Tonga is divided in three island groups; Ha'apai, Tongatapu, Vava'u. The only form of local government is through town and district officials who have been popularly elected since 1965. The town official represents the central government in the villages, the district official has authority over a group of villages.

</doc>
<doc id="30171" url="http://en.wikipedia.org/wiki?curid=30171" title="Demographics of Trinidad and Tobago">
Demographics of Trinidad and Tobago

This article is about the demographic features of the population of Trinidad and Tobago, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
Population.
The total population of Trinidad and Tobago was 1,328,019 according to the 2011 census, an increase of 5.2% since the 2000 census.
According to the 2012 revison of the World Population Prospects the total population was estimeated at 1,328,000 in 2010, compared to only 646,000 in 1950. The proportion of children below the age of 15 in 2010 was 20.7%, 71% was between 15 and 65 years of age, while 8.3% was 65 years or older.
Due to decreasing fertility, the proportion of children below the age of 15 is decreasing, while the proportion of elderly is increasing. The median age has increased from 21.6 in 1980, 24.1 in 1990, 28.1 in 2000 to 32.6 in 2011.
The estimated mid-year population of 2014 is 1,344,000 (medium fertility scenario of The 2012 Revison of the World Population Prospects).
Emigration.
Emigration from Trinidad and Tobago, as with other Caribbean nations, has historically been high; most emigrants go to the United States, Canada, and Britain. Emigration has continued, albeit at a lower rate, even as the birth-rate sharply dropped to levels typical of industrialised countries. Largely because of this phenomenon, as of 2011, Trinidad and Tobago has been experiencing a low population growth rate (0.48%).
Ethnic groups.
Indo-Trinidadian.
Indo-Trinidadians make up the country's largest ethnic group (approximately 37.6%). They are primarily descendants from indentured workers from India, brought to replace freed African slaves who refused to continue working on the sugar plantations. The Indian community is divided roughly half-and-half between those who maintained their original religions and those who have converted to Christianity or have no religious affiliation. Through cultural preservation groups, Trinidadians of Indian descent maintain many of their customs and rites.
Afro-Trinidadian and Tobagonians.
Afro-Trinidadian and Tobagonian make up the country's second largest ethnic group (approximately 36.3%). Although African slaves were first imported in 1517, they constituted only 11 percent of the population (310) in 1783.
The majority of the African slaves were brought in the last few years of Trinidad's Spanish Colonial era, and the beginning of the British colonial period. The Cedula of Population transformed a small colony of 1000 in 1773 to 18,627 by 1797. In the census of 1777 there were only 2,763 people recorded as living on the island, including some 2,000 Arawaks. During this time there were many African slave owners. In 1807, the UK Parliament passed the Slave Trade Act 1807 that abolished the trading of slaves, and the Slavery Abolition Act 1833 abolished the practice of slavery.
Euro-Trinidadians.
The Euro-Trinidadian (or white Trinidadian) population is primarily descended from early settlers and immigrants. The recent census counted 8,669 people of European descent. These numbers do not include people who have at least some European ancestry or self-identify as African or Indian.
The French arrived mostly during the Spanish period to take advantage of free agricultural lands. Some Portuguese were brought to replace freed African slaves when they refused to accept low wages. The majority however arrived as religious refugees in mid nineteenth century and more came at the turn of the century for economic reasons. The Europeans who remained in Trinidad live in areas in and around Port of Spain. In Tobago, most Europeans are retirees from Germany and Scandinavia who have recently arrived there.
Mixed ethnicity.
Given the large number of ethnic identities in Trinidad and Tobago, many citizens have a mixed ethnic heritage due to influences from French, West African, Creole, Chinese, Indian, Irish, German, Swiss, Portuguese, British, Italian, Spanish, Dutch, Norwegian, Polish, Arab, Lebanese, Russian and African American ancestors. Additionally, there are also nationals of Hispanic ancestry, mostly of Amerindian descent, mainly from Venezuela along with a small number from Puerto Rico and the Dominican Republic. Common ethnic mixtures include people of European and African descent, mulattos, and Indian and African descent (often colloquially known as dougla). This mixed population is estimated at around 22.8%; however, it is much higher when considering the various degrees of African, Indian, European, and indigenous Amerindian ancestry of the total population. A person might self identify as Black based on physical appearance, for instance, but he or she might be genetically more similar to a person of Indian descent (dougla).
Sino-Trinidadians and Arab-Trinidadians.
There are groups of Chinese who, like the Portuguese and Indians, are descended from indentured labourers. They account for about 4,003 people and live mostly in Port-of-Spain and San Fernando. 
"In Trinidad there were, about twenty years ago [i.e. about 1886], 4,000 or 5,000 Chinese, but they have decreased to probably about 2,000 or 3,000, [2,200 in 1900]. They used to work in sugar plantations, but are now principally shopkeepers,as well as general merchants, miners and railway builders,etc.
There are also about 1,062 Arabs, originating from Syria and Lebanon who live mostly in Port-of-Spain. The Syrian and Lebanese communities of Trinidad are predominantly Christian, migrating from the Middle East in the 19th century while fleeing religious persecution received from the Ottoman Empire later landing in the Caribbean and Latin America. Other Lebanese and Syrians came in the early to middle 20th century to escape the war and turmoil in the region. Finally there are the mixed raced Caribs who are descended from the native, precolonial people of the islands. They are organized around the Santa Rosa Carib Community and live mostly in and around Arima.
Religion.
In 2011, according to census, Roman Catholicism was again the largest religious denomination with 285,671 followers (21.6% of the total population), having declined from a membership of 289,711 in 2000 (26% of the population). Other religious denominations that experienced decreases in their membership in 2011 were Hinduism (from 22.5% in 2000 to 18.2% in 2011), Anglican (from 7.8% to 5.7%), Presbyterian/Congregational (from 3.3% to 2.5%) and Methodist (from 0.9% to 0.7%). The number of persons claiming affiliation to Pentecostal/Evangelical/Full Gospel more than doubled from 76,327 in 2000 (6.8%) to 159,033 in 2011 (12.0%). The number of Muslims slightly increased but as proportion of the total population there was a decrease from 5.8% in 2000 to 5.0% in 2011. The category ‘None’ witnessed a small increase from 1.9% to 2.2%, while those who did not state a religion increased significantly, from 1.4% to 11.1%. 1.2% of the population are adherents of Baha'i.

</doc>
<doc id="30174" url="http://en.wikipedia.org/wiki?curid=30174" title="Telecommunications in Trinidad and Tobago">
Telecommunications in Trinidad and Tobago

Telecommunications in Trinidad and Tobago include radio, television, fixed and mobile telephones, and the Internet.
Radio and television.
BBC World Service radio is available on 98.7 FM.
Telephones.
Country Code: +1868<br>
International Call Prefix: 011 (outside NANP)
Calls from Trinidad and Tobago to the US, Canada, and other NANP Caribbean nations, are dialed as 1 + NANP area code + 7-digit number. Calls from Trinidad and Tobago to non-NANP countries are dialed as 011 + country code + phone number with local area code.
Number Format: nxx-xxxx
Internet.
Facebook is the most popular social media platform.
Internet censorship and surveillance.
There are no government restrictions on access to the Internet or credible reports that the government monitors e-mail or Internet chat rooms without judicial oversight.
The constitution and the law provide for freedom of speech and press, and the government generally respects these rights in practice. An independent press, an effective judiciary, and a functioning democratic political system combine to ensure freedom of speech and press. The law prohibits acts that would offend or insult another person or group on the basis of race, origin, or religion or that would incite racial or religious hatred. The constitution and the law prohibit arbitrary interference with privacy, family, home, or correspondence, and the government generally respects these prohibitions in practice.

</doc>
<doc id="30223" url="http://en.wikipedia.org/wiki?curid=30223" title="Telecommunications in the Turks and Caicos Islands">
Telecommunications in the Turks and Caicos Islands

Communications in the Turks and Caicos Islands
Telephone.
Telephones - main lines in use: 3,000 (1994)
Telephones - mobile cellular: 0 (1994)
Telephone system: fair cable and radiotelephone services
Radio.
Radio broadcast stations: AM 3 (one inactive), FM 6, shortwave 0 (1998)
A partial list of AM/FM/SW stations in the Turks and Caicos Islands is provided below:
Radios: 8,000 (1997)
Television.
Television broadcast stations: 2
WIV Cable has been operating on the islands for over 10 years (Channel 4)
New to Turks & Caicos, TCeyeTV started broadcasting on 3 July 2007
broadcasts from The Bahamas are also received; cable television is established) (1997)
Internet.
Internet Service Providers (ISPs): 3 (2013)
Country code (Top level domain): TC

</doc>
<doc id="30344" url="http://en.wikipedia.org/wiki?curid=30344" title="Tanakh">
Tanakh

The Tanakh (; Hebrew: תַּנַ"ךְ‎, ] or ]; also "Tenakh", "Tenak", "Tanach") or Mikra is the canon of the Hebrew Bible. The traditional Hebrew text is known as the Masoretic Text.
"Tanakh" is an acronym of the first Hebrew letter of each of the Masoretic Text's three traditional subdivisions: Torah ("Teaching", also known as the Five Books of Moses), Nevi'im ("Prophets") and Ketuvim ("Writings")—hence TaNaKh. The name "Mikra" (מקרא), meaning "that which is read", is another Hebrew word for the "Tanakh". The books of the Tanakh were passed on by each generation, and according to rabbinic tradition were accompanied by an oral tradition, called the Oral Torah.
Terminology.
The three-part division reflected in the acronym "Tanakh" is well attested in literature of the Rabbinic period. During that period, however, "Tanakh" was not used. Instead, the proper title was "Mikra" (or "Miqra", מקרא, meaning "reading" or "that which is read") because the biblical texts were read publicly. "Mikra" continues to be used in Hebrew to this day, alongside Tanakh, to refer to the Hebrew scriptures. In modern spoken Hebrew, they are interchangeable.
Development and codification.
There is no scholarly consensus as to when the Hebrew Bible canon was fixed: some scholars argue that it was fixed by the Hasmonean dynasty, while others argue it was not fixed until the second century CE or even later. 
According to the Talmud much of the contents of the Tanakh was compiled by the men of the Great Assembly ("Anshei K'nesset HaGedolah"), a task completed in 450 BCE, and has remained unchanged since that date.
Formal closure of the canon has often been ascribed to Rabbinic Judaism after the destruction of the First Temple in 587 BCE.
The twenty-four book canon is mentioned in the Midrash Koheleth 12:12.
Language and pronunciation.
The original writing system of the Hebrew text was an abjad: consonants written with some applied vowel letters ("matres lectionis"). During the early Middle Ages scholars known as the Masoretes created a single formalized system of vocalization. This was chiefly done by Aaron ben Moses ben Asher, in the Tiberias school, based on the oral tradition for reading the Tanakh, hence the name Tiberian vocalization. It also included some innovations of Ben Naftali and the Babylonian exiles. Despite the comparatively late process of codification, some traditional sources and some Orthodox Jews hold the pronunciation and cantillation to derive from the revelation at Sinai, since it is impossible to read the original text without pronunciations and cantillation pauses. The combination of a text (מקרא "mikra"), pronunciation (ניקוד "niqqud") and cantillation (טעמים "te`amim") enable the reader to understand both the simple meaning and the nuances in sentence flow of the text.
Books of the Tanakh.
The Tanakh consists of twenty-four books: it counts as one book each Samuel, Kings, Chronicles and Ezra-Nehemiah and counts "Trei Asar" (תרי עשר, the Twelve Prophets; literally "twelve") as a single book.
Torah.
The Torah (תּוֹרָה, literally "teaching") consists of five books, commonly referred to as the "Five Books of Moses". Printed versions of the Torah are often called "Chamisha Chumshei Torah" (חמישה חומשי תורה, literally the "five five-sections of the Torah"), and informally a "Chumash".
In Hebrew, the five books of the Torah are identified by the first prominent word in each book.
Nevi'im.
"Nevi'im" (Hebrew: נְבִיאִים "Nəḇî'îm"‎, "Prophets") is the second main division of the Hebrew Bible, between the Torah and Ketuvim. It contains two sub-groups, the Former Prophets ("Nevi'im Rishonim" נביאים ראשונים, the narrative books of Joshua, Judges, Samuel and Kings) and the Latter Prophets ("Nevi'im Aharonim" נביאים אחרונים, the books of Isaiah, Jeremiah and Ezekiel and the Twelve Minor Prophets). This division includes the books which cover the time from the entrance of the Israelites into the Land of Israel until the Babylonian captivity of Judah (the "period of prophecy"). 
Their distribution is not chronological, but substantive.
The Twelve Minor Prophets (תרי עשר, "Trei Asar", "The Twelve") considered as one book in Judaism.
Ketuvim.
"Ketuvim" (כְּתוּבִים, "Writings") consists of eleven books, described below.
The poetic books.
In masoretic manuscripts (and some printed editions), Psalms, Proverbs and Job are presented in a special two-column form emphasizing the parallel stichs in the verses, which are a function of their poetry. Collectively, these three books are known as "Sifrei Emet" (an acronym of the titles in Hebrew, איוב, משלי, תהלים yields "Emet" אמ"ת, which is also the Hebrew for "truth").
These three books are also the only ones in Tanakh with a special system of cantillation notes that are designed to emphasize parallel stichs within verses. However, the beginning and end of the book of Job are in the normal prose system.
The five scrolls ("Hamesh Megillot").
The five relatively short books of the Song of Songs, the Book of Ruth, the Book of Lamentations, Ecclesiastes and the Book of Esther are collectively known as the "Hamesh Megillot" (Five Megillot). These are the latest books collected and designated as "authoritative" in the Jewish canon even though they were not complete until the 2nd century CE. These scrolls are traditionally read over the course of the year in many Jewish communities. The list below presents them in the order they are read in the synagogue on holidays, beginning with the Song of Solomon on Passover.
Other books.
Besides the three poetic books and the five scrolls, the remaining books in Ketuvim are Daniel, Ezra–Nehemiah and Chronicles. Although there is no formal grouping for these books in the Jewish tradition, they nevertheless share a number of distinguishing characteristics. 
Order.
The following list presents the books of Ketuvim in the order they appear in most printed editions. It also divides them into three subgroups based on the distinctiveness of "Sifrei Emet" and "Hamesh Megillot".
The three poetic books ("Sifrei Emet")
The Five Megillot ("Hamesh Megillot")
Other books
The Jewish textual tradition never finalized the order of the books in Ketuvim. The Babylonian Talmud (Bava Batra 14b-15a) gives their order as Ruth, Psalms, Job, Proverbs, Ecclesiastes, Song of Solomon, Lamentations of Jeremiah, Daniel, Scroll of Esther, Ezra, Chronicles.
In Tiberian Masoretic codices, including the Aleppo Codex and the Leningrad Codex, and often in old Spanish manuscripts as well, the order is Chronicles, Psalms, Job, Proverbs, Ruth, Song of Solomon, Ecclesiastes, Lamentations of Jeremiah, Esther, Daniel, Ezra.
Jewish commentaries.
There are two major approaches towards study of, and commentary on, the Tanakh. In the Jewish community, the classical approach is religious study of the Bible, where it is assumed that the Bible is divinely inspired. Another approach is to study the Bible as a human creation. In this approach, Biblical studies can be considered as a sub-field of religious studies. The later practice, when applied to the Torah, is considered heresy by the Orthodox Jewish community. As such, much modern day Bible commentary written by non-Orthodox authors is considered forbidden by rabbis teaching in Orthodox yeshivas. Some classical rabbinic commentators, such as Abraham Ibn Ezra, Gersonides, and Maimonides, used many elements of contemporary biblical criticism, including their knowledge of history, science, and philology. Their use of historical and scientific analysis of the Bible was considered acceptable by historic Judaism due to the author's faith commitment to the idea that God revealed the Torah to Moses on Mount Sinai.
The Modern Orthodox Jewish community allows for a wider array of biblical criticism to be used for biblical books outside of the Torah, and a few Orthodox commentaries now incorporate many of the techniques previously found in the academic world, e.g. the Da'at Miqra series. Non-Orthodox Jews, including those affiliated with Conservative Judaism and Reform Judaism, accept both traditional and secular approaches to Bible studies. "Jewish commentaries on the Bible", discusses Jewish Tanakh commentaries from the Targums to classical rabbinic literature, the midrash literature, the classical medieval commentators, and modern day commentaries.

</doc>
<doc id="30364" url="http://en.wikipedia.org/wiki?curid=30364" title="Transition metal">
Transition metal

In chemistry, the term transition metal (or transition element) has two possible meanings:
Jensen reviews the history of the terms "transition element" (or "metal") and "d-block". The word "transition" was first used to describe the elements now known as the d-block by the English chemist Charles Bury in 1921, who referred to a transition series of elements during the change of an inner layer of electrons (for example n=3 in the 4th row of the periodic table) from a stable group of 8 to one of 18, or from 18 to 32.
Classification.
In the "d"-block the atoms of the elements have between 1 and 10 "d" electrons.
The typical electronic structure of transition metal atoms can be written as [ ]"ns"2("n-1")"d"m, following the Madelung rule where the inner "d" orbital is predicted to be filled after the valence-shell "s" orbital. This is actually not the case; the 4s electrons are higher in energy than the 3d as shown spectroscopically. An ion such as Fe2+ has no "4s" electrons: it has the electronic configuration [Ar]3d6 as compared with the configuration of the atom, [Ar]4s23d6.
The elements of groups 3–12 are now generally recognized as transition metals, although the elements La-Lu and Ac-Lr and Group 12 attract different definitions from different authors.
Zinc, cadmium, and mercury are sometimes excluded from the transition metals as they have the electronic configuration [ ]"d"10s2, with no incomplete "d" shell. In the oxidation state +2 the ions have the electronic configuration [ ] d10. However, these elements can exist in other oxidation states, including the +1 oxidation state, as in the diatomic ion Hg22+. The group 12 elements Zn, Cd and Hg may be classed as post-transition metals in this case, because of the formation of a covalent bond between the two atoms of the dimer. However, it is often convenient to include these elements in a discussion of the transition elements. For example, when discussing the crystal field stabilization energy of first-row transition elements, it is convenient to also include the elements calcium and zinc, as both Ca2+ and Zn2+ have a value of zero against which the value for other transition metal ions may be compared. Another example occurs in the Irving-Williams series of stability constants of complexes.
The recent synthesis of mercury(IV) fluoride (HgF4) has been taken by some to reinforce the view that the group 12 elements should be considered transition metal, but some authors still consider this compound to be exceptional.
Position in the Periodic Table.
The d-block as stated earlier, is present in the centre of the long form of periodic table. These are flanked or surrounded by elements belonging to "s" and "p"-blocks on both sides. These are called transition elements since they represent a transition i.e., there is a change from metallic character of "s"-block elements to non-metallic character of "p"-block elements through "d"-block elements which are also metals. As pointed above there are four transition series in this block. Since the filling of electrons takes place in "(n-1)d" orbitals, the periods to which these series belong, is actually one more than the actual series. For example, the elements included in "3d" series belong to fourth period ; the elements included in "4d" series belong to the fifth period and so on.
Electronic configuration.
The general electronic configuration of the "d"-block elements is [Inert gas] "(n-1)d1-10n s1-2"
The "d"-sub-shell is the penultimate (last but one) sub-shell and is denoted as "(n-1) d"-sub-shell. The number of s electrons may vary from one to two. The "s"-sub-shell in the valence shell is represented as the "ns" sub-shell. However, palladium (Pd) is an exception with no electron in the "s"-sub shell. In the periodic table, the transition metals are present in ten groups (3 to 12). Group-2 belongs to the "s"- block with an "ns2" configuration.
The elements in group-3 have an "ns2(n-1)d1" configuration. The first transition series is present in the 4thperiod, and starts after Ca (Z=20) of group-2 which has configuration [Ar]4"s2". The electronic configuration of scandium (Sc), the first element of group-3 with atomic number Z=21 is[Ar]4"s23d1". As we move from left to right, electrons are added to the same "d"-sub-shell till it is complete. The element of group-12 in the first transition series is zinc (Zn) with configuration [Ar]4"s23d10". Since the electrons added fill the "(n-1)d" orbitals, the properties of the "d"-block elements are quite different from those of "s" and "p" block elements in which the filling occurs either in "s" or in "p"-orbitals of the valence shell.
The electronic configuration of the individual elements present in all the transition series are given below:
First (3"d") Transition Series (Sc-Zn)
Second (4"d") Transition Series (Y-Cd)
Third (5"d") Transition Series (Lu-Hg)
Fourth (6"d") Transition Series (Lr-Cn)
A careful look at the electronic configuration of the elements reveals that there are certain exceptions shown by Pt, Au and Hg.. These are either because of the symmetry or nuclear-electron and electron-electron force.
The "(n-1)d" orbitals that are involved in the transition metals are very significant because they influence such properties as magnetic character, variable oxidation states, formation of colored compounds etc. The valence "s(ns)" and "p(np)" orbitals have very little contribution in this regard since they hardly change in the moving from left to the right in a transition series.
In transition metals, there is a greater horizontal similarities in the properties of the elements in a period in comparison to the periods in which the "d"-orbitals are not involved. This is because in a transition series, the valence shell electronic configuration of the elements do not change. However, there are some group similarities as well.
Characteristic properties.
There are a number of properties shared by the transition elements that are not found in other elements, which results from the partially filled "d" shell. These include
Coloured compounds.
Colour in transition-series metal compounds is generally due to electronic transitions of two principal types.
A metal-to-ligand charge transfer (MLCT) transition will be most likely when the metal is in a low oxidation state and the ligand is easily reduced.
In centrosymmetric complexes, such as octahedral complexes, "d"-"d" transitions are forbidden by the Laporte rule and only occur because of vibronic coupling in which a molecular vibration occurs together with a "d-d" transition. Tetrahedral complexes have somewhat more intense colour because mixing "d" and "p" orbitals is possible when there is no centre of symmetry, so transitions are not pure "d-d" transitions. The molar absorptivity (ε) of bands caused by "d-d" transitions are relatively low, roughly in the range 5-500 M−1cm−1 (where M = mol dm−3). Some "d"-"d" transitions are spin forbidden. An example occurs in octahedral, high-spin complexes of manganese(II),
which has a "d"5 configuration in which all five electron has parallel spins; the colour of such complexes is much weaker than in complexes with spin-allowed transitions. Many compounds of manganese(II) appear almost colourless. The spectrum of [Mn(H2O)6]2+ shows a maximum molar absorptivity of about 0.04 M−1cm−1 in the visible spectrum.
Oxidation states.
A characteristic of transition metals is that they exhibit two or more oxidation states, usually differing by one. For example, compounds of vanadium are known in all oxidation states between −1, such as [V(CO)6]-, and +5, such as VO43-.
Main group elements in groups 13 to 17 also exhibit multiple oxidation states. The "common" oxidation states of these elements typically differ by two. For example, compounds of gallium in oxidation states +1 and +3 exist in which there is a single gallium atom. No compound of Ga(II) is known: any such compound would have an unpaired electron and would behave as a free radical and be destroyed rapidly. The only compounds in which gallium has a formal oxidation state of +2 are dimeric compounds, such as [Ga2Cl6]2-, which contain a Ga-Ga bond formed from the unpaired electron on each Ga atom. Thus the main difference in oxidation states, between transition elements and other elements is that oxidation states are known in which there is a single atom of the element and one or more unpaired electrons.
The maximum oxidation state in the first row transition metals is equal to the number of valence electrons from titanium (+4) up to manganese (+7), but decreases in the later elements. In the second and third rows the maximum occurs with ruthenium and osmium (+8). In compounds such as [MnO4]- and OsO4 the elements achieve a stable octet by forming four covalent bonds.
The lowest oxidation states are exhibited in metal carbonyl complexes such as Cr(CO)6 (oxidation state zero) and [Fe(CO)4]2- (oxidation state −2) in which the 18-electron rule is obeyed. These complexes are also covalent.
Ionic compounds are mostly formed with oxidation states +2 and +3. In aqueous solution the ions are hydrated by (usually) six water molecules arranged octahedrally.
Magnetism.
Transition metal compounds are paramagnetic when they have one or more unpaired "d" electrons. In octahedral complexes with between four and seven "d" electrons both high spin and low spin states are possible. Tetrahedral transition metal complexes such as [FeCl4]2- are high spin because the crystal field splitting is small so that the energy to be gained by virtue of the electrons being in lower energy orbitals is always less than the energy needed to pair up the spins. Some compounds are diamagnetic. These include octahedral, low-spin, "d"6 and square-planar "d8 complexes". In these cases, crystal field splitting is such that all the electrons are paired up.
Ferromagnetism occurs when individual atoms are paramagnetic and the spin vectors are aligned parallel to each other in a crystalline material. Metallic iron and the alloy alnico are examples of ferromagnetic materials involving transition metals. Anti-ferromagnetism is another example of a magnetic property arising from a particular alignment of individual spins in the solid state.
Catalytic properties.
The transition metals and their compounds are known for their homogeneous and heterogeneous catalytic activity. This activity is ascribed to their ability to adopt multiple oxidation states and to form complexes. Vanadium(V) oxide (in the contact process), finely divided iron (in the Haber process), and nickel (in catalytic hydrogenation) are some of the examples. Catalysts at a solid surface (nanomaterial-based catalysts) involve the formation of bonds between reactant molecules and atoms of the surface of the catalyst (first row transition metals utilize 3d and 4s electrons for bonding). This has the effect of increasing the concentration of the reactants at the catalyst surface and also weakening of the bonds in the reacting molecules (the activation energy is lowered). Also because the transition metal ions can change their oxidation states, they become more effective as catalysts.
Other properties.
As implied by the name, all transition metals are metals and conductors of electricity.
In general, transition metals possess a high density and high melting points and boiling points. These properties are due to metallic bonding by delocalized d electrons, leading to cohesion which increases with the number of shared electrons. However the group 12 metals have much lower melting and boiling points since their full d subshells prevent d–d bonding. Mercury has a melting point of −38.83 °C and is a liquid at room temperature.
Many transition metals can be bound to a variety of ligands.

</doc>
<doc id="30652" url="http://en.wikipedia.org/wiki?curid=30652" title="Trypsin">
Trypsin

Trypsin (EC ) is a serine protease from the PA clan superfamily, found in the digestive system of many vertebrates, where it hydrolyses proteins. Trypsin is produced in the pancreas as the inactive protease trypsinogen. Trypsin cleaves peptide chains mainly at the carboxyl side of the amino acids lysine or arginine, except when either is followed by proline. It is used for numerous biotechnological processes. The process is commonly referred to as trypsin proteolysis or trypsinisation, and proteins that have been digested/treated with trypsin are said to have been trypsinized.
Function.
In the duodenum, trypsin catalyzes the hydrolysis of peptide bonds, breaking down proteins into smaller peptides. The peptide products are then further hydrolyzed into amino acids via other proteases, rendering them available for absorption into the blood stream. Tryptic digestion is a necessary step in protein absorption as proteins are generally too large to be absorbed through the lining of the small intestine.
Trypsin is produced as the inactive zymogen trypsinogen in the pancreas. When the pancreas is stimulated by cholecystokinin, it is then secreted into the first part of the small intestine (the duodenum) via the pancreatic duct. Once in the small intestine, the enzyme enteropeptidase activates trypsinogen into trypsin by proteolytic cleavage. Auto catalysis can happen with trypsin using trypsinogen as the substrate. This activation mechanism is common for most serine proteases, and serves to prevent autodegradation of the pancreas.
Mechanism.
The enzymatic mechanism is similar to that of other serine proteases. These enzymes contain a catalytic triad consisting of histidine-57, aspartate-102, and serine-195. These three residues form a charge relay that increases nucleophilicity of the active site serine. This is achieved by modifying the electrostatic environment of the serine. The enzymatic reaction that trypsin catalyzes is thermodynamically favorable but requires significant activation energy (it is "kinetically unfavorable"). In addition, trypsin contains an "oxyanion hole" formed by the backbone amide hydrogen atoms of Gly-193 and Ser-195, which serves to stabilize the developing negative charge on the carbonyl oxygen atom of the cleaved amides.
The aspartate residue (Asp 189) located in the catalytic pocket (S1) of trypsin is responsible for attracting and stabilizing positively charged lysine and/or arginine, and is, thus, responsible for the specificity of the enzyme. This means that trypsin predominantly cleaves proteins at the carboxyl side (or "C-terminal side") of the amino acids lysine and arginine except when either is bound to a C-terminal proline, although large-scale mass spectrometry data suggest cleavage occurs even with proline. Trypsin is considered an endopeptidase, i.e., the cleavage occurs within the polypeptide chain rather than at the terminal amino acids located at the ends of polypeptides.
Properties.
Trypsin has an optimal operating pH of about 7.5-8.5 and optimal operating temperature of about 37.1°C.
As a protein trypsin has various molecular weights depending on the source. For example, a molecular weight of 23.3 kDa is reported for trypsin from bovine and porcine sources. 
The activity of trypsin is not affected by the enzyme inhibitor tosyl phenylalanyl chloromethyl ketone, TPCK, which deactivates chymotrypsin. This is important because, in some applications, like mass spectrometry, the specificity of cleavage is important.
Trypsin should be stored at very cold temperatures (between −20°C and −80°C) to prevent autolysis, which may also be impeded by storage of trypsin at pH 3 or by using trypsin modified by reductive methylation. When the pH is adjusted back to pH 8, activity returns.
Isozymes.
The following human genes encode proteins with trypsin enzymatic activity:
Other isoforms of trypsin may also be found in other organisms.
Clinical significance.
Activation of trypsin from proteolytic cleavage of trypsinogen in the pancreas can lead to a series of events that cause pancreatic self-digestion, resulting in pancreatitis. One consequence of the autosomal recessive disease cystic fibrosis is a deficiency in transport of trypsin and other digestive enzymes from the pancreas. This leads to the disorder termed meconium ileus. This disorder involves intestinal obstruction (ileus) due to overly thick meconium, which is normally broken down by trypsin and other proteases, then passed in feces.
Applications.
Trypsin is available in high quantity in pancreases, and can be purified rather easily. Hence it has been used widely in various biotechnological processes.
In a tissue culture lab, trypsin is used to re-suspend cells adherent to the cell culture dish wall during the process of harvesting cells. Some cell types have a tendency to "stick" - or adhere - to the sides and bottom of a dish when cultivated "in vitro". Trypsin is used to cleave proteins bonding the cultured cells to the dish, so that the cells can be suspended in fresh solution and transferred to fresh dishes.
Trypsin can also be used to dissociate dissected cells (for example, prior to cell fixing and sorting).
Trypsin can be used to break down casein in breast milk. If trypsin is added to a solution of milk powder, the breakdown of casein will cause the milk to become translucent. The rate of reaction can be measured by using the amount of time it takes for the milk to turn translucent.
Trypsin is commonly used in biological research during proteomics experiments to digest proteins into peptides for mass spectrometry analysis, e.g. in-gel digestion. Trypsin is particularly suited for this, since it has a very well defined specificity, as it hydrolyzes only the peptide bonds in which the carbonyl group is contributed either by an Arg or Lys residue.
Trypsin can also be used to dissolve blood clots in its microbial form and treat inflammation in its pancreatic form.
In food.
Commercial protease preparations usually consist of a mixture of various protease enzymes that often includes trypsin. These preparations are widely utilized in food processing:
Trypsin inhibitor.
In order to prevent the action of active trypsin in the pancreas which can be highly damaging, inhibitors such as BPTI and SPINK1 in the pancreas and α1-antitrypsin in the serum are present as part of the defense against its inappropriate activation. Any trypsin prematurely formed from the inactive trypsinogen would be bound by the inhibitor. The protein-protein interaction between trypsin and its inhibitors is one of the tightest found, and trypsin is bound by some of its pancreatic inhibitors essentially irreversibly. In contrast with nearly all known protein assemblies, some complexes of trypsin bound by its inhibitors do not readily dissociate after treatment with 8M urea.
See also.
Trypsinization
PA clan of proteases

</doc>
<doc id="30657" url="http://en.wikipedia.org/wiki?curid=30657" title="Terabyte">
Terabyte

The terabyte is a multiple of the unit byte for digital information. The prefix "tera" represents the fourth power of 1000, and means 1012 in the International System of Units (SI), and therefore one terabyte is one trillion (short scale) bytes. The unit symbol for the terabyte is TB.
1 TB = = = .
A related unit, the tebibyte (TiB), using a binary prefix, is the corresponding 4th power of 1024. One terabyte is about 0.9095 tebibytes, or 931 gibibytes.
History.
The first hard disk drives were created in the 1950s and 1960s and were the size of a refrigerator, with a capacity of a few megabytes. In 1982, the first IBM PC with a hard disk drive was released, and had a capacity of 5 megabytes (0.000 005 TB). The first single hard disks of terabyte size reached the mass market in early 2008. s of 2014[ [update]], 1 terabyte solid state drives use an mSATA form factor.
Costs.
In 1991, consumer grade, 1 gigabyte (1/1000 TB) disk drives were available for US$2699 and more, and two years later prices for this capacity had dropped to US$1499. By 1995, 1 GB drives could be purchased for US$849.
Illustrative usage examples.
Examples of the use of "terabyte" to describe data sizes in different fields are:

</doc>
<doc id="30664" url="http://en.wikipedia.org/wiki?curid=30664" title="Telescopium">
Telescopium

Telescopium is a minor constellation in the southern celestial hemisphere, one of twelve created in the 18th century by French astronomer Nicolas Louis de Lacaille and one of several depicting scientific instruments. Its name is a Latinized form of the Greek word for telescope. Telescopium was later much reduced in size by Francis Baily and Benjamin Gould.
The brightest star in the constellation is Alpha Telescopii, a blue-white subgiant with an apparent magnitude of 3.5, followed by the orange giant star Zeta Telescopii at magnitude 4.1. Eta and PZ Telescopii are two young star systems with debris disks and brown dwarf companions. Telescopium hosts two unusual stars with very little hydrogen that are likely to be the result of two merged white dwarfs: HD 168476, also known as PV Telescopii, is a hot blue extreme helium star, while RS Telescopii is an R Coronae Borealis variable. RR Telescopii is a cataclysmic variable that brightened as a nova to magnitude 6 in 1948.
History.
Telescopium was introduced in 1751–52 by Nicolas Louis de Lacaille with the French name "le Telescope", depicting an aerial telescope, after he had observed and catalogued 10,000 southern stars during a two-year stay at the Cape of Good Hope. He devised 14 new constellations in uncharted regions of the Southern Celestial Hemisphere not visible from Europe. All but one honored instruments that symbolised the Age of Enlightenment. Covering 40 degrees of the night sky, the telescope stretched out northwards between Sagittarius and Scorpius. Lacaille had Latinised its name to "Telescopium" by 1763.
The constellation was known by other names. It was called "Tubus Astronomicus" in the eighteenth century, during which time three constellations depicting telescopes were recognised—Tubus Herschelii Major between Gemini and Auriga and Tubus Herschelii Minor between Taurus and Orion, both of which had fallen out of use by the nineteenth century. Johann Bode called it the "Astronomische Fernrohr" in his 1805 "Gestirne" and kept its size, but later astronomers Francis Baily and Benjamin Gould subsequently shrank its boundaries. The much-reduced constellation lost several brighter stars to neighbouring constellations: Beta Telescopii became Eta Sagittarii, which it had been before Lacaille placed it in Telescopium, Gamma was placed in Scorpius and renamed G Scorpii by Gould, Theta Telescopii reverted to its old appellation of d Ophiuchi, and Sigma Telescopii was placed in Corona Australis. Initially uncatalogued, the latter is now known as HR 6875. The original object Lacaille had named Eta Telescopii—the open cluster Messier 7—was in what is now Scorpius, and Gould used the Bayer designation for a magnitude 5 star, which he felt warranted a letter.
Characteristics.
A small constellation, Telescopium is bordered by Sagittarius and Corona Australis to the north, Ara to the west, Pavo to the south, and Indus to the east, cornering on Microscopium to the northeast. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'Tel'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a quadrilateral ("illustrated in infobox"). In the equatorial coordinate system, the right ascension coordinates of these borders lie between 18h 09.1m and 20h 29.5m, while the declination coordinates are between −45.09° and −56.98°. The whole constellation is visible to observers south of latitude 33°N.
Notable features.
Stars.
Within the constellation's borders, there are 57 stars brighter than or equal to apparent magnitude 6.5. With a magnitude of 3.5, Alpha Telescopii is the brightest star in the constellation. It is a blue-white subgiant of spectral type B3IV which lies around 250 light-years away. Close by Alpha Telescopii are the two blue-white stars sharing the designation of Delta Telescopii. Delta¹ Telescopii is of spectral type B6IV and apparent magnitude 4.9, while Delta² Telescopii is of spectral type B3III and magnitude 5.1. They form an optical double, as the stars are estimated to be around 710 and 1190 light-years away respectively. The faint (magnitude 12.23) Gliese 754, a red dwarf of spectral type M4.5V, is one of the nearest 100 stars to Earth at 19.3 light-years distant. Its eccentric orbit around the Galaxy indicates that it may have originated in the Milky Way's thick disk.
At least four of the fifteen stars visible to the unaided eye are orange giants of spectral class K. The second brightest star in the constellation—at apparent magnitude 4.1—is Zeta Telescopii, an orange subgiant of spectral type K1III-IV. Around 1.53 times as massive as the Sun, it shines with 512 times its luminosity. Located 127 light years away from Earth, it has been described as yellow or reddish in appearance. Epsilon Telescopii is a binary star system: the brighter component, Epsilon Telescopii A, is an orange giant of spectral type K0III with an apparent magnitude of +4.52, while the 13th magnitude companion, Epsilon Telescopii B, is 21 arcseconds away from the primary, and just visible with a 15 cm aperture telescope on a dark night. The system is 417 light-years away. Iota Telescopii and HD 169405—magnitude 5 orange giants of spectral types K0III and K0.5III respectively—make up the quartet. They are around 370 and 497 light-years away from the Sun respectively. Another ageing star, Kappa Telescopii is a yellow giant with a spectral type G9III and apparent magnitude of 5.18. Around 1.87 billion years old, this star of around 1.6 solar masses has swollen to 11 times the Sun's diameter. It is approximately 293 light-years from Earth, and is another optical double.
Xi Telescopii is an irregular variable star that ranges between magnitudes 4.89 and 4.94. Located 1079 light-years distant, it is a red giant of spectral type M2III that has a diameter around 5.6 times the Sun's, and a luminosity around 2973 times that of the Sun. Another irregular variable, RX Telescopii is a red supergiant that varies between magnitudes 6.45 and 7.47, just visible to the unaided eye under good viewing conditions. BL Telescopii is an Algol-like eclipsing binary system that varies between apparent magnitudes 7.09 and 9.08 over a period of just over 778 days (2 years 48 days). The primary is a yellow supergiant that is itself intrinsically variable. Dipping from its baseline magnitude of 9.6 to 16.5, RS Telescopii is a rare R Coronae Borealis variable—an extremely hydrogen-deficient supergiant thought to have arisen as the result of the merger of two white dwarfs; fewer than 100 have been discovered as of 2012. The dimming is thought to be caused by carbon dust expelled by the star. As of 2012, four dimmings have been observed. PV Telescopii is a class B-type (blue) extreme helium star that is the prototype of a class of variables known as PV Telescopii variables. First discovered in 1952, it was found to have a very low level of hydrogen. One theory of its origin is that it is the result of a merger between a helium- and a carbon-oxygen white dwarf. If the combined mass does not exceed the Chandrasekhar limit, the former will accrete onto the latter star and ignite to form a supergiant. Later this will become an extreme helium star before cooling to become a white dwarf.
While RR Telescopii, also designated "Nova Telescopii 1948", is often called a slow nova, it is now classified as a symbiotic nova system composed of an M5III pulsating red giant and a white dwarf; between 1944 and 1948 it brightened by about 7 magnitudes before being noticed at apparent magnitude 6.0 in mid-1948. It has since faded slowly to about apparent magnitude 12. QS Telescopii is a binary system composed of a white dwarf and main sequence donor star, in this case the two are close enough to be tidally locked, facing one another. Known as polars, material from the donor star does not form an accretion disk around the white dwarf, but rather streams directly onto it. This is due to the presence of the white dwarf's strong magnetic field.
Although no star systems in Telescopium have confirmed planets, several have been found to have brown dwarf companions. A member of the 12-million-year-old Beta Pictoris moving group of stars that share a common proper motion through space, Eta Telescopii is a young white main sequence star of magnitude 5.0 and spectral type A0V. It has a debris disk and brown dwarf companion of spectral type M7V or M8V that is between 20 and 50 times as massive as Jupiter. The system is complex, as it has a common proper motion with (and is gravitationally bound to) the star HD 181327, which has its own debris disk. This latter star is a yellow-white main sequence star of spectral type F6V of magnitude 7.0. PZ Telescopii is another young star with a debris disk and substellar brown dwarf companion, though at 24 million years of age appears too old to be part of the Beta Pictoris moving group. HD 191760 is a yellow subgiant—a star that is cooling and expanding off the main sequence—of spectral type G3IV/V. Estimated to be just over four billion years old, it is slightly (1.1 to 1.3 times) more massive as the Sun, 2.69 times as luminous, and has around 1.62 times its radius. Using the High Accuracy Radial Velocity Planet Searcher (HARPS) instrument on the ESO 3.6 m Telescope, it was found to have a brown dwarf around 38 times as massive as Jupiter orbiting at an average distance of 1.35 AU with a period of 505 days. This is an unusually close distance from the star, within a range that has been termed the brown-dwarf desert.
Deep sky objects.
The globular cluster NGC 6584 lies near Theta Arae and is 45,000 light-years distant from Earth. It is an Oosterhoff type I cluster, and contains at least 69 variable stars, most of which are RR Lyrae variables. The planetary nebula IC 4699 is of 13th magnitude and lies midway between Alpha and Epsilon Telescopii.
IC 4889 is an elliptical galaxy of apparent magnitude 11.3, which can be found 2 degrees north-north-west of 5.3-magnitude Nu Telescopii. Observing it through a 40 cm telescope will reveal its central region and halo. The Telescopium group is group of twelve galaxies spanning three degrees in the northeastern part of the constellation, lying around 37 megaparsecs (120 million light-years) from our own galaxy. The brightest member is the elliptical galaxy NGC 6868, and to the west lies the spiral galaxy NGC 6861. These are the brightest members of two respective subgroups within the galaxy group, and are heading toward a merger in the future. Occupying an area of around 4' × 2', NGC 6845 is an interacting system of four galaxies—two spiral and two lenticular galaxies—that is estimated to be around 88 megaparsecs (287 million light-years) distant. SN 2008da was a type II supernova observed in one of the spiral galaxies, NGC 6845A, in June 2008. SN 1998bw was a luminous supernova observed in the spiral arm of the galaxy ESO184-G82 in April 1998, and is notable in that it is highly likely to the source of the gamma ray burst GRB 980425. 
References.
Sources
</dl>
Online sources
</dl>
Coordinates: 

</doc>
<doc id="30710" url="http://en.wikipedia.org/wiki?curid=30710" title="Tree of life">
Tree of life

The concept of a tree of life has been used in biology, religion, philosophy, and mythology. A tree of life is a common motif in various world theologies, mythologies, and philosophies. It alludes to the interconnection of all life on our planet and serves as a metaphor for common descent in the evolutionary sense. The term "tree of life" may also be used as a synonym for "sacred tree".
The tree of knowledge, connecting to heaven and the underworld, and the tree of life, connecting all forms of creation, are both forms of the world tree or cosmic tree, according to the "Encyclopædia Britannica", and are portrayed in various religions and philosophies as the same tree.
Religion and mythology.
Various trees of life are recounted in folklore, culture and fiction, often relating to immortality or fertility. They had their origin in religious symbolism.
Ancient Iran.
In pre-Islamic Persian mythology, the Gaokerena world tree is a large, sacred Haoma tree which bears all seeds. Ahriman (Ahreman, Angremainyu) created a frog to invade the tree and destroy it, aiming to prevent all trees from growing on the earth. As a reaction, God (Ahura Mazda) created two kar fish staring at the frog to guard the tree. The two fishes are always staring at the frog and stay ready to react to it. Because Ahriman is responsible for all evil including death, while Ahura Mazda is responsible for all good (including life) the concept of world tree in Persian Mythology is very closely related to the concept of Tree of Life.
The sacred plant "haoma" and the drink made from it. The preparation of the drink from the plant by pounding and the drinking of it are central features of Zoroastrian ritual. Haoma is also personified as a divinity. It bestows essential vital qualities—health, fertility, husbands for maidens, even immortality. The source of the earthly haoma plant is a shining white tree that grows on a paradisiacal mountain. Sprigs of this white haoma were brought to earth by divine birds.
Haoma is the Avestan form of the Sanskrit "soma". The near identity of the two in ritual significance is considered by scholars to point to a salient feature of an Indo-Iranian religion antedating Zoroastrianism.
Another related issue in ancient mythology of Iran is the myth of Mashyа and Mashyane, two trees who were the ancestors of all living beings. This myth can be considered as a prototype for the creation myth where living beings are created by Gods (who have a human form).
Ancient Egypt.
To the Ancient Egyptians, the Tree of Life represented the hierarchical chain of events that brought every thing into existence. The spheres of the Tree of Life demonstrate the order, process, and method of creation.
In Egyptian mythology, in the Ennead system of Heliopolis, the first couple, apart from Shu and Tefnut (moisture and dryness) and Geb and Nuit (earth and sky), are Isis and Osiris. They were said to have emerged from the acacia tree of Iusaaset, which the Egyptians considered the tree of life, referring to it as the "tree in which life and death are enclosed." Acacia trees contain DMT, a psychedelic drug associated with spiritual experiences. A much later myth relates how Set killed Osiris, putting him in a coffin, and throwing it into the Nile, the coffin becoming embedded in the base of a tamarisk tree.
The Egyptians' Holy Sycamore also stood on the threshold of life and death, connecting the two worlds.
Armenia.
In ancient Armenia, the Tree of Life (Կենաց Ծառ) was a religious symbol and was drawn on walls of fortresses and carved on the armor of warriors. The branches of the tree were equally divided on the right and left sides of the stem, with each branch having one leaf, and one leaf on the apex of the tree. Servants stood on each side of the tree with one of their hands up as if they are taking care of the tree.
Assyria.
The Assyrian Tree of Life was represented by a series of nodes and criss-crossing lines. It was apparently an important religious symbol, often attended to by eagle-headed gods and priests, or the King. Assyrilogists have not reached consensus as to the meaning of this symbol. It is multi-valent. The name "Tree of Life" has been attributed to it by modern scholarship; it is not used in the Assyrian sources. In fact, no textual evidence pertaining to the symbol is known to exist.
Baha'i Faith.
The concept of the tree of life appears in the writings of the Baha'i Faith, where it can refer to the Manifestation of God, a great teacher who appears to humanity from age to age. An example of this can be found in the "Hidden Words" of Bahá'u'lláh:
"Have ye forgotten that true and radiant morn, when in those hallowed and blessed surroundings ye were all gathered in My presence beneath the shade of the tree of life, which is planted in the all-glorious paradise? Awestruck ye listened as I gave utterance to these three most holy words: O friends! Prefer not your will to Mine, never desire that which I have not desired for you, and approach Me not with lifeless hearts, defiled with worldly desires and cravings. Would ye but sanctify your souls, ye would at this present hour recall that place and those surroundings, and the truth of My utterance should be made evident unto all of you."
Bahá'u'lláh refers to his male descendents as branches (Aghsán) and calls women leaves.
A distinction has been made between the tree of life and the tree of the knowledge of good and evil. The latter represents the physical world with its opposites, such as good and evil and light and dark. In a different context from the one above, the tree of life represents the spiritual realm, where this duality does not exist.
Buddhism.
The "Bo" tree, also called "Bodhi" tree, according to Buddhist tradition, is the pipal (Ficus religiosa) under which the Buddha sat when he attained Enlightenment (Bodhi) at Bodh Gaya (near Gaya, west-central Bihar state, India). A living pipal at Anuradhapura, Ceylon (now Sri Lanka), is said to have grown from a cutting from the Bo tree sent to that city by King Ashoka in the 3rd century BCE.
According to Tibetan tradition when Buddha went to the holy Lake Manasorovar along with 500 monks, he took with him the energy of Prayaga Raj. Upon his arrival, he installed the energy of Prayaga Raj near Lake Manasorovar, at a place now known as Prayang. Then he planted the seed of this eternal banyan tree next to Mt. Kailash on a mountain known as the "Palace of Medicine Buddha".
China.
In Chinese mythology, a carving of a Tree of Life depicts a phoenix and a dragon; the dragon often represents immortality. A Taoist story tells of a tree that produces a peach every three thousand years. The one who eats the fruit receives immortality.
An archaeological discovery in the 1990s was of a sacrificial pit at Sanxingdui in Sichuan, China. Dating from about 1200 BCE, it contained three bronze trees, one of them 4 meters high. At the base was a dragon, and fruit hanging from the lower branches. At the top is a strange bird-like (phoenix) creature with claws. Also found in Sichuan, from the late Han dynasty (c 25 – 220 CE), is another tree of life. The ceramic base is guarded by a horned beast with wings. The leaves of the tree are coins and people. At the apex is a bird with coins and the Sun.
Christianity.
In Catholic Christianity, the Tree of Life represents the immaculate state of humanity free from corruption and Original Sin before the Fall. Pope Benedict XVI has said that "the Cross is the true tree of life." Saint Bonaventure taught that the medicinal fruit of the Tree of Life is Christ himself. Saint Albert the Great taught that the Eucharist, the Body and Blood of Christ, is the Fruit of the Tree of Life.
In Eastern Christianity the tree of life is the love of God.
Latter Day Saint movement.
The tree of life appears in the Book of Mormon in a revelation to Lehi (see ). It is symbolic of the love of God (see ). Its fruit is described as "most precious and most desirable above all other fruits," which "is the greatest of all the gifts of God" (see ). In another scriptural book, salvation is called "the greatest of all the gifts of God" (see ). In the same book eternal life is also called the "greatest of all the gifts of God" (see ). Because of these references, the tree of life and its fruit is sometimes understood to be symbolic of salvation and post-mortal existence in the presence of God and his love.
Swedenborgianism.
According to Swedenborgianism, the first twelve chapters of Genesis are a symbolic retelling of ancient truths. In his "Arcana Coelestia",
Emanuel Swedenborg (1688–1772) expounded on the symbolism and underlying spiritual meaning of both Genesis and Exodus, and the symbolism regarding the tree of life.
Europe.
In "Dictionaire Mytho-Hermetiqe" (Paris, 1737), Antoine-Joseph Pernety, a famous alchemist, identified the Tree of Life with the Elixir of Life and the Philosopher's Stone.
In "Eden in the East" (1998), Stephen Oppenheimer suggests that a tree-worshipping culture arose in Indonesia and was diffused by the so-called "Younger Dryas" event of c. 8000 BCE, when the sea level rose. This culture reached China (Szechuan), then India and the Middle East. Finally the Finno-Ugaritic strand of this diffusion spread through Russia to Finland where the Norse myth of Yggdrasil took root.
Georgia.
The Borjgali (Georgian: ბორჯღალი) is an ancient Georgian Tree of Life symbol.
Germanic paganism and Norse mythology.
In Germanic paganism, trees played (and, in the form of reconstructive Heathenry and Germanic Neopaganism, continue to play) a prominent role, appearing in various aspects of surviving texts and possibly in the name of gods.
The tree of life appears in Norse religion as "Yggdrasil", the world tree, a massive tree (sometimes considered a yew or ash tree) with extensive lore surrounding it. Perhaps related to Yggdrasil, accounts have survived of Germanic Tribes' honouring sacred trees within their societies. Examples include Thor's Oak, sacred groves, the Sacred tree at Uppsala, and the wooden Irminsul pillar. In Norse Mythology, the apples from Iðunn's ash box provide immortality for the gods.
Hinduism.
The Eternal Banyan Tree ("Akshaya Vata") is located on the bank of the Yamuna inside the courtyard of Allahabad Fort near the confluence of the Yamuna and Ganga Rivers in Allahabad. The eternal and divine nature of this tree has been documented at length in the scriptures.
During the cyclic destruction of creation when the whole earth was enveloped by waters, "akshaya vata" remained unaffected. It is on the leaves of this tree that Lord Krishna rested in the form of a baby when land was no longer visible. And it is here that the immortal sage, Markandeya, received the cosmic vision of the Lord. It is under this tree that Buddha meditates eternally. Legend also has it that the Bodi tree at Gaya is a manifestation of this tree.
Islam.
The "Tree of Immortality" (Arabic: شجرة الخلود) is the tree of life motif as it appears in the Quran. It is also alluded to in hadiths and tafsir. Unlike the biblical account, the Quran mentions only one tree in Eden, also called the tree of immortality, which Allah specifically forbade to Adam and Eve. Satan, disguised as a serpent, repeatedly told Adam to eat from the tree, and eventually both Adam and Eve did so, thus disobeying Allah. The hadiths also speak about other trees in heaven.
According to the Ahmadiyya movement, Quranic reference to the tree is symbolic; eating of the forbidden tree signifies that Adam disobeyed God.
Jewish sources.
"Etz Chaim", Hebrew for "tree of life," is a common term used in Judaism. The expression, found in the Book of Proverbs, is figuratively applied to the Torah itself. "Etz Chaim" is also a common name for yeshivas and synagogues as well as for works of Rabbinic literature. It is also used to describe each of the wooden poles to which the parchment of a Sefer Torah is attached.
The tree of life is mentioned in the Book of Genesis; it is distinct from the tree of the knowledge of good and evil. After Adam disobeyed God by eating fruit from the tree of the knowledge of good and evil, he was driven out of the garden of Eden. Remaining in the garden, however, was the tree of life. To prevent the man's access to this tree in the future, Cherubims with a flaming sword were placed at the east of the garden. ()
In the Book of Proverbs, the tree of life is associated with wisdom: "[Wisdom] is a tree of life to them that lay hold upon her, and happy "[is every one]" that retaineth her." () In the tree of life is associated with calmness: "A soothing tongue is a tree of life; but perverseness therein is a wound to the spirit."
The Book of Enoch, generally considered non-canonical, states that in the time of the great judgment God will give all those whose names are in the Book of Life fruit to eat from the Tree of Life.
Kabbalah.
Jewish mysticism depicts the Tree of Life in the form of ten interconnected nodes, as the central symbol of the Kabbalah. It comprises the ten Sephirot powers in the Divine realm. The panentheistic and anthropomorphic emphasis of this emanationist theology interpreted the Torah, Jewish observance, and the purpose of Creation as the symbolic esoteric drama of unification in the Sephirot, restoring harmony to Creation. From the time of the Renaissance onwards, Jewish Kabbalah became incorporated as an important tradition in non-Jewish Western culture, first through its adoption by Christian Cabala, and continuing in Western esotericism occult Hermetic Qabalah. These adapted the Judaic Kabbalah Tree of Life syncretically by associating it with other religious traditions, esoteric theologies, and magical practices.
Mesoamerica.
The concept of world trees is a prevalent motif in pre-Columbian Mesoamerican cosmologies and iconography. World trees embodied the four cardinal directions, which represented also the fourfold nature of a central world tree, a symbolic "axis mundi" connecting the planes of the Underworld and the sky with that of the terrestrial world.
Depictions of world trees, both in their directional and central aspects, are found in the art and mythological traditions of cultures such as the Maya, Aztec, Izapan, Mixtec, Olmec, and others, dating to at least the Mid/Late Formative periods of Mesoamerican chronology. Among the Maya, the central world tree was conceived as or represented by a "ceiba" tree, and is known variously as a "wacah chan" or "yax imix che", depending on the Mayan language. The trunk of the tree could also be represented by an upright caiman, whose skin evokes the tree's spiny trunk.
Directional world trees are also associated with the four Yearbearers in Mesoamerican calendars, and the directional colors and deities. Mesoamerican codices which have this association outlined include the Dresden, Borgia and Fejérváry-Mayer codices. It is supposed that Mesoamerican sites and ceremonial centers frequently had actual trees planted at each of the four cardinal directions, representing the quadripartite concept.
World trees are frequently depicted with birds in their branches, and their roots extending into earth or water (sometimes atop a "water-monster," symbolic of the underworld). The central world tree has also been interpreted as a representation of the band of the Milky Way.
Middle East.
The "Epic of Gilgamesh" is a similar quest for immortality. In Mesopotamian mythology, Etana searches for a 'plant of birth' to provide him with a son. This has a solid provenance of antiquity, being found in cylinder seals from Akkad (2390–2249 BCE).
"The Book of One Thousand and One Nights" has a story, 'The Tale of Buluqiya', in which the hero searches for immortality and finds a paradise with jewel-encrusted trees. Nearby is a Fountain of Youth guarded by Al-Khidr. Unable to defeat the guard, Buluqiya has to return empty-handed.
North America.
In a myth passed down among the Iroquois, "The World on the Turtle's Back", explains the origin of the land in which a "tree of life" is described. According to the myth, it is found in the heavens, where the first humans lived, until a pregnant woman fell and landed in an endless sea. Saved by a giant turtle from drowning, she formed the world on its back by planting bark taken from the tree.
The tree of life motif is present in the traditional Ojibway cosmology and traditions. It is sometimes described as Grandmother Cedar, or "Nookomis Giizhig" in Anishinaabemowin.
In the book Black Elk Speaks, Black Elk, an Oglala Lakota (Sioux) "wičháša wakȟáŋ" (medicine man and holy man), describes his vision in which after dancing around a dying tree that has never bloomed he is transported to the other world (spirit world) where he meets wise elders, 12 men and 12 women. The elders tell Black Elk that they will bring him to meet "Our Father, the two-legged chief" and bring him to the center of a hoop where he sees the tree in full leaf and bloom and the "chief" standing against the tree. Coming out of his trance he hopes to see that the earthly tree has bloomed, but it is dead.
Serer religion.
In Serer religion, the tree of life as a religious concept forms the basis of Serer cosmogony. Trees were the first things created on Earth by the supreme being Roog (or Koox among the Cangin). In the competing versions of the Serer creation myth, the "Somb" ("Prosopis africana") and the "Saas" tree (acacia albida) are both viewed as trees of life. However, the prevailing view is that, the "Somb" was the first tree on Earth and the progenitor of plant life. The "Somb" was also used in the Serer tumuli and burial chambers, many of which had survived for more than a thousand years. Thus, "Somb" is not only the Tree of Life in Serer society, but the symbol of immortality.
Turkic world.
The World Tree or Tree of Life is a central symbol in Turkic mythology. The blue sky around the tree reflects the peaceful nature of the country and the red ring that surrounds all of the elements symbolizes the ancient faith of rebirth, growth and development of the Turkic peoples.
Biology.
The tree of life is a metaphor describing the relationship of all life on Earth in an evolutionary context. Charles Darwin talked about envisioning evolution as a "tangled bank" in "On the Origin of Species"; however, the book's sole illustration is of a branched diagram that is very tree-like. 
From the first growth of the tree, many a limb and branch has decayed and dropped off; and these fallen branches of various sizes may represent those whole orders, families, and genera which have now no living representatives, and which are known to us only in a fossil state. As we here and there see a thin, straggling branch springing from a fork low down in a tree, and which by some chance has been favoured and is still alive on its summit, so we occasionally see an animal like the Ornithorhynchus (Platypus) or Lepidosiren (South American lungfish), which in some small degree connects by its affinities two large branches of life, and which has apparently been saved from fatal competition by having inhabited a protected station. As buds give rise by growth to fresh buds, and these, if vigorous, branch out and overtop on all sides many a feebler branch, so by generation I believe it has been with the great Tree of Life, which fills with its dead and broken branches the crust of the earth, and covers the surface with its ever-branching and beautiful ramifications.—Charles Darwin, On the Origin of Species
The evolutionary relationships of the tree of life were refined using genetic data by the American microbiologist Carl Woese, the discoverer of the domain Archaea and a pioneer in molecular (genetic) methods in evolutionary biology. In February 2009, BBC One broadcast an animated, interactive tree of life as part of its "Darwin Season."
The Tree of Life Web Project is an ongoing Internet project containing information about phylogeny and biodiversity, produced by biologists from around the world. Each page contains information about one group of organisms and is organized according to a branched tree-like form, thus showing relationships between organisms and groups of organisms.
The neuroanatomical term "arbor vitae" (tree of life) describes the branching pattern between the cortical grey matter and subcortical white matter of the cerebellum.
Popular culture.
Art and architecture.
A 2½ story high "Tree of Life" sculpture by Wisconsin artist Nancy Metz White was installed in Mitchell Boulevard Park in Milwaukee in 2002. The tree is made of brightly painted welded steel and forge flashings recycled from Milwaukee heavy industry.
Austrian symbolist artist Gustav Klimt portrayed his version of the tree of life in his painting, "The Tree of Life, Stoclet Frieze". This iconic painting later inspired the external facade of the "New Residence Hall" (also called the "Tree House"), a colorful 21-story student residence hall at Massachusetts College of Art and Design in Boston, Massachusetts.
Contemporary Welsh artist Jen Delyth created a Celtic Tree of Life symbol, in part based on ancient Celtic veneration of trees and traditional Celtic designs.
Music.
In their album "Emissaries" the black metal Melechesh make a reference to the Tree of Life in their song "Touching the Spheres of Sephiroth."
American rock band O.A.R. featured a tree of life both on the cover art and on the actual c.d. for the album In Between Now and Then
Double album "Bath"/"Leaving Your Body Map" by avant-garde metal band maudlin of the Well was constructed based upon a parallel qabalistic Tree of Life structure.
The double album Axis Mutatis by the electronic group The Shamen contains in some limited editions the instrumental album "Arbor Bona Arbor Mala." The title refers to the tree of life, the ancient symbol found in virtually all Shamanic cultures, linking the underworld with the earth and the heavens. Also, on the cover of Axis Mutatis appears a representation of the tree of life by William Latham.
In Korean boy band, EXO's Music Video MAMA, the twelve forces (EXO's members) will be the one to restore the tree of life.
Rapper Ab-Soul uses the tree of life as the cover of his Control System album

</doc>
<doc id="30736" url="http://en.wikipedia.org/wiki?curid=30736" title="The Book of the Law">
The Book of the Law

Liber AL vel Legis (]) is the central sacred text of Thelema, written down from dictation mostly by Aleister Crowley, although Rose Edith Crowley is also known to have written two phrases into the manuscript of the Book after its dictation. Crowley claimed it was dictated to him by a discarnate entity named Aiwass or Aiwaz. However, the three chapters are largely written in the first person by the Thelemic deities Nuit, Hadit, and Ra-Hoor-Khuit respectively, rather than by Aiwass/Aiwaz.
The technical title of the book is Liber AL vel Legis, sub figura CCXX, as delivered by XCIII=418 to DCLXVI, although this title never occurs in the Book itself, which refers to itself as "the Book of the Law" and "the threefold Book of Law" (chapters 1:35, 3:75). CCXX is 220 in Roman figures, representing The Tree of Life (10 numbers times 22 paths), and is the number of verses of the Book in typescript. XCIII is 93, the enumeration of both "The word of the law" Thelema and Aiwass. DCLXVI is 666, the number of Crowley as Great Beast both as Adept and Magus. This is a way of saying that the book was delivered by Aiwass (whose number is both 93 and 418) to Crowley, who is The Beast 666.
The facsimile manuscript of the Book is not, however, numbered 220, but XXXI (31) as the first chapter's verses are unnumbered in the original manuscript: that is, no verse numbers were dictated to Crowley for chapter one. Both editions were titled by Crowley AL, pronounced "El", value 31, so therefore Liber 31 is the manuscript of "The Book of the Law" called AL (not be confused with Liber 31 by C. S. Jones (Frater Achad), which is an exegesis of some of the qabalistic symbolism of the "Book"), whereas Liber 220 is the edited (strictly according to the editing instructions dictated as part of the text of the Book itself), printed form of the text: see "The Equinox of The Gods" for a full account by Crowley of the reception and publishing of the "Book" according to these internal instructions.
Through the reception of the "Book", Crowley proclaimed the arrival of a new stage in the spiritual evolution of humanity, to be known as the "Æon of Horus". The primary precept of this new aeon is the charge to "Do what thou wilt".
The book contains three chapters, each of which was alleged to be written down in one hour, beginning at noon, on 8 April 9 April, and 10 April in Cairo, Egypt, in the year 1904. Crowley says that the author was an entity named Aiwass, whom he later referred to as his personal Holy Guardian Angel (analogous to but not identical with "Higher Self"). Biographer Lawrence Sutin quotes private diaries that fit this story, and writes that "if ever Crowley uttered the truth of his relation to the "Book"," his public account accurately describes what he remembered on this point.
Crowley himself wrote "Certain very serious questions have arisen with regard to the method by which this Book was obtained. I do not refer to those doubts—real or pretended—which hostility engenders, for all such are dispelled by study of the text; no forger could have prepared so complex a set of numerical and literal puzzles[...]"
The original title of the book was "Liber L vel Legis". Crowley retitled it "Liber AL vel Legis" in 1921, when he also gave the handwritten manuscript its own title, "Liber XXXI".
The book is often referred to simply as "Liber AL", "Liber Legis" or just "AL", though technically the latter two refer only to the manuscript.
Creation.
Summons.
According to Crowley, the story began on 16 March 1904, when he tried to "shew the Sylphs" by use of the Bornless Ritual to his wife, Rose Edith Kelly, while spending the night in the King's Chamber of the Great Pyramid of Giza. Although she could see nothing, she did seem to enter into a light trance and repeatedly said, "They're waiting for you!" Since Rose had no interest in magic or mysticism, he took little interest. However, on the 18th, after invoking Thoth (the god of knowledge), she mentioned Horus by name as the one waiting for him. Crowley, still skeptical, asked her numerous questions about Horus, which she answered accurately supposedly without having any prior study of the subject:
We cannot too strongly insist on the extraordinary character of this identification.
Calculate the odds! We cannot find a mathematical expression for tests 1,2,4,5, or 6, but the other 7 tests give us: 1/10 x 1/84 x 1/4 x 1/6 x 1/7 x 1/10 x 1/15 = 1/21,168,000
Twenty-one million to one against her getting through half the ordeal!
Crowley also gives a different chronology, in which an invocation of Horus preceded the questioning. Lawrence Sutin says this ritual described Horus in detail, and could have given Rose the answers to her husband's questions.
As part of his 'test' for Rose, Crowley claimed they visited the Bulaq Museum (even though that museum had been closed in 1902), where Crowley asked her to point out an image of Horus. Much to Crowley's initial amusement, she passed by several common images of the god, and went upstairs. From across the room Rose identified Horus on the stele of Ankh-ef-en-Khonsu, then housed under inventory number 666 (since moved to the Egyptian Museum of Cairo, number A 9422). The stela would subsequently be known to Thelemites (adherents of Thelema) as the "Stele of Revealing."
On 20 March, Crowley invoked Horus, "with great success". Between 23 March and 8 April, Crowley had the hieroglyphs on the stele translated. Also, Rose revealed that her "informant" was not Horus himself, but his messenger, Aiwass.
Finally, on 7 April, Rose gave Crowley his instructions—for three days he was to enter the "temple" and write down what he heard between noon and 1:00 p.m.
Writing.
Crowley said he wrote "The Book of the Law" on 8, 9 and 10 April 1904, between the hours of noon and 1:00 pm, in the flat where he and his new wife were staying for their honeymoon, which he described as being near the Boulak Museum in a fashionable European quarter of Cairo, let by the firm Congdon & Co. The apartment was on the ground floor, and the "temple" was the drawing room.
Crowley described the encounter in detail in "The Equinox of the Gods", saying that as he sat at his desk in Cairo, the voice of Aiwass came from over his left shoulder in the furthest corner of the room. This voice is described as passionate and hurried, and was "of deep timbre, musical and expressive, its tones solemn, voluptuous, tender, fierce or aught else as suited the moods of the message. Not bass—perhaps a rich tenor or baritone." Further, the voice was devoid of "native or foreign accent".
Crowley also got a "strong impression" of the speaker's general appearance. Aiwass had a body composed of "fine matter," which had a gauze-like transparency. Further, he "seemed to be a tall, dark man in his thirties, well-knit, active and strong, with the face of a savage king, and eyes veiled lest their gaze should destroy what they saw. The dress was not Arab; it suggested Assyria or Persia, but very vaguely."
Despite initially writing that it was an "excellent example of automatic writing," Crowley later insisted that it was not just automatic writing (though the writing included aspects of this, since when Crowley tried to stop writing he was compelled to continue. The writing also recorded Crowley's own thoughts). Rather he said that the experience was exactly like an actual voice speaking to him. This resulted in a few transcription errors, about which the scribe had to later inquire.
Note, moreover, with what greedy vanity I claim authorship even of all the other A∴A∴ Books in Class A, though I wrote them inspired beyond all I know to be I. Yet in these Books did Aleister Crowley, the master of English both in prose and in verse, partake insofar as he was That. Compare those Books with The Book of the Law! The style [of the former] is simple and sublime; the imagery is gorgeous and faultless; the rhythm is subtle and intoxicating; the theme is interpreted in faultless symphony. There are no errors of grammar, no infelicities of phrase. Each Book is perfect in its kind.
I, daring to snatch credit for these [...] dared nowise to lay claim to have touched The Book of the Law, not with my littlest finger-tip.
He also admits to the possibility that Aiwass may be identified with his own subconscious, although he thought this was unlikely:
Of course I wrote them, ink on paper, in the material sense; but they are not My words, unless Aiwaz be taken to be no more than my subconscious self, or some part of it: in that case, my conscious self being ignorant of the Truth in the Book and hostile to most of the ethics and philosophy of the Book, Aiwaz is a severely suppressed part of me. Such a theory would further imply that I am, unknown to myself, possessed of all sorts of praeternatural knowledge and power.
Crowley's former secretary Israel Regardie, on the other hand considered this statement by Crowley to be no real objection to Aiwass being a part of Crowley's unconscious mind, claiming that:
It can safely be said that current psychological theory would agree that any one person is possessed of all sorts of knowledge and power of which he is totally unconscious... Both Freudian and Jungian theory are on the side of such an assumption...
In his introduction to his edition of "The Law is for All", Israel Regardie stated:
It really makes little difference in the long run whether "The Book of the Law" was dictated to [Crowley] by a preterhuman intelligence named Aiwass or whether it stemmed from the creative deeps of Aleister Crowley. The book was written. And he became the mouthpiece for the Zeitgeist, accurately expressing the intrinsic nature of our time as no one else has done to date.
Crowley himself was initially opposed to the book and its message. "I was trying to forget the whole business."
The fact of the matter was that I resented "The Book of the Law" with my whole soul. For one thing, it knocked my Buddhism completely on the head. ... I was bitterly opposed to the principles of the Book on almost every point of morality. The third chapter seemed to me gratuitously atrocious.
Shortly after making a few copies for evaluation by close friends, the manuscript was misplaced and forgotten about. It would be several years before it was found, and the first official publication occurred in 1909.
"The Book of the Law" annoyed me; I was still obsessed by the idea that secrecy was necessary to a magical document, that publication would destroy its importance. I determined, in a mood which I can only describe as a fit of ill temper, to publish "The Book of the Law", and then get rid of it for ever.
Changes to the manuscript.
The final version of "Liber Legis" includes text that did not appear in the original writing, including many small changes to spelling. In several cases, stanzas from the Stele of Revealing were inserted within the text. For example, chapter 1, page 2, line 9 was written as "V.1. of Spell called the Song" and was replaced with:
<poem>
Above, the gemmèd azure is
She bends in ecstasy to kiss
The wingèd globe, the starry blue,
</poem>
On page 6 of chapter 1, the following is in the original manuscript:
And the sign shall be my ecstasy, the consciousness of the continuity of existence, the unfragmentary non-atomic fact of my universality." along with a note: "Write this in whiter words But go forth on.
This was later changed to:
And the sign shall be my ecstasy, the consciousness of the continuity of existence, the omnipresence of my body. (AL I:26)
Again in chapter 1, on page 19, Crowley writes, "(Lost 1 phrase) The shape of my star is—". Later, it was Rose who filled in the lost phrase:
The Five Pointed Star, with a Circle in the Middle, & the circle is Red. (AL I:60)
Chapter 2 has very few changes or corrections. Chapter 3 has a few spelling changes, and includes large chunks inserted from Crowley's paraphrase of The Stele of Revealing.
The phrase "Force of Coph Nia", which is found in chapter 3, on page 64 (verse 72), was filled in by Rose Kelly because that place in the manuscript had been left incomplete as not having been properly heard by Crowley during the supposed dictation. Israel Regardie proposed that Coph Nia could have been intended to represent Ain Soph, the Cabalistic phrase for Infinity, and that Rose might not have known that Hebrew letters are written from right to left or their meaning.
Speakers.
Although the "messenger" of "Liber AL" was Aiwass, each chapter is presented as an expression of one of three god-forms: Nuit, Hadit, and Ra-Hoor-Khuit.
The first chapter is spoken by Nuit, the Egyptian goddess of the night sky, called the Queen of Space. Crowley calls her the "Lady of the Starry Heaven, who is also Matter in its deepest metaphysical sense, who is the infinite in whom all we live and move and have our being."
The second chapter is spoken by Hadit, who refers to himself as the "complement of Nu," his bride. As such, he is the infinitely condensed point, the center of her infinite circumference. Crowley says of him, "He is eternal energy, the Infinite Motion of Things, the central core of all being. The manifested Universe comes from the marriage of Nuit and Hadit; without this could no thing be. This eternal, this perpetual marriage-feast is then the nature of things themselves; and therefore, everything that exists is a "crystallisation of divine ecstasy", and "He sees the expansion and the development of the soul through joy."
The third chapter is spoken by Ra-Hoor-Khuit, "a god of War and of Vengeance", also identified as Hoor-paar-kraat, the Crowned and Conquering Child.
Crowley sums up the speakers of the three chapters thus, "we have Nuit, Space, Hadit, the point of view; these experience congress, and so produce Heru-Ra-Ha, who combines the ideas of Ra-Hoor-Khuit and Hoor-paar-kraat."
The book also introduces:
Interpretation.
Thanks in large part to The Comment, interpretation of the often cryptic text is generally considered by Thelemites a matter for the individual reader. Crowley wrote about Liber AL in great detail throughout the remainder of his life, apparently attempting to decipher its mysteries.
The emancipation of mankind from all limitations whatsoever is one of the main precepts of the Book.
Aiwass, uttering the word Thelema (with all its implications), destroys completely the formula of the Dying God. Thelema implies not merely a new religion, but a new cosmology, a new philosophy, a new ethics. It co-ordinates the disconnected discoveries of science, from physics to psychology, into a coherent and consistent system. Its scope is so vast that it is impossible even to hint at the universality of its application.
Symbology of the "New Aeon of the Child".
The child is not merely a symbol of growth, but of complete moral independence and innocence. We may then expect the New Aeon to release mankind from its pretence of altruism, its obsession of fear and its consciousness of sin. It will possess no consciousness of the purpose of its own existence. It will not be possible to persuade it that it should submit to incomprehensible standards; it will suffer from spasms of transitory passion; it will be absurdly sensitive to pain and suffer from meaningless terror; it will be utterly conscienceless, cruel, helpless, affectionate and ambitious, without knowing why; it will be incapable of reason, yet at the same time intuitively aware of truth.
I might go on indefinitely to enumerate the stigmata of child psychology, but the reader can do it equally for himself, and every idea that comes to him as characteristic of children will strike him as applicable to the events of history since 1904, from the Great War to Prohibition. And if he possess any capacity for understanding the language of symbolism, he will be staggered by the adequacy and accuracy of the summary of the spirit of the New Aeon given in The Book of the Law.
Qabalah of "The Book of the Law".
The general method that Crowley used to interpret the obscurities of "Liber AL" was the Qabalah, especially its numerological method of gematria. He writes, "Many such cases of double entendre, paronomasia in one language or another, sometimes two at once, numerical-literal puzzles, and even (on one occasion) an illuminating connexion of letters in various lines by a slashing scratch, will be found in the Qabalistic section of the Commentary." In "Magick Without Tears" he wrote:
Now there was enough comprehensible at the time to assure me that the Author of the Book knew at least as much Qabalah as I did: I discovered subsequently more than enough to make it certain without error that he knew a very great deal more, and that of an altogether higher order, than I knew; finally, such glimmerings of light as time and desperate study have thrown on many other obscure passages, to leave no doubt whatever in my mind that he is indeed the supreme Qabalist of all time.
He considered the various gematria values of certain key words and phrases, overlapping between the English, Greek, and Hebrew languages, as evidence of the Book's praeterhuman origin.
... it claims to be the statement of transcendental truth, and to have overcome the difficulty of expressing such truth in human language by what really amounts to the invention of a new method of communicating thought, not merely a new language, but a new type of language; a literal and numerical cipher involving the Greek and Hebrew Cabbalas, the highest mathematics etc. It also claims to be the utterance of an illuminated mind co-extensive with the ultimate ideas of which the universe is composed.
How could he prove that he was in fact a being of a kind superior to any of the human race, and so entitled to speak with authority? Evidently he must show KNOWLEDGE and POWER such as no man has ever been known to possess.
He showed his KNOWLEDGE chiefly by the use of cipher or cryptogram in certain passages to set forth recondite facts, including some events which had yet to take place, such that no human being could possibly be aware of them; thus, the proof of his claim exists in the manuscript itself. It is independent of any human witness.
The study of these passages necessarily demands supreme human scholarship to interpret— it needs years of intense application. A great deal has still to be worked out. But enough has been discovered to justify his claim; the most sceptical intelligence is compelled to admit its truth.
This matter is best studied under the Master Therion, whose years of arduous research have led him to enlightenment.
On the other hand, the language of most of the Book is admirably simple, clear and vigorous. No one can read it without being stricken in the very core of his being.
The more than human POWER of Aiwass is shewn by the influence of his Master, and of the Book, upon actual events: and history fully supports the claim made by him. These facts are appreciable by everyone; but are better understood with the help of the Master Therion.
The existence of true religion presupposes that of some discarnate intelligence, whether we call him God or anything else. And this is exactly what no religion had ever proved scientifically. And this is what "The Book of the Law" does prove by internal evidence, altogether independent of any statement of mine. This proof is evidently the most important step in science that could possibly be made: for it opens up an entirely new avenue to knowledge. The immense superiority of this particular intelligence, AIWASS, to any other with which mankind has yet been in conscious communication is shown not merely by the character of the book itself, but by the fact of his comprehending perfectly the nature of the proof necessary to demonstrate the fact of his own existence and the conditions of that existence. And, further, having provided the proof required.
Prophecy of the Book.
Crowley would later consider the subsequent events of his life, and the apparent fulfilment of certain 'predictions' of the book, as further proof:
The author of "The Book of the Law" foresaw and provided against all such difficulties by inserting in the text discoveries which I did not merely not make for years afterwards, but did not even possess the machinery for making. Some, in fact, depend upon events which I had no part in bringing about.
One such key event was Charles Stansfeld Jones claiming the grade of Magister Templi, which Crowley saw as the birth of his 'Magical Son'. Crowley believed that Jones later went on to "discover the Key of it all" as foretold in the book (II:76, III:47).
Crowley believed that Jones' discovery of the critical value of 31 gave Crowley further insight into his qabalistic understanding and interpretation of the book. Upon receiving notification of this discovery, Crowley replied:
\ = 418. "Thou knowest not." Your key opens Palace. CCXX has unfolded like a flower. All solved, even II.76 & III.47. Did you know Π = 3.141593? And oh! lots more!
"The Comment".
Based on several passages, including: "My scribe Ankh-af-na-khonsu, the priest of the princes, shall not in one letter change this book; but lest there be folly, he shall comment thereupon by the wisdom of Ra-Hoor-Khuit" (AL I:36), Crowley felt compelled to interpret AL in writing. He wrote two large sets of commentaries where he attempted to decipher each line.
However, he was not satisfied with these attempts. In 1912, he prepared AL and his current comments on it for publication in The Equinox, I(7). He recalls in his confessions (p. 674) that he thought the existing commentary was "shamefully meagre and incomplete." He later explains, "I had stupidly supposed this Comment to be a scholarly exposition of the Book, an elucidation of its obscurities and a demonstration of its praeterhuman origin. I understand at last that this idea is nonsense. The Comment must be an interpretation of the Book intelligible to the simplest minds, and as practical as the Ten Commandments." Moreover, this Comment should be arrived at "inspirationally," as the Book itself had been.
Years later in 1925 while in Tunis, Tunisia, Crowley received his inspiration. He published the Comment in the Tunis edition of "AL", of which only 11 copies were printed, and what was to become called simply The Comment (which is also called the Short Comment or Tunis Comment), and signed it as Ankh-f-n-khonsu (lit. "He Lives in Khonsu"—a historical priest who lived in Thebes in the 26th dynasty, associated with the Stele of Revealing). It advises the reader that the "study" of the Book is forbidden and states that those who "discuss the contents" are to be shunned. It also suggests that the book be destroyed after first reading.
Crowley later tasked his friend and fellow O.T.O. member Louis Wilkinson with preparing an edited version of Crowley's commentaries which was published some time after Crowley's death as "The Law is for All."
Michael Aquino's commentary.
Michael Aquino of the Temple of Set produced a commentary on "The Book of the Law" based on a Setian perspective. Aquino's commentary is based on concepts introduced in "The Book of Coming Forth by Night", a text that Aquino claimed was divinely inspired by the Egyptian god Set. Aquino stated that the commentary is based on "the perceptual vantage-point of the Aeon of Set as opposed to that of the Aeon of Horus." Aquino claimed that Crowley incorrectly identified the deities depicted on the Stele of Revealing as belonging to the "Osirian triad" (i.e. Osiris, Isis, and Horus the Younger) whereas they are actually associated with the Theban Sun-cult associated with Horus the Elder. In Egyptian mythology, Horus the Younger was the enemy of Set, whereas Horus the Elder, also known as "Harwer" was actually closely associated with Set and was also cast as "the champion of Set in the Osirian mythos".
Skeptical interpretations.
Crowley's former secretary Israel Regardie argued in his biography of Crowley, "The Eye in the Triangle", that Aiwass was an unconscious expression of Crowley's personality. Regardie stated that although Crowley initially regarded Aiwass as one of the secret chiefs, years later he came to believe that Aiwass was his own Holy Guardian Angel. Regardie argued: "If Aiwass was his own Higher Self, then the inference is none other than that Aleister Crowley was the author of the Book, and that he was the external mask for a variety of different hierarchical personalities ... The man Crowley was the lowest rung of the hierarchical ladder, the outer shell of a God, even as we all are, the persona of a Star ... He is the author of "The Book of the Law" even as he is the author of "The Book of the Heart Girt with a Serpent" and "Liber Lapidis Lazuli", and so forth ... these latter books reveal a dialogue between the component parts of Crowley. It seems to me that basically this "Liber Legis" is no different". Regardie also noted resemblances between "The Book of the Law" and these latter holy books, such as the inclusion of "rambling, unintelligible" passages, "some repugnant to reason by their absurdity, and their jarring goatish quality". In 1906 Crowley wrote: "It has struck me – in connection with reading Blake that Aiwass, etc. "Force and Fire" is the very thing I lack. My "conscience" is really an obstacle and a delusion, being a survival of heredity and education." Regardie considered this an "illuminating admission" and argued that due to Crowley’s early religious training he developed an overly rigid superego or conscience. When he rebelled against Christianity, "he must have yearned for qualities and characteristics diametrically opposed to his own. In "The Book of the Law" the wish is fulfilled". "The Book of the Law" was therefore a "colossal wish-fulfilment". Regardie noted that the Book’s rejection of Judaeo-Christian mores was completely in accord with Crowley’s own moral and religious values and that in this sense "it is his Book". Furthermore, although Crowley claimed to have initially objected to the Book's contents, Regardie said that he could not see what a person like Crowley would possibly object to. Regardie referred to Crowley's 1909 statement: "I want blasphemy, murder, rape, revolution, anything, bad or good, but strong", and pointed out that "The Book of the Law" delivered all these things.
He also argued that Rose's ability to answer Crowley's questions about Horus and the Qabala was not as remarkable as Crowley claimed. Rose had been married to Crowley for eight months at this point and Regardie stated that Crowley may well have used Rose as a 'sounding board' for many of his own ideas. Therefore she may not have been as ignorant of magick and mysticism as Crowley let on.
Charles R. Cammell, author of "Aleister Crowley: The Man, the Mage, the Poet" also believed the Book was an expression of Crowley's personality:
The mind behind the maxims is cold, cruel and relentless. Mercy there is none, nor consolation; nor hope save in the service of this dread messenger of the gods of Egypt. Such is "Liber Legis" in letter and spirit; and as such, and in consideration of its manner of reception, it is a document of curious interest. That it is in part (but in part only) an emanation from Crowley's unconscious mind I can believe; for it bears a likeness to his own Daemonic personality.
Journalist Sarah Veale has also argued that Aiwass was an externalised part of Crowley's psyche, and in support of this hypothesis quotes Crowley himself as saying:
Ah, you realize that magick is something we do to ourselves. But it is more convenient to assume the objective existence of an angel who gives us new knowledge than to allege that our invocation has awakened a supernormal power in ourselves." (Kaczynski, 542).
Veale also pointed out the similarity in rhythmic style between "The Book of the Law" and some of Crowley's own non-channelled writings. In "Magick in theory and practice", Crowley claimed that invoking the "barbarous names" in iambic tetrameter was very useful. Many of his own poems are written in iambic tetrameter, such as this excerpt from "The Riddle", a poem to his former lover, Jerome Pollitt:
<poem>Habib hath heard; let all Iran
who spell aright from A to Z
Exalt thy fame and understand
with whom I made a marriage-bed</poem>
Veale states that there are other similarities in writing styles besides the use of the same poetic meter. The fact that a supposedly discarnate intelligence just happened to have the same writing style as Crowley, suggests that Aiwass may have just been part of Crowley's unconscious mind after all.
Scholar Joshua Gunn also argued that the stylistic similarities between the Book and Crowley's poetic writings were too great for it to be anything other than Crowley's work:
Although Crowley sincerely believed that "The Book of the Law" was inspired by superhuman intelligences, its clichéd imagery, overwrought style, and overdone ecophonetic displays are too similar to Crowley's other poetic writings to be the product of something supernatural.
Editions.
The original manuscript of "The Book of the Law" was sent on Crowley's death to Karl Germer, the executor of his will and head of the A.'.A.'. On Germer's death no trace of it could be found in his papers. There matters rested until 1984, when Tom Whitmore, the new owner of a house in Berkeley, California, began searching through the junk left in the basement by the previous owner. Among the used mattresses, lumber, and outdated high school textbooks were two boxes of assorted papers and newspaper clippings dealing with Germer's affairs, the charter of the O.T.O. and an envelope containing the manuscript of "The Book of the Law". Whitmore donated the papers to the O.T.O. How they found their way to a Berkeley basement remains a complete mystery. 
"Liber AL" is also published in many books, including:
And at least one out-of-print audio version common on eBay:

</doc>
<doc id="30746" url="http://en.wikipedia.org/wiki?curid=30746" title="Theory">
Theory

Theory is a contemplative and rational type of abstract or generalizing thinking, or the results of such thinking. Depending on the context, the results might for example include generalized explanations of how nature works. The word has its roots in ancient Greek, but in modern use it has taken on several different related meanings. A theory is not the same as a hypothesis. A theory provides an explanatory framework for some observation, and from the assumptions of the explanation follows a number of possible hypotheses that can be tested in order to provide support for, or challenge, the theory.
A theory can be "normative" (or prescriptive), meaning a postulation about what ought to be. It provides "goals, norms, and standards". A theory can be a body of knowledge, which may or may not be associated with particular explanatory models. To theorize is to develop this body of knowledge.
As already in Aristotle's definitions, theory is very often contrasted to "practice" (from Greek "", πρᾶξις) a Greek term for "doing", which is opposed to theory because pure theory involves no doing apart from itself. A classical example of the distinction between "theoretical" and "practical" uses the discipline of medicine: medical theory involves trying to understand the causes and nature of health and sickness, while the practical side of medicine is trying to make people healthy. These two things are related but can be independent, because it is possible to research health and sickness without curing specific patients, and it is possible to cure a patient without knowing how the cure worked.
In modern science, the term "theory" refers to scientific theories, a well-confirmed type of explanation of nature, made in a way consistent with scientific method, and fulfilling the criteria required by modern science. Such theories are described in such a way that any scientist in the field is in a position to understand and either provide empirical support ("verify") or empirically contradict ("falsify") it. Scientific theories are the most reliable, rigorous, and comprehensive form of scientific knowledge, in contrast to more common uses of the word "theory" that imply that something is unproven or speculative (which is better characterized by the word 'hypothesis'). Scientific theories are distinguished from hypotheses, which are individual empirically testable conjectures, and scientific laws, which are descriptive accounts of how nature will behave under certain conditions.
Ancient uses.
The English word theory was derived from a technical term in philosophy in Ancient Greek. As an everyday word, "theoria", θεωρία, meant "a looking at, viewing, beholding", but in more technical contexts it came to refer to contemplative or speculative understandings of natural things, such as those of natural philosophers, as opposed to more practical ways of knowing things, like that of skilled orators or artisans. The word has been in use in English since at least the late 16th century. Modern uses of the word "theory" are derived from the original definition, but have taken on new shades of meaning, still based on the idea that a theory is a thoughtful and rational explanation of the general nature of things.
Although it has more mundane meanings in Greek, the word θεωρία apparently developed special uses early in the recorded history of the Greek language. In the book "From Religion to Philosophy", Francis Cornford suggests that the Orphics used the word "theory" to mean 'passionate sympathetic contemplation'. Pythagoras changed the word to mean a passionate sympathetic contemplation of mathematical knowledge, because he considered this intellectual pursuit the way to reach the highest plane of existence. Pythagoras emphasized subduing emotions and bodily desires in order to enable the intellect to function at the higher plane of theory. Thus it was Pythagoras who gave the word "theory" the specific meaning which leads to the classical and modern concept of a distinction between theory as uninvolved, neutral thinking, and practice.
In Aristotle's terminology, as has already been mentioned above, theory is contrasted with "praxis" or practice, which remains the case today. For Aristotle, both practice and theory involve thinking, but the aims are different. Theoretical contemplation considers things which humans do not move or change, such as nature, so it has no human aim apart from itself and the knowledge it helps create. On the other hand, "praxis" involves thinking, but always with an aim to desired actions, whereby humans cause change or movement themselves for their own ends. Any human movement which involves no conscious choice and thinking could not be an example of "praxis" or doing.
Theories formally and scientifically.
Theories are analytical tools for understanding, explaining, and making predictions about a given subject matter. There are theories in many and varied fields of study, including the arts and sciences. A formal theory is syntactic in nature and is only meaningful when given a semantic component by applying it to some content (i.e. facts and relationships of the actual historical world as it is unfolding). Theories in various fields of study are expressed in natural language, but are always constructed in such a way that their general form is identical to a theory as it is expressed in the formal language of mathematical logic. Theories may be expressed mathematically, symbolically, or in common language, but are generally expected to follow principles of rational thought or logic.
Theory is constructed of a set of sentences which consist entirely of true statements about the subject matter under consideration. However, the truth of any one of these statements is always relative to the whole theory. Therefore the same statement may be true with respect to one theory, and not true with respect to another. This is, in ordinary language, where statements such as "He is a terrible person" cannot be judged to be true or false without reference to some interpretation of who "He" is and for that matter what a "terrible person" is under the theory.
Sometimes two theories have exactly the same explanatory power because they make the same predictions. A pair of such theories is called indistinguishable or observationally equivalent, and the choice between them reduces to convenience or philosophical preference.
The form of theories is studied formally in mathematical logic, especially in model theory. When theories are studied in mathematics, they are usually expressed in some formal language and their statements are closed under application of certain procedures called rules of inference. A special case of this, an axiomatic theory, consists of axioms (or axiom schemata) and rules of inference. A theorem is a statement that can be derived from those axioms by application of these rules of inference. Theories used in applications are abstractions of observed phenomena and the resulting theorems provide solutions to real-world problems. Obvious examples include arithmetic (abstracting concepts of number), geometry (concepts of space), and probability (concepts of randomness and likelihood).
Gödel's incompleteness theorem shows that no consistent, recursively enumerable theory (that is, one whose theorems form a recursively enumerable set) in which the concept of natural numbers can be expressed, can include all true statements about them. As a result, some domains of knowledge cannot be formalized, accurately and completely, as mathematical theories. (Here, formalizing accurately and completely means that all true propositions—and only true propositions—are derivable within the mathematical system.) This limitation, however, in no way precludes the construction of mathematical theories that formalize large bodies of scientific knowledge.
Underdetermination.
A theory is "underdetermined" (also called "indeterminacy of data to theory") if, given the available evidence cited to support the theory, there is a rival theory which is inconsistent with it that is at least as consistent with the evidence. Underdetermination is an epistemological issue about the relation of evidence to conclusions.
Intertheoretic reduction and elimination.
If there is a new theory which is better at explaining and predicting phenomena than an older theory (i.e. it has more explanatory power), we are justified in believing that the newer theory describes reality more correctly. This is called an "intertheoretic reduction" because the terms of the old theory can be reduced to the terms of the new one. For instance, our historical understanding about "sound", "light" and "heat" have today been reduced to "wave compressions and rarefactions", "electromagnetic waves", and "molecular kinetic energy", respectively. These terms which are identified with each other are called "intertheoretic identities." When an old theory and a new one are parallel in this way, we can conclude that we are describing the same reality, only more completely.
In cases where a new theory uses new terms which do not reduce to terms of an older one, but rather replace them entirely because they are actually a misrepresentation it is called an "intertheoretic elimination." For instance, the obsolete scientific theory that put forward an understanding of heat transfer in terms of the movement of caloric fluid was eliminated when a theory of heat as energy replaced it. Also, the theory that phlogiston is a substance released from burning and rusting material was eliminated with the new understanding of the reactivity of oxygen.
Theories vs. theorems.
Theories are distinct from theorems. Theorems are derived deductively from objections according to a formal system of rules, sometimes as an end in itself and sometimes as a first step in testing or applying a theory in a concrete situation; theorems are said to be true in the sense that the conclusions of a theorem are logical consequences of the objections. Theories are abstract and conceptual, and to this end they are always considered true. They are supported or challenged by observations in the world. They are 'rigorously tentative', meaning that they are proposed as true and expected to satisfy careful examination to account for the possibility of faulty inference or incorrect observation. Sometimes theories are incorrect, meaning that an explicit set of observations contradicts some fundamental objection or application of the theory, but more often theories are corrected to conform to new observations, by restricting the class of phenomena the theory applies to or changing the assertions made. An example of the former is the restriction of Classical mechanics to phenomena involving macroscopic lengthscales and particle speeds much lower than the speed of light.
"Sometimes a hypothesis never reaches the point of being considered a theory because the answer is not found to derive its assertions analytically or not applied empirically."
Philosophical theories.
Theories whose subject matter consists not in empirical data, but rather in ideas are in the realm of "philosophical theories" as contrasted with "scientific theories". At least some of the elementary theorems of a philosophical theory are statements whose truth cannot necessarily be scientifically tested through empirical observation.
Fields of study are sometimes named "theory" because their basis is some initial set of objections describing the field's approach to a subject matter. These assumptions are the elementary theorems of the particular theory, and can be thought of as the axioms of that field. Some commonly known examples include set theory and number theory; however literary theory, critical theory, and music theory are also of the same form.
Metatheory.
One form of philosophical theory is a "metatheory" or "meta-theory". A metatheory is a theory whose subject matter is some other theory. In other words it is a theory about a theory. Statements made in the metatheory about the theory are called metatheorems.
Political theories.
A political theory is an ethical theory about the law and government. Often the term "political theory" refers to a general view, or specific ethic, political belief or attitude, about politics.
Scientific theories.
In science, the term "theory" refers to "a well-substantiated explanation of some aspect of the natural world, based on a body of facts that have been repeatedly confirmed through observation and experiment." Theories must also meet further requirements, such as the ability to make falsifiable predictions with consistent accuracy across a broad area of scientific inquiry, and production of strong evidence in favor of the theory from multiple independent sources.
The strength of a scientific theory is related to the diversity of phenomena it can explain, which is measured by its ability to make falsifiable predictions with respect to those phenomena. Theories are improved (or replaced by better theories) as more evidence is gathered, so that accuracy in prediction improves over time; this increased accuracy corresponds to an increase in scientific knowledge. Scientists use theories as a foundation to gain further scientific knowledge, as well as to accomplish goals such as inventing technology or curing disease.
Definitions from scientific organizations.
The United States National Academy of Sciences defines scientific theories as follows:
The formal scientific definition of "theory" is quite different from the everyday meaning of the word. It refers to a comprehensive explanation of some aspect of nature that is supported by a vast body of evidence. Many scientific theories are so well established that no new evidence is likely to alter them substantially. For example, no new evidence will demonstrate that the Earth does not orbit around the sun (heliocentric theory), or that living things are not made of cells (cell theory), that matter is not composed of atoms, or that the surface of the Earth is not divided into solid plates that have moved over geological timescales (the theory of plate tectonics)...One of the most useful properties of scientific theories is that they can be used to make predictions about natural events or phenomena that have not yet been observed.
From the American Association for the Advancement of Science:
A scientific theory is a well-substantiated explanation of some aspect of the natural world, based on a body of facts that have been repeatedly confirmed through observation and experiment. Such fact-supported theories are not "guesses" but reliable accounts of the real world. The theory of biological evolution is more than "just a theory." It is as factual an explanation of the universe as the atomic theory of matter or the germ theory of disease. Our understanding of gravity is still a work in progress. But the phenomenon of gravity, like evolution, is an accepted fact.
Note that the term "theory" would not be appropriate for describing untested but intricate hypotheses or even scientific models.
Philosophical views.
The logical positivists thought of scientific theories as "deductive theories" - that a theory's content is based on some formal system of logic and on basic axioms. In a deductive theory, any sentence which is a logical consequence of one or more of the axioms is also a sentence of that theory. This is called the received view of theories.
In the semantic view of theories, which has largely replaced the received view, theories are viewed as scientific models. A model is a logical framework intended to represent reality (a "model of reality"), similar to the way that a map is a graphical model that represents the territory of a city or country. In this approach, theories are a specific category of models which fulfill the necessary criteria. (See Theories as models for further discussion.)
In physics.
In physics the term "theory" is generally used for a mathematical framework—derived from a small set of basic postulates (usually symmetries, like equality of locations in space or in time, or identity of electrons, etc.)—which is capable of producing experimental predictions for a given category of physical systems. One good example is classical electromagnetism, which encompasses results derived from gauge symmetry (sometimes called gauge invariance) in a form of a few equations called Maxwell's equations. The specific mathematical aspects of classical electromagnetic theory are termed "laws of electromagnetism", reflecting the level of consistent and reproducible evidence that supports them. Within electromagnetic theory generally, there are numerous hypotheses about how electromagnetism applies to specific situations. Many of these hypotheses are already considered to be adequately tested, with new ones always in the making and perhaps untested.
The term "theoretical".
Acceptance of a theory does not require that all of its major predictions be tested, if it is already supported by sufficiently strong evidence. For example, certain tests may be unfeasible or technically difficult. As a result, theories may make predictions that have not yet been confirmed or proven incorrect; in this case, the predicted results may be described informally with the term "theoretical." These predictions can be tested at a later time, and if they are incorrect, this may lead to revision or rejection of the theory.

</doc>
<doc id="30758" url="http://en.wikipedia.org/wiki?curid=30758" title="Age of Enlightenment">
Age of Enlightenment

The Age of Enlightenment (or simply the Enlightenment, or Age of Reason) is an era from the 1650s to the 1780s in which cultural and intellectual forces in Western Europe emphasized reason, analysis, and individualism rather than traditional lines of authority. It was promoted by philosophes and local thinkers in urban coffee houses, salons, and Masonic lodges. It challenged the authority of institutions that were deeply rooted in society, especially the Catholic Church; there was much talk of ways to reform society with toleration, science and skepticism.
Philosophers including Francis Bacon (1562–1626), René Descartes (1596–1650), John Locke (1632–1704), Baruch Spinoza (1632–1677), Pierre Bayle (1647–1706), Voltaire (1694–1778), David Hume (1711–1776), Cesare Beccaria (1738–1794), Immanuel Kant (1724–1804), and Sir Isaac Newton (1642–1727) influenced society by publishing widely read works. Upon learning about enlightened views, some rulers met with intellectuals and tried to apply their reforms, such as allowing for toleration, or accepting multiple religions, in what became known as enlightened absolutism. Coinciding with the Age of Enlightenment was the Scientific revolution, spearheaded by Newton.
New ideas and beliefs spread around the continent and were fostered by an increase in literacy due to a departure from solely religious texts. Publications include "Encyclopédie" (1751–72) that was edited by Denis Diderot and (until 1759) Jean le Rond d'Alembert. Some 25,000 copies of the 35 volume encyclopedia were sold, half of them outside France. The "Dictionnaire philosophique" (Philosophical Dictionary, 1764) and "Letters on the English" (1733) written by Voltaire (1694–1778) were revolutionary texts that spread the ideals of the Enlightenment. Some of these ideals proved influential and decisive in the course of the French Revolution, which began in 1789. After the Revolution, the Enlightenment was followed by an opposing intellectual movement known as Romanticism.
Use of the term.
The term "Enlightenment" emerged in English in the later part of the 19th century, with particular reference to French philosophy, as the equivalent of the French term 'Lumières' (used first by Dubos in 1733 and already well established by 1751). From Immanuel Kant's 1784 essay "Beantwortung der Frage: Was ist Aufklärung?" ("") the German term became 'Aufklärung' ("aufklären" = to illuminate; "sich aufklären" = to clear up).
However, scholars have never agreed on a definition of the Enlightenment, or on its chronological or geographical extent. Terms like "les Lumières" (French), "illuminismo" (Italian), "ilustración" (Spanish) and "Aufklärung" (German) referred to partly overlapping movements. Not until the late nineteenth century did English scholars agree they were talking about "the Enlightenment."
Enlightenment historiography began in the period itself, from what "Enlightenment figures" said about their work. A dominant element was the intellectual angle they took. D'Alembert's "Preliminary Discourse of l'Encyclopédie" provides a history of the Enlightenment which comprises a chronological list of developments in the realm of knowledge – of which the "Encyclopédie" forms the pinnacle.
A more philosophical example of this was the 1783 essay contest (in itself an activity typical of the Enlightenment) announced by the Berlin newspaper "Berlinische Monatsschrift", which asked that very question: "What is Enlightenment?" Jewish philosopher Moses Mendelssohn was among those who responded, referring to Enlightenment as a process by which man was educated in the use of reason ("Jerusalem", 1783).
Immanuel Kant also wrote a response, referring to Enlightenment as "man's release from his self-incurred tutelage", tutelage being "man's inability to make use of his understanding without direction from another". "For Kant, Enlightenment was mankind's final coming of age, the emancipation of the human consciousness from an immature state of ignorance." According to historian Roy Porter, the thesis of the liberation of the human mind from the dogmatic state of ignorance that he argues was prevalent at the time is the epitome of what the age of enlightenment was trying to capture.
According to Bertrand Russell, however, the enlightenment was a phase in a progressive development, which began in antiquity, and that reason and challenges to the established order were constant ideals throughout that time. Russell argues that the enlightenment was ultimately born out of the Protestant reaction against the Catholic counter-reformation, when the philosophical views of the past two centuries crystallized into a coherent world view. He argues that many of the philosophical views, such as affinity for democracy against monarchy, originated among Protestants in the early 16th century to justify their desire to break away from the Pope and the Catholic Church. Though many of these philosophical ideals were picked up by Catholics, Russell argues, by the 18th century the Enlightenment was the principal manifestation of the schism that began with Martin Luther.
Chartier (1991) argues that the Enlightenment was only invented after the fact for a political goal. He claims the leaders of the French Revolution created an Enlightenment canon of basic text, by selecting certain authors and identifying them with the Enlightenment in order to legitimize their republican political agenda.
Jonathan Israel rejects the attempts of postmodern and Marxian historians to understand the revolutionary ideas of the period purely as by-products of social and economic transformations. He instead focuses on the history of ideas in the period from 1650 to the end of the 18th century, and claims that it was the ideas themselves that caused the change that eventually led to the revolutions of the latter half of the 18th century and the early 19th century. Israel argues that until the 1650s Western civilization "was based on a largely shared core of faith, tradition and authority".
Up until this date most intellectual debates revolved around "confessional" – that is, Catholic, Lutheran, Reformed (Calvinist), or Anglican issues, and the main aim of these debates was to establish which bloc of faith ought to have the "monopoly of truth and a God-given title to authority". After this date everything thus previously rooted in tradition was questioned and often replaced by new concepts in the light of philosophical reason. After the second half of the 17th century and during the 18th century a "general process of rationalization and secularization set in which rapidly overthrew theology's age-old hegemony in the world of study", and thus confessional disputes were reduced to a secondary status in favor of the "escalating contest between faith and incredulity".
Time span.
There is little consensus on the precise beginning of the age of Enlightenment; the beginning of the 18th century (1701) or the middle of the 17th century (1650) are often used as an approximate starting point. If taken back to the mid-17th century, the Enlightenment would trace its origins to Descartes' "Discourse on Method", published in 1637. In France, many cited the publication of Isaac Newton's "Principia Mathematica" in 1687. It is argued by several historians and philosophers that the beginning of the Enlightenment is when Descartes shifted the epistemological basis from external authority to internal certainty by his cogito ergo sum published in 1637.
As to its end, most scholars use the last years of the century – often choosing the French Revolution of 1789 or the beginning of the Napoleonic Wars (1804–15) as a convenient point in time with which to date the end of the Enlightenment.
Furthermore, the term "Enlightenment" is anachronistic and often applied across epochs. For example, in their work Dialectic of Enlightenment, Max Horkheimer and Theodor W. Adorno see developments of the 20th century as late consequences of the Enlightenment: humans are installed as "Master" of a world being freed from its magic; truth is understood as a system; rationality becomes an instrument and an ideology managed by apparatuses; civilisation turns into the barbarism of fascism; civilizing effects of the Enlightenment turn into their opposite; and exactly this – they claim – corresponds to the problematic structure of the Enlightenment's way of thinking. Jürgen Habermas, however, disagrees with his teachers' (Adorno and Horkheimer's) view of the Enlightenment as a process of decay. He talks about an "incomplete project of modernity" which, in a process of communicative actions, always asks for rational reasons.
Goals.
Although Enlightenment thinkers generally shared a similar set of values, their philosophical perspectives and methodological approaches to accomplishing their goals varied in significant and sometimes contradictory ways. As Outram notes, the Enlightenment comprised "many different paths, varying in time and geography, to the common goals of progress, of tolerance, and the removal of abuses in Church and state".
In his essay "What is Enlightenment?" (1784), Immanuel Kant described it simply as freedom to use one's own intelligence. More broadly, the Enlightenment period is marked by increasing empiricism, scientific rigor, and reductionism, along with increased questioning of religious orthodoxy.
Historian Peter Gay asserts that the Enlightenment broke through "the sacred circle," whose dogma had circumscribed thinking. The Sacred Circle is a term he uses to describe the interdependent relationship between the hereditary aristocracy, the leaders of the church, and the text of the Bible. This interrelationship manifests itself as kings invoking the doctrine "Divine Right of Kings" to rule. Thus, the church sanctioned the rule of the king and in return the king defended the church.
Zafirovski (2010) argues that the Enlightenment is the source of critical ideas, such as the centrality of freedom, democracy, and reason as primary values of society – as opposed to the divine right of kings or traditions as the ruling authority. This view argues that the establishment of a contractual basis of rights would lead to the market mechanism and capitalism, the scientific method, religious tolerance, and the organization of states into self-governing republics through democratic means. In this view, the tendency of the "philosophes" in particular to apply rationality to every problem is considered the essential change. Later critics of the Enlightenment, such as the Romantics of the 19th century, contended that its goals for rationality in human affairs were too ambitious ever to be achieved.
A variety of 19th-century movements, including liberalism and neo-classicism, traced their intellectual heritage back to the Enlightenment.
National variations.
The Enlightenment took hold in most European countries, often with a specific local emphasis. For example, in France it became associated with anti-government and anti-Church radicalism while in Germany it reached deep into the middle classes and where it expressed a spiritualistic and nationalistic tone without threatening governments or established churches.
Government responses varied widely. In France, the government was hostile, and the "philosophes" fought against its censorship, sometimes being imprisoned or hounded into exile. The British government for the most part ignored the Enlightenment's leaders in England and Scotland although it did give Isaac Newton a knighthood and a very lucrative government office.
Enlightened absolutism.
In several nations, powerful rulers – called "enlightened despots" by historians – welcomed leaders of the Enlightenment at court and asked them to help design laws and programs to reform the system, typically to build stronger national states. The most prominent of those rulers were Frederick the Great of Prussia, Catherine the Great, Empress of Russia from 1762 to 1796, Leopold II, who had ruled the Grand Duchy of Tuscany from 1765 to 1790, and Joseph II, Emperor of Austria from 1780 to 1790. Joseph was over-enthusiastic, announcing so many reforms that had so little support that revolts broke out and his regime became a comedy of errors and nearly all his programs were reversed. Senior ministers Pombal in Portugal and Struensee in Denmark governed according to Enlightenment ideals.
Britain.
Scotland.
By 1750 Scotland's major cities had created an intellectual infrastructure of mutually supporting institutions such as universities, reading societies, libraries, periodicals, museums and masonic lodges. The Scottish network was "predominantly liberal Calvinist, Newtonian, and 'design' oriented in character which played a major role in the further development of the transatlantic Enlightenment". In France, Voltaire said "we look to Scotland for all our ideas of civilization," and the Scots in turn paid close attention to French ideas. Historian Bruce Lenman says the Scots' "central achievement was a new capacity to recognize and interpret social patterns." The first major philosopher of the Scottish Enlightenment was Francis Hutcheson, who held the Chair of Philosophy at the University of Glasgow from 1729 to 1746. A moral philosopher who produced alternatives to the ideas of Thomas Hobbes, one of his major contributions to world thought was the utilitarian and consequentialist principle that virtue is that which provides, in his words, "the greatest happiness for the greatest numbers". Much of what is incorporated in the scientific method (the nature of knowledge, evidence, experience, and causation) and some modern attitudes towards the relationship between science and religion were developed by his protégés David Hume and Adam Smith. Hume became a major figure in the skeptical philosophical and empiricist traditions of philosophy. He and other Scottish Enlightenment thinkers developed a 'science of man', which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar, and William Robertson, all of whom merged a scientific study of how humans behaved in ancient and primitive cultures with a strong awareness of the determining forces of modernity. Modern sociology largely originated from this movement, and Hume's philosophical concepts that directly influenced James Madison (and thus the U.S. Constitution) and as popularised by Dugald Stewart, would be the basis of classical liberalism. Adam Smith published "The Wealth of Nations", often considered the first work on modern economics. It had an immediate impact on British economic policy that continues into the 21st century. The focus of the Scottish Enlightenment ranged from intellectual and economic matters to the specifically scientific as in the work of William Cullen, physician and chemist; James Anderson, an agronomist; Joseph Black, physicist and chemist; and James Hutton, the first modern geologist.
Francis Hutcheson, Adam Smith, and David Hume paved the way for the modernization of Scotland and the entire Atlantic world. Hutcheson, the father of the Scottish Enlightenment, championed political liberty and the right of popular rebellion against tyranny. Smith, in his monumental "Wealth of Nations" (1776), advocated liberty in the sphere of commerce and the global economy. Hume developed philosophical concepts that directly influenced James Madison and thus the U.S. Constitution.
Scientific progress was influenced by, amongst others, the discovery of carbon dioxide (fixed air) by the chemist Joseph Black, the argument for deep time by the gentleman geologist James Hutton, and the invention of the steam engine by James Watt. In a similar vein, the University of Edinburgh's Medical School was arguably the leading scientific institution of Europe. Students from far and wide travelled to the university to study chemistry with William Cullen, James Black, and Thomas Charles Hope, natural history with John Hope, John Walker, and Robert Jameson, and anatomy with the Alexander Monro primus, secondus, and tertius.
The second stage of the Scottish Enlightenment, from the 1780s to the 1810s, consisted of a younger generation of scholars intent on popularizing the ideas of their predecessors. The end result was a reinterpretation and popularisation of the 'Scottish Enlightenment' as a set of ideals that were in turn significantly influential on liberal politics and the university systems of Britain, America and, later, Australia. The de facto leader of this movement was Dugald Stewart. Other names include Sir Walter Scott, Alexander Fraser Tytler, Sir James Hall, and John Playfair.
Dugald Stewart was a student of Adam Ferguson in Edinburgh. He then spent the years 1771 and 1772 under the instruction of Thomas Reid in Glasgow; it was Reid rather than Ferguson who was crucial to Stewart's philosophical development. From an early age, Dugald Stewart exhibited the kind of intelligence typical to a polymath. Though his main interest was philosophy, his talent for mathematics led to his job, at the age of 25, as Professor of Mathematics at Edinburgh. He held that position, initially with and then in succession to his father, Matthew Stewart. Having also substituted in the moral philosophy chair from 1778 to 1779, when Ferguson was in America working for the British government, Stewart finally took the place of his father in 1785. He held this second Chair for 25 years, and lectured so famously well that by the time of his retirement from teaching in 1810, he had developed a distinguished reputation in Europe and in North America. Stewart had a huge impact on the intellectual climate of his time, partly through his lectures, partly through his writings. He attracted students from England, Europe and America, as well as domestic students, in numbers that had never been seen before. Their impact was exceptional. Lord Cockburn, a student of Stewart’s and subsequently a Scottish judge of considerable distinction, records that ‘To me Stewart’s lectures were like the opening of the heavens. I felt that I had a soul. Dugald Stewart was one of the greatest didactic orators’. Stewart lectured at the University of Edinburgh during the 1790s and then took his views to the British public through his books and many essays in the progressive periodicals that circulated across the British Empire. These late Enlightenment publications, combined with his many books, went on to have a profound impact on 19th-century utilitarianism, psychology, metaphysics, political economy, and, crucially, classic liberalism.
England.
Thomas Hobbes wrote the 1651 book "Leviathan", which provided the foundation for social contract theory. Though he was a champion of absolutism for the sovereign, Hobbes also developed some of the fundamentals of European liberal thought: the right of the individual; the natural equality of all men; the artificial character of the political order (which led to the later distinction between civil society and the state); the view that all legitimate political power must be "representative" and based on the consent of the people; and a liberal interpretation of law which leaves people free to do whatever the law does not explicitly forbid.
John Locke was one of the most influential Enlightenment thinkers. He influenced other thinkers such as Rousseau and Voltaire, among others. "He is one of the dozen or so thinkers who are remembered for their influential contributions across a broad spectrum of philosophical subfields – in Locke's case, across epistemology, the philosophy of language, the philosophy of mind, metaphysics, rational theology, ethics, and political philosophy."
Closely associated with the 1st Earl of Shaftesbury, who led the parliamentary grouping that later became the Whig party, Locke is still known today for his liberalism in political theory. He was particularly known for developing the social contract theory, an idea in political philosophy typically associated with Locke and Rousseau. The theory stated that a government and its subjects enter into an unspoken contract when that government takes power. The contract states that in exchange for some societal freedoms to the government or establishment and its laws, the subjects receive and are free to demand protection. The government’s authority lies in the consent of the governed. Locke is well known for his assertion that individuals have a right to "Life, Liberty and Property", and his belief that the natural right to property is derived from labor. Tutored by Locke, Anthony Ashley-Cooper, 3rd Earl of Shaftesbury wrote in 1706: "There is a mighty Light which spreads its self over the world especially in those two free Nations of England and Holland; on whom the Affairs of Europe now turn".
Mary Wollstonecraft was one of England's earliest feminist philosophers. She argued for a society based on reason, and that women, as well as men, should be treated as rational beings. She is best known for her work "A Vindication of the Rights of Woman" (1791).
Thirteen American Colonies.
Several Americans, especially Benjamin Franklin and Thomas Jefferson, played a major role in bringing Enlightenment ideas to the new world and in influencing British and French thinkers.
The Americans closely followed English and Scottish political ideas, as well as some French thinkers such as Montesquieu. As deists, they were influenced by ideas of John Toland (1670–1722) and Matthew Tindal (1656–1733). During the Enlightenment there was a great emphasis upon liberty, democracy, republicanism and religious tolerance. Attempts to reconcile science and religion resulted in a widespread rejection of prophecy, miracle and revealed religion in preference for Deism – especially by Thomas Paine in "The Age of Reason" and by Thomas Jefferson in his short "Jefferson Bible" – from which all supernatural aspects were removed.
Benjamin Franklin was influential in England, Scotland, and the United States and France, for his political activism and for his advances in physics.
The cultural exchange during the Age of Enlightenment ran in both directions across the Atlantic. Historian Charles C. Mann points out that thinkers such as Paine, Locke, and Rousseau all take Native American cultural practices as examples of natural freedom.
Dutch Republic.
For the Dutch the Enlightenment initially sprouted during the Dutch Golden Age.
Developments during this period were to have a profound influence in the shaping of western civilization, as science, art, philosophy and economic development flourished in the Dutch Republic. Some key players in the Dutch Enlightenment were: René Descartes, originator of cogito ergo sum, Baruch Spinoza, a philosopher that wrote on pantheism and a one substance philosophy as a critique of Cartesian Dualism; Pierre Bayle, a French philosopher who advocated separation between science and religion; Eise Eisinga, an astronomer who built a planetarium; Lodewijk Meyer, a radical who claimed the Bible was obscure and doubtful; Adriaan Koerbagh, a scholar and critic of religion and conventional morality; and Burchard de Volder, a natural philosopher.
Greece.
The Greek Enlightenment was given impetus by wealthy Greek merchants in the major cities of the Ottoman Empire. The most important centers of Greek learning, schools and universities, were situated in Ioannina, Chios, Smyrna (İzmir) and Ayvalik. The transmission of Enlightenment ideas into Greek thought also influenced the development of a national consciousness. The publication of the journal "Hermes o Logios" encouraged the ideas of the Enlightenment. The journal's objective was to advance Greek science, philosophy and culture. Two of the main figures of the Greek Enlightenment, Rigas Feraios and Adamantios Korais, encouraged Greek nationalists to pursue contemporary political thought.
Italy.
Italy was changed by the Enlightenment and it influenced Italian philosophy. Enlightened thinkers often met to discuss in private salons and coffeehouses; notably in the cities of Milan, Turin and Venice. Cities with important universities such as Padua, Bologna, Naples and Rome, however, also remained great centres of scholarship and the intellect, especially Giambattista Vico (1668–1744) and Antonio Genovesi. Parts of Italian society also dramatically changed during the Enlightenment, with rulers such as Leopold II of Tuscany abolishing the death penalty in Tuscany. The Church's power was significantly reduced which led to a period of great thought and invention, with scientists such as Alessandro Volta and Luigi Galvani making new discoveries and greatly contributing to Western science. Cesare Beccaria, one of the greatest Italian Enlightenment writers, became famous for his masterpiece "Of Crimes and Punishments" (1764), which was later translated into 22 languages. Another prominent intellectual was Francesco Mario Pagano, who wrote important studies such as "Saggi Politici" (Political Essays, 1783), one of the major works of the Enlightenment in Naples, and "Considerazioni sul processo criminale" (Considerations on the criminal trial, 1787), which established him as an international authority on criminal law.
France.
In the mid-18th century, Paris became the center of an explosion of philosophic and scientific activity challenging traditional doctrines and dogmas. French historians usually place the period, called the "Siècle des Lumières" (Century of Enlightenments), between 1715 and 1789, from the beginning of the reign of Louis XV until the French Revolution. The philosophic movement was led by Voltaire and Jean-Jacques Rousseau, who argued for a society based upon reason rather than faith and Catholic doctrine, for a new civil order based on natural law, and for science based on experiments and observation. The political philosopher Montesquieu introduced the idea of a separation of powers in a government, a concept which was enthusiastically adopted by the authors of the United States Constitution. While the "Philosophes" of the French Enlightenment were not revolutionaries, and many were members of the nobility, their ideas played an important part in undermining the legitimacy of the Old Regime and shaping the French Revolution, 
Much of the scientific activity was based at the Louvre, where the French Academy of Sciences, founded in 1666, was located; it had separate sections for geometry, astronomy, mechanics, anatomy, chemistry and botany. Under Louis XVI new sections were added on physics, natural history and mineralogy. French scientists rivalled British scientists in mathematics and astronomy, and were ahead in chemistry and natural history. The biologist and natural historian Georges-Louis Leclerc, Comte de Buffon directed the Jardin des Plantes, and made it a leading center for botanic research. The mathematicians Joseph-Louis Lagrange, Jean-Charles de Borda, and Pierre-Simon Laplace; the botanist René Louiche Desfontaines, the chemists Claude Louis Berthollet, Antoine François, comte de Fourcroy and Antoine Lavoisier, all contributed to the new scientific revolution taking place in Paris. 
The new ideas and discoveries were publicized throughout Europe by book publishers in Paris. Between 1720 and 1780, the number of books about science and art published in Paris doubled, while the number of books about religion dropped to just one-tenth of the total. 
Denis Diderot and Jean le Rond d'Alembert published their "Encyclopedie" in seventeen volumes between 1751 and 1766. It provided intellectuals across Europe with a high quality survey of human knowledge. Scientists came to Paris from across Europe and from the United States to share ideas; Benjamin Franklin came in 1767 to meet with Voltaire and to talk about his experiments with electricity.
Some of the discoveries of Paris scientists, particularly in the field of chemistry, were quickly put to practical use; the experiments of Lavoisier were used to create the first modern chemical plants in Paris, and the production of hydrogen gas enabled the Montgolfier Brothers to launch the first manned flight in a hot-air balloon on 21 November 1783, from the Château de la Muette, near the Bois de Boulogne.
Poland.
The Age of Enlightenment reached Poland later than in Germany or Austria, as szlachta (nobility) culture (Sarmatism) together with the Polish-Lithuanian Commonwealth political system (Golden Freedoms) were in deep crisis. The period of Polish Enlightenment began in the 1730s–1740s, peaked in the reign of Poland's last king, Stanisław August Poniatowski (second half of the 18th century), went into decline with the Third Partition of Poland (1795), and ended in 1822, replaced by Romanticism in Poland. The model constitution of 1791 expressed Enlightenment ideals but was in effect for only one year as the nation was partitioned among its neighbors. More enduring were the cultural achievements, which created a nationalist spirit in Poland.
Prussia and the German States.
By the mid-18th century the German Enlightenment in music, philosophy, science and literature emerged as an intellectual force. Frederick the Great (1712–86), the king of Prussia 1740–1786, saw himself as a leader of the Enlightenment and patronized philosophers and scientists at his court in Berlin. He was an enthusiast for French classicism as he criticized German culture and was unaware of the remarkable advances it was undergoing. Voltaire, who had been imprisoned and maltreated by the French government, was eager to accept Frederick's invitation to live at his palace. Frederick explained, "My principal occupation is to combat ignorance and prejudice ... to enlighten minds, cultivate morality, and to make people as happy as it suits human nature, and as the means at my disposal permit." Other rulers were supportive, such as Karl Friedrich, Grand Duke of Baden, who ruled Baden for 73 years (1738–1811).
Christian Wolff (1679–1754) was the pioneer as a writer who expounded the Enlightenment to German readers; he legitimized German as a philosophic language. Johann Gottfried von Herder (1744–1803) broke new ground in philosophy and poetry, specifically in the Sturm und Drang movement of proto-Romanticism. Weimar Classicism ("Weimarer Klassik") was a cultural and literary movement based in Weimar that sought to establish a new humanism by synthesizing Romantic, classical and Enlightenment ideas. The movement, from 1772 until 1805, involved Herder as well as polymath Johann Wolfgang von Goethe (1749–1832) and Friedrich Schiller (1759–1805), a poet and historian. Herder argued that every folk had its own particular identity, which was expressed in its language and culture. This legitimized the promotion of German language and culture and helped shape the development of German nationalism. Schiller's plays expressed the restless spirit of his generation, depicting the hero's struggle against social pressures and the force of destiny.
German music, sponsored by the upper classes, came of age under composers such as Carl Philipp Emanuel Bach (1714–1788), Joseph Haydn (1732–1809), and Wolfgang Amadeus Mozart (1756–1791).
In remote Königsberg philosopher Immanuel Kant (1724–1804) tried to reconcile rationalism and religious belief, individual freedom and political authority. As well as map out a view of the public sphere through private and public reason. Kant's work contained basic tensions that would continue to shape German thought – and indeed all of European philosophy – well into the 20th century.
The German Enlightenment won the support of princes, aristocrats and the middle classes and permanently reshaped the culture.
Russia.
In Russia the Enlightenment of the mid-eighteenth century saw the government begin to actively encourage the proliferation of arts and sciences. This era produced the first Russian university, library, theatre, public museum, and independent press. Like other enlightened despots, Catherine the Great played a key role in fostering the arts, sciences, and education. She used her own interpretation of Enlightenment ideals, assisted by notable international experts such as Voltaire (by correspondence) and, in residence, world class scientists such as Leonhard Euler, Peter Simon Pallas, Fedor Ivanovich Iankovich de Mirievo (also spelled Teodor Janković-Mirijevski), and Anders Johan Lexell. The national Enlightenment differed from its Western European counterpart in that it promoted further Modernization of all aspects of Russian life and was concerned with attacking the institution of serfdom in Russia. Historians argue that the Russian enlightenment centered on the individual instead of societal enlightenment and encouraged the living of an enlightened life.
Spain.
Charles III, king of Spain from 1759 to 1788, tried to rescue his empire from decay through far-reaching reforms such as weakening the Church and its monasteries, promoting science and university research, facilitating trade and commerce, modernizing agriculture, and avoiding wars. He was unable to control budget deficits, and borrowed more and more. Spain relapsed after his death.
Historiography.
Debates.
Historian Keith Thomas says the Enlightenment has always been contested territory. He says that its supporters:
However, he adds, "its enemies accuse it of 'shallow' rationalism, naïve optimism, unrealistic universalism, and moral darkness."
Thomas points out that from the start there was a Counter-Enlightenment in which conservative and clerical defenders of traditional religion attacked materialism and skepticism as evil forces that encouraged immorality. By 1794, they pointed to the Terror during the French Revolution as confirmation of their predictions. As the Enlightenment was ending, new generations of Romantic philosophers argued that excessive dependence on reason was a mistake perpetuated by the Enlightenment, because it disregarded the powerful bonds of history, myth, faith and tradition that were necessary to hold society together.
Political thought.
Like the French Revolution, the Enlightenment has long been hailed as the foundation of modern Western political and intellectual culture. It has been frequently linked to the French Revolution of 1789. However, as Roger Chartier points out, it was perhaps the Revolution that "invented the Enlightenment by attempting to root its legitimacy in a corpus of texts and founding authors reconciled and united ... by their preparation of a rupture with the old world".
In other words, the revolutionaries elevated to heroic status those philosophers, such as Voltaire and Rousseau, who could be used to justify their radical break with the Ancien Régime. In any case, two 19th-century historians of the Enlightenment, Hippolyte Taine and Alexis de Tocqueville, did much to solidify this link of Enlightenment causing revolution and the intellectual perception of the Enlightenment itself.
An alternative view is that the "consent of the governed" philosophy as delineated by Locke in "Two Treatises of Government" (1689) represented a paradigm shift from the old governance paradigm under feudalism known as the "divine right of kings". In this view, the revolutions of the late 1700s and early 1800s were caused by the fact that this governance paradigm shift often could not be resolved peacefully, and therefore violent revolution was the result. Clearly a governance philosophy where the king was never wrong was in direct conflict with one whereby citizens by natural law had to consent to the acts and rulings of their government.
John Locke was able to root his governance philosophy in social contract theory, a predominant subject that permeated Enlightenment political thought. Formally, it was the English philosopher Thomas Hobbes who ushered in this new debate with his work "Leviathan" in 1651. Both John Locke and Jean-Jacques Rousseau developed their own social contract theories in "Two Treatises of Government" and "Discourse on Inequality", respectively. While quite different works, all three argue that a social contract is necessary for man to live in civil society.
For Hobbes, the state of nature is a state of impoverished anarchic violence in which human life is "solitary, poor, nasty, brutish, and short". To counter this, Hobbes argues that society enters into a social contract with itself to have an all-powerful, absolute leader, giving up a few personal liberties in exchange for security and lawfulness.
In 1689 John Locke published his Two Treatises of Government. In it, he defines his state of nature as a condition in which humans are rational and follow natural law; in which all men are born equal and with the right to life, liberty and property. However, when one citizen breaks the Law of Nature, both the transgressor and the victim enter into a state of war, from which it is virtually impossible to break free. Therefore, Locke argues that individuals enter into civil society to protect their natural rights via an “unbiased judge” or common authority, such as courts, to appeal to.
Contrastingly, Rousseau’s conception of both the state of nature and civil society, and how man moves from one to the other, relies on the supposition that civil man is corrupted. In his work "Discourse on Inequality", Rousseau argues natural man is a sentient being that has no want he cannot fulfil himself. Natural man is only taken out of the state of nature when “the first man who, having enclosed a piece of ground, to whom it occurred to say this is mine, and found people sufficiently simple to believe him, was the true founder of civil society". Once the inequality associated with private property is established, society is corrupted and thusly perpetuates inequality through the division of labor and, ultimately, power relations. With this in mind, Rousseau wrote "On the Social Contract" to spell out his contract theory. He argues that men join into civil society via the social contract to achieve unity while preserving individual freedom. This is embodied in the sovereignty of the general will, the moral and collective legislative body constituted by citizens.
Though much of Enlightenment political thought was dominated by social contract theorists, both David Hume and Adam Ferguson criticized this camp. In his essay, "Of the Original Contract", Hume argues that governments derived from consent are rarely seen, rather civil government is grounded in a ruler's habitual authority and force. It is precisely because of the ruler's authority over-and-against the subject, that the subject tacitly consents; Hume argues that the subjects would "never imagine that their consent made him sovereign", rather the authority did so. Similarly, Ferguson did not believe citizens built the state, rather polities grew out of social development. In his 1767 "An Essay on the History of Civil Society", Ferguson uses the four stages of progress, a theory that was very popular in Scotland at the time, to explain how humans advance from a hunting and gathering society to a commercial and civil society without "signing" a social contract.
Both Rousseau and Locke's social contract theories rest on the presupposition of natural rights. A natural right is not given to man by law or custom, rather it is something that all men have in pre-political societies, and is therefore universal and inalienable. The most famous natural right formulation comes from John Locke in his "Second Treatise", when he introduces the state of nature. As previously discussed, man is perfectly free in the state of nature, within the bounds of the law of nature and reason. For Locke the law of nature is grounded on mutual security, or the idea that one cannot infringe on another's natural rights, as every man is equal and has the same inalienable rights. These natural rights include perfect equality and freedom, and the right to preserve life and property.
Based on his formulation, John Locke argued against slavery on the basis that enslaving yourself goes against the law of nature; you cannot surrender your own rights, your freedom is absolute and no one can take it from you. Additionally, Locke argues that one person cannot enslave another because it is morally reprehensible. Locke does introduce a caveat in his indictment of slavery, he believes one can be made a slave during times of war and conflict because this is merely a continuation of the state of war. Therefore, one cannot sell oneself into slavery, but if one were to find oneself a lawful captive, ones enslavement would not go against ones natural rights.
Locke's theory of natural rights has influenced many political documents including the French National Constituent Assembly's Declaration of the Rights of Man and of the Citizen and the United States Declaration of Independence, to name a few.
In his "L’Ancien Régime" (1876), Hippolyte Taine traced the roots of the French Revolution back to French Classicism. However, this was not without the help of the Enlightenment view of the world, which wore down the "monarchical and religious dogma of the old regime". In other words, Taine was only interested in the Enlightenment insofar as it advanced scientific discourse and transmitted what he perceived to be the intellectual legacy of French classicism.
Alexis de Tocqueville painted a more elaborate picture of the Enlightenment in "L'Ancien Régime et la Révolution" (1850). For de Tocqueville, the Revolution was the inevitable result of the radical opposition created in the 18th century between the monarchy and the men of letters of the Enlightenment. These men of letters constituted a sort of "substitute aristocracy that was both all-powerful and without real power". This illusory power came from the rise of "public opinion", born when absolutist centralization removed the nobility and the bourgeoisie from the political sphere. The "literary politics" that resulted promoted a discourse of equality and was hence in fundamental opposition to the monarchical regime.
De Tocqueville "clearly designates ... the cultural effects of transformation in the forms of the exercise of power". Nevertheless, it took another century before cultural approach became central to the historiography, as typified by Robert Darnton, "The Business of Enlightenment: A Publishing History of the Encyclopédie, 1775–1800" (1979).
De Dijn argues that Peter Gay, in "The Enlightenment: An Interpretation" (1966), first formulated the interpretation that the Enlightenment brought political modernization to the West, in terms of introducing democratic values and institutions and the creation of modern, liberal democracies. While the thesis has many critics it has been widely accepted by Anglophone scholars and has been reinforced by the large-scale studies by Robert Darnton, Roy Porter and most recently by Jonathan Israel.
Religious debate.
Enlightenment era religious commentary was a response to the preceding century of religious conflict in Europe, especially the Thirty Years' War. Theologians of the Enlightenment wanted to reform their faith to its generally non-confrontational roots and to limit the capacity for religious controversy to spill over into politics and warfare while still maintaining a true faith in God.
For moderate Christians, this meant a return to simple Scripture. John Locke abandoned the corpus of theological commentary in favor of an "unprejudiced examination" of the Word of God alone. He determined the essence of Christianity to be a belief in Christ the redeemer and recommended avoiding more detailed debate. Thomas Jefferson in the "Jefferson Bible" went further; he dropped any passages dealing with miracles, visitations of angels, and the resurrection of Jesus after his death. He tried to extract the practical Christian moral code of the New Testament.
Enlightenment scholars sought to curtail the political power of organized religion and thereby prevent another age of intolerant religious war. Spinoza determined to remove politics from contemporary and historical theology (e.g. disregarding Judaic law). Moses Mendelssohn advised affording no political weight to any organized religion, but instead recommended that each person follow what s/he found most convincing. A good religion based in instinctive morals and a belief in God should not theoretically need force to maintain order in its believers, and both Mendelssohn and Spinoza judged religion on its moral fruits, not the logic of its theology.
A number of novel religious ideas developed with Enlightened faith, including Deism and talk of atheism. Deism, according to Thomas Paine, is the simple belief in God the Creator, with no reference to the Bible or any other miraculous source. Instead, the Deist relies solely on personal reason to guide his creed, which was eminently agreeable to many thinkers of the time.
Atheism was much discussed but there were few proponents. Wilson and Reill note that, "In fact, very few enlightened intellectuals, even when they were vocal critics of Christianity, were true atheists. Rather, they were critics of orthodox belief, wedded rather to skepticism, deism, vitalism, or perhaps pantheism."
Some followed Pierre Bayle and argued that atheists could indeed be moral men. Many others like Voltaire held that without belief in a God who punishes evil, the moral order of society was undermined. That is, since atheists gave themselves to no Supreme Authority and no law, and had no fear of eternal consequences, they were far more likely to disrupt society. Bayle (1647–1706) observed that in his day, "prudent persons will always maintain an appearance of [religion].". He believed that even atheists could hold concepts of honor and go beyond their own self-interest to create and interact in society. Locke considered the consequences for mankind if there were no God and no divine law. The result would be moral anarchy. Every individual “could have no law but his own will, no end but himself. He would be a god to himself, and the satisfaction of his own will the sole measure and end of all his actions”.
Intellectual history.
In the meantime, though, intellectual history remained the dominant historiographical trend. The German scholar Ernst Cassirer is typical, writing in his "The Philosophy of the Enlightenment" (1932) that the Enlightenment was "a part and a special phase of that whole intellectual development through which modern philosophic thought gained its characteristic self-confidence and self-consciousness". Borrowing from Kant, Cassirer states that Enlightenment is the process by which the spirit "achieves clarity and depth in its understanding of its own nature and destiny, and of its own fundamental character and mission". In short, the Enlightenment was a series of philosophical, scientific and otherwise intellectual developments that took place mostly in the 18th century – the birthplace of intellectual modernity.
Recent work.
Only in the 1970s did interpretation of the Enlightenment allow for a more heterogeneous and even extra-European vision. A. Owen Aldridge demonstrated how Enlightenment ideas spread to Spanish colonies and how they interacted with indigenous cultures, while Franco Venturi explored how the Enlightenment took place in normally unstudied areas – Italy, Greece, the Balkans, Poland, Hungary, and Russia.
Robert Darnton's cultural approach launched a new dimension of studies. He said, :
"Perhaps the Enlightenment was a more down-to-earth affair than the rarefied climate of opinion described by textbook writers, and we should question the overly highbrow, overly metaphysical view of intellectual life in the eighteenth century."
Darnton examines the underbelly of the French book industry in the 18th century, examining the world of book smuggling and the lives of those writers (the "Grub Street Hacks") who never met the success of their "philosophe" cousins. In short, rather than concerning himself with Enlightenment canon, Darnton studies "what Frenchmen wanted to read", and who wrote, published and distributed it.
Similarly, in "The Business of Enlightenment. A Publishing History of the Encyclopédie 1775–1800", Darnton states that there is no need to further study the encyclopædia itself, as "the book has been analyzed and anthologized dozen of times: to recapitulate all the studies of its intellectual content would be redundant". He instead, as the title of the book suggests, examines the social conditions that brought about the production of the "Encyclopédie". This is representative of the social interpretation as a whole – an examination of the social conditions that brought about Enlightenment ideas rather than a study of the ideas themselves.
The work of German philosopher Jürgen Habermas was central to this emerging social interpretation; his seminal work "The Structural Transformation of the Public Sphere" (published under the title "Strukturwandel der Öffentlichkeit" in 1962) was translated into English in 1989. The book outlines the creation of the "bourgeois public sphere" in 18th-century Europe. Essentially, this public sphere describes the new venues and modes of communication allowing for rational exchange that appeared in the 18th century. Habermas argued that the public sphere was bourgeois, egalitarian, rational, and independent from the state, making it the ideal venue for intellectuals to critically examine contemporary politics and society, away from the interference of established authority.
Habermas's work, though influential, has come under criticism on all fronts. While the public sphere is generally an integral component of social interpretations of the Enlightenment, numerous historians have brought into question whether the public sphere was bourgeois, oppositional to the state, independent from the state, or egalitarian.
These historiographical developments have done much to open up the study of Enlightenment to a multiplicity of interpretations. In "A Social History of Truth" (1994), for example, Steven Shapin makes the largely sociological argument that, in 17th-century England, the mode of sociability known as civility became the primary discourse of truth; for a statement to have the potential to be considered true, it had to be expressed according to the rules of civil society.
According to Jonathan Israel, this period saw the shaping of two distinct lines of enlightenment thought: Firstly the "radical enlightenment", largely inspired by the one-substance philosophy of Spinoza, which in its political form adhered to: "democracy; racial and sexual equality; individual liberty of lifestyle; full freedom of thought, expression, and the press; eradication of religious authority from the legislative process and education; and full separation of church and state".
Secondly the "moderate enlightenment", which in a number of different philosophical systems, like those in the writings of Descartes, John Locke, Isaac Newton or Christian Wolff, expressed some support for critical review and renewal of the old modes of thought, but in other parts sought reform and accommodation with the old systems of power and faith. These two lines of thought were again met by the conservative Counter-Enlightenment, encompassing those thinkers who held on to the traditional belief-based systems of thought.
Feminist interpretations have also appeared, with Dena Goodman being one notable example. In "The Republic of Letters: A Cultural History of the French Enlightenment" (1994), Goodman argues that many women in fact played an essential part in the French Enlightenment, due to the role they played as "salonnières" in Parisians salons. These salons "became the civil working spaces of the project of Enlightenment" and women, as salonnières, were "the legitimate governors of [the] potentially unruly discourse" that took place within. On the other hand, Carla Hesse, in "The Other Enlightenment: How French Women Became Modern" (2001), argues that "female participation in the public cultural life of the Old Regime was ... relatively marginal". It was instead the French Revolution, by destroying the old cultural and economic restraints of patronage and corporatism (guilds), that opened French society to female participation, particularly in the literary sphere.
Social and cultural interpretation.
In opposition to the intellectual historiographical approach of the Enlightenment, which examines the various currents or discourses of intellectual thought within the European context during the 17th and 18th centuries, the cultural (or social) approach examines the changes that occurred in European society and culture. Under this approach, the Enlightenment is less a body of thought than a process of changing sociabilities and cultural practices – both the "content" and the processes by which this content was spread are now important. Roger Chartier describes it as follows:
This movement [from the intellectual to the cultural/social] implies casting doubt on two ideas: first, that practices can be deduced from the discourses that authorize or justify them; second, that it is possible to translate into the terms of an explicit ideology the latent meaning of social mechanisms.
One of the primary elements of the cultural interpretation of the Enlightenment is the rise of the public sphere in Europe. Jürgen Habermas has influenced thinking on the public sphere more than any other, though his model is increasingly called into question. The essential problem that Habermas attempted to answer concerned the conditions necessary for "rational, critical, and genuinely open discussion of public issues". Or, more simply, the social conditions required for Enlightenment ideas to be spread and discussed. His response was the formation in the late 17th century and 18th century of the "bourgeois public sphere", a "realm of communication marked by new arenas of debate, more open and accessible forms of urban public space and sociability, and an explosion of print culture". More specifically, Habermas highlights three essential elements of the public sphere:
James Van Horn Melton provides a good summary of the values of this bourgeois public sphere: its members held reason to be supreme; everything was open to criticism (the public sphere is critical); and its participants opposed secrecy of all sorts. This helps explain what Habermas meant by the domain of "common concern". Habermas uses the term to describe those areas of political/social knowledge and discussion that were previously the exclusive territory of the state and religious authorities, now open to critical examination by the public sphere.
Habermas credits the creation of the bourgeois public sphere to two long-term historical trends: the rise of the modern nation state and the rise of capitalism. The modern nation state in its consolidation of public power created by counterpoint a private realm of society independent of the state – allowing for the public sphere. Capitalism also increased society's autonomy and self-awareness, and an increasing need for the exchange of information. As the nascent public sphere expanded, it embraced a large variety of institutions; the most commonly cited were coffee houses and cafés, salons and the literary public sphere, figuratively localized in the Republic of Letters.
Dorinda Outram further describes the rise of the public sphere. The context was the economic and social change commonly associated with the Industrial Revolution: "economic expansion, increasing urbanization, rising population and improving communications in comparison to the stagnation of the previous century"." Rising efficiency in production techniques and communication lowered the prices of consumer goods at the same time as it increased the amount and variety of goods available to consumers (including the literature essential to the public sphere). Meanwhile, the colonial experience (most European states had colonial Empires in the 18th century) began to expose European society to extremely heterogeneous cultures. Outram writes that the end result was the breaking down of "barriers between cultural systems, religious divides, gender differences and geographical areas". In short, the social context was set for the public sphere to come into existence.
A reductionist view of the Habermasian model has been used as a springboard to showcase historical investigations into the development of the public sphere. There are many examples of noble and lower class participation in areas such as the coffeehouses and the freemasonic lodges, demonstrating that the bourgeois-era public sphere was enriched by cross-class influences. A rough depiction of the public sphere as independent and critical of the state is contradicted by the diverse cases of government-sponsored public institutions and government participation in debate, along with the cases of private individuals using public venues to promote the status quo.
Exclusivity of the public sphere.
The word "public" implies the highest level of inclusivity – the public sphere by definition should be open to all. However, as the analysis of many "public" institutions of the Enlightenment will show, this sphere was only public to relative degrees. Indeed, as Roger Chartier emphasizes, Enlightenment thinkers frequently contrasted their conception of the "public" with that of the people: Chartier cites Condorcet, who contrasted "opinion" with populace; Marmontel with "the opinion of men of letters" versus "the opinion of the multitude"; and d'Alembert, who contrasted the "truly enlightened public" with "the blind and noisy multitude". In France the aristocracy played a central role in the public sphere when it moved from the King's palace at Versailles to Paris about 1720. Their rich spending stimulated the trade in luxuries and artistic creations, especially fine paintings.
As Mona Ozouf underlines, public opinion was defined in opposition to the opinion of the greater population. While the nature of public opinion during the Enlightenment is as difficult to define as it is today, it is nonetheless clear that the body that held it (i.e. the public sphere) was exclusive rather than inclusive. This observation will become more apparent during the descriptions of the institutions of the public sphere, most of which excluded both women and the lower classes.
Social and cultural implications in music.
Because of the focus on reason over superstition, the Enlightenment cultivated the arts. Emphasis on learning, art and music became more widespread, especially with the growing middle class. Areas of study such as literature, philosophy, science, and the fine arts increasingly explored subject matter that the general public in addition to the previously more segregated professionals and patrons could relate to.
As musicians depended more and more on public support, public concerts became increasingly popular and helped supplement performers' and composers' incomes. The concerts also helped them to reach a wider audience. Handel, for example, epitomized this with his highly public musical activities in London. He gained considerable fame there with performances of his operas and oratorios. The music of Haydn and Mozart, with their Viennese Classical styles, are usually regarded as being the most in line with the Enlightenment ideals.
Another important text that came about as a result of Enlightenment values was Charles Burney's "A General History of Music: From the Earliest Ages to the Present Period", originally published in 1776. This text was a historical survey and an attempt to rationalize elements in music systematically over time.
As the economy and the middle class expanded, there was an increasing number of amateur musicians. One manifestation of this involved women, who became more involved with music on a social level. Women were already engaged in professional roles as singers, and increased their presence in the amateur performers' scene, especially with keyboard music.
The desire to explore, record and systematize knowledge had a meaningful impact on music publications. Jean-Jacques Rousseau's "Dictionnaire de musique" (published 1767 in Geneva and 1768 in Paris) was a leading text in the late 18th century. This widely available dictionary gave short definitions of words like genius and taste, and was clearly influenced by the Enlightenment movement. Additionally, music publishers began to cater to amateur musicians, putting out music that they could understand and play. The majority of the works that were published were for keyboard, voice and keyboard, and chamber ensemble.
After these initial genres were popularized, from the mid-century on, amateur groups sang choral music, which then became a new trend for publishers to capitalize on. The increasing study of the fine arts, as well as access to amateur-friendly published works, led to more people becoming interested in reading and discussing music. Music magazines, reviews, and critical works which suited amateurs as well as connoisseurs began to surface.
Although the ideals of the Enlightenment were rejected in postmodernism, they held fast in modernism and have extended well beyond the 18th century even to the present. Recently, musicologists have shown renewed interest in the ideas and consequences of the Enlightenment. For example, Rose Rosengard Subotnik's "Deconstructive Variations" (subtitled "Music and Reason in Western Society") compares Mozart's "Die Zauberflöte" (1791) using the Enlightenment and Romantic perspectives, and concludes that the work is "an ideal musical representation of the Enlightenment".
Separation of church and state.
According to Jonathan Israel, this period saw the shaping of the "Radical Enlightenment", which promoted the concept of separating church and state. A concept that is often credited to the writings of English philosopher John Locke (1632–1704). According to his principle of the social contract, Locke argued that the government lacked authority in the realm of individual conscience, as this was something rational people could not cede to the government for it or others to control. For Locke, this created a natural right in the liberty of conscience, which he argued must therefore remain protected from any government authority.
These views on religious tolerance and the importance of individual conscience, along with his social contract, became particularly influential in the American colonies and the drafting of the United States Constitution. In which Thomas Jefferson called for a wall of separation between church and state at the federal level. He previously had supported successful efforts to disestablish the Church of England in Virginia, and authored the Virginia Statute for Religious Freedom. Thomas Jefferson's political ideals were greatly influenced by the writings of John Locke, Francis Bacon, and Isaac Newton whom he considered the three greatest men that ever lived.
Dissemination of ideas.
The "philosophes" spent a great deal of energy disseminating their ideas among educated men and women in cosmopolitan cities. They used many venues, some of them quite new.
The Republic of Letters.
The term "Republic of Letters" was coined by Pierre Bayle in 1664, in his journal "Nouvelles de la Republique des Lettres". Towards the end of the 18th century, the editor of "Histoire de la République des Lettres en France", a literary survey, described the Republic of Letters as being:
In the midst of all the governments that decide the fate of men; in the bosom of so many states, the majority of them despotic ... there exists a certain realm which holds sway only over the mind ... that we honour with the name Republic, because it preserves a measure of independence, and because it is almost its essence to be free. It is the realm of talent and of thought.
The ideal of the Republic of Letters was the sum of a number of Enlightenment ideals: an egalitarian realm governed by knowledge that could act across political boundaries and rival state power. It was a forum that supported "free public examination of questions regarding religion or legislation". Immanuel Kant considered written communication essential to his conception of the public sphere; once everyone was a part of the "reading public", then society could be said to be enlightened. The people who participated in the Republic of Letters, such as Diderot and Voltaire, are frequently known today as important Enlightenment figures. Indeed, the men who wrote Diderot's "Encyclopédie" arguably formed a microcosm of the larger "republic".
Dena Goodman has argued that women played a major role in French salons – "salonnières" to complement the male "philosophes". Discursively, she bases the Republic of Letters in polite conversation and letter writing; its principal social institution was the salon.
Robert Darnton's "The Literary Underground of the Old Regime" was the first major historical work to critique this ideal model. He argues that, by the mid-18th century, the established men of letters ("gens de lettres") had fused with the elites ("les grands") of French society. Consider the definition of "Goût" (taste) as written by Voltaire in the "Dictionnaire philosophique" (taken from Darnton): "Taste is like philosophy. It belongs to a very small number of privileged souls ... It is unknown in bourgeois families, where one is constantly occupied with the care of one's fortune". In the words of Darnton, Voltaire "thought that the Enlightenment should begin with the "grands"". The historian cites similar opinions from d'Alembert and Louis Sébastien Mercier.
Grub Street.
Darnton argues that the result of this "fusion of "gens de lettres" and "grands"" was the creation of an oppositional literary sphere, Grub Street, the domain of a "multitude of versifiers and would-be authors". These men, lured by the glory of the Republic of Letters, came to London to become authors, only to discover that their dreams of literary success were little more than chimeras. The literary market simply could not support large numbers of writers, who, in any case, were very poorly remunerated by the publishing-bookselling guilds. The writers of Grub Street, the Grub Street Hacks, were left feeling extremely bitter about the relative success of their literary cousins, the men of letters.
This bitterness and hatred found an outlet in the literature the Grub Street Hacks produced, typified by the "libelle". Written mostly in the form of pamphlets, the "libelles" "slandered the court, the Church, the aristocracy, the academies, the salons, everything elevated and respectable, including the monarchy itself". Darnton designates "Le Gazetier cuirassé" by Charles Théveneau de Morande as the prototype of the genre. Consider:
The devout wife of a certain Maréchal de France (who suffers from an imaginary lung disease), finding a husband of that species too delicate, considers it her religious duty to spare him and so condemns herself to the crude caresses of her butler, who would still be a lackey if he hadn't proven himself so robust.
or,
The public is warned that an epidemic disease is raging among the girls of the Opera, that it has begun to reach the ladies of the court, and that it has even been communicated to their lackeys. This disease elongates the face, destroys the complexion, reduces the weight, and causes horrible ravages where it becomes situated. There are ladies without teeth, others without eyebrows, and some are completely paralyzed.
It was Grub Street literature that was most read by the reading public during the Enlightenment. More importantly, Darnton argues, the Grub Street hacks inherited the "revolutionary spirit" once displayed by the "philosophes", and paved the way for the Revolution by desacralizing figures of political, moral and religious authority in France.
The book industry.
The increased consumption of reading materials of all sorts was one of the key features of the "social" Enlightenment. Developments in the Industrial Revolution allowed consumer goods to be produced in greater quantities at lower prices, encouraging the spread of books, pamphlets, newspapers and journals – "media of the transmission of ideas and attitudes". Commercial development likewise increased the demand for information, along with rising populations and increased urbanisation. However, demand for reading material extended outside of the realm of the commercial, and outside the realm of the upper and middle classes, as evidenced by the Bibliothèque Bleue. Literacy rates are difficult to gauge, but Robert Darnton writes that, in France at least, the rates doubled over the course of the 18th century.
Reading underwent serious changes in the 18th century. In particular, Rolf Engelsing has argued for the existence of a "Reading Revolution". Until 1750, reading was done "intensively: people tended to own a small number of books and read them repeatedly, often to small audience. After 1750, people began to read "extensively", finding as many books as they could, increasingly reading them alone. This is supported by increasing literacy rates, particularly among women.
Of course, the vast majority of the reading public could not afford to own a private library. And while most of the state-run "universal libraries" set up in the 17th and 18th centuries were open to the public, they were not the only sources of reading material.
On one end of the spectrum was the "Bibliothèque Bleue", a collection of cheaply produced books published in Troyes, France. Intended for a largely rural and semi-literate audience these books included almanacs, retellings of medieval romances and condensed versions of popular novels, among other things. While historians, such as Roger Chartier and Robert Darnton, have argued against the Enlightenment's penetration into the lower classes, the Bibliothèque Bleue, at the very least, represents a desire to participate in Enlightenment sociability, whether or not this was actually achieved.
Moving up the classes, a variety of institutions offered readers access to material without needing to buy anything. Libraries that lent out their material for a small price started to appear, and occasionally bookstores would offer a small lending library to their patrons. Coffee houses commonly offered books, journals and sometimes even popular novels to their customers. "The Tatler" and "The Spectator", two influential periodicals sold from 1709 to 1714, were closely associated with coffee house culture in London, being both read and produced in various establishments in the city. Indeed, this is an example of the triple or even quadruple function of the coffee house: reading material was often obtained, read, discussed and even produced on the premises.
As Darnton describes in "The Literary Underground of the Old Regime", it is extremely difficult to determine what people actually read during the Enlightenment. For example, examining the catalogs of private libraries not only gives an image skewed in favor of the classes wealthy enough to afford libraries, it also ignores censured works unlikely to be publicly acknowledged. For this reason, Darnton argues that a study of publishing would be much more fruitful for discerning reading habits.
All across continental Europe, but in France especially, booksellers and publishers had to negotiate censorship laws of varying strictness. The "Encyclopédie", for example, narrowly escaped seizure and had to be saved by Malesherbes, the man in charge of the French censure. Indeed, many publishing companies were conveniently located outside of France so as to avoid overzealous French censors. They would smuggle their merchandise – both pirated copies and censured works – across the border, where it would then be transported to clandestine booksellers or small-time peddlers.
Darnton provides a detailed record of one clandestine bookseller's (one de Mauvelain) business in the town of Troyes. At the time, the town's population was 22,000. It had one masonic lodge and an "important" library, even though the literacy rate seems to have been less than 50 percent. Mauvelain's records give us a good representation of what literate Frenchmen might have truly read, since the clandestine nature of his business provided a less restrictive product choice. The most popular category of books was political (319 copies ordered).
This included five copies of D'Holbach's "Système social", but around 300 libels and pamphlets. Readers were far more interested in sensationalist stories about criminals and political corruption than they were in political theory itself. The second most popular category, "general works" (those books "that did not have a dominant motif and that contained something to offend almost everyone in authority") likewise betrayed the high demand for generally low-brow subversive literature. These works, however, like the vast majority of work produced by Darnton's "grub street hacks", never became part of literary canon, and are largely forgotten today as a result.
Nevertheless, the Enlightenment was not the exclusive domain of illegal literature, as evidenced by the healthy, and mostly legal, publishing industry that existed throughout Europe. "Mostly legal" because even established publishers and book sellers occasionally ran afoul of the law. The Encyclopédie, for example, condemned not only by the King but also by Clement XII, nevertheless found its way into print with the help of the aforementioned Malesherbes and creative use of French censorship law.
But many works were sold without running into any legal trouble at all. Borrowing records from libraries in England, Germany and North America indicate that more than 70 percent of books borrowed were novels; that less than 1 percent of the books were of a religious nature supports a general trend of declining religiosity.
Natural history.
A genre that greatly rose in importance was that of scientific literature. Natural history in particular became increasingly popular among the upper classes. Works of natural history include René-Antoine Ferchault de Réaumur's "Histoire naturelle des insectes" and Jacques Gautier d'Agoty's "La Myologie complète, ou description de tous les muscles du corps humain" (1746). However, as François-Alexandre Aubert de La Chesnaye des Bois's "Dictionnaire de la Noblesse" (1770) indicates, natural history was very often a political affair. As E. C. Spary writes, the classifications used by naturalists "slipped between the natural world and the social ... to establish not only the expertise of the naturalists over the natural, but also the dominance of the natural over the social". From this basis, naturalists could then develop their own social ideals based on their scientific works.
The target audience of natural history was French polite society, evidenced more by the specific discourse of the genre than by the generally high prices of its works. Naturalists catered to polite society's desire for erudition – many texts had an explicit instructive purpose. But the idea of taste ("le goût") was the real social indicator: to truly be able to categorize nature, one had to have the proper taste, an ability of discretion shared by all members of polite society. In this way natural history spread many of the scientific developments of the time, but also provided a new source of legitimacy for the dominant class.
Outside ancien régime France, natural history was an important part of medicine and industry, encompassing the fields of botany, zoology, meteorology, hydrology and mineralogy. Students in Enlightenment universities and academies were taught these subjects to prepare them for careers as diverse as medicine and theology. As shown by M D Eddy, natural history in this context was a very middle class pursuit and operated as a fertile trading zone for the interdisciplinary exchange of diverse scientific ideas.
Scientific and literary journals.
The many scientific and literary journals (predominantly composed of book reviews) that were published during this time are also evidence of the intellectual side of the Enlightenment. In fact, Jonathan Israel argues that the learned journals, from the 1680s onwards, influenced European intellectual culture to a greater degree than any other "cultural innovation".
The first journal appeared in 1665– the Parisian "Journal des Sçavans" – but it was not until 1682 that periodicals began to be more widely produced. French and Latin were the dominant languages of publication, but there was also a steady demand for material in German and Dutch. There was generally low demand for English publications on the Continent, which was echoed by England's similar lack of desire for French works. Languages commanding less of an international market – such as Danish, Spanish and Portuguese – found journal success more difficult, and more often than not, a more international language was used instead. Although German did have an international quality to it, it was French that slowly took over Latin's status as the "lingua franca" of learned circles. This in turn gave precedence to the publishing industry in Holland, where the vast majority of these French language periodicals were produced.
Israel divides the journals' intellectual importance into four elements. First was their role in shifting the attention of the "cultivated public" away from "established authorities" to "what was new, innovative, or challenging." Secondly, they did much to promote the "'enlightened' ideals of toleration and intellectual objectivity." Thirdly, the journals were an implicit critique of existing notions of universal truth monopolized by monarchies, parliaments, and religious authorities. The journals suggested a new source of knowledge – through science and reason – that undermined these sources of authority. And finally, they advanced Christian enlightenment that upheld "the legitimacy of God-ordained authority"—the Bible—in which there had to be agreement between the biblical and natural theories.
Schools and universities.
Most work on the Enlightenment tends to emphasise what intellectuals wrote about what education should be and not about what education actually was during the seventeenth and eighteenth centuries. Leading educational theorists like England's John Locke and Switzerland's Jean Jacques Rousseau both emphasised the importance of shaping young minds early. By the late Enlightenment there was a rising demand for a more universal approach to education, particularly after the American and French Revolutions.
Enlightenment children were taught to memorise facts through oral and graphic methods that originated during the Renaissance. The predominant educational psychology from the 1750s onward, especially in northern European countries was associationism, the notion that the mind associates or dissociates ideas through repeated routines. In addition to being conducive to Enlightenment ideologies of liberty, self-determination and personal responsibility, it offered a practical theory of the mind that allowed teachers to transform longstanding forms of print and manuscript culture into effective graphic tools of learning for the lower and middle orders of society.
Many of the leading universities associated Enlightenment progressive principles were located in northern Europe, with the most renowned being the universities of Leiden, Göttingen, Halle, Montpellier, Uppsala and Edinburgh. These universities, especially Edinburgh, produced professors whose ideas had a significant impact on Britain's North American colonies and, later, the American Republic. Within the natural sciences Edinburgh's medical also led the way in chemistry, anatomy and pharmacology.
However, in general the universities and schools of France and most of Europe were bastions of traditionalism and were not hospitable to the Enlightenment. In France the major exception was the medical university at Montpellier.
Learned academies.
The history of Academies in France during the Enlightenment begins with the Academy of Science, founded in 1635 in Paris. It was closely tied to the French state, acting as an extension of a government seriously lacking in scientists. It helped promote and organize new disciplines, and it trained new scientists. It also contributed to the enhancement of scientists' social status, and considered them to be the "most useful of all citizens". Academies demonstrate the rising interest in science along with its increasing secularization, as evidenced by the small number of clerics who were members (13 percent).
In the first flush of scientific confidence, the thinkers of the Enlightenment tried to carry over into every human intellectual endeavour the search for first principles which, in Newton's physics, had been attended with such success. This search brought with it a sceptical attitude towards authority, rejecting everything that had no secure foundation in experience. In history, morals, metaphysics and literature the Enlightenment attitude briefly prevailed, giving rise to the phenomenal ambitions of the French encyclopaedists, and to their materialist, almost clockwork, vision of the universe. It produced the political theories which motivated the French and American revolutions, and the systematic explorations in chemistry and biology that were to find fruition in nineteenth-century evolutionism. It also brought about the technical achievements which precipitated modern industrialism, and while thus preparing the way for the miseries of revolution and factory labour, it infected the minds of the educated classes with a serenity of outlook, and a trust in human capacities, that weathered the assaults of Hume's scepticism, of Vice's anti-rationalism, of the growing introversion and doom-laden mysticism of the romantics. This was the Augustan age of English poetry, the age of Johnson and Goldsmith, of Voltaire, Diderot and Rousseau, of Lessing and Winckelmann. From the point of view of the historian it is perhaps the richest and most exciting of all intellectual eras, not because of the content, but because of the influence, of the ideas that were current in it."
”
"A Short History of Modern Philosophy"
The presence of the French academies in the public sphere cannot be attributed to their membership; although the majority of their members were bourgeois, the exclusive institution was only open to elite Parisian scholars. They did perceive themselves to be "interpreters of the sciences for the people". Indeed, it was with this in mind that academians took it upon themselves to disprove the popular pseudo-science of mesmerism.
However, the strongest case for the French Academies' being part of the public sphere comes from the concours académiques (roughly translated as 'academic contests') they sponsored throughout France. As Jeremy L. Caradonna argues in a recent article in the "Annales", "Prendre part au siècle des Lumières: Le concours académique et la culture intellectuelle au XVIIIe siècle", these academic contests were perhaps the most public of any institution during the Enlightenment.
"L'Académie française" revived a practice dating back to the Middle Ages when it revived public contests in the mid-17th century. The subject matter was generally religious and/or monarchical, and featured essays, poetry, and painting. By roughly 1725, however, this subject matter had radically expanded and diversified, including "royal propaganda, philosophical battles, and critical ruminations on the social and political institutions of the Old Regime." Controversial topics were not always avoided: Caradonna cites as examples the theories of Newton and Descartes, the slave trade, women's education, and justice in France.
More importantly, the contests were open to all, and the enforced anonymity of each submission guaranteed that neither gender nor social rank would determine the judging. Indeed, although the "vast majority" of participants belonged to the wealthier strata of society ("the liberal arts, the clergy, the judiciary, and the medical profession"), there were some cases of the popular classes submitting essays, and even winning.
Similarly, a significant number of women participated – and won – the competitions. Of a total of 2300 prize competitions offered in France, women won 49 – perhaps a small number by modern standards, but very significant in an age in which most women did not have any academic training. Indeed, the majority of the winning entries were for poetry competitions, a genre commonly stressed in women's education.
In England, the Royal Society of London also played a significant role in the public sphere and the spread of Enlightenment ideas. It was given a royal charter in 1662 by the King of England and was founded by a group of independent Scientists. In particular, the Society played a large role in spreading Robert Boyle's experimental philosophy around Europe, and acted as a clearinghouse for intellectual correspondence and exchange. As Steven Shapin and Simon Schaffer have argued, Robert Boyle was "a founder of the experimental world in which scientists now live and operate". Boyle's method based knowledge on experimentation, which had to be witnessed to provide proper empirical legitimacy. This is where the Royal Society came into play: witnessing had to be a "collective act", and the Royal Society's assembly rooms were ideal locations for relatively public demonstrations.
However, not just any witness was considered to be credible; "Oxford professors were accounted more reliable witnesses than Oxfordshire peasants." Two factors were taken into account: a witness's knowledge in the area; and a witness's "moral constitution". In other words, only civil society were considered for Boyle's public.
Coffeehouses.
Coffeehouses were especially important to the spread of knowledge during the Enlightenment because they created a unique environment in which people from many different walks of life gathered and shared ideas. Coffeehouse culture was frequently criticized by nobles who feared and abhorred the possibility of an environment in which class and its accompanying titles and privileges were disregarded. Such an environment was especially intimidating to monarchs who derived much of their power from the disparity between classes of people. If classes were to join together under the influence of Enlightenment thinking, they might recognize the all-encompassing oppression and abuses of their monarchs and, because of their size, might be able to carry out successful revolts. Monarchs also resented the idea of their subjects convening as one to discuss political matters—especially those concerning foreign affairs—for rulers thought political affairs to be their business only, a result of their supposed divine right to rule.
The first English coffeehouse opened in Oxford in 1650. Historian Brian Cowan argues that Oxford coffeehouses developed into "penny universities", offering a locus of learning that was less formal than structured institutions. These penny universities occupied a significant position in Oxford academic life, as they were frequented by those consequently referred to as the "virtuosi", who conducted their research on some of the resulting premises. According to Cowan, "the coffeehouse was a place for like-minded scholars to congregate, to read, as well as learn from and to debate with each other, but was emphatically not a university institution, and the discourse there was of a far different order than any university tutorial."
Although many coffeehouse patrons were scholars, a great deal were not. Coffeehouse culture attracted a diverse set of people including not only the educated wealthy but also more ignorant members of the bourgeoisie and even the lower class. While it may seem positive that patrons, being doctors, lawyers, merchants, etc. represented almost all classes, the coffeeshop environment sparked fear in those who sought to preserve class distinction. According to historian Lawrence E. Klein, one of the most popular critiques of the coffeehouse claimed that it "allowed promiscuous association among people from different rungs of the social ladder, from the artisan to the aristocrat" and was therefore compared to Noah's Ark, receiving all types of animals, clean or unclean.
This unique culture served as a catalyst for journalism when Joseph Addison and Richard Steele Steele recognized its potential as an audience. Together, Steele and Addison published "The Spectator (1711)", a daily publication which aimed, through fictional narrator Mr. Spectator, both to entertain and to provoke discussion regarding serious philosophical matters. Steele alone published "The Tatler", a British literary and society journal that discussed, in the first person, news and gossip overheard in popular coffeehouses.
Francesco Procopio dei Coltelli – François Procope – established the first café in Paris, the Café Procope, in 1686; by the 1720s there were around 400 cafés in the city. The Café Procope in particular became a center of Enlightenment, welcoming such celebrities as Voltaire and Rousseau. The Café Procope was where Diderot and D'Alembert decided to create the "Encyclopédie". Robert Darnton in particular has studied Parisian café conversation in great detail. He describes how the cafés were one of the various "nerve centers" for "bruits publics", public noise or rumour. These "bruits" were allegedly a much better source of information than were the actual newspapers available at the time.
Moreover, coffeehouses represent a turning point in history during which people discovered that they could have enjoyable social lives within their communities. Coffeeshops became homes away from home for many who sought, for the first time, to engage in discourse with their neighbors and discuss intriguing and thought-provoking matters, especially those regarding philosophy to politics. Coffeehouses were essential to the Enlightenment, for they were centers of free-thinking and self-discovery.
Debating societies.
The Debating Societies that rapidly came into existence in 1780 London present an almost perfect example of the public sphere during the Enlightenment. Donna T Andrew provides four separate origins:
In any event, popular debating societies began, in the late 1770s, to move into more "genteel", or respectable rooms, a change which helped establish a new standard of sociability: "order, decency, and liberality", in the words of the Religious Society of Old Portugal Street. Respectability was also encouraged by the higher admissions prices (ranging from 6d. to 3s.), which also contributed to the upkeep of the newer establishments. The backdrop to these developments was what Andrew calls "an explosion of interest in the theory and practice of public elocution". The debating societies were commercial enterprises that responded to this demand, sometimes very successfully. Indeed, some societies welcomed from 800 to 1200 spectators a night.
These societies discussed an extremely wide range of topics. One broad area was women: societies debated over "male and female qualities", courtship, marriage, and the role of women in the public sphere. Societies also discussed political issues, varying from recent events to "the nature and limits of political authority", and the nature of suffrage. Debates on religion rounded out the subject matter. It is important to note, however, that the critical subject matter of these debates did not necessarily translate into opposition to the government. In other words, the results of the debate quite frequently upheld the status quo.
From a historical standpoint, one of the most important features of the debating society was their openness to the public; women attended and even participated in almost every debating society, which were likewise open to all classes providing they could pay the entrance fee. Once inside, spectators were able to participate in a largely egalitarian form of sociability that helped spread "Enlightening ideas".
Masonic lodges.
Historians have long debated the extent to which the secret network of Freemasonry was a main factor in the Enlightenment. Historians agree that the famous leaders of the Enlightenment included Freemasons such as Diderot, Montesquieu, Voltaire, Pope, Horace Walpole, Sir Robert Walpole, Mozart, Goethe, Frederick the Great, Benjamin Franklin, and George Washington. In long-term historical perspective, historian Norman Davies has argued that Freemasonry was a powerful force on behalf of Liberalism in Europe, from about 1700 to the twentieth century. It expanded rapidly during the Age of Enlightenment, reaching practically every country in Europe. It was especially attractive to powerful aristocrats and politicians as well as intellectuals, artists and political activists.
During the Age of Enlightenment, American historian Margaret Jacob argues, Freemasons comprised an international network of like-minded men, often meeting in secret in ritualistic programs at their lodges. they promoted the ideals of the Enlightenment, and helped diffuse these values across Britain and France and other places. Freemasonry as a systematic creed with its own myths, values and set of rituals originated in Scotand around 1600 and spread first to England and then across the Continent in the eighteenth-century. They fostered new codes of conduct – including a communal understanding of liberty and equality inherited from guild sociability – "liberty, fraternity, and equality" Scottish soldiers and Jacobite Scots brought to the Continent ideals of fraternity which reflected not the local system of Scottish customs but the institutions and ideals originating in the English Revolution against royal absolutism. Freemasonry was particularly prevalent in France – by 1789, there were perhaps as many as 100,000 French Masons, making Freemasonry the most popular of all Enlightenment associations. The Freemasons displayed a passion for secrecy and created new degrees and ceremonies. Similar societies, partially imitating Freemasonry, emerged in France, Germany, Sweden and Russia. One example was the "Illuminati" founded in Bavaria in 1776, which was copied after the Freemasons but was never part of the movement. The Illuminati was an overtly political group, which most Masonic lodges decidedly were not.
Jacob further argues that Masonic lodges created a private model for public affairs. They "reconstituted the polity and established a constitutional form of self-government, complete with constitutions and laws, elections and representatives”. In other words, the micro-society set up within the lodges constituted a normative model for society as a whole. This was especially true on the Continent: when the first lodges began to appear in the 1730s, their embodiment of British values was often seen as threatening by state authorities. For example, the Parisian lodge that met in the mid 1720s was composed of English Jacobite exiles. Furthermore, freemasons all across Europe explicitly linked themselveto the Enlightenment as a whole. In French lodges, for example, the line “As the means to be enlightened I search for the enlightened” was a part of their initiation rites. British lodges assigned themselves the duty to “initiate the unenlightened”. This did not necessarily link lodges to the irreligious, but neither did this exclude them from the occasional heresy. In fact, many lodges praised the Grand Architect, the masonic terminology for the deistic divine being who created a scientifically ordered universe.
German historian Reinhart Koselleck claimed that "On the Continent there were two social structures that left a decisive imprint on the Age of Enlightenment: the Republic of Letters and the Masonic lodges.". Scottish professor Thomas Munck argues that "although the Masons did promote international and cross-social contacts which were essentially non-religious and broadly in agreement with enlightened values, they can hardly be described as a major radical or reformist network in their own right." Many of the Masons values seemed to greatly appeal to Enlightenment values and thinkers. Diderot discusses the link between Freemason ideals and the enlightenment in D'Alembert's Dream, exploring masonry as a way of spreading enlightenment beliefs. Historian Margaret Jacob stresses the importance of the Masons in indirectly inspiring enlightened political thought.
On the nagative side, Daniel Roche contests claims that Masonry promoted egalitarianism. He argues that “the real equality of the lodges was elitist”, only attracting men of similar social backgrounds. The presence of noble women in the French “lodges of adoption” that formed in the 1780s was largely due to the close ties shared between these lodges and aristocratic society.
Masons and the French and American revolutions.
The great enemy of Freemasonry was the Roman Catholic Church, so that in countries with a large Catholic element, such as France, Italy, Spain, and Mexico, much of the ferocity of the political battles involve the confrontation between what Davies calls the reactionary Church and enlightened Freemasonry.
In terms of their impact on revolutionary politics historian Robert Roswell Palmer concluded that even in France, Masons were politically "innocuous if not ridiculous" and did not act as a group. American historians, while noting that Benjamin Franklin and George Washington were indeed active Masons, have downplayed the importance of Freemasonry in causing the American Revolution because the Masonic order was non-political and included both Patriots and their enemy the Loyalists.

</doc>
<doc id="30785" url="http://en.wikipedia.org/wiki?curid=30785" title="Tulsa race riot">
Tulsa race riot

The Tulsa race riot was a large-scale, racially motivated conflict on May 31 and June 1, 1921, in which a group of white people attacked the black community of Tulsa, Oklahoma. It resulted in the Greenwood District, also known as 'the Black Wall Street' and the wealthiest black community in the United States, being burned to the ground.
During the 16 hours of the assault, more than 800 people were admitted to local white hospitals with injuries (the two black hospitals were burned down), and police arrested and detained more than 6,000 black Greenwood residents at three local facilities.:108–109 An estimated 10,000 blacks were left homeless, and 35 city blocks composed of 1,256 residences were destroyed by fire. The official count of the dead by the Oklahoma Department of Vital Statistics was 39, but other estimates of black fatalities vary from 55 to about 300.:108, 228 
The events of the riot were long omitted from local and state histories. "The Tulsa race riot of 1921 was rarely mentioned in history books, classrooms or even in private. Blacks and whites alike grew into middle age unaware of what had taken place." With the number of survivors declining, in 1996, the state legislature commissioned a report to establish the historical record of the events, and acknowledge the victims and damages to the black community. Released in 2001, the report included the commission's recommendations for some compensatory actions, most of which were not implemented by the state and city governments. The state passed legislation to establish some scholarships for descendants of survivors, economic development of Greenwood, and a memorial park to the victims in Tulsa. The latter was dedicated in 2010.
Background.
The riot occurred in the racially and politically tense atmosphere of post-World War I northeastern Oklahoma. The territory, which was declared a state on November 16, 1907, had received many settlers from the South who had been slaveholders before the American Civil War. In the early 20th century, lynchings were common in Oklahoma, as part of a continuing effort by whites to assert and maintain white supremacy. Between the declaration of statehood and the Tulsa race riot 13 years later, 31 persons were lynched in Oklahoma; 26 were black and nearly all were men and boys. During the twenty years following the riot, the number of lynchings statewide fell to two.
The newly created state legislature passed racial segregation laws, commonly known as Jim Crow laws, as one of its first orders of business. Its 1907 constitution and laws had voter registration rules that disfranchised most blacks; this also barred them from serving on juries or in local office, a situation that lasted until the federal Voting Rights Act of 1965, part of civil rights legislation passed by the U.S. Congress. Major cities passed their own restrictions.
On August 16, 1916, Tulsa passed an ordinance forbidding blacks or whites from residing on any block where three-fourths or more of the residents were of the other race. This made residential segregation mandatory in the city. Although the United States Supreme Court declared the ordinance unconstitutional the next year, it remained on the books.
As cities absorbed returning veterans into the labor market following World War I, there was social tension and anti-black sentiment. At the same time, black veterans pushed to have their civil rights enforced, believing they had earned full citizenship by military service. In what became known as the "Red Summer" of 1919, industrial cities across the Midwest and North had severe race riots, often led by ethnic whites among recent immigrant groups, who competed most with blacks for jobs. In Chicago and some other cities, blacks defended themselves for the first time with force but were outnumbered.
Northeastern Oklahoma had an economic slump that put men out of work. Since 1915, the Ku Klux Klan had been growing in urban chapters across the country, particularly since veterans had been returning from the war. It first appeared in Oklahoma in a major way on August 12, 1921, less than three months after the Tulsa riot. The historian Charles Alexander estimated that by the end of 1921, Tulsa had 3,200 residents in the Klan. The city population was 72,000 in 1920.
The traditionally black district of Greenwood in Tulsa had a commercial district so prosperous it was known as "the Negro Wall Street" (now commonly referred to as "the Black Wall Street"). Blacks had created their own businesses and services in their enclave, including several groceries, two independent newspapers, two movie theaters, nightclubs, and numerous churches. Black professionals—doctors, dentists, lawyers, and clergy—served the community. Because of residential segregation in the city, most classes of blacks lived together in Greenwood. They selected their own leaders, and there was capital formation within the community. In the surrounding areas of northeastern Oklahoma, blacks also enjoyed relative prosperity and participated in the oil boom.
Monday, May 30, 1921 – Memorial Day.
Encounter in the elevator.
Sometime around or after 4 p.m., 19-year-old Dick Rowland, a black shoeshiner employed at a Main Street shine parlor, entered the only elevator of the nearby Drexel Building, at 319 South Main Street, to use the top-floor restroom, which was restricted to blacks. He encountered Sarah Page, the 17-year-old white elevator operator who was on duty. The two likely knew each other at least by sight, as this building was the only one nearby with a washroom that Rowland had express permission to use, and the elevator operated by Page was the only one in the building. A clerk at Renberg's, a clothing store located on the first floor of the Drexel, heard what sounded like a woman's scream and saw a young black man rushing from the building. The clerk went to the elevator and found Page in what he said was a distraught state. Thinking she had been assaulted, he summoned the authorities.
The 2000 official commission report notes that it was unusual for both Rowland and Page to be working downtown on Memorial Day, when most stores and businesses were closed. It suggests that Rowland had a simple accident, such as tripping and steadying himself against the girl, or perhaps they were lovers and had a quarrel.
 Whether – and to what extent – Dick Rowland and Sarah Page knew each other has long been a matter of speculation. It seems reasonable that they would have least been able to recognize each other on sight, as Rowland would have regularly ridden in Page's elevator on his way to and from the restroom. Others, however, have speculated that the pair might have been lovers – a dangerous and potentially deadly taboo, but not an impossibility... Whether they knew each other or not, it is clear that both Dick Rowland and Sarah Page were downtown on Monday, May 30, 1921 – although this, too, is cloaked in some mystery. On Memorial Day, most – but not all – stores and businesses in Tulsa were closed. Yet, both Rowland and Page were apparently working that day...
What happened next is anyone's guess. After the riot, the most common explanation was that Dick Rowland tripped as he got onto the elevator and, as he tried to catch his fall, he grabbed onto the arm of Sarah Page, who then screamed. It also has been suggested that Rowland and Page had a lovers' quarrel. However, it simply is unclear what happened. Yet, in the days and years that followed, everyone who knew Dick Rowland agreed on one thing: that he would never have been capable of rape.
The word "rape" was rarely used in newspapers or academia in the early 20th century. Instead, "assault" was used to describe such an attack.
A brief investigation.
Although the police likely questioned Page, no written account of her statement has surfaced. It is generally accepted that they determined what happened between the two teenagers was something less than an assault. The authorities conducted a rather low-key investigation rather than launching a man-hunt for her alleged assailant. Afterward, Page told the police that she would not press charges.
Regardless of whether assault had occurred, Rowland had reason to be fearful, as at the time, such an accusation alone put him at risk for attack by racist white men. Realizing the gravity of the situation, Rowland fled to his mother's house in the Greenwood neighborhood.
Tuesday, May 31, 1921.
Suspect arrested.
On the morning after the incident, Detective Henry Carmichael and Henry C. Pack, a black patrolman, located Rowland on Greenwood Avenue and detained him. Pack was one of two black officers on the city's approximately 45-man police force. Rowland was initially taken to the Tulsa city jail at First and Main. Late that day, Police Commissioner J. M. Adkison said he had received an anonymous telephone call threatening Rowland's life. He ordered Rowland transferred to the more secure jail on the top floor of the Tulsa County Courthouse.
Word quickly spread in Tulsa's legal circles. As patrons of the shine shop where Rowland worked, many attorneys knew him. Witnesses recounted hearing several attorneys defending him in personal conversations with one another. One of the men said, "Why, I know that boy, and have known him a good while. That's not in him."
Newspaper coverage.
The "Tulsa Tribune", one of two white-owned papers published in Tulsa, broke the story in that afternoon's edition with the headline: "Nab Negro for Attacking Girl In an Elevator", describing the alleged incident. According to some witnesses, the same edition of the "Tribune" included an editorial warning of a potential lynching of Rowland, and entitled "To Lynch Negro Tonight". The paper was known at the time to have a "sensationalist" style of news writing. All original copies of that issue of the paper have apparently been destroyed, and the relevant page is missing from the microfilm copy, so the exact content of the column (and whether it existed at all) remains in dispute.
Stand-off at the courthouse.
The afternoon edition of the "Tribune" hit the streets shortly after 3 p.m., and soon news of the potential lynching spread. By 4 p.m., the local authorities were on alert. White people began congregating at and near the Tulsa County Courthouse. By sunset at 7:34 p.m., the several hundred whites assembled outside the courthouse appeared to have the makings of a lynch mob. Willard M. McCullough, the newly elected sheriff of Tulsa County, was determined to avoid events such as the 1920 lynching of Roy Belton in Tulsa, which occurred during the term of his predecessor. The sheriff took steps to ensure the safety of Rowland. McCullough organized his deputies into a defensive formation around Rowland, who was terrified. The sheriff positioned six of his men, armed with rifles and shotguns, on the roof of the courthouse. He disabled the building's elevator, and had his remaining men barricade themselves at the top of the stairs with orders to shoot any intruders on sight. The sheriff went outside and tried to talk the crowd into going home, but to no avail. According to an account by Ellsworth, the sheriff was "hooted down".
About 8:20 p.m., three white men entered the courthouse, demanding that Rowland be turned over to them. Although vastly outnumbered by the growing crowd out on the street, Sheriff McCullough was determined to prevent another lynching and turned the men away.
Offer of help.
A few blocks away on Greenwood Avenue, members of the black community were gathering to discuss the situation at the courthouse. Given the recent lynching of Roy Belton, a white man accused of murder, they believed that Rowland was greatly at risk. The community was determined to prevent the lynching of another young black man, but divided about the tactics to be used. Young World War I veterans were preparing for a battle by collecting guns and ammunition. Older, more prosperous men feared a destructive confrontation that likely would cost them dearly. O. W. Gurley walked to the courthouse, where the sheriff assured him that there would be no lynching. Returning to Greenwood, Gurley tried to calm the group, but failed. About 7:30 p.m., a mob of approximately 30 black men, armed with rifles and shotguns, decided to go to the courthouse and support the sheriff and his deputies to defend Rowland from the mob. Assuring them that Rowland was safe, the sheriff and his black deputy, Barney Cleaver, encouraged the men to return home.
Taking up arms.
Having seen the armed blacks, some of the more than 1,000 whites at the courthouse went home for their own guns. Others headed for the National Guard armory at Sixth Street and Norfolk Avenue, where they planned to arm up. The armory contained a supply of small arms and ammunition. Major James Bell of the 180th Infantry had already learned of the mounting situation downtown and the possibility of a break-in, and he took appropriate measures to prevent this. He called the commanders of the three National Guard units in Tulsa, who ordered all the Guard members to put on their uniforms and report quickly to the armory. When a group of whites arrived and began pulling at the grating over a window, Bell went outside to confront the crowd of 300–400 men. Bell told them that the Guard members inside were armed and prepared to shoot anyone who tried to enter. After this show of force, the crowd withdrew from the armory.
At the courthouse, the crowd had swollen to nearly 2,000, many of them now armed. Several local leaders, including Reverend Charles W. Kerr, pastor of the First Presbyterian Church, tried to dissuade mob action. The chief of police, John A. Gustafson, later claimed that he tried to talk the crowd into going home.
Anxiety on Greenwood Avenue was rising. The black community was worried about the safety of Rowland. Small groups of armed black men began to venture toward the courthouse in automobiles, partly for reconnaissance, and to demonstrate they were prepared to take necessary action to protect Rowland.
Many white men interpreted these actions as a "Negro uprising" and became concerned. Eyewitnesses reported gunshots, presumably fired into the air, increasing in frequency during the evening.
Second offer.
In Greenwood, rumors began to fly – in particular, a report that whites were storming the courthouse. Shortly after 10 p.m., a second, larger mob of approximately seventy-five armed black men decided to go to the courthouse. They offered their support to the sheriff, who declined their help. According to witnesses, a white man is alleged to have told one of the armed black men to surrender his pistol. The man refused, and a shot was fired. That first shot may have been accidental, or meant as a warning shot; it was a catalyst for an exchange of gunfire.
The riot.
The gunshots triggered an almost immediate response by the white men, many of whom fired on the blacks, who continued firing back at the whites. The first "battle" was said to last a few seconds or so, but took a toll, as ten whites and two blacks lay dead or dying in the street. The black contingent retreated toward Greenwood. A rolling gunfight ensued. The armed white mob pursued the black group toward Greenwood, with many stopping to loot local stores for additional weapons and ammunition. Along the way innocent bystanders, many of whom were leaving a movie theater after a show, were caught off guard by the mob and began fleeing. Panic set in as the white mob began firing on any blacks in the crowd. The mob also shot and killed at least one white man in the confusion.
At around 11 p.m., members of the Oklahoma National Guard unit began to assemble at the armory to organize a plan to subdue the rioters. Several groups were deployed downtown to set up guard at the courthouse, police station, and other public facilities. Members of the local chapter of the American Legion joined in on patrols of the streets. The forces appeared to have been deployed to protect the white districts adjacent to Greenwood. This manner of deployment led to the National Guard being set in apparent opposition to the black community. The National Guard began rounding up blacks who had not returned to Greenwood and taking them to the Convention Hall on Brady Street for detention.
Many prominent Tulsa whites also participated in the riot, including Tulsa founder and KKK member W. Tate Brady who participated in the riot as a night watchman. He reported seeing "five dead negroes," including one man who was dragged behind a car by a noose around his neck.
At around midnight, white rioters again assembled outside the courthouse. It was a smaller group but more organized and determined. They shouted in support of a lynching. When they attempted to storm the building, the sheriff and his deputies turned them away and dispersed them.
Wednesday, June 1, 1921.
Throughout the early morning hours, groups of armed whites and blacks squared off in gunfights. At this point the fighting was concentrated along sections of the Frisco tracks, a dividing line between the black and white commercial districts. A rumor circulated that more blacks were coming by train from Muskogee to help with an invasion of Tulsa. At one point, passengers on an incoming train were forced to take cover on the floor of the train cars, as they had arrived in the midst of crossfire, with the train taking hits on both sides.
Small groups of whites made brief forays by car into Greenwood, indiscriminately firing into businesses and residences. They often received return fire. Meanwhile, white rioters threw lighted oil rags into several buildings along Archer street, igniting them.
Fires begin.
At around 1 a.m., the white mob began setting fires, mainly in businesses on commercial Archer Street at the southern edge of the Greenwood district. As crews from the Tulsa Fire Department arrived to put out fires, they were turned away at gunpoint. By 4 a.m., an estimated two-dozen black-owned businesses had been set ablaze.
As news traveled among Greenwood residents in the early morning hours, many began to take up arms in defense of their community, while others began a mass exodus from the city. Throughout the night both sides continued fighting, sometimes only sporadically.
Daybreak.
Upon the 5 a.m. sunrise, reportedly a train whistle was heard (Hirsch said it was a siren). Many believed this to be a signal for the rioters to launch an all-out assault on Greenwood. A white man stepped out from behind the Frisco depot and received a fatal bullet from a sniper in Greenwood. Crowds of rioters poured from places of shelter, on foot and by car, into the streets of the black community. Five white men in a car led the charge, but were killed by a fusillade of gunfire before they had gone a block.
Overwhelmed by the sheer number of whites, more blacks retreated north on Greenwood Avenue to the edge of town. Chaos ensued as terrified residents fled for their lives. The rioters shot indiscriminately, and killed many residents along the way. Splitting up into small groups, they began breaking into houses and buildings, looting them and taking whatever they fancied. Several blacks later testified that whites broke into occupied homes, and ordered the residents out to the street, where they could be driven or forced to walk to detention centers.
A rumor spread among the whites that the new Mount Zion Baptist Church was being used as a fortress and armory. Supposedly twenty caskets full of rifles had been delivered to the church, though no evidence was ever found.
Attack by air.
Numerous witness accounts described airplanes carrying white assailants, who fired rifles and dropped firebombs on buildings, homes, and fleeing families. The planes, six biplane two-seater trainers left over from World War I, were dispatched from the nearby Curtiss-Southwest Field outside Tulsa. Law enforcement officials later stated the planes were to provide reconnaissance and protect against a "Negro uprising". Eyewitness accounts and testimony from the survivors maintained that on the morning of June 1, the planes dropped incendiary bombs and fired rifles at black residents on the ground.
Several groups of blacks attempted to organize a defense, but they were overwhelmed by the number of armed whites. Many blacks surrendered. Others returned fire, and ultimately died. As the fires spread northward through Greenwood, countless black families continued to flee. Many were estimated to have died when trapped by the flames.
Other whites.
As unrest spread to other parts of the city, many middle class white families who employed blacks in their homes as live-in cooks and servants were accosted by white rioters. They demanded that families turn over their employees to be taken to detention centers around the city. Many white families complied, and those who refused were subjected to attacks and vandalism in turn.
Arrival of state troops.
Adjutant General Charles Barrett of the Oklahoma National Guard arrived with 109 troops from Oklahoma City by special train about 9:15 a.m. He could not legally act until he had contacted all the appropriate local authorities, including the mayor, the sheriff and the police chief. Meanwhile his troops paused to eat breakfast. Barrett also summoned reinforcements from several other Oklahoma cities. By this time, most of the surviving black citizens had either fled the city or were in custody at the various detention centers. The troops declared martial law at 11:49 a.m., and by noon had managed to suppress most of the remaining violence. A 1921 letter from an Officer of the Service Company, Third Infantry, Oklahoma National Guard arriving May 31, 1921, reports taking about 30-40 African Americans in custody; putting a machine gun on a truck and putting it on patrol; being fired on from Negro snipers from the "Church" and returning fire; being fired on by white men; turning the prisoners over to deputies to take them to Police headquarters; being fired upon again by negroes and having two NCO slightly wounded; searching for negroes and firearms; detailing a NCO to take 170 Negroes to the Civil authorities; and then delivering an additional 150 Negroes to the Convention Hall.
Aftermath.
Casualties.
Human deaths and injuries.
The reported number of dead varies widely. On June 1, 1921, the "Tulsa Tribune" reported that 9 whites and 68 blacks had died in the riot, but shortly afterward changed this to a total of 176 dead. On the next day, the same paper reported the count as 9 whites and 21 blacks. The "New York Times" said that 77 people had been killed, including 68 blacks, but then lowered the total to 33. The "Richmond Times Dispatch" reported that 85 {including 25 whites} killed; it also reported that the Police Chief had reported to the Governor Robertson that the total was 75; and that a Police Major put the figure as 175. The Oklahoma Department of Vital Statistics count put the number of dead at 36: 26 black and 10 white. Walter Francis White of the N.A.A.C.P. reported that although officials and undertakers reported the numbers of fatalities as being ten white and 21 colored, he estimated that the true numbers to be 50 whites and between 150 and 200 Negroes; he also states that ten white men were killed on Tuesday; six white men drove into the black section and never came out and that thirteen whites were killed on Wednesday; he also remarks that the head of the Salvation Army in Tulsa stated that thirty seven negroes had been employed as gravediggers and had buried 120 negroes in individual graves without coffins on Friday and Saturday. Maurice Willows, an American Red Cross social worker, reported that up to 300 blacks were killed. He also reported that there was a rush to bury the bodies and that no records were made of many burials.
Of the some 800 people admitted to local hospitals for injuries, the majority are believed to have been white, as both black hospitals had been burned in the rioting. Additionally, even if the white hospitals had admitted blacks because of the riot, against their usual segregation policy, injured blacks had little means to get to these hospitals, which were located across the city from Greenwood. More than 6,000 black Greenwood residents were arrested and detained at three local facilities: Convention Hall, now known as the Brady Theater, the Fairgrounds (then located about a mile northeast of Greenwood), and McNulty Park (a baseball stadium at Tenth Street and Elgin Avenue).
Several blacks were known to have died while in the internment centers. While most of the deaths are said to have been accurately recorded, no records have been found as to how many detainees were treated for injuries and survived. These numbers could reasonably have been more than a thousand, perhaps several thousand.
Property losses.
The commercial section of Greenwood was destroyed. This included 191 businesses, a junior high school, several churches and the only hospital in the district. The Red Cross reported that 1,256 houses were burned and another 215 were looted but not burned. The Tulsa Real Estate Exchange estimated property losses amounted to $1.5 million in real estate and $750,000 in personal property ($ million in 2015). Local citizens had filed more than $1.8 million in riot-related claims against the city by June 6, 1922.
Legal actions.
A grand jury in Tulsa ruled that Police Chief John Gustafson was responsible for the riot because he neglected his duty, and removed him from office. In a subsequent trial, he was found guilty of failing to take proper precautions for protecting life and property, and for conspiring to free automobile thieves and collect rewards. But, the former chief never served time in prison. Instead, he returned to his private detective practice. No legal records indicate that any other white official was ever charged of wrongdoing or even negligence.
Dick Rowland remained safe in the county jail until the next morning, when the police transported him out of town in secrecy. All charges were dropped. He never returned to Tulsa.
No charges were filed against individual white rioters. Other lawsuits against insurance companies for losses were unsuccessful as well.
Attempt to prevent reconstruction of Greenwood.
The division between white and black residents of Tulsa was so deep that the end of the riot did not begin to bring reconciliation. The widespread destruction of Greenwood was not sufficient for those whites who wanted to separate even further from blacks. A week after the riot, W. Tate Brady was appointed to the Tulsa Real Estate Exchange ("The Exchange"). The Tulsa Chamber of Commerce had created the group to estimate the value of property damaged or destroyed in Greenwood. The Exchange also contrived a scheme to relocate black Tulsans farther north and east of the original Greenwood.
In cooperation with the City Commission, the Exchange prepared new building codes for the original Greenwood that would make rebuilding prohibitively expensive for the original owners. The land could then be redeveloped as a commercial and industrial district - no longer residential. The plan was never implemented because the Oklahoma Supreme Court overruled the proposed ordinances as unconstitutional. B. C. Franklin, the lead attorney of the black community who challenged the ordinance, was the father of John Hope Franklin, who became a notable historian.
Tulsa Race Riot Commission.
In 1996, following increased attention to the riot because of the 75th anniversary of the event, the state legislature authorized the Tulsa Race Riot Commission, to study and prepare a "historical account" of the riot. Undertaking the study "enjoyed strong support from members of both political parties and all political persuasions." The Commission delivered its report on February 21, 2001.
In addition to thoroughly documenting the causes and damages of the riot, the report recommended actions for substantial restitution to the black community; in order of priority:
The Tulsa Reparations Coalition, sponsored by the Center for Racial Justice, Inc., was formed on April 7, 2001 to obtain restitution for the damages suffered by Tulsa's Black community, as recommended by the Oklahoma Commission.
In June 2001, the Oklahoma state legislature passed the "1921 Tulsa Race Riot Reconciliation Act". While falling short of the Commission's recommendations, it provided for the following:
The state government has made limited attempts to find suspected mass graves used to bury the unknown numbers of black dead. The Commission reported that it was not authorized to undertake the necessary archaeological work to verify the claims.
Lawsuit against Tulsa and Oklahoma.
Five elderly survivors of the riot, represented by a legal team including Johnnie Cochran and Charles Ogletree, filed suit against the city of Tulsa and the state of Oklahoma ("Alexander, et al., v. Oklahoma, et al.") in February 2003, based on the findings of the 2001 report. Ogletree said the state and city should compensate the victims and their families "to honor their admitted obligations as detailed in the commission's report." The plaintiffs did not seek reparations as such; rather, they asked for the establishment of educational and health-care resources for current residents of Greenwood. The federal district and appellate courts dismissed the suit, citing the statute of limitations on the 80-year-old case. The Supreme Court of the United States declined to hear the appeal.
In April 2007, Ogletree appealed to the U.S. Congress to pass a bill extending the statute of limitations for the case, given the long suppression of material about it.

</doc>
<doc id="30801" url="http://en.wikipedia.org/wiki?curid=30801" title="Truso">
Truso

Truso, situated on Lake Drużno, was an Old Prussian (Pomesanian) town near the Baltic Sea just east of the Vistula River. It was one of the trading posts on the Amber Road, and is thought to be the antecedent of the city of Elbląg (Elbing). In the words of Marija Gimbutas, "the name of the town is the earliest known historically in the Baltic Sea area". The main export goods of Truso were amber, furs, and slaves, while blacksmithing and amber working were the major industries. The beginnings of the town can be dated back to approximately the end of the 8th century, while in the second half of the 10th century, the town declined and was eclipsed as a trade center by nearby Gdańsk. 
History.
Truso was situated in a central location upon the Eastern European trade routes, which led from Birka in the north to the island of Gotland and to Visby in the Baltic Sea and later included the Hanseatic city of Elbląg. From there, traders continued further south to Carnuntum in the Alps. This was called the Amber Road. The ancient amber roads led further south-west and south-east to the Black Sea and eventually to Asia. "For East Prussia, Truso played the same role as Haithabu (Slesvig) or Hedeby for north-western Germany or Slavic Vineta for Pomerania", Gimbutas has observed.
East-west trade route went from Truso and Wiskiauten (a rival centre in Prussia which sprang up at the south-western corner of the Courish Lagoon), along the Baltic Sea to Jutland, and from there up the Slien inlet to Haithabu/Hedeby, a large trading center in Jutland. Hedeby, which lay near the modern city of Schleswig in Schleswig-Holstein, was pretty centrally located and could be reached from all four directions over land as well as from the North Sea, the Atlantic Ocean, and the Baltic Sea. 
Around the year 890, Wulfstan of Hedeby (by his own account) undertook a seven-days boat journey from Hedeby to Truso at the behest of king Alfred the Great. One possible reason for this expedition was because Alfred needed aid in his defense against the Danes or Vikings, who had taken over most of England. The reasons for this journey are fundamentally unclear, since Truso was at the time little more than a trading center, and Alfred the Great, the West Saxon ruler, already kept in close contact with the continental Saxons and the Franks. 
Archaeology.
First attempts at finding the exact location of the town date back to early sixteenth century. Based on archaeological finds from 1897 and excavations which began in the 1920s, archaeologists located Truso around Janów Pomorski, Poland, in the south-eastern suburb of Elbląg. Found artifacts, dating from the 7th to 12th century, were stored in a local museum and are now on exhibition at the Elbląg Museum. In the 1980s, the Polish archaeologist Marek Jagodziński resumed excavations and cleared a c. 20 hectare site, in which a series of structures were burnt down around the year 1000 AD.
Gwyn Jones notes that "no true town has been found and excavated" and that the identification of the site in Elbląg with Truso is based on "finds of Norse weapons" and the presence of "a large Viking Age cemetery" nearby, According to Mateusz Bogucki "by now, there is no doubt that the settlement really is Wulfstan's Truso" The Elbląg Museum brochure: "Truso- A Discovered Legend", by Marek F Jagodziński, describes a large number of buildings found during the recent excavations, with burnt remains of posts suggesting buildings of c. 5 x 10 m and long houses of about 6 x 21 m.

</doc>
<doc id="30805" url="http://en.wikipedia.org/wiki?curid=30805" title="Tape bias">
Tape bias

Tape bias is the term for two phenomena, AC bias and DC bias, that improve the fidelity of analogue magnetic tape sound recordings. DC bias is the addition of a direct current to the audio signal that is being recorded. AC bias is the addition of an inaudible high-frequency signal (generally from 40 to 150 kHz) to the audio signal. Magnetic tape has a nonlinear response at low signal strengths, as measured by its coercivity. Bias increases the signal quality of most audio recordings significantly by pushing the signal into the linear zone of the tape's transfer function.
History.
Magnetic recording was proposed as early as 1888 by Oberlin Smith, who published 1888-09-08 in "The Electrical World" as "Some possible forms of phonograph". By 1898 Valdemar Poulsen had demonstrated a magnetic recorder and proposed magnetic tape. Fritz Pfleumer was granted <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi/fmt%3Akev%3Amtx%3Apatent&rft.number=500900&rft.cc=DE&rft.title=">  for a "Sound recording carrier" on 1928-01-31, but it was later overturned in favour of the earlier <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi/fmt%3Akev%3Amtx%3Apatent&rft.number=1653467&rft.cc=US&rft.title=">  by Joseph A. O'Neill.
DC bias.
The earliest magnetic recording systems simply applied the unadulterated (baseband) input signal to a recording head, resulting in recordings with poor low-frequency response and high distortion. Within short order, the addition of a suitable direct current to the signal was found to reduce distortion by operating the head substantially within its linear response region. The principal disadvantage of DC bias was that it left the tape with a net magnetisation which, because of the grain of the tape particles, generated significant noise on replay. Some early DC bias systems used a permanent magnet that was placed near the record head. It had to be swung out of the way for replay. DC bias was re-adopted by some very low cost cassette recorders.
AC bias.
Although the improvements are marked with such DC bias, even more dramatic improvement results if an alternating current bias is used instead. While several people around the world discovered AC bias, it was the German developments that were widely used in practice and served as the model for future work.
The first patent for AC bias was filed by W. L. Carlson and Glenn L. Carpenter in 1921, eventually resulting in <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi/fmt%3Akev%3Amtx%3Apatent&rft.number=1640881&rft.cc=US&rft.title="> . The value of AC bias was somewhat masked by the primitive state of other aspects of magnetic recording, however, and Carlson and Carpenter's achievement was largely ignored. Teiji Igarishi, Mokoto Ishikawa, and Kenzo Nagai of Japan published a paper on AC biasing in 1938 and received a Japanese patent in 1940. Marvin Camras (USA) also discovered high frequency (AC) bias independently in 1941 and received <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi/fmt%3Akev%3Amtx%3Apatent&rft.number=2351004&rft.cc=US&rft.title="> .
The reduction in distortion and noise provided by AC bias was rediscovered in 1940 by Walter Weber (1907–1944), while working for Hans Joachim von Braunmühl at the Reichs-Rundfunk-Gesellschaft (RRG). The German pair received several related patents, including <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi/fmt%3Akev%3Amtx%3Apatent&rft.number=743411&rft.cc=DE&rft.title=">  for "high frequency treatment of the sound carrier".
Possibly independently of Weber and Braunmühl, the UK company Boosey & Hawkes produced a steel wire recorder under government contract during the Second World War that was equipped with AC bias. Examples still surface from time to time, many having been disposed of as government surplus stock. After the war, Boosey and Hawkes also produced a "Reporter" tape recorder in the early 1950s using magnetic tape, rather than wire, which was based on German wartime technology.
Theory.
As the tape leaves the trailing edge of the gap in the tape head, the oscillating magnetic field due to the applied bias is rapidly reduced to the average magnetic field of the much slower changing audio signal and the tape particles are therefore left in this magnetic condition. The non linearity of the magnetic particles in the tape coating is overcome by having the AC bias field greater by at least an 'order of magnitude' (10x the maximum audio field) which saturates these particles in both magnetic directions while they pass the gap in the recording head. The AC bias level is quite critical and, after being adjusted for a particular tape formulation with a specific recording machine, was usually left unchanged.
The mechanism of action is the same as in Deperming and Degaussing: the large AC signal acts as a degaussing signal that decays exponentially as the tape head moves along, while the audio signal is the residual field that remains imprinted on the magnetic media.
Practice.
The characteristics of the recording system change quite markedly as the level of the bias current is changed. There is a level at which the system gives the minimum distortion (which is the highest bias). There is also a level at which the high frequency response is at maximum (lowest bias). These conditions unfortunately do not occur at the same bias level. Professional reel to reel and cassette recorders are always set up for minimum distortion. Consumer equipment, and in particular compact audio cassette recorders have the bias set at a compromise level (usually a little higher) to give good frequency response and acceptably low distortion.
Bang & Olufsen invented and patented the so-called Dolby HX PRO (Headroom eXtension) principle for combining bias control with the Dolby system for better high frequency response in cassette recorders. Tandberg invented the cross-field recording system for tape recorders where a separate head with the bias was used. This produced a better bias than by mixing the two signals in the recording head, but mechanical tolerances for cross-field are tight. The system required frequent readjustment and was largely abandoned.
Different amplitudes of bias field are optimal for different types of tape, so most recorders offer a bias setting switch on the control panel, or, in the case of the compact audio cassette, may switch automatically according to cutouts on the cassette shell. Ferric based tapes require the lowest bias field, with Chrome based tapes (including the pseudo chromes) requiring a higher level. Metal particle requires even more. Metal Evaporated tape accepts the highest level of bias, but it is mostly used for digital recording purposes (which does not have bias as the non linearity is not a major problem). The same is valid for a combination cassette tape, the FeCr-variant, on which a thicker Ferric layer was covered by a thinner chrome layer. The idea behind this was that at lower frequencies and higher head currents the Ferric layer would be more deeply magnetized, while at higher frequencies only the top Cr-layer was active. In practice, this didn't work well, and some claimed that this thin chrome layer was quickly polished off in heavy use.

</doc>
<doc id="30814" url="http://en.wikipedia.org/wiki?curid=30814" title="Taoiseach">
Taoiseach

The Taoiseach () is the head of government or prime minister of Ireland. The Taoiseach is appointed by the President upon the nomination of Dáil Éireann, the lower house of the Oireachtas (parliament), and must, in order to remain in office, retain the support of a majority in the Dáil. The current Taoiseach is Enda Kenny, TD, who was appointed on 9 March 2011.
The word means "chief" in the Irish language. The earliest known use of the term is from a 5th- or 6th-century ogham inscription in both the Gaelic and Brittonic languages.
Overview.
Under the Constitution of Ireland, the Taoiseach must be appointed from among the members of Dáil Éireann. He is nominated by a simple majority of the chamber's members, and formally appointed to office by the President.
If the Taoiseach loses the support of a majority in Dáil Éireann, he is not automatically removed from office but, rather, is compelled to "either" resign "or" persuade the President to dissolve the Dáil. The President may refuse to grant a dissolution and, in effect, force the Taoiseach to resign, but, to date, no president has exercised this prerogative, though the option arose in 1944 and 1994 and twice in 1982. The Taoiseach may lose the support of Dáil Éireann by the passage of a vote of no confidence, the failure of a vote of confidence or, alternatively, the Dáil may refuse "supply". In the event of the Taoiseach's resignation, he continues to exercise the duties and functions of his office until the appointment of a successor.
The Taoiseach nominates the remaining members of the Government, who are then, with the consent of the Dáil, appointed by the President. The Taoiseach also has authority to advise the President to dismiss cabinet ministers from office, advice the President is required to follow by convention. The Taoiseach is further responsible for appointing eleven members of the Seanad.
The Department of the Taoiseach is the government department which supports and advises the Taoiseach in carrying out his various duties.
Salary.
Since 2013, the Taoiseach's annual salary is €185,350. It was cut from €214,187 to €200,000 when Kenny took office, before being cut further to €185,350 under the Haddington Road Agreement in 2013.
A proposed increase of €38,000 in 2007 was deferred when Brian Cowen became Taoiseach and in October 2008, the government announced a 10% salary cut for all ministers, including the Taoiseach. However this was a voluntary cut and the salaries remained nominally the same with ministers and the Taoiseach essentially refusing 10% of their salary. This courted controversy in December 2009 when a salary cut of 20% was based on the higher figure before the refused amount was deducted. The Taoiseach is also allowed an additional €118,981 in annual expenses.
Residence.
There is no official residence of the Taoiseach. However, in 2008 it was reported speculatively that the former Steward's Lodge at Farmleigh adjoining the Phoenix Park would become the official residence of the Taoiseach, however no official statements were made nor any action taken. The house, which forms part of the Farmleigh estate acquired by the State in 1999 for €29.2m, was renovated at a cost of nearly €600,000 in 2005 by the Office of Public Works. Former Taoiseach Bertie Ahern did not use it as a residence, but his successor Brian Cowen used it "from time to time".
History.
Origins and etymology.
The words "Taoiseach" (]) and "Tánaiste" (the title of the deputy prime minister) are both from the Irish language and of ancient origin. Though the Taoiseach is described in the Constitution of Ireland as "the head of the Government or Prime Minister", its literal translation is "chieftain" or "leader". Tánaiste in turn refers to the system of tanistry, the Gaelic system of succession whereby a leader would appoint an heir apparent while still living.
In Scottish Gaelic, "tòiseach" translates as clan chief and both words originally had similar meaning in the Gaelic languages of Scotland and Ireland. The related Welsh language word "tywysog" (current meaning: "prince") has a similar origin and meaning. Both derive ultimately from the proto-Celtic *"towissākos" "chieftain, leader".
The plural of "taoiseach" is "taoisigh" (] or ]).
Debate on the title.
In 1937 when the draft Constitution of Ireland was being debated in the Dáil, an opposition politician moved an amendment to substitute "Prime Minister" for the proposed "Taoiseach" title in the English text of the Constitution. It was proposed to keep "Taoiseach" title in the Irish language text. The proponent remarked:
It seems to me to be mere make-believe to try to incorporate a word like "Taoiseach" in the English language. It would be pronounced wrongly by 99 percent of the people. I have already ascertained it is a very difficult word to pronounce correctly. That being so, even for the sake of the dignity of the Irish language, it would be more sensible that when speaking English we should be allowed to refer to the gentleman in question as the Prime Minister... It is just one more example of the sort of things that are being done here as if for the purpose of putting off the people in the North. No useful purpose of any kind can be served by compelling us, when speaking English, to refer to the Taoiseach rather than to the Prime Minister.
The President of the Executive Council, Éamon de Valera gave the term's meaning as "chieftain" or "Captain". He said he was "not disposed" to support the proposed amendment and felt the word "Taoiseach" did not need to be changed. The proposed amendment was defeated on a vote and "Taoiseach" was included as the title ultimately adopted by plebiscite of the people.
Modern office.
The modern position of Taoiseach was established by the 1937 Constitution of Ireland, to replace the position of President of the Executive Council of the 1922–1937 Irish Free State. The positions of Taoiseach and President of the Executive Council differed in certain fundamental respects. Under the Constitution of the Irish Free State, the latter was vested with considerably less power and was largely just the chairman of the cabinet, the Executive Council. For example, the President of the Executive Council could not dismiss a fellow minister. Instead, the Executive Council had to be disbanded and reformed entirely in order to remove one of its number. The President of the Executive Council could also not personally ask the Governor-General to dissolve Dáil Éireann, that power belonging collectively to the Executive Council.
In contrast, the Taoiseach created in 1937 possesses a much more powerful role. He can both advise the President to dismiss ministers and dissolve Parliament on his own authority—advice that the President is almost always required to follow by convention. His role is greatly enhanced because under the Constitution, he is both "de jure" and "de facto" chief executive, since the Constitution explicitly vests executive power in the Government. In most other parliamentary democracies, the head of state is at least the nominal chief executive.
Historically, where there have been multi-party or coalition governments, the Taoiseach has come from the leader of the largest party in the coalition. One exception to this was John A. Costello, who was not leader of his party, but an agreed choice to head the government, because the other parties refused to accept then Fine Gael leader Richard Mulcahy as Taoiseach.
List of office holders.
Before the enactment of the 1937 Constitution, the head of government was referred to as the President of the Executive Council. This office was first held by W. T. Cosgrave of Cumann na nGaedheal from 1922–32, and then by Éamon de Valera from 1932–37. By convention, Taoisigh are numbered to include Cosgrave, for example Enda Kenny is considered the 13th Taoiseach, not the 12th.
Living former officeholders.
There are four living former taoisigh:
Further reading.
The book "Chairman or Chief: The Role of the Taoiseach in Irish Government (1971)" by Brian Farrell provides a good overview of the conflicting roles for the Taoiseach. Though long out of print, it may still be available in libraries or from AbeBooks. Biographies are also available of de Valera, Lemass, Lynch, Cosgrave, FitzGerald, Haughey, Reynolds and Ahern. FitzGerald wrote an autobiography, while an authorised biography was produced of de Valera.

</doc>
<doc id="30853" url="http://en.wikipedia.org/wiki?curid=30853" title="Transylvania">
Transylvania

Transylvania (Romanian: "Transilvania" or "Ardeal", Hungarian: "Erdély", German: "Siebenbürgen", Polish: "Siedmiogród", Latin: "Transsilvania", Turkish: "Erdel") is a historical region in the central part of Romania. Bounded on the east and south by its natural borders, the Carpathian mountain range, historical Transylvania extended in the west to the Apuseni Mountains; however, the term sometimes encompasses not only Transylvania proper, but also the historical regions of Crișana, Maramureș, and Romanian part of Banat.
The region of Transylvania is known for the scenic beauty of its Carpathian landscape and its rich history. In the English-speaking world it has been commonly associated with vampires, chiefly due to the influence of Bram Stoker's famous novel "Dracula" as well as the many later film adaptations.
Etymology.
In Romanian, the region is known as "Ardeal" (]) or "Transilvania" (]); in Hungarian as "Erdély" (]); in German as Siebenbürgen (]); and in Turkish as "Transilvanya" (]) but historically as "Erdel" or "Erdehstan"; see also other denominations.
History.
Transylvania has been dominated by several different peoples and countries throughout its history. It was once the nucleus of the Kingdom of Dacia (82 BC–106 AD). In 106 AD the Roman Empire conquered the territory, systematically exploiting its resources. After the Roman legions withdrew in 271 AD, it was overrun by a succession of various tribes, bringing it under the control of the Carpi (Dacian tribe), Visigoths, Huns, Gepids, Avars and Slavs. From 9th to 11th century Bulgarians ruled Transylvania. It is a subject of dispute whether elements of the mixed Daco–Roman population survived in Transylvania through the Dark Ages (becoming the ancestors of modern Romanians) or the first Vlachs/Romanians appeared in the area in the 13th century after a northward migration from the Balkan Peninsula. There is an ongoing scholarly debate over the ethnicity of Transylvania's population before the Hungarian conquest (see Origin of the Romanians).
The Magyars conquered much of Central Europe at the end of the 9th century. According to Gesta Hungarorum, Transylvania was ruled by Vlach voivode Gelou after the Hungarians arrived. The Kingdom of Hungary established a partial control over Transylvania in 1003, when king Stephen I, according to legend, defeated the prince named "Gyula". Transylvania was occupied by Hungarians in several stages between the 10th and 13th centuries
Between 1003 and 1526, Transylvania was a voivodeship in the Kingdom of Hungary, led by a voivode appointed by the King of Hungary. After the Battle of Mohács in 1526, Transylvania became part of the Kingdom of János Szapolyai which, in 1571, was transformed into the Principality of Transylvania ruled primarily by Calvinist Hungarian-speaking princes. However, several ethnic groups lived in this principality the most numerous being the Romanians, along with a significant German minority. For most of this period, Transylvania, maintaining its internal autonomy, was under the suzerainty of the Ottoman Empire.
The Habsburgs acquired the territory shortly after the Battle of Vienna in 1683. In 1687, the rulers of Transylvania recognized the suzerainty of the Habsburg emperor Leopold I, and the region was officially attached to the Habsburg Empire. The Habsburgs acknowledged Principality of Transylvania as one of the Lands of the Crown of Saint Stephen, but the territory of principality was administratively separated from Habsburg Hungary and subjected to the direct rule of the emperor's governors. In 1699 the Turks legally conceded their loss of Transylvania in the Treaty of Karlowitz; however, some anti-Habsburg elements within the principality submitted to the emperor only in the 1711 Peace of Szatmár, and Habsburg control over Principality of Transylvania was consolidated. In 1765, the Grand Principality of Transylvania was proclaimed.
After the Ausgleich of 1867, the Principality of Transylvania was abolished and its territory was absorbed into Transleithania or the Hungarian part of the newly established Austro-Hungarian Empire.
Following defeat in World War I, Austria-Hungary disintegrated. The ethnic Romanian majority in Transylvania elected representatives, who then proclaimed Union with Romania on December 1, 1918. The "Proclamation of Union" of Alba Iulia was adopted by the Deputies of the Romanians from Transylvania, and supported one month later by the vote of the Deputies of the Saxons from Transylvania. In 1920, the Treaty of Trianon as a result of the war, established a new border between Romania and Hungary, leaving the whole of Transylvania within the Romanian state. Hungary protested against the new borders, as over 1,600,000 Hungarian people and representing 31.6% of the transylvanian population were living on the Romanian side of the border, mainly in Székely Land of Eastern Transylvania, and along the newly created border. In August 1940, Hungary gained about 40% of Transylvania by the Vienna Award, with the arbitration of Germany and Italy. That territory was assigned back by France, Great Britain and the USA to Romania in 1945 and this was confirmed in the 1947 Paris Peace Treaties.
Geography and ethnography.
The Transylvanian plateau, 300 to 500 metres (1,000-1,600 feet) high, is drained by the Mureș, Someș, Criș, and Olt rivers, as well as other tributaries of the Danube. This core of historical Transylvania roughly corresponds with nine counties of modern Romania. Other areas to the west and north, which also united with Romania in 1918 (inside the border established by peace treaties in 1919-20), are since that time widely considered part of Transylvania.
"See also Administrative divisions of the Kingdom of Hungary". In common reference, the Western border of Transylvania has come to be identified with the present Romanian-Hungarian border, settled in the Treaty of Trianon, although geographically the two are not identical.
Administrative divisions.
Bihor 
Arad 
Timiș 
Caraș-Severin 
Hunedoara 
Satu Mare 
Sălaj 
Alba 
Sibiu 
Braşov 
Covasna 
Harghita 
Mureș 
Cluj 
Bistriţa-Năsăud 
Maramureș 
Light yellow – historical region of Transylvania
Dark yellow – historical regions of Banat, Crișana and Maramureș
Grey – historical regions of Wallachia, Moldavia and Dobruja
The area of the historical Voivodeship is 55146 km².
The regions granted to Romania in 1920 covered 23 counties including nearly 102200 km² (102,787–103,093 km² in Hungarian sources and 102,200 km² in contemporary Romanian documents). Nowadays, due to the several administrative reorganisations, the territory covers 16 counties (Romanian: "judeţ"), with an area of 99837 km², in central and northwest Romania.
The 16 counties are: Alba, Arad, Bihor, Bistriţa-Năsăud, Brașov, Caraș-Severin, Cluj, Covasna, Harghita, Hunedoara, Maramureș, Mureș, Sălaj, Satu Mare, Sibiu, and Timiș.
The most populous cities (as of 2011 census):
Population.
Historical population.
Official censuses with information on Transylvania's population have been conducted since the 18th century. On May 1, 1784 the Emperor Joseph II called for the first official census of the Habsburg Empire, including Transylvania. The data was published in 1787, and this census showed only the overall population (1,440,986 inhabitants). , a 19th-century Hungarian statistician, estimated in 1842 that in the population of Transylvania for the years 1830-1840 the majority were 62.3% Romanians and 23.3% Hungarians.
The first official census in Transylvania that made a distinction between nationalities (distinction made on the basis of mother tongue) was performed by Austro-Hungarian authorities in 1869, distributed among the ethnic groups as follows: Romanians 59.0%, Hungarians 24.9%, Germans 11.9%.
In the last quarter of the 19th century, the Hungarian population of Transylvania increased from 24.9% in 1869 to 31.6%, as indicated in the 1910 Hungarian census. At the same time, the percentage of Romanian population decreased from 59.0% to 53.8% and the percentage of German population decreased from 11.9% to 10.7%, for a total population of 5,262,495. Magyarization policies greatly contributed to this shift.
The percentage of Romanian majority has significantly increased since the declaration of the union of Transylvania with Romania after World War I in 1918. The proportion of Hungarians in Transylvania was in steep decline as more of the region's inhabitants moved into urban areas, where the pressure to assimilate and Romanianize was greater. The expropriation of the estates of Magyar magnates, the distribution of the lands to the Romanian peasants, and the policy of cultural Romanianization that followed the Treaty of Trianon were major causes of friction between Hungary and Romania. Other factors include the emigration of non-Romanian peoples, assimilation and internal migration within Romania (estimates show that between 1945 and 1977, some 630,000 people moved from the Old Kingdom to Transylvania, and 280,000 from Transylvania to the Old Kingdom, most notably to Bucharest).
Current population.
The 2002 census classified Transylvania as the entire region of Romania west of the Carpathians. This region has a population of 7,221,733, with a large Romanian majority (75.9%). There are also sizeable Hungarian (19.6%), Roma (3.3%), German (0.7%) and Bulgarians (0.1%) communities. The ethnic Hungarian population of Transylvania, largely composed of Székely, form a majority in the counties of Covasna and Harghita.
The population is mostly of Hungarian origin in Harghita County (84.8%) and Covasna County (73.6%). The Hungarians are also numerous in the following counties: Mureş (37.8%), Satu Mare (34.5%), Bihor (25.2%) and Sălaj (23.2%).
Economy.
Transylvania is rich in mineral resources, notably lignite, iron, lead, manganese, gold, copper, natural gas, salt and sulfur.
There are large iron and steel, chemical, and textile industries. Stock raising, agriculture, wine production and fruit growing are important occupations. Timber is another valuable resource.
IT, electronics and automotive industries are important in urban and university centers like Cluj-Napoca (Nokia), Timișoara (Alcatel-Lucent, Flextronics and Continental AG), Brașov, Sibiu, Oradea and Arad.
Native brands include: Roman of Brașov (trucks and buses), Azomureș of Târgu Mureș (fertilizers), Terapia of Cluj-Napoca (pharmaceuticals), Banca Transilvania of Cluj-Napoca (finance), Romgaz and Transgaz of Mediaș (natural gas), Jidvei of Alba county (alcoholic beverages), Timişoreana of Timișoara (alcoholic beverages) and others.
Transylvania accounts for around 35% of Romania's GDP, and has a GDP per capita (PPP) of around $11,500, around 10% higher than the Romanian average.
Historical coat of arms of Transylvania.
The first heraldic representations of Transylvania date from the 16th century. One of the predominant early symbols of Transylvania was the coat of arms of Sibiu city. In 1596 Levinus Hulsius created a coat of arms for the imperial province of Transylvania, consisting of a shield party per fess, with a rising eagle in the upper field and seven hills with towers on top in the lower field. He published it in his work "Chronologia", issued in Nuremberg the same year. The seal from 1597 of Sigismund Báthory, prince of Transylvania, reproduced the new coat of arms with some slight changes: in the upper field the eagle was flanked by a sun and a moon and in the lower field the hills were replaced by simple towers.
The seal of Michael the Brave from 1600 depicts the territory of the former Dacian kingdom: Wallachia, Moldavia and Transylvania:
The Diet of 1659 codified the representation of the privileged nations in Transylvania's coat of arms. It depicted a black turul on a blue background, representing the Hungarian nobility, a Sun and the Moon representing the Székelys, and seven red towers on a yellow background representing the seven fortified cities of the Transylvanian Saxons. The red dividing band was originally not part of the coat of arms.
Currently, unlike the counties included in it, the region of Transylvania does not have its own official coat of arms. Nonetheless, the historical coat of arms is currently present in the coat of arms of Romania, alongside the traditional coats of arms of the rest of Romania's historical regions.
In culture.
Following the publication of Emily Gerard's "The Land Beyond the Forest" (1888), Bram Stoker wrote his gothic horror novel "Dracula" in 1897, using Transylvania as a setting. With its success, Transylvania became associated in the English-speaking world with vampires. Since then it has been represented in fiction and literature as a land of mystery and magic. For example, in Paulo Coelho's novel "The Witch of Portobello", the main character, Sherine Khalil, is described as a Transylvanian orphan with a Romani mother, in an effort to add to the character's exotic mystique. The so-called Transylvanian trilogy of historical novels by Miklos Banffy, "The Writing on the Wall", is an extended treatment of the 19th- and early 20th-century social and political history of the country. Among the first actors to portray Bram Stoker's Dracula in film was Bela Lugosi, who was born in Banat, in present-day Romania.
The Munsters were also said to be from Transylvania, referring to it several times in the show both by name and as "The Old Country".
References.
K. Horedt (1958) Contribuţii la istoria Transilvaniei în secolele IV-XIII, Editura Academiei RSR, 1958 p. 113

</doc>
<doc id="30867" url="http://en.wikipedia.org/wiki?curid=30867" title="Tatting">
Tatting

Tatting is a technique for handcrafting a particularly durable lace constructed by a series of knots and loops. Tatting can be used to make lace edging as well as doilies, collars, and other decorative pieces. The lace is formed by a pattern of rings and chains formed from a series of cow hitch, or half-hitch knots, called double stitches (ds), over a core thread. Gaps can be left between the stitches to form picots, which are used for practical construction as well as decorative effect.
Tatting dates to the early 19th century. The term for tatting in most European languages is derived from French "frivolité", which refers to the purely decorative nature of the textiles produced by this technique. The technique was developed to imitate point lace
In German tatting is called "Schiffchenarbeit", which means the work of the little boat, referring to the boat-shaped shuttle, and in Italian tatting is called "chiacchierino", which means chatty.
Technique and materials.
Shuttle tatting.
Tatting with a shuttle is the earliest method of creating tatted lace. A tatting "shuttle" facilitates tatting by holding a length of wound thread and guiding it through loops to make the requisite knots. Historically it was a metal or ivory pointed oval shape less than 3 in long, but shuttles come in a variety of shapes and materials. Shuttles often have a point or hook on one end to aid in the construction of the lace. Antique shuttles and unique shuttles have become highly sought after by collectors — even those who do not tat. 
To make the lace, the tatter wraps the thread around one hand and manipulates the shuttle with the other hand. No tools other than the thread, the hands, and the shuttle are used, though a crochet hook may be necessary if the shuttle does not have a point or hook.
Needle tatting.
Traditional shuttle tatting may be simulated using a tatting needle or doll needle instead of a shuttle. There are two basic techniques for needle tatting. With the more widely disseminated technique a double thread passes through the stitches. The result is similar to shuttle tatting but is slightly thicker and looser. The second technique approximates shuttle tatting because a single thread passes through the stitches.
Needle tatting originated in the early twentieth century, but did not become popular until much later. A tatting needle is a long, blunt needle that does not change thickness at the eye of the needle. The needle used must match the thickness of the thread chosen for the project. Rather than winding the shuttle, the needle is threaded with a length of thread. To work with a second color, a second needle is used. Although needle tatting looks similar to shuttle tatting, it differs in structure and is slightly thicker and looser because both the needle and the thread must pass through the stitches.
However, it may be seen that the Victorian tatting pin would function as a tatting needle. As well, Florence Hartley, in 1859, refers in "The Ladies' Hand Book of Fancy and Ornamental Work" to the use of the tatting needle. So it cannot have originated later than the mid-1800's.
In the late twentieth century, tatting needles became commercially available in a variety of sizes, from fingering yarn down to size 80 tatting thread. Few patterns are written specifically for needle tatting; some shuttle tatting patterns may be used without modification. There are currently two manufacturers of tatting needles.
Cro-tatting.
Cro-tatting combines needle tatting with crochet. The cro-tatting tool is a tatting needle with a crochet hook at the end. One can also cro-tat with a bullion crochet hook or a very straight crochet hook. In the nineteenth century, "crochet tatting" patterns were published which simply called for a crochet hook. One of the earliest patterns is for a crocheted afghan with tatted rings forming a raised design. Patterns are available in English and are equally divided between yarn and thread. In its most basic form the rings are tatted with a length of plain thread between them, as in single shuttle tatting. In modern patterns, beginning in the early twentieth century, the rings are tatted and the arches or chains are crocheted. Many people consider cro-tatting more difficult than crochet or needle tatting. Some tatting instructors recommend using a tatting needle and a crochet hook to work cro-tatting patterns. Cro-tatting is most popular in Japan.
The stitches of cro-tatting (and needle tatting before you close a ring) unravel easily, unlike tatting that is made with a shuttle.
Materials.
Older designs, especially through the early 1900s, tend to use fine white or ivory thread (50 to 100 widths to the inch) and intricate designs. Often they were constructed of small pieces 10 cm or less in diameter which were then tied to each other to form a larger piece -- shawl, veil, umbrella. This thread was either made of silk or a silk blend, to allow for improper stitches to be easily removed. The mercerization process strengthened cotton threads and spread their use in tatting. Newer designs from the 1920s and onward often use thicker thread in one or more colors, and newer joining methods to reduce the number of thread ends to be hidden. The best thread for tatting is a "hard" thread that does not untwist readily. DMC Cordonnet thread is a common tatting thread; Perl cotton is an example of a beautiful cord that is nonetheless a bit loose for tatting purposes. Some tatting designs incorporate ribbons and beads.
Patterns.
Older patterns use a long hand notation to describe the stitches needed while newer patterns tend to make extensive use of abbreviations and an almost mathematical looking notation. The following examples describe the same small piece of tatting (the first Ring in the "Hen and Chicks" pattern)
Some tatters prefer a visual pattern where the design is drawn schematically with annotations indicating the number of ds and order of construction. This can either be used on its own or alongside a written pattern.
History.
Some believe that tatting may have developed from netting and decorative ropework as sailors and fishers would put together motifs for girlfriends and wives at home. Decorative ropework employed on ships includes techniques (esp. coxcombing) that show striking similarity with tatting. A good description of this can be found in "Knots, Splices and Fancywork".
Some believe tatting originated over 200 years ago, often citing shuttles seen in eighteenth century paintings of women such as Charlotte of Mecklenburg-Strelitz, Madame Adelaide (daughter of Louis XV of France), and Anne, Countess of Albemarle. A close inspection of those paintings shows that the shuttles in question are too large to be tatting shuttles, and that they are actually knotting shuttles. There is no documentation, nor any examples of tatted lace, that date prior to 1800. All of the available evidence shows that tatting originated in the early 19th century.
As most fashion magazines and home economics magazines from the first half of the 20th century attest, tatting had a substantial following. When fashion included feminine touches such as lace collars and cuffs, and inexpensive yet nice baby shower gifts were needed, this creative art flourished. As the fashion moved to a more modern look and technology made lace an easy and inexpensive commodity to purchase, hand-made lace began to decline.
Tatting has been used in occupational therapy to keep convalescent patients' hands and minds active during recovery, as documented, for example, in Betty MacDonald's "The Plague & I".

</doc>
<doc id="30870" url="http://en.wikipedia.org/wiki?curid=30870" title="Transport in Afghanistan">
Transport in Afghanistan

Transport in Afghanistan is limited and in the developing stage. Much of the nation's road network was built during the 1960s but left to ruin during the 1980s and 90s wars. New national highways, roads, and bridges have been rebuilt in the last decade to help increase travel as well as trade with neighboring countries. In 2008, there were about 731,607 vehicles registered inside the country, which serve a 29 million people.
Landlocked Afghanistan has no seaports but the Amu Darya river, which forms part of the nation's border with Turkmenistan, Uzbekistan, and Tajikistan, does have barge traffic. Rebuilding of airports, roads, and a railway line has led to rapid economic boost recent years. The nation has about 53 airports and a number of heliports.
Highways and roads.
Most major roads were built in the 1960s with assistance from the United States and the Soviet Union. The Soviets built a road and tunnel through the Salang pass in 1964, connecting northern and southern Afghanistan. A highway connecting the principal cities of Herat, Kandahar, Ghazni, and Kabul with links to highways in neighboring Pakistan formed the primary road system.
The network includes 12,350 kilometers of paved roads and 29,800 kilometers of unpaved roads, for an approximate total road system of 42,150 kilometers as of 2006. Traffic in Afghanistan is right hand, with about 731,607 registered vehicles in the country. The Afghan government passed a law banning the import of cars older than 10 years
Long distant road journeys are made by company-owned Mercedes-Benz coach buses or various types of vans, trucks and private cars. Although nationwide bus service is available between major cities, flying is safer, especially for foreigners. There are occasional highway robberies by bandits or militant groups. The roads are also dangerous due to accidents and lack of security forces.
Major highways.
The highway system is currently going through a total reconstruction phase. Most of the regional roads are also being repaired or improved. For the last 30 years, the poor state of the Afghan transportation and communication networks have further fragmented and hampered the struggling economy.
Since the fall of the Taliban many roads have been rebuilt, including the following:
A road bridge linking Tajikistan and Afghanistan which cost $37 million was inaugurated in 2007. The bridge, nearly 700 metres long and 11 metres across, straddles the Panj river which forms a natural border between the two countries, between the ports of Nizhny Panj on the Tajik side and Shir Khan Bandar in Afghanistan. The Delaram-Zaranj highway was constructed with Indian assistance and was inaugurated in January 2009.
Railways.
Afghanistan-Uzbekistan rail service.
There is a 75 kilometres railway service between Uzbekistan and the northern Afghan city of Mazar-i-Sharif, all of which is built to broad gauge. The line begins from Termez and crosses the Amu Darya river on the Soviet-built Afghanistan–Uzbekistan Friendship Bridge, finally reaching a site next to the Mazar-i-Sharif Airport. The Afghan government expects to have the rail line extended to Kabul and then to the eastern border town of Torkham, connecting with Pakistan Railways. The work is carried out by China Metallurgical Group Corporation (MCC) and is expected to be completed by 2014. For strategic reasons, past Afghan governments preferred to discourage the construction of railways which could aid foreign interference in Afghanistan by Britain or Russia.
Turkmenistan border.
A 10 kilometer long broad gauge line extends from Serhetabat in Turkmenistan to the town of Towraghondi in Afghanistan. An upgrade of this Soviet-built line began in 2007.
Iranian border.
The nearest railhead in Iran is a standard gauge line which terminates at Mashhad. This line is being extended 191 kilometers east to Herat, of which 77 km is located inside Iran and the remaining 114 km in Afghanistan.
Pakistan border.
Two broad gauge Pakistan Railways lines terminate near the border at Chaman in Balochistan near the Khojak Pass; and at Torkham, the border town near the Khyber Pass. Various proposals exist to extend these lines on to Kandahar and Kabul respectively. In July 2010, Pakistan and Afghanistan signed a Memorandum of understanding for going ahead with the laying of rail tracks between the two countries. Work on the proposed project was set to start in late 2010.
Other borders.
There are no rail links to China or Tajikistan, though a connection to the latter was proposed in 2008.
Pipelines.
There are petroleum pipelines from Bagram into Uzbekistan and Shindand into Turkmenistan. These pipelines have been in disrepair and disuse for years. There are 180 kilometers of natural gas pipelines. The $3 billion Trans-Afghanistan Pipeline proposal for a natural gas pipeline across Afghanistan into Pakistan is moving forward.
Ports and harbours.
The chief inland waterway of land-locked Afghanistan is the Amu Darya River which forms part of Afghanistan's northern border. The river handles barge traffic up to about 500 metric tons. The main river ports are located at Kheyrabad and Shir Khan Bandar.
Air transport.
Air transport in Afghanistan is provided by the national carrier Ariana Afghan Airlines, and by private companies such as Afghan Jet International, East Horizon Airlines, Kam Air, Pamir Airways, and Safi Airways. Airlines from a number of nations also provide air services to fly in and out of the country. These include Air India, Emirates, Gulf Air, Iran Aseman Airlines, Pakistan International Airlines, Turkish Airlines and others.
Kabul International Airport is the country's main airport. As of May, 2014 the country had four international airports (Herat International Airport , Kabul International Airport, Kandahar International Airport, Mazari Sharif Airport) and around a dozen domestic airports which had regularly scheduled flights to Kabul and/or Herat. 
The nation has approximately 53 airports, about 19 of these have paved runways. Of those, 4 have runways over 3,000 meters, 3 have runways between about 2,500 and 3,000 meters, 8 have runways between 1500 and 2500 meters, and 2 has a runway under 1000 meters. About 34 have unpaved runways. Of those, 5 have runways between 2500 and 3000 meters, 14 have runways between 1500 and 2500 meters, 6 have runways between 1000 and 1500 meters, and 9 have runways under 1000 meters.
Bagram Air Base is used by NATO-led forces. It has heavy military traffic, especially helicopters. It can also handle larger airliners such as Boeing 747s, C-5 Galaxy and C-17 Globemaster III military cargo planes. KBR and some other companies fly into and out of Bagram on a regular basis.
Heliports.
There are at least eleven heliports.
External links.
 This article incorporates public domain material from the document .

</doc>
<doc id="30984" url="http://en.wikipedia.org/wiki?curid=30984" title="Toyota">
Toyota

Toyota Motor Corporation (Japanese: トヨタ自動車株式会社, Hepburn: Toyota Jidōsha KK, ], ) is a Japanese automotive manufacturer headquartered in Toyota, Aichi, Japan. In March 2014 the multinational corporation consisted of 338,875 employees worldwide and, as of November 2014, is the twelfth-largest company in the world by revenue. Toyota was the largest automobile manufacturer in 2012 (by production) ahead of the Volkswagen Group and General Motors. In July of that year, the company reported the production of its 200-millionth vehicle. Toyota is the world's first automobile manufacturer to produce more than 10 million vehicles per year. It did so in 2012 according to OICA, and in 2013 according to company data. s of July 2014[ [update]], Toyota was the largest listed company in Japan by market capitalization (worth more than twice as much as #2-ranked SoftBank) and by revenue.
The company was founded by Kiichiro Toyoda in 1937 as a spinoff from his father's company Toyota Industries to create automobiles. Three years earlier, in 1934, while still a department of Toyota Industries, it created its first product, the Type A engine, and, in 1936, its first passenger car, the Toyota AA. Toyota Motor Corporation produces vehicles under 5 brands, including the Toyota brand, Hino, Lexus, Ranz, and Scion. It also holds a 51.2% stake in Daihatsu, a 16.66% stake in Fuji Heavy Industries, a 5.9% stake in Isuzu, and a 0.27% stake in Tesla, as well as joint-ventures with two in China (GAC Toyota and Sichuan FAW Toyota Motor), one in India (Toyota Kirloskar), one in the Czech Republic (TPCA), along with several "nonautomotive" companies. TMC is part of the Toyota Group, one of the largest conglomerates in the world.
Corporate governance.
Toyota is headquartered in Toyota City, Aichi. The main headquarters of Toyota is located in a four-story building in Toyota. As of 2006 the head office has the "Toyopet" Toyota logo and the words "Toyota Motor". The Toyota Technical Center, a 14-story building, and the Honsha plant, Toyota's second plant engaging in mass production and formerly named the Koromo plant, are adjacent to one another in a location near the headquarters. Vinod Jacob from "The Hindu" described the main headquarters building as "modest". In 2013 company head Akio Toyoda reported that it had difficulties retaining foreign employees at the headquarters due to the lack of amenities in the city.
Its Tokyo office is located in Bunkyo, Tokyo. Its Nagoya office is located in Nakamura-ku, Nagoya. In addition to manufacturing automobiles, Toyota provides financial services through its Toyota Financial Services division, and also builds robots.
President of Toyota Motor Company:
In 1981, Toyota Motor Co., Ltd. announced plans to merge with its sales entity Toyota Motor Sales Co., Ltd. Since 1950, the two entities existed as separate companies due to a prerequisite for reconstruction in postwar Japan. Shoichiro Toyoda presided over Toyota Motor Sales in preparation for the consummation of the merger that occurred in 1982—Shoichiro then succeeded his uncle Eiji as the President of the combined organization that then became known as Toyota Motor Corporation.
President of Toyota Motor Corporation:
CEO of Toyota Motor Corporation:
Chairman of Toyota Motor Corporation:
On June 14, 2013, Toyota Motor Corp. announced the appointment of outside board members; the appointment was a first for the corporation and occurred following approval from general shareholders at a meeting on the same day. Additionally, Vice Chairman Takeshi Uchiyamada replaced Fujio Cho as chairman, as the latter became an honorary chairman, while Toyoda remains in the post of President.
Toyota is publicly traded on the Tokyo, Osaka, Nagoya, Fukuoka, and Sapporo exchanges under company code TYO: . In addition, Toyota is foreign-listed on the New York Stock Exchange under NYSE: [ TM] and on the London Stock Exchange under LSE: . Toyota has been publicly traded in Japan since 1949 and internationally since 1999.
As reported on its consolidated financial statements, Toyota has 540 consolidated subsidiaries and 226 affiliates.
Financial results.
In 2011, the Toyota Group (including Daihatsu, Hino and Chinese joint ventures) fell to place three with 8,050,181 units produced globally. According to an unofficial count, based on unit production reported by major automakers, Toyota regained its top rank with 9,909,440 units produced globally in calendar year 2012. 
On May 8, 2013, Toyota announced plans to produce 10.1 million units in fiscal year 2013, which, if achieved, would make it the first auto manufacturer to cross the 10-million-unit threshold.
On May 8, 2009, Toyota reported a record annual net loss of US$, making it the latest automobile maker to be severely affected by the global financial crisis that started in 2007. Toyota's financial unit had asked for an emergency loan from a state-backed lender on March 16, 2009, with reports putting the figure at more than . It said the international financial situation was squeezing its business, forcing it to ask for an emergency loan from the Japan Bank for International Cooperation. This was the first time the state-backed bank has been asked to lend to a Japanese car manufacturer.
On May 8, 2013, Toyota Motor Corporation announced its financial results for the fiscal year ended March 31, 2013. Net revenues totaled (+18.7%). Operating income was (+371%), net income (+239%).
History.
1930s.
In 1924, Sakichi Toyoda invented the Toyoda Model G Automatic Loom. The principle of "jidoka", which means the machine stops itself when a problem occurs, became later a part of the Toyota Production System. Looms were built on a small production line. In 1929, the patent for the automatic loom was sold to a British company, generating the starting capital for the automobile development.
Toyota was started in 1933 as a division of Toyoda Automatic Loom Works devoted to the production of automobiles under the direction of the founder's son, Kiichiro Toyoda. Its first vehicles were the A1 passenger car and the G1 in 1935. The Toyota Motor Co. was established as an independent company in 1937. In 2008, Toyota's sales surpassed General Motors, making Toyota number one in the world.
Vehicles were originally sold under the name "Toyoda" (トヨダ), from the family name of the company's founder, Kiichirō Toyoda. In April 1936, Toyoda's first passenger car, the Model AA, was completed. The sales price was 3,350 yen, 400 yen cheaper than Ford or GM cars.
In September 1936, the company ran a public competition to design a new logo. Of 27,000 entries, the winning entry was the three Japanese "katakana" letters for "Toyoda" in a circle. But Risaburō Toyoda, who had married into the family and was not born with that name, preferred "Toyota" (トヨタ) because it took eight brush strokes (a lucky number) to write in Japanese, was visually simpler (leaving off the diacritic at the end), and with a voiceless consonant instead of a voiced one (voiced consonants are considered to have a "murky" or "muddy" sound compared to voiceless consonants, which are "clear").
Since "toyoda" literally means "fertile rice paddies", changing the name also prevented the company from being associated with old-fashioned farming. The newly formed word was trademarked and the company was registered in August 1937 as the Toyota Motor Company.
1940s–1950s.
From September 1947, Toyota's small-sized vehicles were sold under the name "Toyopet" (トヨペット). The first vehicle sold under this name was the Toyopet SA, but it also included vehicles such as the Toyopet SB light truck, Toyopet Stout light truck, Toyopet Crown, Toyopet Master, and the Toyopet Corona. The word "" was a nickname given to the Toyota SA due to its small size, as the result of a naming contest the Toyota Company organized in 1947. However, when Toyota eventually entered the American market in 1957 with the Crown, the name was not well received due to connotations of toys and pets. The name was soon dropped for the American market, but continued in other markets until the mid-1960s.
1960s–1970s.
By the early 1960s, the US had begun placing stiff import tariffs on certain vehicles. The so-called "chicken tax" of 1964 placed a 25% tax on imported light trucks. In response to the tariff, Toyota, Nissan Motor Co. and Honda Motor Co. began building plants in the US by the early 1980s.
1980s.
Toyota received its first Japanese Quality Control Award at the start of the 1980s and began participating in a wide variety of motorsports. Due to the 1973 oil crisis, consumers in the lucrative US market began turning to small cars with better fuel economy. American car manufacturers had considered small economy cars to be an entry-level product, and their small vehicles employed a low level of quality to keep the price low.
In 1982, the Toyota Motor Company and Toyota Motor Sales merged into one company, the Toyota Motor Corporation. Two years later, Toyota entered into a joint venture with General Motors called the New United Motor Manufacturing, Inc, NUMMI, operating an automobile-manufacturing plant in Fremont, California. The factory was an old General Motors plant that had been closed for two years. Toyota then started to establish new brands at the end of the 1980s, with the launch of their luxury division Lexus in 1989.
1990s.
In the 1990s, Toyota began to branch out from producing mostly compact cars by adding many larger and more luxurious vehicles to its lineup, including a full-sized pickup, the T100 (and later the Tundra); several lines of SUVs; a sport version of the Camry, known as the Camry Solara; and the Scion brand, a group of several affordable, yet sporty, automobiles targeted specifically to young adults. Toyota also began production of the world's best-selling hybrid car, the Prius, in 1997.
With a major presence in Europe, due to the success of Toyota Team Europe, the corporation decided to set up Toyota Motor Europe Marketing and Engineering, TMME, to help market vehicles in the continent. Two years later, Toyota set up a base in the United Kingdom, TMUK, as the company's cars had become very popular among British drivers. Bases in Indiana, Virginia, and Tianjin were also set up. In 1999, the company decided to list itself on the New York and London Stock Exchanges.
2000s.
In 2001, Toyota's Toyo Trust and Banking merged with two other banks to form UFJ Bank, which was accused of corruption by Japan's government for making bad loans to alleged Yakuza crime syndicates with executives accused of blocking Financial Service Agency inspections. The UFJ was listed among "Fortune Magazine"'s largest money-losing corporations in the world, with Toyota's chairman serving as a director. At the time, the UFJ was one of the largest shareholders of Toyota. As a result of Japan's banking crisis, UFJ merged with the Bank of Tokyo-Mitsubishi to become the Mitsubishi UFJ Financial Group.
In 2002, Toyota managed to enter a Formula One works team and establish joint ventures with French motoring companies Citroën and Peugeot a year after Toyota started producing cars in France.
Toyota ranked eighth on Forbes 2000 list of the world's leading companies for the year 2005 but slid to 55 for 2011. The company was number one in global automobile sales for the first quarter of 2008.
In 2007, Toyota released an update of its full-sized truck, the Tundra, produced in two American factories, one in Texas and one in Indiana. "Motor Trend" named the Tundra "Truck of the Year", and the 2007 Toyota Camry "Car of the Year" for 2007. It also began the construction of two new factories, one to build the RAV4 in Woodstock, Ontario, Canada, and the other to build the Toyota Prius in Blue Springs, Mississippi, USA. This plant was originally intended to build the Toyota Highlander, but Toyota decided to use the plant in Princeton, Indiana, USA, instead. The company has also found recent success with its smaller models—the Corolla and Yaris—as gasoline prices have risen rapidly in the last few years.
2010s.
In 2011, Toyota, along with large parts of the Japanese automotive industry, suffered from a series of natural disasters. The 2011 Tōhoku earthquake and tsunami led to a severe disruption of the supplier base and a drop in production and exports. Severe flooding during the 2011 monsoon season in Thailand affected Japanese automakers that had chosen Thailand as a production base. Toyota is estimated to have lost production of 150,000 units to the tsunami and production of 240,000 units to the floods.
The automaker narrowly topped global sales for the first half of 2014, selling 5.1 million vehicles in the six months ending June 30, 2014, an increase of 3.8% on the same period the previous year. Volkswagen AG, which recorded sales of 5.07 million vehicles, was close behind.
In August 2014, Toyota announced it would be cutting its spare-parts prices in China by up to 35%. The company admitted the move was in response to a probe foreshadowed earlier in the month by China's National Development and Reform Commission of Toyota's Lexus spare-parts policies, as part of an industry-wide investigation into what the Chinese regulator considers exorbitantly high prices being charged by automakers for spare parts and after-sales servicing.
Recalls.
2009.
From November 2009 through 2010, Toyota recalled more than 9 million cars and trucks worldwide in several recall campaigns, and briefly halted production and sales. Toyota initiated the recalls, the first two with the assistance of the U.S. National Highway Traffic Safety Administration (NHTSA), after reports that several vehicles experienced unintended acceleration.
2012.
In October 2012, Toyota announced a recall of 7.43 million vehicles worldwide to fix malfunctioning power window switches, the largest recall since that of Ford Motor Company in 1996. The move came after a series of recalls between 2009 and 2011 in which it pulled back around 10 million cars amidst claims of faulty mechanics. In March 2014, Toyota agreed to pay a fine of US$ for concealing information and misleading the public about the safety issues behind the recalls on Toyota and Lexus vehicles affected by unintended acceleration.
2014.
In early November 2014, Toyota USA enlisted a recall involving defective inflaters and propellant devices that may deploy improperly in the event of a crash, shooting metal fragments into vehicle occupants. More than 7 million vehicles are potentially affected in the United States. This recall only effects vehicles equipped with Takata airbags released after the year 2000 in North America. The airbags were manufactured by Takata automotive manufacturing. Toyota is offering a free repair to all effected vehicles worldwide. The fault in the Takata air bags also affected other North American automobile manufacturers.
Logo and branding.
In 1936, Toyota entered the passenger car market with its Model AA and held a competition to establish a new logo emphasizing speed for its new product line. After receiving 27,000 entries, one was selected that additionally resulted in a change of its moniker to "Toyota" from the family name "Toyoda". The new name was believed to sound better, and its eight-stroke count in the Japanese language was associated with wealth and good fortune. The original logo no longer is found on its vehicles, but remains the corporate emblem used in Japan.
Still, no guidelines existed for the use of the brand name, so "TOYOTA", which was used throughout most of the world, led to inconsistencies in its worldwide marketing campaigns.
To remedy this, Toyota introduced a new worldwide logo in October 1989 to commemorate the 50th year of the company, and to differentiate it from the newly released luxury Lexus brand. The logo made its debut on the 1989 Toyota Celsior and quickly gained worldwide recognition. The three ovals in the new logo combine to form the letter "T", which stands for Toyota. The overlapping of the two perpendicular ovals inside the larger oval represent the mutually beneficial relationship and trust between the customer and the company, while the larger oval surrounding both of these inner ovals represents the "global expansion of Toyota's technology and unlimited potential for the future."
The new logo started appearing on all printed material, advertisements, dealer signage, and the vehicles themselves in 1990.
In predominantly Chinese-speaking countries or regions using traditional Chinese characters, e.g. Hong Kong and Taiwan, Toyota is known as "豐田". In predominantly Chinese-speaking countries using simplified Chinese characters (e.g. China and Singapore), Toyota is known as "丰田" (pronounced as "Fēngtián" in Mandarin Chinese and "Hɔng Tshan" in Minnanese). These are the same characters as the founding family's name "Toyoda" in Japanese.
Marketing.
Toyota's marketing efforts in North America have focused on emphasizing the positive experiences of ownership and vehicle quality. The ownership experience has been targeted in slogans such as "You asked for it! You got it!" (1975–1979), "Oh, what a feeling!" (1979 – September 1985, in the US), "Who could ask for anything more?" (September 1985 – 1989), "I love what you do for me, Toyota!" (1989–1997), "Everyday" (1997–2001)", "Get the feeling!" (2001–2004), "Moving Forward" (2004–2012), and "Let's Go Places" (2012–present).
Japan.
In Japan, Toyota currently maintains separate dealership sales channels. The first sales channel established in 1946 called "Toyota Store" (トヨタ店 "Toyota Mise") sells large luxury sedans such as the Toyota Century, and the Toyota Crown. In 1955 the "Toyopet Store" (トヨペット店 "Toyopetto-ten") arrived, originally established to sell the Toyota Corona and the Toyopet ToyoAce truck. ( in Japanese.) Toyota also operated a commercial dealership called (トヨタディーゼル店 "Toyota Dīzeru-ten") from 1957 until 1988, that sold various commercial platform trucks, buses, and forklifts, such as the Toyota Dyna and the Toyota Coaster. Hino products were sold at specific Hino locations, and shared at Toyota Diesel Store locations after Toyota acquired the company in 1967. Starting in 1980, the Diesel Shop also sold the Starlet, Corolla, Corona, Vista and Crown installed with diesel engines. When the Toyota Diesel Store was disbanded, commercial products were divided between Toyota Store and Toyopet Store locations.
Currently, the "Toyota Corolla Store" (トヨタカローラ店 "Toyota Karōra-ten") was renamed from the "Toyota Publica Store" (トヨタパブリカ店 "Toyota Paburika-ten"), which was established to sell the Toyota Publica in 1961, then renamed to sell the Toyota Corolla in 1966.
In 1980, the "Toyota Vista Store" replaced the "Toyota Auto Store" (トヨタオート店 "Toyota Ōto-ten") sales network that sold the Corolla companion, called the Toyota Sprinter established in 1967. The "Vista" name was used on a new Camry-clone, called the Toyota Vista. The Toyota Vista network was replaced with two networks; "Toyota NETZ" (ネッツ店 "Nettsu-ten") in August 1998, and Lexus in 2004. Some former Vista models were rebranded as Lexus (レクサス "Rekusasu"), such as the Altezza and the Aristo, while other products have been taken over by the "Toyota NETZ", which was already selling the Toyota ist and the Toyota RAV4. "NETZ" is an acronym for "Network of Energetic Teams for Zenith".
NETZ locations have been repositioned to resemble the North American Toyota network, called Scion, and are currently exclusive for the Toyota 86. Most models were exclusive to particular retail chains, while some models, like the Prius, are available at all sales channels.
The following is a list of all past and present models and where they were available at retail channels nationally, as retail chains in Tokyo and Osaka are different.
Century, Crown Majesta, Crown, Master, SAI, Prius, Aqua, Allion, Succeed, Blade, Corolla RunX, Porte, Estima, Isis, FJ Cruiser, Comfort, Land Cruiser, Hilux Surf, Land Cruiser Prado, Dyna, Stout, Coaster, QuickDelivery, 2000GT, Carina, Carina ED, Brevis, Gaia, Cavalier, Classic, MasterAce, Hilux, Mega Cruiser, Soarer, Origin, Caldina.
Mark X, SAI, Premio, Prius, Aqua, Belta, Mark X ZiO, Succeed, Ractis, Blade, Porte, Harrier, Vanguard, Rush, Alphard, Comfort, HiAce, ToyoAce, Pixis Space, Mark II-Mark II Qualis-Mark II Blit, Corona, Corona EXiV, Corona Coupe, Corsa, Opa, Avalon, Progrès, Cami, ist, Platz, Soarer, Hilux, Cynos, Regius, Celsior, Origin, Caldina, Ipsum.
SAI, Camry, Prius, Aqua, Corolla Axio, Belta, Probox, Corolla Rumion, Ractis, Passo, Sera, Vanguard, Estima, Noah, Sienta, TownAce, all Daihatsu products, Publica, Tercel, Windom, Scepter, Corolla Ceres, Origin, Nadia, WiLL, RAV4, Sports 800, Celica, Supra, Corolla Levin, Celica XX.
Vitz, SAI, Prius, Aqua, ist, Auris, bB, Avensis, Raum, Wish, Voxy, RAV4, Kluger, Vellfire, iQ, Allex, Fun Cargo, Altezza, Verossa, Curren, Aristo, MR-S, MR2, Starlet, Vista, Cresta, Sprinter, Voltz, Blizzard, Chaser, Sprinter Marino, Carib, Granvia, Sprinter Trueno, LiteAce, Ipsum, GT-86, WiLL (1999-2004).
Sports.
Toyota sponsors several teams and has purchased naming rights for several venues, including:
Company strategy.
Toyota's management philosophy has evolved from the company's origins and has been reflected in the terms "Lean Manufacturing" and Just In Time Production, which it was instrumental in developing. Toyota's managerial values and business methods are known collectively as the Toyota Way.
In April 2001, Toyota adopted the "Toyota Way 2001", an expression of values and conduct guidelines that all Toyota employees should embrace. Under the two headings of Respect for People and Continuous Improvement, Toyota summarizes its values and conduct guidelines with these five principles:
According to external observers, the Toyota Way has four components:
The Toyota Way incorporates the Toyota Production System.
Operations.
Toyota has long been recognized as an industry leader in manufacturing and production. Three stories of its origin have been found, one that they studied Piggly-Wiggly's just-in-time distribution system, one that they followed the writings of W. Edwards Deming, and one that they were given the principles from a WWII US government training program (Training Within Industry).
As described by external observers of Toyota, the principles of the Toyota Way are:
Toyota has grown to a large multinational corporation from where it started and expanded to different worldwide markets and countries. It displaced GM and became the world's largest automobile maker for the year 2008. It held the title of the most profitable automobile maker (US$ in 2006) along with increasing sales in, among other countries, the United States. The world headquarters of Toyota are located in its home country in Toyota City, Japan. Its subsidiary, Toyota Financial Services sells financing and participates in other lines of business. Toyota brands include Scion and Lexus and the corporation is part of the Toyota Group. Toyota also owns 51% of Daihatsu, and 16.7% of Fuji Heavy Industries, which manufactures Subaru vehicles. They also acquired 5.9% of Isuzu Motors Ltd. on November 7, 2006 and will be introducing Isuzu diesel technology into their products.
Toyota has introduced new technologies, including one of the first mass-produced hybrid gasoline-electric vehicles, of which it says it has sold 2 million globally as of 2010, Advanced Parking Guidance System (automatic parking), a four-speed electronically controlled automatic with buttons for power and economy shifting, and an eight-speed automatic transmission. Toyota, and Toyota-produced Lexus and Scion automobiles, consistently rank near the top in certain quality and reliability surveys, primarily J.D. Power and Consumer Reports although they led in automobile recalls for the first time in 2009.
In 2005, Toyota, combined with its half-owned subsidiary Daihatsu Motor Company, produced 8.54 million vehicles, about 500,000 fewer than the number produced by GM that year. Toyota has a large market share in the United States, but a small market share in Europe. Its also sells vehicles in Africa and is a market leader in Australia. Due to its Daihatsu subsidiary it has significant market shares in several fast-growing Southeast Asian countries.
According to the 2008 Fortune Global 500, Toyota is the fifth largest company in the world. Since the recession of 2001, it has gained market share in the United States. Toyota's market share struggles in Europe where its Lexus brand has 0.3% market share, compared to nearly 2% market share as the US luxury segment leader.
In the first three months of 2007, Toyota together with its half-owned subsidiary Daihatsu reported number one sales of 2.348 million units. Toyota's brand sales had risen 9.2% largely on demand for Corolla and Camry sedans. The difference in performance was largely attributed to surging demand for fuel-efficient vehicles. In November 2006, Toyota Motor Manufacturing Texas added a facility in San Antonio. Toyota has experienced quality problems and was reprimanded by the government in Japan for its recall practices. In 2007, Toyota maintained over 16% of the US market share and was listed second only to GM in terms of volume. Toyota Century Royal is the official state car of the Japanese imperial family, namely for the current Emperor of Japan.
Toyota was hit by the global financial crisis of 2008 as it was forced in December 2008 to forecast its first annual loss in 70 years.
In January 2009, it announced the closure of all of its Japanese plants for 11 days to reduce output and stocks of unsold vehicles.
Akio Toyoda became the new president and CEO of the company on June 23, 2009, by replacing Katsuaki Watanabe, who became the new vice chairman by replacing Katsuhiro Nakagawa.
Worldwide presence.
Toyota has factories in most parts of the world, manufacturing or assembling vehicles for local markets in Japan, Australia, India, Sri Lanka, Canada, Indonesia, Poland, South Africa, Turkey, Colombia, the United Kingdom, the United States, France, Brazil, Portugal, and more recently, Argentina, Czech Republic, Mexico, Malaysia, Thailand, Pakistan, Egypt, China, Vietnam, Venezuela, the Philippines, and Russia.
Toyota's net revenue by geographical regions for the year ended March 31, 2007:
In 2002, Toyota initiated the Innovative International Multi-purpose Vehicle project (IIMV) to optimize global manufacturing and supply systems for pickup trucks and multipurpose vehicles, and to satisfy market demand in more than 140 countries worldwide. IIMV called for diesel engines to be made in Thailand, gasoline engines in Indonesia, and manual transmissions in India and the Philippines, for supply to the countries charged with vehicle production. For vehicle assembly, Toyota would use plants in Thailand, Indonesia, Argentina, and South Africa. These four main IIMV production and export bases supply Asia, Europe, Africa, Oceania, Latin America, and the Middle East with three vehicles: The Toyota Hilux (Vigo), the Fortuner, and the Toyota Innova.
North America.
Toyota Motor North America headquarters is located in New York City, NY, and operates as a holding company in North America. Its manufacturing headquarters is located in Erlanger, Kentucky, and is known as Toyota Motor Engineering & Manufacturing North America.
Toyota Canada Inc. has been in production in Canada since 1983 with an aluminium wheel plant in Delta, British Columbia, which currently employs a workforce of roughly 260. Its first vehicle assembly plant, in Cambridge, Ontario, since 1988, now produces Corolla compact cars, Matrix crossover vehicles, and Lexus RX 350 luxury SUVs, with a workforce of 4,300 workers. Its second assembly operation in Woodstock, Ontario, began manufacturing the RAV4 late in 2008. In 2006, Toyota's subsidiary Hino Motors opened a heavy duty truck plant, also in Woodstock, employing 45 people and producing 2000 trucks annually.
Toyota has a large presence in the United States with six major assembly plants in Huntsville, Alabama, Georgetown, Kentucky, Princeton, Indiana, San Antonio, Texas, Buffalo, West Virginia, and Blue Springs, Mississippi. Toyota had a joint-venture operation with General Motors at New United Motor Manufacturing Inc. in Fremont, California, which began in 1984 and ended in 2009. It still has a joint venture with Subaru at Subaru of Indiana Automotive, Inc. in Lafayette, Indiana, which started in 2006. In these assembly plants, the Camry and the Tundra are manufactured, among others.
Toyota marketing, sales, and distribution in the US are conducted through a separate subsidiary, Toyota Motor Sales, U.S.A., Inc. It has started producing larger trucks, such as the new Tundra, to go after the large truck market in the United States. Toyota is also pushing hybrid vehicles in the US such as the Prius, Camry Hybrid, Highlander Hybrid, and various Lexus products. Currently, Toyota has no plans to offer diesel motor options in its North American products, including the light-duty pickup trucks.
Australia.
In 1963, Australia was one of the first countries to assemble Toyotas outside Japan. However, in February 2014, Toyota was the last of Australia's major automakers to announce the end of production in Australia. The closure of Toyota's Australian plant will be completed by 2017. Before Toyota, Ford and GM's Holden had announced similar moves, all citing an unfavorable currency and attendant high manufacturing costs.
Product line.
Electric technology.
Hybrid electric vehicles.
Toyota is one of the largest companies to push hybrid electric vehicles in the market and the first to commercially mass-produce and sell such vehicles, with the introduction of the Toyota Prius in 1997. The company eventually began providing this option on the main smaller cars such as Camry and later with the Lexus divisions, producing some hybrid luxury vehicles. It labeled such technology in Toyota cars as "Hybrid Synergy Drive" and in Lexus versions as "Lexus Hybrid Drive."
s of 2013[ [update]], Toyota Motors Corporation sells 24 Toyota and Lexus hybrid models and one plug-in hybrid in about 80 countries and regions around the world, and the carmaker has plans to introduce 15 new hybrid models before the end of 2015. The Prius liftback is the top selling hybrid gasoline-electric car in world, with cumulative sales of 3 million units since its introduction in 1997 through June 2013. The United States was the world's largest hybrid market, and TMC's second, with over 2 million TMC hybrids sold through August 2013, representing 70% of the American hybrid market. The Prius liftback ranks as the top selling hybrid car in the U.S. market, and surpassed the 1 million milestone in April 2011. Cumulative sales of the Prius in Japan reached the 1 million mark in August 2011 and the 2 million mark was reached in October 2012. s of 2013[ [update]], Japan is Toyota's largest hybrid market, with 2.814 million Toyota and Lexus hybrids sold, followed by the United States with 2.302 million units. Europe surpassed the 500,000 sales mark in December 2012, and as of 2013[ [update]], TMC hybrid sales totaled 646,6 thousand units.
Worldwide sales of hybrid vehicles produced by Toyota reached 1.0 million vehicles by May 31, 2007, and the 2.0 million mark was reached by August 2009, with hybrids sold in 50 countries. The 5 million hybrid sales milestone was reached in March 2013, and Toyota estimates that up to 31 March 2013, its hybrids have saved about 3 billion gallons of gasoline (11.356 billion liters) of gasoline compared to the amount used by gasoline-powered vehicles of similar size, and have emitted approximately 34 million fewer tons of carbon dioxide (CO2) emissions than would have been emitted by gasoline-powered vehicles of similar size and driving performance. During 2012, Toyota and Lexus hybrid models sold more than 1 million units a year for the first time, with 1.219 million units sold. During 2013, TMC sold 1.279 million units, and the 6 million sales milestone was achieved in December 2013, just nine months after its latest million unit milestone. The 7 million sales mark was reached in September 2014, again, selling one million hybrids in ninth months.
Besides the three generations of Prius liftback, Toyota's hybrid lineup includes the Camry Hybrid (1st and 2nd generation), Toyota Highlander Hybrid (Kluger
Hybrid in Japan), Toyota Avalon Hybrid, Toyota Auris Hybrid, Toyota Yaris Hybrid (Europe only), and the following models sold only in Japan: Alphard Hybrid/Vellfire Hybrid, Estima Hybrid, Toyota Sai, Toyota Harrier, and Toyota Crown Hybrid. Toyota released the hybrid versions of the Corolla Axio sedan and Corolla Fielder station wagon in Japan in August 2013. Both cars are equipped with a 1.5-liter hybrid system similar to the one used in the Prius c.
Beginning in 2011, TMC introduced three new members to the Prius family, the Prius v (Prius α in Japan and Prius + in Europe), the Prius c (Toyota Aqua in Japan), and the Toyota Prius Plug-in Hybrid, released in 2012 in Japan, the U.S. and Europe. With a total of 247,230 vehicles sold during the first quarter of 2012, the Toyota Prius family became the third top selling nameplate in the world in terms of total global sales, after the Toyota Corolla (300,800 units) and the Ford Focus (277,000 units). Until September 2012, the Prius liftback was the top selling new car in Japan for 16 months in a row, until it was surpassed by the Toyota Aqua (Prius c) in October 2012. With 266,567 units sold in Japan in 2012, the Aqua is considered the most successful nameplate launch in the Japanese market in the last 20 years. The Prius c/Aqua model, with global sales of 409,500 units through March 2013, is TMC's second best selling hybrid after the Prius liftback, followed by the two generations of the Camry Hybrid, with 357,000 units sold worldwide.
Lexus also has their own hybrid lineup, consisting of the GS 450h, RX 400h/RX 450h, the LS 600h/LS 600h L, Lexus HS 250h, Lexus CT 200h, and Lexus ES 300h. Global cumulative sales of Lexus brand hybrids reached the 500 thousand mark in November 2012. The Lexus RX 400h/RX 450h is the top selling Lexus hybrid with 268.2 thousand units sold through March 2013, followed by the Lexus CT 200h with 137.3 thousand units.
Plug-in hybrids.
Toyota's plug-in hybrid electric vehicle project began in 2007, with road trials of the prototype vehicle in France and the UK in 2008. Toyota made 600 Prius plug-in demonstration vehicles for lease to fleet and government customers. 230 were delivered in Japan beginning in late December 2009, 125 models released in the U.S. by early 2010, and 200 units in 18 European countries in 2010. France, the UK and Germany had the largest fleets with 150 PHEVs. Canada, China, Australia, and New Zealand also participated in the global demonstration program.
The production version of the Toyota Prius Plug-in Hybrid was released in Japan in January 2012, followed by the United States in late February, and deliveries in Europe began in late June 2012. A total of 65,310 units Prius PHVs have been sold worldwide as of 2014[ [update]]. The market leader is the United States with 36,680 units delivered, followed by Japan with 19,100 units, and Europe with 9,133 units. s of 2014[ [update]], the Prius PHV ranks as the world's second top selling plug-in hybrid after the Chevrolet Volt, and also as the world's third best selling plug-in electric vehicle.
All-electric vehicles.
The first generation Toyota RAV4 EV was leased in the United States from 1997 to 2003, and at the lessees' request, many units were sold after the vehicle was discontinued. A total of 1,484 were leased and/or sold in California to meet the state’s CARB mandate for Zero-emissions vehicle. As of mid-2012, there were almost 500 units still in use.
In May 2010, Toyota launched a collaboration with Tesla Motors to create electric vehicles. Toyota agreed to purchase US$ of Tesla common stock subsequent to the closing of Tesla's planned initial public offering. Toyota, with the assistance of Tesla, built 35 converted RAV4s (Phase Zero vehicles) for a demonstration and evaluation program that ran through 2011. The lithium metal-oxide battery and other power train components were supplied by Tesla Motors.
The Toyota RAV4 EV Concept was released in September 2012. The RAV4 EV is assembled at Toyota's facility in Woodstock, Ontario along with the regular gasoline version. Tesla is building the electric powertrain at its plant at Tesla Factory in Fremont, California, and then ship them to Canada. The RAV4 EV is sold only in California, beginning with the San Francisco Bay Area, Los Angeles/Orange County and San Diego. Production will be limited to 2,600 during the first three years. s of 31 2013[ [update]], a total of 402 RAV4 EVs have been sold in the U.S.
A prototype of the Toyota iQ EV (Scion iQ EV in the US) was exhibited at the 2011 Geneva Motor Show. The Scion iQ EV is the successor to the FT-EV II as an electric vehicle based on the Toyota iQ chassis. Toyota produced three generations of FT-EV concept cars, and the iQ EV is a production version of those concepts, incorporating the technological and design strengths of all three models. The exterior of the production version is based on the FT-EV III concept shown at the 2011 Tokyo Motor Show.
The U.S. launch of the Scion iQ EV was announced for 2012, and according to Toyota, for the initial roll-out the iQ EV would not be available to individual consumers, instead the carmaker decided to focus on fleet customers and car sharing programs. The iQ EV was scheduled to be produced at Toyota’s Takaoka Plant in Toyota City beginning in August 2012 and the initial production was planned to be limited to 600 units, with 400 staying in Japan, 100 units destined to the U.S. and the other 100 for Europe. In September 2012 Toyota announced that due to customers' concerns about range and charging time, the production of the Scion iQ (Toyota eQ in Japan) will be limited to about 100 units for special fleet use in Japan and the U.S. only. The iQ EV/eQ was scheduled to be released in both countries in December 2012.
The first 30 iQ EVs were delivered in the U.S. to the University of California, Irvine in March 2013 for use in its Zero Emission Vehicle-Network Enabled Transport (ZEV-NET) carsharing fleet. Since 2002 the ZEV-NET program has been serving the transport needs of the Irvine community with all-electric vehicles for the critical last mile of commutes from the Irvine train station to the UC campus and local business offices.
In addition, Toyota announced that is backing away from fully electric vehicles. The company's vice chairman, Takeshi Uchiyamada, said ""The current capabilities of electric vehicles do not meet society’s needs, whether it may be the distance the cars can run, or the costs, or how it takes a long time to charge"." Toyota's emphasis would be re-focused on the hybrid concept, and 21 new hybrid gas-electric models scheduled to be on the market by 2015.
In any case, in , Harmonious Mobility Network, is promoting Toyota i-Road, an all electric vehicle combines the potential of both cars and motorbikes used in Grenoble (France) since October 2014 and Toyoya city (Japan).
Hydrogen fuel-cell.
In 2002 Toyota began a development and demonstration program to test the Toyota FCHV, a hybrid hydrogen fuel cell vehicle based on the Toyota Highlander production SUV. Toyota also built a FCHV bus based on the Hino Blue Ribbon City low-floor bus. Toyota has built several prototypes/concepts of the FCHV since 1997, including the Toyota FCHV-1, FCHV-2, FCHV-3, FCHV-4, and Toyota FCHV-adv. The Toyota FCV-R fuel cell concept car was unveiled at the 2011 Tokyo Motor Show. The FCV-R sedan seats four and has a fuel cell stack including a 70 MPa high-pressure hydrogen tank, which can deliver a range of 435 mi under the Japanese JC08 test cycle. Toyota said the car was planned for launch in about 2015.
In August 2012 Toyota announced its plans to start retail sales of a hydrogen fuel-cell sedan in California in 2015. Toyota expects to become a leader in this technology. The prototype of its first hydrogen fuel cell vehicle will be exhibited at the November 2013 Tokyo Motor Show, and in the United States at the January 2014 Consumer Electronics Show.
In November 2014 it was revealed that the Toyota fuel-cell vehicle will be called 'Mirai', meaning 'the future' in Japanese.
Cars.
As of 2009, Toyota officially lists approximately 70 different models sold under its namesake brand, including sedans, coupes, vans, trucks, hybrids, and crossovers. Many of these models are produced as passenger sedans, which range from the subcompact Toyota Yaris, to compact Corolla, to mid-size Camry, and full-size Avalon. Vans include the Previa/Estima, Sienna, and others. Several small cars, such as the xB and tC, are sold under the Scion brand.
SUVs and crossovers.
Toyota crossovers range from the compact Matrix and RAV4, to midsize Venza and Kluger/Highlander. Toyota SUVs range from the midsize 4Runner to full-size Land Cruiser. Other SUVs include the Prado, FJ Cruiser, Fortuner, and Sequoia.
Pickup trucks.
Toyota first entered the pickup truck market in 1947 with the SB that was only sold in Japan and limited Asian markets. It was followed in 1954 by the RK (renamed in 1959 as the Stout) and in 1968 by the compact Hilux. With continued refinement, the Hilux (simply known as the Pickup in some markets) became famous for being extremely durable and reliable, and many of these trucks from as early as the late 1970s are still on the road today, some with over 300,000 miles. Extended- and crew-cab versions of these small haulers were eventually added, and Toyota continues to produce them today under various names depending on the market.
Riding on the success of the compact pickups in the US, Toyota decided to attempt to enter the traditionally domestic-dominated full-size pickup market, introducing the T100 for the 1993 US model year, with production ending in 1998. While having a bed at the traditional full-size length of 8 feet, the suspension and engine characteristics were still similar to that of a compact pickup. It proved to be as economical and reliable as any typical Toyota pickup, but sales never became what Toyota brass had hoped for. It was criticized as being too small to appeal to the traditional American full-size pickup buyer. Another popular full-size truck essential, a V8 engine, was never available. Additionally, the truck was at first only available as a regular cab, though Toyota addressed this shortcoming and added the Xtracab version in mid-1995.
In 1999 for the 2000 model year, Toyota replaced the T100 with the larger Tundra. The Tundra addressed criticisms that the T100 did not have the look and feel of a legitimate American-style full-size pickup. It also added the V8 engine that the T100 was criticized for not having. However, the Tundra still came up short in towing capacity as well as still feeling slightly carlike. These concerns were addressed with an even larger 2007 redesign. A stronger V6 and a second V8 engine among other things were added to the option list. As of early 2010, the Tundra has captured 16% of the full-size half-ton market in the US. The all-new Tundra was assembled in San Antonio, Texas, US. Toyota assembled around 150,000 Standard and Double Cabs, and only 70,000 Crew Max's in 2007. The smaller Tacoma (which traces its roots back to the original Hilux) was also produced at the company's San Antonio facility.
Outside the United States, Toyota produced the Hilux in Standard and Double Cab, gasoline and diesel engine, and 2WD and 4WD versions. The BBC's "Top Gear" TV show featured two episodes of a Hilux that was deemed "virtually indestructible".
Luxury-type vehicles.
As of 2009, the company sold nine luxury-branded models under its Lexus division, ranging from the LS sedan to RX crossover and LX SUV. Luxury-type sedans produced under the Toyota brand included the Century, Toyota Crown, and Toyota Crown Majesta. A limited-edition model produced for the Emperor of Japan was the Century Royal.
Motorsports.
Toyota has been involved in many global motorsports series. They also represent their Lexus brand in other sports car racing categories. Toyota also makes engines and other auto parts for other Japanese motorsports including formula Nippon, Super GT, formula 3 and formula Toyota series. Toyota also runs a driver development programme known as the (Toyota Young Drivers Program, which they made for funding and educating future Japanese motorsports talent. Toyota Motorsport GmbH, with headquarters in Cologne, Germany, has been responsible for Toyota's major motorsports development including Formula One, the World Rally Championship, the Le Mans Series, and most recently the FIA World Endurance Championship. Toyota enjoyed success in all these motorsports categories. In 2002, Toyota entered Formula One as a constructor and engine supplier; however, despite having experienced drivers and a larger budget than many other teams, they failed to match their success in other categories, with five second-place finishes as their best results. On November 4, 2009, Toyota announced they were pulling out of the sport due to the global economic situation.
Toyota's nationwide driver hunt of drivers for Etios Motor Racing Series ended up with selection of 25 drivers, who will participate in the race in 2013.
TRD.
Toyota Racing Development (TRD) was brought about to help develop true high-performance racing parts for many Toyota vehicles. TRD has often had much success with their aftermarket tuning parts, as well as designing technology for vehicles used in all forms of racing. TRD is also responsible for Toyota's involvement in NASCAR motorsports. TRD also made Lexus's performance division "F-Sport".
Nonautomotive activities.
Aerospace.
Toyota is a minority shareholder in Mitsubishi Aircraft Corporation, having invested US$ in the new venture which will produce the Mitsubishi Regional Jet, slated for first deliveries in 2017. Toyota has also studied participation in the general aviation market and contracted with Scaled Composites to produce a proof of concept aircraft, the TAA-1, in 2002.
Philanthropy.
Toyota is supporter of the Toyota Family Literacy Program along with the National Center for Family Literacy, helping low-income community members for education, United Negro College Fund (40 annual scholarships), and National Underground Railroad Freedom Center (US$) among others. Toyota created the Toyota USA Foundation. Toyota has also donated its "kaizen" management practices training to charities such as the Food Bank For New York City to improve services to the poor.
Higher education.
Toyota established the Toyota Technological Institute in 1981, as Sakichi Toyoda had planned to establish a university as soon as he and Toyota became successful. Toyota Technological Institute founded the Toyota Technological Institute at Chicago in 2003. Toyota is supporter of the Toyota Driving Expectations Program, Toyota Youth for Understanding Summer Exchange Scholarship Program, Toyota International Teacher Program, Toyota TAPESTRY, Toyota Community Scholars (scholarship for high school students), United States Hispanic Chamber of Commerce Internship Program, and Toyota Funded Scholarship. It has contributed to a number of local education and scholarship programs for the University of Kentucky, Indiana, and others.
Robotics.
In 2004, Toyota showcased its trumpet-playing robot. Toyota has been developing multitask robots destined for elderly care, manufacturing, and entertainment. A specific example of Toyota's involvement in robotics for the elderly is the Brain Machine Interface. Designed for use with wheelchairs, it "allows a person to control an electric wheelchair accurately, almost in real-time", with his or her mind. The thought controls allow the wheelchair to go left, right, and forward with a delay between thought and movement of just 125 milliseconds. Toyota also played a part in the development of Kirobo, the world's first 'robot astronaut'.
Agricultural biotechnology.
Toyota invests in several small start-up businesses and partnerships in biotechnology, including:
Environmental record.
Toyota has been a leader in environmentally friendly vehicle technologies, most notably the RAV4 EV (produced from 1997 to 2003) and the Toyota Prius (1997 to present). Toyota is now working on their next generation Prius and second generation RAV4 EV both due out in 2012.
Toyota implemented its Fourth Environmental Action Plan in 2005. The plan contains four major themes involving the environment and the corporation's development, design, production, and sales. The five-year plan is directed at the, "arrival of a revitalized recycling-based society." Toyota had previously released its Eco-Vehicle Assessment System (Eco-VAS) which is a systematic life cycle assessment of the effect a vehicle will have on the environment including production, usage, and disposal. The assessment includes, "... fuel efficiency, emissions and noise during vehicle use, the disposal recovery rate, the reduction of substances of environmental concern, and CO2 emissions throughout the life cycle of the vehicle from production to disposal." 2008 marks the ninth year for Toyota's Environmental Activities Grant Program which has been implemented every year since 2000. Themes of the 2008 program consist of "Global Warming Countermeasures" and "Biodiversity Conservation."
Since October 2006, Toyota's new Japanese-market vehicle models with automatic transmissions are equipped with an Eco Drive Indicator. The system takes into consideration rate of acceleration, engine and transmission efficiency, and speed. When the vehicle is operated in a fuel-efficient manner, the Eco Drive Indicator on the instrument panel, lights up. Individual results vary depending on traffic issues, starting and stopping the vehicle, and total distance traveled, but the Eco Drive Indicator may improve fuel efficiency by as much as 4%. Along with Toyota's eco-friendly objectives on production and use, the company plans to donate US$ and five vehicles to the Everglades National Park. The money will be used to fund environmental programs at the park. This donation is part of a program which provides US$ and 23 vehicles for five national parks and the National Parks Foundation. However new figures from the United States National Research Council show that the continuing hidden health costs of the auto industry to the US economy in 2005 amounted to US$.
The United States EPA has awarded Toyota Motor Engineering & Manufacturing North America, Inc (TEMA) with an ENERGY STAR Sustained Excellence Award in 2007, 2008 and 2009.
In 2007, Toyota's Corporate Average Fuel Economy (CAFE) fleet average of 26.69 mpgus exceeded all other major manufactures selling cars within the United States. Only Lotus Cars, which sold the Elise and Exige (powered by Toyota's "2ZZ-GE" engine), did better with an average of 30.2 mpgus.

</doc>
<doc id="31093" url="http://en.wikipedia.org/wiki?curid=31093" title="Turing Award">
Turing Award

The ACM A.M. Turing Award is an annual prize given by the Association for Computing Machinery (ACM) to "an individual selected for contributions of a technical nature made to the computing community". It is stipulated that "The contributions should be of lasting and major technical importance to the computer field". The Turing Award is generally recognized as the highest distinction in computer science and the "Nobel Prize of computing".
The award is named after Alan Turing, mathematician and reader in mathematics at the University of Manchester. Turing is often credited for being the key founder of theoretical computer science and artificial intelligence. From 2007-2013, the award was accompanied by a prize of US$250,000, with financial support provided by Intel and Google. Since 2014 the award has been accompanied by a prize of US$1 million, with financial support provided by Google.
The first recipient, in 1966, was Alan Perlis, of Carnegie Mellon University. Frances E. Allen of IBM, in 2006, was the first female recipient in the award's forty-year history. The 2008 and 2012 awards also went to women, Barbara Liskov and Shafi Goldwasser, respectively.

</doc>
<doc id="31112" url="http://en.wikipedia.org/wiki?curid=31112" title="Tesseract">
Tesseract

In geometry, the tesseract is the four-dimensional analog of the cube; the tesseract is to the cube as the cube is to the square. Just as the surface of the cube consists of 6 square faces, the hypersurface of the tesseract consists of 8 cubical cells. The tesseract is one of the six convex regular 4-polytopes.
The tesseract is also called an 8-cell, regular octachoron, cubic prism, and tetracube (although this last term can also mean a polycube made of four cubes). It is the four-dimensional hypercube, or 4-cube as a part of the dimensional family of hypercubes or "measure polytopes".
According to the "Oxford English Dictionary", the word "tesseract" was coined and first used in 1888 by Charles Howard Hinton in his book "A New Era of Thought", from the Greek τέσσερεις ακτίνες (téssereis aktines or "four rays"), referring to the four lines from each vertex to other vertices. In this publication, as well as some of Hinton's later work, the word was occasionally spelled "tessaract".
Geometry.
The tesseract can be constructed in a number of ways. As a regular polytope with three cubes folded together around every edge, it has Schläfli symbol {4,3,3} with hyperoctahedral symmetry of order 384. Constructed as a 4D hyperprism made of two parallel cubes, it can be named as a composite Schläfli symbol {4,3} × { }, with symmetry order 96. As a duoprism, a Cartesian product of two squares, it can be named by a composite Schläfli symbol {4}×{4}, with symmetry order 64. As an orthotope it can be represented by composite Schläfli symbol { } × { } × { } × { } or { }4, with symmetry order 16.
Since each vertex of a tesseract is adjacent to four edges, the vertex figure of the tesseract is a regular tetrahedron. The dual polytope of the tesseract is called the hexadecachoron, or 16-cell, with Schläfli symbol {3,3,4}.
The standard tesseract in Euclidean 4-space is given as the convex hull of the points (±1, ±1, ±1, ±1). That is, it consists of the points:
A tesseract is bounded by eight hyperplanes ("x"i = ±1). Each pair of non-parallel hyperplanes intersects to form 24 square faces in a tesseract. Three cubes and three squares intersect at each edge. There are four cubes, six squares, and four edges meeting at every vertex. All in all, it consists of 8 cubes, 24 squares, 32 edges, and 16 vertices.
Projections to 2 dimensions.
The construction of a hypercube can be imagined the following way:
It is possible to project tesseracts into three- or two-dimensional spaces, as projecting a cube is possible on a two-dimensional space.
Projections on the 2D-plane become more instructive by rearranging the positions of the projected vertices. In this fashion, one can obtain pictures that no longer reflect the spatial relationships within the tesseract, but which illustrate the connection structure of the vertices, such as in the following examples:
A tesseract is in principle obtained by combining two cubes. The scheme is similar to the construction of a cube from two squares: juxtapose two copies of the lower-dimensional cube and connect the corresponding vertices. Each edge of a tesseract is of the same length. This view is of interest when using tesseracts as the basis for a network topology to link multiple processors in parallel computing: the distance between two nodes is at most 4 and there are many different paths to allow weight balancing.
Tessellation.
The tesseract, along with all hypercubes, tesselates Euclidean space. The self-dual tesseractic honeycomb consisting of 4 tesseracts around each face has Schläfli symbol {4,3,3,4}. Hence, the tesseract has a dihedral angle of 90°.
Related uniform polytopes.
It is in a sequence of regular 4-polytopes and honeycombs with tetrahedral vertex figures.
It is in a sequence of regular 4-polytope and honeycombs with cubic cells.
In popular culture.
Since their discovery, four-dimensional hypercubes have been a popular theme in art, architecture, and fiction. Notable examples include:

</doc>
<doc id="31151" url="http://en.wikipedia.org/wiki?curid=31151" title="Thresher">
Thresher

Thresher may refer to:

</doc>
<doc id="31156" url="http://en.wikipedia.org/wiki?curid=31156" title="Taiwan independence movement">
Taiwan independence movement

Taiwan Independence movement is a political movement whose goals are primarily to formally establish the Republic of Taiwan or the State of Taiwan by renaming the Republic of China (ROC) (commonly known as Taiwan), strengthening Taiwanese national identity, rejecting unification and One country, two systems with the People's Republic of China (PRC) (commonly known as China) and a Chinese identity, and obtain international recognition as a sovereign state, such as referring to Article 32 of the UN Charter. The success of this movement would be one possible outcome for the resolution of the political status of Taiwan.
This movement is supported by the Pan-Green Coalition in Taiwan, but opposed by the Pan-Blue Coalition which seeks to retain the somewhat ambiguous status quo of the ROC under the 1992 consensus, or gradually reunify with mainland China at some point. Due to the PRC's claim of sovereignty over Taiwan and repeated military threats made by the PRC, a formal declaration of independence could lead to a military confrontation between Taiwan and China, possibly escalating and involving other countries, such as the United States and Japan.
The use of "independence" for Taiwan can be ambiguous. If some supporters articulate that they agree to the independence of Taiwan, they may either be referring to the notion of formally creating an independent Republic of Taiwan, or to the notion that Taiwan has become synonymous with the current Republic of China and is already independent (as reflected in the concept of One Country on Each Side), these ideas run counter to the claims of the People's Republic of China. Some supporters advocate withdrawing from Kinmen and Matsu, which are controlled by Taiwan but are closer to mainland China. Therefore, the differences between the opinions of different camps in support and against independence can be very subtle.
From 1683 to 1894, both Taiwan and mainland China were ruled by the Qing Empire. Following the First Sino-Japanese War in 1895, Taiwan was ceded by Qing government to the Empire of Japan via the Treaty of Shimonoseki. At the end of World War II in 1945, Taiwan was taken over by the ROC forces who then ruled most of mainland China. Since the defeat and expulsion of the ruling Kuomintang ROC government by the Communist Party of China from mainland China in 1949, the ROC government has controlled only Taiwan and its surrounding islands. It is a point of contention as to whether Taiwan has already achieved "de facto" independence under the Constitution of the Republic of China amended in 2005
History of the movement.
Many supporters of independence for Taiwan view the history of Taiwan since the 17th century as a continuous struggle for independence and use it as an inspiration for the current political movement.
According to this view, the people indigenous to Taiwan and those who have taken up residence there have been repeatedly occupied by groups including the Dutch, the Spanish, the Ming, Koxinga and the Ming loyalists, the Qing, the Japanese and finally the Chinese Nationalists led by the Kuomintang. From a pro-independence supporter's point of view, the movement for Taiwan independence began under Qing rule in the 1680s which led to a well known saying those days, "Every three years an uprising, every five years a rebellion". Taiwan Independence supporters compared Taiwan under Kuomintang rule to South Africa under apartheid. The Taiwan independence movement under Japan was supported by Mao Zedong in the 1930s as a means of freeing Taiwan from Japanese rule. With the end of World War II in 1945, by issuing "General Order No. 1" to the Supreme Commander for the Allied Powers, the Allies agreed that the Republic of China Army under the Kuomintang would "temporarily occupy Taiwan, on behalf of the Allied forces."
Martial law period.
Modern-day political movement for Taiwan independence dates back to the Japanese colonial period but only became a viable political force within Taiwan in the 1990s. Taiwanese independence was advocated periodically during the Japanese colonial period, but was suppressed by the Japanese government. These efforts were the goal of the Taiwanese Communist Party of the late 1920s. Unlike current formulations, and in line with the thinking of the Comintern, such a state would have been a proletarian one. With the end of World War II in 1945, Japanese rule ended, but the subsequent autocratic rule of the ROC's Kuomintang (KMT) later revived calls for local rule. However, it was a movement supported by the Chinese students who were born on the Island and not associated with KMT. It found its roots in the US and Japan. In the 1950s a Republic of Taiwan Provisional Government was set up in Japan. Thomas Wen-I Liao was nominally the President. At one time it held quasi-official relations with the newly independent Indonesia. This was possible mainly through the connections between Sukarno and the Provisional Government's Southeast Asian liaison, Chen Chih-hsiung, who had assisted in Indonesia's local resistance movements against the Japanese rule.
After the Kuomintang began to rule the island, the focus of the movement was as a vehicle for discontent from the native Taiwanese against the rule of "mainlanders" (i.e. mainland Chinese-born people who fled to Taiwan with KMT in the late 1940s). The 228 incident in 1947 and the ensuing martial law which lasted until 1987 contributed to a so-called sense of White Terror on the island. In 1979, the Kaohsiung Incident, occurred as the movement for democracy and independence intensified.
Between 1949 and 1991, the official position of the ROC government on Taiwan was that it was the legitimate government of all of China and it used this position as justification for authoritarian measures such as the refusal to vacate the seats held by delegates elected on the mainland in 1947 for the Legislative Yuan. The Taiwan independence movement intensified in response to this and presented an alternative vision of a sovereign and independent Republic of Taiwan. This vision was represented through a number of symbols such as the use of Taiwanese in opposition to the school-taught Mandarin Chinese. Several scholars drafted various versions of a constitution, as both political statement or vision and as intellectual exercise. Most of these drafts favor a bicameral parliamentary rather than presidential system. In at least one such draft, seats in the upper house would be divided equally among Taiwan's established ethnicities. In the 1980s the Chinese Nationalist government considered publication of these ideas criminal. In the most dramatic case, it decided to arrest the pro-independence publisher Cheng Nan-jung for publishing a version in his Tang-wai magazine, "Liberty Era Weekly" (自由時代週刊). Rather than giving himself up, Cheng self-immolated in protest. Other campaigns and tactics toward such a State have included soliciting designs from the public for a new national flag (see image) and anthem (for example, "Taiwan the Formosa"). More recently the Taiwan Name Rectification Campaign (台灣正名運動) has played an active role. More traditional independence supporters, however, have criticized name rectification as merely a superficial tactic devoid of the larger vision inherent in the Republic of Taiwan agenda.
Various overseas Taiwan Independence movements, such as the Formosan Association, World United Formosans for Independence, United Young Formosans for Independence (Japan), Union for Formosa's Independence in Europe, United Formosans in America for Independence, Committee for Human Rights in Formosa (Toronto, Ont.), published "The Independent Formosa" in several volumes with the publisher "Formosan Association." In "The Independent Formosa, Volumes 2-3", they tried to justify Taiwanese collaboration with Japan during World War II by saying that the "atmosphere covered the whole Japanese territories, including Korea and Formosa, and the Japanese mainlands as well", when Taiwanese publications supported Japan's "holy war", and that the people who did it were not at fault.
The Anti-communist Kuomintang leader Chiang Kai-shek, President of the Republic of China on Taiwan, believed the Americans were going to plot a coup against him along with Taiwan Independence. In 1950, Chiang Ching-kuo became director of the secret police, which he remained until 1965. Chiang also considered some people who were friends to Americans to be his enemies. An enemy of the Chiang family, Wu Kuo-chen, was kicked out of his position of governor of Taiwan by Chiang Ching-kuo and fled to America in 1953. Chiang Ching-kuo, educated in the Soviet Union, initiated Soviet style military organization in the Republic of China Military, reorganizing and Sovietizing the political officer corps, surveillance, and Kuomintang party activities were propagated throughout the military. Opposed to this was Sun Li-jen, who was educated at the American Virginia Military Institute. Chiang orchestrated the controversial court-martial and arrest of General Sun Li-jen in August 1955, for plotting a coup d'état with the American CIA against his father Chiang Kai-shek and the Kuomintang. The CIA allegedly wanted to help Sun take control of Taiwan and declare its independence.
During the martial law era lasting until 1987, discussion of Taiwan independence was forbidden in Taiwan, at a time when recovery of the mainland and national unification were the stated goals of the ROC. During that time, many advocates of independence and other dissidents fled overseas, and carried out their advocacy work there, notably in Japan and the United States. Part of their work involved setting up think tanks, political organizations, and lobbying networks in order to influence the politics of their host countries, notably the United States, the ROC's main ally at the time, though they would not be very successful until much later. Within Taiwan, the independence movement was one of many dissident causes among the intensifying democracy movement of the 1970s, which culminated in the 1979 Kaohsiung Incident. The Democratic Progressive Party (DPP) was eventually formed to represent dissident causes.
Multiparty period.
After the lifting of martial law in 1987, and the acceptance of multi-party politics, the Democratic Progressive Party became increasingly identified with Taiwan independence, which entered its party platform in 1991. At the same time, many overseas independence advocates and organizations returned to Taiwan and for the first time openly promoted their cause in Taiwan, gradually building up political support. Many had fled into the US or Europe and had been on a black list held by KMT, which had held them back from going back to Taiwan. There they had built many organisations like European Federation of Taiwanese Associations or Formosan Association for Public Affairs. By the late 1990s, DPP and Taiwan independence have gained a solid electoral constituency in Taiwan, supported by an increasingly vocal and hardcore base.
As the electoral success of the DPP, and later, the DPP-led Pan-Green Coalition grew in recent years, the Taiwan independence movement shifted focus to identity politics by proposing many plans involving symbolism and social engineering. The interpretation of historical events such as the 228 Incident, the use of broadcast language and mother tongue education in schools, the official name and flag of the ROC, slogans in the army, orientation of maps all have been issues of concern to the present-day Taiwan independence movement. The movement, at its peak in the 70s through the 90s in the form of the Taiwan literature movement and other cultural upheavals, has moderated in recent years with the assimilation of these changes. Friction between "mainlander" and "native" communities on Taiwan has decreased due to shared interests: increasing economic ties with mainland China, continuing threats by the PRC to invade, and doubts as to whether or not the United States would support a unilateral declaration of independence. Since the late 1990s many supporters of Taiwan independence have argued that Taiwan, as the ROC, is already independent from the mainland, making a formal declaration unnecessary. In May 1999, the Democratic Progressive Party formalized this position in its "Resolution on Taiwan's Future".
In 1995, Taiwanese president Lee Teng-hui was given permission to speak at Cornell University about his dream of Taiwanese independence, the first time a Taiwanese leader had been allowed to visit the United States. This led to a military response from China that included buying Russian submarines and conducting missile tests near Taiwan.
DPP administration: (2000–2008).
In February 2007, President Chen Shui-bian initiated the change of names of state-owned enterprises, nation's embassies and overseas representative offices. As a result Chunghwa Post Co. (中華郵政) was renamed Taiwan Post Co (臺灣郵政) until the Kuomintang victories of 2008 votes, and Chinese Petroleum Corporation (中國石油) is now called "CPC Corporation, Taiwan" (臺灣中油) and the signs in Taiwan's embassies now display the word "Taiwan" in brackets after "Republic of China".
However, the name of the post office was reverted to 'Chunghwa Post Co.' following the inauguration of incumbent president Ma Ying-jeou in 2008. 
In 2007, the recently renamed Taiwan Post Co. issued stamps bearing the name "Taiwan" in remembrance of the 228 Incident.
The Pan-Blue camp voiced its opposition to the changes and the former KMT Chairman Ma Ying-jeou () said that it would generate diplomatic troubles and cause cross-strait tensions. It also argued that without a change in the relevant legislation pertaining to state-owned enterprises, the name changes of these enterprises could not be valid. As the Pan-Blue camp held a slim parliamentary majority throughout the administration of President Chen, the Government's motion to change the law to this effect were blocked by the opposition. Later, U.S. Department of State spokesman Sean McCormack said that the U.S. does not support administrative steps that would appear to change Taiwan's status or move toward independence.
Former president Lee Teng-hui has stated that it is unnecessary to pursue Taiwanese independence. Lee views Taiwan as already an independent state, and that the call for "Taiwanese independence" could even confuse the international community by implying that Taiwan once viewed itself as part of China. From this perspective, Taiwan is independent even if it remains unable to enter the UN. Lee said the most important goals are to improve the people's livelihoods, build national consciousness, make a formal name change and draft a new constitution that reflects the present reality so that Taiwan can officially identify itself as a country.
KMT administration (2008–present).
Legislative elections were held on January 12, 2008, resulting in a supermajority (86 of the 113 seats) in the legislature for the Kuomintang (KMT) and the Pan-Blue Coalition. President Chen Shui-bian's Democratic Progressive Party was handed a heavy defeat, winning only the remaining 27 seats. The junior partner in the Pan-Green Coalition, the Taiwan Solidarity Union, won no seats.
Two months later, the election for the 12th-term President and Vice-President of the Republic of China was held in the Free Area of the Republic of China on Saturday, March 22, 2008. Kuomintang (KMT) nominee Ma Ying-jeou won, with 58% of the vote, ending eight years of Democratic Progressive Party rule. Along with the 2008 legislative election, Ma's landslide victory brought the Kuomintang back to power in Taiwan.
Following his election, Ma Ying-jeou publicly stated that he did not wish his inauguration commemoration stamps to be marked "Taiwan Post", because the name change was "illegal". In respect of the administration's wishes, the postal service marked the inauguration stamps with Chinese characters for the "Republic of China", as well as "Republic of China (Taiwan)" in English.
On August 1, 2008, the Board of Directors of Taiwan Post Co. resolved to reverse the name change and restored the name "Chunghwa Post". The Board of Directors, as well as resolving to restore the name of the corporation, also resolved to re-hire the chief executive dismissed in 2007, and to withdraw defamation proceedings against him.
The Executive Yuan on August 21, 2008, under the new Ma Administration, officially restored the name "National Chiang Kai-Shek Memorial Hall" to the hall commemorating late President Chiang Kai-Shek, which had been renamed "National Taiwan Democracy Memorial Hall by the Chen Administration, sparking off a standoff between the Central Government of President Chen and the Taipei Municipal Government run by then-Mayor Ma Ying-jeou.
On September 2, 2008, President Ma defined the relations between Taiwan and mainland China as "special", but "not that between two states" - they are relations based on two areas of one state, with Taiwan considering that state to be the Republic of China, and mainland China considering that state to be the People's Republic of China.
Significance.
Domestically, the issue of independence has dominated Taiwanese politics for the past few decades. This is also a grave issue for mainland China. The creation of the Republic of Taiwan is formally the goal of the Taiwan Solidarity Union and former President Lee Teng-hui. Although the Democratic Progressive Party was originally also an advocate for both the idea of the Republic of Taiwan and Taiwan independence, as it took power the DPP has tried taking a middle line in which a sovereign, independent Taiwan is identified with the "Republic of China (Taiwan)" and its symbols.
Internationally, this movement is significant in that a formal declaration of independence is one of the five conditions the PRC has stated or implied under which it will take military action against Taiwan to force reunification — the other four being that Taiwan makes a military alliance with a foreign power, there is internal turmoil in Taiwan, Taiwan gains weapons of mass destruction, or Taiwan refuses to negotiate on the basis of "one China". (Recently, the PRC government warned that if the situation in Taiwan becomes "worse" it will not look on "indifferently." Given the terms of the Taiwan Relations Act, this raises the possibility of a superpower conflict in East Asia.) The United States would likely be obligated to come to the aid of Taiwan under the terms of the Act according to US domestic law. However, this interpretation of the Act is disputed. Constitutional law requires that a normal declaration of war be sought by the President of the United States in an act of Congress signed by the President.
Responses.
The questions of independence and the island's relationship to Mainland China are complex and inspire very strong emotions among Taiwanese people. There are some who continue to maintain the KMT position that the ROC is the sole legitimate government for all of China (including Taiwan) and that the aim of the government should be eventual reunification of the mainland and Taiwan under the rule of the ROC. Some argue that Taiwan has been, and should continue to be, completely independent from China and should become a sovereign nation under the name "Republic of Taiwan". Then, there are numerous positions running the entire spectrum between these two extremes.
On October 25, 2004, in Beijing, the U.S. Secretary of State Colin Powell said Taiwan is "not sovereign", provoking strong comments from both the Pan-Green and Pan-Blue coalitions – but for very different reasons. From the DPP's side, President Chen declared that "Taiwan is definitely a sovereign, independent country, a great country that absolutely does not belong to the People's Republic of China". The TSU (Taiwan Solidarity Union) criticized Powell, and questioned why the US sold weapons to Taiwan if it was not a sovereign state. From the KMT, Chairman Ma Ying-jeou announced that "the Republic of China has been a sovereign state ever since it was formed [in 1912]". The pro-unification PFP Party Chairman, James Soong, called it "Taiwan's biggest failure in diplomacy".
Support for independence.
The first view considers the move for Taiwan independence as a nationalist movement. This is the opinion, historically, put forward by such pro-independence groups on Taiwan as the tang wai movement (which later grew into the Democratic Progressive Party), which argue that the ROC under the Kuomintang has been in the past a "foreign regime" forcibly imposed on Taiwan. Since the 1990s, supporters of Taiwan independence no longer actively make this argument. Instead, the argument has been that in order to survive against the growing power of the PRC, Taiwan must view itself as a separate and distinct entity from "China". This involves removing the name of China from official and unofficial items in Taiwan, making changes in history books to focus mainly on Taiwan as a central entity, promoting the use of Taiwanese language including in government and education, reducing economic links with mainland China, and in general thinking of Taiwan as a separate entity. In this view, China is a foreign entity, and the goal of this movement is to create an internationally recognized country which is separate from any concept of China. Kinmen and Matsu off the coast of Fujian and some of the islands in the South China Sea, which are historically not part of Taiwan, are to be excluded from the proposed state of Taiwan. Some supporters of Taiwan independence argue that the Treaty of San Francisco justifies Taiwan independence by not explicitly granting Taiwan to either the ROC or the PRC. This legal justification is rejected by both the PRC and ROC governments. It is also thought that if formal independence were declared, Taiwan's foreign policies would lean further towards Japan and the United States and the desirable option of United Nations Trusteeship Council is also considered.
Support for status quo.
A second view is that Taiwan is already an independent nation with the official name "Republic Of China", which has been independent (i.e. de facto separate from Mainland China) since the end of the Chinese Civil War in 1949, when the ROC lost control of mainland China, with only Taiwan (including the Penghu islands), Kinmen, the Matsu Islands off the coast of Fujian Province, and some of the islands in the South China Sea remaining under its administration. Although previously no major political faction adopted this pro-status quo viewpoint, because it is a "compromise" in face of PRC threats and American warnings against a unilateral declaration of independence, the DPP combined it with their traditional belief to form their latest official policy. This viewpoint has not been adopted by more radical groups such as the Taiwan Solidarity Union, which favor only the third view described above and are in favor of a Republic or State of Taiwan. In addition, many members of the Pan-Blue Coalition are rather suspicious of this view, fearing that adopting this definition of Taiwan independence is merely an insincere stealth tactical effort to advance desinicization and the third view of Taiwan independence. As a result, supporters of pan-blue tend to make a clear distinction between Taiwan independence and Taiwan sovereignty, while supporters of Pan-Green tend to try to blur the distinction between the two.
Most Taiwanese and political parties of the ROC support the status quo, and recognize that this is de facto independence through sovereign self-rule. Even among those who believe Taiwan is and should remain independent, the threat of war from PRC softens their approach, and they tend to support maintaining the status quo rather than pursuing an ideological path that could result in war with the PRC. When the two-states policy was put forward by President Lee Teng-hui, he received 80 percent support. A similar situation arose when President Chen Shui-bian declared that there was "one country on each side" of the Taiwan Strait. The parties disagree, sometimes bitterly, on such things as territory, name (R.O.C. or Taiwan), future policies, and interpretations of history. The Pan-Blue Coalition and the PRC believe that Lee Teng-hui and Chen Shui-bian are intent on publicly promoting a moderate form of Taiwan independence in order to advance secretly deeper forms of Taiwan independence, and that they intend to use popular support on Taiwan for political separation to advance notions of cultural and economic separation.
Opposition to independence.
The third view, put forward by the government of the PRC, defines Taiwan independence as "splitting Taiwan from China, causing division of the nation and the people." What PRC claims by this statement is somewhat ambiguous according to supporters of Taiwanese independence, as some statements by the PRC seem to identify China solely and uncompromisingly with the PRC, and others indicate a broader and more flexible definition suggesting a cultural and geographic entity of which both mainland China and Taiwan are parts but divided politically due to the Chinese Civil War. The PRC considers itself the sole legitimate government of all China, and the ROC to be a defunct entity replaced in the Communist revolution which succeeded in 1949. Therefore, assertions that the ROC is a sovereign state are construed as support for Taiwan independence while proposals to change the name of the ROC to Republic of Taiwan are paradoxically met with even more disapproval since this would be the equivalent of formally dropping the notion that Taiwan is part of the greater China entity (as a side of an unresolved Chinese civil war). Before the passing of UN Resolution 2758 in 1971, the Republic Of China was recognized as the legal government of China by the UN. Afterwards, the PRC became recognized as the legal government of China by the UN. During PRC President Hu Jintao's visit to the United States on 20 April 2006, U.S. President George W. Bush reaffirmed to the world that the U.S. would uphold its "one China" policy.
The official position of the PRC is that Taiwan is a province of China, and has "always" been part of China. The PRC often claims independence is wanted by only a small group, and that this group is trying to brainwash the local population to support this objective. In the 2000 White Paper, the PRC government stated that the people of Taiwan do not have the right to determine their own fate by declaring independence through a referendum or otherwise because "The sovereignty over Taiwan belongs to all the Chinese people including Taiwan compatriots, and not to some of the people in Taiwan." The paper further stated that unification with mainland China is the only option. Efforts to change names of official buildings, government organizations, tributary monuments by replacing "China" with "Taiwan" have met with opposition – whether this is due to a political view that Taiwan is part of China or out of concern that such actions may result in retaliation or invasion from Beijing is unclear.

</doc>
<doc id="31202" url="http://en.wikipedia.org/wiki?curid=31202" title="Thalassa">
Thalassa

Thalassa may refer to:

</doc>
<doc id="31208" url="http://en.wikipedia.org/wiki?curid=31208" title="The Angry Brigade">
The Angry Brigade

The Angry Brigade were a British anarchist group responsible for a series of bomb attacks in Britain between 1970 and 1972.
History.
Origins.
In mid-1968 demonstrations took place in London, centred on the US embassy in Grosvenor Square, against US involvement in the Vietnam War. One of the organisers of these demonstrations, the well-known radical Tariq Ali, claims to recall an approach by someone representing the "Angry Brigade" who wished to bomb the embassy; he told them it was a terrible idea and no bombing took place.
1970s.
The Angry Brigade decided to launch a bombing campaign with small bombs - in order to maximise media exposure to their demands while keeping collateral damage to a minimum. The campaign started in August 1970 and continued for a year until arrests took place the following summer.
Targets included banks, embassies, the Miss World event in 1970 (or rather a BBC Outside Broadcast vehicle earmarked for use in the BBC's coverage) and the homes of Conservative MPs. In total, police attributed 25 bombings to the Angry Brigade. The bombings mostly caused property damage; one person was slightly injured.
Resurfaced Angry Brigade of the 1980s.
In the 1980s the Angry Brigade resurfaced as the Angry Brigade Resistance Movement - part of the IRSM.
Aftermath.
Jake Prescott, whose origins were in the mining community of Dunfermline, was arrested and tried in 1971. Melford Stevenson sentenced him to 15 years imprisonment (later reduced to 10), mostly spent in maximum security jails. Later he said he realised then that he "was the one who was angry and the people [he] met were more like the Slightly Cross Brigade". The other members of the group from North-East London, the "Stoke Newington Eight", were prosecuted for carrying out bombings as the Angry Brigade in one of the longest criminal trials of English history (it lasted from 30 May to 6 December 1972). As a result of the trial, John Barker, Jim Greenfield, Hilary Creek and Anna Mendleson received prison sentences of 10 years. A number of other defendants were found not guilty, including Stuart Christie, who had previously been imprisoned in Spain for carrying explosives with the intent to assassinate the dictator Francisco Franco, and Angela Mason who became a director of the lesbian, gay, bisexual and transgender rights group Stonewall and was awarded an OBE for services to homosexual rights.
In February 2002, Jake Prescott apologised for his role in bombing Robert Carr's house and called on other members of the Angry Brigade to also come forward.
On 3 February 2002, the Guardian newspaper reported a history of the Angry Brigade and an update on what its former members are doing now.
On 9 August 2002, BBC R4 aired Graham White’s historical drama, ‘The Trial Of The Angry Brigade’. Produced by Peter Kavanagh, this was a reconstruction of the trial combined with other background information. The cast included Kenneth Cranham, Juliet Stevenson, Mark Strong.
In March 2009, British family care activist and novelist Erin Pizzey reportedly declined to comment on the temporary withdrawal by its publishers of the book "Andrew Marr's History of Modern Britain" following her complaint it had falsely linked her to the Angry Brigade.

</doc>
<doc id="31211" url="http://en.wikipedia.org/wiki?curid=31211" title="Bolzano–Weierstrass theorem">
Bolzano–Weierstrass theorem

In mathematics, specifically in real analysis, the Bolzano–Weierstrass theorem, named after Bernard Bolzano and Karl Weierstrass, is a fundamental result about convergence in a finite-dimensional Euclidean space R"n". The theorem states that 
each bounded sequence in R"n" has a convergent subsequence. An equivalent formulation is that a subset of R"n" is sequentially compact if and only if it is closed and bounded. The theorem is sometimes called the sequential compactness theorem.
Proof.
First we prove the theorem when "n" = 1, in which case the ordering on R can be put to good use. Indeed we have the following result.
Lemma: Every sequence { "x""n" } in R has a monotone subsequence.
Proof: Let us call a positive integer "n" a "peak of the sequence" if "m" > "n" implies  "x" "n" > "x" "m"  "i.e.", if  "x""n" is greater than every subsequent term in the sequence. Suppose first that the sequence has infinitely many peaks, "n"1 < "n"2 < "n"3 < … < "n""j" < …. Then the subsequence  formula_1  corresponding to these peaks is monotonically decreasing, and we are done. So suppose now that there are only finitely many peaks, let "N" be the last peak and . Then "n"1 is not a peak, since "n"1 > "N", which implies the existence of an "n"2 > "n"1 with  formula_2  Again, "n"2 > "N" is not a peak, hence there is "n"3 > "n"2 with formula_3  Repeating this process leads to an infinite non-decreasing subsequence  formula_4, as desired.
Now suppose we have a bounded sequence in R; by the Lemma there exists a monotone subsequence, necessarily bounded. It follows from the monotone convergence theorem that this subsequence must converge.
Finally, the general case can be easily reduced to the case of "n" = 1 as follows: given a bounded sequence in R"n", the sequence of first coordinates is a bounded real sequence, hence has a convergent subsequence. We can then extract a subsubsequence on which the second coordinates converge, and so on, until in the end we have passed from the original sequence to a subsequence "n" times — which is still a subsequence of the original sequence — on which each coordinate sequence converges, hence the subsequence itself is convergent.
Sequential compactness in Euclidean spaces.
Suppose "A" is a subset of R"n" with the property that every sequence in "A" has a subsequence converging to an element of "A". Then "A" must be bounded, since otherwise there exists a sequence "x""m" in "A" with || "x""m" || ≥ "m" for all "m", and then every subsequence is unbounded and therefore not convergent. Moreover "A" must be closed, since from a noninterior point "x" in the complement of "A" one can build an "A"-valued sequence converging to "x". Thus the subsets "A" of R"n" for which every sequence in "A" has a subsequence converging to an element of "A" – i.e., the subsets which are sequentially compact in the subspace topology – are precisely the closed and bounded sets.
This form of the theorem makes especially clear the analogy to the Heine–Borel theorem, 
which asserts that a subset of R"n" is compact if and only if it is closed and bounded. In fact, general topology tells us that a metrizable space is compact if and only if it is sequentially compact, so that the Bolzano–Weierstrass and Heine–Borel theorems are essentially the same.
History.
The Bolzano–Weierstrass theorem is named after mathematicians Bernard Bolzano and Karl Weierstrass. It was actually first proved by Bolzano in 1817 as a lemma in the proof of the intermediate value theorem. Some fifty years later the result was identified as significant in its own right, and proved again by Weierstrass. It has since become an essential theorem of analysis.
Application to economics.
There are different important equilibrium concepts in economics, the proofs of the existence of which often require variations of the Bolzano–Weierstrass theorem. One example is the existence of a Pareto efficient allocation. An allocation is a matrix of consumption bundles for agents in an economy, and an allocation is Pareto efficient if no change can be made to it which makes no agent worse off and at least one agent better off (here rows of the allocation matrix must be rankable by a preference relation). The Bolzano–Weierstrass theorem allows one to prove that if the set of allocations is compact and non-empty, then the system has a Pareto-efficient allocation.

</doc>
<doc id="31217" url="http://en.wikipedia.org/wiki?curid=31217" title="Typography">
Typography

Typography is the art and technique of arranging type to make written language readable and appealing. The arrangement of type involves selecting typefaces, point size, line length, line-spacing (leading), letter-spacing (tracking), and adjusting the space within letters pairs (kerning). Type design is a closely related craft, sometimes considered part of typography; most typographers do not design typefaces, and some type designers do not consider themselves typographers. In modern times, typography has been put in film, television and online broadcasts to add emotion to communication.
Typography is performed by typesetters, compositors, typographers, graphic designers, art directors, manga artists, comic book artists, graffiti artists, clerical workers, and everyone else who arranges type for a product. Until the Digital Age, typography was a specialized occupation. Digitization opened up typography to new generations of visual designers and lay users, and David Jury, Head of Graphic Design at Colchester Institute in England, states that "typography is now something everybody does."
History.
Typography, from the Greek words τύπος "typos" "form" and γράφειν "graphein" "to write", traces its origins to the first punches and dies used to make seals and currency in ancient times. The uneven spacing of the impressions on brick stamps found in the Mesopotamian cities of Uruk and Larsa, dating from the 2nd millennium BC, may have been evidence of type where the reuse of identical characters were applied to create cuneiform text. Babylonian cylinder seals were used to create an impression on a surface by rolling the seal on wet clay. Typography was also realized in the Phaistos Disc, an enigmatic Minoan print item from Crete, Greece, which dates between 1850 and 1600 BC. It has been proposed that Roman lead pipe inscriptions were created by movable type printing, but German typographer Herbert Brekle recently dismissed this view.
The essential criterion of type identity was met by medieval print artifacts such as the Latin Pruefening Abbey inscription of 1119 that was created by the same technique as the Phaistos disc. The silver altarpiece of patriarch Pellegrinus II (1195−1204) in the cathedral of Cividale was printed with individual letter punches. The same printing technique can apparently be found in 10th to 12th century Byzantine reliquaries. Other early examples include individual letter tiles where the words are formed by assembling single letter tiles in the desired order were reasonably widespread in medieval Northern Europe.
Typography with movable type was invented in 11th-century China by Bi Sheng (990–1051) during the Song Dynasty. His movable type system was manufactured from ceramic materials, and clay type printing continued to be practiced in China until the Qing Dynasty. Wang Zhen was one of the pioneers of wooden movable type. Although the wooden type was more durable under the mechanical rigors of handling, repeated printing wore the character faces down, and the types could only be replaced by carving new pieces. Metal type was first invented in Korea during the Goryeo Dynasty around 1230. Hua Sui introduced bronze type printing to China in 1490 AD. However, the diffusion of both movable-type systems was limited and the technology did not spread beyond East and Central Asia.
Modern movable type, along with the mechanical printing press, is most often attributed to the goldsmith Johannes Gutenberg. His type pieces from a lead-based alloy suited printing purposes so well that the alloy is still used today. Gutenberg developed specialized techniques for casting and combining cheap copies of letterpunches in the vast quantities required to print multiple copies of texts. This technical breakthrough was instrumental in starting the Printing Revolution and printing the world's first book (with movable type) the Gutenberg Bible.
Computer technology revolutionized typography in the 20th century. Personal computers in the 1980s like the Macintosh allowed type designers to create types digitally using commercial graphic design software. Digital technology also enabled designers to create more experimental typefaces, alongside the practical fonts of traditional typography. Designs for typefaces could be created faster with the new technology, and for more specific functions. The cost for developing typefaces was drastically lowered, becoming widely available to the masses. The change has been called the "democratization of type" and has given new designers more opportunities to enter the field.
Evolution.
The design of typography has developed alongside the development of typesetting systems. Although typography has evolved significantly from its origins, it is a largely conservative art that tends to cleave closely to tradition. This is because legibility is paramount, and so the types that are the most readable are often retained. In addition, the evolution of typography is inextricably intertwined with lettering by hand and related art forms, especially formal styles, which thrived for centuries preceding typography, and so the evolution of typography must be discussed with reference to this relationship.
In the nascent stages of European printing, the type (blackletter, or Gothic) was designed in imitation of the popular hand-lettering styles of scribes. Initially, this type was difficult to read, because each letter was set in place individually and made to fit tightly into the allocated space. The art of manuscript writing, whose origin was in Hellenistic and Roman bookmaking, reached its zenith in the illuminated manuscripts of the Middle Ages. Metal types notably altered the style, making it "crisp and uncompromising", and also brought about "new standards of composition".
Claude Garamond, during the Renaissance period, was partially responsible for the adoption of Roman typeface in France, which supplanted Gothic (blackletter) fonts, which were more common. Roman type was also based on hand-lettering styles.
The Roman typeface’s development can be traced back to Greek lapidary letters. Although Greek lapidary letters are not examples of typography, since they were carved into stone, they were nonetheless "one of the first formal uses of Western letterforms"; after that, Roman lapidary letterforms transitioned into the monumental capitals, which laid the foundation for Western typographical design, especially serif typefaces. There are two styles of Roman typography: the old style, and the modern. The former is characterized by its similarly-weighted lines, while the latter is distinguished by its contrast of light and heavy lines. These styles are often combined.
By the twentieth century, computers turned type design into a rather simplified process. This has allowed the number of type styles to proliferate exponentially, as there are now thousands of fonts available.
Experimental typography.
Experimental typography is defined as the unconventional and more artistic approach to setting type. Francis Picabia was a Dada pioneer in the early 20th Century. David Carson is often associated with this movement, particularly for his work in "Ray Gun" magazine in the 1990s. His work caused an uproar in the design community due to his abandonment of standards in typesetting practices, layout, and design. Experimental typography places emphasis on communicating emotion, rather than on legibility.
Scope.
In contemporary use, the practice and study of typography is very broad, covering all aspects of letter design and application, both mechanical (typesetting and type design) and manual (handwriting and calligraphy). Typography can appear in a wide variety of situations, including:
Since digitization, typography has spread to a wider range of applications, appearing on web pages, LCD mobile phone screens, and hand-held video games.
Text typography.
In traditional typography, text is "composed" to create a readable, coherent, and visually satisfying whole that works invisibly, without the awareness of the reader. Even distribution of typeset material, with a minimum of distractions and anomalies, is aimed at producing clarity and transparency.
Choice of typeface(s) is the primary aspect of text typography—prose fiction, non-fiction, editorial, educational, religious, scientific, spiritual and commercial writing all have differing characteristics and requirements of appropriate typefaces and fonts. For historic material established text typefaces are frequently chosen according to a scheme of historical "genre" acquired by a long process of accretion, with considerable overlap between historical periods.
Contemporary books are more likely to be set with state-of-the-art seriffed "text romans" or "book romans" with design values echoing present-day design arts, which are closely based on traditional models such as those of Nicolas Jenson, Francesco Griffo (a punchcutter who created the model for Aldine typefaces), and Claude Garamond. With their more specialized requirements, newspapers and magazines rely on compact, tightly fitted seriffed text fonts specially designed for the task, which offer maximum flexibility, readability and efficient use of page space. Sans serif text fonts are often used for introductory paragraphs, incidental text and whole short articles. A current fashion is to pair sans-serif type for headings with a high-performance seriffed font of matching style for the text of an article.
Typography is modulated by orthography and linguistics, word structures, word frequencies, morphology, phonetic constructs and linguistic syntax. Typography is also subject to specific cultural conventions. For example, in French it is customary to insert a non-breaking space before a colon (:) or semicolon (;) in a sentence, while in English it is not.
Color.
In typography, "color" is the overall density of the ink on the page, determined mainly by the typeface, but also by the word spacing, leading and depth of the margins. Text layout, tone or color of the set text, and the interplay of text with the white space of the page in combination with other graphic elements impart a "feel" or "resonance" to the subject matter. With printed media typographers are also concerned with binding margins, paper selection and printing methods when determining the correct color of the page.
Principles of the craft.
Legibility is primarily the concern of the typeface designer, to ensure that each individual character or glyph is unambiguous and distinguishable from all other characters in the font. Legibility is also in part the concern of the typographer to select a typeface with appropriate clarity of design for the intended use at the intended size. An example of a well-known design, Brush Script, contains a number of illegible letters, since many of the characters can be easily misread especially if seen out of textual context.
Readability is primarily the concern of the typographer or information designer. It is the intended result of the complete process of presentation of textual material in order to communicate meaning as unambiguously as possible. A reader should be assisted in navigating around the information with ease, by optimal inter-letter, inter-word and particularly inter-line spacing, coupled with appropriate line length and position on the page, careful editorial "chunking" and choice of the text architecture of titles, folios, and reference links.
The two concepts are distinguished by Walter Tracy in "Letters of Credit": these ‘two aspects of a type’ are
fundamental to its effectiveness. Because the common meaning of "legible" is "readable" there are those – even some professionally involved in typography – who think that the term "legibility" is all that is needed in any discussion on the effectiveness of types. However, legibility and readability are separate, though connected aspects of type. Properly understood… the two terms can help to describe the character and function of type more precisely than legibility alone. … In typography we need to draw the definition… of legibility… to mean the quality of being decipherable and recognisable – so that we can say, for example, that the lowercase h in a particular old style italic is not legible in small sizes because its in-turned leg makes it look like the letter b; or a figure 3 in a classified advertisement is too similar to the 8. … In display sizes, legibility ceases to be a serious matter; a character that causes uncertainty at 8 point size is plain enough at 24 point.
Note that the above applies to people with 20/20 vision at appropriate reading distance and under optimal lighting. The analogy of an opticians chart, testing for visual acuity and independent of meaning, is useful to indicate the scope of the concept of legibility.
In typography… if the columns of a newspaper or magazine or the pages of a book can be read for many minutes at a time without strain or difficulty, then we can say the type has good readability. The term describes the quality of visual comfort – an important requirement in the comprehension of long stretches of text but, paradoxically, not so important in such things as telephone directories or air-line time-tables, where the reader is not reading continuously but searching for a single item of information. The difference in the two aspects of visual effectiveness is illustrated by the familiar argument on the suitability of sans-serif types for text setting. The characters in a particular sans-serif face may be perfectly legible in themselves, but no one would think of setting a popular novel in it because its readability is low.
Legibility ‘refers to perception’ and readability ‘refers to comprehension’. Typographers aim to achieve excellence in both.
"The typeface chosen should be legible. That is, it should be read without effort. Sometimes legibility is simply a matter of type size; more often, however, it is a matter of typeface design. In general, typefaces that are true to the basic letterforms are more legible than typefaces that have been condensed, expanded, embellished, or abstracted.
However, even a legible typeface can become unreadable through poor setting and placement, just as a less legible typeface can be made more readable through good design.
Studies of both legibility and readability have examined a wide range of factors including type size and type design. For example, comparing serif vs. sans-serif type, roman type vs. "oblique type" and "italic type", line length, line spacing, color contrast, the design of right-hand edge (for example, justification, straight right hand edge) vs. ranged left, and whether text is hyphenated.
Legibility research has been published since the late nineteenth century. Although there are often commonalities and agreement on many topics, others often create poignant areas of conflict and variation of opinion. For example, no one has provided a conclusive answer as to which font, serifed or sans serif, provides the most legibility according to Alex Poole. Other topics such as justified "vs" unjustified type, use of hyphens, and proper fonts for people with reading difficulties such as dyslexia, have continued to be subjects of debate.
Legibility is usually measured through speed of reading, with comprehension scores used to check for effectiveness (that is, not a rushed or careless read). For example, Miles Tinker, who published numerous studies from the 1930s to the 1960s, used a speed of reading test that required participants to spot incongruous words as an effectiveness filter.
The "Readability of Print Unit" at the Royal College of Art under Professor Herbert Spencer with Brian Coe and Linda Reynolds did important work in this area and was one of the centres that revealed the importance of the saccadic rhythm of eye movement for readability—in particular, the ability to take in (i.e., recognise the meaning of groups of) around three words at once and the physiognomy of the eye, which means the eye tires if the line required more than 3 or 4 of these saccadic jumps. More than this is found to introduce strain and errors in reading (e.g. Doubling).
These days, legibility research tends to be limited to critical issues, or the testing of specific design solutions (for example, when new typefaces are developed). Examples of critical issues include typefaces for people with visual impairment, and typefaces for highway signs, or for other conditions where legibility may make a key difference.
Much of the legibility research literature is somewhat atheoretical—various factors were tested individually or in combination (inevitably so, as the different factors are interdependent), but many tests were carried out in the absence of a model of reading or visual perception. Some typographers believe that the overall word shape (Bouma) is very important in readability, and that the theory of parallel letterwise recognition is either wrong, less important, or not the entire picture.
Studies distinguishing between Bouma recognition and parallel letterwise recognition with regard to how people actually recognize words when they read, have favored parallel letterwise recognition, which is widely accepted by cognitive psychologists.
Some commonly agreed findings of legibility research include:
Readability can also be compromised by letter-spacing, word spacing, or leading that is too tight or too loose. It can be improved when generous vertical space separates lines of text, making it easier for the eye to distinguish one line from the next, or previous line. Poorly designed fonts and those that are too tightly or loosely fitted can also result in poor legibility.
Typography is an element of all printed material. Periodical publications, especially newspapers and magazines, use typographical elements to achieve an attractive, distinctive appearance, to aid readers in navigating the publication, and in some cases for dramatic effect. By formulating a style guide, a periodical standardizes on a relatively small collection of typefaces, each used for specific elements within the publication, and makes consistent use of type sizes, italic, boldface, large and small capital letters, colors, and other typographic features. Some publications, such as "The Guardian" and "The Economist", go so far as to commission a type designer to create customized typefaces for their exclusive use.
Different periodical publications design their publications, including their typography, to achieve a particular tone or style. For example, "USA Today" uses a bold, colorful, and comparatively modern style through their use of a variety of typefaces and colors; type sizes vary widely, and the newspaper's name is placed on a colored background. In contrast, "The New York Times" uses a more traditional approach, with fewer colors, less typeface variation, and more columns.
Especially on the front page of newspapers and on magazine covers, headlines are often set in larger display typefaces to attract attention, and are placed near the masthead.
Display typography.
Display typography is a potent element in graphic design, where there is less concern for readability and more potential for using type in an artistic manner. Type is combined with negative space, graphic elements and pictures, forming relationships and dialog between words and images.
Color and size of type elements are much more prevalent than in text typography. Most display typography exploits type at larger sizes, where the details of letter design are magnified. Color is used for its emotional effect in conveying the tone and nature of subject matter.
Display typography encompasses:
Advertising.
Typography has long been a vital part of promotional material and advertising. Designers often use typography to set a theme and mood in an advertisement; for example using bold, large text to convey a particular message to the reader. Type is often used to draw attention to a particular advertisement, combined with efficient use of color, shapes and images. Today, typography in advertising often reflects a company's brand. Fonts used in advertisements convey different messages to the reader, classical fonts are for a strong personality, while more modern fonts are for a cleaner, neutral look. Bold fonts are used for making statements and attracting attention. In communicating a message, a balance has to be achieved between the visual and the verbal aspects in design. Digital technology in the 20th and 21st centuries has enabled the creation of typefaces for advertising that are more experimental than traditional typefaces.
Inscriptional and architectural lettering.
The history of inscriptional lettering is intimately tied to the history of writing, the evolution of letterforms and the craft of the hand. The widespread use of the computer and various etching and sandblasting techniques today has made the hand carved monument a rarity, and the number of letter-carvers left in the US continues to dwindle.
For monumental lettering to be effective it must be considered carefully in its context. Proportions of letters need to be altered as their size and distance from the viewer increases. An expert letterer gains understanding of these nuances through much practice and observation of their craft. Letters drawn by hand and for a specific project have the possibility of being richly specific and profoundly beautiful in the hand of a master. Each can also take up to an hour to carve, so it is no wonder that the automated sandblasting process has become the industry standard.
To create a sandblasted letter, a rubber mat is laser cut from a computer file and glued to the stone. The sand then bites a coarse groove or channel into the exposed surface. Unfortunately, many of the computer applications that create these files and interface with the laser cutter do not have many typefaces available, and often have inferior versions of typefaces that are available. What can now be done in minutes, however, lacks the striking architecture and geometry of the chisel-cut letter that allows light to play across its distinct interior planes.
Notes.
General References.
</dl>

</doc>
<doc id="31278" url="http://en.wikipedia.org/wiki?curid=31278" title="Tudor dynasty">
Tudor dynasty

The Tudor dynasty or House of Tudor was a royal house of Welsh and English origin,
descended in the male line from the Tudors of Penmynydd. Tudor monarchs ruled the Kingdom of England and its realms, including their ancestral Wales and the Lordship of Ireland (later the Kingdom of Ireland) from 1485 until 1603. The first monarch, Henry VII, descended through his mother from a legitimised branch of the English royal House of Lancaster. The Tudor family rose to power in the wake of the Wars of the Roses, which left the House of Lancaster, to which the Tudors were aligned, extinct.
Henry Tudor was able to establish himself as a candidate not only for traditional Lancastrian supporters, but also for the discontented supporters of their rival House of York, and he rose to capture the throne in battle, becoming Henry VII. His victory was reinforced by his marriage to Elizabeth of York, symbolically uniting the former warring factions under a new dynasty. The Tudors extended their power beyond modern England, achieving the full union of England and the Principality of Wales in 1542 (Laws in Wales Acts 1535–1542), and successfully asserting English authority over the Kingdom of Ireland. They also maintained the nominal English claim to the Kingdom of France; although none of them made substance of it, Henry VIII fought wars with France trying to reclaim that title. After him, his daughter Mary I lost control of all territory in France permanently with the fall of Calais in 1558.
In total, five Tudor monarchs ruled their domains for just over a century. Henry VIII of England was the only male-line male heir of Henry VII to live to the age of maturity. Issues around the royal succession (including marriage and the succession rights of women) became major political themes during the Tudor era. The House of Stuart came to power in 1603 when the Tudor line failed, as Elizabeth I died without issue.
Ascent to the throne.
The Tudors are descended on Henry VII's mother's side from John Beaufort, 1st Earl of Somerset, one of the illegitimate children of the 14th century English Prince John of Gaunt, 1st Duke of Lancaster (the third surviving son of Edward III of England) by Gaunt's long-term mistress Katherine Swynford. The descendants of an illegitimate child of English Royalty would normally have no claim on the throne, but the situation was complicated when Gaunt and Swynford eventually married in 1399, when John Beaufort was 25. The church retroactively declared the Beauforts legitimate by way of a papal bull the same year, confirmed by an Act of Parliament in 1397. A subsequent proclamation by John of Gaunt's legitimate son, Henry IV of England, also recognised the Beauforts' legitimacy, but declared them ineligible ever to inherit the throne. Nevertheless, the Beauforts remained closely allied with Gaunt's legitimate descendants from his first marriage, the House of Lancaster.
On 1 November 1455, John Beaufort's granddaughter, Margaret Beaufort, Countess of Richmond and Derby, married Henry VI of England's half-brother Edmund Tudor, 1st Earl of Richmond. It was his father, Owen Tudor (Welsh: "Owain ap Maredudd ap Tewdur ap Goronwy ap Tewdur ap Goronwy ap Ednyfed Fychan"), who abandoned the Welsh patronymic naming practice and adopted a fixed surname. When he did, he did not choose, as was generally the custom, his father’s name, Maredudd, but chose his grandfather’s instead. Tewdur or Tudor is derived from the words "tud" "territory" and "rhi" "king".
Owen Tudor was one of the body guards for Queen Dowager Catherine of Valois, whose husband, Henry V of England, had died in 1422. Evidence suggests that the two were secretly married in 1429. The two sons born of the marriage, Edmund and Jasper, were among the most loyal supporters of the House of Lancaster in its struggle against the House of York.
Henry VI ennobled his half brothers. Edmund became earl of Richmond and was married to Margaret Beaufort, the great-granddaughter of John of Gaunt, the progenitor of the house of Lancaster. Jasper became earl of Pembroke and by 1460 had collected so many offices in Wales that he had become the virtual viceroy of the country. Edmund died in November 1456. On 28 January 1457, his widow, who had just attained her fourteenth birthday, gave birth to a son, Henry VII of England, at her brother-in-law’s castle of Pembroke.
Henry Tudor spent his childhood at Raglan Castle, the home of William Herbert, 1st Earl of Pembroke, a leading Yorkist. Following the murder of Henry VI and his son, Edward, in 1471, Henry became the person upon whom the Lancastrian cause rested. Concerned for his young nephew's life, Jasper Tudor took Henry to Brittany for safety. Lady Margaret remained in England and remarried, living quietly while advancing the Lancastrian, and her son's cause. Capitalizing on the growing unpopularity of King Richard III of England, she was able to forge an alliance with discontented Yorkists in support of her son. Two years after Richard III was crowned, Henry and Jasper sailed from the mouth of the Seine to the Milford Haven Waterway and defeated Richard III at the Battle of Bosworth Field. Upon this victory, Henry Tudor proclaimed himself King Henry VII.
Henry VII.
Now King, Henry's first concern was to secure his hold on the throne. On 18 January 1486 at Westminster, he honoured a pledge made three years earlier and married Elizabeth of York. They were third cousins, as both were great-great-grandchildren of John of Gaunt. The marriage unified the warring houses of Lancaster and York and gave his children a strong claim to the throne. The unification of the two houses through this marriage is symbolized by the heraldic emblem of the Tudor rose, a combination of the white rose of York and the red rose of Lancaster.
Henry VII and Queen Elizabeth had several children, four of whom survived infancy: Arthur, Prince of Wales; Henry, Duke of Richmond; Margaret, who married James IV of Scotland; and Mary, who married Louis XII of France. One of the objectives of Henry VII's foreign policy was dynastic security, which is portrayed through the alliance forged with the marriage of his daughter Margaret to James IV of Scotland and through the marriage of his eldest son. Henry VII married his son Arthur to Catherine of Aragon, cementing an alliance with the Spanish monarchs, Ferdinand II of Aragon and Isabella I of Castile, and the two spent their honeymoon at Ludlow Castle, the traditional seat of the Prince of Wales. However, four months after the marriage, Arthur died, leaving his younger brother Henry as heir apparent. Henry VII acquired a papal dispensation allowing Prince Henry to marry Arthur's widow; however, Henry VII delayed the marriage. Henry VII limited his involvement in European politics. He went to war only twice, once in 1489 during the Breton crisis and the invasion of Brittany, and in 1496–1497 in revenge for Scottish support of Perkin Warbeck and for their invasion of Northern England. Henry VII made peace with France in 1492 and the war against Scotland was abandoned because of the Western Rebellion of 1497. Henry VII came to peace with James IV in 1502, paving the way for the marriage of his daughter Margaret.
One of the main concerns of Henry VII during his reign was the re-accumulation of the funds in the royal treasury. England had never been one of the wealthier European countries, and after the War of the Roses this was even more true. Through his strict monetary strategy, he was able to leave a considerable amount of money in the Treasury for his son and successor, Henry VIII. Although it is debated whether Henry VII was a great king, he certainly was a successful one if only because he restored the nation's finances, strengthened the judicial system and successfully denied all other claimants to the throne, thus further securing it for his heir.
Henry VIII.
The new King Henry VIII married Catherine of Aragon on 11 June 1509; they were crowned at Westminster Abbey on 24 June the same year. Catherine was Henry's older brother's wife, making the path for their marriage a rocky one from the start. A papal dispensation had to be granted for Henry to be able to marry Catherine, and the negotiations took some time. Despite the fact that Henry's father died before he was married to Catherine, he was determined to marry her anyway and make sure that everyone knew he intended on being his own master. When Henry first came to the throne, he had very little interest in actually ruling; rather, he preferred to indulge in luxuries and to partake in sports. He let others control the kingdom for the first two years of his reign, and then when he became more interested in military strategy, he took more interest in ruling his own throne. In his younger years, Henry was described as a man of gentle friendliness, gentle in debate, and who acted as more of a companion than a king. He was generous in his gifts and affection and was said to be easy to get along with. However, the Henry that many people picture when they hear his name is the Henry of his later years, when he became obese, volatile, and was known for his great cruelty. Unfortunately, Catherine did not bear Henry the sons he was desperate for; Catherine's first child, a daughter, was stillborn, and her second child, a son named Henry, Duke of Cornwall, died 52 days after the birth. A further set of stillborn children were conceived, until a daughter Mary was born in 1516. When it became clear to Henry that the Tudor dynasty was at risk, he consulted his chief minister Cardinal Thomas Wolsey about the possibility of annulling his marriage to Catherine. Along with Henry's concern that he would not have an heir, it was also obvious to his court that he was becoming tired of his aging wife, who was six years older than he. Wolsey visited Rome, where he hoped to get the Pope's consent for an annulment. However, the church was reluctant to rescind the earlier papal dispensation and felt heavy pressure from Catherine's nephew, Charles V, Holy Roman Emperor, in support of his aunt. Catherine contested the proceedings, and a protracted legal battle followed. Wolsey fell from favour as a result of his failure to procure the annulment, and Henry appointed Thomas Cromwell in his place. Despite his failure to produce the results that Henry wanted, Wolsey actively pursued the annulment—divorce was synonymous with annulment at that time—however, he never planned that Henry would marry Anne Boleyn, with whom the king had become enamoured while she was lady-in-waiting in Queen Catherine's household. It is unclear how far Wolsey was actually responsible for the Reformation, but it is very clear that Henry's desire to marry Anne Boleyn precipitated the schism with the Church. Henry's concern about having an heir to secure his family line and increase his security while alive would have prompted him to ask for a divorce sooner or later, whether Anne had precipitated it or not. Only Wolsey's sudden death at Leicester on his journey to the Tower of London saved him from the public humiliation and inevitable execution he would have suffered upon his arrival at the Tower.
Break with Rome.
In order to allow Henry to divorce his wife, the English parliament enacted laws breaking ties with Rome, and declaring the king Supreme Head of the Church of England (from Elizabeth I the monarch is known as the Supreme Governor of the Church of England), thus severing the ecclesiastical structure of England from the Catholic Church and the Pope. The newly appointed Archbishop of Canterbury, Thomas Cranmer, was then able to declare Henry's marriage to Catherine annulled. Catherine was removed from Court, and she spent the last three years of her life in various English houses under "protectorship," similar to house arrest. This allowed Henry to marry one of his courtiers Anne Boleyn, the daughter of a minor diplomat Sir Thomas Boleyn. Anne had become pregnant by the end of 1532 and gave birth on 7 September 1533 to Elizabeth named in honour of Henry's mother. Anne may have had later pregnancies which ended in miscarriage or stillbirth. In May 1536, Anne was arrested, along with six courtiers. Thomas Cromwell stepped in again, claiming that Anne had taken lovers during her marriage to Henry, and she was tried for high treason, witchcraft and incest; these charges were most likely fabricated, but she was found guilty, and executed in May 1536.
Protestant alliance.
Henry married again, for the third time, to Jane Seymour, the daughter of a Wiltshire knight, and with whom he had become enamoured while she was still a lady-in-waiting to Queen Anne. Jane became pregnant, and in 1537 produced a son, who became King Edward VI following Henry's death in 1547. Jane died of puerperal fever only a few days after the birth, leaving Henry devastated. Cromwell continued to gain the king's favour when he designed and pushed through the Laws in Wales Acts, uniting England and Wales.
In 1540 Henry married for the fourth time to the daughter of a Protestant German duke, Anne of Cleves, thus forming an alliance with the Protestant German states. Henry was reluctant to marry again, especially to a Protestant, but he was persuaded when the court painter Hans Holbein the Younger showed him a flattering portrait of her. She arrived in England in December 1539, and Henry rode to Rochester to meet her on 1 January 1540. Although the historian Gilbert Burnet claimed that Henry called her a "Flanders Mare", there is no evidence that he said this; in truth, court ambassadors negotiating the marriage praised her beauty. Whatever the circumstances were, the marriage failed, and Anne agreed to a peaceful annulment, assumed the title "My Lady, the King's Sister", and received a massive divorce settlement, which included Richmond Palace, Hever Castle, and numerous other estates across the country. Although the marriage made sense in terms of foreign policy, Henry was still enraged and offended by the match. Henry chose to blame Cromwell for the failed marriage, and ordered him beheaded on 28 July 1540. Henry kept his word and took care of Anne in his last years alive; however, after his death Anne suffered from extreme financial hardship because Edward VI's councillors refused to give her any funds and confiscated the homes she had been given. She pleaded to her brother to let her return home, but he only sent a few agents who tried to assist in helping her situation and refused to let her return home. Anne died on 16 July 1557 in Chelsea Manor.
The fifth marriage was to the Catholic Catherine Howard, the niece of Thomas Howard, the third Duke of Norfolk, who was promoted by Norfolk in the hope that she would persuade Henry to restore the Catholic religion in England. Henry called her his “rose without a thorn”, but the marriage ended in failure. Henry's fancy with Catherine started before the end of his marriage with Anne when she was still a member of Anne's court. Catherine was young and vivacious, but Henry's age made him less inclined to use Catherine in the bedroom; rather, he preferred to admire her, which Catherine soon grew tired of. Catherine, forced into a marriage to an unattractive, obese man over 30 years her senior, had never wanted to marry Henry, and conducted an affair with the King's favourite, Thomas Culpeper, while Henry and she were married. During her questioning, Catherine first denied everything but eventually she was broken down and told of her infidelity and her pre-nuptial relations with other men. Henry, first enraged, threatened to torture her to death but later became overcome with grief and self-pity. She was accused of treason and was executed on 13 February 1542, destroying the English Catholic holdouts' hopes of a national reconciliation with the Catholic Church. Her execution also marked the end of the Howard family's power within the court.
By the time Henry conducted another Protestant marriage with his final wife Catherine Parr in 1543, the old Roman Catholic advisers, including the powerful third Duke of Norfolk had lost all their power and influence. The duke himself was still a committed Catholic, and he was nearly persuaded to arrest Catherine for preaching Lutheran doctrines to Henry while she attended his ill health. However, she managed to reconcile with the King after vowing that she had only argued about religion with him to take his mind off the suffering caused by his ulcerous leg. Her peacemaking also helped reconcile Henry with his daughters Mary and Elizabeth and fostered a good relationship between her and the crown prince.
Meanwhile, Edward was brought up a strict and devout Protestant by numerous tutors, including Bishop Richard Cox, John Belmain, and Sir John Cheke. The lady in charge of his upbringing was Blanche Herbert Lady Troy, whose ancestors had residual Lollard connections. Her elegy includes the lines: ...To King Edward she was a true – (And) wise lady of dignity, – In charge of his fosterage (she was pre-eminent)...
Edward VI: Protestant zeal.
Henry died on 28 January 1547. His will had reinstated his daughters by his annulled marriages to Catherine of Aragon and Anne Boleyn to the line of succession, but did not legitimise them. (Because his marriages had been annulled, they legally never occurred, so his children by those marriages were illegitimate.) In the event that all 3 of his children died without heir, the will stipulated that the descendant of his younger sister Mary would take precedence over the descendants of his elder sister, Margaret, Queen of Scotland. Edward, his nine-year-old son by Jane Seymour, succeeded as Edward VI of England. Unfortunately, the young King's kingdom was usually in turmoil between nobles who were trying to strengthen their own position in the kingdom by using the Regency in their favour.
Duke of Somerset's England.
Although Henry had specified a group of men to act as regents during Edward's minority, Edward Seymour, Edward's uncle, quickly seized complete control, and created himself Duke of Somerset on 15 February 1547. His domination of the Privy Council, the king's most senior body of advisers, was unchallenged. Somerset aimed to unite England and Scotland by marrying Edward to the young Scottish queen Mary, and aimed to forcibly impose the English Reformation on the Church of Scotland. Somerset led a large and well equipped army to Scotland, where he and the Scottish regent James Hamilton, 2nd Earl of Arran, commanded their armies at the Battle of Pinkie Cleugh on 10 September 1547. Somerset's army eventually defeated the Scots, but the young Queen Mary was smuggled to France, where she was betrothed to the Dauphin, the future Francis II of France. Despite Somerset's disappointment that no Scottish marriage would take place, his victory at Pinkie Cleugh made his position appear unassailable.
Meanwhile, Edward VI, despite the fact that he was only a child of nine, had his mind set on religious reform. In 1549, Edward ordered the publication of the Book of Common Prayer, containing the forms of worship for daily and Sunday church services. The controversial new book was not welcomed by either reformers or Catholic conservatives; and it was especially condemned in Devon and Cornwall, where traditional Catholic loyalty was at its strongest. In Cornwall at the time, many of the people could only speak the Cornish language, so the uniform English Bibles and church services were not understood by many. This caused the Prayer Book Rebellion, in which groups of Cornish non-conformists gathered round the mayor. The rebellion worried Somerset, now Lord Protector, and he sent an army to impose military solution to the rebellion. One in ten of the indigenous Cornish population was slaughtered. The rebellion did not persuade Edward to tread carefully, and only hardened his attitude towards Catholic non-conformists. This extended to Edward's elder sister, the daughter of Catherine of Aragon, Mary Tudor, who was a pious and devout Catholic. Although called before the Privy Council several times to renounce her faith and stop hearing the Catholic Mass, she refused. He had a good relationship with his sister Elizabeth, who was a Protestant, albeit a moderate one, but this was strained when Elizabeth was accused of having an affair with the Duke of Somerset's brother, Thomas Seymour, 1st Baron Seymour of Sudeley, the husband of Henry's last wife Catherine Parr. Elizabeth was interviewed by one of Edward's advisers, and she was eventually found not to be guilty, despite forced confessions from her servants Catherine Ashley and Thomas Parry. Thomas Seymour was arrested and beheaded on 20 March 1549.
Problematic succession.
Lord Protector Somerset was also losing favour. After forcibly removing Edward VI to Windsor Castle, with the intention of keeping him hostage, Somerset was removed from power by members of the council, led by his chief rival, John Dudley, the first Earl of Warwick, who created himself Duke of Northumberland shortly after his rise. Northumberland effectively became Lord Protector, but he did not use this title, learning from the mistakes his predecessor made. Northumberland was furiously ambitious, and aimed to secure Protestant uniformity while making himself rich with land and money in the process. He ordered churches to be stripped of all traditional Catholic symbolism, resulting in the simplicity often seen in Church of England churches today. A revision of the Book of Common Prayer was published in 1552. When Edward VI became ill in 1553, his advisers looked to the possible imminent accession of the Catholic Lady Mary, and feared that she would overturn all the reforms made during Edward's reign. Perhaps surprisingly, it was the dying Edward himself who feared a return to Catholicism, and wrote a new will repudiating the 1544 will of Henry VIII. This gave the succession to his cousin Lady Jane Grey, the granddaughter of Henry VIII's sister Mary Tudor, who, after the death of Louis XII of France in 1515 had married Henry VIII's favourite Charles Brandon, the first Duke of Suffolk. Lady Jane's mother was Lady Frances Brandon, the daughter of Suffolk and Princess Mary. Northumberland married Jane to his youngest son Guildford Dudley, allowing himself to get the most out of a necessary Protestant succession. Most of Edward's council signed the "Devise for the Succession", and when Edward VI died on 6 July 1553 from his battle with tuberculosis, Lady Jane was proclaimed queen. However, the popular support for the proper Tudor dynasty–even a Catholic member–overruled Northumberland's plans, and Jane, who had never wanted to accept the crown, was deposed after just nine days. Mary's supporters joined her in a triumphal procession to London, accompanied by her younger sister Elizabeth.
Mary I: A troubled queen's reign.
However, Mary soon announced that she was intending to marry the Spanish prince Philip, son of her mother's nephew Charles V, Holy Roman Emperor. The prospect of a marriage alliance with Spain proved unpopular with the English people, who were worried that Spain would use England as a satellite, involving England in wars without the popular support of the people. Popular discontent grew; a Protestant courtier, Thomas Wyatt the younger led a rebellion against Mary, with the aim of deposing and replacing her with her half-sister Elizabeth. The plot was discovered, and Wyatt's supporters were hunted down and killed. Wyatt himself was tortured, in the hope that he would give evidence that Elizabeth was involved so that Mary could have her executed for treason. Wyatt never implicated Elizabeth, and he was beheaded. Elizabeth spent her time between different prisons, including the Tower of London.
Mary married Philip at Winchester Cathedral, on 25 July 1554. Philip found her unattractive, and only spent a minimal amount of time with her. Despite Mary believing she was pregnant numerous times during her five-year reign, she never reproduced. Devastated that she rarely saw her husband, and anxious that she was not bearing an heir to Catholic England, Mary became bitter. In her determination to restore England to the Catholic faith and to secure her throne from Protestant threats, she had many Protestants burnt at the stake between 1555 and 1558. Mary's main goal was to restore the Catholic faith to England; however, the Marian Persecutions were unpopular with the Protestant majority of England, though naturally supported by the Catholic minority. Because of her actions against the Protestants, Mary is to this day referred to as "Bloody Mary". English author Charles Dickens stated that "as bloody Queen Mary this woman has become famous, and as Bloody Queen Mary she will ever be remembered with horror and detestation"
Mary's dream of a resurrected Catholic Tudor dynasty was finished, and her popularity further declined when she lost the last English area on French soil, Calais, to Francis, Duke of Guise, on 7 January 1558. Mary's reign, however, introduced a new coining system that would be used until the 18th century, and her marriage to Philip II created new trade routes for England. Mary's government took a number of steps towards reversing the inflation, budgetary deficits, poverty, and trade crisis of her kingdom. She explored the commercial potential of Russian, African, and Baltic markets, revised the customs system, worked to counter the currency debasements of her predecessors, amalgamated several revenue courts, and strengthened the governing authority of the middling and larger towns. Mary also welcomed the first Russian ambassador to England, creating relations between England and Russia for the first time. Had she lived a little longer, then the Catholic religion that she worked so hard to restore into the realm may have taken deeper roots than it did; however, Mary died on 17 November 1558 at the relatively young age of 42.
The age of intrigues and plots: Elizabeth I.
Elizabeth I, who was staying at Hatfield House at the time of her accession, rode to London to the cheers of both the ruling class and the common people.
When Elizabeth came to the throne, there was much apprehension among members of the council appointed by Mary, due to the fact that many of them (as noted by the Spanish ambassador) had participated in several plots against Elizabeth, such as her imprisonment in the Tower, trying to force her to marry a foreign prince and thereby sending her out of the realm, and even pushing for her death. In response to their fear, she chose as her chief minister Sir William Cecil, a Protestant, and former secretary to Lord Protector the Duke of Somerset and then to the Duke of Northumberland. Under Mary, he had been spared, and often visited Elizabeth, ostensibly to review her accounts and expenditure. He was the cousin and friend of Blanche Parry, the closest person to Elizabeth for 56 years. Elizabeth also appointed her personal favourite, the son of the Duke of Northumberland Lord Robert Dudley, her Master of the Horse, giving him constant personal access to the queen.
The early years.
Elizabeth had a long, turbulent path to the throne. She had a number of problems during her childhood, one of the main ones being after the execution of her mother, Anne Boleyn. When Anne was beheaded, Henry declared Elizabeth an illegitimate child and she would, therefore, not be able to inherit the throne. After the death of her father, she was raised by his widow, Catherine Parr and her husband Thomas Seymour, 1st Baron Seymour of Sudeley. A scandal arose with her and the Lord Admiral to which she stood trial. During the examinations, she answered truthfully and boldly and all charges were dropped. She was an excellent student, well-schooled in Latin, French, Italian, and somewhat in Greek, and was a talented writer. She was supposedly a very skilled musician as well, in both singing and playing the lute. After the rebellion of Thomas Wyatt the younger, Elizabeth was imprisoned in the Tower of London. No proof could be found that Elizabeth was involved and she was released and retired to the countryside until the death of her sister, Mary I of England.
Imposing the Church of England.
Elizabeth was a moderate Protestant; she was the daughter of Anne Boleyn, who played a key role in the English Reformation in the 1520s. She had been brought up by Blanche Herbert Lady Troy. At her coronation in January 1559, many of the bishops – Catholic, appointed by Mary, who had expelled many of the Protestant clergymen when she became queen in 1553 – refused to perform the service in English. Eventually, the relatively minor Bishop of Carlisle, Owen Oglethorpe, performed the ceremony; but when Oglethorpe attempted to perform traditional Catholic parts of the Coronation, Elizabeth got up and left. Following the Coronation, two important Acts were passed through parliament: the Act of Uniformity and the Act of Supremacy, establishing the Protestant Church of England and creating Elizabeth Supreme Governor of the Church of England ("Supreme Head", the title used by her father and brother, was seen as inappropriate for a woman ruler). These acts, known collectively as the Elizabethan Religious Settlement, made it compulsory to attend church services every Sunday; and imposed an oath on clergymen and statesmen to recognise the Church of England, the independence of the Church of England from the Catholic Church, and the authority of Elizabeth as Supreme Governor. Elizabeth made it clear that if they refused the oath the first time, they would have a second opportunity, after which, if the oath was not sworn, the offenders would be deprived of their offices and estates.
Pressure to marry.
Even though Elizabeth was only twenty-five when she came to the throne, she was absolutely sure of her God-given place to be the queen and of her responsibilities as the 'handmaiden of the Lord'. She never let anyone challenge her authority as queen, even though many people, who felt she was weak and should be married, tried to do so. The popularity of Elizabeth was extremely high, but her Privy Council, her Parliament and her subjects thought that the unmarried queen should take a husband; it was generally accepted that, once a queen regnant was married, the husband would relieve the woman of the burdens of head of state. Also, without an heir, the Tudor dynasty would end; the risk of civil war between rival claimants was a possibility if Elizabeth died childless. Numerous suitors from nearly all European nations sent ambassadors to English court to put forward their suit. Risk of death came dangerously close in 1564 when Elizabeth caught smallpox; when she was most at risk, she named Robert Dudley as Lord Protector in the event of her death. After her recovery, she appointed Dudley to the Privy Council and created him Earl of Leicester, in the hope that he would marry Mary, Queen of Scots. Mary rejected him, and instead married Henry Stuart, Lord Darnley, a descendant of Henry VII, giving Mary a stronger claim to the English throne. Although many Catholics were loyal to Elizabeth, many also believed that, because Elizabeth was declared illegitimate after her parents' marriage was annulled, Mary was the strongest legitimate claimant. Despite this, Elizabeth would not name Mary her heir; as she had experienced during the reign of her predecessor Mary I, the opposition could flock around the heir if they were disheartened with Elizabeth's rule.
Numerous threats to the Tudor dynasty occurred during Elizabeth's reign. In 1569, a group of Earls led by Charles Neville, the sixth Earl of Westmorland, and Thomas Percy, the seventh Earl of Northumberland attempted to depose Elizabeth and replace her with Mary, Queen of Scots. In 1571, the Protestant-turned-Catholic Thomas Howard, the fourth Duke of Norfolk, had plans to marry Mary, Queen of Scots, and then replace Elizabeth with Mary. The plot, masterminded by Roberto di Ridolfi, was discovered and Norfolk was beheaded. The next major uprising was in 1601, when Robert Devereux, the second Earl of Essex, attempted to raise the city of London against Elizabeth's government. The city of London proved unwilling to rebel; Essex and most of his co-rebels were executed. Threats also came from abroad. In 1570, Pope Pius V issued a Papal bull, "Regnans in Excelsis", excommunicating Elizabeth, and releasing her subjects from their allegiance to her. Elizabeth came under pressure from Parliament to execute Mary, Queen of Scots, to prevent any further attempts to replace her; though faced with several official requests, she vacillated over the decision to execute an anointed queen. Finally, she was persuaded of Mary's (treasonous) complicity in the plotting against her, and she signed the death warrant in 1586. Mary was executed at Fotheringay Castle on 8 February 1587, to the outrage of Catholic Europe.
There are many reasons debated as to why Elizabeth never married. It was rumoured that she was in love with Robert Dudley, 1st Earl of Leicester, and that on one of her summer progresses she had birthed his illegitimate child. This rumour was just one of many that swirled around the two's long-standing friendship. However, more important to focus on were the disasters that many women, such as Lady Jane Grey, suffered due to being married into the royal family. Her sister Mary's marriage to Philip brought great contempt to the country, for many of her subjects despised Spain and Philip and feared that he would try to take complete control. Recalling her father's disdain for Anne of Cleves, Elizabeth also refused to enter into a foreign match with a man that she had never seen before, so that also eliminated a large number of suitors.
Last hopes of a Tudor heir.
Despite the uncertainty of Elizabeth's – and therefore the Tudor dynasty's – hold on England, she never married. The closest she came to marriage was between 1579 and 1581, when she was courted by Francis, Duke of Anjou, the son of Henry II of France and Catherine de' Medici. Despite Elizabeth's government constantly begging her to marry in the early years of her reign, it was now persuading Elizabeth not to marry the French prince for his mother, Catherine de' Medici, was suspected of ordering the St Bartholomew's Day massacre of tens of thousands of French Protestant Huguenots in 1572. Elizabeth bowed to public feeling against the marriage, learning from the mistake her sister made when she married Philip II of Spain, and sent the Duke of Anjou away. Elizabeth knew that the continuation of the Tudor dynasty was now impossible; she was forty-eight in 1581, and too old to bear children.
By far the most dangerous threat to the Tudor dynasty during Elizabeth's reign was the Spanish Armada of 1588. Launched by Elizabeth's old suitor Philip II of Spain, this was commanded by Alonso de Guzmán El Bueno, the seventh Duke of Medina Sidonia. The Spanish invasion fleet outnumbered the English fleet's 22 galleons and 108 armed merchant ships; however, the Spanish lost as a result of bad weather on the English Channel and poor planning and logistics, and in the face of the skills of Sir Francis Drake and Charles Howard, the second Baron Howard of Effingham (later first Earl of Nottingham).
While Elizabeth declined physically with age, her running of the country continued to benefit her people. In response to famine across England due to bad harvests in the 1590s, Elizabeth introduced the poor law, allowing peasants who were too ill to work a certain amount of money from the state. All the money Elizabeth had borrowed from Parliament in 12 of the 13 parliamentary sessions was paid back; by the time of her death, Elizabeth not only had no debts, but was in credit. Elizabeth died childless at Richmond Palace on 24 March 1603. She never named a successor. However, her chief minister Sir Robert Cecil had corresponded with the Protestant King James VI of Scotland, son of Mary, Queen of Scots, and James's succession to the English throne was unopposed. The Tudor dynasty survived only in the female line, and the House of Stuart occupied the English throne for most of the following century.
Before and after comparisons.
Public interference regarding the Roses dynasties was always a threat until the 17th century Stuart/Bourbon re-alignment occasioned by a series of events such as the execution of Lady Jane Grey, despite her brother in law, Leicester's reputation in Holland, the Rising of the North (in which the old Percy-Neville feud and even anti-Scottish sentiment was discarded on account of religion; Northern England shared the same Avignonese bias as the Scottish court, on par with Valois France and Castile, which became the backbone of the Counter-Reformation, with Protestants being solidly anti-Avignonese) and death of Elizabeth I of England without children.
The Tudors made no substantial changes in their foreign policy from either Lancaster or York, whether the alliance was with Aragon or Cleves, the chief foreign enemies continuing as the Auld Alliance, but the Tudors resurrected old ecclesiastic arguments once pursued by Henry II of England and his son John of England. Yorkists were tied so much to the old order that Catholic rebellions (such as the Pilgrimage of Grace) and aspirations (exemplified by William Allen) were seen as continuing in their reactionary footsteps, when in opposition to the Tudors' reformation policies, although the Tudors were not uniformly Protestant according to Continental definition—instead were true to their Lancastrian Beaufort allegiance, in the appointment of Reginald Pole.
The essential difference between the Tudors and their predecessors, is the nationalization and integration of John Wycliffe's ideas to the Church of England, holding onto the alignment of Richard II of England and Anne of Bohemia, in which Anne's Hussite brethren were in alliance to her husband's Wycliffite countrymen against the Avignon Papacy. The Tudors otherwise rejected or suppressed other religious notions, whether for the Pope's award of "Fidei Defensor" or to prevent them from being in the hands of the common laity, who might be swayed by cells of foreign Protestants, with whom they had conversation as Marian exiles, pursuing a strategy of containment which the Lancastrians had done (after being vilified by Wat Tyler), even though the phenomenon of "Lollard knights" (like John Oldcastle) had become almost a national sensation all on its own.
In essence, the Tudors followed a composite of Lancastrian (the court party) and Yorkist (the church party) policies. Henry VIII tried to extend his father's balancing act between the dynasties for opportunistic interventionism in the Italian Wars, which had unfortunate consequences for his own marriages and the Papal States; the King furthermore tried to use similar tactics for the "via media" concept of Anglicanism. A further parallelism was effected by turning Ireland into a kingdom and sharing the same episcopal establishment as England, whilst enlarging England by the annexation of Wales. The progress to Northern/Roses government would thenceforth pass across the border into Scotland, in 1603, due not only to the civil warring, but also because the Tudors' own dynasty was fragile and insecure, trying to reconcile the mortal enemies who had weakened England to the point of having to bow to new pressures, rather than dictate diplomacy on English terms.
Tudor monarchs of England and Ireland[1].
The six Tudor monarchs were:
1. ^ To the Tudor period belongs the elevation of the English-ruled state in Ireland from a Lordship to a Kingdom (1541) under Henry VIII.
Armorial.
Before the succession.
Patrilineal descent.
Patrilineal descent, the descent from a male ancestor in which all intervening ancestors are also male, is the principle behind membership in royal houses, as it can be traced back through the paternal line. Note that as siblings, Edward, Mary and Elizabeth, share a generation number.
Tudor Badges.
The Welsh Dragon supporter honored the Tudor's Welsh origins. The most popular symbol of the house of Tudor was the Tudor rose (see top of page). When Henry Tudor took the crown of England from Richard III in battle, he brought about the end of the Wars of the Roses between the House of Lancaster (whose badge was a red rose) and the House of York (whose badge was a white rose). He married Elizabeth of York to bring all factions together.
On his marriage, Henry adopted the Tudor Rose badge conjoining the White Rose of York and the Red Rose of Lancaster. It symbolized the Tudor's right to rule as well the uniting of the kingdom after the Wars of the Roses. It was used by every British Monarch since Henry VII as a Royal Badge.
Lineage and the Tudor name.
Patrimonial Lineage.
As noted above Tewdur or Tudor is derived from the words tud "territory" and rhi "king". Owen Tudor took it as a surname on being knighted. It is doubtful whether the Tudor kings used the name on the throne. Kings and princes were not seen as needing a name, and a " 'Tudor' name for the royal family was hardly known in the sixteenth century. The royal surname was never used in official publications, and hardly in ‘histories’ of various sorts before 1584. ... Monarchs were not anxious to publicize their descent in the paternal line from a Welsh adventurer, stressing instead continuity with the historic English and French royal families. Their subjects did not think of them as ‘Tudors’, or of themselves as ‘Tudor people’". Princes and Princess would have been known as "of England". The medieval practice of colloquially calling princes after their place birth (e.g. Henry of Bolingbroke for Henry IV or Henry of Monmouth for Henry V) was not followed. Henry VII was likely known as "Henry of Richmond" before his taking of the throne.
Royal Lineage.
The Tudors claim to the throne was the strongest one at the end of the Wars of the Roses, as it combined the Lancastrian claim in their descent from the Beauforts and the Royal Yorkist claim by the marriage of Henry VII to the heiress of Edward IV.
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="31556" url="http://en.wikipedia.org/wiki?curid=31556" title="The Magnificent Seven">
The Magnificent Seven

The Magnificent Seven (1960) is an American western film directed by John Sturges and starring Yul Brynner, Eli Wallach and Steve McQueen. The picture is an Old West-style remake of Akira Kurosawa's Japanese-language film "Seven Samurai" (1954). The supporting cast features Charles Bronson, Robert Vaughn, James Coburn, Brad Dexter, and Horst Buchholz. They play a group of seven American gunfighters hired to protect a small agricultural village in Mexico from a group of marauding native bandits led by Calvera (Eli Wallach). The film's musical score was composed by Elmer Bernstein. In 2013, the film was selected for preservation in the United States National Film Registry by the Library of Congress as being "culturally, historically, or aesthetically significant".
A remake film is currently filming and is scheduled to be released on January 13, 2017.
Plot.
A Mexican village is periodically raided for food and supplies by Calvera (Eli Wallach) and his bandits. As he and his men ride away from their latest visit, Calvera promises to return to loot the village again. Taking what meager goods they have, the village leaders ride to a town just inside the American border hoping to barter for weapons to defend themselves. While there, they encounter Chris Adams (Yul Brynner), a veteran Cajun gunslinger. After listening to their tale, Chris suggests that the village hire gunfighters, which would be cheaper than guns and ammunition. The village men relentlessly try to convince him to be their gunman. At first he agrees only to help them find men, but later he decides to recruit six other men to help him defend the village, despite the poor pay offered.
The other men include hotheaded, inexperienced Chico (Horst Buchholz); Chris's friend Harry Luck (Brad Dexter), who believes Chris is seeking treasure; the drifter Vin Tanner (Steve McQueen), who has gone broke after a round of gambling and is loath to accept a position as a store clerk; Bernardo O'Reilly (Charles Bronson), a gunfighter of Irish-Mexican heritage who has fallen on hard times; a cowboy, Britt (James Coburn), who joins for the challenge involved; and an on-the-run gunman Lee (Robert Vaughn) in the midst of a crisis of confidence. The group recognizes they will be outnumbered, but they hope that Calvera will move on to an easier village when he sees professional resistance.
Arriving at the village, the seven gunmen begin to train the villagers to defend themselves. Each finds himself bonding with the villagers. When they realize that the small meal made for them by the women consists of all the food in the village, the gunmen share it with the villagers. Chico is fascinated by Petra (Rosenda Monteros), one of the village's young women. Bernardo bonds with three of the village's little boys. Lee struggles with nightmares and fears the loss of his skills. Calvera and his bandits soon arrive, sustain heavy losses, and are run out of town by the gunmen and the villagers working in concert. Chico, who is Mexican, follows Calvera back to his camp, pretending to be one of the bandits. He learns that Calvera must raid the village because he is desperate for food to feed his men.
Chico reports this to Chris. Some of the men believe they should leave, but Chris insists that they stay. They ride out to make a surprise raid on Calvera's camp, but find the camp empty. Returning to the village, they find that the fearful villagers have allowed Calvera to take control. Calvera spares the gunmen's lives, believing that they have learned that the simple farmers are not worth defending; he also fears reprisals from the U.S. Army if he should kill these Americans. The seven gunmen are escorted out of the village. They debate their next move, and all but Harry agree to return and free the village from Calvera. Harry believes the effort will lead to their deaths and rides off alone.
The six gunmen return and a gunfight begins. The villagers, recognizing the courage of the gunmen, join in the fight. Bernardo is killed protecting the children he had befriended, and Britt and Lee die after killing a considerable number of bandits. Harry, who had a change of heart, arrives in time to protect Chris but is fatally shot. The bandits are routed, and Chris shoots Calvera. Calvera, in his dying breath, asks him, "You came back... to a place like this? Why? A man like you? Why?"
The three remaining gunmen help to bury the dead. Chico decides to stay in the village with Petra, but Chris and Vin prepare to leave. The village elder bids them farewell and says that only the villagers have really won: "You're like the wind, blowing over the land and... passing on... ¡Vaya con Dios!" As they leave, they pass the graves of their fallen comrades. Chris says, "The Old Man was right. Only the farmers won. We lost. We'll always lose."
Cast.
Robert Vaughn is the last surviving primary cast member.
Pre-production.
Script credit was a subject of contention. Walter Bernstein, a blacklisted scriptwriter, was commissioned by Morheim to produce the first draft "faithfully" adapted from the original script written by Shinobu Hashimoto, Hideo Oguni and Akira Kurosawa; when Mirisch and Brynner took over the production, they brought on Walter Newman, whose version "is largely what's on screen." When Newman was unavailable to be on-site during the film's principal photography in Mexico, William Roberts was hired, in part to make changes required by Mexican censors. When Roberts asked the Writers Guild of America for a co-credit, Newman asked that his name be removed from the credits.
Production.
Filming began on March 1, 1960, on location in Mexico, where both the village and the U.S. border town were built for the film. The location filming was in Cuernavaca, Durango, and Tepoztlán and at the Churubusco Studios. The first scene shot was the first part of the six gunfighters' journey to the Mexican village, prior to Chico being brought into the group.
The film was shot in Panavision; an anamorphic format.
Score.
The film's score is by Elmer Bernstein. Along with the iconic main theme and effective support of the story line, the score also contains allusions to twentieth-century symphonic works, such as the reference to Bartok's Concerto for Orchestra, second movement, in the tense quiet scene just before the shoot out. The original soundtrack was not released at the time until reused and rerecorded by Bernstein for the soundtrack of "Return of the Seven". Electric guitar cover versions by Al Caiola in the U.S. and John Barry in the U.K. were successful on the popular charts. A vocal theme not written by Bernstein was used in a trailer.
In 1994, James Sedares conducted a re-recording of the score performed by The Phoenix Symphony Orchestra (which also included a suite from Bernstein's score for "The Hallelujah Trail", issued by Koch Records; Bernstein himself conducted the Royal Scottish National Orchestra for a performance released by RCA in 1997, but the original film soundtrack was not released until the following year by Rykodisc (Varèse Sarabande reissued this album in 2004).
Bernstein's score has frequently been quoted in the media and popular culture. Starting in 1963, the theme was used in commercials in the U.S. for Marlboro cigarettes. A similar-sounding (but different) tune was used for Victoria Bitter beer in Australia. The theme was included in the James Bond film "Moonraker".
Other uses include in the 2004 documentary film "Fahrenheit 9/11"; in the 2005 film "The Ringer"; as entrance music for the British band James, as well as episodes of "The Simpsons" that had a "Western" theme (mainly in the episode titled "Dude, Where's My Ranch?"). The opening horn riff in Arthur Conley's 1967 hit "Sweet Soul Music" is borrowed from the theme. Canadian band Kon Kan use the opening bars of the theme in their single "I Beg Your Pardon", Celtic Football Club (Glasgow, Scotland) used the theme music whenever Henrik Larsson scored a goal.
The Mick Jones 1980s band Big Audio Dynamite covered the song as "Keep off the Grass" (although this cover was not officially released). In 1995, the KLF also did a drum and bass cover of the main title as "The Magnificent"; it was released under the group alias One World Orchestra on the charity compilation "The Help Album".
In 1992, the main theme of "The Magnificent Seven" came into use on a section of the Euro Disneyland Railroad at Disneyland Paris. Portions of the theme play as the train exits the Grand Canyon diorama tunnel behind Phantom Manor, enters Frontierland, and travels along the bank of the Rivers of the Far West.
The "Main Title" was used as an intro tune on many nights of Bruce Springsteen's 2012 Wrecking Ball Tour. The theme was played as the E Street Band entered the stage, adding to the dramatic atmosphere in the stadium.
Reception.
Howard Thompson of "The New York Times" called the film a "pallid, pretentious and overlong reflection of the Japanese original"; according to Thompson, "don't expect anything like the ice-cold suspense, the superb juxtaposition of revealing human vignettes and especially the pile-driver tempo of the first "Seven"." According to "Variety" magazine's December 31, 1960 review, "Until the women and children arrive on the scene about two-thirds of the way through, "The Magnificent Seven" is a rip-roaring rootin' tootin' western with lots of bite and tang and old-fashioned abandon. The last third is downhill, a long and cluttered anti-climax in which "The Magnificent Seven" grow slightly too magnificent for comfort." Akira Kurosawa, however, was reportedly so impressed by the film that he presented John Sturges with a sword.
At the 33rd Academy Awards, the score was nominated for Best Score of a Dramatic or Comedy Picture, losing to Ernest Gold's score for "Exodus". "The Magnificent Seven" was listed at 8 on the American Film Institute's list of the top 25 American film scores.
The film has grown greatly in esteem since its release, largely due to its cast (several of whom would go on to become superstars over the decade following its release) and its music score. As of 2012, it has a freshness rating of 93 percent on Rotten Tomatoes. It is the second most shown film in U.S. television history, behind only "The Wizard of Oz". The film is also ranked 79 on the AFI's list of American cinema's 100 most-thrilling films.
Sequels, remakes and adaptations.
Movies.
The film was a box office disappointment in the United States, but proved to be such a smash hit in Europe that it ultimately went into profit. Three sequels were eventually made:
None of these were as successful as the original film.
The "Seven Samurai" plotline was revisited once more for 1980 sci-fi film "Battle Beyond the Stars". "Magnificent Seven" actor Robert Vaughn played one of seven mercenaries hired to save a farming planet from alien marauders.
"I sette magnifici gladiatori" (1983) aka"The Seven Magnificent Gladiators" was a Sword-and-sandal variation on the Seven Samurai/Magnificent Seven theme, starring Lou Ferrigno and Sybil Danning (who also appeared in "Battle Beyond the Stars").
The 1986 comedy "Three Amigos" directly parodies many aspects of "The Magnificent Seven", from the hiring of a team of Americans to defend a small Mexican village, to the training of the villagers by the mercenaries, to the megalomaniacal over-the-top character of the Mexican gang leader.
Yul Brynner appeared as an unnamed android gunslinger, costumed almost exactly the same as his "Magnificent Seven" character, in 1973 sc-fi hit "Westworld" and its 1976 sequel "Futureworld".
A remake of the film is currently in the works, with Antoine Fuqua set to direct and Denzel Washington, Chris Pratt, Ethan Hawke, Vincent D’Onofrio, Lee Byung-hun, Luke Grimes, Wagner Moura, Haley Bennett, Matt Bomer, and Peter Sarsgaard are all set to star.
Television.
The film also inspired a television series, "The Magnificent Seven", which ran from 1998 to 2000. Robert Vaughn was a recurring guest star, a judge who hires the seven to protect the town in which his widowed daughter-in-law and his grandson live.
The 1980s action-adventure series "The A-Team" was initially devised as a combination of "The Dirty Dozen" and "The Magnificent Seven". The show's pilot film plays much on the plot of "The Magnificent Seven", and there are similar plot echoes in various other episodes. James Coburn was originally approached to play John "Hannibal" Smith, the team's leader, a role that ultimately went to George Peppard in the series; and Robert Vaughn was added to the cast in the final season as part of a revamp attempt to boost fading ratings.
Books.
The plot of Stephen King's 2003 novel "Wolves of the Calla" is loosely based on "The Magnificent Seven". In the story, gunslinger Roland Deschain and his allies defend a small village from a raiding party that steals children once a generation. The village's name, "Calla Bryn Sturgis", is a nod to Sturges and Brynner, and the similarity in plot leads Roland's allies from 20th century New York to realize that they are taking part in a similar story. The novel also includes the misquoted epigraph "Mister, we deal in lead."
Robert B. Parker's 2001 novel "Potshot" borrows heavily from the film's end for the final confrontation between Spenser's group of seven outlaws and the Dell, the story's antagonists, going so far as to acknowledge it in dialog between characters and having one of them say the line, "We deal in lead, friend."
Comics.
Roy Thomas from Marvel Comics wrote a 4-issue story arc (issue numbers 7, 8, 9 & 10 from the original Marvel series) "Star Wars" story based on the "Seven Samurai"/"The Magnificent Seven" plot. In the comic, Han and Chewbacca recruit five other aliens from different backgrounds to help defend a city from a band of marauders called the Cloud Riders.

</doc>
<doc id="31574" url="http://en.wikipedia.org/wiki?curid=31574" title="The Weakest Link (UK game show)">
The Weakest Link (UK game show)

The Weakest Link is a British television quiz show, mainly broadcast on BBC Two, but also on BBC One. It was devised by Fintan Coyle and Cathy Dunning, and developed for television by the BBC Entertainment Department. The first ever episode went on air on 14 August 2000. It has since been replicated around the world. The UK version was hosted by Anne Robinson and narrated by Jon Briggs. In April 2011, Robinson announced that she would end her role as the show's hostess by the time her contract would expire as she had served longer than she originally intended to. The original run ended on Saturday 31 March 2012 with the 1,693rd episode. The BBC continues to air the show on BBC Two and internationally on BBC Entertainment.
Format.
The original format features a team of nine contestants who take turns answering general knowledge questions. The object of each round is to create a chain of nine consecutive correct answers and earn an increasing amount for a single communal pot within a certain time limit. An incorrect answer breaks the chain and loses any money earned in that chain. However, before their question is asked, a contestant can choose to bank the current amount of money earned in a chain to a safe pot, after which the chain starts afresh. A contestant's decision not to bank, in anticipation that they will be able to correctly answer the upcoming question allows the money to grow, as each successive correct answer earns proportionally more money.
When the allotted time for each round ends, any money not banked is lost, and if the host is in the middle of asking a question, or has asked a question but the contestant has yet to answer, the question is abandoned. Occasionally, the host gives the correct answer whether the contestant is able to answer the question correctly or not. The round automatically ends if the team successfully reaches the maximum amount for the round before the allotted time expires, and the next person says "Bank". Each round thereafter is reduced by 10 seconds as players are eliminated. The remaining two players have 90 seconds on the clock for the triple stakes round.
The first person to be asked a question in the first round is the player whose name is first alphabetically. Every subsequent round starts with the "strongest link"—the player with the most correct answers—from the previous round, unless that person has been voted off, in which case the second strongest answers first.
Money tree.
The money tree was as follows:
Voting and elimination.
At the end of each round, contestants must vote one player out of the game. Until the beginning of the next round, only the television audience knows exactly who the strongest and weakest links are statistically due to Briggs' narration. While the contestants work as a team when answering questions, they are at this point encouraged to be ruthless with one another. Players often decide to vote off weaker rivals, but occasionally opt to eliminate stronger players as well, in hope that it then improves their chances of winning the game. After the revealing of the votes, the host will interrogate the players on their choice of voting, the reasons behind their choice, as well as about their background and their interests. After interrogation, the player with the most votes is given a stern "You are the weakest link. Goodbye!" and must walk off the stage in what is called the "Walk of shame." In the event of a tie, the strongest link has the final decision about who is eliminated. If they voted for a tied player, they have the option of sticking with their vote or changing it. The strongest link usually sticks with their original choice, unless another player in the tie has voted for them. Occasionally, the strongest link has voted for someone who is not in the tie, and so is forced to change their decision one way or the other.
End of the game.
Final round.
The final two contestants work together in a final round identical to the previous ones; however, all money banked at the end of this round is tripled and added to the current money pool, forming the final total for the game. At the end of this round, there is no elimination, with the game instead moving to a head to head round.
Head to head.
For the head to head round, the remaining two players are each required to answer five questions each in a penalty shootout format. The strongest link from the previous round chooses who goes first. Whoever has the most correct answers at the end of the round wins the game. In the event of a tie, the game goes to "sudden death". Each player continues to be asked questions as usual, until one person answers a question correctly and the other incorrectly.
The winner of the game is declared "the strongest link" takes home all of the money accumulated in the prize pool for the game, and the loser leaves with nothing, like all previous eliminated players. In daytime episodes, the maximum possible winnings are £10,000; in primetime and special celebrity charity episodes, the maximum is £50,000.
Variant versions.
After the huge success of the show in its early evening slot on BBC Two, a prime-time edition was shown on BBC One, usually broadcast on Wednesday evenings.
Originally, "The Weakest Link: Champions League", which featured eight players who had won games on the daytime edition, battled off once again for £20,000 (with a money tree of £50-£100-£200-£500-£1,000-£1,500-£2,000-£2,500; with the seventh round being a double round for £5,000). The set was slightly altered, with electronic podiums being installed, as well as the adding of a studio audience. The Champions format was not successful, and instead new players competed for the money. A few months later, the contestants were cut down to seven, as well as the time from 45 minutes to 30, however, the prize money remained the same (with a money tree of £50-£100-£250-£1,000-£1,750-£2,500; the sixth round being a triple round for £7,500).
After the seven-player edition, the studio was revamped once again to add two more podiums, and the potential prize money was raised to £50,000. Non-celebrities played on the show at first, however, at present, the primetime version features celebrities playing for charity. Although Briggs and Robinson state that eight players will leave with nothing, normally the losing celebrities receive a "house" amount to give to their chosen charity, as well as their own fee for appearing on the show. In some celebrity editions, two celebrities have represented one position in the game, with the two conferring before giving their answer. There have also been several editions featuring entirely celebrity couples. A Christmas edition of the programme has also regularly featured in the schedules in recent years. Some contestants, such as Christopher Biggins, Peter Duncan and Basil Brush, have appeared several times. A puppet edition also aired, which included a Robinson puppet introducing the show before twelve famous puppets played for charity.
The daytime version has also seen its share of variance, as was the case in two particular episodes. An April Fools' Day show which aired in 2003 featured Robinson being strangely nice to the contestants, and abandoning her traditional black wardrobe in favour of a metallic pink overcoat. However, she did not remain kind to the contestants for the entire episode, resuming her old behaviour after declaring the winner and contestants as "so stupid".
Another variant of the daytime show was the 1,000th episode, complete with an audience, a departure from the normally spectator-free background. Fan-favourites played again for £10,000, and some previous contestants also sat in the audience. The show's first winner, David Bloomfield was one of the returning contestants, and was asked the question: "If there have been a thousand episodes of "The Weakest Link", each with nine players, how many contestants in total have appeared on the show?" He answered the question correctly (9,000) but banked prior to it being asked. He did not win any money on the 1,000th episode, and was voted off in only the third round, despite having been the statistical strongest link in the first two rounds. In the end, Miss Evans (who had previously appeared on the Strong Women special but had lost out to curate Emma Langley) defeated Basil Brush, winning £2,710, which she split with her co-finalist to give to charity. Robinson then announced that a bonus of £1,000 would be added to the final total, as it was the 1,000th episode, resulting in a final total of £3,710, or both contestants receiving £1,855 each. It also marked the first time that Anne Robinson did not say the phrase "...you leave with nothing." to the losing contestant.
Two fictional television shows, "Doctor Who" and "My Family", have depicted their own versions of "Weakest Link" in their episodes. The "Doctor Who" edition, broadcast in 2005, showed a futuristic version of the show in the year 200,100, with only six contestants, and presented by an 'Anne Droid' (voiced by Anne Robinson) who disintegrates the contestants being voted off (it is later revealed that she actually shoots a transmat beam that transports the contestants to a Dalek ship for extermination or be converted into Daleks). The "My Family" version was broadcast in 2007, and was essentially portraying all of the main characters on an ordinary episode of the show, except for the fact that it was a 'family special'. Comedy series "That Mitchell and Webb Look" broadcast a sketch based on "Weakest Link" called "Hole in the Ring", featuring Robert Webb as an overly harsh presenter who makes mistakes whilst reading questions.
A later special edition of "Weakest Link" featured nine cast members of "Doctor Who" playing the game, and the show was introduced by the Anne Droid. The real Anne walked on stage almost instantly as the droid began the show, unplugged it, and said, "I don't think so! We'll do that again!" She then began the show herself and proceeded as normal.
The final edition.
The final episode was titled "You Are "The Weakest Link" - Goodbye" and was aired on BBC One on 31 March 2012. Filming for the final edition took place on 11 December 2011. It was the 1,693rd edition of "The Weakest Link" in the United Kingdom. The ending of the show was the only special part to the final edition.
A normal daytime edition of the show was made, with some of Anne's favourite contestants from over the years taking part, and with no audience present during filming or changes to the money tree (see above). The first round of questions was notably different and was mainly about "The Weakest Link" and the host, Anne Robinson. The last question asked was "If the Roman numeral 'X' is halved, the result can be represented by which other Roman numeral?", the answer being "V". The last ever UK winner was Archie Bland, the editor of "The Independent" newspaper's Saturday edition.
A short montage of clips from the show was shown at the end of the game. After saying goodbye, all of the lights turned off with Anne being the only person left in the studio. The programme was eventually replaced by the Alexander Armstrong-fronted "Pointless" as the big BBC teatime quiz (it had aired on BBC One for some years previously).
Success.
Much of the show's success has been attributed to its host, Anne Robinson. She was already famous in the UK for her sarcasm while presenting the consumer programme "Watchdog", and "The Weakest Link" saw her develop this further, particularly in her taunting of contestants. Her sardonic summary to the team, usually berating them for their lack of intelligence for not achieving the target became a trademark of the show, and her call of "You are the weakest link—goodbye!" became a popular catchphrase.
The presence of elements inspired by "Big Brother" and "Who Wants to Be a Millionaire?" differentiated the programme from most previous quiz shows, as it invites open conflict between players, and uses a host who is openly hostile to the competitors, rather than a positive figure.
In autumn 2001, for the first time ever, "The Weakest Link" was placed directly head-to-head with "Millionaire" in the television schedules. Between the two, "Millionaire" ultimately emerged on top, attracting 10.2m viewers compared to "The Weakest Link"'s 3.8m.
From 9 to 13 August 2010, five "10th Anniversary Specials" aired at the usual time on BBC One.
International versions.
The format has been licensed across the world, with many countries producing their own series of the programme. As with the original British version, all of the hosts wear dark, usually black clothing. Most versions also have disciplinarian female hosts, similar in attitude and appearance to Robinson in the British version.

</doc>
<doc id="31592" url="http://en.wikipedia.org/wiki?curid=31592" title="TVP">
TVP

TVP may stand for:

</doc>
<doc id="31595" url="http://en.wikipedia.org/wiki?curid=31595" title="TNT (disambiguation)">
TNT (disambiguation)

TNT is trinitrotoluene, an explosive chemical compound.
TNT may also refer to:

</doc>
<doc id="31605" url="http://en.wikipedia.org/wiki?curid=31605" title="Two-party system">
Two-party system

A two-party system is a system where two major political parties dominate politics within a government. One of the two parties typically holds a majority in the legislature and is usually referred to as the "majority party" while the other is the "minority party". The term has different senses. For example, in the United States, Jamaica, and Malta, the sense of "two party system" describes an arrangement in which all or nearly all elected officials only belong to one of the two major parties, and third parties rarely win any seats in the legislature. In such arrangements, two-party systems are thought to result from various factors like winner takes all election rules. In such systems, while chances for third party candidates winning election to major national office are remote, it is possible for groups within the larger parties, or in opposition to one or both of them, to exert influence on the two major parties. In contrast, in the United Kingdom and in other parliamentary systems and elsewhere, the term "two-party system" is sometimes used to indicate an arrangement in which two major parties dominate elections but in which there are viable third parties which do win seats in the legislature, and in which the two major parties exert proportionately greater influence than their percentage of votes would suggest.
Examples.
There is strong agreement that the United States has a two-party system; historically, there have been few instances in which third party candidates won an election. In the First Party System, only the Federalist Party and Thomas Jefferson's Republican Party were significant political parties. Toward the end of the First Party System, the Republicans held a single-party system (primarily under the Presidency of James Monroe). Under the Second Party System, the Republican Party split during the election of 1824 into Adams' Men and Jackson's Men. In 1828, the modern Democratic Party formed in support of Andrew Jackson. The National Republicans were formed in support of John Quincy Adams. After the National Republicans collapsed, the Whig Party and the Free Soil Party quickly formed and collapsed. In 1854, the modern Republican Party formed from a loose coalition of former Whigs, Free Soilers and other anti-slavery activists.
During the Third Party System, the Republican Party was the dominant political faction, but the Democrats held a strong, loyal coalition in the Solid South. During the Fourth Party System, the Republicans remained the dominant Presidential party, although Democrats Grover Cleveland and Woodrow Wilson were both elected to two terms. In 1932, at the onset of the Fifth Party System, Democrats took firm control of national politics with the landslide victories of Franklin D. Roosevelt in four consecutive elections. Other than the two terms of Republican Dwight Eisenhower from 1953 to 1961, Democrats retained firm control of the Presidency until the mid-1960s. Since the mid-1960s, despite a number of land slides (such as Ronald Reagan carrying 49 states and 58% of the popular vote over Walter Mondale in 1984), Presidential elections have been competitive between the predominant Republican and Democratic parties and no one party has been able to hold the Presidency for more than three consecutive terms. In the election of 2012, only 4% separated the popular vote between Barack Obama (51%) and Mitt Romney (47%), although Obama won the electoral vote by a landslide (332-206).
Throughout every American party system, no third party has won a Presidential election or majorities in either house of Congress. Despite that, third parties and third party candidates have gained traction and support. In the election of 1912, Theodore Roosevelt won 27% of the popular vote and 88 electoral votes running as a Progressive. In the 1992 Presidential election, Ross Perot won 19% of the popular vote but no electoral votes running as an Independent.
In countries such as Britain and Spain, two major parties emerge which have strong influence and tend to elect most of the candidates, but a multitude of lesser parties exist with varying degrees of influence, and sometimes these lesser parties are able to elect officials who participate in the legislature. A report in the "Christian Science Monitor", for example, suggested that Spain was moving towards a "greater two-party system" while acknowledging that Spain has "many small parties." In political systems based on the Westminster system, which is a particular style of parliamentary democracy based on the British model and found in many commonwealth countries, a majority party will form the government and the minority party will form the opposition, and coalitions of lesser parties are possible; in the rare circumstance in which neither party is the majority, a hung parliament arises. Sometimes these systems are described as "two-party systems" but they are usually referred to as "multi-party" systems. There is not always a sharp boundary between a two-party system and a multi-party system.
Generally, a two-party system becomes a dichotomous division of the political spectrum with an ostensibly right-wing and left-wing party: the Nationalist Party vs. the Labour Party in Malta, Liberal vs. Labor in Australia, Republicans vs. Democrats in the United States and the Conservative Party vs. the Labour Party in the United Kingdom.
Other parties in these countries may have seen candidates elected to local or subnational office, however. Historian John Hicks claims that the United States has never possessed for any considerable period of time the two party system in its pure and undefiled form.
In some governments, certain chambers may resemble a two-party system and others a multi-party system. For example, the politics of Australia are largely two-party (if the Liberal Party and National Party are considered the same party at a national level due to their long-standing alliance) for the Australian House of Representatives, which is elected by Instant Runoff Voting, known within Australia as preferential voting. However, third parties are more common in the Australian Senate, which uses a proportional voting system more amenable to minor parties.
India too is showing characteristics of two party system with United Progressive Alliance (UPA) and National Democratic Alliance (NDA) as the two main players. It is to be noted that both UPA and NDA are not two political parties but alliances of several smaller parties. Other smaller parties not aligned with either NDA or UPA exist, and overall command about 20% of the 2009 seats in the Lok Sabha.
Malta are somewhat unusual in that while the electoral system is single transferable vote (STV), traditionally associated with proportional representation, minor parties have not earned much success. No third parties won any seats in the Parliament in Malta's most recent election, for example. The Labour Party and the Nationalist Party are the dominant parties.
South Korea has a multi-party system that has sometimes been described as having characteristics of a two-party system.
Historically, Brazil had a two-party system during its military dictatorship (1964–1985).
Comparisons with other party systems.
Two-party systems can be compared with:
Causes.
There are several reasons why, in some systems, two major parties dominate the political landscape. There has been speculation that a two-party system arose in the United States from early political battling between the federalists and anti-federalists in the first few decades after the ratification of the Constitution, according to several views. In addition, there has been more speculation that the winner-takes-all electoral system as well as particular state and federal laws regarding voting procedures helped to cause a two-party system. 
Political scientists such as Maurice Duverger and William H. Riker claim that there are strong correlations between voting rules and type of party system. Jeffrey D. Sachs agreed that there was a link between voting arrangements and the effective number of parties. Sachs explained how the First Past The Post voting arrangement tended to promote a two-party system:
The main reason for America's majoritarian character is the electoral system for Congress. Members of Congress are elected in single-member districts according to the "first-past-the-post" (FPTP) principle, meaning that the candidate with the plurality of votes is the winner of the congressional seat. The losing party or parties win no representation at all. The first-past-the-post election tends to produce a small number of major parties, perhaps just two, a principle known in political science as Duverger's Law. Smaller parties are trampled in first-past-the-post elections.—Sachs, "The Price of Civilization", 2011
Consider a system in which voters can vote for any candidate from any one of many parties. Suppose further that if a party gets 15% of votes, then that party will win 15% of the seats in the legislature. This is termed "proportional representation" or more accurately as "party-proportional representation". Political scientists speculate that proportional representation leads logically to multi-party systems, since it allows new parties to build a niche in the legislature:
Because even a minor party may still obtain at least a few seats in the legislature, smaller parties have a greater incentive to organize under such electoral systems than they do in the United States.—Schmidt, Shelley, Bardes (2008), 
In contrast, a voting system that allows only a single winner for each possible legislative seat is sometimes termed a plurality voting system or single-winner voting system and is usually described under the heading of a "winner–takes–all" arrangement. Each voter can cast a single vote for any candidate within any given legislative district, but the candidate with the most votes wins the seat, although variants, such as requiring a majority, are sometimes used. What happens is that in a general election, a party that consistently comes in third in every district is unlikely to win any legislative seats even if there is a significant proportion of the electorate favoring its positions. This arrangement strongly favors large and well–organized political parties that are able to appeal to voters in many districts and hence win many seats, and discourages smaller or regional parties. Politically oriented people consider their only realistic way to capture political power is to be either a Republican or Democrat.
In the U.S., forty-eight states have a standard "winner-takes-all" electoral system for amassing presidential votes in the Electoral College system. The "winner–takes–all" principle applies in presidential elections, since if a presidential candidate gets the most votes in any particular state, "all" of the electoral votes from that state are awarded. In all but two states, Maine and Nebraska, the presidential candidate winning a plurality of votes wins all of the electoral votes, a practice called the unit rule.
Duverger concluded that "plurality election single-ballot procedures are likely to produce two-party systems, whereas proportional representation and runoff designs encourage multipartyism." He suggested there were two reasons why "winner–takes–all" systems leads to a two-party system. First, the weaker parties are pressured to form an alliance, sometimes called a "fusion", to try to become big enough to challenge a large dominant party and, in so doing, gain political clout in the legislature. Second, voters learn, over time, not to vote for candidates outside of one of the two large parties since their votes for third party candidates are usually ineffectual. As a result, weaker parties are eliminated by voters over time. Duverger pointed to statistics and tactics to suggest that voters tended to gravitate towards one of the two main parties, a phenomenon which he called "polarization", and tend to shun third parties. For example, some analysts suggest that the Electoral College system in the United States, by favoring a system of winner–takes–all in presidential elections, is a structural choice favoring only two major parties.
Gary Cox suggested that America's two-party system was highly related with economic prosperity in the country:
The bounty of the American economy, the fluidity of American society, the remarkable unity of the American people, and, most important, the success of the American experiment have all mitigated against the emergence of large dissenting groups that would seek satisfaction of their special needs through the formation of political parties.—Cox, according to George Edwards
An effort in 2012 by centrist groups to promote ballot access by Third Party candidates called Americans Elect spent $15 million to get ballot access but failed to elect any candidates. The lack of choice in a two-party model in politics has often been compared to the variety of choices in the marketplace.
Politics has lagged our social and business evolution ... There are 30 brands of Pringles in our local grocery store. How is it that Americans have so much selection for potato chips and only two brands — and not very good ones — for political parties?—Scott Ehredt of the Centrist Alliance
Third parties.
Third parties, meaning a party other than one of the two dominant parties, are possible in two-party systems, but they are often unlikely to exert much influence by gaining control of legislatures or by winning elections. While there are occasional opinions in the media expressed about the possibility of third parties emerging in the United States, for example, political insiders such as the 1980 presidential candidate John Anderson think the chances of one appearing in the early twenty-first century is remote. A report in "The Guardian" suggested that American politics has been "stuck in a two-way fight between Republicans and Democrats" since the Civil War, and that third-party runs had little meaningful success.
Third parties in a two-party system can be:
When third parties are built around an ideology which is at odds with the majority mindset, many members belong to such a party not for the purpose of expecting electoral success but rather for personal or psychological reasons. In the U.S., third parties include older ones such as the Libertarian Party and the Green Party and newer ones such as the Pirate Party. Many believe that third parties don't affect American politics by winning elections, but they can act as "spoilers" by taking votes from one of the two major parties. They act like barometers of change in the political mood since they push the major parties to consider their demands. An analysis in "New York Magazine" by Ryan Lizza in 2006 suggested that third parties arose from time to time in the nineteenth century around single-issue movements such as abolition, women's suffrage, and the direct election of senators, but were less prominent in the twentieth century.
A so-called "third party" in the United Kingdom are the Liberal Democrats. In the 2010 election, the Liberal Democrats received 23% of the votes but only 9% of the seats in the House of Commons. While electoral results do not necessarily translate into legislative seats, the Liberal Democrats can exert influence if there is a situation such as a hung parliament. In this instance, neither of the two main parties (at present, the Conservative Party and the Labour Party) have sufficient authority to run the government. Accordingly, the Liberal Democrats can in theory exert tremendous influence in such a situation since they can ally with one of the two main parties to form a coalition. This happened in the Coalition government of 2010. Yet in that more than 13% of the seats in the British House of Commons are held in 2011 by representatives of political parties other than the two leading political parties of that nation, contemporary Britain is considered by some to be a multi-party system, and not a two-party system. The two party system in the United Kingdom allows for other parties to exist, although the main two parties tend to dominate politics; in this arrangement, other parties are not excluded and can win seats in Parliament. In contrast, the two party system in the United States has been described as a duopoly or an enforced two-party system, such that politics is almost entirely dominated by either the Republicans or Democrats, and third parties rarely win seats in Congress.
Advantages.
Some historians have suggested that two-party systems promote centrism and encourages political parties to find common positions which appeal to wide swaths of the electorate. It can lead to political stability which leads, in turn, to economic growth. Historian Patrick Allitt of the Teaching Company suggested that it is difficult to overestimate the long term economic benefits of political stability. Sometimes two-party systems have been seen as preferable to multi-party systems because they are simpler to govern, with less fractiousness and greater harmony, since it discourages radical minor parties, while multi-party systems can sometimes lead to hung parliaments. Italy, with a multi-party system, has had years of divisive politics since 2000, although analyst Silvia Aloisi suggested in 2008 that the nation may be moving closer to a two-party arrangement. The two-party has been identified as simpler since there are fewer voting choices. One analyst suggested the two-party system, in contrast with proportional representation, prevented excessive government interference with economic policy.
Disadvantages.
Two-party systems have been criticized for downplaying alternative views, being less competitive, encouraging voter apathy since there is a perception of fewer choices, and putting a damper on debate within a nation. In a proportional representation system, lesser parties can moderate policy since they are not usually eliminated from government. One analyst suggested the two-party approach may not promote inter-party compromise but may encourage partisanship. In "The Tyranny of the Two–party system", Lisa Jane Disch criticizes two-party systems for failing to provide enough options since only two choices are permitted on the ballot. She wrote:
Herein lies the central tension of the two–party doctrine. It identifies popular sovereignty with choice, and then limits choice to one party or the other. If there is any truth to Schattschneider's analogy between elections and markets, America's faith in the two–party system begs the following question: Why do voters accept as the ultimate in political freedom a binary option they would surely protest as consumers? ... This is the tyranny of the two–party system, the construct that persuades United States citizens to accept two–party contests as a "condition" of electoral democracy.—Lisa Jane Disch, 2002
There have been arguments that the winner-take-all mechanism discourages independent or third-party candidates from running for office or promulgating their views. Ross Perot's former campaign manager wrote that the problem with having only two parties is that the nation loses "the ability for things to bubble up from the body politic and give voice to things that aren’t being voiced by the major parties." One analyst suggested that parliamentary systems, which typically are multi-party in nature, lead to a better "centralization of policy expertise" in government. Multi-party governments permit wider and more diverse viewpoints in government, and encourage dominant parties to make deals with weaker parties to form winning coalitions. While there is considerable debate about the relative merits of a constitutional arrangement such as that of the United States versus a parliamentary arrangement such as Britain, analysts have noted that most democracies around the world have chosen the British multi-party model. Analyst Chris Weigant of the "Huffington Post" wrote that "the parliamentary system is inherently much more open to minority parties getting much better representation than third parties do in the American system." After an election in which the party changes, there can be a “polar shift in policy-making” when voters react to changes.
History.
Beginnings of parties in Britain.
The two-party system, in the sense of the looser definition, where two parties dominate politics but in which third parties can elect members and gain some representation in the legislature, can be traced to the development of political parties in the United Kingdom. There was a division in English politics at the time of the Civil War and Glorious Revolution in the late 17th century. The Whigs supported Protestant constitutional monarchy against absolute rule and the Tories, originating in the Royalist (or "Cavalier") faction of the English Civil War, were conservative royalist supporters of a strong monarchy as a counterbalance to the republican tendencies of Parliament. In the following century, the Whig party's support base widened to include emerging industrial interests and wealthy merchants.
The basic matters of principle that defined the struggle between the two factions, were concerning the nature of constitutional monarchy, the desirability of a Catholic king, the extension of religious toleration to nonconformist Protestants, and other issues that had been put on the liberal agenda through the political concepts propounded by John Locke, Algernon Sidney and others.
Vigorous struggle between the two factions characterised the period from the Glorious Revolution to the 1715 Hanoverian succession, over the legacy of the overthrow of the Stuart dynasty and the nature of the new constitutional state. This proto two-party system fell into relative abeyance after the accession to the throne of George I and the consequent period of Whig supremacy under Robert Walpole, during which the Tories were systematically purged from high positions in government. However, although the Tories were dismissed from office for half a century, they still retained a measure of party cohesion under William Wyndham and acted as a united, though unavailing, opposition to Whig corruption and scandals. At times they cooperated with the "Opposition Whigs", Whigs who were in opposition to the Whig government; however, the ideological gap between the Tories and the Opposition Whigs prevented them from coalescing as a single party.
Emergence of the two-party system in Britain.
The old Whig leadership dissolved in the 1760s into a decade of factional chaos with distinct "Grenvillite", "Bedfordite", "Rockinghamite", and "Chathamite" factions successively in power, and all referring to themselves as "Whigs". Out of this chaos, the first distinctive parties emerged. The first such party was the Rockingham Whigs under the leadership of Charles Watson-Wentworth and the intellectual guidance of the political philosopher Edmund Burke. Burke laid out a philosophy that described the basic framework of the political party as "a body of men united for promoting by their joint endeavours the national interest, upon some particular principle in which they are all agreed". As opposed to the instability of the earlier factions, which were often tied to a particular leader and could disintegrate if removed from power, the two party system was centred on a set of core principles held by both sides and that allowed the party out of power to remain as the Loyal Opposition to the governing party.
A genuine two-party system began to emerge, with the accession to power of William Pitt the Younger in 1783 leading the new Tories, against a reconstituted "Whig" party led by the radical politician Charles James Fox.
The two party system matured in the early 19th century era of political reform, when the franchise was widened and politics entered into the basic divide between conservatism and liberalism that has fundamentally endured up to the present. The modern Conservative Party was created out of the 'Pittite' Tories by Robert Peel, who issued the Tamworth Manifesto in 1834 which set out the basic principles of Conservatism; - the necessity in specific cases of reform in order to survive, but an opposition to unnecessary change, that could lead to "a perpetual vortex of agitation". Meanwhile, the Whigs, along with free trade Tory followers of Robert Peel, and independent Radicals, formed the Liberal Party under Lord Palmerston in 1859, and transformed into a party of the growing urban middle-class, under the long leadership of William Ewart Gladstone. The two party system had come of age at the time of Gladstone and his Conservative rival Benjamin Disraeli after the 1867 Reform Act.
History of American political parties.
Although the Founding Fathers of the United States did not originally intend for American politics to be partisan, early political controversies in the 1790s saw the emergence of a proto two-party political system, the Federalist Party and the Democratic-Republican Party, centred on the differing views on federal government powers of Secretary of the Treasury Alexander Hamilton and James Madison. However, a consensus reached on these issues ended party politics in 1816 for a decade, a period commonly known as the Era of Good Feelings. 
Partisan politics revived in 1829 with the split of the Democratic-Republican Party into the Jacksonian Democrats led by Andrew Jackson, and the Whig Party, led by Henry Clay. The former evolved into the modern Democratic Party and the latter was replaced with the Republican Party as one of the two main parties in the 1850s.

</doc>
<doc id="31621" url="http://en.wikipedia.org/wiki?curid=31621" title="Transient ischemic attack">
Transient ischemic attack

A transient ischemic attack (TIA) is a transient episode of neurologic dysfunction caused by ischemia (loss of blood flow) – either focal brain, spinal cord, or retinal – without acute infarction (tissue death). TIAs have the same underlying cause as strokes: a disruption of cerebral blood flow (CBF), and are often referred to as mini-strokes. This is different from the definition of TIA used before 2009, which was based on the temporal, arbitrary time point of < 24 hours of associated neurological symptoms.<ref name="2009 New def of TIA by AHA/ASAS"></ref> TIAs cause the same symptoms associated with stroke, such as contralateral paralysis (opposite side of body from affected brain hemisphere) or sudden weakness or numbness. A TIA may cause sudden dimming or loss of vision (amaurosis fugax), aphasia, slurred speech (dysarthria) and mental confusion. But unlike a stroke, the symptoms of a TIA can resolve within a few minutes or 24 hours. Brain injury may still occur in a TIA lasting only a few minutes. Having a TIA is a risk factor for eventually having a stroke or a silent stroke.
A silent stroke or silent cerebral infarct (SCI) differs from a TIA in that there are no immediately observable symptoms. An SCI may still cause long lasting neurological dysfunction affecting such areas as mood, personality and cognition. An SCI often occurs before or after a TIA or major stroke.
A cerebral infarct that lasts longer than 24 hours but fewer than 72 hours is called a reversible ischemic neurologic deficit or RIND.
Signs and symptoms.
Symptoms can vary widely across people, and across brain regions. The most frequent symptoms include temporary loss of vision (typically "amaurosis fugax"); difficulty speaking (aphasia); weakness on one side of the body (hemiparesis); and numbness or tingling (paresthesia), usually on one side of the body. Impairment of consciousness is very uncommon. There have been cases of temporary and partial paralysis affecting the face and tongue of the afflicted. The symptoms of a TIA are short-lived and usually last a few seconds to a few minutes and most symptoms disappear within 60 minutes. Some individuals may have a lingering feeling that something odd happened to the body. Dizziness, lack of coordination or poor balance are also symptoms related to TIA. Symptoms vary in severity.
Causes.
The most common cause of a TIA is an embolus that occludes an artery in the brain. This usually arises from a dislodged atherosclerotic plaque in one of the carotid arteries ("i.e." two of the four major arteries supplying the brain) or from the vertebral-basilar arteries. Another common cause is from a thrombus ("i.e." a blood clot) that has originated from the (usually left) atrium of the heart due to atrial fibrillation. In a TIA, the blockage period is very short-lived and hence there is no permanent damage. The cholesterol build-up is gradual and eventually narrows the lumen. With time, blood flow to that side of the brain is reduced and a stroke may occur as a result. In other cases, cholesterol particles from the atherosclerotic plaque may suddenly break off and enter the brain. In some people, these fragments come off from the heart and go to the brain. This often happens during a heart attack or an infection of the valves.
Other reasons include excessive narrowing of large vessels resulting from an atherosclerotic plaque and increased blood viscosity caused by some blood diseases. TIA is related to other medical conditions such as hypertension, heart disease (especially atrial fibrillation), migraine, hypercholesterolemia, and diabetes mellitus.
Diagnosis.
TIA will usually be diagnosed after a doctor performs a history and a physical exam. There are several radiological tests that are done to evaluate patients who have had a TIA. These include a CT scan or an MRI of the brain, ultrasound of the neck, or an echocardiogram of the heart. In most cases, the source of atherosclerosis is usually identified with an ultrasound.
Differential diagnoses.
Other diagnoses may have symptoms similar to those of a TIA:
Prevention.
A TIA may be prevented by changes in lifestyle; although most of these recommendations have no solid empirical data, most medical professionals believe them to be so. These include:
Treatment and management.
The mainstay of the treatment following acute recovery from a TIA depends on the underlying cause. It is not always immediately possible to tell the difference between a CVA (stroke) and a TIA. Most patients who are diagnosed at a hospital's emergency department as having suffered from a TIA will be admitted to the hospital if it is their first TIA, for telemetry, blood pressure monitoring and other specific tests. If it is a second or subsequent TIA, most patients will be discharged home and advised to contact their primary physician to organize further investigations. A TIA can be considered as the last warning. The reason for the condition should be immediately examined by imaging of the brain. Occasionally, none of these tests will determine the cause, but the symptoms show that the stroke did occur. Actually TIA is a form of stroke in nature.
To reduce risk of recurrence, patients are recommended to undergo lifestyle changes including quitting smoking, losing weight, eating more fruits and vegetables and exercising regularly.
Surgery.
If the TIA affects an area that is supplied by the carotid arteries, an ultrasound (TCD) scan may demonstrate carotid stenosis. For people with a greater than 70% stenosis within the carotid artery, removal of atherosclerotic plaque by surgery, specifically a carotid endarterectomy, may be recommended. The blood vessel is opened up and the plaque is removed. The carotid may be replaced with a vessel retrieved from the lower leg or foot. The procedure is not technically difficult but carries the potential complication of inducing a stroke. A stroke can occur during surgery or after the procedure. The chance of a stroke ranges from 1–4 percent.
Some patients may also be given modified-release dipyridamole or clopidogrel.
Medication.
The use of anti-coagulant medications, heparin and warfarin, or anti-platelet medications such as aspirin may be warranted. Antiplatelet drugs prevent platelets from agglutinating or sticking to each other, hence the term "blood thinner." The initial treatment is aspirin, second line is clopidogrel (PLAVIX), third line is ticlopidine. If TIA is recurrent after aspirin treatment, the combination of aspirin and dipyridamole is needed (Aggrenox).
Thinning the blood helps to ensure that small particles do not form and travel to the brain. These drugs require frequent monitoring. These drugs also have side effects such as easy bruising and bleeding from mild trauma.
An electrocardiogram (ECG) may show atrial fibrillation, a common cause of TIAs, or other arrhythmias that may cause embolisation to the brain. An echocardiogram is useful in detecting thrombus within the heart chambers. Such patients benefit from anticoagulation.
Prognosis.
People diagnosed with TIA are sometimes said to have had a warning for an approaching stroke. If the time period of blood supply impairment lasts more than a few minutes, the nerve cells of that area of the brain die and cause permanent neurologic damage. One third of the people with TIA later have recurrent TIAs and one third have a stroke because of permanent nerve cell loss. Other sources cite that 10% of TIAs will develop stroke within 90 days, half of which will occur in the first two days following the TIA.
The risk of a stroke occurring after a TIA can be predicted using the ABCD² score.

</doc>
<doc id="31629" url="http://en.wikipedia.org/wiki?curid=31629" title="Threads">
Threads

 
Threads is a 1984 BAFTA award-winning British television drama, produced jointly by the BBC, Nine Network and Western-World Television Inc. Written by Barry Hines and directed by Mick Jackson, it is a docudrama account of nuclear war and its effects on the city of Sheffield in Northern England.
The primary plot centres on two families, the working-class Kemps and the middle-class Becketts, as a confrontation between the United States and the Soviet Union erupts and escalates. As the United Kingdom prepares for war, the members of each family deal with their own personal crises. Meanwhile, a secondary plot centred upon the Chief Executive of Sheffield City Council serves to illustrate the British government's then-current continuity of government arrangements. As nuclear exchanges between NATO and the Warsaw Pact begin, the harrowing details of the characters' struggle to survive the attacks and their aftermath is dramatically depicted. The balance of the story details the fate of each family as the characters face the medical, economic, social and environmental consequences of nuclear war. 
Shot on a budget of £250,000–350,000, the film was notable in being the first of its kind to depict a nuclear winter. Certain reviewers have nominated "Threads" as the "film which comes closest to representing the full horror of nuclear war and its aftermath, as well as the catastrophic impact that the event would have on human culture". It has been compared to the earlier programme "The War Game" produced in Britain in the 1960s and its contemporary "The Day After", a 1983 ABC television film depicting a similar scenario in the United States.
Storyline.
Background on the war.
The chronology of the events leading up to the war is depicted entirely via television and radio news broadcasts. An allegedly US-backed coup d'état in Iran prompts the Soviet Union to occupy the northern part of the country, ostensibly to prevent the return of a pro-Shah regime. On 8 May, the USA hints at deploying troops to Iran, to prevent the Soviets from reaching the oil fields in the south. On 11 May, the US Navy in the Indian Ocean is put on high alert when rumours begin abounding of the disappearance of the USS "Los Angeles" in the Persian Gulf. The next day, a collision in the Gulf of Oman between the Soviet battlecruiser "Kirov" and the USS "Callaghan" leaves the former badly damaged. Subsequent discoveries by American and Israeli search and rescue vessels reveal debris and an oil slick from the missing "Los Angeles", prompting the US President to warn the Soviets over the possibility of an "armed confrontation – with incalculable consequences for all mankind."
On the 17th, the US sends its rapid deployment force to take defensive positions around Isfahan in western Iran, hoping to deter the Soviets from making further advances to the south, with a supporting role being taken by squadrons of B-52 bombers and AWACS early warning aircraft landing at US airbases in Turkey. The Soviets respond by transporting nuclear warheads into their newly established base in Mashhad. On 20 May, the USA proposes a joint withdrawal from Iran to take effect by noon on the 22nd, while Britain sends troops to Europe amidst a build-up of Warsaw Pact troops in East Germany. The Soviets ignore the US ultimatum and, an hour after the expiry, are attacked at their base in Mashhad by B-52 bombers using conventional weapons. The Soviets defend the base with a nuclear warhead, delivered by a surface-to-air missile, destroying many B-52s. The battle ceases after US forces destroy the base with a battlefield nuclear weapon.
The next day, fighting breaks out between the US and Soviet navies. On 24 May, amidst rioting in East Germany, the Soviets cut the road links into and out of West Berlin, whilst offering occupying NATO forces free passage to the west. The USS "Kitty Hawk" is sunk in the Persian Gulf, and the USA blockades Cuba. Severe damage to Russian consulates ensues during anti-Soviet riots in major US cities. The next day, the BBC reports on the Mashhad nuclear exchange, stating that the weapons used were within the range of 50–100 kilotons, and that cities in western Pakistan are being evacuated due to the fallout.
On 26 May, at 8:35 a.m. GMT, the Soviets detonate a warhead high above the North Sea, producing an electromagnetic pulse which damages communications throughout the UK and Northwestern Europe. Two minutes later, the first missile salvoes begin hitting NATO targets. Overall, the resulting East-West exchanges amount to 3000 megatons, with 210 falling on the UK.
Plot.
Young Sheffield residents Ruth Beckett (Karen Meagher) and Jimmy Kemp (Reece Dinsdale) decide to marry due to an unplanned pregnancy. Meanwhile, as tensions between the US and the Soviet Union over Iran escalate, the Home Office directs Sheffield City Council to assemble an emergency operations team, which establishes itself in a makeshift bomb shelter in the basement of the Town Hall. After an ignored US ultimatum to the Soviets results in a brief tactical nuclear skirmish, Britain is gripped by fear, with looting and rioting erupting. "Known subversives" (including peace activists and some trade unionists) are arrested and interned under the Emergency Powers Act.
At 8:30 a.m. (3:30 a.m. in Washington, D.C.) on 26 May, Attack Warning Red is transmitted, and Sheffield's air raid sirens sound. A warhead air bursts over the North Sea, then another hits RAF Finningley 20 mi from Sheffield. Although the city is not heavily damaged, chaos breaks out. Jimmy is last seen attempting to reach Ruth. Shortly after the first strike, Sheffield is hit by a one megaton warhead over the Tinsley Viaduct, causing enormous destruction. A title card states that strategic targets, including steel and chemical factories in the Midlands, are attacked, with two-thirds of all homes being destroyed and immediate deaths ranging between 17 and 30 million.
Town Hall collapses, trapping the Sheffield emergency operations team under it. Within hours, nuclear fallout from a ground burst at Crewe begins descending upon Sheffield, with the surviving Kemps succumbing to radiation sickness. Fallout keeps rescuers from fighting fires or rescuing those trapped under the debris. Ruth goes to the Sheffield Royal Infirmary, where there is no electricity, no running water and no sanitation, with drugs and medical supplies having long since run out.
A month after the attack, soldiers dig into the Town Hall basement and find that the emergency operations staff have all died of suffocation. No efforts are made to bury the dead and burning the bodies is considered a waste of fuel, which leads to an outbreak of diseases such as cholera and typhoid. The government authorises the use of capital punishment and special courts are authorised to shoot prisoners. As money no longer has any value, the only viable currency is food, given as a reward for work or withheld as punishment. Due to the millions of tons of soot, smoke and dust that have been blown into the upper atmosphere, a nuclear winter develops. Ruth is later seen working on a farm in Buxton, having defied official advice and fled the city, eventually giving birth to a daughter in a farm out-building.
A year after the war, sunlight begins to return but food production is poor due to the lack of proper equipment, fertilisers and fuel. Damage to the ozone layer also means this sunlight is heavy with ultraviolet radiation, with cataracts and cancer becoming more common.
Ten years later, Britain's population has fallen to medieval levels of around 4 to 11 million people. Survivors work in fields using primitive farming tools, and children born since the attack are educationally stunted, speaking a broken form of English. Prematurely aged and blind with cataracts, Ruth dies, survived by her 10-year-old daughter Jane (Victoria O'Keefe). By this time, the country has recovered slightly, with some resumption of manual coal mining, limited electricity, and some mechanisation from traction engines. However, the population continues to live in squalid conditions among the ruins.
Three years after Ruth's death, Jane and two boys are caught stealing food. One boy is shot in the ensuing confusion; Jane wrestles for the food with the other boy and they have (what the script describes as) "crude intercourse". Months later, Jane finds a makeshift hospital and gives birth. The film ends as she is about to scream in horror as she looks upon the baby she has just given birth to.
Cast.
Although Jackson initially considered casting actors from "Coronation Street", he later decided to take a neorealist approach, and opted to cast relatively unknown actors in order to heighten the film's impact through the use of characters the audience could relate to.
Production and themes.
"Threads" was first commissioned by the Director-General of the BBC Alasdair Milne, after he watched the 1965 drama-documentary "The War Game", which had not been shown on the BBC when it was made, due to pressure from the Wilson government, although it had had a limited release in cinemas. Mick Jackson was hired to direct the film, as he had previously worked in the area of nuclear apocalypse in 1982, producing the BBC "Q.E.D." documentary "A Guide to Armageddon". This was considered a breakthrough at the time, considering the previous banning of "The War Game", which BBC staff believed would have resulted in mass suicides if aired. Jackson subsequently travelled around the UK and the US, consulting leading scientists, psychologists, doctors, defence specialists and strategic experts in order to create the most realistic depiction of nuclear war possible for his next film. Jackson consulted various sources in his research, including the 1983 "Science" article "Nuclear Winter: Global Consequences of Multiple Nuclear Explosions", penned by Carl Sagan and James B. Pollack. Details on a possible attack scenario and the extent of the damage were derived from "Doomsday: A Nuclear Attack on the United Kingdom" (1983), while the ineffective post-war plans of the UK government came from Duncan Campbell's 1982 exposé "War Plan UK". In portraying the psychological damage suffered by survivors, Jackson took inspiration from the behaviour of the Hibakusha and Magnus Clarke's 1982 book "Nuclear Destruction of Britain". Sheffield was chosen as the main location because of its "Nuclear-free zone" policy, as mentioned in the article about the nickname "People's Republic of South Yorkshire".
Jackson hired Barry Hines to write the script because of his political awareness. The relationship between the two was strained on several occasions, as Hines spent much of his time on set, and apparently disliked Jackson on account of his middle class upbringing. As part of their research, the two spent a week at the Home Office training centre for 'official survivors' in Easingwold which, according to Hines, showed just 'how disorganised post-war reconstruction would be'.
Auditions were advertised in "The Star", and took place in the ballroom of Sheffield City Hall, where 1,100 candidates turned up. All extras were chosen on the basis of height and age, and were all told to look 'miserable' and to wear raggedy clothes. The makeup for extras playing third degree burn victims consisted of Rice Krispies and tomato ketchup. The scenes taking place six weeks after the attack were shot in the Peak District National Park, though because weather conditions were considered too fine to pass off as a nuclear winter, stage snow had to be spread around the rocks and heather, and cameramen installed light filters on their equipment to block out the sunlight.
Jackson later recalled that while BBC productions would usually be followed by phone calls of congratulations from friends or colleagues immediately after airing, no such calls came after the first screening of "Threads". Jackson later "realised... that people had just sat there thinking about it, in many cases not sleeping or being able to talk." He later said that he had it on good authority that Ronald Reagan watched the film when it aired in the US. Hines himself received a letter of praise from Labour leader Neil Kinnock.
Broadcast and release history.
"Threads" was a co-production of the BBC, Nine Network Australia and Western-World Television, Inc. It was first broadcast on BBC Two on 23 September 1984 at 9:30 pm, and achieved the highest ratings (6.9 million) of the week. It was repeated on BBC One on 1 August 1985 as part of a week of programmes marking the fortieth anniversary of the atomic bombings of Hiroshima and Nagasaki, which also saw the first television screening of "The War Game" (which had been deemed too disturbing for television in the 20 years since it had been made). "Threads" was not shown again on British screens until the digital channel BBC Four broadcast it in October 2003. "Threads" was broadcast in the United States on cable network Superstation TBS on 13 January 1985, followed by a panel discussion on nuclear war. It was also shown in syndication to local commercial stations and, later, on many PBS stations. "Threads" was broadcast in Australia on the Nine Network on 19 June 1985. Unusually for a commercial network, it broadcast the film without commercial breaks.
"Threads" was originally released by BBC Video (on VHS and, for a very short period, Betamax) in 1987 in the United Kingdom. The play was re-released on both VHS and DVD in 2000 on the Revelation label, followed by a new DVD edition in 2005. Due to licensing difficulties the 1987 release replaced Chuck Berry's recording of his song "Johnny B. Goode" with an alternative recording of the song.
Reception.
"Threads" was nominated for seven BAFTA awards in 1985. It won for Best Single Drama, Best Design, Best Film Cameraman and Best Film Editor. Its other nominations were for Best Costume Design, Best Make-Up, and Best Film Sound.

</doc>
<doc id="31647" url="http://en.wikipedia.org/wiki?curid=31647" title="Article Two of the United States Constitution">
Article Two of the United States Constitution

Article Two of the United States Constitution creates the executive branch of the government, consisting of the President, the Vice President, and other executive officers and staffers appointed by the President, including the Cabinet. Pursuant to Article Two, the executive power of the federal government is vested in the President.
Section 1: President and Vice President.
Clause 1: Executive Power.
The executive Power shall be vested in a President of the United States of America. He shall hold his Office during the Term of four Years, and, together with the Vice President, chosen for the same Term, be elected, as follows
Clause one is a "vesting clause," similar to other clauses in Articles One and Three, but it vests the power to execute the instructions of Congress, which has the exclusive power to make laws; "To make all laws which shall be necessary and proper for carrying into execution the foregoing powers, and all other powers vested by this Constitution in the government of the United States, or in any department or officer thereof."
The head of the Executive Branch is the President of the United States. The President and the Vice President are elected every four years.
Clause 2: Method of choosing electors.
Each State shall appoint, in such Manner as the Legislature thereof may direct, a Number of Electors, equal to the whole Number of Senators and Representatives to which the State may be entitled in the Congress: but no Senator or Representative, or Person holding an Office of Trust or Profit under the United States, shall be appointed an Elector.
Under the U.S. Constitution the President and Vice President are chosen by Electors, under a constitutional grant of authority delegated to the legislatures of the several states and the District of Columbia (see "Bush v. Gore"). The constitution reserves the choice of the precise manner for creating Electors to the will of the state legislatures. It does not define or delimit what process a state legislature may use to create its "state" college of Electors. In practice, the state legislatures have generally chosen to create Electors through an indirect popular vote, since the 1820s.
In an indirect popular vote, it is the names of the electors who are on the ballot to be elected. Typically, their names are aligned under the name of the candidate for President and Vice President, that they, the Elector, have pledged they will support. It is fully understood by the voters and the Electors themselves that they are the representative "stand-ins" for the individuals to whom they have pledged to cast their electoral college ballots to be President and Vice President. In some states, in past years, this pledge was informal, and Electors could still legally cast their electoral ballot for whomever they chose. More recently, state legislatures have mandated in law that Electors "shall" cast their electoral college ballot for the Presidential Candidate to whom they are pledged. The constitutionality of such mandates is uncertain.
Each state chooses as many Electors as it has Representatives and Senators representing it in Congress. Under the Twenty-third Amendment, the District of Columbia may choose no more electors than the state with the lowest number of electoral votes. No Senators, Representatives or federal officers may become Electors.
Clause 3: Electors.
The Electors shall meet in their respective States, and vote by Ballot for two Persons, of whom one at least shall not be an Inhabitant of the same State with themselves. And they shall make a List of all the Persons voted for, and of the Number of Votes for each; which List they shall sign and certify, and transmit sealed to the Seat of the Government of the United States, directed to the President of the Senate. The President of the Senate shall, in the Presence of the Senate and House of Representatives, open all the Certificates, and the Votes shall then be counted. The Person having the greatest Number of Votes shall be the President, if such Number be a Majority of the whole Number of Electors appointed; and if there be more than one who have such Majority, and have an equal Number of Votes, then the House of Representatives shall immediately chuse ["sic"] by Ballot one of them for President; and if no Person have a Majority, then from the five highest on the List the said House shall in like Manner chuse ["sic"] the President. But in chusing ["sic"] the President, the Votes shall be taken by States, the Representation from each State having one Vote; A quorum for this Purpose shall consist of a Member or Members from two thirds of the States, and a Majority of all the States shall be necessary to a Choice. In every Case, after the Choice of the President, the Person having the greatest Number of Votes of the Electors shall be the Vice President. But if there should remain two or more who have equal Votes, the Senate shall chuse ["sic"] from them by Ballot the Vice President.
"(Note: This procedure was changed by the Twelfth Amendment in 1804.)"
In modern practice, each state chooses its electors in popular elections. Once chosen, the electors meet in their respective states to cast ballots for the President and Vice President. Originally, each elector cast two votes for President; at least one of the individuals voted for had to be from a state different from the elector's. The individual with the majority of votes became President, and the runner-up became Vice President. In case of a tie, the House of Representatives could choose one of the tied candidates; if no person received a majority, then the House could again choose one of the five with the greatest number of votes. When the House voted, each state delegation cast one vote, and the vote of a majority of states was necessary to choose a President. If second-place candidates were tied, then the Senate broke the tie. A quorum of two-thirds applied in both Houses: at least one member from each of two-thirds of the states in the House of Representatives, and at least two-thirds of the Senators in the Senate. This procedure was followed in 1801 after the electoral vote produced a tie, and nearly resulted in a deadlock in the House.
The Twelfth Amendment introduced a number of important changes to the procedure. Now, Electors do not cast two votes for President; rather, they cast one vote for President and another for Vice President. In case no Presidential candidate receives a majority, the House chooses from the top three (not five, as with Vice Presidential candidates). The Amendment also requires the Senate to choose the Vice President from those with the two highest figures if no Vice Presidential candidate receives a majority of electoral votes (rather than only if there's a tie for second for President). It also stipulates that to be the Vice President, a person must be qualified to be the President.
Clause 4: Election day.
The Congress may determine the Time of chusing ["sic"] the Electors, and the Day on which they shall give their Votes; which Day shall be the same throughout the United States.
Congress sets a national Election Day. Currently, Electors are chosen on the Tuesday following the first Monday in November, in the year before the President's term is to expire. The Electors cast their votes on the Monday following the second Wednesday in December of that year. Thereafter, the votes are opened and counted by the Vice President, as President of the Senate, in a joint session of Congress.
Clause 5: Qualifications for office.
Section 1 of Article Two of the United States Constitution sets forth the eligibility requirements for serving as president of the United States:
No Person except a natural born Citizen, or a Citizen of the United States, at the time of the Adoption of this Constitution, shall be eligible to the Office of President; neither shall any person be eligible to that Office who shall not have attained to the Age of thirty five Years, and been fourteen Years a Resident within the United States.
By the time of their inauguration, the President and Vice President must be:
Eligibility for holding the office of President and Vice-President were modified by subsequent amendments:
Clause 6: Vacancy and disability.
In Case of the Removal of the President from Office, or of his Death, Resignation, or Inability to discharge the Powers and Duties of the said Office, the Same shall devolve on the Vice President, and the Congress may by Law provide for the Case of Removal, Death, Resignation or Inability, both of the President and Vice President, declaring what Officer shall then act as President, and such Officer shall act accordingly, until the Disability be removed, or a President shall be elected.
The wording of this clause caused much controversy at the time it was first used. When William Henry Harrison died in office, a debate arose over whether the Vice President would become President, or if he would just inherit the powers, thus becoming an Acting President. Harrison's Vice President, John Tyler, believed that he had the right to become President. However, many Senators argued that he only had the right to assume the powers of the presidency long enough to call for a new election. Because the wording of the clause is so vague, it was impossible for either side to prove its point. Tyler ended up taking the Oath of Office and became President, setting a precedent that is followed to this day. Tyler's precedent made it possible for Vice Presidents Millard Fillmore, Andrew Johnson, Chester Arthur, Theodore Roosevelt, Calvin Coolidge, Harry Truman, and Lyndon Johnson to ascend to the presidency (Gerald Ford took office after the passage of the Twenty-fifth Amendment).
John Tyler's precedent established that if the President's office becomes vacant due to death, resignation or disqualification, the Vice President becomes President. The Congress may provide for a line of succession beyond the Vice President. The Presidential Succession Act establishes the order as: the Speaker of the House of Representatives, the President "pro tempore" of the Senate and then the fifteen Cabinet Secretaries in order of that Department's establishment.
The Twenty-fifth Amendment explicitly states that when the Presidency is vacant, then the Vice President becomes President. This provision applied at the time Gerald Ford succeeded to the Presidency. In case of a Vice Presidential vacancy, the Amendment permits the President to appoint, with the approval of both Houses of Congress, a new Vice President. Furthermore, the Amendment provides that the President, or the Vice President and Cabinet, can declare the President unable to discharge his duties, in which case the Vice President becomes Acting President. If the declaration is done by the Vice President and Cabinet, the Amendment permits the President to take control back, unless the Vice President and Cabinet challenge the President and two-thirds of both Houses vote to sustain the findings of the Vice President and Cabinet. If the declaration is done by the President, he may take control back without risk of being overridden by the Congress.
Clause 7: Salary.
The President shall, at stated Times, receive for his Services, a Compensation, which shall neither be increased nor diminished during the Period for which he shall have been elected, and he shall not receive within that Period any other Emolument from the United States, or any of them.
The President's salary, currently $400,000 a year, must remain constant throughout the President's term. The President may not receive other compensation from either the federal or any state government.
Clause 8: Oath or affirmation.
Before he enters the Execution of his Office, he shall take the following Oath or Affirmation:—"I do solemnly swear (or affirm) that I will faithfully execute the Office of President of the United States, and will to the best of my Ability, preserve, protect and defend the Constitution of the United States."
According to the Joint Congressional Committee on Presidential Inaugurations, George Washington added the words "So help me God" during his first inaugural, though this has been disputed. There are no contemporaneous sources for this fact, and no eyewitness sources to Washington's first inaugural mention the phrase at all—including those that transcribed what he said for his oath.
Also, the President-elect's name is typically added after the "I", for example, "I, George Washington, do..." Normally, the Chief Justice of the United States administers the oath. It is sometimes asserted that the oath bestows upon the President the power to do whatever is necessary to "preserve, protect and defend the Constitution." Andrew Jackson, while vetoing an Act for the renewal of the charter of the national bank, implied that the President could refuse to execute statutes that he felt were unconstitutional. In suspending the privilege of the writ of "habeas corpus", President Abraham Lincoln claimed that he acted according to the oath. His action was challenged in court and overturned by the U.S. Circuit Court in Maryland (led by Chief Justice Roger B. Taney) in "Ex Parte Merryman", 17 F. Cas. 144 (C.C.D. Md. 1861). Lincoln ignored Taney's order. Finally, Andrew Johnson's counsel referred to the theory during his impeachment trial. Otherwise, few have seriously asserted that the oath augments the President's powers.
The Vice President also has an oath of office, but it is not mandated by the Constitution and is prescribed by statute. Currently, the Vice Presidential oath is the same as that for Members of Congress.
Section 2: Presidential powers.
In the landmark decision "Nixon v. General Services Administration" Justice William Rehnquist, afterwards the Chief Justice, declared in his dissent the need to "fully describe the preeminent position that the President of the United States occupies with respect to our Republic. Suffice it to say that the President is made the sole repository of the executive powers of the United States, and the powers entrusted to him as well as the duties imposed upon him are awesome indeed."
Clause 1: Command of military; Opinions of cabinet secretaries; Pardons.
The President shall be Commander in Chief of the Army and Navy of the United States, and of the Militia of the several States, when called into the actual Service of the United States; he may require the Opinion, in writing, of the principal Officer in each of the executive Departments, upon any Subject relating to the Duties of their respective Offices, and he shall have Power to grant Reprieves and Pardons for Offenses against the United States, except in Cases of Impeachment.
The Constitution vests the President with Executive Power. That power reaches its zenith when wielded to protect national security. And federal courts in the United States must pay proper deference to the Executive in assessing the threats that face the nation. The President is the military's commander-in-chief; however Article One gives Congress and not the President the exclusive right to declare war. Nevertheless, the power of the president to initiate hostilities has been subject to question. According to historian Thomas Woods, "Ever since the Korean War, Article II, Section 2 [...] has been interpreted 'The president has the power to initiate hostilities without consulting Congress' [...]But what the framers actually meant by that clause was that once war has been declared, it was the President’s responsibility as commander-in-chief to direct the war. Alexander Hamilton spoke in such terms when he said that the president, although lacking the power to declare war, would have “the direction of war when authorized or begun.” The president acting alone was authorized only to repel sudden attacks (hence the decision to withhold from him only the power to “declare” war, not to “make” war, which was thought to be a necessary emergency power in case of foreign attack).
 Since World War II, every major military action has been technically a U.S. military operation or a U.N. "police action", which are deemed legally legitimate by Congress, and various United Nations Resolutions because of decisions such as the Gulf of Tonkin Resolution or the The Resolution of The Congress Providing Authorization for Use of Force In Iraq.
The President may require the "principal officer" of any executive department to tender his advice in writing. Thus, implicitly, the Constitution creates a Cabinet that includes the principal officers of the various departments.
The President, furthermore, may grant pardon or reprieves, except in cases of impeachment. Originally, as ruled by the Supreme Court in "United States v. Wilson" (1833), the pardon could be rejected by the convict. In "Biddle v. Perovich" 274 U.S. (1927), the Supreme Court reversed the doctrine, ruling that "[a] pardon in our days is not a private act of grace from an individual happening to possess power. It is a part of the Constitutional scheme. When granted it is the determination of the ultimate authority that the public welfare will be better served by inflicting less than what the judgment fixed."
Clause 2: Advice and Consent Clause.
The President exercises the powers in the Advice and Consent Clause with the advice and consent of the Senate.
He shall have Power, by and with the Advice and Consent of the Senate, to make Treaties, provided two thirds of the Senators present concur; and he shall nominate, and by and with the Advice and Consent of the Senate, shall appoint Ambassadors, other public Ministers and Consuls, Judges of the supreme Court, and all other Officers of the United States, whose Appointments are not herein otherwise provided for, and which shall be established by Law: but the Congress may by Law vest the Appointment of such inferior Officers, as they think proper, in the President alone, in the Courts of Law, or in the Heads of Departments.
Treaties.
The President may enter the United States into treaties, but they are not effective until ratified by a two-thirds vote in the Senate. In Article II however, the Constitution is not very explicit about the termination of treaties. The first abrogation of a treaty occurred in 1798, when Congress passed a law terminating a 1778 Treaty of Alliance with France. In the nineteenth century, several Presidents terminated treaties after Congress passed resolutions requesting the same. In 1854, however, President Franklin Pierce terminated a treaty with Denmark with the consent of the Senate alone. A Senate committee ruled that it was correct procedure for the President to terminate treaties after being authorized by the Senate alone, and not the entire Congress. President Pierce's successors, however, returned to the former procedure of obtaining authorization from both Houses. Some Presidents have claimed to themselves the exclusive power of terminating treaties. Abraham Lincoln, for instance, terminated a treaty without prior Congressional authorization, but Congress retroactively approved his decision at a later point. The first unambiguous case of a President terminating a treaty without authorization, granted prior to or after the termination, occurred when Jimmy Carter terminated a treaty with the Republic of China. For the first time, judicial determination was sought, but the effort proved futile: the Supreme Court could not find a majority agreeing on any particular principle, and therefore instructed the trial court to dismiss the case.
Appointments.
The President may also appoint judges, ambassadors, consuls, ministers and other officers with the advice and consent of the Senate. By law, however, Congress may allow the President, heads of executive departments, or the courts to appoint inferior officials.
The Senate has a long-standing practice of permitting motions to reconsider previous decisions. In 1931, the Senate granted advice and consent to the President on the appointment of a member of the Federal Power Commission. The officer in question was sworn in, but the Senate, under the guise of a motion to reconsider, rescinded the advice and consent. In the writ of quo warranto proceedings that followed, the Supreme Court ruled that the Senate was not permitted to rescind advice and consent after the officer had been installed.
After the Senate grants advice and consent, however, the President is under no compulsion to commission the officer. It has not been settled whether the President has the prerogative to withhold a commission after having signed it. This issue played a large part in the famous court case "Marbury v. Madison".
At times the President has asserted the power to remove individuals from office. Congress has often explicitly limited the President's power to remove; during the Reconstruction Era, Congress passed the Tenure of Office Act, purportedly preventing Andrew Johnson from removing, without the advice and consent of the Senate, anyone appointed with the advice and consent of the Senate. President Johnson ignored the Act, and was later impeached and acquitted. The constitutionality of the Act was not immediately settled. In "Myers v. United States", 272 U.S. (1926), the Supreme Court held that Congress could not limit the President's power to remove an executive officer (the Postmaster General), but in "Humphrey's Executor v. United States", 295 U.S. (1935) it upheld Congress's authority to restrict the President's power to remove officers of the Federal Trade Commission, an "administrative body [that] cannot in any proper sense be characterized as an arm or eye of the executive."
Congress may repeal the legislation that authorizes the appointment of an executive officer. But it "cannot reserve for itself the power of an officer charged with the execution of the laws except by impeachment." Congress has from time to time changed the number of justices in the Supreme Court.
Clause 3: Recess appointments.
The President shall have Power to fill up all Vacancies that may happen during the Recess of the Senate, by granting Commissions which shall expire at the End of their next Session.
During recesses of the Senate, the President may appoint officers, but their commissions expire at the conclusion of the Senate's next session.
Section 3: Presidential responsibilities.
He shall from time to time give to the Congress Information of the State of the Union, and recommend to their Consideration such Measures as he shall judge necessary and expedient; he may, on extraordinary Occasions, convene both Houses, or either of them, and in Case of Disagreement between them, with Respect to the Time of Adjournment, he may adjourn them to such Time as he shall think proper; he shall receive Ambassadors and other public Ministers; he shall take Care that the Laws be faithfully executed, and shall Commission all the Officers of the United States.
Clause 1: State of the Union.
The President must give the Congress information on the "State of the Union" "from time to time." This is called the State of the Union Clause. Originally, Presidents personally delivered annual addresses to Congress. Thomas Jefferson, who felt that the procedure resembled the Speech from the Throne delivered by British monarchs, chose instead to send written messages to Congress for reading by clerks. Jefferson's procedure was followed by future Presidents until Woodrow Wilson reverted to the former procedure of personally addressing Congress, which has continued to this day[ [update]].
Kesavan and Sidak explain the purpose of the State of the Union clause:
Clause 2: Making recommendations to Congress.
The president has the power and duty to recommend, for the consideration of Congress, such measures which the president deems as "necessary and expedient". At his inauguration George Washington declared in his : "By the article establishing the executive department it is made the duty of the President "to recommend to your consideration such measures as he shall judge necessary and expedient."" This is the Recommendation Clause.'
Kesavan and Sidak explain the purpose of the Recommendation clause:
Sidak explained that there is a connection between the Recommendation clause and the Petition Clause of the first amendment: "Through his performance of the duty to recommend measures to Congress, the President functions as the agent of a diffuse electorate who seek the redress of grievances. To muzzle the President, therefore, is to diminish the effectiveness of this right expressly reserved to the people under the first amendment.":2119, note 7 Kesavan and Sidak also cited a Professor Bybee who stated in this context: "The Recommendation Clause empowers the President to represent the people before Congress, by recommending measures for the reform of government, for the general welfare, or for the redress of grievances. The Right of Petition Clause prevents Congress from abridging the right of the people to petition for a redress of grievances.":43
The Recommendation clause imposes a duty, but its performance rests solely with the President. Congress possesses no power to compel the President to recommend, as he alone is the "judge" of what is "necessary and expedient." Unlike the Necessary and Proper Clause of Article I, which limits Congress's discretion to carrying out only its delegated powers, the phrase "necessary and expedient" implies a wider range of discretion for the President. Because this is a political question, there has been little judicial involvement with the President's actions under the clause as long as Presidents have not tried to extend their legislative powers. In Youngstown Sheet & Tube Co. v. Sawyer (1952), the Supreme Court noted that the Recommendations Clause serves as a reminder that the President cannot make law by himself: "The power to recommend legislation, granted to the President, serves only to emphasize that it is his function to recommend and that it is the function of the Congress to legislate." The Court made a similar point in striking down the line-item veto in Clinton v. City of New York (1998). When President William Jefferson Clinton attempted to shield the records of the President's Task Force on Health Care Reform as essential to his functions under the Recommendations Clause, a federal circuit court rejected the argument and noted in Ass'n of American Physicians & Surgeons v. Clinton (1993): "[T]he Recommendation Clause is less an obligation than a right. The President has the undisputed authority to recommend legislation, but he need not exercise that authority with respect to any particular subject or, for that matter, any subject."
Clause 3: Calling Congress into extraordinary session; adjourning Congress.
The President may call extraordinary sessions of one or both Houses of Congress. If the two Houses cannot agree on a date for adjournment, the President may adjourn both Houses to such a time as befits the circumstances. The last time this power was exercised was in 1948, when President Harry S Truman called a special session of Congress. That was the twenty-seventh time in American history that a president convened such a session.
Clause 4: Receiving foreign representatives.
The President receives all foreign Ambassadors. This clause of the Constitution has been interpreted to imply that the President can be granted broad power over all matters of foreign policy by Congress.
Clause 5: Caring for the faithful execution of the law.
The President must "take care that the laws be faithfully executed." This clause in the Constitution imposes a duty on the President to take due care while executing laws and is called the Take Care Clause, also known as the Faithful Execution Clause or Faithfully Executed Clause. This clause is meant to ensure that a law is faithfully executed by the President, even if he disagrees with the purpose of that law. By virtue of his executive power, the President may execute the law and control the law execution of others. Under the Take Care Clause, however, the President must exercise his law-execution power to "take Care that the Laws be faithfully executed." Addressing the North Carolina ratifying convention, William Maclaine declared that the Faithful Execution Clause was "one of the [Constitution's] best provisions." If the President "takes care to see the laws faithfully executed, it will be more than is done in any government on the continent; for I will venture to say that our government, and those of the other states, are, with respect to the execution of the laws, in many respects mere ciphers." President George Washington interpreted this clause as imposing on him a unique duty to ensure the execution of federal law. Discussing a tax rebellion, Washington observed, "it is my duty to see the Laws executed: to permit them to be trampled upon with impunity would be repugnant to [that duty.]"
According to former United States Assistant Attorney General Walter E. Dellinger III, the Supreme Court and the Attorneys General have long interpreted the Take Care Clause to mean that the President has no inherent constitutional authority to suspend the enforcement of the laws, particularly of statutes. The Take Care Clause demands that the President obey the law, the Supreme Court said in Humphrey's Executor v. United States, and repudiates any notion that he may dispense with the law's execution. In Printz v. United States, 521 U.S. 898 (1997), the Supreme Court explained how the President executes the law: "The Constitution does not leave to speculation who is to administer the laws enacted by Congress; the President, it says, "shall take Care that the Laws be faithfully executed," Art. II, §3, personally and through officers whom he appoints (save for such inferior officers as Congress may authorize to be appointed by the "Courts of Law" or by "the Heads of Departments" who with other presidential appointees), Art. II, §2."
The President may not prevent a member of the executive branch from performing a ministerial duty lawfully imposed upon him by Congress. (See Marbury v. Madison (1803); and Kendall v. United States ex rel. Stokes (1838)). Nor may the President take an action not authorized either by the Constitution or by a lawful statute. (See Youngstown Sheet & Tube Co. v. Sawyer (1952)). Finally, the President may not refuse to enforce a constitutional law, or "cancel" certain appropriations, for that would amount to an extra-constitutional veto or suspension power.
The President, while having to enforce the law, also possesses wide discretion in deciding how and even when to enforce laws. He also has a range of interpretive discretion in deciding the meaning of laws he must execute. When an appropriation provides discretion, the President can gauge when and how appropriated moneys can be spent most efficiently.
Some Presidents have claimed the authority under this clause to impound money appropriated by Congress. President Jefferson, for example, delayed the expenditure of money appropriated for the purchase of gunboats for over a year. President Franklin D. Roosevelt and his successors sometimes refused outright to expend appropriated money. The Supreme Court, however, has held that impoundments without Congressional authorization are unconstitutional.
It has been asserted that the President's responsibility in the "faithful" execution of the laws entitles him to suspend the privilege of the writ of "habeas corpus". Article One provides that the privilege may not be suspended save during times of rebellion or invasion, but it does not specify who may suspend the privilege. The Supreme Court ruled that Congress may suspend the privilege if it deems it necessary. During the American Civil War, President Abraham Lincoln suspended the privilege, but, owing to the vehement opposition he faced, obtained congressional authorization for the same. Since then, the privilege of the writ has only been suspended upon the express authorization of Congress.
In "Mississippi v. Johnson", 71 U.S. (1867), the Supreme Court ruled that the judiciary may not restrain the President in the execution of laws. In that case the Supreme Court refused to entertain a request for an injunction preventing President Andrew Johnson from executing the Reconstruction Acts, which were claimed to be unconstitutional. The Court found that "[t]he Congress is the legislative department of the government; the President is the executive department. Neither can be restrained in its action by the judicial department; though the acts of both, when performed, are, in proper cases, subject to its cognizance." Thus, the courts cannot bar the passage of a law by Congress, though it may strike down such a law as unconstitutional. A similar construction applies to the executive branch.
Clause 6: Officers' commissions.
The President commissions "all the Officers of the United States." These include officers in both military and foreign service. (Under Article I, Section 8, the States have authority for "the Appointment of the Officers . . . of the [State] Militia . . ..")
The presidential authority to commission officers had a large impact on the 1803 case "Marbury v. Madison", where outgoing Federalist President John Adams feverishly signed many commissions to the judiciary on his final day in office, hoping to, as incoming Democratic-Republican President Thomas Jefferson put it, "[retire] into the judiciary as a stronghold." However, in his haste, Adams' Secretary of State neglected to have all the commissions delivered. Incoming President Jefferson was enraged with Adams, and ordered his Secretary of State, James Madison, to refrain from delivering the remaining commissions. William Marbury took the matter to the Supreme Court, where the famous "Marbury" was decided. 
Section 4: Impeachment.
The President, Vice President and all civil Officers of the United States, shall be removed from Office on Impeachment for, and Conviction of, Treason, Bribery, or other High crimes and Misdemeanors.
The Constitution also allows for involuntary removal from office. The President, Vice-President, Cabinet Secretaries, and other executive officers, as well as judges, may be impeached by the House of Representatives and tried in the Senate.
Any official convicted by impeachment is immediately removed from office. The Senate may also choose to bar the removed official from holding any federal office in the future. No other punishments may be inflicted pursuant to the impeachment proceeding, but the convicted party remains liable to trial and punishment in the courts for civil and criminal charges.

</doc>
<doc id="31658" url="http://en.wikipedia.org/wiki?curid=31658" title="Sixth Amendment to the United States Constitution">
Sixth Amendment to the United States Constitution

The Sixth Amendment (Amendment VI) to the United States Constitution is the part of the United States Bill of Rights that sets forth rights related to criminal prosecutions. The Supreme Court has applied the protections of this amendment to the states through the Due Process Clause of the Fourteenth Amendment.
Text.
In all criminal prosecutions, the accused shall enjoy the right to a speedy and public trial, by an impartial jury of the State and district wherein the crime shall have been committed, which district shall have been previously ascertained by law, and to be informed of the nature and cause of the accusation; to be confronted with the witnesses against him; to have compulsory process for obtaining witnesses in his favor, and to have the Assistance of Counsel for his defence.
Rights secured.
Speedy trial.
Criminal defendants have the right to a speedy trial. In "Barker v. Wingo", 407 U.S. (1972), the Supreme Court laid down a four-part case-by-case balancing test for determining whether the defendant's speedy trial right has been violated. The four factors are:
In "Strunk v. United States", 412 U.S. (1973), the Supreme Court ruled that if the reviewing court finds that a defendant's right to a speedy trial was violated, then the indictment must be dismissed and/or the conviction overturned. The Court held that, since the delayed trial is the state action which violates the defendant's rights, no other remedy would be appropriate. Thus, a reversal or dismissal of a criminal case on speedy trial grounds means that no further prosecution for the alleged offense can take place.
Public trial.
In "Sheppard v. Maxwell", 384 U.S. (1966), the Supreme Court ruled that the right to a public trial is not absolute. In cases where excess publicity would serve to undermine the defendant's right to due process, limitations can be put on public access to the proceedings. According to "Press-Enterprise Co. v. Superior Court", 478 U.S. (1986), trials can be closed at the behest of the government if there is "an overriding interest based on findings that closure is essential to preserve higher values and is narrowly tailored to serve that interest." The accused may also request a closure of the trial; though, it must be demonstrated that "first, there is a substantial probability that the defendant's right to a fair trial will be prejudiced by publicity that closure would prevent, and second, reasonable alternatives to closure cannot adequately protect the defendant's right to a fair trial."
Impartial jury.
The right to a jury has always depended on the nature of the offense with which the defendant is charged. Petty offenses—those punishable by imprisonment for no more than six months—are not covered by the jury requirement. Even where multiple petty offenses are concerned, the total time of imprisonment possibly exceeding six months, the right to a jury trial does not exist. Also, in the United States, except for serious offenses (such as murder), minors are usually tried in a juvenile court, which lessens the sentence allowed, but forfeits the right to a jury.
Originally, the Supreme Court held that the Sixth Amendment right to a jury trial indicated a right to “a trial by jury as understood and applied at common law, and includes all the essential elements as they were recognized in this country and England when the Constitution was adopted.” Therefore, it was held that juries had to be composed of twelve persons and that verdicts had to be unanimous, as was customary in England. 
When, under the Fourteenth Amendment, the Supreme Court extended the right to a trial by jury to defendants in state courts, it re-examined some of the standards. It has been held that twelve came to be the number of jurors by "historical accident," and that a jury of six would be sufficient, but anything less would deprive the defendant of a right to trial by jury. The Sixth Amendment mandates unanimity in a federal jury trial. However, the Supreme Court has ruled that the Due Process Clause of the Fourteenth Amendment, while requiring states to provide jury trials for serious crimes, does not incorporate all the elements of a jury trial within the meaning of the Sixth Amendment. Thus, states are not mandated to require jury unanimity, unless the jury has only six members.
Impartiality.
The Sixth Amendment requires juries to be impartial. Impartiality has been interpreted as requiring individual jurors to be unbiased. At "voir dire," each side may question potential jurors to determine any bias, and challenge them if the same is found; the court determines the validity of these challenges for cause. Defendants may not challenge a conviction because a challenge for cause was denied incorrectly if they had the opportunity to use peremptory challenges.
Venire of juries.
Another factor in determining the impartiality of the jury is the nature of the panel, or venire, from which the jurors are selected. Venires must represent a fair cross-section of the community; the defendant may establish that the requirement was violated by showing that the allegedly excluded group is a "distinctive" one in the community, that the representation of such a group in venires is unreasonable and unfair in regard to the number of persons belonging to such a group, and that the under-representation is caused by a systematic exclusion in the selection process. Thus, in "Taylor v. Louisiana", 419 U.S. (1975), the Supreme Court invalidated a state law that exempted women who had not made a declaration of willingness to serve from jury service, while not doing the same for men.
Sentencing.
In "Apprendi v. New Jersey", and "Blakely v. Washington", the Supreme Court ruled that a criminal defendant has a right to a jury trial not only on the question of guilt or innocence, but also regarding any fact used to increase the defendant's sentence beyond the maximum otherwise allowed by statutes or sentencing guidelines. In "Alleyne v. United States", the Court expanded on "Apprendi" and "Blakely" by ruling that a defendant's right to a jury applies to any fact that would increase a defendant's sentence beyond the minimum otherwise required by statute.
Vicinage.
 of the Constitution requires defendants be tried by juries and in the state in which the crime was committed. The Sixth Amendment requires the jury to be selected from judicial districts ascertained by statute. In "Beavers v. Henkel", 194 U.S. (1904), the Supreme Court ruled that the place where the offense is charged to have occurred determines a trial's location. Where multiple districts are alleged to have been locations of the crime, any of them may be chosen for the trial. In cases of offenses not committed in any state (for example, offenses committed at sea), the place of trial may be determined by the Congress.
Notice of accusation.
A criminal defendant has the right to be informed of the nature and cause of the accusation against him. Therefore, an indictment must allege all the ingredients of the crime to such a degree of precision that it would allow the accused to assert double jeopardy if the same charges are brought up in subsequent prosecution. The Supreme Court held in "United States v. Carll", 105 U.S. (1881) that “in an indictment ... it is not sufficient to set forth the offense in the words of the statute, unless those words of themselves fully, directly, and expressly, without any uncertainty or ambiguity, set forth all the elements necessary to constitute the offense intended to be punished.” Vague wording, even if taken directly from a statute, does not suffice. However, the government is not required to hand over written copies of the indictment free of charge.
Confrontation.
The Confrontation Clause relates to the common law rule preventing the admission of hearsay, that is to say, testimony by one witness as to the statements and observations of another person to prove that the statement or observation was accurate. The rationale was that the defendant had no opportunity to challenge the credibility of and cross-examine the person making the statements. Certain exceptions to the hearsay rule have been permitted; for instance, admissions by the defendant are admissible, as are dying declarations. Nevertheless, in "California v. Green", 399 U.S. (1970), the Supreme Court has held that the hearsay rule is not the same as the Confrontation Clause. Hearsay is admissible under certain circumstances. For example, in "Bruton v. United States", 391 U.S. 123 (1968), the Supreme Court ruled that while a defendant's out of court statements were admissible in proving the defendant's guilt, they were inadmissible hearsay against another defendant. Hearsay may, in some circumstances, be admitted though it is not covered by one of the long-recognized exceptions. For example, prior testimony may sometimes be admitted if the witness is unavailable. However, in "Crawford v. Washington", 541 U.S. (2004), the Supreme Court increased the scope of the Confrontation Clause by ruling that "testimonial" out-of-court statements are inadmissible if the accused did not have the opportunity to cross-examine that accuser and that accuser is unavailable at trial. In "Davis v. Washington" 547 U.S. (2006), the Court ruled that "testimonial" refers to any statement that an objectively reasonable person in the declarant's situation would believe likely to be used in court. In "Melendez-Diaz v. Massachusetts", 557 U.S. ___ (2009), and "Bullcoming v. New Mexico", 564 U.S. ___ (2011), the Court ruled that admitting a lab chemist's analysis into evidence, without having him testify, violated the Confrontation Clause. In "Michigan v. Bryant", (2011), the Court ruled that the "primary purpose" of a shooting victim's statement as to who shot him, and the police's reason for questioning him, each had to be objectively determined. If the "primary purpose" was for dealing with an "ongoing emergency", then any such statement was not testimonial and so the Confrontation Clause would not require the person making that statement to testify in order for that statement to be admitted into evidence.
The right to confront and cross-examine witnesses also applies to physical evidence; the prosecution must present physical evidence to the jury, providing the defense ample opportunity to cross-examine its validity and meaning. Prosecution generally may not refer to evidence without first presenting it.
In the late 20th and early 21st century this clause became an issue in the use of the silent witness rule.
Compulsory process.
The Compulsory Process Clause gives any criminal defendant the right to call witnesses in his favor. If any such witness refuses to testify, that witness may be compelled to do so by the court at the request of the defendant. However, in some cases the court may refuse to permit a defense witness to testify. For example, if a defense lawyer fails to notify the prosecution of the identity of a witness to gain a tactical advantage, that witness may be precluded from testifying.
Assistance of counsel.
A criminal defendant has the right to be represented by counsel.
In "Powell v. Alabama", 287 U.S. (1932), the Supreme Court ruled that “in a capital case, where the defendant is unable to employ counsel, and is incapable adequately of making his own defense because of ignorance, feeble mindedness, illiteracy, or the like, it is the duty of the court, whether requested or not, to assign counsel for him.” In "Johnson v. Zerbst", 304 U.S. (1938), the Supreme Court ruled that in all federal cases, counsel would have to be appointed for defendants who were too poor to hire their own. However, in "Betts v. Brady", 316 U.S. (1942), the Court declined to extend this requirement to the state courts under the Fourteenth Amendment unless the defendant demonstrated "special circumstances" requiring the assistance of counsel.
In 1961, the Court extended the rule that applied in federal courts to state courts. It held in "Hamilton v. Alabama", 368 U.S. (1961), that counsel had to be provided at no expense to defendants in capital cases when they so requested, even if there was no "ignorance, feeble mindedness, illiteracy, or the like." "Gideon v. Wainwright", 372 U.S. (1963), explicitly overruled "Betts v. Brady" and found that counsel must be provided to indigent defendants in all felony cases. Under "Argersinger v. Hamlin", 407 U.S. (1972), counsel must be appointed in any case resulting in a sentence of actual imprisonment. Regarding sentences not immediately leading to imprisonment, the Court in "Scott v. Illinois", 440 U.S. (1979), ruled that counsel did not need to be appointed, but in "Alabama v. Shelton", 535 U.S. (2002), the Court held that a suspended sentence that may result in incarceration can not be imposed if the defendant did not have counsel at trial.
As stated in "Brewer v. Williams", 430 U.S. (1977), the right to counsel “[means] at least that a person is entitled to the help of a lawyer at or after the time that judicial proceedings have been initiated against him, whether by formal charge, preliminary hearing, indictment, information, or arraignment.” "Brewer" goes on to conclude that once adversary proceeding have begun against a defendant, he has a right to legal representation when the government interrogates him and that when a defendant is arrested, “arraigned on [an arrest] warrant before a judge,” and “committed by the court to confinement,” “[t]here can be no doubt that judicial proceedings ha[ve] been initiated.”
Self-representation.
A criminal defendant may represent himself, unless a court deems the defendant to be incompetent to waive the right to counsel.
In "Faretta v. California", 422 U.S. (1975), the Supreme Court recognized a defendant's right to "pro se" representation. However, under "Godinez v. Moran", 509 U.S. (1993), a court that believes the defendant is less than fully competent to represent himself can require that defendant to be represented by counsel. In "Martinez v. Court of Appeal of California", 528 U.S. (2000), the Supreme Court ruled the right to "pro se" representation did not apply to appellate courts. In "Indiana v. Edwards", 554 U.S. 164 (2008), the Court ruled that a criminal defendant could be simultaneously competent to stand trial, but not competent to represent himself.
In "Bounds v. Smith", 430 U.S. (1977), the Supreme Court held that the constitutional right of "meaningful access to the courts" can be satisfied by counsel or access to legal materials. "Bounds" has been interpreted by several United States courts of appeals to mean a "pro se" defendant does not have a constitutional right to access a prison law library to research his defense when access to the courts has been provided through appointed counsel.

</doc>
<doc id="31716" url="http://en.wikipedia.org/wiki?curid=31716" title="Utah">
Utah

Utah ( or ; (Navajo: "Áshįįh bi Tó Hahoodzo"; Arapaho: "Wo'tééneihí" ) is a state in the western United States. It became the 45th state admitted to the Union on January 4, 1896. Utah is the 13th-largest, the 33rd-most populous, and the 10th-least-densely populated of the 50 United States. Utah has a population of about 2.9 million, approximately 80% of whom live along the Wasatch Front, centering on Salt Lake City. Utah is bordered by Colorado to the east, Wyoming to the northeast, Idaho to the north, Arizona to the south, and Nevada to the west. It also touches a corner of New Mexico in the southeast.
Approximately 62% of Utahns are reported to be members of The Church of Jesus Christ of Latter-day Saints or LDS (Mormons), which greatly influences Utah culture and daily life. The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital, Salt Lake City. Utah is the most religiously homogeneous state in the United States, the only state with a Mormon majority, and the only state with a majority population belonging to a single church.
The state is a center of transportation, education, information technology and research, government services, mining, and a major tourist destination for outdoor recreation. In 2013, the U.S. Census Bureau estimated that Utah had the second fastest-growing population of any state. St. George was the fastest–growing metropolitan area in the United States from 2000 to 2005. A 2012 Gallup national survey found Utah overall to be the "best state to live in" based on 13 forward-looking measurements including various economic, lifestyle, and health-related outlook metrics.
Etymology.
The name "Utah" is derived from the name of the Ute tribe. It means "people of the mountains" in the Ute language.
History.
Pre-Columbian.
Thousands of years before the arrival of European explorers, the Anasazi/Ancestral Pueblo and the Fremont tribes lived in what is now known as Utah. These Native American tribes are subgroups of the Ute-Aztec Native American ethnicity, and were sedentary. The Anasazi built their homes through excavations in mountains, and the Fremont built houses of straw before disappearing from the region around the 15th century.
Another group of Native Americans, the Navajo, settled in the region around the 18th century. In the mid-18th century, other Uto-Aztecan tribes, including the Goshute, the Paiute, the Shoshone, and the Ute people, also settled in the region. These five groups were present when the first European explorers arrived.
Spanish exploration (1540).
The southern Utah region was explored by the Spanish in 1540, led by Francisco Vásquez de Coronado, while looking for the legendary Cíbola. A group led by two Catholic priests—sometimes called the Dominguez-Escalante Expedition—left Santa Fe in 1776, hoping to find a route to the coast of California. The expedition traveled as far north as Utah Lake and encountered the native residents. The Spanish made further explorations in the region, but were not interested in colonizing the area because of its desert nature. In 1821, the year Mexico achieved its independence from Spain, the region of Utah became part of Mexico, as part of Alta California.
Trappers and fur traders explored some areas of Utah in the early 19th century. The city of Provo, Utah was named for one of those men, Étienne Provost, who visited the area in 1825. The city of Ogden, Utah was named after Peter Skene Ogden, a Canadian explorer who traded furs in the Weber Valley.
In late 1824, Jim Bridger became the first white person to sight the Great Salt Lake. Due to the high salinity of its waters, Bridger thought he had found the Pacific Ocean; he subsequently found that this body of water was nothing but a giant salt lake. After the discovery of the lake, hundreds of traders and trappers established trading posts in the region. In the 1830s, thousands of people traveling from the East toward the U.S. West began to make stops in the region of the Great Salt Lake, then known as Lake Youta.
LDS settlement (1847).
Following the death of Joseph Smith in 1844, Brigham Young as president of the Quorum of the Twelve became the effective leader of the Latter Day Saints in Nauvoo, Illinois. To address the growing conflicts between his people and their neighbors, Young agreed with Illinois Governor Thomas Ford in October 1845 that the Mormons would leave by the following year.
Brigham Young and the first band of Mormon pioneers came to the Salt Lake Valley on July 24, 1847. Over the next 22 years, more than 70,000 pioneers crossed the plains and settled in Utah.
For the first few years, Brigham Young and the thousands of early settlers of Salt Lake City struggled to survive. The barren desert land was deemed by the Mormons as desirable as a place where they could practice their religion without harassment.
Utah was the source of many pioneer settlements located elsewhere in the West. Salt Lake City was the hub of a "far-flung commonwealth" of Mormon settlements. Fed by a continuing supply of church converts coming from the East and around the world, Church leaders often assigned groups of church members to establish settlements throughout the West. Beginning with settlements along Utah's Wasatch front (Salt Lake City, Bountiful and Weber Valley, and Provo and Utah Valley), irrigation enabled the establishment of fairly large pioneer populations in an area that Jim Bridger had advised Young would be inhospitable for the cultivation of crops because of frost. Throughout the remainder of the 19th century, Mormon pioneers called by Brigham Young would leave Salt Lake City and establish hundreds of other settlements in Utah, Idaho, Nevada, Arizona, Wyoming, California, Canada, and Mexico – including in Las Vegas, Nevada; Franklin, Idaho (the first white settlement in Idaho); San Bernardino, California; Star Valley, Wyoming; and Carson Valley, Nevada.
Prominent settlements in Utah included St. George, Logan, and Manti (where settlers completed the first three temples in Utah, each started after but finished many years before the larger and better known temple built in Salt Lake City was completed in 1893), as well as Parowan, Cedar City, Bluff, Moab, Vernal, Fillmore (which served as the territorial capital between 1850 and 1856), Nephi, Levan, Spanish Fork, Springville, Provo Bench (now Orem), Pleasant Grove, American Fork, Lehi, Sandy, Murray, Jordan, Centerville, Farmington, Huntsville, Kaysville, Grantsville, Tooele, Roy, Brigham City, and many other smaller towns and settlements. Young had an expansionist's view of the territory that he and the Mormon pioneers were settling, calling it Deseret – which according to the Book of Mormon was an ancient word for "honeybee" – hence the beehive which can still be found on the Utah flag, and the state's motto, "Industry."
Utah was Mexican territory when the first pioneers arrived in 1847. Early in the Mexican-American War in late 1846, the United States had captured New Mexico and California, and the whole Southwest became U.S. territory upon the signing of the Treaty of Guadalupe Hidalgo, February 2, 1848. The treaty was ratified by the United States Senate on March 11. Learning that California and New Mexico were applying for statehood, the settlers of the area (originally having planned to petition for territorial status) applied for statehood with an ambitious plan for a State of Deseret.
Utah Territory (1850–1896).
The Utah Territory was much smaller than the proposed state of Deseret, but it still contained all of the present states of Nevada and Utah as well as pieces of modern Wyoming and Colorado. It was created with the Compromise of 1850, and Fillmore, named after President Millard Fillmore, was designated the capital. The territory was given the name Utah after the Ute tribe of Native Americans. Salt Lake City replaced Fillmore as the territorial capital in 1856.
Disputes between the Mormon inhabitants and the U.S. government intensified due to prejudice against The Church of Jesus Christ of Latter-day Saints in the Northeast and the practice of plural marriage, or polygamy, among its members. The Mormons were still pushing for the establishment of a State of Deseret with the new borders of the Utah Territory. Most, if not all of the members of the U.S. government opposed the polygamous practices of the Mormons.
Members of the LDS Church were viewed as un-American and rebellious when news of their polygamous practices spread. In 1857, particularly heinous accusations of abdication of government and general immorality were stated by former associate justice William W. Drummond, among others. The detailed reports of life in Utah caused the administration of James Buchanan to send a secret military "expedition" to Utah. When the supposed rebellion should be quelled, Alfred Cumming would take the place of Brigham Young as territorial governor. The resulting conflict is known as the Utah War, nicknamed "Buchanan's Blunder" by the Mormon leaders.
In September 1857, about 120 American settlers of the Baker–Fancher wagon train, en route to California from Arkansas, were murdered by Utah Territorial Militia and some Paiute Native Americans in the Mountain Meadows massacre.
Before troops led by Albert Sidney Johnston entered the territory, Brigham Young ordered all residents of Salt Lake City to evacuate southward to Utah Valley and sent out a force, known as the Nauvoo Legion, to delay the government's advance. Although wagons and supplies were burned, eventually the troops arrived in 1858, and Young surrendered official control to Cumming, although most subsequent commentators claim that Young retained true power in the territory. A steady stream of governors appointed by the president quit the position, often citing the traditions of their supposed territorial government. By agreement with Young, Johnston established Camp Floyd, 40 mi away from Salt Lake City, to the southwest.
Salt Lake City was the last link of the First Transcontinental Telegraph, completed in October 1861. Brigham Young was among the first to send a message, along with Abraham Lincoln and other officials.
Because of the American Civil War, federal troops were pulled out of Utah Territory in 1861. This was a boon to the local economy as the army sold everything in camp for pennies on the dollar before marching back east to join the war. The territory was then left in LDS hands until Patrick E. Connor arrived with a regiment of California volunteers in 1862. Connor established Fort Douglas just 3 mi east of Salt Lake City and encouraged his people to discover mineral deposits to bring more non-Mormons into the territory. Minerals were discovered in Tooele County and miners began to flock to the territory.
Beginning in 1865, Utah's Black Hawk War developed into the deadliest conflict in the territory's history. Chief Antonga Black Hawk died in 1870, but fights continued to break out until additional federal troops were sent in to suppress the Ghost Dance of 1872. The war is unique among Indian Wars because it was a three-way conflict, with mounted Timpanogos Utes led by Antonga Black Hawk fighting federal and LDS authorities.
On May 10, 1869, the First Transcontinental Railroad was completed at Promontory Summit, north of the Great Salt Lake. The railroad brought increasing numbers of people into the territory and several influential businesspeople made fortunes there.
During the 1870s and 1880s laws were passed to punish polygamists due, in part, to the stories coming forth regarding Utah. Notably, Ann Eliza Young—tenth wife to divorce Brigham Young, women's advocate, national lecturer and author of "Wife No. 19 or My Life of Bondage" and Mr. and Mrs. Fanny Stenhouse, authors of "The Rocky Mountain Saints" (T. B. H. Stenhouse, 1873) and "Tell It All: My Life in Mormonism" (Fanny Stenhouse, 1875) . Both of these women, Ann Eliza and Fanny, testify to the happiness of the very early Church members before polygamy began to be practiced. They independently published their books in 1875. These books and the lectures of Ann Eliza Young have been credited with the United States Congress passage of anti-polygamy laws by newspapers throughout the United States as recorded in "The Ann Eliza Young Vindicator", a pamphlet which detailed Ms Young's travels and warm reception throughout her lecture tour.
T. B. H. Stenhouse, former Utah Mormon polygamist, Mormon missionary for thirteen years and a Salt Lake City newspaper owner, finally left Utah and wrote "The Rocky Mountain Saints". His book gives a witnessed account of his life in Utah, both the good and the bad. He finally left Utah and Mormonism after financial ruin occurred when Brigham Young sent Stenhouse to relocate to Ogden, Utah, according to Stenhouse, to take over his thriving pro-Mormon "Salt Lake Telegraph" newspaper. In addition to these testimonies, "The Confessions of John D. Lee", written by John D. Lee—alleged "Scape goat" for the Mountain Meadow Massacre—also came out in 1877. The corroborative testimonies coming out of Utah from Mormons and former Mormons had an impact on Congress and the people of the United States.
In the 1890 Manifesto, the LDS Church banned polygamy. When Utah applied for statehood again, it was accepted. One of the conditions for granting Utah statehood was that a ban on polygamy be written into the state constitution. This was a condition required of other western states that were admitted into the Union later. Statehood was officially granted on January 4, 1896.
20th century.
Beginning in the early 20th century, with the establishment of such national parks as Bryce Canyon National Park and Zion National Park, Utah became known for its natural beauty. Southern Utah became a popular filming spot for arid, rugged scenes, and such natural landmarks as Delicate Arch and "the Mittens" of Monument Valley are instantly recognizable to most national residents. During the 1950s, '60s, and '70s, with the construction of the Interstate highway system, accessibility to the southern scenic areas was made easier.
Beginning in 1939, with the establishment of Alta Ski Area, Utah's skiing has become world-renowned. The dry, powdery snow of the Wasatch Range is considered some of the best skiing in the world (thus the license plate, "the Greatest Snow on Earth"). Salt Lake City won the bid for the 2002 Winter Olympic Games in 1995, and this has served as a great boost to the economy. The ski resorts have increased in popularity, and many of the Olympic venues scattered across the Wasatch Front continue to be used for sporting events. This also spurred the development of the light-rail system in the Salt Lake Valley, known as TRAX, and the re-construction of the freeway system around the city.
In 1957, Utah created the Utah State Parks Commission with just four parks. Today, Utah State Parks manages 43 parks and several undeveloped areas totaling over 95000 acre of land and more than 1000000 acre of water. Utah's state parks are scattered throughout Utah; from Bear Lake State Park at the Utah/Idaho border to Edge of the Cedars State Park Museum deep in the Four Corners region, and everywhere in between. Utah State Parks is also home to the state's off highway vehicle office, state boating office and the trails program.
During the late 20th century, the state grew quickly. In the 1970s growth was phenomenal in the suburbs of the Wasatch Front. Sandy was one of the fastest-growing cities in the country at that time. Today, many areas of Utah are seeing phenomenal growth. Northern Davis, southern and western Salt Lake, Summit, eastern Tooele, Utah, Wasatch, and Washington counties are all growing very quickly. Transportation and urbanization are major issues in politics as development consumes agricultural land and wilderness areas.
Geography.
Utah is known for its natural diversity and is home to features ranging from arid deserts with sand dunes to thriving pine forests in mountain valleys. It is a rugged and geographically diverse state that is located at the convergence of three distinct geological regions: the Rocky Mountains, the Great Basin, and the Colorado Plateau.
Utah is one of the Four Corners states, and is bordered by Idaho in the north, Wyoming in the north and east; by Colorado in the east; at a single point by New Mexico to the southeast; by Arizona in the south; and by Nevada in the west. It covers an area of 84899 sqmi. The state is one of only three U.S. states (with Colorado and Wyoming) that have only lines of latitude and longitude for boundaries.
One of Utah's defining characteristics is the variety of its terrain. Running down the middle of the northern third of the state is the Wasatch Range, which rises to heights of almost 12000 ft above sea level. Utah is home to world-renowned ski resorts, made popular by the light, fluffy snow, and winter storms which regularly dump 1 to 3 feet of overnight snow accumulation. In the northeastern section of the state, running east to west, are the Uinta Mountains, which rise to heights of over 13,000 ft. The highest point in the state, Kings Peak, at 13,528 ft, lies within the Uinta Mountains.
At the western base of the Wasatch Range is the Wasatch Front, a series of valleys and basins that are home to the most populous parts of the state. It stretches approximately from Brigham City at the north end to Nephi at the south end. Approximately 75 percent of the population of the state live in this corridor, and population growth is rapid.
Western Utah is mostly arid desert with a basin and range topography. Small mountain ranges and rugged terrain punctuate the landscape. The Bonneville Salt Flats are an exception, being comparatively flat as a result of once forming the bed of ancient Lake Bonneville. Great Salt Lake, Utah Lake, Sevier Lake, and Rush Lake are all remnants of this ancient freshwater lake, which once covered most of the eastern Great Basin. West of the Great Salt Lake, stretching to the Nevada border, lies the arid Great Salt Lake Desert. One exception to this aridity is Snake Valley, which is (relatively) lush due to large springs and wetlands fed from groundwater derived from snow melt in the Snake Range, Deep Creek Range, and other tall mountains to the west of Snake Valley. Great Basin National Park is just over the Nevada state line in the southern Snake Range. One of western Utah's most impressive, but least visited attractions is Notch Peak, the tallest limestone cliff in North America, located west of Delta.
Much of the scenic southern and southeastern landscape (specifically the Colorado Plateau region) is sandstone, specifically Kayenta sandstone and Navajo sandstone. The Colorado River and its tributaries wind their way through the sandstone, creating some of the world's most striking and wild terrain (the area around the confluence of the Colorado and Green Rivers was the last to be mapped in the lower 48 United States). Wind and rain have also sculpted the soft sandstone over millions of years. Canyons, gullies, arches, pinnacles, buttes, bluffs, and mesas are the common sight throughout south-central and southeast Utah.
This terrain is the central feature of protected state and federal parks such as Arches, Bryce Canyon, Canyonlands, Capitol Reef, and Zion national parks, Cedar Breaks, Grand Staircase-Escalante, Hovenweep, and Natural Bridges national monuments, Glen Canyon National Recreation Area (site of the popular tourist destination, Lake Powell), Dead Horse Point and Goblin Valley state parks, and Monument Valley. The Navajo Nation also extends into southeastern Utah. Southeastern Utah is also punctuated by the remote, but lofty La Sal, Abajo, and Henry mountain ranges.
Eastern (northern quarter) Utah is a high-elevation area covered mostly by plateaus and basins, particularly the Tavaputs Plateau and San Rafael Swell, which remain mostly inaccessible, and the Uinta Basin, where the majority of eastern Utah's population lives. Economies are dominated by mining, oil shale, oil, and natural gas-drilling, ranching, and recreation. Much of eastern Utah is part of the Uintah and Ouray Indian Reservation. The most popular destination within northeastern Utah is Dinosaur National Monument near Vernal.
Southwestern Utah is the lowest and hottest spot in Utah. It is known as Utah's Dixie because early settlers were able to grow some cotton there. Beaverdam Wash in far southwestern Utah is the lowest point in the state, at 2,000 ft. The northernmost portion of the Mojave Desert is also located in this area. Dixie is quickly becoming a popular recreational and retirement destination, and the population is growing rapidly. Although the Wasatch Mountains end at Mount Nebo near Nephi, a complex series of mountain ranges extends south from the southern end of the range down the spine of Utah. Just north of Dixie and east of Cedar City is the state's highest ski resort, Brian Head.
Like most of the western and southwestern states, the federal government owns much of the land in Utah. Over 70 percent of the land is either BLM land, Utah State Trustland, or U.S. National Forest, U.S. National Park, U.S. National Monument, National Recreation Area or U.S. Wilderness Area. Utah is the only state where every county contains some national forest.
Climate.
Utah features a dry, semi-arid to desert climate, although its many mountains feature a large variety of climates, with the highest points in the Uinta Mountains being above the timberline. The dry weather is a result of the state's location in the rain shadow of the Sierra Nevada in California. The eastern half of the state lies in the rain shadow of the Wasatch Mountains. The primary source of precipitation for the state is the Pacific Ocean, with the state usually lying in the path of large Pacific storms from October to May. In summer, the state, especially southern and eastern Utah, lies in the path of monsoon moisture from the Gulf of California.
Most of the lowland areas receive less than 12 in of precipitation annually, although the I-15 corridor, including the densely populated Wasatch Front, receives approximately 15 in. The Great Salt Lake Desert is the driest area of the state, with less than 5 in. Snowfall is common in all but the far southern valleys. Although St. George only receives about 3 in per year, Salt Lake City sees about 60 in, enhanced by the lake-effect snow from the Great Salt Lake, which increases snowfall totals to the south, southeast, and east of the lake.
Some areas of the Wasatch Range in the path of the lake-effect receive up to 500 in per year. The consistently deep powder snow led Utah's ski industry to adopt the slogan "the Greatest Snow on Earth" in the 1980s. In the winter, temperature inversions are a common phenomenon across Utah's low basins and valleys, leading to thick haze and fog that can sometimes last for weeks at a time, especially in the Uintah Basin. Although at other times of year its air quality is good, winter inversions give Salt Lake City some of the worst wintertime pollution in the country.
Utah's temperatures are extreme, with cold temperatures in winter due to its elevation, and very hot summers statewide (with the exception of mountain areas and high mountain valleys). Utah is usually protected from major blasts of cold air by mountains lying north and east of the state, although major Arctic blasts can occasionally reach the state. Average January high temperatures range from around 30 F in some northern valleys to almost 55 F in St. George.
Temperatures dropping below 0 F should be expected on occasion in most areas of the state most years, although some areas see it often (for example, the town of Randolph averages about 50 days per year with temperatures dropping that low). In July, average highs range from about 85 to. However, the low humidity and high elevation typically leads to large temperature variations, leading to cool nights most summer days. The record high temperature in Utah was 118 F, recorded south of St. George on July 4, 2007, and the record low was -69 F, recorded at Peter Sinks in the Bear River Mountains of northern Utah on February 1, 1985. However, the record low for an inhabited location is -49 F at Woodruff on December 12, 1932.
Utah, like most of the western United States, has few days of thunderstorms. On average there are fewer than 40 days of thunderstorm activity during the year, although these storms can be briefly intense when they do occur. They are most likely to occur during monsoon season from about mid-July through mid-September, especially in southern and eastern Utah. Dry lightning strikes and the general dry weather often spark wildfires in summer, while intense thunderstorms can lead to flash flooding, especially in the rugged terrain of southern Utah. Although spring is the wettest season in northern Utah, late summer is the wettest period for much of the south and east of the state. Tornadoes are uncommon in Utah, with an average of two striking the state yearly, rarely higher than EF1 intensity.
One exception of note, however, was the unprecedented F2 Salt Lake City Tornado that moved directly across downtown Salt Lake City on August 11, 1999, killing 1 person, injuring 60 others, and causing approximately $170 million in damage. The only other reported tornado fatality in Utah's history was a 7-year-old girl who was killed while camping in Summit County on July 6, 1884. The last tornado of above (E)F0 intensity occurred on September 8, 2002, when an F2 tornado hit Manti. On August 11, 1993, an F3 tornado hit the Uinta Mountains north of Duchesne at an elevation of 10500 ft, causing some damage to a Boy Scouts campsite. This is the strongest tornado ever recorded in Utah.
Demographics.
The United States Census Bureau estimates that the population of Utah was 2,942,902 on July 1, 2014, a 6.48% increase since the 2010 United States Census. The center of population of Utah is located in Utah County in the city of Lehi. Much of the population lives in cities and towns along the Wasatch Front, a metropolitan region that runs north-south with the Wasatch Mountains rising on the eastern side. Growth outside the Wasatch Front is also increasing. The St. George metropolitan area is currently the second-fastest growing in the country after the Las Vegas metropolitan area, while the Heber micropolitan area is also the second-fastest growing in the country (behind Palm Coast, Florida).
Utah contains 5 metropolitan areas (Logan, Ogden-Clearfield, Salt Lake City, Provo-Orem, and St. George), and 6 micropolitan areas (Brigham City, Heber, Vernal, Price, Richfield, and Cedar City).
Health and fertility.
Utah ranks 47th in teenage pregnancy, lowest in percentage of births out of wedlock, lowest in number of abortions per capita, and lowest in percentage of teen pregnancies terminated in abortion. However, statistics relating to pregnancies and abortions may also be artificially low from teenagers going out of state for abortions because of parental notification requirements. Utah has the lowest child poverty rate in the country, despite its young demographics. According to the Gallup State of Well-Being Report, Utah has the fourth highest well-being in the United States as of 2013. A widely circulated national prescription drug study from 2002 observed that antidepressant drugs were "prescribed in Utah more often than in any other state, at a rate nearly twice the national average"; however, more recent studies by the CDC have shown rates of depression in Utah to be no higher than the national average.
Ancestry and race.
At the 2010 Census, 81.4% of the population was non-Hispanic White, down from 91.2% in 1990, 1% non-Hispanic Black or African American, 1% non-Hispanic American Indian and Alaska Native, 2% non-Hispanic Asian, 0.9% non-Hispanic Native Hawaiian and Other Pacific Islander, 0.1% from some other race (non-Hispanic) and 1.8% of two or more races (non-Hispanic). 13.0% of Utah's population was of Hispanic, Latino, or Spanish origin (of any race).
The largest ancestry groups in the state are:
Most Utahns are of Northern European descent. In 2011 one-third of Utah's workforce was reported to be bilingual, developed through a program of acquisition of second languages beginning in elementary school, and related to Mormonism's missionary goals for its young people.
In 2011, 28.6% of Utah's population younger than the age of one were minorities, meaning that they had at least one parent who was not non-Hispanic white.
Religion.
A majority of the state's residents are members of The Church of Jesus Christ of Latter-day Saints (LDS Church). As of 2012, 62.2% of Utahns are counted as members of The Church of Jesus Christ of Latter-day Saints, although only 41.6% of them are active members. Mormons now make up about 34%–41% of Salt Lake City, while rural and suburban areas tend to be prominently Mormon. The religious body with the largest number of congregations is The Church of Jesus Christ of Latter-day Saints (with 4,815 congregations).
Though the LDS Church officially maintains a policy of neutrality in regards to political parties, the church's doctrine has a strong regional influence on politics. Another doctrine effect can be seen in Utah's high birth rate (25 percent higher than the national average; the highest for a state in the U.S.). The Mormons in Utah tend to have conservative views when it comes to most political issues and the majority of voter-age Utahns are unaffiliated voters (60%) who vote overwhelmingly Republican. Mitt Romney received 72.8% of the Utahn votes in 2012, while John McCain polled 62.5% in the United States presidential election, 2008 and 70.9% for George W. Bush in 2004. In 2010 the Association of Religion Data Archives (ARDA) reported that the three largest denominational groups in Utah are The Church of Jesus Christ of Latter-day Saints with 1,910,504 adherents; the Catholic Church with 160,125 adherents, and the Southern Baptist Convention with 12,593 adherents. There is a growing Jewish presence in the state including Chabad and Rohr Jewish Learning Institute.
According to a report produced by the Pew Forum on Religion & Public Life the self-identified religious affiliations of Utahns over the age of 18 as of 2008 are:
Margin of error +/− 6%
According to results from the 2010 United States Census, Mormons (members of The Church of Jesus Christ of Latter-day Saints) represented 62.1% of Utah's total population. The Utah county with the lowest percentage of Mormons was Grand County, at 26.5%, while the county with the highest percentage was Morgan County, at 86.1%. In addition, the result for the most populated county, Salt Lake County, was 51.4%.
According to a Gallup poll, Utah had the 2nd-highest number of people reporting as "Very Religious" in 2011, at 57% (trailing only Mississippi). However, it also had a higher rate of people reporting as "Nonreligious" (28%) than any of the other "most religious" states, and the smallest percentage of people reporting as "Moderately Religious" (15%) of any state.
Age and gender.
Utah has the highest total birth rate and accordingly, the youngest population of any U.S. state. In 2010, the state's population was 50.2% male and 49.8% female.
Economy.
According to the Bureau of Economic Analysis, the gross state product of Utah in 2012 was $130.5 billion, or 0.87% of the total United States GDP of $14.991 trillion for the same year. The per capita personal income was $45,700 in 2012. Major industries of Utah include: mining, cattle ranching, salt production, and government services.
According to the 2007 State New Economy Index, Utah is ranked the top state in the nation for Economic Dynamism, determined by "the degree to which state economies are knowledge-based, globalized, entrepreneurial, information technology-driven and innovation-based". In 2014, Utah was ranked number one in Forbes' list of "Best States For Business". A November 2010 article in "Newsweek" highlighted Utah and particularly the Salt Lake City area's economic outlook, calling it "the new economic Zion", and examined how the area has been able to bring in high-paying jobs and attract high-tech corporations to the area during a recession. s of September 2014[ [update]], the state's unemployment rate was 3.5%. In terms of "small business friendliness", in 2014 Utah emerged as number one, based on a study drawing upon data from over 12,000 small business owners.
In eastern Utah petroleum production is a major industry. Near Salt Lake City, petroleum refining is done by a number of oil companies. In central Utah, coal production accounts for much of the mining activity.
A 2009 study published in the "Journal of Economic Perspectives" found that Utah residents were the largest consumers of paid internet pornography per capita in the United States. However Edelman, the study's author, came to this conclusion after looking at subscriptions for just one top-10 seller of online adult entertainment, comparing ZIP codes associated with all credit card subscriptions between 2006 and 2008. The author is also quick to admit that the difference in usage between states is relatively small. Another report found that Utah was not significantly higher than other states in regards to pornographic Google search terms; and in regards to the most common search term, was ranked last.
According to Internal Revenue Service tax returns, Utahns rank first among all U.S. states in the proportion of income given to charity by the wealthy. This is due to the standard 10% of all earnings that Mormons give to the LDS Church. According to the Corporation for National and Community Service, Utah had an average of 884,000 volunteers between 2008 and 2010, each of whom contributed 89.2 hours per volunteer. This figure equates to $3.8 billion of service contributed, ranking Utah number one for volunteerism in the nation.
Taxation.
Utah collects personal income tax; since 2008 the tax has been a flat 5 percent for all taxpayers. The state sales tax has a base rate of 6.45 percent, with cities and counties levying additional local sales taxes that vary among the municipalities. Property taxes are assessed and collected locally. Utah does not charge intangible property taxes and does not impose an inheritance tax.
Tourism.
Tourism is a major industry in Utah. With five national parks (Arches, Bryce Canyon, Canyonlands, Capitol Reef, and Zion), Utah has the third most national parks of any state after Alaska and California. In addition, Utah features seven national monuments (Cedar Breaks, Dinosaur, Grand Staircase-Escalante, Hovenweep, Natural Bridges, Rainbow Bridge, and Timpanogos Cave), two national recreation areas (Flaming Gorge and Glen Canyon), seven national forests (Ashley, Caribou-Targhee, Dixie, Fishlake, Manti-La Sal, Sawtooth, and Uinta-Wasatch-Cache), and numerous state parks and monuments.
The Moab area, in the southeastern part of the state, is known for its challenging mountain biking trails, including Slickrock. Moab also hosts the famous Moab Jeep Safari semiannually.
Utah has seen an increase in tourism since the 2002 Winter Olympics. Park City is home to the United States Ski Team. Utah's ski resorts are primarily located in northern Utah near Salt Lake City, Park City, Ogden, and Provo. Between 2007 and 2011 Deer Valley in Park City, has been ranked the top ski resort in North America in a survey organized by "Ski Magazine".
In addition to having prime snow conditions and world-class amenities, Northern Utah's ski resorts are well liked among tourists for their convenience and proximity to a large city and international airport, as well as the close proximity to other ski resorts, allowing skiers the ability to ski at multiple locations in one day. The 2009 Ski Magazine reader survey concluded that six out of the top ten resorts deemed most "accessible" and six out of the top ten with the best snow conditions were located in Utah. In Southern Utah, Brian Head Ski Resort is located in the mountains near Cedar City. Former Olympic venues including Utah Olympic Park and Utah Olympic Oval are still in operation for training and competition and allows the public to participate in numerous activities including ski jumping, bobsleigh, and speed skating.
Utah features many cultural attractions such as Temple Square, the Sundance Film Festival, the Red Rock Film Festival, the DOCUTAH Film Festival, and the Utah Shakespearean Festival. Temple Square is ranked as the 16th most visited tourist attraction in the United States by "Forbes" magazine, with over five million annual visitors.
Other attractions include Monument Valley, the Great Salt Lake, the Bonneville Salt Flats, and Lake Powell.
Mining.
Beginning in the late 19th century with the state's mining boom (including the Bingham Canyon Mine, among the world's largest open pit mines), companies attracted large numbers of immigrants with job opportunities. Since the days of the Utah Territory mining has played a major role in Utah's economy. Historical mining towns include Mercur in Tooele County, Silver Reef in Washington County, Eureka in Juab County, Park City in Summit County and numerous coal mining camps throughout Carbon County such as Castle Gate, Spring Canyon, and Hiawatha.
These settlements were characteristic of the boom and bust cycle that dominated mining towns of the American West. During the early part of the Cold War era, uranium was mined in eastern Utah. Today mining activity still plays a major role in the state's economy. Minerals mined in Utah include copper, gold, silver, molybdenum, zinc, lead, and beryllium. Fossil fuels including coal, petroleum, and natural gas continue to play a large role in Utah's economy, especially in the eastern part of the state in counties such as Carbon, Emery, Grand, and Uintah.
Incidents.
In 2007, nine people were killed at the Crandall Canyon Mine collapse.
On March 22, 2013, one miner died and another was injured after they became trapped in a cave-in at a part of the Castle Valley Mining Complex, about 10 miles west of the small mining town of Huntington in Emery County.
Energy.
Source:
Potential to use renewable energy sources.
Utah has the potential to generate 31.6 TWh/year from 13.1 GW of wind power, and 10,290 TWh/year from solar power using 4,048 GW of photovoltaic (PV), including 5.6 GW of rooftop photovoltaic, and 1,638 GW of concentrated solar power.
Transportation.
I-15 and I-80 are the main interstate highways in the state, where they intersect and briefly merge near downtown Salt Lake City. I-15 traverses the state north-to-south, entering from Arizona near St. George, paralleling the Wasatch Front, and crossing into Idaho near Portage. I-80 spans northern Utah east-to-west, entering from Nevada at Wendover, crossing the Wasatch Mountains east of Salt Lake City, and entering Wyoming near Evanston. I-84 West enters from Idaho near Snowville (from Boise) and merges with I-15 from Tremonton to Ogden, then heads southeast through the Wasatch Mountains before terminating at I-80 near Echo Junction.
I-70 splits from I-15 at Cove Fort in central Utah and heads east through mountains and rugged desert terrain, providing quick access to the many national parks and national monuments of southern Utah, and has been noted for its beauty. The 103-mile (163 km) stretch from Salina to Green River is the longest stretch of interstate in the country without services and, when completed in 1970, was the longest stretch of entirely new highway constructed in the U.S. since the Alaska Highway was completed in 1943.
TRAX, a light rail system in the Salt Lake Valley, consists of three lines. The Blue Line (formerly Salt Lake/Sandy Line) begins in the suburb of Draper and ends in Downtown Salt Lake City. The Red Line (Mid-Jordan/University Line) begins in the Daybreak Community of South Jordan, a southwestern valley suburb, and ends at the University of Utah. The Green Line begins in West Valley City passes through downtown Salt Lake City and ends at the Salt Lake City International Airport.
The Utah Transit Authority (UTA), which operates TRAX, also operates a bus system that stretches across the Wasatch Front, west into Grantsville, and east into Park City. In addition, UTA provides winter service to the ski resorts east of Salt Lake City, Ogden, and Provo. Several bus companies also provide access to the ski resorts in winter, and local bus companies also serve the cities of Cedar City, Logan, Park City, and St. George. A commuter rail line known as "FrontRunner", also operated by UTA, runs between Pleasant View and Provo via Salt Lake City. Amtrak's "California Zephyr", with one train in each direction daily, runs east-west through Utah with stops in Green River, Helper, Provo, and Salt Lake City.
The cities of Logan, Hyrum, Smithfield, Richmond, in addition to nearby Preston, Idaho, are served by a local sales-tax-funded zero-fare bus system called the Cache Valley Transit District (CVTD). It is the only system-wide free public transit system in the state.
Salt Lake City International Airport is the only international airport in the state and serves as one of the hubs for Delta Air Lines. The airport has consistently ranked first in on-time departures and had the fewest cancellations among U.S. airports. The airport has non-stop service to over 100 destinations throughout the United States, Canada, and Mexico, as well as to Paris and Tokyo. Canyonlands Field (near Moab), Cedar City Regional Airport, Ogden-Hinckley Airport, Provo Municipal Airport, St. George Municipal Airport, and Vernal Regional Airport all provide limited commercial air service. An entirely new regional airport at St. George opened on January 12, 2011, replacing the old airport that existed on top of a plateau and had no room for expansion. SkyWest Airlines is also headquartered in St. George and maintains a hub at Salt Lake City. The airlines and service from Provo have seem to remain in a state of change since commercial service began in 2011.
Law and government.
Utah government, like most U.S. states, is divided into three branches: executive, legislative, and judicial. The current governor of Utah is Gary Herbert, who was sworn in on August 11, 2009. The governor is elected for a four-year term. The Utah State Legislature consists of a Senate and a House of Representatives. State senators serve four-year terms and representatives two-year terms. The Utah Legislature meets each year in January for an annual forty-five-day session.
The Utah Supreme Court is the court of last resort in Utah. It consists of five justices, who are appointed by the governor, and then subject to retention election. The Utah Court of Appeals handles cases from the trial courts. Trial level courts are the district courts and justice courts. All justices and judges, like those on the Utah Supreme Court, are subject to retention election after appointment.
Counties.
Utah is divided into political jurisdictions designated as "counties". Since 1918 there have been 29 counties in the state, ranging from 298 to.
Women's rights.
Utah granted full voting rights to women in 1870, 26 years before becoming a state. Among all U.S. states, only Wyoming granted suffrage to women earlier. However, in 1887 the initial Edmunds-Tucker Act was passed by Congress in an effort to curtail excessive Mormon influence in the territorial government. One of the provisions of the Act was the repeal of women's suffrage; full suffrage was not returned until Utah was admitted to the Union in 1896.
Utah is one of the 15 states that have not ratified the U.S. Equal Rights Amendment.
Constitution.
The constitution of Utah was enacted in 1895. Notably, the constitution outlawed polygamy, as requested by Congress when Utah had applied for statehood, and reestablished the territorial practice of women's suffrage. Utah's Constitution has been amended many times since its inception.
Alcohol, tobacco and gambling laws.
Utah's laws in regard to alcohol, tobacco and gambling are strict. Utah is an alcoholic beverage control state. The Utah Department of Alcoholic Beverage Control regulates the sale of alcohol; wine and spirituous liquors may only be purchased at state liquor stores, and local laws may prohibit the sale of beer and other alcoholic beverages on Sundays. The state bans the sale of fruity alcoholic drinks at grocery stores and convenience stores. The law states that such drinks must now have new state-approved labels on the front of the products that contain capitalized letters in bold type telling consumers the drinks contain alcohol and at what percentage. The Utah Indoor Clean Air Act is a statewide smoking ban, that prohibits smoking in many public places. Utah is one of few states to set a smoking age of 19, as opposed to 18, as in most other states.
Utah is also one of only two states in the United States to outlaw all forms of gambling; the other is Hawaii.
Same-sex marriage.
Same-sex marriage became legal in Utah on December 20, 2013 when judge Robert J. Shelby of the United States District Court for the District of Utah issued a ruling in "Kitchen v. Herbert." As of close of business December 26, more than 1,225 marriage licenses were issued, with at least 74 percent, or 905 licenses, issued to gay and lesbian couples. Utah's counties generated more than $49,000 in a three-and-a-half-day period from licenses, which cost $30–$50. The state Attorney General's office was granted a stay of the ruling by the United States Supreme Court on January 6, 2014 while the Tenth Circuit Court of Appeals considers the case. On Monday October 6, 2014, the Supreme Court of the United States declined a Writ of Certiorari, and the 10th Circuit Court issued their mandate later that day, lifting their stay. Same-sex marriages commenced again in Utah that day.
Politics.
In the late 19th century, the federal government took issue with polygamy in the LDS Church. The LDS Church discontinued plural marriage in 1890, and in 1896 Utah gained admission to the Union. Many new people settled the area soon after the Mormon pioneers. Relations have often been strained between the LDS population and the non-LDS population. These tensions have played a large part in Utah's history (Liberal Party vs. People's Party).
Utah was the single most Republican-leaning state in the country in every presidential election from 1976 to 2004, measured by the percentage point margin between the Republican and Democratic candidates. In 2008 Utah was only the third-most Republican state (after Wyoming and Oklahoma), but in 2012, with Mormon Mitt Romney atop the Republican ticket, Utah returned to its position as the most Republican state.
Both of Utah's U.S. Senators, Orrin Hatch and Mike Lee, are Republican. Four more Republicans, Rob Bishop, Chris Stewart, Jason Chaffetz, and Mia Love, represent Utah in the United States House of Representatives. After Jon Huntsman, Jr., resigned to serve as U.S. Ambassador to China, Gary Herbert was sworn in as governor on August 11, 2009. Herbert was elected to serve out the remainder of the term in a special election in 2010, defeating Democratic nominee Salt Lake County Mayor Peter Corroon with 64% of the vote. He won election to a full four-year term in 2012, defeating Democratic Businessman Peter Cooke with 68% of the vote.
The LDS Church maintains an official policy of neutrality with regard to political parties and candidates.
Utah votes predominately Republican. Self-identified Latter-day Saints are more likely to vote for the Republican ticket than non-Mormons, and Utah is one of the most Republican states in the nation.
In the 1970s, then-Apostle Ezra Taft Benson was quoted by the Associated Press that it would be difficult for a faithful Latter-day Saint to be a liberal Democrat. Although the LDS Church has officially repudiated such statements on many occasions, Democratic candidates—including LDS Democrats—believe that Republicans capitalize on the perception that the Republican Party is doctrinally superior. Political scientist and pollster Dan Jones explains this disparity by noting that the national Democratic Party is associated with liberal positions on gay marriage and abortion, both of which the LDS Church is against. The Republican Party in heavily Mormon Utah County presents itself as the superior choice for Latter-day Saints. Even though Utah Democratic candidates are predominantly LDS, socially conservative, and pro-life, no Democrat has won in Utah County since 1994.
David Magleby, dean of Social and Behavioral Sciences at Brigham Young University, a lifelong Democrat and a political analyst, asserts that the Republican Party actually has more conservative positions than the LDS Church. Magleby argues that the locally conservative Democrats are in better accord with LDS doctrine. For example, the Republican Party of Utah opposes almost all abortions while Utah Democrats take a more liberal approach, although more conservative than their national counterparts. On Second Amendment issues, the state GOP has been at odds with the LDS Church position opposing concealed firearms in places of worship and in public spaces.
In 1998 the church expressed concern that Utahns perceived the Republican Party as an LDS institution and authorized lifelong Democrat and Seventy Marlin Jensen to promote LDS bipartisanship.
Utah is much more conservative than the United States as a whole, particularly on social issues. Compared to other Republican-dominated states in the Mountain West such as Wyoming, Utah politics have a more moralistic and less libertarian character according to David Magleby.
About 80% of Utah's Legislature are members of The Church of Jesus Christ of Latter-day Saints, while they account for 61 percent of the population. Since becoming a state in 1896, Utah has had only two non-Mormon governors.
In 2006, the legislature passed legislation aimed at banning joint-custody for a non-biological parent of a child. The custody measure passed the legislature and was vetoed by the governor, a reciprocal benefits supporter.
Carbon County's Democrats are generally made up of members of the large Greek, Italian, and Southeastern European communities, whose ancestors migrated in the early 20th century to work in the extensive mining industry. The views common amongst this group are heavily influenced by labor politics, particularly of the New Deal Era.
The Democrats of Summit County are the by-product of the migration of wealthy families from California in the 1990s to the ski resort town of Park City; their views are generally supportive of the economic policies favored by unions and the social policies favored by the liberals.
The state's most Republican areas tend to be Utah County, which is the home to Brigham Young University in the city of Provo, and nearly all the rural counties. These areas generally hold socially conservative views in line with that of the national Religious Right.
The state has not voted for a Democrat for president since 1964. Historically, Republican presidential nominees score one of their best margins of victory here. Utah was the Republicans' best state in the 1976, 1980, 1984, 1988, 1996, 2000, and 2004 elections. In 1992, Utah was the only state in the nation where Democratic candidate Bill Clinton finished behind both Republican candidate George HW Bush and Independent candidate Ross Perot. In 2004, Republican George W. Bush won every county in the state and Utah gave him his largest margin of victory of any state. He won the state's five electoral votes by a margin of 46 percentage points with 71.5% of the vote. In the 1996 Presidential elections the Republican candidate received a smaller 54% of the vote while the Democrat earned 34%.
Major cities and towns.
Utah's population is concentrated in two areas, the Wasatch Front in the north-central part of the state, with a population of over 2 million; and Washington County, in southwestern Utah, locally known as "Dixie", with over 150,000 residents in the metropolitan area.
According the 2010 Census, Utah was the second-fastest growing state (at 23.8 percent) in the United States between 2000 and 2010 (behind Nevada). St. George, in the southwest, is the second-fastest growing metropolitan area in the United States, trailing Greeley, Colorado.
The three fastest-growing counties from 2000 to 2010 were Wasatch County (54.7%), Washington County (52.9%), and Tooele County (42.9%). However, Utah County added the most people (148,028). Between 2000 and 2010, Saratoga Springs (1,673%), Herriman (1,330%), Eagle Mountain (893%), Cedar Hills (217%), South Willard (168%), Nibley (166%), Syracuse (159%), West Haven (158%), Lehi (149%), Washington (129%), and Stansbury Park (116%) all at least doubled in population. West Jordan (35,376), Lehi (28,379), St. George (23,234), South Jordan (20,981), West Valley City (20,584), and Herriman (20,262) all added at least 20,000 people.
Sports.
Utah is the least populous U.S. state to have a major professional sports league franchise. The Utah Jazz of the National Basketball Association play at EnergySolutions Arena in Salt Lake City. The team moved to the city from New Orleans in 1979 and has been one of the most consistently successful teams in the league (although they have yet to win a championship). Salt Lake City was previously host to the Utah Stars, who competed in the ABA from 1970–76 and won 1 championship, and to the Utah Starzz of the WNBA from 1997 to 2003.
Real Salt Lake of Major League Soccer were founded in 2005 and play their home matches at Rio Tinto Stadium in Sandy. The team has enjoyed several years of success as the Utah outpost of the world's most popular sport. RSL remain the only Utah major league sports team to have won a major league national championship, having won the MLS Cup in 2009.
The Utah Blaze began play in the original AFL in 2006 that folded before the 2009 season, then returned to play when the league was re-founded in 2010. They compete at the Maverik Center in West Valley City.
Utah's highest level minor league baseball team is the Salt Lake Bees, who play at Smith's Ballpark in Salt Lake City and are part of the AAA level Pacific Coast League, one notch below Major League Baseball. Utah also has one minor league hockey team, the Utah Grizzlies, who play at the Maverik Center and compete in the ECHL (the third tier of U.S. hockey).
Utah has six universities that compete in Division I of the NCAA. Three of the schools have football programs that participate in the top-level Football Bowl Subdivision: Utah in the Pacific-12 Conference, Utah State in the Mountain West Conference, and BYU as an independent. Two more schools participate in FCS football: Weber State and Southern Utah (SUU) in the Big Sky Conference. Utah Valley, which has no football program, is a full member of the Great West Conference.
Salt Lake City hosted the 2002 Winter Olympics. After early financial struggles and scandal, the 2002 Olympics eventually became among the most successful Winter Olympics in history from a marketing and financial standpoint. Watched by over 2 billion viewers, the Games ended up with a profit of 40 million dollars.
Utah has hosted professional golf tournaments such as the Uniting Fore Care Classic and currently the Utah Championship.
Rugby has been growing quickly in the state of Utah, growing from 17 teams in 2009 to 70 teams as of 2013, including with over 3,000 players, and more than 55 high school varsity teams. The growth has been inspired in part by the 2008 movie Forever Strong. Utah fields two of the most competitive teams in the nation in college rugby — BYU and Utah.
Branding.
The state of Utah relies heavily on income from tourists and travelers taking advantage of the state's ski resorts and natural beauty, and thus the need to "brand" Utah and create an impression of the state throughout the world has led to several state slogans, the most famous of which being "The Greatest Snow on Earth", which has been in use in Utah officially since 1975 (although the slogan was in unofficial use as early as 1962) and now adorns nearly 50 percent of the state's license plates. In 2001, Utah Governor Mike Leavitt approved a new state slogan, "Utah! Where Ideas Connect", which lasted until March 10, 2006, when the Utah Travel Council and the office of Governor Jon Huntsman announced that "Life Elevated" would be the new state slogan.
Entertainment.
Utah is the setting of or the filming location for many books, films, television series, music videos, and video games. A selective list of each appears below.
Film.
"See "
Utah's Monument Valley has been location to several productions, such as "127 Hours", Tim Burton's "Planet of the Apes", and "Butch Cassidy and the Sundance Kid".
References.
 "This article incorporates public domain material from the website of the ".

</doc>
<doc id="31740" url="http://en.wikipedia.org/wiki?curid=31740" title="University of Michigan">
University of Michigan

The University of Michigan (UM, U-M, UMich, or U of M), frequently referred to simply as Michigan, is a public research university located in Ann Arbor, Michigan, United States. Originally, founded in 1817 in Detroit as the "Catholepistemiad", or University of Michigania, 20 years before the Michigan Territory officially became a state, the University of Michigan is the state's oldest university. The university moved to Ann Arbor in 1837 onto 40 acre of what is now known as Central Campus. Since its establishment in Ann Arbor, the university campus has expanded to include more than 584 major buildings with a combined area of more than 34 million gross square feet (781 acres or 3.16 km²), and has two satellite campuses located in Flint and Dearborn. The University was one of the founding members of the Association of American Universities.
Considered one of the foremost research universities in the United States, the university has very high research activity and its comprehensive graduate program offers doctoral degrees in the humanities, social sciences, and STEM fields (Science, Technology, Engineering and Mathematics) as well as professional degrees in medicine, law, pharmacy, nursing, social work and dentistry. Michigan's body of living alumni (as of 2012) comprises more than 500,000. Besides academic life, Michigan's athletic teams compete in Division I of the NCAA and are collectively known as the Wolverines. They are members of the Big Ten Conference.
History.
The University of Michigan was established in Detroit in 1817 as the "Catholepistemiad", or University of Michigania, by the governor and judges of Michigan Territory. The Rev. John Monteith was one of the university's founders and its first President. Ann Arbor had set aside 40 acre in the hopes of being selected as the state capital; when Lansing was chosen as the state capital, the city offered the land for a university. What would become the university moved to Ann Arbor in 1837 thanks to governor Stevens T. Mason. The original 40 acre was the basis of the current Central Campus. The first classes in Ann Arbor were held in 1841, with six freshmen and a sophomore, taught by two professors. Eleven students graduated in the first commencement in 1845. 
By 1866, enrollment increased to 1,205 students, many of whom were Civil War veterans. Women were first admitted in 1870. James Burrill Angell, who served as the university's president from 1871 to 1909, aggressively expanded U-M's curriculum to include professional studies in dentistry, architecture, engineering, government, and medicine. U-M also became the first American university to use the seminar method of study. Among the early students in the School of Medicine was Jose Celso Barbosa, who in 1880 graduated as valedictorian and the first Puerto Rican to get a university degree in the United States. He returned to Puerto Rico to practice medicine and also served in high-ranking posts in the government.
From 1900 to 1920, the university constructed many new facilities, including buildings for the dental and pharmacy programs, chemistry, natural sciences, Hill Auditorium, large hospital and library complexes, and two residence halls. In 1920 the university reorganized the College of Engineering and formed an advisory committee of 100 industrialists to guide academic research initiatives. The university became a favored choice for bright Jewish students from New York in the 1920s and 1930s, when the Ivy League schools had quotas restricting the number of Jews to be admitted. Because of its high standards, U-M gained the nickname "Harvard of the West," which became commonly parodied in reverse after John F. Kennedy referred to himself as "a graduate of the Michigan of the East, Harvard University" in his speech proposing the formation of the Peace Corps while on the front steps of the Michigan Union. During World War II, U-M's research supported military efforts, such as U.S. Navy projects in proximity fuzes, PT boats, and radar jamming.
After the war, enrollment expanded rapidly and by 1950, it reached 21,000, of which more than one third (or 7,700) were veterans supported by the G.I. Bill. As the Cold War and the Space Race took hold, U-M received numerous government grants for strategic research and helped to develop peacetime uses for nuclear energy. Much of that work, as well as research into alternative energy sources, is pursued via the Memorial Phoenix Project.
Lyndon B. Johnson gave his speech outlining his Great Society program as the lead speaker during U-M's 1964 spring commencement ceremony. During the 1960s, the university campus was the site of numerous protests against the Vietnam War and university administration. On March 24, 1965, a group of U-M faculty members and 3,000 students held the nation's first ever faculty-led "teach-in" to protest against American policy in Southeast Asia. In response to a series of sit-ins in 1966 by "Voice", the campus political party of Students for a Democratic Society, U-M's administration banned sit-ins. In response, 1,500 students participated in a one-hour sit-in inside the LSA Building, which housed administrative offices.
Former U-M student and noted architect Alden B. Dow designed the current Fleming Administration Building, which was completed in 1968. The building's plans were drawn in the early 1960s, before student activism prompted a concern for safety. But the Fleming Building's narrow windows, all located above the first floor, and fortress-like exterior led to a campus rumor that it was designed to be riot-proof. Dow denied those rumors, claiming the small windows were designed to be energy efficient.
During the 1970s, severe budget constraints slowed the university's physical development; but in the 1980s, the university received increased grants for research in the social and physical sciences. The university's involvement in the anti-missile Strategic Defense Initiative and investments in South Africa caused controversy on campus. During the 1980s and 1990s, the university devoted substantial resources to renovating its massive hospital complex and improving the academic facilities on the North Campus. In its 2011 annual financial report, the university announced that it had dedicated $497 million per year in each of the prior 10 years to renovate buildings and infrastructure around the campus. The university also emphasized the development of computer and information technology throughout the campus.
In the early 2000s, U-M faced declining state funding due to state budget shortfalls. At the same time, the university attempted to maintain its high academic standing while keeping tuition costs affordable. There were disputes between U-M's administration and labor unions, notably with the Lecturers' Employees Organization (LEO) and the Graduate Employees Organization (GEO), the union representing graduate student employees. These conflicts led to a series of one-day walkouts by the unions and their supporters. The university is engaged in a $2.5 billion construction campaign.
In 2003, two lawsuits involving U-M's affirmative action admissions policy reached the U.S. Supreme Court ("Grutter v. Bollinger" and "Gratz v. Bollinger"). President George W. Bush publicly opposed the policy before the court issued a ruling. The court found that race may be considered as a factor in university admissions in all public universities and private universities that accept federal funding. But, it ruled that a point system was unconstitutional. In the first case, the court upheld the Law School admissions policy, while in the second it ruled against the university's undergraduate admissions policy.
The debate continues because in November 2006, Michigan voters passed Proposal 2, banning most affirmative action in university admissions. Under that law, race, gender, and national origin can no longer be considered in admissions. U-M and other organizations were granted a stay from implementation of the law soon after that referendum. This has allowed time for proponents of affirmative action to decide legal and constitutional options in response to the initiative results. The university has stated it plans to continue to challenge the ruling; in the meantime, the admissions office states that it will attempt to achieve a diverse student body by looking at other factors, such as whether the student attended a disadvantaged school, and the level of education of the student's parents.
On May 1, 2014, University of Michigan was named one of 55 higher education institutions under investigation by the Office of Civil Rights “for possible violations of federal law over the handling of sexual violence and harassment complaints." President Barack Obama's White House Task Force To Protect Students from Sexual Assault was organized for such investigations.
The University of Michigan became more selective in the early 2010s. The acceptance rate declined from 50.6% in 2010 to 32.2% in 2014. The rate of new freshman enrollment has been fairly stable since 2010.
Campus.
The Ann Arbor campus is divided into four main areas: the North, Central, Medical, and South Campuses. The physical infrastructure includes more than 500 major buildings, with a combined area of more than 34 million square feet or 781 acre. The Central and South Campus areas are contiguous, while the North Campus area is separated from them, primarily by the Huron River. There is also leased space in buildings scattered throughout the city, many occupied by organizations affiliated with the University of Michigan Health System. An East Medical Campus has recently been developed on Plymouth Road, with several university-owned buildings for outpatient care, diagnostics, and outpatient surgery.
In addition to the U-M Golf Course on South Campus, the university operates a second golf course on Geddes Road, Radrick Farms Golf Course. The golf course is only open to faculty, staff, and alumni. Another off-campus facility is the Inglis House, which the university has owned since the 1950s. The Inglis House is a 10,000 sqft mansion used to hold various social events, including meetings of the board of regents, and to host visiting dignitaries. The university also operates a large office building called Wolverine Tower in southern Ann Arbor near Briarwood Mall. Another major facility is the Matthaei Botanical Gardens, which is located on the eastern outskirts of Ann Arbor.
All four campus areas are connected by bus services, the majority of which connect the North and Central Campuses. There is a shuttle service connecting the University Hospital, which lies between North and Central Campuses, with other medical facilities throughout northeastern Ann Arbor.
Central Campus.
Central Campus was the original location of U-M when it moved to Ann Arbor in 1837. It originally had a school and dormitory building (where Mason Hall now stands) and several houses for professors on forty acres of land bounded by North University Avenue, South University Avenue, East University Avenue, and State Street. The President's House, located on South University Avenue, is the oldest building on campus as well as the only surviving building from the original forty acre campus. Because Ann Arbor and Central Campus developed simultaneously, there is no distinct boundary between the city and university, and some areas contain a mixture of private and university buildings. Residence halls located on Central Campus are split up into two groups: the Hill Neighborhood and Central Campus.
Central Campus is the location of the College of Literature, Science and the Arts, and is immediately adjacent to the medical campus. Most of the graduate and professional schools, including the Ross School of Business, the Gerald R. Ford School of Public Policy, the Law School and the School of Dentistry, are on Central Campus. Two prominent libraries, the Harlan Hatcher Graduate Library and the Shapiro Undergraduate Library (which are connected by a skywalk), are also on Central Campus, as well as museums housing collections in archaeology, anthropology, paleontology, zoology, dentistry, and art. Ten of the buildings on Central Campus were designed by Detroit-based architect Albert Kahn between 1904 and 1936. The most notable of the Kahn-designed buildings are the Burton Memorial Tower and nearby Hill Auditorium.
North Campus.
North Campus is the most contiguous campus, built independently from the city on a large plot of farm land—approximately 800 acre—that the university bought in 1952. It is newer than Central Campus, and thus has more modern architecture, whereas most Central Campus buildings are classical or gothic in style. The architect Eero Saarinen, based in Birmingham, Michigan, created one of the early master plans for North Campus and designed several of its buildings in the 1950s, including the Earl V. Moore School of Music Building. North and Central Campuses each have unique bell towers that reflect the predominant architectural styles of their surroundings. Each of the bell towers houses a grand carillon. The North Campus tower is called Lurie Tower. The University of Michigan's largest residence hall, Bursley Hall, is located on North Campus.
North Campus houses the College of Engineering, the School of Music, Theatre & Dance, the School of Art & Design, the Taubman College of Architecture and Urban Planning, and an annex of the School of Information. The campus is served by the Duderstadt Center, which houses the Art, Architecture and Engineering Library. The Duderstadt Center also contains multiple computer labs, video editing studios, electronic music studios, an audio studio, a video studio, multimedia workspaces, and a 3D virtual reality room. Other libraries located on North Campus include the Gerald R. Ford Presidential Library and the Bentley Historical Library.
South Campus.
South Campus is the site for the athletic programs, including major sports facilities such as Michigan Stadium, Crisler Center, and Yost Ice Arena. South Campus is also the site of the Buhr library storage facility, Revelli Hall, home of the Michigan Marching Band, the Institute for Continuing Legal Education, and the Student Theatre Arts Complex, which provides shop and rehearsal space for student theatre groups. The university's departments of public safety and transportation services offices are located on South Campus.
U-M's golf course is located south of Michigan Stadium and Crisler Arena. It was designed in the late 1920s by Alister MacKenzie, the designer of Augusta National Golf Club in Augusta, Georgia (home of The Masters Tournament). The course opened to the public in the spring of 1931. The University of Michigan Golf Course was included in a listing of top holes designed by what "Sports Illustrated" calls "golf's greatest course architect." The U-M Golf Course's signature No. 6 hole—a 310 yd par 4, which plays from an elevated tee to a two-tiered, kidney-shaped green protected by four bunkers—is the second hole on the Alister MacKenzie Dream 18 as selected by a five-person panel that includes three-time Masters champion Nick Faldo and golf course architect Tom Doak. The listing of "the best holes ever designed by Augusta National architect Alister MacKenzie" is featured in SI's Golf Plus special edition previewing the Masters on April 4, 2006.
Organization and administration.
The University of Michigan consists of a flagship campus in Ann Arbor, with two regional campuses in Dearborn and Flint. The Board of Regents, which governs the university and was established by the Organic Act of March 18, 1837, consists of eight members elected at large in biennial state elections for overlapping eight-year terms. Between the establishment of the University of Michigan in 1837 and 1850, the Board of Regents ran the university directly; although they were, by law, supposed to appoint a Chancellor to administer the university, they never did. Instead a rotating roster of professors carried out the day-to-day administration duties.
The President of the University of Michigan is the principal executive officer of the university. The office was created by the Michigan Constitution of 1850, which also specified that the president was to be appointed by the Regents of the University of Michigan and preside at their meetings, but without a vote. Today, the president's office is at the Ann Arbor campus, and the president has the privilege of living in the President's House, the university's oldest building located on Central Campus in Ann Arbor. Mark Schlissel is the 14th and current president of the university and has served since July 2014.
There are thirteen undergraduate schools and colleges. By enrollment, the three largest undergraduate units are the College of Literature, Science, and the Arts, the College of Engineering, and the Ross School of Business. At the graduate level, the Rackham Graduate School serves as the central administrative unit of graduate education at the university. There are 18 graduate schools and colleges, the largest of which are the College of Literature, Science, and the Arts, the College of Engineering, the Law School, and the Ross School of Business. Professional degrees are conferred by the Schools of Public Health, Dentistry, Law, Medicine, and Pharmacy. The Medical School is partnered with the University of Michigan Health System, which comprises the university's three hospitals, dozens of outpatient clinics, and many centers for medical care, research, and education.
Endowment.
As of March 2014, U-M's financial endowment (the "University Endowment Fund") was valued at $9.47 billion. In 2013, Michigan's endowment was the eighth largest endowment in the U.S. and the third-largest among U.S public universities at that time; it has been the fastest growing endowment in the nation over the last 21 years. The endowment is primarily used according to the donors' wishes, which include the support of teaching and research. In mid-2000, U-M embarked on a massive fund-raising campaign called "The Michigan Difference," which aimed to raise $2.5 billion, with $800 million designated for the permanent endowment. Slated to run through December 2008, the university announced that the campaign had reached its target 19 months early in May 2007. Ultimately, the campaign raised $3.2 billion over 8 years. Over the course of the capital campaign, 191 additional professorships were endowed, bringing the university total to 471 as of 2009. Like nearly all colleges and universities, U-M suffered significant realized and unrealized losses in its endowment during the second half of 2008. In February 2009, a university spokesperson estimated losses of between 20 and 30 percent.
In November 2013, the university launched the "Victors for Michigan" campaign, which with a $4 billion goal, is its largest fundraising campaign to date.
Student government.
Housed in the Michigan Union, the Central Student Government (CSG) is the central student government of the University. With representatives from each of the University's colleges and schools, CSG represents students and manages student funds on the campus. CSG is a 501(c)(3) organization, independent from the University of Michigan. In recent years CSG has organized airBus, a transportation service between campus and the Detroit Metropolitan Wayne County Airport, and has led the university's efforts to register its student population to vote, with its Voice Your Vote Commission (VYV) registering 10,000 students in 2004. VYV also works to improve access to non-partisan voting-related information and increase student voter turnout. CSG was successful at reviving Homecoming activities, including a carnival and parade, for students after a roughly eleven-year absence in October 2007, and during the 2013-14 school year, was instrumental in persuading the University to rescind an unpopular change in student football seating policy at Michigan Stadium.
There are student governance bodies in each college and school. The two largest colleges at the University of Michigan are the College of Literature, Science, and the Arts (LS&A) and the College of Engineering. Undergraduate students in the LS&A are represented by the LS&A Student Government (LSA SG). Engineering Student Government (ESG) manages undergraduate student government affairs for the College of Engineering. Graduate students enrolled in the Rackham Graduate School are represented by the Rackham Student Government (RSG). In addition, the students that live in the residence halls are represented by the University of Michigan Residence Halls Association (RHA).
A longstanding goal of the student government is to create a student-designated seat on the Board of Regents, the university's governing body. Such a designation would achieve parity with other Big Ten schools that have student regents. In 2000, students Nick Waun and Scott Trudeau ran for the board on the statewide ballot as third-party nominees. Waun ran for a second time in 2002, along with Matt Petering and Susan Fawcett. Although none of these campaigns has been successful, a poll conducted by the State of Michigan in 1998 concluded that a majority of Michigan voters would approve of such a position if the measure were put before them. A change to the board's makeup would require amending the Michigan Constitution.
Academics.
The University of Michigan is a large, four-year, residential research university accredited by the North Central Association of Colleges and Schools. The four year, full-time undergraduate program comprises the majority of enrollments and emphasizes instruction in the arts, sciences, and professions and there is a high level of coexistence between graduate and undergraduate programs. The university has "very high" research activity and the "comprehensive" graduate program offers doctoral degrees in the humanities, social sciences, and STEM fields as well as professional degrees in medicine, law, and dentistry. U-M has been included on Richard Moll's list of Public Ivies. With over 200 undergraduate majors, 100 doctoral and 90 master's programs, U-M conferred 6,490 undergraduate degrees, 4,951 graduate degrees, and 709 first professional degrees in 2011-2012.
National honor societies such as Phi Beta Kappa, Phi Kappa Phi, and Tau Beta Pi have chapters at U-M. Degrees "with Highest Distinction" are recommended to students who rank in the top 3% of their class, "with High Distinction" to the next 7%, and "with Distinction" to the next 15%. Students earning a minimum overall GPA of 3.4 who have demonstrated high academic achievement and capacity for independent work may be recommended for a degree "with Highest Honors," "with High Honors," or "with Honors." Those students who earn all A's for two or more consecutive terms in a calendar year are recognized as James B. Angell Scholars and are invited to attend the annual Honors Convocation, an event which recognizes undergraduate students with distinguished academic achievements.
Out-of-state undergraduate students pay between US $36,001.38 and $43,063.38 annually for tuition alone while in-state undergraduate students paid between US $11,837.38 and $16,363.38 annually. U-M provides financial aid in the form of need-based loans, grants, scholarships, work study, and non-need based scholarships, with 77% of undergraduates in 2007 receiving financial aid. For undergraduates in 2008, 46% graduated with about $25,586 of debt. The university is attempting to increase financial aid availability to students by devoting over $1.53 billion in endowment funds to support financial aid.
Student body.
In Fall 2012, the university had an enrollment of 43,625 students: 28,395 undergraduate students, 12,565 academic degree-seeking graduate students, and 2,665 first professional students in a total of 600 academic programs. Of all students, 36,650 (87.4 percent) are U.S. citizens or permanent residents and 5,274 (12.6 percent) are international students. Each year, some 45,000 people apply for freshman admission; just under a third of applicants are admitted and approximately 6,000 new students enroll. Students come from all 50 U.S. states and more than 100 countries. Approximately 95 percent of the university's incoming class of 2013 had an unweighted high school GPA of 3.5 and higher, with the average accepted unweighted GPA being a 3.85. The middle 50 percent of admitted applicants reported an SAT score of 2030-2250 (Critical Reading 650-740, Math 680-780, Writing 660-760) and an ACT score of 30-33. Full-time students make up about 97 percent of the student body. Among full-time students, the university has a first-time student retention rate of 97 percent.
In 2012, undergraduates were enrolled in 12 schools: About 62 percent in the College of Literature, Science, and the Arts; 21 percent in the College of Engineering; 4 percent in the Ross School of Business; 3 percent in the School of Kinesiology; 3 percent in the School of Music, Theatre & Dance; and 2 percent in the School of Nursing. Small numbers of undergraduates were enrolled in the colleges or schools of Art & Design, Architecture & Urban Planning, Dentistry, Education, Pharmacy, and Public Policy. In 2014, the School of Information opened to undergraduates, with the new Bachelor of Science in Information degree. Among undergraduates, 70 percent graduate with a bachelor's degree within four years, 86 percent graduate within five years and 88 percent graduating within six years.
Of the university's 12,714 non-professional graduate students, 5,367 are seeking academic doctorates and 6,821 are seeking master's degrees. The largest number of master's degree students are enrolled in the Ross School of Business (1,812 students seeking MBA or Master of Accounting degrees) and the College of Engineering (1,456 students seeking M.S. or M.Eng. degrees). The largest number of doctoral students are enrolled in the College of Literature, Science, and the Arts (2,076) and College of Engineering (1,496). While the majority of U-M's graduate degree-granting schools and colleges have both undergraduate and graduate students, a few schools only issue graduate degrees. Presently, the School of Natural Resources and Environment, School of Public Health, and School of Social Work only have graduate students.
In Fall 2010, 2,709 Michigan students were enrolled in U-M's professional schools: the School of Dentistry (439 students), Law School (1,182 students), Medical School (802 students), and College of Pharmacy (439 students).
Research.
The university is one of the founding members (1900) of the Association of American Universities. With over 6,200 faculty members, 73 of whom are members of the National Academy and 471 of whom hold an endowed chair in their discipline, the university manages one of the largest annual collegiate research budgets of any university in the United States, totaling about $1 billion in 2009. The Medical School spent the most at over $445 million, while the College of Engineering was second at more than $160 million. U-M also has a technology transfer office, which is the university conduit between laboratory research and corporate commercialization interests. In 2009, the university consummated a deal to purchase a facility formerly owned by Pfizer. The acquisition includes over 170 acre of property, and 30 major buildings comprising roughly 1600000 sqft of wet laboratory space, and 400000 sqft of administrative space. As of the purchase date, the university's intentions for the space were not announced, but the expectation is that the new space will allow the university to ramp up its research and ultimately employ in excess of 2,000 people.
The university is also a major contributor to the medical field with the EKG, gastroscope, and the announcement of Jonas Salk's polio vaccine. The university's 13000 acre biological station in the Northern Lower Peninsula of Michigan is one of only 47 Biosphere Reserves in the United States.
In the mid-1960s U-M researchers worked with IBM to develop a new virtual memory architectural model that became part of IBM's Model 360/67 mainframe computer (the 360/67 was initially dubbed the 360/65M where the "M" stood for Michigan). The Michigan Terminal System (MTS), an early time-sharing computer operating system developed at U-M, was the first system outside of IBM to use the 360/67's virtual memory features.
U-M is home to the National Election Studies and the University of Michigan Consumer Sentiment Index. The Correlates of War project, also located at U-M, is an accumulation of scientific knowledge about war. The university is also home to major research centers in optics, reconfigurable manufacturing systems, wireless integrated microsystems, and social sciences. The University of Michigan Transportation Research Institute and the Life Sciences Institute are located at the university. The Institute for Social Research (ISR), the nation's longest-standing laboratory for interdisciplinary research in the social sciences, is home to the Survey Research Center, Research Center for Group Dynamics, Center for Political Studies, Population Studies Center, and Inter-Consortium for Political and Social Research. Undergraduate students are able to participate in various research projects through the Undergraduate Research Opportunity Program (UROP) as well as the UROP/Creative-Programs.
The U-M library system comprises nineteen individual libraries with twenty-four separate collections—roughly 13.3 million volumes. U-M was the original home of the JSTOR database, which contains about 750,000 digitized pages from the entire pre-1990 backfile of ten journals of history and economics, and has initiated a book digitization program in collaboration with Google. The University of Michigan Press is also a part of the U-M library system.
In the late 1960s U-M, together with Michigan State University and Wayne State University, founded the Merit Network, one of the first university computer networks. The Merit Network was then and remains today administratively hosted by U-M. Another major contribution took place in 1987 when a proposal submitted by the Merit Network together with its partners IBM, MCI, and the State of Michigan won a national competition to upgrade and expand the National Science Foundation Network (NSFNET) backbone from 56,000 to 1.5 million, and later to 45 million bits per second. In 2006, U-M joined with Michigan State University and Wayne State University to create the University Research Corridor. This effort was undertaken to highlight the capabilities of the state's three leading research institutions and drive the transformation of Michigan's economy. The three universities are electronically interconnected via the Michigan LambdaRail (MiLR, pronounced 'MY-lar'), a high-speed data network providing 10 Gbit/s connections between the three university campuses and other national and international network connection points in Chicago.
The University of Michigan is a participant in the Committee on Institutional Cooperation (CIC), an academic consortium of the universities in the Big Ten Conference plus former conference member the University of Chicago. The initiative also allows students at participating institutions to take distance courses at other participating institutions and forms a partnership of research. Students at participating schools are also allowed "in-house" viewing privileges at other participating schools' libraries.
Student life.
Residential life.
The University of Michigan's campus housing system can accommodate up to 10,900 people, or nearly 30 percent of the total student population at the university. The residence halls are located in three distinct geographic areas on campus: Central Campus, Hill Area (between Central Campus and the University of Michigan Medical Center) and North Campus. Family housing is located on North Campus and mainly serves graduate students. The largest residence hall has a capacity of 1,240 students, while the smallest accommodates 25 residents. A majority of upper-division and graduate students live in off-campus apartments, houses, and cooperatives, with the largest concentrations in the Central and South Campus areas.
The residential system has a number of "living-learning communities" where academic activities and residential life are combined. These communities focus on areas such as research through the Michigan Research Community, medical sciences, community service and the German language. The Michigan Research Community and the Women in Science and Engineering Residence Program are housed in Mosher-Jordan Hall. The Residential College (RC), a living-learning community that is a division of the College of Literature, Science and the Arts, also has its principal instructional space in East Quad. Also housed in East Quad is the Michigan Community Scholars Program, which is dedicated to civic engagement, community service learning and intercultural understanding and dialogue. The Lloyd Hall Scholars Program (LHSP) is located in Alice Lloyd Hall. The Health Sciences Scholars Program (HSSP) is located in Couzens Hall. The North Quad complex houses two additional living-learning communities: the Global Scholars Program and the Max Kade German Program. It is "technology-rich," and houses communication-related programs, including the School of Information, the Department of Communication Studies, and the Department of Screen Arts and Cultures. North Quad is also home to services such as the Language Resource Center and the Sweetland Center for Writing.
The residential system also has a number of "theme communities" where students have the opportunity to be surrounded by students in a residential hall who share similar interests. These communities focus on global leadership, the college transition experience, and internationalism. The Adelia Cheever Program is housed in the Helen Newberry House. The First Year Experience is housed in the Baits II Houses, Northwood Houses, and Markley Hall. The Sophomore Experience is housed in Stockwell Hall and the Transfer Year Experience is housed in Northwood III. The newly organized International Impact program is housed in North Quad.
Groups and activities.
The University lists 1,438 student organizations. With a history of student activism, some of the most visible groups include those dedicated to causes such as civil rights and labor rights. One group is Students for a Democratic Society, which recently reformed with a new chapter on campus as of February 2007. Another student labor campaign organization recently established on campus is the United Students Against Sweatshops (USAS). This group seeks to hold accountable multinational companies that exploit their workers in factories around the world where college apparel is produced. Though the student body generally leans toward left-wing politics, there are also conservative groups, such as Young Americans for Freedom, and non-partisan groups, such as the Roosevelt Institution.
There are also several engineering projects teams, including the University of Michigan Solar Car Team, which has placed first in the North American Solar Challenge six times and third in the World Solar Challenge four times. Michigan Interactive Investments, the TAMID Israel Investment Group, and the Michigan Economics Society are also affiliated with the university.
The university also showcases many community service organizations and charitable projects, including Foundation for International Medical Relief of Children, Dance Marathon at the University of Michigan, The Detroit Partnership, Relay For Life, U-M Stars for the Make-A-Wish Foundation, InnoWorks at the University of Michigan, SERVE, Letters to Success, PROVIDES, Circle K, Habitat for Humanity, and Ann Arbor Reaching Out. Intramural sports are popular, and there are recreation facilities for each of the three campuses.
Fraternities and sororities play a role in the university's social life; approximately 18 percent of undergraduates are involved in Greek life. Membership numbers for the 2009-2010 school year reached the highest in the last two decades. Four different Greek councils—the Interfraternity Council, Multicultural Greek Council, National Pan-Hellenic Council, and Panhellenic Association—represent most Greek organizations. Each council has a different recruitment process.
The Michigan Union and Michigan League are student activity centers located on Central Campus; Pierpont Commons is on North Campus. The Michigan Union houses a majority of student groups, including the student government. The William Monroe Trotter House, located east of Central Campus, is a multicultural student center operated by the university's Office of Multi-Ethnic Student Affairs. The University Activities Center (UAC) is a student-run programming organization and is composed of 14 committees. Each group involves students in the planning and execution of a variety of events both on and off campus.
The Michigan Marching Band, composed of more than 350 students from almost all of U-M's schools, is the university's marching band. Over 100 years old, the band performs at every home football game and travels to at least one away game a year. The student-run and led University of Michigan Pops Orchestra is another musical ensemble that attracts students from all academic backgrounds. It performs regularly in the Michigan Theater. The University of Michigan Men's Glee Club, founded in 1859 and the second oldest such group in the country, is a men's chorus with over 100 members. Its eight-member subset a cappella group, the University of Michigan Friars, which was founded in 1955, is the oldest currently running "a cappella" group on campus.
The University of Michigan also encourages many cultural and ethnic student organizations on campus. There are currently over 317 organizations under this category. There are organizations for almost every culture from the to to even the 
. These organizations hope to promote various aspects of their culture along with raising political and social awareness around campus by hosting an assortment of events throughout the school year. These clubs also help students make this large University into a smaller community to help find people with similar interests and backgrounds.
Media and publications.
The student newspaper is "The Michigan Daily", founded in 1890 and editorially and financially independent of the university. "The Daily" is published five days a week during academic year, and weekly from May to August. Other student publications at the university include the conservative "The Michigan Review" and the progressive "Michigan Independent". The humor publications "Gargoyle" and "The Michigan Every Three Weekly" are also published by Michigan students.
WCBN-FM (88.3 FM) is the student-run college radio station which plays in freeform format. WOLV-TV is the student-run television station that is primarily shown on the university's cable television system.
Several academic journals are published at the university:
Athletics.
The University of Michigan's sports teams are called the Wolverines. They participate in the NCAA's Football Bowl Subdivision (formerly Division I-A) and in the Big Ten Conference in all sports except women's water polo, which is a member of the Collegiate Water Polo Association. U-M boasts 27 varsity sports, including 13 men's teams and 14 women's teams. In 10 of the past 14 years concluding in 2009, U-M has finished in the top five of the NACDA Director's Cup, a ranking compiled by the National Association of Collegiate Directors of Athletics to tabulate the success of universities in competitive sports. U-M has finished in the top 10 of the Directors' Cup standings in 14 of the award's 16 seasons and has placed in the top six in nine of the last 10 seasons.
The Michigan football program ranks first in NCAA history in both total wins (903 through the end of the 2012 season) and winning percentage (.735). The team won the first Rose Bowl game in 1902. U-M had 40 consecutive winning seasons from 1968 to 2007, including consecutive bowl game appearances from 1975 to 2007. The Wolverines have won a record 42 Big Ten championships. The program has eleven national championships, most recently in 1997, and has produced three Heisman Trophy winners: Tom Harmon, Desmond Howard and Charles Woodson.
Michigan Stadium is the largest college football stadium in the nation and one of the largest football-only stadiums in the world, with an official capacity of 109,901 (the extra seat is said to be "reserved" for Fritz Crisler) though attendance—frequently over 111,000 spectators—regularly exceeds the official capacity. The NCAA's record-breaking attendance has become commonplace at Michigan Stadium, especially since the arrival of head coach Bo Schembechler. U-M has fierce rivalries with many teams, including Michigan State, Notre Dame, and Ohio State; ESPN has referred to the Michigan-Ohio State rivalry as the greatest rivalry in American sports. U-M also has all-time winning records against Michigan State, Notre Dame, and Ohio State.
The men's ice hockey team, which plays at Yost Ice Arena, has won nine national championships, while the men's basketball team, which plays at the Crisler Center, has appeared in five Final Fours and won the national championship in 1989. The men's basketball program became involved in a scandal involving payments from a booster during the 1990s. This led to the program being placed on probation for a four-year period. The program also voluntarily vacated victories from its 1992–1993 and 1995–1999 seasons in which the payments took place, as well as its 1992 and 1993 Final Four appearances.
The men's wrestling, men's gymnastics, and women's volleyball teams compete at the Cliff Keen Arena, dedicated and named after longtime wrestling coach Cliff Keen in 1990.
Through the 2008 Summer Olympic Games, 178 U-M students and coaches had participated in the Olympics, winning medals in every Summer Olympics except 1896, and winning gold medals in all but four Olympiads. U-M students have won a total of 133 Olympic medals: 65 gold, 30 silver, and 38 bronze.
School songs.
The University of Michigan's fight song, "The Victors," was written by student Louis Elbel in 1898 following the last-minute football victory over the University of Chicago that won a league championship. The song was declared by John Philip Sousa as "the greatest college fight song ever written." The song refers to the university as being "the Champions of the West." At the time, U-M was part of the Western Conference, which would later become the Big Ten Conference. Michigan was considered to be on the Western Frontier when it was founded in the old Northwest Territory. Although mainly used at sporting events, the fight song can be heard at other events. President Gerald Ford had it played by the United States Marine Band as his entrance anthem during his term as president from 1974 to 1977, in preference over the more traditional "Hail to the Chief", and the Michigan Marching Band performed a slow-tempo variation on the fight song at his funeral. The fight song is also sung during graduation commencement ceremonies. The university's alma mater song is "The Yellow and Blue." A common rally cry is "Let's Go Blue!," which had a complementary short musical arrangement written by former students Joseph Carl, a sousaphonist, and Albert Ahronheim, a drum major.
Before "The Victors" was officially the University's fight song, the song "There'll Be a Hot Time in the Old Town Tonight" was considered to be the school song. After Michigan temporarily withdrew from the Western Conference in 1907, a new Michigan fight song "Varsity" was written in 1911 because the line "champions of the West" was no longer appropriate.
Alumni.
In addition to the late U.S. president Gerald Ford, the university has produced twenty-six Rhodes Scholars. As of 2012, the university has almost 500,000 living alumni.
More than 250 Michigan graduates have served as legislators as either a United States Senator (40 graduates) or as a Congressional representative (over 200 graduates), including former House Majority Leader Dick Gephardt and U.S. Representative Justin Amash, who represents Michigan's Third Congressional District. Mike Duggan, Mayor of Detroit, earned his bachelor and law degree at Michigan, while Michigan Governor Rick Snyder earned his bachelor, M.B.A., and J.D. degrees from Michigan. U-M's contributions to aeronautics include aircraft designer Clarence "Kelly" Johnson of Lockheed Skunk Works fame, Lockheed president Willis Hawkins, and several astronauts including the all-U-M crews of both Gemini 4 and Apollo 15. U-M counts among its matriculants twenty-one billionaires and prominent company founders and co-founders including Google co-founder Larry Page and Dr. J. Robert Beyster, who founded Science Applications International Corporation (SAIC) in 1969. Several U-M graduates contributed greatly to the field of computer science, including Claude Shannon (who made major contributions to the mathematics of information theory), and Turing Award winners Edgar Codd, Stephen Cook, Frances E. Allen and Michael Stonebraker. Marjorie Lee Browne received her M.S. in 1939 and her doctoral degree in 1950, becoming the third African American woman to earn a PhD in mathematics.
Notable writers who attended U-M include playwright Arthur Miller, essayists Susan Orlean and Sven Birkerts, journalists and editors Mike Wallace, Jonathan Chait of "The New Republic", Daniel Okrent, and Sandra Steingraber, food critics Ruth Reichl and Gael Greene, novelists Brett Ellen Block, Elizabeth Kostova, Marge Piercy, Brad Meltzer, Betty Smith, and Charles Major, screenwriter Judith Guest, Pulitzer Prize-winning poet Theodore Roethke, National Book Award winners Keith Waldrop and Jesmyn Ward, composer/author/puppeteer Forman Brown, and Alireza Jafarzadeh (a Middle East analyst, author, and TV commentator).
In Hollywood, famous alumni include actors James Earl Jones, David Alan Grier, actresses Lucy Liu, Gilda Radner, and Selma Blair, and filmmaker Lawrence Kasdan. Many Broadway and musical theatre actors, including Gavin Creel, Andrew Keenan-Bolger, and his sister Celia Keenan-Bolger, attended U-M for musical theatre. The creators of "A Very Potter Musical", known as StarKid Productions, also graduated from the University of Michigan. A member of Starkid, actor and singer Darren Criss, is a series regular on the television series "Glee".
Musical graduates include operatic soprano Jessye Norman, singer Joe Dassin, jazz guitarist Randy Napoleon, and Mannheim Steamroller founder Chip Davis. Classical composer Frank Ticheli and Broadway composer Andrew Lippa attended. Pop Superstar Madonna and rock legend Iggy Pop attended but did not graduate.
Other U-M graduates include HH Holmes (noted serial killer), Donald Kohn (past Vice Chairman of the Board of Governors of the Federal Reserve System), Temel Kotil (president and CEO of Turkish Airlines), current Dean of Harvard Law School Martha Minow, assisted-suicide advocate Dr. Jack Kevorkian, Weather Underground radical activist Bill Ayers, activist Tom Hayden, architect Charles Moore, Rensis Likert (a sociologist who specialized in management styles and developed the Likert scale), the Swedish Holocaust hero Raoul Wallenberg, and Benjamin D. Pritchard (the Civil War general who captured Jefferson Davis). Neurosurgeon and CNN chief medical correspondent Sanjay Gupta attended both college and medical school at U-M. Clarence Darrow attended law school at U-M at a time when many lawyers did not receive any formal education. Frank Murphy, who was mayor of Detroit, governor of Michigan, attorney general of the United States, and Supreme Court justice was also a graduate of the Law School. Conservative pundit Ann Coulter is another U-M law school graduate (J.D. 1988).
Vaughn R. Walker, a federal district judge in California who overturned the controversial California Proposition 8 in 2010 and ruled it unconstitutional, received his undergraduate degree from U-M in 1966.
Some more notorious graduates of the University are 1910 convicted murderer (though perhaps wrongfully so) Dr. Harvey Crippen, late 19th-century American serial killer Herman Mudgett, and "Unabomber" Ted Kaczynski.
U-M athletes have starred in Major League Baseball, the National Football League and National Basketball Association as well as other professional sports. Notable among recent players is Tom Brady of the New England Patriots. Three players have won college football's Heisman Trophy, awarded to the player considered the best in the nation: Tom Harmon (1940), Desmond Howard (1991) and Charles Woodson (1997). Professional golfer John Schroeder and Olympic swimmer Michael Phelps also attended the University of Michigan, with the latter studying Sports Marketing and Management. Phelps also swam competitively for Club Wolverine, a swimming club associated with the university. National Hockey League players Marty Turco, Chris Summers, Max Pacioretty, Carl Hagelin, Brendan Morrison, Jack Johnson, and Michael Cammalleri all played for U-M's ice hockey team. Baseball Hall of Famers George Sisler and Barry Larkin also played baseball at the university.
The university claims the only alumni association with a chapter on the moon, established in 1971 when the crew of Apollo 15 placed a charter plaque for a new U-M Alumni Association on the lunar surface. The plaque states: "The Alumni Association of The University of Michigan. Charter Number One. This is to certify that The University of Michigan Club of The Moon is a duly constituted unit of the Alumni Association and entitled to all the rights and privileges under the Association's Constitution." According to the Apollo 15 astronauts, several small U-M flags were brought on the mission. The presence of a U-M flag on the moon is a long-held campus myth.
References.
General.
</dl>

</doc>
<doc id="31758" url="http://en.wikipedia.org/wiki?curid=31758" title="United States congressional delegations from Alabama">
United States congressional delegations from Alabama

These are tables of congressional delegations from Alabama to the United States Senate and United States House of Representatives.
Current delegation
Alabama's current delegation
House of Representatives.
1818 – 1819: 1 non-voting delegate.
Starting on January 29, 1818, Alabama Territory sent a non-voting delegate to the House.
1819 – 1823: 1 seat.
After statehood, Alabama had one seat in the House.
1823 – 1833: 3 seats.
Following the 1820 census, Alabama had three seats.
1833 – 1843: 5 seats.
Following the 1830 census, Alabama had five seats. During the 27th Congress, those seats were all elected state-wide at-large on a general ticket.
1843 – 1863: 7 seats.
Following the 1840 census, Alabama resumed the use of districts, now increased to seven.
1863 – 1873: 6 seats.
Following the 1860 census, Alabama was apportioned six seats.
1873 – 1893: 8 seats.
Following the 1870 census, Alabama was apportioned eight seats. From 1873 to 1877, the two new seats were elected at large, state-wide. After 1877, however, the entire delegation was redistricted.
1893 – 1913: 9 seats.
Following the 1890 census, Alabama was apportioned nine seats.
1913 – 1933: 10 seats.
Following the 1910 census, Alabama was apportioned ten seats. At first, the extra seat was elected at-large. Starting with the 1916 elections, the seats were redistricted and a tenth district was added.
1933 – 1963: 9 seats.
Following the 1930 census, Alabama was apportioned nine seats.
1963 – 1973: 8 seats.
Following the 1960 census, Alabama was apportioned eight seats.
1973 – Present: 7 seats.
Since the 1970 census, Alabama has been apportioned seven seats.
Living former Members of the U.S. House of Representatives from Alabama.
s of 2015[ [update]], there are seventeen former members of the U.S. House of Representatives from the U.S. State of Alabama who are currently living at this time.
Living former U.S. Senators from Alabama.
s of 2015[ [update]], there are two former U.S. Senators from the U.S. State of Alabama who are currently living at this time, two from Class 3.

</doc>
<doc id="31802" url="http://en.wikipedia.org/wiki?curid=31802" title="Universal access to education">
Universal access to education

Universal access to education is the ability of all people to have equal opportunity in education, regardless of their social class, gender, ethnicity background or physical and mental disabilities. The term is used both in college admission for the middle and lower classes, and in assistive technology for the disabled. Some critics find this idea an example of "political correctness". In order to facilitate the access of education to all, certain countries have right to education.
Universal access to education encourages a variety of pedagogical approaches to accomplish the dissemination of knowledge across the diversity of social, political, cultural, economic, national and biological backgrounds. Initially developed with the theme of equal opportunity access and inclusion of students with learning or physical and mental disabilities, the themes governing universal access to education have now expanded across all forms of ability and diversity. However, as the definition of diversity is within itself is a broad amalgamation, teachers exercising universal access will continually face challenges and incorporate adjustments in their lesson plan to foster themes of equal opportunity of education. 
As universal access continues to be incorporated into the U.S. education system, professors and instructors at the college level are required (in some instances by law) to rethink methods of facilitating universal access in their classrooms. Universal access to college education may involve the provision of a variety of different assessment methods of learning and retention. For example, in order to determine how much of the material was learned, a professor may enlist multiple methods of assessment. Methods of assessment may include a comprehensive exam, unit exams, portfolios, research papers, literature reviews, an oral exam or homework assignments. Providing a variety of ways to assess the extent of learning and retention will not only identify the gaps in universal access but may also elucidate the ways to improve universal access.
Access to Education By Law.
The Indian government has provided right to education to every citizen of its country.

</doc>
<doc id="31803" url="http://en.wikipedia.org/wiki?curid=31803" title="Trinity College, Cambridge">
Trinity College, Cambridge

Trinity College is a constituent college of the University of Cambridge in England. With around 600 undergraduates, 300 graduates, and over 180 fellows, it is the largest college in either of the Oxbridge universities. By student numbers, it is second to Homerton College, Cambridge.
Members of Trinity have won 32 Nobel Prizes out of the 90 won by members of Cambridge University, the highest number of any college. Five Fields Medals in Mathematics were won by members of the college (of the six awarded to members of British universities) and one Abel Prize.
Trinity alumni include six British prime ministers (all Tory or Whig/Liberal), physicists Isaac Newton and Niels Bohr, the poet Lord Byron, philosophers Ludwig Wittgenstein and Bertrand Russell (whom it expelled before reaccepting), and Soviet spies Kim Philby, Guy Burgess, and Anthony Blunt.
Two members of the British Royal Family have studied at Trinity and been awarded degrees as a result: Prince William of Gloucester and Edinburgh, who gained an MA in 1790, and Prince Charles, who was awarded a lower second class BA in 1970. Other British Royal family members have studied there without obtaining degrees, including King Edward VII, King George VI, and Prince Henry, Duke of Gloucester.
Trinity has many college societies, including the Trinity Mathematical Society, which is the oldest mathematical university society in the United Kingdom, and the First and Third Trinity Boat Club, its rowing club, which gives its name to the college's May Ball. Along with Christ's, Jesus, King's and St John's colleges, it has also provided several of the well known members of the Apostles, an intellectual secret society.
In 1848, Trinity hosted the meeting at which Cambridge undergraduates representing private schools such as Westminster drew up the first formal rules of football, known as the Cambridge Rules.
Trinity's sister college in Oxford is Christ Church. Like that college, Trinity has been linked with Westminster School since the school's re-foundation in 1560, and its Master is an "ex officio "governor of the school.
History.
Foundation.
The college was founded by Henry VIII in 1546, from the merger of two existing colleges: Michaelhouse (founded by Hervey de Stanton in 1324), and King's Hall (established by Edward II in 1317 and refounded by Edward III in 1337). At the time, Henry had been seizing church lands from abbeys and monasteries. The universities of Oxford and Cambridge, being both religious institutions and quite rich, expected to be next in line. The king duly passed an Act of Parliament that allowed him to suppress (and confiscate the property of) any college he wished. The universities used their contacts to plead with his sixth wife, Catherine Parr. The queen persuaded her husband not to close them down, but to create a new college. The king did not want to use royal funds, so he instead combined two colleges (King's Hall and Michaelhouse) and seven hostels (Physwick (formerly part of Gonville and Caius College, Cambridge), Gregory's, Ovyng's, Catherine's, Garratt, Margaret's, and Tyler's) to form Trinity.
Nevile's expansion.
Contrary to popular belief, the monastic lands granted by Henry VIII were not on their own sufficient to ensure Trinity's eventual rise. In terms of architecture and royal association, it was not until the Mastership of Thomas Nevile (1593–1615) that Trinity assumed both its spaciousness and its courtly association with the governing class that distinguished it until the Civil War. In its infancy Trinity had owed a great deal to its neighbouring college of St John's: in the exaggerated words of Roger Ascham Trinity was little more than a "colonia deducta". Its first four Masters were educated at St John's, and it took until around 1575 for the two colleges' application numbers to draw even, a position in which they have remained since the Civil War. In terms of wealth, Trinity's current fortunes belie prior fluctuations; Nevile's building campaign drove the college into debt from which it only surfaced in the 1640s, and the mastership of Richard Bentley (notorious for the construction of a hugely expensive staircase in the Master's Lodge, and Bentley's repeated refusals to step down despite pleas from the Fellowship) adversely affected applications and finances.
Most of the Trinity's major buildings date from the 16th and 17th centuries. Thomas Nevile, who became Master of Trinity in 1593, rebuilt and redesigned much of the college. This work included the enlargement and completion of Great Court, and the construction of Nevile's Court between Great Court and the river Cam. Nevile's Court was completed in the late 17th century when the Wren Library, designed by Sir Christopher Wren, was built.
Modern day.
In the 20th century, Trinity College, St John's College and King's College were for decades the main recruiting grounds for the Cambridge Apostles, an elite, intellectual secret society.
In 2011, the John Templeton Foundation awarded Trinity College's Master, the astrophysicist Martin Rees, its controversial million-pound Templeton Prize, for "affirming life's spiritual dimension".
Since graduating from Trinity, HRH The Prince of Wales has retained a private apartment in usufruct at the back of the college. The apartment is maintained for private and informal visits. On a recent visit, his sons William (latterly HRH The Duke of Cambridge) and Harry both visited during their motorcycle tour of the UK.
Trinity is the richest Oxbridge college, with a landholding alone worth £800 million.
Trinity is sometimes suggested to be the second, third or fourth wealthiest landowner in the UK (or in England) — after the Crown Estate, the National Trust and the Church of England. (A variant of this legend is repeated in the Tom Sharpe novel "Porterhouse Blue".) In 2005, Trinity's annual rental income from its properties was reported to be in excess of £20 million.
Trinity owns:
Legends.
Lord Byron purportedly kept a pet bear whilst living in the college.
A second legend is that it is possible to walk from Cambridge to Oxford on land solely owned by Trinity. Several varieties of this legend exist – others refer to the combined land of Trinity College, Cambridge and Trinity College, Oxford, of Trinity College, Cambridge and Christ Church, Oxford, or St John's College, Oxford and St John's College, Cambridge. All are almost certainly false.
Trinity is often cited as the inventor of an English, less sweet, version of crème brûlée, known as "Trinity burnt cream", although the college chefs have sometimes been known to refer to it as "Trinity Creme Brulee". The burnt-cream, first introduced at Trinity High Table in 1879, in fact differs quite markedly from French recipes, the earliest of which is from 1691.
Trinity in Camberwell.
Trinity College has a long-standing relationship with the Parish of St George's, Camberwell, in South London. Students from the College have helped to run holiday schemes for children from the parish since 1966. The relationship was formalised in 1979 with the establishment of Trinity in Camberwell as a registered charity (Charity Commission no. 279447) which exists ‘to provide, promote, assist and encourage the advancement of education and relief of need and other charitable objects for the benefit of the community in the Parish of St George's, Camberwell, and the neighbourhood thereof.’
Buildings and grounds.
Great Gate.
The Great Gate is the main entrance to the college, leading to the Great Court. A statue of the college founder, Henry VIII, stands in a niche above the doorway. In his hand he holds a table leg instead of the original sword and myths abound as to how the switch was carried out and by whom. In 1704, the University's first astronomical observatory was built on top of the gatehouse. Beneath the founder's statue are the coats of arms of Edward III, the founder of King's Hall, and those of his five sons who survived to maturity, as well as William of Hatfield, whose shield is blank as he died as an infant, before being granted arms.
Great Court.
Great Court (built principally 1599–1608) was the brainchild of Thomas Nevile, who demolished several existing buildings on this site, including almost the entirety of the former college of Michaelhouse. The sole remaining building of Michaelhouse was replaced by the then current Kitchens (designed by James Essex) in 1770–1775. The Master's Lodge is the official residence of the Sovereign when in Cambridge.
King's Hostel (built 1377–1416) is located to the north of Great Court, behind the Clock Tower, this is (along with the King's Gate), the sole remaining building from King's Hall.
Bishop's Hostel (built 1671, Robert Minchin): A detached building to the southwest of Great Court, and named after John Hacket, Bishop of Lichfield and Coventry. Additional buildings were built in 1878 by Arthur Blomfield.
Nevile's Court.
Nevile's Court (built 1614) is located between Great Court and the river, this court was created by a bequest by the college's master, Thomas Nevile, originally two-thirds of its current length and without the Wren Library. The appearance of the upper floor was remodelled slightly two centuries later. Cloisters run around the court, providing sheltered walkways from the rear of Great Hall to the college library and reading room as well as the Wren Library and New Court.
Wren Library (built 1676–1695, Christopher Wren) is located at the west end of Nevile's Court, the Wren is one of Cambridge's most famous and well-endowed libraries. Among its notable possessions are two of Shakespeare's First Folios, a 14th-century manuscript of The Vision of Piers Plowman, and letters written by Sir Isaac Newton. Below the building are the pleasant Wren Library Cloisters, where students may enjoy a fine view of the Great Hall in front of them, and the river and Backs directly behind.
New Court.
New Court (or "King's Court"; built 1825, William Wilkins) is located to the south of Nevile's Court, and built in Tudor-Gothic style, this court is notable for the large tree in the centre. A myth is sometimes circulated that this was the tree from which the apple dropped onto Isaac Newton; in fact, Newton was at Woolsthorpe when he deduced his theory of gravity – and the tree is a chestnut tree. Many other "New Courts" in the colleges were built at this time to accommodate the new influx of students.
Other courts.
Whewell's Court (actually two courts with a third in between, built 1860 & 1868, architect Anthony Salvin) is located across the street from Great Court, and was entirely paid for by William Whewell, the Master of the college from 1841 until his death in 1866. The north range was later remodelled by W.D. Caroe.
Angel Court (built 1957–1959, H. C. Husband) is located between Great Court and Trinity Street, and is used along with the Wolfson Building for accommodating first year students.
The Wolfson Building (built 1968–1972, Architects Co-Partnership) is located to the south of Whewell's Court, on top of a podium above shops, this building resembles a brick-clad ziggurat, and is used exclusively for first-year accommodation. Having been renovated during the academic year 2005–06, rooms are now almost all en-suite.
Blue Boar Court (built 1989, MJP Architects and Wright) is located to the south of the Wolfson Building, on top of podium a floor up from ground level, and including the upper floors of several surrounding Georgian buildings on Trinity Street, Green Street and Sidney Street.
Burrell's Field (built 1995, MJP Architects) is located on a site to the west of the main College buildings, opposite the Cambridge University Library.
There are also College rooms above shops in Bridge Street and Jesus Lane, behind Whewell's Court, and graduate accommodation in Portugal Street and other roads around Cambridge.
Chapel.
Trinity College Chapel dates from the mid 16th Century and is Grade I listed.
There are a number of memorials to former Fellows of Trinity within the Chapel, including statues, brasses, and two memorials to graduates and Fellows who died during the World Wars.
The Chapel is a performance space for the college choir which comprises around 30 Choral Scholars and 2 Organ Scholars, all of whom are ordinarily undergraduate members of the college.
Grounds.
The Fellows’ Garden is located on the west side of Queen's Road, opposite the drive that leads to the Backs.
The Fellows’ Bowling Green is located behind the Master's Lodge it is the site for many of the tutors' garden parties in the summer months, while the Master's Garden is located behind the Master's Lodge.
The Old Fields are located on the western side of Grange Road, next to Burrell's Field. It currently houses the college's gym, changing rooms, squash courts, badminton courts, rugby, hockey and football pitches along with tennis and netball courts.
Academic profile.
Since 1997, the college has always come at least eighth in the Tompkins Table, which ranks the 29 Cambridge colleges according to the academic performance of their undergraduates, and on five occasions it has been in first place. Its average position has been third. On this benchmark, it has been behind Emmanuel (average second place) and above Christ's (average fourth place). In 2011, 37% of Trinity undergraduates achieved Firsts – a recent record among Cambridge colleges. The College improved on this in 2012, when 37.9% of its undergraduates were awarded Firsts. In 2013, the College topped the Table for the third successive year. Its undergraduates achieved a new record with the highest proportion of first degree passes ever achieved at 41.7 per cent, eight percentage points ahead of second-placed Pembroke.
Admissions.
Currently, about 50% of Trinity's undergraduates attended independent schools. In 2006 it accepted a smaller proportion of students from state schools (39%) than any other Cambridge college, and on a rolling three-year average it has admitted a smaller proportion of state school pupils (42%) than any other college at either Cambridge or Oxford.
According to the "Good Schools Guide", about 7% of British school-age students attend private schools. Trinity states that it disregards what type of school its applicants attend, and accepts students solely on the basis of their academic prospects.
Trinity admitted its first woman graduate student in 1976 and its first woman undergraduate in 1978, and appointed its first female fellow in 1977.
Scholarships and prizes.
The Scholars, together with the Master and Fellows, make up the Foundation of the College.
In order of seniority:
Research Scholars receive funding for graduate studies. Typically one must graduate in the top ten percent of one's class and continue for graduate study at Trinity. They are given first preference in the assignment of college rooms and number approximately 25.
The Senior Scholars consist of those who attain a degree with First Class honours or higher in any year after the first of an undergraduate tripos, but also, those who obtain a high First class marks in their first year. The college pays them a stipend of £250 a year and allows them to choose rooms directly following the research scholars. There are around 40 senior scholars at any one time.
The Junior Scholars are those who are not senior scholars but still obtained a First in their first year. Their stipend is £175 a year. They are given preference in the room ballot over 2nd years who are not scholars.
These scholarships are tenable for the academic year following that in which the result was achieved. If a scholarship is awarded but the student does not continue at Trinity then only a quarter of the stipend is given. However all students who achieve a First are awarded an additional £240 prize upon announcement of the results.
All final year undergraduates who achieve first-class honours in their final exams are offered full financial support to read for a Master's degree at Cambridge (this funding is also sometimes available for students who achieved high second-class honours marks). Other support is available for PhD degrees. The College also offers a number of other bursaries and studentships open to external applicants. The right to walk on the grass in the college courts is exclusive to Fellows of the college and their guests. Scholars do, however, have the right to walk on Scholar's Lawn, but only in full academic dress.
Traditions.
The Great Court Run.
The Great Court Run is an attempt to run round the 400-yard perimeter of Great Court (approximately 367 m), in the 43 seconds of the clock striking twelve. Students traditionally attempt to complete the circuit on the day of the Matriculation Dinner. It is a rather difficult challenge: one needs to be a fine sprinter to achieve it, but it is by no means necessary to be of Olympic standard, despite assertions made in the press.
It is widely believed that Sebastian Coe successfully completed the run when he beat Steve Cram in a charity race in October 1988. Sebastian Coe's time on 29 October 1988 was reported by Norris McWhirter to have been 45.52 seconds, but it was actually 46.0 seconds (confirmed by the video tape), while Cram's was 46.3 seconds. The clock on that day took 44.4 seconds (i.e. a "long" time, probably two days after the last winding) and the video film confirms that Coe was some 12 metres short of his finish line when the fateful final stroke occurred. The television commentators were more than a little disingenuous in suggesting that the dying sounds of the bell could be included in the striking time, thereby allowing Coe's run to be claimed as successful.
One reason Olympic runners Cram and Coe found the challenge so tough is that they started at the middle of one side of the Court, thereby having to negotiate four right-angle turns. In the days when students started at the corner, only three turns were needed.
Until the mid-1990s, the run was traditionally attempted by first-year students, at midnight following their Matriculation Dinner. Following a number of accidents to drunk undergraduates running on slippery cobbles, the college now organises a more formal Great Court Run, at 12 noon on the day of Matriculation Dinner: the challenge is only open to freshers, many of whom compete in fancy dress.
Open-air concerts.
 
"Singing on the River", 9 June 2013
One Sunday each June (the exact date depending on the university term), the College Choir perform a short concert immediately after the clock strikes noon. Known as "Singing from the Towers", half of the choir sings from the top of the Great Gate, while the other half sings from the top of the Clock Tower approximately 60 metres away, giving a strong antiphonal effect. Midway through the concert, the Cambridge University Brass Ensemble performs from the top of the Queen's Tower.
Later that same day, the College Choir gives a second open-air concert, known as "Singing on the River", where they perform madrigals and arrangements of popular songs from a raft of punts lit with lanterns or fairy lights on the river. For the finale, John Wilbye's madrigal "Draw on, sweet night", the raft is unmoored and punted downstream to give a fade out effect. As a tradition, however, this latter concert dates back only to the mid-1980s, when the College Choir first acquired female members. In the years immediately before this, an annual concert on the river was given by the University Madrigal Society.
Mallard.
Another tradition relates to an artificial duck known as the Mallard, which should reside in the rafters of the Great Hall. Students occasionally moved the duck from one rafter to another without permission from the college. This is considered difficult; access to the Hall outside meal-times is prohibited and the rafters are dangerously high, so it was not attempted for several years. During the Easter term of 2006, the Mallard was knocked off its rafter by one of the pigeons which enter the Hall through the pinnacle windows. It was found intact on the floor, and revealed to not be made out of wood as generally believed. It is currently held by the College and it is unknown whether it will be reinstated.
Chair legs and bicycles.
The sceptre held by the statue of Henry VIII mounted above the medieval Great Gate was replaced with a chair leg as a prank many years ago. It has remained there to this day: when in the 1980s students exchanged the chair leg for a bicycle pump, the College replaced the chair leg.
For many years it was the custom for students to place a bicycle high in branches of the tree in the centre of New Court. Usually invisible except in winter, when the leaves had fallen, such bicycles tended to remain for several years before being removed by the authorities. The students then inserted another bicycle.
College rivalry.
The college remains a great rival of St John's which is its main competitor in sports and academia (John's is situated next to Trinity). This has given rise to a number of anecdotes and myths. It is often cited as the reason why the older courts of Trinity generally have no J staircases, despite including other letters in alphabetical order. A far more likely reason remains the absence of the letter J in the Latin alphabet, and it should be noted that St John's College's older courts also lack J staircases. There are also two small muzzle-loading cannons on the bowling green pointing in the direction of John's, though this orientation may be coincidental. Another story sometimes told is that the reason that the clock in Trinity Great Court strikes each hour twice is that the fellows of St John's once complained about the noise it made. Generally the colleges maintain a cordial relationship with one other, and Trinity's benefaction and association with her neighbouring colleges has always far outweighed such rivalries; compatriotism led famously to the splitting of the atomic nucleus in 1932 by Ernest Walton and John Cockcroft, of Trinity and St John's respectively.
Minor traditions.
Trinity College undergraduate gowns are readily distinguished from the black gowns favoured by most other Cambridge colleges. They are instead dark blue with black facings. They are expected to be worn to formal events such as formal halls and also when an undergraduate sees the Dean of the College in a formal capacity.
Trinity students, along with those of King's and St John's, are the first to be presented to the Congregation of the Regent House at graduation.
College Grace.
Each evening before dinner, grace is recited by the senior Fellow presiding. The simple grace is as follows:
If both of the two High Tables are in use then the following antiphonal formula is prefixed to the main grace:
Following the meal, the simple formula "Benedicto benedicatur" is pronounced.
People associated with the college.
Notable fellows and alumni.
The Parish of the Ascension Burial Ground in Cambridge contains the graves of 27 Fellows of Trinity College, Cambridge most of whom are also commemorated in Trinity College Chapel with brass plaques.
Fields Medallists.
Trinity College also has claim to a number of winners of the Fields Medal (commonly regarded as the mathematical equivalent of the Nobel Prize): Michael Atiyah, Alan Baker, Richard Borcherds and Timothy Gowers. Atiyah is also an Abel Prize winner.
British Prime Ministers.
Other Trinity politicians include Robert Devereux, 2nd Earl of Essex, courtier of Elizabeth I; William Waddington, Prime Minister of France; Erskine Hamilton Childers, President of Ireland; Jawaharlal Nehru, the first and longest serving Prime Minister of India; Rajiv Gandhi, Prime Minister of India; Lee Hsien Loong, Prime Minister of Singapore; Samir Rifai, Prime Minister of Jordan and William Whitelaw, 1st Viscount Whitelaw, Lady Thatcher's Home Secretary and subsequent Deputy Prime Minister.
Masters.
The head of Trinity College is the Master, who is customarily of the highest academic distinction.
The role is a Crown appointment, formerly made by the Monarch on the advice of the Prime Minister. Nowadays the Fellows of the College choose the new Master, and the Royal role is only nominal. In modern times the Master has customarily been of the highest academic distinction.
The first Master, John Redman, was appointed in 1546.
The last four Masters have all been Fellows of the college. The last master was Martin Rees, Baron Rees of Ludlow (until end of June 2012). He was succeeded by Sir Gregory Winter on 2 October 2012.

</doc>
<doc id="31808" url="http://en.wikipedia.org/wiki?curid=31808" title="U">
U

U (named "u" , plural "ues") is the twenty-first letter in the ISO basic Latin alphabet and the last vowel in the English alphabet.
History.
The letter u ultimately comes from the Semitic letter "Waw" by way of the letter y. See the letter y for details.
During the late Middle Ages, two forms of 'v' developed, which were both used for its ancestor 'u' and modern 'v'. The pointed form 'v' was written at the beginning of a word, while a rounded form 'u' was used in the middle or end, regardless of sound. So whereas 'valor' and 'excuse' appeared as in modern printing, 'have' and 'upon' were printed 'haue' and 'vpon'. The first distinction between the letters 'u' and 'v' is recorded in a Gothic alphabet from 1386, where 'v' preceded 'u'. Printers eschewed capital 'U' into the 17th century and the distinction between the two letters was not fully accepted by the French Academy until 1762.
Use in English.
In English, the letter u has four primary pronunciations. There are "long" and "short" pronunciation. Short u, found originally in closed syllables, most commonly represents (as in 'duck'), though it retains its old pronunciation after labial consonants in some words (as in 'put') and occasionally elsewhere (as in 'sugar'). Long u, found originally in words of French origin (with the original long u being respelled ou), most commonly represents (as in 'mule'), reducing to after ar (as in 'rule') and sometimes (or optionally) after el (as in 'lute'), and after additional consonants in American English (see do–dew merger). (After ess, /sjuː, zjuː/ have assimilated to /ʃuː, ʒuː/.) In a few words, short 'u' represent other sounds, such as in 'business' and in 'bury'. 
The letter u is used in the digraphs "au" , "ou" (various pronunciations), and with the value of "long u" in "eu", "ue", and in a few words "ui" (as in 'fruit'). U often has the sound before a vowel in the sequences "qu" (as in 'quick'), "gu" (as in 'anguish'), and "su" (as in 'suave'), though it is silent in final "-que" (as in 'unique') and in many words with "gu" (as in 'guard'). 
Additionally, the letter 'u' is used in text messaging and internet and other written slang to denote 'you', by virtue of both being pronounced .
One thing to note is that other varieties of the English language (i.e. British English, Canadian English, etc.) use the letter U in words such as "colour", "labour", "valour", etc. However, in American English the letter is not used and said words mentioned are spelled as "color", for example.
Use in other languages.
In most languages that use the Latin alphabet, 'U' represents the close back rounded vowel /u/.
In French orthography the letter represents the close front rounded vowel (/y/); /u/ is represented by 'ou'.
In mathematics and science.
The symbol 'U' is the chemical symbol for uranium.
'u' is the symbol for the atomic mass unit.
U is also the source of the mathematical symbol ∪, representing a union. It is used mainly for Venn diagrams and geometry.
It is also used as a graphic approximation of the Greek letter μ (mu) for "micro-" in metric measurements, as in "um" for μm (micrometer).

</doc>
<doc id="31838" url="http://en.wikipedia.org/wiki?curid=31838" title="Economy of the United Arab Emirates">
Economy of the United Arab Emirates

The economy of the United Arab Emirates is the second largest in the Arab world (after Saudi Arabia), with a gross domestic product (GDP) of $570 billion (AED2.1 trillion) in 2014. The United Arab Emirates has been successfully diversifying its economy. 71% of UAE's total GDP comes from non-oil sectors.
Although UAE has the most diversified economy in the GCC, the UAE's economy remains extremely reliant on oil. With the exception of Dubai, most of the UAE is dependent on oil revenues. Petroleum and natural gas continue to play a central role in the economy, especially in Abu Dhabi. More than 85% of the UAE's economy was based on the oil exports in 2009. While Abu Dhabi and other UAE emirates have remained relatively conservative in their approach to diversification, Dubai, which has far smaller oil reserves, was bolder in its diversification policy. In 2011, oil exports accounted for 77% of the UAE's state budget.
Dubai suffered from a significant economic crisis in 2007-2010 and was bailed out by Abu Dhabi's oil wealth. Dubai's current prosperity has been attributed to Abu Dhabi's petrodollars. Dubai is currently in extreme debt. Tourism is one of the main sources of revenue in the UAE, with some of the world's most luxurious hotels being based in the UAE. Although the UAE is now less dependent on natural resources as a source of revenue, petroleum and natural gas exports still play an important role in the economy, especially in Abu Dhabi. A massive construction boom, an expanding manufacturing base, and a thriving services sector are helping the UAE diversify its economy. Nationwide, there is currently $350 billion worth of active construction projects.
The UAE is a member of the World Trade Organization and OPEC.
Overview.
UAE has the second-largest economy in the Arab world (after Saudi Arabia), with a gross domestic product (GDP) of $377 billion (AED1.38 trillion) in 2012. A third of the GDP is from oil revenues. The economy was expected to grow 4–4.5% in 2013, compared to 2.3–3.5% over the previous five years. Since independence in 1971, UAE's economy has grown by nearly 231 times to AED1.45 trillion in 2013. The non-oil trade has grown to AED1.2 trillion, a growth of around 28 times from 1981 to 2012.
Prior to independence from the UK and unification in 1971, each emirate was responsible for its own economy. At the time, pearl diving, seafaring and fishing were together the mainstay of the economy, until the development of Japanese cultured pearls and the discovery of commercial quantities of oil. Previous UAE President Sheikh Zayed Bin Sultan Al Nahyan is credited with bringing the country forward into the 20th century and using the revenue from oil exports to fund all the necessary development. Likewise, former UAE vice-president Sheikh Rashid Bin Saeed Al Maktoum had a bold vision for the Emirate of Dubai and foresaw the future in not petroleum alone, but also other industries.
External trade.
With imports totaling $273.5 billion in 2012, UAE passed Saudi Arabia as the largest consumer market in the region. Exports totaled $314 billion, which makes UAE the second largest exporter in the region.
UAE and India are each other's main trading partners, with the latter having many of its citizens working and living in the former. The trade totals over $75 billion (AED275.25 billion).
The report, GCC Trade and Investment Flows, found that China and India in particular have the greatest potential to drive the region’s economic profile. China especially is touted as a “game-changer”: GCC-China trade grew faster than with any other significant trade partner in the three years to 2013, according to the study by the Economist Intelligence Unit (EIU). Asia as a whole accounted for 67 per cent of GCC exports and 40 per cent of imports in 2013.
The UAE is the main trading partner of Africa within the GCC, with approximately 80% of the UAE’s imports from Africa being primary products, such as food produce and beverages. In Q3 2014, Dubai’s total non-oil trade with Africa reached a value of AED89 billion, according to Dubai Customs. Vision (Magazine) stated that total non-oil trade between Dubai and Africa has grown by over 700% over the last decade.
The UAE also has significant economic ties with the USA, UK and other European countries.
Diversification.
Although UAE has the most diversified economy in the GCC, the UAE's economy remains extremely reliant on oil. With the exception of Dubai, most of the UAE is dependent on oil revenues. Petroleum and natural gas continue to play a central role in the economy, especially in Abu Dhabi. More than 85% of the UAE's economy was based on the oil exports in 2009. While Abu Dhabi and other UAE emirates have remained relatively conservative in their approach to diversification, Dubai, which has far smaller oil reserves, was bolder in its diversification policy. In 2011, oil exports accounted for 77% of the UAE's state budget.
Dubai suffered from a significant economic crisis in 2007-2010 and was bailed out by Abu Dhabi's oil wealth. Dubai's current prosperity has been attributed to Abu Dhabi's petrodollars. Dubai is currently in extreme debt.
The UAE government has worked towards reducing the economy's dependence on oil exports by 2030. Various projects are underway to help achieve this, the most recent being the Khalifa Port, opened in the Emirate of Abu Dhabi at the end of 2012. The UAE has also won the right to host the World Expo 2020, which is believed to have a positive effect on future growth, although there are some skeptics which mention the opposite.
Over the decades, the Emirate of Dubai has started to look for additional sources of revenue. High-class tourism and international finance continue to be developed. In line with this initiative, the Dubai International Financial Centre was announced, offering 55.5% foreign ownership, no withholding tax, freehold land and office space and a tailor-made financial regulatory system with laws taken from best practice in other leading financial centres like New York, London, Zürich and Singapore. A new stock market for regional companies and other initiatives were announced in DIFC. Dubai has also developed Internet and Media free zones, offering 100% foreign ownership, no tax office space for the world's leading ICT and media companies, with the latest communications infrastructure to service them. Many of the world's leading companies have now set up branch offices, and even changed headquarters to, there. Recent liberalisation in the property market allowing non citizens to buy freehold land has resulted in a major boom in the construction and real estate sectors, with several signature developments such as the 2 Palm Islands, the World (archipelago), Dubai Marina, Jumeirah Lake Towers, and a number of other developments, offering villas and high rise apartments and office space. Emirates (part of the Emirates Group) was formed by the Dubai Government in the 1980s and is presently one of the few airlines to witness strong levels of growth. Emirates is also the largest operator of the Airbus A380 aircraft.
s of 2001[ [update]], budgeted government revenues were about AED 29.7 billion, and expenditures were about AED 22.9 billion.
In addition, to finding new ways of sustaining the national economy, the UAE has also made progress in installing new, sustainable methods of generating electricity. This is evidenced by various solar energy initiatives at Masdar City and by other renewable energy developments in parts of the country.
Human Resources and Employment.
Emiratisation is an initiative by the government of the UAE to employ more UAE Nationals in a meaningful and efficient manner in the public and private sectors. While the program has been in place for more than a decade and results can be seen in the public sector, the private sector is still lagging behind with citizens only representing 0.34% of the private sector workforce. This is generally because the current generation of UAE locals prefer cushy government jobs and consider private sector jobs to be below them. Also, because of the benefits available from the UAE government, most locals would rather rely on these benefits and not go to work.
While there is general agreement over the importance of Emiratisation for social, economic and political reasons, there is also some contention as to the impact of localization on organizational efficiency. It is yet unknown whether, and the extent to which, employment of nationals generates returns for MNEs operating in the Middle East. Recent research cautions that localization is not always advantageous for firms operating in the region, and its effectiveness depends on a number of contingent factors.
In December 2009 however, a positive impact of UAE citizens in the workplace was identified in a newspaper article citing a yet unpublished study, this advantage being the use of networks within the evolving power structures.
Overall, however, uptake in the private sector remains low regardless of significant investments in education, which have reached record levels with education now accounting for 22.5% – or $2.6 billion – of the overall budget planned for 2010. Multiple governmental initiatives are actively promoting Emiratisation by training anyone from highschool dropouts to graduates in a multitude of skills needed for the - essentially Western - work environment of the UAE, these initiatives include Tawteen UAE, ENDP or the Abu Dhabi Tawteen Council.
There are very few anti-discrimination laws in relation to labour issues, with Emiratis – and other GCC nationals – being given preference when it comes to employment, even though they show scant regard for work and learning on the job. Unions are generally banned and workers with any labour issues are advised to be in touch with the Ministry of Labour, instead of protesting or refusing to work. Abuse of blue collar workers by their employers, is rife. Migrant employees often complain of poor workplace safety and wages based on nationality, although this is being slowly addressed.
Beyond directly sponsoring educational initiatives, the Emirates Foundation for Philanthropy is funding major research initiatives into Emiratisation through competitive research grants, allowing universities such as United Arab Emirates University or Dubai School of Government to build and disseminate expertise on the topic.
Academics working on various aspects of Emiratisation include "Paul Dyer"[12] and "Natasha Ridge" from Dubai School of Government, "Ingo Forstenlechner"[13] from United Arab Emirates University, "Kasim Randaree" from the British University of Dubai and Paul Knoglinger from the FHWien.
Investment.
The stock market capitalisation of listed companies in the UAE was valued at $109.9 billion in October 2012 by Bloomberg.

</doc>
<doc id="31855" url="http://en.wikipedia.org/wiki?curid=31855" title="Geography of Uzbekistan">
Geography of Uzbekistan

Uzbekistan is a country of Central Asia, located north of Turkmenistan and Afghanistan. With an area of 447,000 square kilometers (approximately the size of Spain or California), Uzbekistan stretches 1,425 kilometers from west to east and 930 kilometers from north to south. Bordering Turkmenistan to the southwest, Kazakhstan to the north, and Tajikistan and Kyrgyzstan to the south and east, Uzbekistan is not only one of the larger Central Asian states but also the only Central Asian state to border all of the other four. Uzbekistan also shares a short border with Afghanistan to the south. As the Caspian Sea is an inland sea with no direct link to the oceans, Uzbekistan is one of only two "doubly landlocked" countries—countries completely surrounded by other landlocked countries. The other is Liechtenstein.
Topography and drainage.
The physical environment of Uzbekistan is diverse, ranging from the flat, desert topography that comprises almost 80% of the country's territory to mountain peaks in the east reaching about 4,500 m above sea level. The southeastern portion of Uzbekistan is characterized by the foothills of the Tian Shan mountains, which rise higher in neighboring Kyrgyzstan and Tajikistan and form a natural border between Central Asia and China. The vast Qizilqum (Turkic for "red sand"—Russian spelling Kyzyl Kum) Desert, shared with southern Kazakhstan, dominates the northern lowland portion of Uzbekistan. The most fertile part of Uzbekistan, the Fergana Valley, is an area of about 21,440 km2 directly east of the Qizilqum and surrounded by mountain ranges to the north, south, and east. The western end of the valley is defined by the course of the Syr Darya, which runs across the northeastern sector of Uzbekistan from southern Kazakhstan into the Qizilqum. Although the Fergana Valley receives just 100 to of rainfall per year, only small patches of desert remain in the center and along ridges on the periphery of the valley.
Water resources, which are unevenly distributed, are in short supply in most of Uzbekistan. The vast plains that occupy two-thirds of Uzbekistan's territory have little water, and there are few lakes. The two largest rivers feeding Uzbekistan are the Amu Darya and the Syr Darya, which originate in the mountains of Tajikistan and Kyrgyzstan, respectively. These rivers form the two main river basins of Central Asia; they are used primarily for irrigation, and several artificial canals have been built to expand the supply of arable land in the Fergana Valley and elsewhere. During the Soviet Era, a plan was devised in which Kyrgyzstan and Tajikistan provided water from these two rivers to Kazakhstan, Turkmenistan, and Uzbekistan in summer, and these three countries provided Kyrgyzstan and Tajikistan with oil and gas during the winter in return. However, this system dissolved after the collapse of the USSR, and a new resource-sharing plan has yet to be put in place. According to the International Crisis Group, this situation could lead to irreparable regional destabilization if it is not resolved. A shallow lake, Sarygamysh Lake, sits on the border with Turkmenistan.
Another important feature of Uzbekistan's physical environment is the significant seismic activity that dominates much of the country. Indeed, much of Uzbekistan's capital city, Tashkent, was destroyed in a major earthquake in 1966, and other earthquakes have caused significant damage before and since the Tashkent disaster. The mountain areas are especially prone to earthquakes.
Climate.
Uzbekistan's climate is classified as continental, with hot summers and cool winters. Summer temperatures often surpass 40 °C; winter temperatures average about -2 °C, but may fall as low as -40 °C. Most of the country also is quite arid, with average annual rainfall amounting to between 100 and and occurring mostly in winter and spring. Between July and September, little precipitation falls, essentially stopping the growth of vegetation during that period of time.
Environmental problems.
Despite Uzbekistan's rich and varied natural environment, decades of environmental neglect in the Soviet Union have combined with skewed economic policies in the Soviet south to make Uzbekistan one of the gravest of the CIS's many environmental crises. The heavy use of agrochemicals, diversion of huge amounts of irrigation water from the two rivers that feed the region, and the chronic lack of water treatment plants are among the factors that have caused health and environmental problems on an enormous scale.
Environmental devastation in Uzbekistan is best exemplified by the catastrophe of the Aral Sea. Because of diversion of the Amu Darya and Syr Darya for cotton cultivation and other purposes, what once was the world's fourth largest inland sea has shrunk in the past thirty years to only about one-third of its 1960 volume and less than half its 1960 geographical size. The desiccation and salinization of the lake have caused extensive storms of salt and dust from the sea's dried bottom, wreaking havoc on the region's agriculture and ecosystems and on the population's health. Desertification has led to the large-scale loss of plant and animal life, loss of arable land, changed climatic conditions, depleted yields on the cultivated land that remains, and destruction of historical and cultural monuments. Every year, many tons of salts reportedly are carried as far as 800 kilometers away. Regional experts assert that salt and dust storms from the Aral Sea have raised the level of particulate matter in the Earth's atmosphere by more than 5%, seriously affecting global climate change.
The Aral Sea disaster is only the most visible indicator of environmental decay, however. The Soviet approach to environmental management brought decades of poor water management and lack of water or sewage treatment facilities; inordinately heavy use of pesticides, herbicides, defoliants, and fertilizers in the fields; and construction of industrial enterprises without regard to human or environmental impact. Those policies present enormous environmental challenges throughout Uzbekistan.
Natural hazards: NA
Environment - current issues:
shrinkage of the Aral Sea is resulting in growing concentrations of chemical pesticides and natural salts; these substances are then blown from the increasingly exposed lake bed and contribute to desertification; water pollution from industrial wastes and the heavy use of fertilizers and pesticides is the cause of many human health disorders; increasing soil salination; soil contamination from agricultural chemicals, including DDT
Environment - international agreements:
<br>"party to:"
Biodiversity, Climate Change, Climate Change-Kyoto Protocol, Desertification, Endangered Species, Environmental Modification, Hazardous Wastes, Ozone Layer Protection, Wetlands
<br>"signed, but not ratified:"
none of the selected agreements
Water pollution.
Large-scale use of chemicals for cotton cultivation, inefficient irrigation systems, and poor drainage systems are examples of the conditions that led to a high filtration of salinized and contaminated water back into the soil. Post-Soviet policies have become even more dangerous; in the early 1990s, the average application of chemical fertilizers and insecticides throughout the Central Asian republics was twenty to twenty-five kilograms per hectare, compared with the former average of three kilograms per hectare for the entire Soviet Union. As a result, the supply of fresh water has received further contaminants. Industrial pollutants also have damaged Uzbekistan's water. In the Amu Darya, concentrations of phenol and oil products have been measured at far above acceptable health standards. In 1989 the minister of health of the Turkmen SSR described the Amu Darya as a sewage ditch for industrial and agricultural waste substances. Experts who monitored the river in 1995 reported even further deterioration.
In the early 1990s, about 60% of pollution control funding went to water-related projects, but only about half of cities and about one-quarter of villages have sewers. Communal water systems do not meet health standards; much of the population lacks drinking water systems and must drink water straight from contaminated irrigation ditches, canals, or the Amu Darya itself.
According to one report, virtually all the large underground fresh-water supplies in Uzbekistan are polluted by industrial and chemical wastes. An official in Uzbekistan's Ministry of Environment estimated that about half of the country's population lives in regions where the water is severely polluted. The government estimated in 1995 that only 230 of the country's 8,000 industrial enterprises were following pollution control standards.
Air pollution.
Poor water management and heavy use of agricultural chemicals also have polluted the air. Salt and dust storms and the spraying of pesticides and defoliants for the cotton crop have led to severe degradation of air quality in rural areas.
In urban areas, factories and auto emissions are a growing threat to air quality. Fewer than half of factory smokestacks in Uzbekistan are equipped with filtration devices, and none has the capacity to filter gaseous emissions. In addition, a high percentage of existing filters are defective or out of operation. Air pollution data for Tashkent, Farghona, and Olmaliq show all three cities exceeding recommended levels of nitrous dioxide and particulates. High levels of heavy metals such as lead, nickel, zinc, copper, mercury, and manganese have been found in Uzbekistan's atmosphere, mainly from the burning of fossil fuels, waste materials, and ferrous and nonferrous metallurgy. Especially high concentrations of heavy metals have been reported in Toshkent Province and in the southern part of Uzbekistan near the Olmaliq Metallurgy Combine. In the mid-1990s, Uzbekistan's industrial production, about 60% of the total for the Central Asian nations excluding Kazakhstan, also yielded about 60% of the total volume of Central Asia's emissions of harmful substances into the atmosphere. Because automobiles are relatively scarce, automotive exhaust is a problem only in Tashkent and Farghona.
Government environmental policy.
The government of Uzbekistan has acknowledged the extent of the country's environmental problems, and it has made a commitment to address them in its Biodiversity Action Plan. But the governmental structures to deal with these problems remain confused and ill defined. Old agencies and organizations have been expanded to address these questions, and new ones have been created, resulting in a bureaucratic web of agencies with no generally understood commitment to attack environmental problems directly. Various nongovernmental and grassroots environmental organizations also have begun to form, some closely tied to the current government and others assuming an opposition stance. For example, environmental issues were prominent points in the original platform of Birlik, the first major opposition movement to emerge in Uzbekistan. By the mid-1990s, such issues had become a key concern of all opposition groups and a cause of growing concern among the population as a whole.
In the first half of the 1990s, many plans were proposed to limit or discourage economic practices that damage the environment. Despite discussion of programs to require payments for resources (especially water) and to collect fines from heavy polluters, however, little has been accomplished. The obstacles are a lack of law enforcement in these areas, inconsistent government economic and environmental planning, corruption, and the overwhelming concentration of power in the hands of a president who shows little tolerance of grassroots activity.
International donors and Western assistance agencies have devised programs to transfer technology and know-how to address these problems. But the country's environmental problems are predominantly the result of abuse and mismanagement of natural resources promoted by political and economic priorities. Until the political will emerges to regard environmental and health problems as a threat not only to the government in power but also to the very survival of Uzbekistan, the increasingly grave environmental threat will not be addressed effectively.
Area and boundaries.
Area:
<br>"total:" 447,400 km²
<br>"land:" 425,400 km²
<br>"water:" 22,000 km²
Area - comparative: slightly larger than California, same size as Morocco, slightly smaller than Sweden
Land boundaries:
<br>"total:" 6,221 km
<br>"border countries:" Afghanistan 137 km, Kazakhstan 2,203 km, Kyrgyzstan 1,099 km, Tajikistan 1,161 km, Turkmenistan 1,621 km
Coastline: 0 km
<br>"note:" Uzbekistan includes the southern portion of the Aral Sea with a 420 km shoreline.
Maritime claims: None. Uzbekistan is one of only two countries (Liechtenstein) in the world that are "doubly landlocked".
Elevation extremes:
<br>"lowest point:" Sariqarnish Kuli -12 m (-39 ft) below sea level.
<br>"highest point:" Khazret Sultan, 4643 m
Resources and land uses.
Natural resources: natural gas, petroleum, coal, gold, uranium, silver, copper, lead and zinc, tungsten, molybdenum
Land use:
<br>"arable land:" 9.61%
<br>"permanent crops:" 0.8%
<br>"other:" 89.58% (2011)
Irrigated land:
<br>41,980 km² (2005)
Total renewable water resources:
48.87 km2 (2011)
Freshwater withdrawal (domestic/industrial/agricultural):
<br>"total:" 56 km2/yr (7%/3%/90%)
<br>per capita: 2,113 m3/yr (2005)

</doc>
<doc id="31934" url="http://en.wikipedia.org/wiki?curid=31934" title="USS Indianapolis (CA-35)">
USS Indianapolis (CA-35)

USS "Indianapolis" (CA-35) was a "Portland" class heavy cruiser of the United States Navy. She was named for the city of Indianapolis, Indiana.
She was flagship for Admiral Raymond Spruance while he commanded the Fifth Fleet in battles across the Central Pacific. Her sinking led to the greatest single loss of life at sea in the history of the U.S. Navy. On 30 July 1945, after delivering parts for the first atomic bomb to the United States air base at Tinian, the ship was torpedoed by the Imperial Japanese Navy submarine "I-58", sinking in 12 minutes. Of 1,196 crewmen aboard, approximately 300 went down with the ship.
The remaining 900 faced exposure, dehydration, saltwater poisoning, and shark attacks while floating with few lifeboats and almost no food or water. The Navy learned of the sinking when survivors were spotted four days later by the crew of a PV-1 Ventura on routine patrol. Only 317 survived.
Construction.
"Indianapolis" was the second of two ships in the "Portland" class; third class of "treaty cruisers" constructed by the United States Navy following the Washington Naval Treaty of 1922, following the two vessels of the "Pensacola" class ordered in 1926 and the six of the "Northampton"-class ordered in 1927. Ordered for the U.S. Navy in fiscal year 1930, "Indianapolis" was originally a light cruiser, given the hull classification symbol CL-35, being re-designated a heavy cruiser with the symbol CA-35 on 1 July 1931, in accordance with the London Naval Treaty.
As built, the "Portland" cruisers were to be 610 ft in length overall, and 592 ft long at the waterline, 64 ft abeam, and with a draft of 21 ft (24 ft maximum). They were designed for a standard displacement of 10258 t, and a full-load displacement of 12755 t. However, when completed she did not reach this weight, displacing 9800 t. The ship had two distinctive raked funnels, a tripod foremast, and a small tower and pole mast aft. In 1943, light tripods were added forward of the second funnel on each ship, and a prominent Naval director was installed aft.
The ship had four propeller shafts and four Parsons GT geared turbines and eight Yarrow boilers. The 107000 shp gave a design speed of 32 knots She was designed for a range of 10000 nmi at 15 knots. She rolled badly until fitted with a bilge keel.
The cruiser had nine Mark 9 8"/55 caliber guns in three triple mounts, a superfiring pair fore and one aft. For anti-aircraft defense, she had eight 5"/25 caliber guns and two QF 3 pounder Hotchkiss guns. In 1945, she received 24 Bofors 40mm guns, arrayed in six quad mounts. Both ships were upgraded with twelve Oerlikon 20 mm cannons. No torpedo tubes were fitted on her.
The "Portland" class was originally had 1 in armor for deck and side protection, but in construction they were given belt armor between 5 in (around the magazines) and 3.25 in in thickness. Armor on the bulkheads was between 2 in and 5.75 in; that on the deck was 2.5 in, the barbettes 1.5 in, the gunhouses 2.5 in, and the conning tower 1.25 in.
"Portland"-class cruisers were to be outfitted as fleet flagships, with space for an admiral and his staff. The class also had an aircraft catapult amidships. They could carry four aircraft. The total crew varied, with a regular designed complement of 807, a wartime complement of 952, which could increase to 1,229 when the cruiser was a fleet flagship.
"Indianapolis" was laid down by New York Shipbuilding Corporation on 31 March 1930. The hull and machinery were provided by the builder. "Indianapolis" was launched on 7 November 1931 and commissioned on 15 November 1932. She was the second ship named for Indianapolis, Indiana following the cargo ship of the same name in 1918. She was sponsored by Lucy Taggart, daughter of former Mayor of Indianapolis Thomas Taggart.
Interwar period.
Under her first captain, John M. Smeallie, "Indianapolis" undertook her shakedown cruise through the Atlantic and into Guantánamo Bay until 23 February 1932. "Indianapolis" then transited the Panama Canal Zone for training off the Chilean coast. After overhaul at Philadelphia Navy Yard, she sailed to Maine to embark President Franklin Delano Roosevelt at Campobello Island in New Brunswick on 1 July 1933. Getting underway the same day, "Indianapolis" arrived at Annapolis, Maryland on 3 July. She hosted six members of the Cabinet along with Roosevelt during its stay there. After disembarking Roosevelt, she departed Annapolis on 4 July, and steamed for Philadelphia Navy Yard.
On 6 September, she embarked Secretary of the Navy Claude A. Swanson for an inspection of the Navy in the Pacific. "Indianapolis" toured the Canal Zone, Hawaii, and installations in San Pedro and San Diego. Swanson disembarked on 27 October. On 1 November 1933, she became flagship of Scouting Force 1, and maneuvered with the force off Long Beach, California. She departed on 9 April 1934 and arrived at New York City and embarked Roosevelt a second time, for a naval review. She returned to Long Beach on 9 November 1934 for more training with the Scouting Force. She remained flagship of Scouting Force 1 until 1941. On 18 November 1936, she embarked Roosevelt a third time at Charleston, South Carolina, and conducted a goodwill cruise to South America with him. She visited Rio de Janeiro, Brazil, Buenos Aires, Argentina, and Montevideo, Uruguay for state visits before returning to Charleston and disembarking Roosevelt's party on 15 December.
World War II.
On 7 December 1941, "Indianapolis" was conducting a mock bombardment at Johnston Atoll during the Japanese attack on Pearl Harbor. "Indianapolis" was absorbed into Task Force 12 and searched for the Japanese carriers responsible for the attack, though the force did not locate them. She returned to Pearl Harbor on 13 December and joined Task Force 11.
New Guinea campaign.
With the task force, she steamed to the South Pacific, to 350 mi south of Rabaul, New Britain, escorting the aircraft carrier "Lexington". Late in the afternoon of 20 February 1942, the American ships were attacked by 18 Japanese aircraft. Of these, 16 were shot down by aircraft from "Lexington" and the other two were destroyed by anti-aircraft fire from the ships.
On 10 March, the task force, reinforced by another force centered on the carrier "Yorktown", attacked Lae and Salamaua, New Guinea, where the Japanese were marshaling amphibious forces. Attacking from the south through the Owen Stanley mountain range, the U.S. air forces surprised and inflicted heavy damage on Japanese warships and transports, losing few aircraft. "Indianapolis" returned to Mare Island shipyard for a refit before escorting a convoy to Australia.
Aleutian Islands campaign.
The "Indianapolis" then headed for the North Pacific to support American units in the Battle of the Aleutian Islands. On 7 August, "Indianapolis" and the task force attacked Kiska Island, a Japanese staging area. Although fog hindered observation, "Indianapolis" and other ships fired their main guns into the bay. Floatplanes from the cruisers reported Japanese ships sunk in the harbor and damage to shore installations. After 15 minutes, Japanese shore batteries returned fire before being destroyed by the ships' main guns. Japanese submarines approaching the force were depth-charged by American destroyers. Japanese seaplanes made an ineffective bombing attack. In spite of a lack of information on the Japanese forces, the operation was considered a success. U.S. forces later occupied Adak Island, providing a naval base further from the Dutch Harbor on Unalaska Island.
1943 operations.
In January 1943, "Indianapolis", supported a landing and occupation on Amchitka, part of an Allied island hopping strategy in the Aleutian Islands.
On the evening of 19 February, "Indianapolis" led two destroyers on a patrol southwest of Attu Island, searching for Japanese ships trying to reinforce Kiska and Attu. She intercepted a Japanese cargo ship, "Akagane Maru" 3100-tons laden with troops, munitions, and supplies. The cargo ship tried to reply to the radio challenge but was shelled by "Indianapolis". "Akagane Maru" exploded and sank with all hands. Through mid-1943, "Indianapolis" remained near the Aleutian Islands escorting American convoys and providing shore bombardments supporting amphibious assaults. In May, the Allies captured Attu, then turned on Kiska, thought to be the final Japanese holdout in the Aleutians. Allied landings there began on 15 August but the Japanese had already abandoned the Aleutian Islands, unbeknownst to the Allies.
After refitting at Mare Island, "Indianapolis" moved to Hawaii as flagship of Vice Admiral Raymond A. Spruance, commanding the 5th Fleet. She sortied from Pearl Harbor on 10 November with the main body of the Southern Attack Force for Operation "Galvanic", the invasion of the Gilbert Islands. On 19 November, "Indianapolis" bombarded Tarawa Atoll and next day pounded Makin (see "Battle of Makin"). The ship then returned to Tarawa as fire-support for the landings. Her guns shot down an enemy plane and shelled enemy strongpoints as landing parties fought Japanese defenders in the bloody and costly battle of Tarawa. She continued this role until the leveled island was secure three days later. The conquest of the Marshall Islands followed victory in the Gilberts. "Indianapolis" was again 5th Fleet flagship.
1944.
The cruiser met other ships of her task force at Tarawa, and on D-Day minus 1, 31 January 1944, she was one of the cruisers which bombarded the islands of Kwajalein Atoll. The shelling continued on D-Day with "Indianapolis" silencing two enemy shore batteries. Next day she obliterated a blockhouse and other shore installations and supported advancing troops with a creeping barrage. The ship entered Kwajalein Lagoon on 4 February, and remained until resistance disappeared. (See "Battle of Kwajalein".)
In March and April, "Indianapolis", still flagship of the 5th Fleet, attacked the Western Carolines. Carrier planes at the Palau Islands on 30–31 March sank three destroyers, 17 freighters, five oilers and damaged 17 other ships. Airfields were bombed and surrounding water mined. Yap and Ulithi were struck on the 31st and Woleai on 1 April. Japanese planes attacked but were driven off without damaging the American ships. "Indianapolis" shot down her second plane, a torpedo bomber, and the Japanese lost 160 planes, including 46 on the ground. These attacks prevented Japanese forces from the Carolines from interfering with the U.S. landings on New Guinea.
In June, the 5th Fleet was busy with the assault on the Mariana Islands. Raids on Saipan began with carrier-based planes on 11 June, followed by surface bombardment, in which "Indianapolis" had a major role, from 13 June. (See "Battle of Saipan".) On D-Day, 15 June, Admiral Spruance heard that battleships, carriers, cruisers, and destroyers were headed south to relieve threatened garrisons in the Marianas. Since amphibious operations at Saipan had to be protected, Admiral Spruance could not withdraw too far. Consequently, a fast carrier force was sent to meet this threat while another force attacked Japanese air bases on Iwo Jima and Chichi Jima in the Bonin and Volcano Islands, bases for potential enemy air attacks.
A combined U.S. fleet fought the Japanese on 19 June in the Battle of the Philippine Sea. Japanese carrier planes, which hoped to use the airfields of Guam and Tinian to refuel and rearm, were met by carrier planes and the guns of the Allied escorting ships. That day, the U.S. Navy destroyed a reported 426 Japanese planes while losing 29. "Indianapolis" shot down one torpedo plane. This day of aerial combat became known as the "Marianas Turkey Shoot". With Japanese air opposition wiped out, the U.S. carrier planes sank "Hiyō", two destroyers, and one tanker and damaged others. Two other carriers, "Taihō" and "Shōkaku", were sunk by submarines.
"Indianapolis" returned to Saipan on 23 June to resume fire support and six days later moved to Tinian to attack shore installations (see "Battle of Tinian"). Meanwhile, Guam had been taken, and "Indianapolis" was the first ship to enter Apra Harbor since early in the war. The ship operated in the Marianas for the next few weeks, then moved to the Western Carolines where further landings were planned. From 12 to 29 September, she bombarded the Peleliu in the Palau Group, before and after the landings (see "Battle of Peleliu"). She then sailed to Manus Island in the Admiralty Islands where she operated for 10 days before returning to the Mare Island Navy Yard.
1945.
Overhauled, "Indianapolis" joined Vice Admiral Marc A. Mitscher's fast carrier task force on 14 February 1945. Two days later, the task force launched an attack on Tokyo to cover the landings on Iwo Jima, scheduled for 19 February. This was the first carrier attack on Japan since the Doolittle Raid. The mission was to destroy Japanese air facilities and other installations in the "Home Islands". The fleet achieved complete tactical surprise by approaching the Japanese coast under cover of bad weather. The attacks were pressed home for two days. The American Navy lost 49 carrier planes while claiming 499 enemy planes, a 10:1 kill/loss ratio. The task force also sank a carrier, nine coastal ships, a destroyer, two destroyer escorts, and a cargo ship. They destroyed hangars, shops, aircraft installations, factories, and other industrial targets.
Immediately after the strikes, the task force raced to Bonin to support the landings on Iwo Jima. The ship remained there until 1 March, protecting the invasion ships and bombarding targets in support of the landings. "Indianapolis" returned to Admiral Mitscher's task force in time to strike Tokyo again on 25 February and Hachijō off the southern coast of Honshū the following day. Although weather was extremely bad, the American force destroyed 158 planes and sank five small ships while pounding ground installations and destroying trains.
The next target for the U.S. forces was Okinawa in the Ryukyu Islands, which were in range of aircraft from the Japanese mainland. The fast carrier force was tasked with attacking airfields in southern Japan until they were incapable of launching effective airborne opposition to the impending invasion. The fast carrier force departed for Japan from Ulithi on 14 March. On 18 March, it launched an attack from a position 100 mi southeast of the island of Kyūshū. The attack targeted airfields on Kyūshū as well as ships of the Japanese fleet in the harbors of Kobe and Kure on southern Honshū. The Japanese located the American task force on 21 March, sending 48 planes to attack the ships. Twenty-four fighters from the task force intercepted and shot down all the Japanese aircraft.
Pre-invasion bombardment of Okinawa began on 24 March. "Indianapolis" spent 7 days pouring 8 in shells into the beach defenses. During this time, enemy aircraft repeatedly attacked the American ships. "Indianapolis" shot down six planes and damaged two others. On 31 March, the ship's lookouts spotted a Japanese fighter as it emerged from the morning twilight and roared at the bridge in a vertical dive. The ship's 20 mm guns opened fire, but within 15 seconds, the plane was over the ship. Tracers converged on it, causing it to swerve, but the enemy pilot managed to release his bomb from a height of 25 ft, crashing his plane into the sea near the port stern. The bomb plummeted through the deck, into the crew's mess hall, down through the berthing compartment, and through the fuel tanks before crashing through the keel and exploding in the water underneath. The concussion blew two gaping holes in the keel which flooded nearby compartments, killing nine crewmen. The ship's bulkheads prevented any progressive flooding. The "Indianapolis", settling slightly by the stern and listing to port, steamed to a salvage ship for emergency repairs. Here, inspection revealed that her propeller shafts were damaged, her fuel tanks ruptured, and her water-distilling equipment ruined. But the "Indianapolis" commenced the long trip across the Pacific to Mare Island under her own power.
Loss.
After major repairs and an overhaul, "Indianapolis" received orders to proceed to Tinian island, carrying parts and the enriched uranium (about half of the world's supply of Uranium-235 at the time) for the atomic bomb "Little Boy", which would later be dropped on Hiroshima. "Indianapolis" departed San Francisco on 16 July 1945. Arriving at Pearl Harbor on 19 July, she raced on unaccompanied, delivering the atomic weapon components to Tinian on 26 July. 
"Indianapolis" was then sent to Guam where a number of the crew who had completed their tours of duty were replaced by other sailors. Leaving Guam on 28 July, she began sailing toward Leyte where her crew was to receive training before continuing on to Okinawa to join Vice Admiral Jesse B. Oldendorf's Task Force 95. 
At 00:14 on 30 July, she was struck on her starboard bow by two Type 95 torpedoes from the Japanese submarine "I-58", under the command of Mochitsura Hashimoto. The explosions caused massive damage. The "Indianapolis" took on a heavy list, and settled by the bow. Twelve minutes later, she rolled completely over, then her stern rose into the air, and she plunged down. Some 300 of the 1,196 crewmen went down with the ship. With few lifeboats and many without lifejackets, the remainder of the crew were set adrift.
Navy command had no knowledge of the ship's sinking until survivors were spotted three and a half days later. At 10:25 on 2 August a PV-1 Ventura flown by Lieutenant Wilbur "Chuck" Gwinn and copilot Lieutenant Warren Colwell spotted the men adrift while on a routine patrol flight. Of the 880 who survived the sinking, only 321 men came out of the water alive; 317 ultimately survived. They suffered from lack of food and water (some found rations such as Spam and crackers amongst the debris), exposure to the elements (hypothermia, dehydration, hypernatremia, photophobia, starvation and dementia), severe desquamation, and shark attacks, while some killed themselves or other survivors in various states of delirium and hallucinations. The Discovery Channel stated in Shark Week episodes "Ocean of Fear" that the "Indianapolis" sinking resulted in the most shark attacks on humans in history, and attributes the attacks to the oceanic whitetip shark species. Tiger sharks might have also killed some sailors. The same show attributed most of the deaths on "Indianapolis" to exposure, salt poisoning and thirst, with the dead being dragged off by sharks.
Gwinn immediately dropped a life raft and a radio transmitter. All air and surface units capable of rescue operations were dispatched to the scene at once. A PBY Catalina seaplane under the command of Lieutenant R. Adrian Marks was dispatched to lend assistance and report. En route to the scene, Marks overflew USS "Cecil J. Doyle" and alerted her captain, future U.S. Secretary of the Navy W. Graham Claytor, Jr., of the emergency. On his own authority, Claytor decided to divert to the scene.
Arriving hours ahead of "Doyle", Marks' crew began dropping rubber rafts and supplies. Having seen men being attacked by sharks, Marks disobeyed standing orders and landed on the open sea. He began taxiing to pick up the stragglers and lone swimmers who were at the greatest risk of shark attack. Learning the men were the crew of "Indianapolis", he radioed the news, requesting immediate assistance. "Doyle" responded while en route. When Marks' plane was full, survivors were tied to the wings with parachute cord, damaging the wings so that the plane would never fly again and had to be sunk. Marks and his crew rescued 56 men that day.
The "Doyle" was the first vessel on the scene. Homing on Marks's Catalina in total darkness, "Doyle" halted to avoid killing or further injuring survivors, and began taking Marks' survivors aboard. Disregarding the safety of his own vessel, Captain Claytor pointed his largest searchlight into the night sky to serve as a beacon for other rescue vessels. This beacon was the first indication to most survivors that rescuers had arrived.
The destroyers "Helm", "Madison", and "Ralph Talbot" were ordered to the rescue scene from Ulithi, along with destroyer escorts "Dufilho", "Bassett", and "Ringness" of the Philippine Sea Frontier. They continued their search for survivors until 8 August.
Two of the rescued survivors, and , died in August 1945.
Navy failure to learn of the sinking.
Operations plotting boards were kept at the Headquarters of Commander Marianas on Guam and of the Commander Philippine Sea Frontier on Leyte. On these boards, the positions of all vessels of which the headquarters was concerned were plotted. However, for ships as large as the "Indianapolis", it was assumed that they would reach their destinations on time, unless reported otherwise. Therefore, their positions were based on predictions, and not on reports. On 31 July, when she should have arrived at Leyte, "Indianapolis" was removed from the board in the headquarters of Commander Marianas. She was also recorded as having arrived at Leyte by the headquarters of Commander Philippine Sea Frontier. Lieutenant Stuart B. Gibson, the Operations Officer under the Port Director, Tacloban, was the officer responsible for tracking the movements of "Indianapolis". The vessel's failure to arrive on schedule was known at once to Lieutenant Gibson, who failed to investigate the matter and made no immediate report of the fact to his superiors.
The "Indianapolis" sent distress calls before sinking. Three stations received the signals; however, none acted upon the call. One commander was drunk, another had ordered his men not to disturb him and a third thought it was a Japanese trap. For a long time the Navy denied that a distress call had been sent. The receipt of the call came to light only after the release of declassified records.
Immediately prior to the attack, the seas had been moderate, the visibility fluctuating but poor in general, and "Indianapolis" had been steaming at 17 kn. When the ship did not reach Leyte on the 31st, as scheduled, no report was made that she was overdue. This omission was due to a misunderstanding of the Movement Report System.
Captain Charles McVay.
Captain Charles B. McVay III, who had commanded "Indianapolis" since November 1944, survived the sinking and was with those rescued days later. In November 1945, he was court-martialed and convicted of "hazarding his ship by failing to zigzag." Several things about the court-martial were controversial. There was evidence that the Navy itself had placed the ship in harm's way, in that McVay's orders were to "zigzag at his discretion, weather permitting." Further, Mochitsura Hashimoto, commander of "I-58", testified that zigzagging would have made no difference.
Fleet Admiral Chester Nimitz remitted McVay's sentence and restored him to active duty. McVay retired in 1949 as a Rear Admiral. While many of "Indianapolis"‍‍ '​‍s survivors said McVay was not to blame for the sinking, the families of some of the men who died thought otherwise – "Merry Christmas! Our family's holiday would be a lot merrier if you hadn't killed my son", read one piece of mail. The guilt that was placed on his shoulders mounted until he committed suicide in 1968, using his Navy-issue revolver. McVay was discovered on his front lawn with a toy sailor in one hand. He was 70 years old. The day the Indianapolis was sunk was his 47th birthday.
In October 2000, the United States Congress passed a resolution that Captain McVay's record should state that "he is exonerated for the loss of "Indianapolis"." President Bill Clinton signed the resolution. The resolution noted that although several hundred ships of the U.S. Navy were lost in combat in World War II, McVay was the only captain to be court-martialed for the sinking of his ship.
In July 2001, the Secretary of the Navy ordered McVay's record cleared of all wrongdoing.
The wreck.
The exact location of "Indianapolis" is in the Philippine Sea – the coordinates given in this article are for the general area. In July–August 2001, an expedition sought to find the wreckage through the use of side-scan sonar and underwater cameras mounted on a remotely operated vehicle. Four "Indianapolis" survivors accompanied the expedition, which was not successful. In June 2005, a second expedition was mounted to find the wreck. "National Geographic" covered the story and released it in July. Submersibles were launched to find any sign of wreckage. The only objects ever found, which have not been confirmed to have belonged to "Indianapolis", were numerous pieces of metal of varying size found in the area of the reported sinking position (this was included in the National Geographic program "Finding of the USS Indianapolis").
Memorials.
The USS "Indianapolis" Museum had its grand opening on 7 July 2007, with its gallery in the Indiana War Memorial Museum at the Indiana World War Memorial Plaza.
The was dedicated on 2 August 1995. It is located on the Canal Walk in Indianapolis. The heavy cruiser is depicted in limestone and granite and sits adjacent to the downtown canal. The crewmembers' names are listed on the monument, with special notations for those who lost their lives.
In May 2011, I-465 around Indianapolis was renamed the USS Indianapolis Memorial Highway.
Some material relating to "Indianapolis" is held by the Indiana State Museum. Her bell and a commissioning pennant are located at the Heslar Naval Armory; the bell had been removed to save weight before her final cruise.
The swim training center at United States Navy Recruit Training Command is named USS "Indianapolis".
Popular culture.
References to the "Indianapolis" sinking and aftermath have been adapted to film, stage, television, and popular culture. The incident itself was the subject of 1991 made-for-television movie "Mission of the Shark: The Saga of the USS Indianapolis", with Stacy Keach portraying Captain Charles Butler McVay III.
Arguably the most well known fictional reference to the events occurs in the 1975 thriller film "Jaws" in a monologue by actor Robert Shaw, whose character Quint is depicted as a survivor of the "Indianapolis" sinking. The monologue particularly focuses on the numerous deaths caused by shark attacks after the sinking. John Milius was specifically brought into the production to write lines for this scene and he based them on survivor stories. However, there are several historical inaccuracies in the monologue: the speech states the date of the sinking as 29 June 1945, when the ship was actually sunk on 30 July, that they were spotted at noon of the fifth day rather than the third day, that 1100 men went into the water and 316 came out (nearer 900 went in and 321 came out, of whom 317 survived) and that because of the secrecy of the atom bomb mission no distress call was broadcast, while declassified Navy documents prove the contrary.
References.
</dl>
</dl>

</doc>
<doc id="32003" url="http://en.wikipedia.org/wiki?curid=32003" title="Umberto Eco">
Umberto Eco

Umberto Eco, (]; born 5 January 1932) is an Italian semiotician, essayist, philosopher, literary critic, and novelist. He is best known for his groundbreaking 1980 historical mystery novel "Il nome della rosa" ("The Name of the Rose"), an intellectual mystery combining semiotics in fiction, biblical analysis, medieval studies and literary theory. He has since written further novels, including "Il pendolo di Foucault" ("Foucault's Pendulum") and "L'isola del giorno prima" ("The Island of the Day Before"). His most recent novel "Il cimitero di Praga" ("The Prague Cemetery"), released in 2010, was a best-seller.
Eco has also written academic texts, children's books and many essays. He is founder of the "Dipartimento di Comunicazione" (Department of Media Studies) at the University of the Republic of San Marino, President of the "Scuola Superiore di Studi Umanistici" (Graduate School for the Study of the Humanities), University of Bologna, member of the Accademia dei Lincei (since November 2010), and an Honorary Fellow of Kellogg College, University of Oxford.
Biography.
Eco was born in the city of Alessandria in the region of Piedmont in northern Italy. His father, Giulio, one of thirteen children, was an accountant before the government called upon him to serve in three wars. During World War II, Umberto and his mother Giovanna moved to a small village in the Piedmontese mountainside. Eco received a Salesian education and has made references to the order and its founder in his works and interviews. His family name is supposedly an acronym of "ex caelis oblatus" (from Latin: a gift from the heavens), which was given to his grandfather (a foundling) by a city official.
Umberto's father urged him to become a lawyer, but he entered the University of Turin to take up medieval philosophy and literature, writing his thesis on Thomas Aquinas and earning a Laurea degree in philosophy in 1954. During his university studies, Eco stopped believing in God and left the Roman Catholic Church. After that, Eco worked as a cultural editor for the state broadcasting station Radiotelevisione Italiana (RAI) and also lectured at the University of Turin (1956–1964). A group of avant-garde artists, painters, musicians, writers, whom he had befriended at RAI (Gruppo 63), became an important and influential component in Eco's future writing career. This was especially true after the publication of his first book in 1956, "Il problema estetico in San Tommaso", which was an extension of his Laurea thesis. This also marked the beginning of his lecturing career at his alma mater.
In September 1962, he married Renate Ramge, a German art teacher with whom he has a son and a daughter. He divides his time between an apartment in Milan and a vacation house near Urbino. He has a 30,000 volume library in the former and a 20,000 volume library in the latter. He was a visiting professor at Columbia University several times in the 1980s and 1990s. In 1992–1993 Eco was the Norton professor at Harvard University. On May 8, 1993, Eco received an honorary Doctor of Humane Letters (D.H.L.) from Indiana University Bloomington in recognition of his over fifteen-year association with the university's Research Center for Language and Semiotic Studies. Six books that were authored, co-authored, or co-edited by Eco were published by the Indiana University Press. Additionally, he frequently collaborated with his friend Thomas Sebeok, noted semiotician and Distinguished Professor of Linguistics at Indiana University. On May 23, 2002, Eco received an honorary Doctor of Letters (D.Litt.) from Rutgers University in New Brunswick, New Jersey. In 2009, the University of Belgrade in Serbia awarded him an honorary doctorate. Eco is a member of the Italian skeptic organization "Comitato Italiano per il Controllo delle Affermazioni sulle Pseudoscienze" (Italian Committee for the Investigation of Claims of the Pseudoscience) CICAP.
Professional and academic activity.
Studies on medieval aesthetics.
In 1959, he published his second book, "Sviluppo dell'estetica medievale" ("The Development of Medieval Aesthetics"), which established Eco as a formidable thinker in medieval philosophy and proved his literary worth to his father. After 18 months' military service in the Italian Army, he left RAI in 1959 to become the senior non-fiction editor of the Bompiani publishing house in Milan, a position he occupied until 1975. Eco's work on medieval aesthetics stressed the distinction between theory and practice. About the Middle Ages, he wrote that there was "a geometrically rational schema of what beauty ought to be, and on the other [hand] the unmediated life of art with its dialectic of forms and intentions", the two cut off from one another as if by a pane of glass. Eco's work in literary theory has changed focus over time. Initially, he was one of the pioneers of "Reader Response".
Literary criticism.
Eco began seriously developing his ideas on the "open" text and on semiotics, writing many essays on these subjects, and in 1962 he published "Opera aperta" (translated into English as "The Open Work"). In it, Eco argued that literary texts are fields of meaning, rather than strings of meaning, that they are understood as open, internally dynamic and psychologically engaged fields. Literature which limits one's potential understanding to a single, unequivocal line, the "closed text", remains the least rewarding, while texts that are the most active between mind and society and life (open texts) are the liveliest and best—although valuation terminology is not his primary area of focus. Eco emphasizes the fact that words do not have meanings that are simply lexical, but rather, they operate in the context of utterance. I. A. Richards and others said as much, but Eco draws out the implications for literature from this idea. He also extended the axis of meaning from the continually deferred meanings of words in an utterance to a play between expectation and fulfilment of meaning. Eco comes to these positions through study of language and from semiotics, rather than from psychology or historical analysis (as did theorists such as Wolfgang Iser, on the one hand, and Hans-Robert Jauss, on the other).
Studies on media culture.
From the late '50s till the late '60s, before his semiotic turn, Eco engaged in studies on mass media and media culture, which were published in various newspapers and journals. According to some these studies were influential although he did not develop a full-scale theory in this field.
His short 1961 essay "Fenomenologia di Mike Bongiorno" ("Phenomenology of Mike Bongiorno", on the most popular quiz show host in Italy, Mike Bongiorno), received much notoriety among the general public and has drawn endless questions by journalists at every public appearance by Eco; the essay was later included in the collection "Diario minimo" (1963). His book "Apocalittici e integrati" (1964) analyzes the phenomenon of mass communication from a sociological perspective.
In 1967 he gave the influential lecture "Towards a Semiological Guerrilla Warfare", which coined the influential term "semiological guerrilla," and influenced the theorization of guerrilla tactics against mainstream mass media culture, such as guerrilla television and culture jamming. Among the expressions used in the essay are "communications guerrilla warfare" and "cultural guerrilla." The essay was later included in Eco's book "Faith in Fakes".
Semiotics.
Eco founded and developed one of the most important approaches in contemporary semiotics, usually referred to as interpretative semiotics. The main books in which he elaborates his theory are "La struttura assente" (1962; literally: "The Absent Structure"), "A Theory of Semiotics" (1975), "The Role of the Reader" (1979), "Semiotics and Philosophy of Language" (1984), "The Limits of Interpretation" (1990) and "Kant and the Platypus" (1997).
Eco co-founded "Versus: Quaderni di studi semiotici" (known as "VS "among Italian academics), an influential semiotic journal. "VS" has become an important publication platform for many scholars whose work is related to signs and signification. The journal's foundation and activities have contributed to the growing influence of semiotics as an academic field in its own right, both in Italy and in the rest of Europe. Most of the well-known European semioticians, including Eco, A. J. Greimas, Jean-Marie Floch, and Jacques Fontanille, as well as philosophers and linguists like John Searle and George Lakoff, have published original articles in "VS". His work with Serbian and Russian scholars and writers included thought on Milorad Pavic and a meeting with Alexander Genis.
Anthropology.
In 1988, at the University of Bologna, Eco created an unusual program called "Anthropology of the West" from the perspective of non-Westerners (African and Chinese scholars), as defined by their own criteria. Eco developed this transcultural international network based on the idea of Alain le Pichon in West Africa. The Bologna program resulted in a first conference in Guangzhou, China, in 1991 entitled "Frontiers of Knowledge." The first event was soon followed by an Itinerant Euro-Chinese seminar on "Misunderstandings in the Quest for the Universal" along the silk trade route from Canton to Beijing. The latter culminated in a book entitled "The Unicorn and the Dragon", which discussed the question of the creation of knowledge in China and in Europe. Scholars contributing to this volume were from China, including Tang Yijie, Wang Bin and Yue Daiyun, as well as from Europe: Furio Colombo, Antoine Danchin, Jacques Le Goff, Paolo Fabbri, Alain Rey.
In 2000 a seminar in Timbuktu (Mali), was followed by another gathering in Bologna to reflect on the conditions of reciprocal knowledge between East and West. This in turn gave rise to a series of conferences in Brussels, Paris, and Goa, culminating in Beijing in 2007. The topics of the Beijing conference were "Order and Disorder","New Concepts of War and Peace", "Human Rights" and "Social Justice and Harmony". Eco presented the opening lecture. Among those giving presentations were anthropologists Balveer Arora, Varun Sahni, and Rukmini Bhaya Nair from India, Moussa Sow? from Africa, Roland Marti and Maurice Olender from Europe, Cha Insuk from Korea, and Huang Ping and Zhao Tinyang from China. Also on the program were scholars from the fields of law and science including Antoine Danchin, Ahmed Djebbar and Dieter Grimm. Eco's interest in East-West dialogue to facilitate international communication and understanding also correlates with his related interest in the international auxiliary language Esperanto.
Style and works.
Themes.
Eco's fiction has enjoyed a wide audience around the world, with many translations. His novels are full of subtle, often multilingual, references to literature and history and his dense, intricate plots tend to take dizzying turns. Eco's work illustrates the concept of intertextuality, or the inter-connectedness of all literary works. Eco cites James Joyce and Jorge Luis Borges as the two modern authors who have influenced his work the most.
Selected narrative works.
Eco employed his education as a medievalist in his first novel "The Name of the Rose" (1980), a historical mystery set in a 14th-century monastery. Franciscan friar William of Baskerville, aided by his assistant Adso, a Benedictine novice, investigates a series of murders at a monastery that is to host an important religious debate. The novel contains many direct or indirect metatextual references to other sources, requiring the detective work of the reader to 'solve'. The title is unexplained in the book. As a symbol, the rose is ubiquitous enough to not confer any single meaning. There is a tribute to Jorge Luis Borges, a major influence on Eco, in the blind monk and librarian Jorge of Burgos: Borges, like Jorge, lived a celibate life consecrated to his passion for books, and also went blind in later life. William of Baskerville is a logically-minded Englishman who is a monk and a detective, and his name evokes both William of Ockham and Sherlock Holmes (by way of "The Hound of the Baskervilles"). Several passages describing him are strongly reminiscent of Sir Arthur Conan Doyle's description of Sherlock Holmes. The underlying mystery of the murder is borrowed from the "Arabian Nights". "The Name of the Rose" was later made into a motion picture starring Sean Connery, F. Murray Abraham, Christian Slater and Ron Perlman which employs the plot but not the philosophical and historical themes from the novel.
In "Foucault's Pendulum" (1988), three under-employed editors who work for a minor publishing house decide to amuse themselves by inventing a conspiracy theory. Their conspiracy, which they call "The Plan", is about an immense and intricate plot to take over the world by a secret order descended from the Knights Templar. As the game goes on, the three slowly become obsessed with the details of this plan. The game turns dangerous when outsiders learn of The Plan, and believe that the men have really discovered the secret to regaining the lost treasure of the Templars.
"The Island of the Day Before" (1994) was Eco's third novel. The book, set in the seventeenth century, is about a man marooned on a ship within sight of an island which he believes is on the other side of the international date-line. The main character is trapped by his inability to swim and instead spends the bulk of the book reminiscing on his life and the adventures that brought him to be marooned.
"Baudolino" was published in 2000. Baudolino is a knight who saves the Byzantine historian Niketas Choniates during the sack of Constantinople in the Fourth Crusade. Claiming to be an accomplished liar, he confides his history, from his childhood as a peasant lad endowed with a vivid imagination, through his role as adopted son of Emperor Frederick Barbarossa, to his mission to visit the mythical realm of Prester John. Throughout his retelling, Baudolino brags of his ability to swindle and tell tall tales, leaving the historian (and the reader) unsure of just how much of his story was a lie.
"The Mysterious Flame of Queen Loana" (2005) is about Giambattista Bodoni, an old bookseller specializing in antiques who emerges from a coma with only some memories to recover his past. Bodoni is pressed to make a very difficult choice, one between his past and his future. He must either abandon his past to live his future or regain his past and sacrifice his future.
"The Prague Cemetery", Eco's sixth novel, was published in 2010. It is the story of a secret agent who "weaves plots, conspiracies, intrigues and attacks, and helps determine the historical and political fate of the European Continent." The book is a narrative of the rise of Modern-day antisemitism, by way of the Dreyfus Affair, The Protocols of the Elders of Zion and other important 19th century events which gave rise to hatred and hostility toward the Jewish people.
Selected bibliography.
Books for children.
</dl>

</doc>
<doc id="32031" url="http://en.wikipedia.org/wiki?curid=32031" title="University of Texas at Austin">
University of Texas at Austin

The University of Texas at Austin, informally UT Austin, UT, University of Texas, or Texas in sports contexts, is a public research university and the flagship institution of The University of Texas System. Founded in 1883 as "The University of Texas," its campus is located in Austin—approximately 1 mi from the Texas State Capitol. The institution has the fifth-largest single-campus enrollment in the nation, with over 50,000 undergraduate and graduate students and over 24,000 faculty and staff. The university has been labeled one of the "Public Ivies," a publicly funded university considered as providing a quality of education comparable to those of the Ivy League.
UT Austin was inducted into the American Association of Universities in 1929, becoming only the third university in the American South to be elected. It is a major center for academic research, with research expenditures exceeding $550 million for the 2013–2014 school year. The university houses seven museums and seventeen libraries, including the Lyndon Baines Johnson Library and Museum and the Blanton Museum of Art, and operates various auxiliary research facilities, such as the J. J. Pickle Research Campus and the McDonald Observatory. Among university faculty are recipients of the Nobel Prize, Pulitzer Prize, the Wolf Prize, and the National Medal of Science, as well as many other awards.
UT Austin student athletes compete as the Texas Longhorns and are members of the Big 12 Conference. Its Longhorn Network is unique in that it is the only sports network featuring the college sports of a single university. The Longhorns have won four NCAA Division I National Football Championships, six NCAA Division I National Baseball Championships and has claimed more titles in men's and women's sports than any other school in the Big 12 since the league was founded in 1996. Current and former UT Austin athletes have won 130 Olympic medals, including 14 in Beijing in 2008 and 13 in London in 2012. The university was recognized by "Sports Illustrated" as "America's Best Sports College" in 2002.
History.
Establishment.
The first mention of a public university in Texas can be traced to the 1827 constitution for the Mexican state of Coahuila y Tejas. Although Title 6, Article 217 of that Constitution promised to establish public education in the arts and sciences, no action was taken by the Mexican government. After Texas obtained its independence from Mexico in 1836, the Texas Congress adopted the Constitution of the Republic, which, under Section 5 of its General Provisions, stated "It shall be the duty of Congress, as soon as circumstances will permit, to provide, by law, a general system of education." On April 18, 1838, "An Act to Establish the University of Texas" was referred to a special committee of the Texas Congress, but was not reported back for further action. On January 26, 1839, the Texas Congress agreed to set aside fifty leagues of land (approx. 288,000 acres) towards the establishment of a publicly funded university. In addition, 40 acre in the new capital of Austin were reserved and designated "College Hill." (The term "Forty Acres" is colloquially used to refer to the University as a whole. The original forty acres is the area from Guadalupe to Speedway and 21st Street to 24th Street )
In 1845, Texas was annexed into the United States. Interestingly, the state's Constitution of 1845 failed to mention the subject of higher education. On February 11, 1858, the Seventh Texas Legislature approved O.B. 102, an act to establish the University of Texas, which set aside $100,000 in United States bonds toward construction of the state's first publicly funded university (the $100,000 was an allocation from the $10 million the state received pursuant to the Compromise of 1850 and Texas' relinquishing claims to lands outside its present boundaries). In addition, the legislature designated land previously reserved for the encouragement of railroad construction toward the university's endowment. On January 31, 1860, the state legislature, wanting to avoid raising taxes, passed an act authorizing the money set aside for the University of Texas to instead be used for frontier defense in west Texas to protect settlers from Indian attacks. Texas' secession from the Union and the American Civil War delayed repayment of the borrowed monies. At the end of the Civil War in 1865, The University of Texas' endowment consisted of a little over $16,000 in warrants and nothing substantive had yet been done to organize the university's operations. This effort to establish a University was again mandated by Article 7, Section 10 of the Texas Constitution of 1876 which directed the legislature to "establish, organize and provide for the maintenance, support and direction of a university of the first class, to be located by a vote of the people of this State, and styled "The University of Texas." Additionally, Article 7, Section 11 of the 1876 Constitution established the Permanent University Fund, a sovereign wealth fund managed by the Board of Regents of the University of Texas and dedicated for the maintenance of the university. Because some state legislators perceived an extravagance in the construction of academic buildings of other universities, Article 7, Section 14 of the Constitution expressly prohibited the legislature from using the state's general revenue to fund construction of any university buildings. Funds for constructing university buildings had to come from the university's endowment or from private gifts to the university, but operational expenses for the university could come from the state's general revenues.
The 1876 Constitution also revoked the endowment of the railroad lands of the Act of 1858 but dedicated 1000000 acre acres of land, along with other property previously appropriated for the university, to the Permanent University Fund. This was greatly to the detriment of the university as the lands granted the university by the Constitution of 1876 represented less than 5% of the value of the lands granted to the university under the Act of 1858 (the lands close to the railroads were quite valuable while the lands granted the university were in far west Texas, distant from sources of transportation and water). The more valuable lands reverted to the fund to support general education in the state (the Special School Fund). On April 10, 1883, the legislature supplemented the Permanent University Fund with another 1,000,000 acres of land in west Texas previously granted to the Texas and Pacific Railroad but returned to the state as seemingly too worthless to even survey. The legislature additionally appropriated $256,272.57 to repay the funds taken from the university in 1860 to pay for frontier defense and for transfers to the state's General Fund in 1861 and 1862. The 1883 grant of land increased the land in the Permanent University Fund to almost 2.2 million acres. Under the Act of 1858, the university was entitled to just over 1,000 acres of land for every mile of railroad built in the state. Had the original 1858 grant of land not been revoked by the 1876 Constitution, by 1883 the university lands would have totaled 3.2 million acres, so the 1883 grant was to restore lands taken from the university by the 1876 Constitution, not an act of munificence. 
On March 30, 1881 the legislature set forth the structure and organization of the university and called for an election to establish its location. By popular election on September 6, 1881, Austin (with 30,913 votes) was chosen as the site of the main university. Galveston, having come in second in the election (20,741 votes) was designated the location of the medical department (Houston was third with 12,586 votes). On November 17, 1882 on the original "College Hill," an official ceremony was held to commemorate the laying of the cornerstone of the Old Main building. University President Ashbel Smith, presiding over the ceremony prophetically proclaimed "Texas holds embedded in its earth rocks and minerals which now lie idle because unknown, resources of incalculable industrial utility, of wealth and power. Smite the earth, smite the rocks with the rod of knowledge and fountains of unstinted wealth will gush forth." The University of Texas officially opened its doors on September 15, 1883.
Expansion and growth.
In 1890, George Washington Brackenridge donated $18,000 for the construction of a three story brick mess hall known as Brackenridge Hall (affectionately known as "B.Hall"), one of the university's most storied buildings and one that played an important place in university life until its demolition in 1952. 
The old Victorian-Gothic Main Building served as the central point of the campus's 40 acre site, and was used for nearly all purposes. But by the 1930s, discussions arose about the need for new library space, and the Main Building was razed in 1934 over the objections of many students and faculty. The modern-day tower and Main Building were constructed in its place.
In 1910, George Washington Brackenridge again displayed his philanthropy, this time donating 500 acre on the Colorado River to the university . A vote by the regents to move the campus to the donated land was met with outrage, and the land has only been used for auxiliary purposes such as graduate student housing. Part of the tract was sold in the late-1990s for luxury housing, and there are controversial proposals to sell the remainder of the tract. The Brackenridge Field Laboratory was established on 82 acre of the land in 1967.
In 1916, Gov. James E. Ferguson became involved in a serious quarrel with the University of Texas. The controversy grew out of the refusal of the board of regents to remove certain faculty members whom the governor found objectionable. When Ferguson found that he could not have his way, he vetoed practically the entire appropriation for the university. Without sufficient funding, the University would have been forced to close its doors. In the middle of the veto controversy, Ferguson's critics brought to light a number of irregularities on the part of the governor. Eventually, The Texas House of Representatives prepared 21 charges against Ferguson and the Senate convicted him on 10 of those charges, including misapplication of public funds and receiving $156,000 from an unnamed source. The Texas Senate removed Ferguson as governor and declared him ineligible to hold office. 
In 1921, the legislature appropriated $1,350,000 for the purchase of land adjacent to the main campus. However, expansion was hampered by the restriction against using state revenues to fund construction of university buildings as set forth in Article 7, Section 14 of the Constitution. With the successful completion of Santa Rita No. 1 well and the discovery of oil on university-owned lands in 1923, the university was able to add significantly to its Permanent University Fund. The additional income from Permanent University Fund investments allowed for bond issues in 1931 and 1947, with the latter expansion necessary from the spike in enrollment following World War II. The university built 19 permanent structures between 1950 and 1965, when it was given the right of eminent domain. With this power, the university purchased additional properties surrounding the original 40 acre.
The discovery of oil on university-owned lands in 1923 and the subsequent addition of money to the university's Permanent University Fund allowed the legislature to address funding for the university along with the Agricultural and Mechanical College (now known as Texas A&M University). With sufficient funds now in the Permanent University Fund to finance construction on both campuses, on April 8, 1931, the Forty Second Legislature passed H.B. 368. which dedicated the Agricultural and Mechanical College a 1/3 interest in the Available University Fund, the annual income from Permanent University Fund investments. 
UT Austin was inducted into the American Association of Universities in 1929.During World War II, the University of Texas was one of 131 colleges and universities nationally that took part in the V-12 Navy College Training Program which offered students a path to a Navy commission. 
On March 6, 1967, the Sixtieth Texas Legislature changed the official name of the University from "The University of Texas" to "The University of Texas at Austin" to reflect the growth of the University of Texas System.
1966 shooting spree.
On August 1, 1966, Texas student Charles Whitman barricaded the observation deck in the tower of the Main Building. With two rifles, a sawed-off shotgun, and various other weapons, he killed a total of 14 people on campus, 11 from the observation deck and below the clocks on the tower, and three more in the tower, as well as wounding two more inside the observation deck. The massacre ended after Whitman was shot and killed by police after they breached the tower. Prior to the massacre, Whitman had killed his mother and his wife. Whitman had been a patient at the University Health Center, and on March 29, preceding the shootings, had conveyed to psychiatrist Maurice Heatley his feelings of overwhelming hostilities and that he was thinking about "going up on the tower with a deer rifle and start shooting people."
Following the Whitman event, the observation deck was closed until 1968, and then closed again in 1975 following a series of suicide jumps during the 1970s. In 1999, after installation of security fencing and other safety precautions, the tower observation deck reopened to the public. There is a turtle pond park near the tower dedicated to all of those affected by the tragedy.
Recent history.
The first presidential library on a university campus was dedicated on May 22, 1971 with former President Johnson, Lady Bird Johnson and then-President Richard Nixon in attendance. Constructed on the eastern side of the main campus, the Lyndon Baines Johnson Library and Museum is one of 13 presidential libraries administered by the National Archives and Records Administration.
The University of Texas at Austin has experienced a wave of new construction recently with several significant buildings. On April 30, 2006, the school opened the Blanton Museum of Art. In August 2008, the AT&T Executive Education and Conference Center opened, with the hotel and conference center forming part of a new gateway to the university. Also in 2008, Darrell K Royal-Texas Memorial Stadium was expanded to a seating capacity of 100,119, making it the largest stadium (by capacity) in the state of Texas at the time.
On January 19, 2011, the university announced the creation of a 24-hour television network in partnership with ESPN, dubbed the Longhorn Network. ESPN will pay a $300 million guaranteed rights fee over 20 years to the university and to IMG College, UT Austin's multimedia rights partner. The network covers the university's intercollegiate athletics, music, cultural arts and academics programs. The channel first aired in September 2011.
Campus.
The University's property totals 1438.5 acres, comprising the 423.5 acres for the Main Campus in central Austin and the J. J. Pickle Research Campus in north Austin and the other properties throughout Texas. The main campus has 150 buildings totalling over 18000000 sqft.
One of the University's most visible features is the Beaux-Arts Main Building, including a 307 ft tower designed by Paul Philippe Cret. Completed in 1937, the Main Building is in the middle of campus. The tower usually appears illuminated in white light in the evening but is lit orange for various special occasions, including athletic victories and academic accomplishments; it is conversely darkened for solemn occasions. At the top of the tower is a carillon of 56 bells, the largest in Texas. Songs are played on weekdays by , in addition to the usual pealing of Westminster Quarters every quarter hour between 6 am and 9 pm In 1998, after the installation of security and safety measures, the observation deck reopened to the public indefinitely for weekend tours.
The university's seven museums and seventeen libraries hold over nine million volumes, making it the seventh-largest academic library in the country. The holdings of the university's Harry Ransom Humanities Research Center include one of only 21 remaining complete copies of the Gutenberg Bible and the first permanent photograph, "View from the Window at Le Gras", taken by Nicéphore Niépce. The newest museum, the 155000 sqft Blanton Museum of Art, is the largest university art museum in the United States and hosts approximately 17,000 works from Europe, the United States, and Latin America.
The University of Texas at Austin has an extensive underground tunnel system that links all of the buildings on campus. Constructed in the 1930s under the supervision of creator Carl Eckhardt, then head of the physical plant, the tunnels have grown along with the university campus. They currently measure approximately six miles in total length. The tunnel system is used for communications and utility service. It is closed to the public and is guarded by silent alarms. Since the late 1940s the university has generated its own electricity. Today its natural gas cogeneration plant has a capacity of 123 MW. The university also operates a TRIGA nuclear reactor at the J. J. Pickle Research Campus.
The university continues to expand its facilities on campus. In 2010, the university opened the state-of-the-art Norman Hackerman building (on the location of the former Experimental Sciences Building) housing chemistry and biology research and teaching laboratories. In 2010, the university broke ground on the $120 million Bill & Melinda Gates Computer Science Complex and Dell Computer Science Hall and the $51 million Belo Center for New Media, both of which are now complete. The new LEED gold-certified, 110000 sqft Student Activity Center (SAC) opened in January 2011, housing study rooms, lounges and food vendors. The SAC was constructed as a result of a student referendum passed in 2006 which raised student fees by $65 per semester. In 2012 the Moody Foundation awarded the College of Communication with $50 million making it the largest endowment any communication college has received, so naming it the Moody College of Communication.
The university operates two public radio stations, KUT with news and information, and KUTX with music, via local FM broadcasts as well as live streaming audio over the Internet. The university uses Capital Metro to provide bus transportation for students around the campus and throughout Austin.
Organization and administration.
Colleges and schools.
The university contains eighteen colleges & schools and one academic unit, each listed with its founding date:
Academics.
The University of Texas at Austin offers more than 100 undergraduate and 170 graduate degrees. In the 2009–2010 academic year, the university awarded a total of 13,215 degrees: 67.7% bachelor's degrees, 22.0% master's degrees, 6.4% doctoral degrees, and 3.9% Professional degrees.
In addition, the university has eight honors programs that span a variety of academic fields: Liberal Arts Honors, the Business Honors Program, the Turing Scholars Program in Computer Science, Engineering Honors, the Dean's Scholars Program in Natural Sciences, the Health Science Scholars Program in Natural Sciences, the Polymathic Scholars Program in Natural Sciences, and the interdisciplinary Plan II Honors program. The university also offers innovative programs for promoting academic excellence and leadership development such as the Freshman Research Initiative and .
Admission.
As a state public university, UT Austin was, until recently, subject to Texas House Bill 588, which guarantees graduating Texas high school seniors in the top 10% of their class admission to any public Texas university. A new state law granting UT (but no other state university) a partial exemption from the top 10% rule, Senate Bill 175, was passed by the 81st Legislature in 2009. It modifies this admissions policy by limiting automatically admitted freshmen to 75% of the entering in-state freshman class, starting in 2011. The university will admit the top one percent, the top two percent and so forth until the cap is reached; the university expects to automatically admit students in the top 8% of their graduating class for 2011. Furthermore, students admitted under Texas House Bill 588 are not guaranteed their choice of college or major, but rather only guaranteed admission to the university as a whole. Many colleges, such as the Cockrell School of Engineering, have secondary requirements that must be met for admission.
For others who go through the traditional application process, selectivity is deemed "more selective" according to the Carnegie Foundation for the Advancement of Teaching and by "U.S. News & World Report". For Fall 2009, 31,362 applied and 45.6% were accepted, and of those accepted, 51.0% enrolled. The university's freshman retention rate in 2009 was 92.5% and the six-year graduation rate was 81.0%. The Fall 2011 entering class had an average ACT composite score of 28 and an average SAT composite score of 1858.
Relative to other universities in the state of Texas, UT Austin is second to Rice University in selectivity according to a "Business Journal" study weighing acceptance rates and the mid-range of the SAT and ACT. UT Austin was ranked as the 18th most selective in the South.
Rankings.
UT Austin is consistently ranked as one of the top public universities in the country, with highly prestigious programs in a variety of fields. Nationally, UT Austin ranked 45th amongst all universities according to "U.S. News and World Report", and tied for 13th place among public universities in 2011. The University of Texas School of Architecture was ranked second among national undergraduate programs in 2012. Additionally, the McCombs School of Business was ranked seventh among undergraduate business programs in 2013, and the Cockrell School of Engineering was ranked ninth among undergraduate engineering programs in 2009. Internationally, UT Austin was ranked 67th in the "World's Best Universities" ranking presented by "U.S. News and World Report", and 35th in the world by Shanghai Jiao Tong University, based on factors such as Nobel laureate affiliation and number of highly cited researchers. In 2009, The Economist ranked the school 49th worldwide. In 2013 London-based Times Higher Education ranked the university 25th in the world, while Human Resources & Labor Review ranked the university 42nd and QS' "World University Rankings" ranked the university 68th internationally.
UT Austin is considered to be a "Public Ivy" – a public university that provides an Ivy League collegiate experience at a public school price, having been ranked in virtually every list of "Public Ivies" since Richard Moll coined the term in his 1985 book "Public Ivies: A Guide to America's best public undergraduate colleges and universities". The seven other "Public Ivy" universities, according to Moll, were The College of William & Mary, Miami University, The University of California, The University of Michigan, The University of North Carolina, The University of Vermont, and The University of Virginia.
As of 2013, "U.S. News and World Report" ranked the Accounting and Latin American History programs as the top in the nation. Additionally, more than 50 other science, humanities and professional programs rank in the top 25 nationally, according to U.S. News & World Report's latest edition of “Best Graduate Schools.” The University of Texas College of Education and College of Pharmacy are each the fourth best in the nation in their fields (with Education ranking first among public universities for the third year in a row and also number one in research expeditures). And the School of Information (iSchool) is sixth best in Library and Information Sciences. Among other overall school rankings, the Cockrell School of Engineering is 11th best (sixth among publics). The McCombs School of Business is 17th best (fifth among publics). The Lyndon B. Johnson School of Public Affairs remains at No. 16, the Jackson School of Geosciences remains at No. 9 for Earth Sciences, and the School of Social Work remains at No. 7. The University of Texas School of Law climbed one place in the rankings, to No. 15 in the nation (fourth among publics).
A 2005 Bloomberg survey ranked the school 5th among all business schools and first among public business schools for the largest number of alumni who are S&P 500 CEOs. Similarly, a 2005 "USA Today" report ranked the university as "the number one source of new Fortune 1000 CEOs." A "payback" analysis published by SmartMoney in 2011 comparing graduates' salaries to tuition costs concluded that the school was the second-best value of all colleges in the nation, behind only Georgia Tech. A 2013 College Database study found that UT was 22nd in the nation in terms of increased lifetime earnings by graduates.
Research.
Except for MIT, UT Austin attracts more federal research grants than any American university without a medical school. For the 2009–2010 school year, the university exceeded $640 million in research funding (up from $590 million the previous year) and has earned more than 300 patents since 2003. UT Austin houses the Office of Technology Commercialization, a technology transfer center which serves as the bridge between laboratory research and commercial development. In 2009, UT Austin created nine new start-up companies to commercialize technology developed at the university and has created 46 start-ups in the past seven years. UT Austin license agreements generated $10.9 million in revenue for the university in 2009.
Research at UT Austin is largely focused in the engineering and physical sciences, and is a world-leading research institution in fields such as computer science. Energy is a major research thrust of the university, with major federally funded projects on biofuels, battery and solar cell technology, and geological carbon dioxide storage, water purification membranes, among others. In 2009, UT Austin founded the Energy Institute, led by former Under Secretary for Science Raymond L. Orbach, to organize and advance multi-disciplinary energy research at the university. While the university does have a medical school, it houses medical programs associated with other campuses and allied health professional programs, as well as major research programs in pharmacy, biomedical engineering, neuroscience, and others.
UT Austin opened the $100 million Dell Pediatric Research Institute in 2010 as part of an effort to increase medical research at the university and establish a medical research complex, and associated medical school, in the city of Austin.
UT Austin operates several major auxiliary research centers. The world's third-largest telescope, the Hobby–Eberly Telescope, and three other large telescopes are part of UT Austin's McDonald Observatory, 450 mi west of Austin. The university manages nearly 300 acre of biological field laboratories, including the Brackenridge Field Laboratory in Austin. The Center for Agile Technology focuses on software development challenges. The J.J. Pickle Research Campus (PRC) is home to the Texas Advanced Computing Center which operates the Ranger supercomputer, one of the most powerful supercomputers in the world, as well as the Microelectronics Research Center which houses micro- and nanoelectronics research and features a 15000 sqfoot cleanroom for device fabrication.
Founded in 1946, UT Austin's Applied Research Laboratories at the PRC has been responsible for the development or testing of the vast majority of high-frequency sonar equipment used by the Navy, and in 2007, was granted a research contract by the Navy funded up to $928 million over ten years. The Institute for Advanced Technology, founded in 1990 and located in the West Pickle Research Building, supports the U.S. Army with basic and applied research in several fields.
The Center for Transportation Research UT Austin is a nationally recognized research institution focusing on transportation research, education, and public service. Established in 1963 as the Center for Highway Research, its current and ongoing projects address virtually all aspects of transportation, including economics, multimodal systems, traffic congestion relief, transportation policy, materials, structures, transit, environmental impacts, driver behavior, land use, geometric design, accessibility, and pavements.
In 2013, UT Austin announced the naming of the O'Donnell Building for Applied Computational Engineering and Sciences. The O'Donnell Foundation of Dallas, headed by Peter O'Donnell and his wife, Edith Jones O'Donnell, has given more than $135 million to UT Austin alone between 1983 and 2013. UT Austin President William C. Powers declared the O'Donnells "among the greatest supporters of the University of Texas in its 130-year history. Their transformative generosity is based on the belief in our power to change society for the better." In 2008, O'Donnell pledged $18 million to finance the hiring of UT Austin faculty members undertaking research in the use of mathematics, computers, and multiple scientific disciplines; his pledge was matched by W. A. "Tex" Moncrief, Jr., a oilman and philanthropist from Fort Worth.
Endowment.
The university has an endowment of $7.2 billion, out of the $16.11 billion (according to 2008 estimates) available to the University of Texas System. This figure reflects the fact that the school has the largest endowment of any public university in the nation.
Thirty percent of the university's endowment comes from Permanent University Fund (PUF), with nearly $15 billion in assets as of 2007. Proceeds from lands appropriated in 1839 and 1876, as well as oil monies, comprise the majority of PUF. At one time, the PUF was the chief source of income for Texas' two university systems, The University of Texas System and the Texas A&M University System; today, however, its revenues account for less than 10 percent of the universities' annual budgets. This has challenged the universities to increase sponsored research and private donations. Privately funded endowments contribute over $2 billion to the University's total endowment value.
Student life.
Student profile.
For Fall 2011, the university enrolled 38,437 undergraduate, 11,497 graduate and 1,178 law students. Out-of-state and international students comprised 9.1% of the undergraduate student body and 20.1% of the total student body, with students from all 50 states and more than 120 foreign countries—most notably, the Republic of Korea, followed by the People's Republic of China, India, Mexico and Taiwan. For Fall 2010, the undergraduate student body was 48.7% male and 51.3% female. The three largest undergraduate majors in 2009 were Biological Sciences, Unspecified Business, and Psychology, while the three largest graduate majors were Business Administration (MBA), Electrical and Computer Engineering, and Pharmacy (PharmD).
Residential life.
The campus is currently home to fourteen residence halls, the newest of which opened for residence in Spring 2007. On-campus housing can hold more than 7,100 students. Jester Center is the largest residence hall with its capacity of 2,945. Academic enrollment exceeds the on-campus housing capacity; as a result, most students must live in private residence halls, housing cooperatives, apartments, or with Greek organizations and other off-campus residences. The Division of Housing and Food Service, which already has the largest market share of 7,000 of the estimated 27,000 beds in the campus area, plans to expand to 9,000 beds in the near future.
Student organizations.
The university recognizes more than 1,000 student organizations. In addition, it supports three official student governance organizations that represent student interests to faculty, administrators, and the Texas Legislature. Student Government, established in 1902, is the oldest governance organization and represents student interests in general. The Senate of College Councils represents students in academic affairs and coordinates the college councils, and the Graduate Student Assembly represents graduate student interests. The University Unions Student Events Center serves as the hub for student activities on campus. The Friar Society serves as the oldest honor society at the university. The Livestrong Texas 4000 for Cancer student organization is the longest annual charity bicycle ride in the world and has raised over $4 million for cancer research from its founding in 2004 to August, 2013.
Greek life.
The University of Texas at Austin is home to an active Greek community. Approximately 14 percent of undergraduate students are in fraternities or sororities. With more than 65 national chapters, the university's Greek community is one of the nation's largest. These chapters are under the authority of one of the school's six Greek council communities, Interfraternity Council, National Pan-Hellenic Council, Texas Asian Pan-Hellenic Council, Latino Pan-Hellenic Council, Multicultural Greek Council and University Panhellenic Council. Other registered student organizations also name themselves with Greek letters and are called affiliates. They are not a part of one of the six councils but have all of the same privileges and responsibilities of any other organization. According to the Office of the Dean of Students' mission statement, Greek life promotes cultural appreciation, scholarship, leadership, and service. Most Greek houses are west of the Drag in the West Campus neighborhood.
Media.
Students express their opinions in and out of class through periodicals including "Study Breaks Magazine", Longhorn Life, "The Daily Texan" (the most award-winning daily college newspaper in the United States), and the "Texas Travesty". Over the airwaves students' voices are heard through Texas Student Television (K29HW-D) and KVRX Radio.
The Computer Writing and Research Lab of the university's Department of Rhetoric and Writing also hosts "the Blogora", a blog for "connecting rhetoric, rhetorical methods and theories, and rhetoricians with public life" by the Rhetoric Society of America.
Traditions.
Traditions at the University of Texas are perpetuated through several school symbols and mediums. At athletic events, students frequently sing "Texas Fight", the university's fight song while displaying the Hook 'em Horns hand gesture—the gesture mimicking the horns of the school's mascot, Bevo the Texas longhorn.
Athletics.
The University of Texas offers a wide variety of varsity and intramural sports programs. As of 2008, the university's athletics program ranked fifth in the nation among Division I schools according to the National Association of Collegiate Directors of Athletics. Due to the breadth of sports offered and the quality of the programs, Texas was selected as "America's Best Sports College" in a 2002 analysis by "Sports Illustrated." Texas was also listed as the number one Collegiate Licensing Company client for the second consecutive year in regards to the amount of annual trademark royalties received from fan merchandise sales. But this ranking is based only on clients of the Collegiate Licensing Company, which does not handle licensing for approximately three-dozen large schools including Ohio State, USC, and UCLA.
Varsity sports.
The University's men's and women's athletics teams are nicknamed the Longhorns. A charter member of the Southwest Conference until it dissolved in 1996, Texas now competes in the Big 12 Conference of the NCAA's Division I-FBS. Texas has won 50 total national championships, 42 of which are NCAA national championships.
The University of Texas has traditionally been considered a college football powerhouse. At the start of the 2007 season, the Longhorns were ranked third in the all-time list of both total wins and winning percentage. The team experienced its greatest success under coach Darrell Royal, winning three national championships in 1963, 1969, and 1970. It won a fourth title under head coach Mack Brown in 2005 after a 41–38 victory over previously undefeated Southern California in the 2006 Rose Bowl.
In recent years, the men's basketball team has gained prominence, advancing to the NCAA Tournament Sweet Sixteen in 2002, the Final Four in 2003, the Sweet Sixteen in 2004, and the Elite Eight in 2006 and 2008.
The University's baseball team is one of the nation's best. It has made more trips to the College World Series (35) than any other school, and it posted wins in 1949, 1950, 1975, 1983, 2002, and 2005.
Additionally, the University's highly successful men's and women's swimming and diving teams lay claim to sixteen NCAA Division I titles. The swim team was first developed under Coach Tex Robertson. In particular, the men's team is led by Eddie Reese, who served as the head men's coach at the 1992 Summer Olympics in Barcelona, the 2004 Games in Athens and the 2008 Games in Beijing.
People.
Faculty.
In the Fall of 2009, the school employed 2,770 full-time faculty members (88.3% of whom hold the terminal degree in their field), with a student-to-faculty ratio of 17.3 to 1. The university's faculty includes 63 members of the National Academy, winners of the Nobel Prize, the Pulitzer Prize, the National Medal of Science, the National Medal of Technology, the Turing Award and other various awards. Nine Nobel Laureates are or have been affiliated with UT Austin. Research expenditures for UT Austin exceeded $550 million for the 2013–2014 school year.
Alumni.
Texas Exes is the official UT alumni organization. "The Alcalde", founded in 1913 and pronounced “all-call-day,” is the university's alumni magazine.
At least 15 graduates have served in the U.S. Senate and U.S. House of Representatives, such as Lloyd Bentsen '42, who served as both a U.S. Senator and U.S. Representative, as well as being the 1988 Democratic Party Vice Presidential nominee. Presidential cabinet members include former United States Secretary of State James Baker '57, former United States Secretary of Education William J. Bennett, and former United States Secretary of Commerce Donald Evans '73. Former First Lady Laura Bush '73 and daughter Jenna '04 both graduated from Texas, as well as former First Lady Lady Bird Johnson '33 & '34 and her eldest daughter Lynda. In foreign governments, the university has been represented by Fernando Belaúnde Terry '36 (42nd President of Peru), Mostafa Chamran (former Minister of Defense for Iran), and Abdullah al-Tariki (co-founder of OPEC). Additionally, the Prime Minister of the Palestinian National Authority, Salam Fayyad, graduated from the university with a PhD in economics. Tom C. Clark, J.D. '22, served as United States Attorney General from 1945 to 1949 and as an Associate Justice of the Supreme Court of the United States from 1949 to 1967.
Alumni in academia include the 26th President of The College of William & Mary Gene Nichol '76, the 10th President of Boston University Robert A. Brown '73 & '75, and the 8th President of the University of Southern California John R. Hubbard. The University also graduated Alan Bean '55, the fourth man to walk on the Moon. Additionally, alumni who have served as business leaders include ExxonMobil Corporation CEO Rex Tillerson '75, Dell founder and CEO Michael Dell, and Gary C. Kelly, Southwest Airlines's CEO.
In literature and journalism, the school boasts 20 Pulitzer Prizes to 18 former students, including Gail Caldwell and Ben Sargent '70. Walter Cronkite, the former CBS Evening News anchor once called the most trusted man in America, attended the University of Texas at Austin, as did CNN anchor Betty Nguyen '95. Alumnus J. M. Coetzee also received the 2003 Nobel Prize in Literature. Novelist Raymond Benson ('78) was the official author of James Bond novels between 1996–2002, the only American to be commissioned to pen them. Donna Alvermann, a distinguished research professor at the University of Georgia, Department of Education also graduated from the University of Texas, as did Wallace Clift ('49) and Jean Dalby Clift ('50, J.D. '52), authors of several books in the fields of psychology of religion and spiritual growth. Alireza Jafarzadeh the author of "The Iran Threat: President Ahmadinejad and the Coming Nuclear Crisis" and television commentator ('82, MS). Though expelled from UT, former student and The Daily Texan writer John Patric went on to become a noted writer for "National Geographic", "Reader's Digest", and author of 1940s best-seller "Why Japan was Strong".
UT Alumni also include 28 Rhodes Scholars, 26 Truman Scholars, 20 Marshall Scholars, and nine Astronauts.
Several musicians and entertainers attended the University, though most dropped out to pursue their respective careers. Janis Joplin, the American singer who was posthumously inducted into the Rock and Roll Hall of Fame and who received a Grammy Lifetime Achievement Award attended the university, as did February 1955 "Playboy" Playmate of the Month and Golden Globe recipient Jayne Mansfield. Composer Harold Morris is a 1910 graduate. Noted film director, cinematographer, writer, and editor Robert Rodriguez is a Longhorn, as are actors Eli Wallach and Matthew McConaughey. Rodriguez dropped out of the university after two years to pursue his career in Hollywood, but he officially completed his degree from the Radio-Television-Film department on May 23, 2009. Rodriguez also gave the keynote address at the university-wide commencement ceremony. Radio-Television-Film alumni Mark Dennis and Ben Foster took their award winning feature film, Strings, to the American film festival circuit in 2011. Web and television actress Felicia Day and film actress Renée Zellweger also attended the university. Day graduated with dual degrees in music performance (violin) and mathematics, while Zellweger graduated with a BA in English. Writer, recording artist Phillip Sandifer attended the university and graduated with a degree in History. Michael "Burnie" Burns is an actor, writer, film director and film producer, graduated from the University of Texas with a degree in Computer Science. He, along with another UT graduate Matt Hullum, also founded the Austin-based production company Rooster Teeth, producing many hit shows including the multi-award-winning Internet series, Red vs. Blue. Farrah Fawcett, one of the original "Charlie's Angels", left after her junior year to pursue a modeling career. Actor Owen Wilson and writer/director Wes Anderson each attended the university. There they wrote "Bottle Rocket" together which became Anderson's first feature film. Another notable writer, Rob Thomas graduated with a BA in History in 1987 and went on to write the young adult novel "Rats Saw God" and created the series "Veronica Mars". Notable illustrator, writer and alum, , is best known for her illustrations in the "If You Give..." children's books series, starting with "If You Give a Mouse a Cookie". Chinese singer-songwriter, producer, actress Cindy Yen (birth name Cindy Wu) graduated with double degrees in Music (piano performance) and Broadcast Journalism in 2008. Noted composer and arranger Jack Cooper received his D.M.A. in 1999 from UT Austin in composition and has gone onto teach in higher education and become well known internationally through the music publishing industry.
Many alumni have found success in professional sports. Legendary pro football coach Tom Landry '49 attended the university as an industrial engineering major but interrupted his education after a semester to serve in the United States Army Air Corps during World War II. Following the war, he returned to the university and played fullback and defensive back on the Longhorns' bowl-game winners on New Year's Day of 1948 and 1949. Seven-time Cy Young Award-winner Roger Clemens entered the MLB after helping the Longhorns win the 1983 College World Series. Three-time NBA scoring champion Kevin Durant entered the 2007 NBA Draft and was selected second overall behind Greg Oden, after sweeping National Player of the Year honors, becoming the first freshman to win any of the awards. After becoming the first freshman in school history to lead Texas in scoring and being named the Big 12 Freshman of the Year, Daniel Gibson entered the 2006 NBA Draft and was selected in the second round by the Cleveland Cavaliers. Several Olympic medalists have also attended the school, including 2008 Summer Olympics athletes Ian Crocker '05 (swimming world record holder and two-time Olympic gold medalist) and 4x400m relay defending Olympic gold medalist Sanya Richards '06. Mary Lou Retton (the first female gymnast outside Eastern Europe to win the Olympic all-around title, five-time Olympic medalist, and 1984 "Sports Illustrated" Sportswoman of the Year) also attended the university. Also an alumnus is Dr. Robert Cade, the inventor of the sport drink Gatorade. In big, global philanthropy, the university is honored by Darren Walker, president of Ford Foundation.
Other notable alumni include prominent businessman Red McCombs, Diane Pamela Wood, the first female chief judge of the United States Court of Appeals for the Seventh Circuit, and astrophysicist Neil deGrasse Tyson. Also an alumnus is Admiral William H. McRaven, credited for organizing and executing Operation Neptune's Spear, the special ops raid that led to the death of Osama bin Laden.
Keene Prize for Literature.
The Keene Prize for Literature is a student literary award given by the University. With a prize of $50,000 it claims to be "one of the world's largest student literary prizes". An additional $50,000 is split between three finalists. The purpose of the award is to "help maintain the university's status as a premier location for emerging writers", and to recognize the winners and their works. The prize was established in 2006, in the College of Liberal Arts. It is named after E. L. Keene, a 1942 graduate of the university.

</doc>
<doc id="32061" url="http://en.wikipedia.org/wiki?curid=32061" title="Urea cycle">
Urea cycle

The urea cycle (also known as the ornithine cycle) is a cycle of biochemical reactions occurring in many animals that produces urea ((NH2)2CO) from ammonia (NH3). This cycle was the first metabolic cycle discovered (Hans Krebs and Kurt Henseleit, 1932), five years before the discovery of the TCA cycle. In mammals, the urea cycle takes place primarily in the liver, and to a lesser extent in the kidney.
Function.
Organisms that cannot easily and quickly remove ammonia usually have to convert it to some other substance, like urea or uric acid, which are much less toxic. Insufficiency of the urea cycle occurs in some genetic disorders (inborn errors of metabolism), and in liver failure. The result of liver failure is accumulation of nitrogenous waste, mainly ammonia, which leads to hepatic encephalopathy.
Reactions.
The urea cycle consists of five reactions: two mitochondrial and three cytosolic. The cycle converts two amino groups, one from NH4+ and one from Asp, and a carbon atom from HCO3−, to the relatively nontoxic excretion product urea at the cost of four "high-energy" phosphate bonds (3 ATP hydrolyzed to 2 ADP and one AMP). Ornithine is the carrier of these carbon and nitrogen atoms.
1 -ornithine2 carbamoyl phosphate3 -citrulline4 argininosuccinate5 fumarate6 -arginine7 urea -Asp -aspartateCPS-1 carbamoyl phosphate synthetase IOTC Ornithine transcarbamoylaseASS argininosuccinate synthetaseASL argininosuccinate lyaseARG1 arginase 1
In the first reaction, NH4+ + HCO3− is equivalent to NH3 + CO2 + H2O.
Thus, the overall equation of the urea cycle is:
Since fumarate is obtained by removing NH3 from aspartate (by means of reactions 3 and 4), and PPi + H2O → 2 Pi, the equation can be simplified as follows:
Note that reactions related to the urea cycle also cause the production of 2 NADH, so the urea cycle releases slightly more energy than it consumes. These NADH are produced in two ways:
The two NADH produced can provide energy for the formation of 4 ATP (cytosolic NADH provides only 1.5 ATP due to the glycerol-3-phosphate shuttle who transfers the electrons from cytosolic NADH to FADH2 and that gives 1.5 ATP), a net production of one high-energy phosphate bond for the urea cycle. However, if gluconeogenesis is underway in the cytosol, the latter reducing equivalent is used to drive the reversal of the GAPDH step instead of generating ATP.
The fate of oxaloacetate is either to produce aspartate via transamination or to be converted to phosphoenolpyruvate, which is a substrate for gluconeogenesis.
Regulation.
N-Acetylglutamic acid.
The synthesis of carbamoyl phosphate and the urea cycle are dependent on the presence of NAcGlu, which allosterically activates CPS1. NAcGlu is an obligate activator of Carbamoyl phosphate synthase. Synthesis of NAcGlu by is stimulated by both "Arg", allosteric stimulator of NAGS, and "Glu", a product in the transamination reactions and one of NAGS's substrates, both of which elevated when free amino acids are elevated. So Glu not only is a substrate for NAGS but also serves as an activator for the urea cycle.
Substrate concentrations.
The remaining enzymes of the cycle are controlled by the concentrations of their substrates. Thus, inherited deficiencies in cycle enzymes other than do not result in significant decreases in urea production (if any cycle enzyme is entirely missing, death occurs shortly after birth). Rather, the deficient enzyme's substrate builds up, increasing the rate of the deficient reaction to normal.
The anomalous substrate buildup is not without cost, however. The substrate concentrations become elevated all the way back up the cycle to NH4+, resulting in hyperammonemia (elevated [NH4+]P).
Although the root cause of NH4+ toxicity is not completely understood, a high [NH4+] puts an enormous strain on the NH4+-clearing system, especially in the brain (symptoms of urea cycle enzyme deficiencies include intellectual disability and lethargy). This clearing system involves and , which decrease the 2-oxoglutarate (2OG) and Glu pools. The brain is most sensitive to the depletion of these pools. Depletion of 2OG decreases the rate of TCAC, whereas Glu is both a neurotransmitter and a precursor to GABA, another neurotransmitter. "(p.734)"
Pathology.
Deficiencies of the various enzymes and transporters involved in the urea cycle can cause urea cycle disorders:
Most urea cycle disorders are associated with hyperammonemia, however argininemia and some forms of argininosuccinic aciduria do not present with elevated ammonia.

</doc>
<doc id="32073" url="http://en.wikipedia.org/wiki?curid=32073" title="USB">
USB

Universal Serial Bus (USB) is an industry standard developed in the mid-1990s that defines the cables, connectors and communications protocols used in a bus for connection, communication, and power supply between computers and electronic devices.
USB was designed to standardize the connection of computer peripherals (including keyboards, pointing devices, digital cameras, printers, portable media players, disk drives and network adapters) to personal computers, both to communicate and to supply electric power. It has become commonplace on other devices, such as smartphones, PDAs and video game consoles. USB has effectively replaced a variety of earlier interfaces, such as serial and parallel ports, as well as separate power chargers for portable devices.
Overview.
In general, there are three basic kinds or sizes related to the USB connectors and types of established connection: the older "standard" size, in its USB 1.1/2.0 and USB 3.0 variants (for example, on USB flash drives), the "mini" size (primarily for the B connector end, such as on many cameras), and the "micro" size, in its USB 1.1/2.0 and USB 3.0 variants (for example, on most modern cellphones).
Unlike other data cables (Ethernet, HDMI etc.), each end of a USB cable uses a different "kind" of connector; an A-type or a B-type. This kind of design was chosen to prevent electrical overloads and damaged equipment, as only the A-type socket provides power. There are cables with A-type connectors on both ends, but they should be used carefully. Therefore, in general, each of the different "sizes" requires four different connectors; USB cables have the A-type and B-type plugs, and the corresponding receptacles are on the computer or electronic device. In common practice, the A-type connector is usually the full size, and the B-type side can vary as needed.
The mini and micro sizes also allow for a reversible AB-type receptacle, which can accept either an A-type or a B-type plug. This scheme, known as "USB On-The-Go", allows one receptacle to perform its double duty in space-constrained applications.
Counter-intuitively, the "micro" size is the most durable from the point of designed insertion lifetime. The standard and mini connectors were designed for less than daily connections, with a design lifetime of 1,500 insertion/removal cycles. (Improved mini-B connectors have reached 5,000-cycle lifetimes.) Micro connectors were designed with frequent charging of portable devices in mind; not only is design lifetime of the connector improved to 10,000 cycles, but it was also redesigned to place the flexible contacts, which wear out sooner, on the easily replaced cable, while the more durable rigid contacts are located in the micro-USB receptacles. Likewise, the springy part of the retention mechanism (parts that provide required gripping force) were also moved into plugs on the cable side.
USB connections also come in five data transfer modes: Low Speed, Full Speed, High Speed, SuperSpeed, and SuperSpeed+. High Speed is only supported by specifically designed USB 2.0 High Speed interfaces (that is, USB 2.0 controllers without the High Speed designation do not support it), as well as by USB 3.0 and newer interfaces. SuperSpeed is supported only by USB 3.0 and newer interfaces, and requires a connector and cable with extra pins and wires, usually distinguishable by the blue inserts in connectors.
History.
A group of seven companies began the development of USB in 1994: Compaq, DEC, IBM, Intel, Microsoft, NEC, and Nortel. The goal was to make it fundamentally easier to connect external devices to PCs by replacing the multitude of connectors at the back of PCs, addressing the usability issues of existing interfaces, and simplifying software configuration of all devices connected to USB, as well as permitting greater data rates for external devices. A team including Ajay Bhatt worked on the standard at Intel; the first integrated circuits supporting USB were produced by Intel in 1995.
The original USB 1.0 specification, which was introduced in January 1996, defined data transfer rates of 1.5 Mbit/s "Low Speed" and 12 Mbit/s "Full Speed". The first widely used version of USB was 1.1, which was released in September 1998. The 12 Mbit/s data rate was intended for higher-speed devices such as disk drives, and the lower 1.5 Mbit/s rate for low data rate devices such as joysticks.
The USB 2.0 specification was released in April 2000 and was ratified by the USB Implementers Forum (USB-IF) at the end of 2001. Hewlett-Packard, Intel, Lucent Technologies (now Alcatel-Lucent), NEC and Philips jointly led the initiative to develop a higher data transfer rate, with the resulting specification achieving 480 Mbit/s, a 40-times increase over the original USB 1.1 specification.
The USB 3.0 specification was published on 12 November 2008. Its main goals were to increase the data transfer rate (up to 5 Gbit/s), decrease power consumption, increase power output, and be backward compatible with USB 2.0. USB 3.0 includes a new, higher speed bus called SuperSpeed in parallel with the USB 2.0 bus. For this reason, the new version is also called SuperSpeed. The first USB 3.0 equipped devices were presented in January 2010.
s of 2008[ [update]], approximately six billion USB ports and interfaces were in the global marketplace, and about two billion were being sold each year.
In December 2014, USB-IF submitted USB 3.1, USB Power Delivery 2.0 and USB type-C specifications to the IEC (TC 100 – Audio, video and multimedia systems and equipment) for inclusion in the international standard IEC 62680 "Universal Serial Bus interfaces for data and power", which is currently based on USB 2.0.
Version history.
Prereleases.
The USB standard evolved through several versions before its official release in 1996:
USB 1.x.
Released in January 1996, USB 1.0 specified data rates of 1.5 Mbit/s ("Low Bandwidth" or "Low Speed") and 12 Mbit/s ("Full Bandwidth" or "Full Speed"). It did not allow for extension cables or pass-through monitors, due to timing and power limitations. Few USB devices made it to the market until USB 1.1 was released in August 1998, fixing problems identified in 1.0, mostly related to using hubs. USB 1.1 was the earliest revision that was widely adopted.
USB 2.0.
USB 2.0 was released in April 2000, adding a higher maximum signaling rate of 480 Mbit/s called "High Speed", in addition to the USB 1.x "Full Speed" signaling rate of 12 Mbit/s. Due to bus access constraints, the effective throughput of the "High Speed" signaling rate is limited to 35 MB/s or 280 Mbit/s.
Further modifications to the USB specification have been made via Engineering Change Notices (ECN). The most important of these ECNs are included into the USB 2.0 specification package available from USB.org:
USB 3.0.
USB 3.0 standard was released in November 2008, defining a new "SuperSpeed" mode. A USB 3.0 port, usually colored blue, is backward compatible with USB 2.0 devices and cables.
The USB 3.0 Promoter Group announced on 17 November 2008 that the specification of version 3.0 had been completed and had made the transition to the USB Implementers Forum (USB-IF), the managing body of USB specifications. This move effectively opened the specification to hardware developers for implementation in products.
The new "SuperSpeed" bus provides a fourth transfer mode with a data signaling rate of 5.0 Gbit/s, in addition to the modes supported by earlier versions. The payload throughput is 4 Gbit/s (due to the overhead induced by used 8b/10b encoding), and the specification considers it reasonable to achieve around 3.2 Gbit/s (0.4 GB/s or 400 MB/s), which should increase with future hardware advances. Communication is full-duplex in SuperSpeed transfer mode; in the modes supported previously, by 1.x and 2.0, communication is half-duplex, with direction controlled by the host.
As with previous USB versions, USB 3.0 ports come in low-power and high-power variants, providing 150 mA and 900 mA respectively, while simultaneously transmitting data at SuperSpeed rates. Additionally, there is a Battery Charging Specification (Version 1.2 – December 2010), which increases the power handling capability to 1.5 A but does "not" allow concurrent data transmission. The Battery Charging Specification requires that the physical ports themselves be capable of handling 5 A of current but limits the maximum current drawn to 1.5 A.
USB 3.1.
A January 2013 press release from the USB group revealed plans to update USB 3.0 to 10 Gbit/s. The group ended up creating a new USB version, USB 3.1, which was released on 31 July 2013, introducing a faster transfer mode called "SuperSpeed USB 10 Gbit/s", putting it on par with a single first-generation Thunderbolt channel. The new mode's logo features a "Superspeed+" caption (stylized as "SUPERSPEED+"). The USB 3.1 standard increases the data signaling rate to 10 Gbit/s in the "USB 3.1 Gen2" mode, double that of USB 3.0 (referred to as "USB 3.1 Gen1") and reduces line encoding overhead to just 3% by changing the encoding scheme to 128b/132b. The first USB 3.1 implementation demonstrated transfer speeds of 7.2 Gbit/s.
The USB 3.1 standard is backward compatible with USB 3.0 and USB 2.0.
USB type-C.
Developed at roughly the same time as the USB 3.1 specification, but distinctly from it, the USB type-C Specification 1.0 defines a new small reversible-plug connector for USB devices. The type-C plug connects to both hosts and devices, replacing various type-B and type-A connectors and cables with a standard meant to be future-proof, similarly to Apple Lightning and Thunderbolt. The 24-pin double-sided connector provides four power/ground pairs, two differential pairs for USB 2.0 data bus (though only one pair is implemented in a type-C cable), four pairs for high-speed data bus, two "sideband use" pins, and two configuration pins for cable orientation detection, dedicated biphase mark code (BMC) configuration data channel, and VCONN +5 V power for active cables. Type-A and type-B adaptors/cables will be required for older devices in order to plug into type-C hosts; adaptors/cables with a type-C receptacle are not allowed.
Full-featured USB type-C cables are active, electronically marked cables that contain a chip with an ID function based on the configuration data channel and vendor-defined messages (VDMs) from the USB Power Delivery 2.0 specification. USB type-C devices also support power currents of 1.5 A and 3.0 A over the 5 V power bus in addition to baseline 900 mA; devices can either negotiate increased USB current through the configuration line, or they can support the full Power Delivery specification using both BMC-coded configuration line and legacy BFSK-coded VBUS line.
Alternate Mode dedicates some of the physical wires in the type-C cable for direct device-to-host transmission of alternate data protocols. The four high-speed lanes, two sideband pins, and—&#X200B;for dock, detachable device and permanent cable applications only—&#X200B;two USB 2.0 pins and one configuration pin can be used for Alternate Mode transmission. The modes are configured using VDMs through the configuration channel. As of December 2014, Alt Mode implementations include DisplayPort 1.3 and MHL 3.0; other serial protocols like PCI Express and Base-T Ethernet are possible.
Among the first devices to accommodate a type-C cable are the Nokia N1 tablet, Apple's 2015 MacBook (which has only a single first-generation USB 3.1 port), and Google's second Chromebook Pixel. The first smartphone that accommodates a USB type-C cable is from a Chinese company LeTV. 
System design.
The design architecture of USB is asymmetrical in its topology, consisting of a host, a multitude of downstream USB ports, and multiple peripheral devices connected in a tiered-star topology. Additional USB hubs may be included in the tiers, allowing branching into a tree structure with up to five tier levels. A USB host may implement multiple host controllers and each host controller may provide one or more USB ports. Up to 127 devices, including hub devices if present, may be connected to a single host controller. USB devices are linked in series through hubs. One hub—built into the host controller—is the root hub.
A physical USB device may consist of several logical sub-devices that are referred to as "device functions". A single device may provide several functions, for example, a webcam (video device function) with a built-in microphone (audio device function). This kind of device is called a "composite device". An alternative to this is "compound device," in which the host assigns each logical device a distinctive address and all logical devices connect to a built-in hub that connects to the physical USB cable.
USB device communication is based on "pipes" (logical channels). A pipe is a connection from the host controller to a logical entity, found on a device, and named an "endpoint". Because pipes correspond 1-to-1 to endpoints, the terms are sometimes used interchangeably. A USB device could have up to 32 endpoints (16 IN, 16 OUT), though it's rare to have so many. An endpoint is defined and numbered by the device during initialization (the period after physical connection called "enumeration") and so is relatively permanent, whereas a pipe may be opened and closed.
There are two types of pipe: stream and message. A message pipe is bi-directional and is used for "control" transfers. Message pipes are typically used for short, simple commands to the device, and a status response, used, for example, by the bus control pipe number 0. A stream pipe is a uni-directional pipe connected to a uni-directional endpoint that transfers data using an "isochronous", "interrupt", or "bulk" transfer:
An endpoint of a pipe is addressable with a tuple "(device_address, endpoint_number)" as specified in a TOKEN packet that the host sends when it wants to start a data transfer session. If the direction of the data transfer is from the host to the endpoint, an OUT packet (a specialization of a TOKEN packet) having the desired device address and endpoint number is sent by the host. If the direction of the data transfer is from the device to the host, the host sends an IN packet instead. If the destination endpoint is a uni-directional endpoint whose manufacturer's designated direction does not match the TOKEN packet (e.g., the manufacturer's designated direction is IN while the TOKEN packet is an OUT packet), the TOKEN packet is ignored. Otherwise, it is accepted and the data transaction can start. A bi-directional endpoint, on the other hand, accepts both IN and OUT packets.
Endpoints are grouped into "interfaces" and each interface is associated with a single device function. An exception to this is endpoint zero, which is used for device configuration and is not associated with any interface. A single device function composed of independently controlled interfaces is called a "composite device". A composite device only has a single device address because the host only assigns a device address to a function.
When a USB device is first connected to a USB host, the USB device enumeration process is started. The enumeration starts by sending a reset signal to the USB device. The data rate of the USB device is determined during the reset signaling. After reset, the USB device's information is read by the host and the device is assigned a unique 7-bit address. If the device is supported by the host, the device drivers needed for communicating with the device are loaded and the device is set to a configured state. If the USB host is restarted, the enumeration process is repeated for all connected devices.
The host controller directs traffic flow to devices, so no USB device can transfer any data on the bus without an explicit request from the host controller. In USB 2.0, the host controller polls the bus for traffic, usually in a round-robin fashion. The throughput of each USB port is determined by the slower speed of either the USB port or the USB device connected to the port.
High-speed USB 2.0 hubs contain devices called transaction translators that convert between high-speed USB 2.0 buses and full and low speed buses. When a high-speed USB 2.0 hub is plugged into a high-speed USB host or hub, it operates in high-speed mode. The USB hub then uses either one transaction translator per hub to create a full/low-speed bus routed to all full and low speed devices on the hub, or uses one transaction translator per port to create an isolated full/low-speed bus per port on the hub.
Because there are two separate controllers in each USB 3.0 host, USB 3.0 devices transmit and receive at USB 3.0 data rates regardless of USB 2.0 or earlier devices connected to that host. Operating data rates for earlier devices are set in the legacy manner.
Device classes.
The functionality of USB devices is defined by class codes, communicated to the USB host to affect the loading of suitable software driver modules for each connected device. This provides for adaptability and device independence of the host to support new devices from different manufacturers.
Device classes include:
USB mass storage / USB drive.
USB implements connections to storage devices using a set of standards called the USB mass storage device class (MSC or UMS). This was at first intended for traditional magnetic and optical drives and has been extended to support flash drives. It has also been extended to support a wide variety of novel devices as many systems can be controlled with the familiar metaphor of file manipulation within directories. The process of making a novel device look like a familiar device is also known as extension. The ability to boot a write-locked SD card with a USB adapter is particularly advantageous for maintaining the integrity and non-corruptible, pristine state of the booting medium.
Though most post-Summer 2004 computers can boot from USB mass storage devices, USB is not intended as a primary bus for a computer's internal storage. Buses such as Parallel ATA (PATA or IDE), Serial ATA (SATA), or SCSI fulfill that role in PC class computers. However, USB has one important advantage, in that it is possible to install and remove devices without rebooting the computer (hot-swapping), making it useful for mobile peripherals, including drives of various kinds.
Firstly conceived and still used today for optical storage devices (CD-RW drives, DVD drives, etc.), several manufacturers offer external portable USB hard disk drives, or empty enclosures for disk drives. These offer performance comparable to internal drives, limited by the current number and types of attached USB devices, and by the upper limit of the USB interface (in practice about 30 MB/s for USB 2.0 and potentially 400 MB/s or more for USB 3.0). These external drives typically include a "translating device" that bridges between a drive's interface to a USB interface port. Functionally, the drive appears to the user much like an internal drive. Other competing standards for external drive connectivity include eSATA, ExpressCard (now at version 2.0), FireWire (IEEE 1394), and most recently Thunderbolt.
Another use for USB mass storage devices is the portable execution of software applications (such as web browsers and VoIP clients) with no need to install them on the host computer.
Media Transfer Protocol.
Media Transfer Protocol (MTP) was designed by Microsoft to give higher-level access to a device's filesystem than USB mass storage, at the level of files rather than disk blocks. It also has optional DRM features. MTP was designed for use with portable media players, but it has since been adopted as the primary storage access protocol of the Android operating system from the version 4.1 Jelly Bean as well as Windows Phone 8 (Windows Phone 7 devices had used the Zune protocol which was an evolution of MTP). The primary reason for this is that MTP does not require exclusive access to the storage device the way UMS does, alleviating potential problems should an Android program request the storage while it is attached to a computer. The main drawback is that MTP is not as well supported outside of Windows operating systems.
Human interface devices.
Joysticks, keypads, tablets and other human-interface devices (HIDs) are also progressively migrating from MIDI, and PC game port connectors to USB.
USB mice and keyboards can usually be used with older computers that have PS/2 connectors with the aid of a small USB-to-PS/2 adapter. For mice and keyboards with dual-protocol support, an adaptor that contains no logic circuitry may be used: the hardware in the USB keyboard or mouse is designed to detect whether it is connected to a USB or PS/2 port, and communicate using the appropriate protocol. Converters also exist that connect PS/2 keyboards and mice (usually one of each) to a USB port. These devices present two HID endpoints to the system and use a microcontroller to perform bidirectional data translation between the two standards.
Device Firmware Upgrade.
"Device Firmware Upgrade" (DFU) is a vendor- and device-independent mechanism for upgrading the firmware of USB devices with improved versions provided by their manufacturers, offering (for example) a way for firmware bugfixes to be deployed. During the firmware upgrade operation, USB devices change their operating mode effectively becoming a PROM programmer. Any class of USB device can implement this capability by following the official DFU specifications.
In addition to its intended legitimate purposes, DFU can also be exploited by uploading maliciously crafted firmwares that cause USB devices to spoof various other device types; one such exploiting approach is known as BadUSB.
Connectors and plugs.
Connector properties.
The connectors the USB committee specifies support a number of USB's underlying goals, and reflect lessons learned from the many connectors the computer industry has used. The connector mounted on the host or device is called the "receptacle", and the connector attached to the cable is called the "plug". The official USB specification documents also periodically define the term "male" to represent the plug, and "female" to represent the receptacle.
Usability and orientation.
By design, it is difficult to insert a USB plug into its receptacle incorrectly. The USB specification states that the required USB icon must be embossed on the "topside" of the USB plug, which "...provides easy user recognition and facilitates alignment during the mating process." The specification also shows that the "recommended" "Manufacturer's logo" ("engraved" on the diagram but not specified in the text) is on the opposite side of the USB icon. The specification further states, "The USB Icon is also located adjacent to each receptacle. Receptacles should be oriented to allow the icon on the plug to be visible during the mating process." However, the specification does not consider the height of the device compared to the eye level height of the user, so the side of the cable that is "visible" when mated to a computer on a desk can depend on whether the user is standing or kneeling.
While it would have been better for usability if the cable could be plugged in with either side up, the original design left this out to make manufacturing as inexpensive as possible. Ajay Bhatt, who was involved in the original USB design team, is working on a new design to make the cable insertable either side up. The new reversible type-C plug is an addition to the USB 3.1 specification; it is much smaller than the current USB 3.0 micro-B connector
Only moderate force is needed to insert or remove a USB cable. USB cables and small USB devices are held in place by the gripping force from the receptacle (without need of the screws, clips, or thumb-turns other connectors have required).
Power-use topology.
The standard connectors were deliberately intended to enforce the directed topology of a USB network: type A connectors on host devices that supply power and type B connectors on target devices that draw power. This is intended to prevent users from accidentally connecting two USB power supplies to each other, which could lead to short circuits and dangerously high currents, circuit failures, or even fire. USB does not support cyclic networks and the standard connectors from incompatible USB devices are themselves incompatible.
However, some of this directed topology is lost with the advent of multi-purpose USB connections (such as USB On-The-Go in smartphones, and USB-powered Wi-Fi routers), which require A-to-A, B-to-B, and sometimes Y/splitter cables. See the USB On-The-Go connectors section below, for a more detailed summary description.
Durability.
The standard connectors were designed to be robust. Because USB is hot-pluggable, the connectors would be used more frequently, and perhaps with less care, than other connectors. Many previous connector designs were fragile, specifying embedded component pins or other delicate parts that were vulnerable to bending or breaking. The electrical contacts in a USB connector are protected by an adjacent plastic tongue, and the entire connecting assembly is usually protected by an enclosing metal sheath.
The connector construction always ensures that the external sheath on the plug makes contact with its counterpart in the receptacle before any of the four connectors within make electrical contact. The external metallic sheath is typically connected to system ground, thus dissipating damaging static charges. This enclosure design also provides a degree of protection from electromagnetic interference to the USB signal while it travels through the mated connector pair (the only location when the otherwise twisted data pair travels in parallel). In addition, because of the required sizes of the power and common connections, they are made after the system ground but before the data connections. This type of staged make-break timing allows for electrically safe hot-swapping.
The newer micro-USB receptacles are designed for up to 10,000 cycles of insertion and removal between the receptacle and plug, compared to 1,500 for the standard USB and 5,000 for the mini-USB receptacle. To accomplish this, a locking device was added and the leaf-spring was moved from the jack to the plug, so that the most-stressed part is on the cable side of the connection. This change was made so that the connector on the less expensive cable would bear the most wear instead of the more expensive micro-USB device.
Compatibility.
The USB standard specifies relatively loose tolerances for compliant USB connectors to minimize physical incompatibilities in connectors from different vendors. To address a weakness present in some other connector standards, the USB specification also defines limits to the size of a connecting device in the area around its plug. This was done to prevent a device from blocking adjacent ports due to the size of the cable strain relief mechanism (usually molding integral with the cable outer insulation) at the connector. Compliant devices must either fit within the size restrictions or support a compliant extension cable that does.
In general, USB cables have only plugs on their ends, while hosts and devices have only receptacles. Hosts almost universally have type-A receptacles, while devices have one or another type-B variety. Type-A plugs mate only with type-A receptacles, and the same applies to their type-B counterparts; they are deliberately physically incompatible. However, an extension to the USB standard specification called USB On-The-Go (OTG) allows a single port to act as either a host or a device, what is selectable by the end of the cable that plugs into the receptacle on the OTG-enabled unit. Even after the cable is hooked up and the units are communicating, the two units may "swap" ends under program control. This capability is meant for units such as PDAs in which the USB link might connect to a PC's host port as a device in one instance, yet connect as a host itself to a keyboard and mouse device in another instance.
USB 3.0 connectors.
Type-A plugs and receptacles from both USB 3.0 and USB 2.0 are designed to interoperate. Type-B plugs and receptacles in USB 3.0 are somewhat larger than those in USB 2.0; thus, USB 2.0 type-B plugs can fit into USB 3.0 type-B receptacles, while the opposite is not possible.
Connector types.
There are several types of USB connector, including some that have been added while the specification progressed. The original USB specification detailed standard-A and standard-B plugs and receptacles; the B connector was necessary so that cabling could be plug ended at both ends and still prevent users from connecting one computer receptacle to another. The first engineering change notice to the USB 2.0 specification added mini-B plugs and receptacles.
The data pins in the standard-A plug are actually recessed in the plug compared to the outside power pins. This permits the power pins to connect first, preventing data errors by allowing the device to power up first and then establish the data connection. Also, some devices operate in different modes depending on whether the data connection is made.
To reliably enable a charge-only feature, modern USB accessory peripherals now include charging cables that provide power connections to the host port but no data connections, and both home and vehicle charging docks are available that supply power from a converter device and do not include a host device and data pins, allowing any capable USB device to charge or operate from a standard USB cable.
Standard connectors.
The USB 2.0 standard-A type of USB plug is a flattened rectangle that inserts into a "downstream-port" receptacle on the USB host, or a hub, and carries both power and data. This plug is frequently seen on cables that are permanently attached to a device, such as one connecting a keyboard or mouse to the computer via USB connection.
USB connections eventually wear out as the connection loosens through repeated plugging and unplugging. The lifetime of a USB-A male connector is approximately 1,500 connect/disconnect cycles.
A standard-B plug—&#X200B;which has a square shape with beveled exterior corners—&#X200B;typically plugs into an "upstream receptacle" on a device that uses a removable cable, e.g., a printer. On some devices, the type-B receptacle has no data connections, being used solely for accepting power from the upstream device. This two-connector-type scheme (A/B) prevents a user from accidentally creating an electrical loop.
Maximum allowed size of the "overmold boot" (which is part of the connector used for its handling) is 16 by 8 mm for the standard-A plug type, while for the type B it is 11.5 by 10.5 mm.
Mini and micro connectors.
Various connectors have been used for smaller devices such as digital cameras, smartphones, and tablet computers. These include the now-deprecated (i.e. de-certified but standardized) mini-A and mini-AB connectors; mini-B connectors are still supported, but are not OTG-compliant (On The Go, used in mobile devices). The mini-B USB connector was standard for transferring data to and from the early smartphones and PDAs. Both mini-A and mini-B plugs are approximately 3 by 7 mm; the mini-A connector and the mini-AB receptacle connector were deprecated on 23 May 2007.
The micro-USB connector was announced by the USB-IF on 4 January 2007. Micro-USB plugs have a similar width to mini-USB, but approximately half the thickness, enabling their integration into thinner portable devices. The micro-A connector is 6.85 by 1.8 mm with a maximum overmold boot size of 11.7 by 8.5 mm, while the micro-B connector is 6.85 by 1.8 mm with a maximum overmold size of 10.6 by 8.5 mm.
The thinner micro connectors are intended to replace the mini connectors in new devices including smartphones, personal digital assistants, and cameras. While some of the devices and cables still use the older mini variant, the newer micro connectors are widely adopted, and as of 2010[ [update]] they are the most widely used. 
The micro plug design is rated for at least 10,000 connect-disconnect cycles, which is more than the mini plug design. The micro connector is also designed to reduce the mechanical wear on the device; instead the easier-to-replace cable is designed to bear the mechanical wear of connection and disconnection. The "Universal Serial Bus Micro-USB Cables and Connectors Specification" details the mechanical characteristics of micro-A plugs, micro-AB receptacles (which accept both micro-A and micro-B plugs), and micro-B plugs and receptacles, along with a standard-A receptacle to micro-A plug adapter.
The cellular phone carrier group Open Mobile Terminal Platform (OMTP) in 2007 endorsed micro-USB as the standard connector for data and power on mobile devices In addition, on 22 October 2009 the International Telecommunication Union (ITU) has also announced that it had embraced micro-USB as the "Universal Charging Solution" its "energy-efficient one-charger-fits-all new mobile phone solution", and added: "Based on the Micro-USB interface, UCS chargers also include a 4-star or higher efficiency rating—&#X200B;up to three times more energy-efficient than an unrated charger."
The European Standardisation Bodies CEN, CENELEC and ETSI (independent of the OMTP/GSMA proposal) defined a common External Power Supply (EPS) for use with smartphones sold in the EU based on micro-USB. 14 of the world's largest mobile phone manufacturers signed the EU's common EPS Memorandum of Understanding (MoU). Apple, one of the original MoU signers, makes micro-USB adapters available – as permitted in the Common EPS MoU – for its iPhones equipped with Apple's proprietary 30 pin dock connector or (later) "Lightning" connector.
USB On-The-Go connectors.
All current USB On-The-Go (OTG) devices are required to have one, and only one, USB connector: a micro-AB receptacle. Non-OTG compliant devices are not allowed to use the micro-AB receptacle, due to power supply shorting hazards on the VBUS line. The micro-AB receptacle is capable of accepting both micro-A and micro-B plugs, attached to any of the legal cables and adapters as defined in revision 1.01 of the micro-USB specification. Prior to the development of micro-USB, USB On-The-Go devices were required to use mini-AB receptacles to perform the equivalent job.
To enable type-AB receptacles to distinguish which end of a cable is plugged in, mini and micro plugs have an "ID" pin in addition to the four contacts found in standard-size USB connectors. This ID pin is connected to GND in type-A plugs, and left unconnected in type-B plugs. Typically, a pull-up resistor in the device is used to detect the presence or absence of an ID connection.
The OTG device with the A-plug inserted is called the A-device and is responsible for powering the USB interface when required and by default assumes the role of host. The OTG device with the B-plug inserted is called the B-device and by default assumes the role of peripheral. An OTG device with no plug inserted defaults to acting as a B-device. If an application on the B-device requires the role of host, then the Host Negotiation Protocol (HNP) is used to temporarily transfer the host role to the B-device.
OTG devices attached either to a peripheral-only B-device or a standard/embedded host have their role fixed by the cable, since in these scenarios it is only possible to attach the cable one way.
Host and device interface receptacles.
A pre-USB 3.0 connector (receptacles and plugs) mating matrix is displayed below. As a note, the assignment of functions (VCC, D+, D−, GND and ID) to pin numbers marked below is mostly consistent, with the exception of mini and micro connectors. When compared to standard-sized connectors (type-A and type-B), mini and micro connectors have their GND connections moved from pin #4 to pin #5, while their pin #4 serves as an ID pin for the On-The-Go host/client identification.
Cable plugs (USB 1.x/2.0).
USB cables exist with various combinations of plugs on each end of the cable, as displayed below. Notes from the section above apply here as well.
Cable plugs (USB 3.0).
USB 3.0 introduced new standard and micro-sized type A and type B plugs and receptacles. The 3.0 receptacles are backward-compatible with the corresponding pre-3.0 plugs. See the micro-B cable plug photo on the right. The micro-B 3.0 plug effectively consists of a standard USB 1.x/2.0 micro-B cable plug, with an additional five-pin plug "stacked" to the side of it. In this way, USB 3.0 micro-A host connectors can achieve backward compatibility with the USB 1.x/2.0 micro-B cable plugs.
Pinouts.
USB is a serial bus, using four shielded wires for the USB 2.0 variant: two for power (VBUS and GND), and two for differential data signals (labelled as D+ and D− in pinouts). Non-Return-to-Zero Inverted (NRZI) encoding scheme is used for transferring data, with a sync field to synchronise the host and receiver clocks. D+ and D− signals are transmitted on a twisted pair, providing half-duplex data transfers for USB 2.0.
USB 3.0 provides two additional twisted pairs (four wires, SSTx+, SSTx−, SSRx+ and SSRx−), providing full-duplex data transfers at "super-speed", which makes it similar to Serial ATA or single-lane PCI Express.
Proprietary connectors and formats.
Manufacturers of personal electronic devices might not include a USB standard connector on their product for technical or marketing reasons. Some manufacturers provide proprietary cables that permit their devices to physically connect to a USB standard port. Full functionality of proprietary ports and cables with USB standard ports is not assured; for example, some devices only use the USB connection for battery charging and do not implement any data transfer functions.
Colors.
USB ports and connectors are often color-coded to distinguish their different functions and USB versions. These colors are not part of the USB specification and can vary between manufacturers; for example, USB 3.0 specification mandates appropriate color-coding while it only recommends blue inserts for standard-A USB 3.0 connectors and plugs.
Cabling.
The data cables for USB 1.x and USB 2.x use a twisted pair to reduce noise and crosstalk. USB 3.0 cables contain twice as many wires as USB 2.x to support SuperSpeed data transmission, and are thus larger in diameter.
The USB 1.1 standard specifies that a standard cable can have a maximum length of 5 meters with devices operating at Full Speed (12 Mbit/s), and a maximum length of 3 meters with devices operating at Low Speed (1.5 Mbit/s).
USB 2.0 provides for a maximum cable length of 5 meters for devices running at Hi Speed (480 Mbit/s). The primary reason for this limit is the maximum allowed round-trip delay of about 1.5 μs. If USB host commands are unanswered by the USB device within the allowed time, the host considers the command lost. When adding USB device response time, delays from the maximum number of hubs added to the delays from connecting cables, the maximum acceptable delay per cable amounts to 26 ns. The USB 2.0 specification requires that cable delay be less than 5.2 ns per meter (192 000 km/s, which is close to the maximum achievable transmission speed for standard copper wire).
The USB 3.0 standard does not directly specify a maximum cable length, requiring only that all cables meet an electrical specification: for copper cabling with AWG 26 wires the maximum practical length is 3 m.
Power.
The USB 1.x and 2.0 specifications provide a 5 V supply on a single wire to power connected USB devices. The specification provides for no more than 5.25 V and no less than 4.75 V (5 V ± 5%) between the positive and negative bus power lines (VBUS voltage). For USB 3.0, the voltage supplied by low-powered hub ports is 4.45–5.25 V.
A unit load is defined as 100 mA in USB 1.x and 2.0, and 150 mA in USB 3.0. A device may draw a maximum of five unit loads (500 mA) from a port in USB 1.x and 2.0, or six unit loads (900 mA) in USB 3.0. There are two types of device: low-power and high-power. A low-power device (such as a USB HID) draws at most one-unit load, with minimum operating voltage of 4.4 V in USB 2.0, and 4 V in USB 3.0. A high-power device draws, at most, the maximum number of unit loads the standard permits. Every device functions initially as low-power (including high-power functions during their low-power enumeration phases), but may request high-power, and get it if available on the providing bus.
Some devices, such as high-speed external disk drives, require more than 500 mA of current and therefore may have power issues if powered from just one USB 2.0 port: erratic function, failure to function, or overloading/damaging the port. Such devices may come with an external power source or a Y-shaped cable that has two USB connectors (one for power and data, the other for power only) to plug into a computer. With such a cable, a device can draw power from two USB ports simultaneously. However, USB compliance specification states that "use of a 'Y' cable (a cable with two A-plugs) is prohibited on any USB peripheral", meaning that "if a USB peripheral requires more power than allowed by the USB specification to which it is designed, then it must be self-powered."
A bus-powered hub initializes itself at one-unit load and transitions to maximum unit loads after it completes hub configuration. Any device connected to the hub draws one-unit load regardless of the current draw of devices connected to other ports of the hub (i.e., one device connected on a four-port hub draws only one-unit load despite the fact that more unit loads are being supplied to the hub).
A self-powered hub supplies maximum supported unit loads to any device connected to it. In addition, the "VBUS" presents one-unit load upstream for communication if parts of the Hub are powered down.
Charging ports.
The "USB Battery Charging Specification Revision 1.1" (released in 2007) defines a new type of USB port, called the "charging port". Contrary to the "standard downstream port", for which current draw by a connected portable device can exceed 100 mA only after digital negotiation with the host or hub, a charging port can supply currents between 500 mA and 1.5 A without the digital negotiation. A charging port supplies up to 500 mA at 5 V, up to the rated current at 3.6 V or more, and drops its output voltage if the portable device attempts to draw more than the rated current. The charger port may shut down if the load is too high.
Two types of charging port exist: the "charging downstream port" (CDP), supporting data transfers as well, and the "dedicated charging port" (DCP), without data support. A portable device can recognize the type of USB port; on a dedicated charging port, the D+ and D− pins are shorted with a resistance not exceeding 200 ohms, while charging downstream ports provide additional detection logic so their presence can be determined by attached devices.
With charging downstream ports, current passing through the thin ground wire may interfere with high-speed data signals; therefore, current draw may not exceed 900 mA during high-speed data transfer. A dedicated charge port may have a rated current between 500 and 1,500 mA. For all charging ports, there is maximum current of 5 A, as long as the connector can handle the current (standard USB 2.0 A-connectors are rated at 1.5 A).
Before the battery charging specification was defined, there was no standardized way for the portable device to inquire how much current was available. For example, Apple's iPod and iPhone chargers indicate the available current by voltages on the D− and D+ lines. When D+ = D− = 2.0 V, the device may pull up to 500 mA. When D+ = 2.0 V and D− = 2.8 V, the device may pull up to 1 A of current. When D+ = 2.8 V and D− = 2.0 V, the device may pull up to 2 A of current.
Dedicated charging ports can be found on USB power adapters that convert utility power or another power source (e.g., a car's electrical system) to run attached devices and battery packs. On a host (such as a laptop computer) with both standard and charging USB ports, the charging ports should be labeled as such.
To support simultaneous charge and data communication, even if the communication port does not support charging a demanding device, so-called "accessory charging adapters (ACA)" are introduced. By using an accessory charging adapter, a device providing a single USB port can be attached to both a charger, and another USB device at the same time.
The "USB Battery Charging Specification Revision 1.2" (released in 2010) makes clear that there are safety limits to the rated current at 5 A coming from USB 2.0. On the other hand, several changes are made and limits are increasing including allowing 1.5 A on charging downstream ports for unconfigured devices, allowing high speed communication while having a current up to 1.5 A, and allowing a maximum current of 5 A. Also, revision 1.2 removes support for USB ports type detection via resistive detection mechanisms.
USB Power Delivery.
In July 2012, the USB Promoters Group announced the finalization of the USB Power Delivery ("PD") specification, an extension that specifies using certified "PD aware" USB cables with standard USB type A and B connectors to deliver increased power (more than 7.5 W) to devices with larger power demand. Devices can request higher currents and supply voltages from compliant hosts – up to 2 A at 5 V (for a power consumption of up to 10 W), and optionally up to 3 A or 5 A at either 12 V (36 W or 60 W) or 20 V (60 W or 100 W). In all cases, both host-to-device and device-to-host configurations are supported.
The intent is to permit uniformly charging laptops, tablets, USB-powered disks and similarly higher power consumer electronics, as a natural extension of existing European and Chinese mobile telephone charging standards. This may also affect the way electric power used for small devices is transmitted and used in both residential and public buildings.
The Power Delivery specification defines six fixed power profiles for the power sources. PD-aware devices implement a flexible power management scheme by interfacing with the power source through a bidirectional data channel and requesting a certain level of electrical power, variable up to 5 A and 20 V depending on supported profile. The power configuration protocol uses a 24 MHz BFSK-coded transmission channel on the VBUS line.
The USB Power Delivery revision 2.0 specification has been released as part of the USB 3.1 suite. It covers the type-C cable and connector with four power/ground pairs and a separate configuration channel, which now hosts a DC coupled low-frequency BMC-coded data channel that reduces the possibilities for RF interference. Power Delivery protocols have been updated to facilitate type-C features such as cable ID function, Alternate Mode negotiation, increased VBUS currents, and VCONN-powered accessories.
Sleep-and-charge ports.
Sleep-and-charge USB ports can be used to charge electronic devices even when the computer is switched off. Normally, when a computer is powered off the USB ports are powered down, preventing phones and other devices from charging. Sleep-and-charge USB ports remain powered even when the computer is off. On laptops, charging devices from the USB port when it is not being powered from AC drains the laptop battery faster; most laptops have a facility to stop charging if their own battery charge level gets too low.
Sleep-and-charge USB ports may be found colored differently than regular ports, mostly red or yellow, though that is not always the case.
On Dell and Toshiba laptops, the port is marked with the standard USB symbol with an added lightning bolt icon on the right side. Dell calls this feature "PowerShare", while Toshiba calls it "USB Sleep-and-Charge". On Acer Inc. and Packard Bell laptops, sleep-and-charge USB ports are marked with a non-standard symbol (the letters "USB" over a drawing of a battery); the feature is simply called "Power-off USB". On some laptops such as Dell and Apple MacBook models, it is possible to plug a device in, close the laptop (putting it into sleep mode) and have the device continue to charge.
Mobile device charger standards.
In China.
s of 14 2007[ [update]], all new mobile phones applying for a license in China are required to use a USB port as a power port for battery charging. This was the first standard to use the convention of shorting D+ and D-.
OMTP/GSMA Universal Charging Solution.
In September 2007, the Open Mobile Terminal Platform group (a forum of mobile network operators and manufacturers such as Nokia, Samsung, Motorola, Sony Ericsson and LG) announced that its members had agreed on micro-USB as the future common connector for mobile devices.
The GSM Association (GSMA) followed suit on 17 February 2009, and on 22 April 2009, this was further endorsed by the CTIA – The Wireless Association, with the International Telecommunication Union (ITU) announcing on 22 October 2009 that it had also embraced the Universal Charging Solution as its "energy-efficient one-charger-fits-all new mobile phone solution", and added: "Based on the Micro-USB interface, UCS chargers will also include a 4-star or higher efficiency rating—up to three times more energy-efficient than an unrated charger."
EU Smartphone Power Supply Standard.
In June 2009, many of the world's largest mobile phone manufacturers signed an EC-sponsored Memorandum of Understanding (MoU), agreeing to make most data-enabled mobile phones marketed in the European Union compatible with a common External Power Supply (EPS). The EU's common EPS specification (EN 62684:2010) references the USB Battery Charging standard and is similar to the GSMA/OMTP and Chinese charging solutions. In January 2011, the International Electrotechnical Commission (IEC) released its version of the (EU's) common EPS standard as IEC 62684:2011.
Non-standard devices.
Some USB devices require more power than is permitted by the specifications for a single port. This is common for external hard and optical disc drives, and generally for devices with motors or lamps. Such devices can use an external power supply, which is allowed by the standard, or use a dual-input USB cable, one input of which is used for power and data transfer, the other solely for power, which makes the device a non-standard USB device. Some USB ports and external hubs can, in practice, supply more power to USB devices than required by the specification but a standard-compliant device may not depend on this.
In addition to limiting the total average power used by the device, the USB specification limits the inrush current (i.e., that used to charge decoupling and filter capacitors) when the device is first connected. Otherwise, connecting a device could cause problems with the host's internal power. USB devices are also required to automatically enter ultra low-power suspend mode when the USB host is suspended. Nevertheless, many USB host interfaces do not cut off the power supply to USB devices when they are suspended.
Some non-standard USB devices use the 5 V power supply without participating in a proper USB network, which negotiates power draw with the host interface. These are usually called "USB decorations". Examples include USB-powered keyboard lights, fans, mug coolers and heaters, battery chargers, miniature vacuum cleaners, and even miniature lava lamps. In most cases, these items contain no digital circuitry, and thus are not standard compliant USB devices. This may cause problems with some computers, such as drawing too much current and damaging circuitry. Prior to the Battery Charging Specification, the USB specification required that devices connect in a low-power mode (100 mA maximum) and communicate their current requirements to the host, which then permits the device to switch into high-power mode.
Some devices, when plugged into charging ports, draw even more power (10 watts or 2.1 amperes) than the Battery Charging Specification allows. The iPad and MiFi 2200 are two such devices.
Barnes & Noble NOOK Color devices also require a special charger that runs at 1.9 amperes.
PoweredUSB.
PoweredUSB is a proprietary extension that adds four additional pins supplying up to 6 A at 5 V, 12 V, or 24 V. It is commonly used in point of sale systems to power peripherals such as barcode readers, credit card terminals, and printers.
Signaling.
USB allows the following signaling rates (the terms "speed" and "bandwidth" are used interchangeably, while "high-" is alternatively written as "hi-"):
USB signals are transmitted using differential signaling on a twisted-pair data cable with 90 Ω ±15% characteristic impedance.
A USB connection is always between a host or hub at the "A" connector end, and a device or hub's "upstream" port at the other end. Originally, this was a "B" connector, preventing erroneous loop connections, but additional upstream connectors were specified, and some cable vendors designed and sold cables that permitted erroneous connections (and potential damage to circuitry). USB interconnections are not as fool-proof or as simple as originally intended.
The host includes 15 kΩ pull-down resistors on each data line. When no device is connected, this pulls both data lines low into the so-called "single-ended zero" state (SE0 in the USB documentation), and indicates a reset or disconnected connection.
A USB device pulls one of the data lines high with a 1.5 kΩ resistor. This overpowers one of the pull-down resistors in the host and leaves the data lines in an idle state called "J". For USB 1.x, the choice of data line indicates of what signal rates the device is capable; full-bandwidth devices pull D+ high, while low-bandwidth devices pull D− high. The "k" state is just the opposite polarity to the "j" state.
USB data is transmitted by toggling the data lines between the J state and the opposite K state. USB encodes data using the NRZI line coding; a 0 bit is transmitted by toggling the data lines from J to K or vice versa, while a 1 bit is transmitted by leaving the data lines as-is. To ensure a minimum density of signal transitions remains in the bitstream, USB uses bit stuffing; an extra 0 bit is inserted into the data stream after any appearance of six consecutive 1 bits. Seven consecutive received 1 bits is always an error. USB 3.0 has introduced additional data transmission encodings.
A USB packet begins with an 8-bit synchronization sequence '00000001'. That is, after the initial idle state J, the data lines toggle KJKJKJKK. The final 1 bit (repeated K state) marks the end of the sync pattern and the beginning of the USB frame. For high bandwidth USB, the packet begins with a 32-bit synchronization sequence.
A USB packet's end, called EOP (end-of-packet), is indicated by the transmitter driving 2 bit times of SE0 (D+ and D− both below max) and 1 bit time of J state. After this, the transmitter ceases to drive the D+/D− lines and the aforementioned pull up resistors hold it in the J (idle) state. Sometimes skew due to hubs can add as much as one bit time before the SE0 of the end of packet. This extra bit can also result in a "bit stuff violation" if the six bits before it in the CRC are '1's. This bit should be ignored by receiver.
A USB bus is reset using a prolonged (10 to 20 milliseconds) SE0 signal.
USB 2.0 devices use a special protocol during reset, called "chirping", to negotiate the high bandwidth mode with the host/hub. A device that is HS capable first connects as an FS device (D+ pulled high), but upon receiving a USB RESET (both D+ and D− driven LOW by host for 10 to 20 ms) it pulls the D− line high, known as chirp K. This indicates to the host that the device is high bandwidth. If the host/hub is also HS capable, it chirps (returns alternating J and K states on D− and D+ lines) letting the device know that the hub operates at high bandwidth. The device has to receive at least three sets of KJ chirps before it changes to high bandwidth terminations and begins high bandwidth signaling. Because USB 3.0 uses wiring separate and additional to that used by USB 2.0 and USB 1.x, such bandwidth negotiation is not required.
Clock tolerance is 480.00 Mbit/s ±500 ppm, 12.000 Mbit/s ±2500 ppm, 1.50 Mbit/s ±15000 ppm.
Though high bandwidth devices are commonly referred to as "USB 2.0" and advertised as "up to 480 Mbit/s", not all USB 2.0 devices are high bandwidth. The USB-IF certifies devices and provides licenses to use special marketing logos for either "basic bandwidth" (low and full) or high bandwidth after passing a compliance test and paying a licensing fee. All devices are tested according to the latest specification, so recently compliant low bandwidth devices are also 2.0 devices.
USB 3 uses tinned copper stranded AWG-28 cables with impedance for its high-speed differential pairs and linear feedback shift register and 8b/10b encoding sent with a voltage of 1 V nominal with a 100 mV receiver threshold; the receiver uses equalization. SSC clock and 300 ppm precision is used. Packet headers are protected with CRC-16, while data payload is protected with CRC-32.
Power up to 3.6 W may be used. One unit load in superspeed mode is equal to 150 mA.
Transmission rates.
The theoretical maximum data rate in USB 2.0 is 480 Mbit/s (60 MB/s) per controller and is shared amongst all attached devices. Some chipset manufacturers overcome this bottleneck by providing multiple USB 2.0 controllers within the southbridge.
According to routine testing performed by CNet, write operations to typical Hi-Speed (USB 2.0) hard drives can sustain rates of 25–30 MB/s, while read operations are at 30–42 MB/s; this is 70% of the total available bus bandwidth. For USB 3.0, typical write speed is 70–90 MB/s, while read speed is 90–110 MB/s. Mask Tests, also known as Eye Diagram Tests, are used to determine the quality of a signal in the time domain. They are defined in the referenced document as part of the electrical test description for the high-speed (HS) mode at 480 Mbit/s.
According to a USB-IF chairman, "at least 10 to 15 percent of the stated peak 60 MB/s (480 Mbit/s) of Hi-Speed USB goes to overhead—the communication protocol between the card and the peripheral. Overhead is a component of all connectivity standards". Tables illustrating the transfer limits are shown in Chapter 5 of the USB spec.
For isochronous devices like audio streams, the bandwidth is constant, and reserved exclusively for a given device. The bus bandwidth therefore only has an effect on the number of channels that can be sent at a time, not the "speed" or latency of the transmission.
Latency.
For USB1 low-speed (1.5 Mbit/s) and full-speed (12 Mbit/s) devices the shortest time for a transaction in one direction is 1 ms. USB2 high-speed (480 Mbit/s) uses transactions within each micro frame (125 µs) where using 1-byte interrupt packet results in a minimal response time of 940 ns. 4-byte interrupt packet results in 984 ns.
Communication.
During USB communication data is transmitted as packets. Initially, all packets are sent from the host, via the root hub and possibly more hubs, to devices. Some of those packets direct a device to send some packets in reply.
After the sync field, all packets are made of 8-bit bytes, transmitted least-significant bit first. The first byte is a packet identifier (PID) byte. The PID is actually 4 bits; the byte consists of the 4-bit PID followed by its bitwise complement. This redundancy helps detect errors. (Note also that a PID byte contains at most four consecutive 1 bits, and thus never needs bit-stuffing, even when combined with the final 1 bit in the sync byte. However, trailing 1 bits in the PID may require bit-stuffing within the first few bits of the payload.)
Packets come in three basic types, each with a different format and CRC (cyclic redundancy check):
Handshake packets.
Handshake packets consist of only a single PID byte, and are generally sent in response to data packets. Error detection is provided by transmitting four bits that represent the packet type twice, in a single PID byte using complemented form. Three basic types are "ACK", indicating that data was successfully received, "NAK", indicating that the data cannot be received and should be retried, and "STALL", indicating that the device has an error condition and cannot transfer data until some corrective action (such as device initialization) occurs.
USB 2.0 added two additional handshake packets: "NYET" and "ERR". NYET indicates that a split transaction is not yet complete, while ERR handshake indicates that a split transaction failed. A second use for a NYET packet is to tell the host that the device has accepted a data packet, but cannot accept any more due to full buffers. This allows a host to switch to sending small PING tokens to inquire about the device's readiness, rather than sending an entire unwanted DATA packet just to elicit a NAK.
The only handshake packet the USB host may generate is ACK. If it is not ready to receive data, it should not instruct a device to send.
Token packets.
Token packets consist of a PID byte followed by two payload bytes: 11 bits of address and a five-bit CRC. Tokens are only sent by the host, never a device.
"IN" and "OUT" tokens contain a seven-bit device number and four-bit function number (for multifunction devices) and command the device to transmit DATAx packets, or receive the following DATAx packets, respectively. An IN token expects a response from a device. The response may be a NAK or STALL response, or a DATAx frame. In the latter case, the host issues an ACK handshake if appropriate. An OUT token is followed immediately by a DATAx frame. The device responds with ACK, NAK, NYET, or STALL, as appropriate.
"SETUP" operates much like an OUT token, but is used for initial device setup. It is followed by an eight-byte DATA0 frame with a standardized format.
Every millisecond (12000 full-bandwidth bit times), the USB host transmits a special "SOF" (start of frame) token, containing an 11-bit incrementing frame number in place of a device address. This is used to synchronize isochronous and interrupt data transfers. High-bandwidth USB 2.0 devices receive seven additional SOF tokens per frame, each introducing a 125 µs "microframe" (60000 high-bandwidth bit times each).
USB 2.0 added "PING" token, which asks a device if it is ready to receive an OUT/DATA packet pair. PING is usually sent by a host when polling a device that most recently responded with NAK or NYET. This avoids the need to send a large data packet to a device that the host suspects to be unwilling to accept it. The device responds with ACK, NAK or STALL, as appropriate.
USB 2.0 also added a larger three-byte "SPLIT" token with a seven-bit hub number, 12 bits of control flags, and a five-bit CRC. This is used to perform split transactions. Rather than tie up the high-bandwidth USB bus sending data to a slower USB device, the nearest high-bandwidth capable hub receives a SPLIT token followed by one or two USB packets at high bandwidth, performs the data transfer at full or low bandwidth, and provides the response at high bandwidth when prompted by a second SPLIT token.
Data packets.
A data packet consists of the PID followed by 0–1,024 bytes of data payload (up to 1,024 bytes for high-speed devices, up to 64 bytes for full-speed devices, and at most eight bytes for low-speed devices), and a 16-bit CRC.
There are two basic forms of data packet, "DATA0" and "DATA1". A data packet must always be preceded by an address token, and is usually followed by a handshake token from the receiver back to the transmitter. The two packet types provide the 1-bit sequence number required by Stop-and-wait ARQ. If a USB host does not receive a response (such as an ACK) for data it has transmitted, it does not know if the data was received or not; the data might have been lost in transit, or it might have been received but the handshake response was lost.
To solve this problem, the device keeps track of the type of DATAx packet it last accepted. If it receives another DATAx packet of the same type, it is acknowledged but ignored as a duplicate. Only a DATAx packet of the opposite type is actually received.
If the data is corrupted while transmitted or received, the CRC check fails. When this happens, the receiver does not generate an ACK, which makes the sender resend the packet.
When a device is reset with a SETUP packet, it expects an 8-byte DATA0 packet next.
USB 2.0 added "DATA2" and "MDATA" packet types as well. They are used only by high-bandwidth devices doing high-bandwidth isochronous transfers that must transfer more than 1024 bits per 125 µs microframe (8,192 kB/s).
PRE packet.
Low-bandwidth devices are supported with a special PID value, "PRE". This marks the beginning of a low-bandwidth packet, and is used by hubs that normally do not send full-bandwidth packets to low-bandwidth devices. Since all PID bytes include four 0 bits, they leave the bus in the full-bandwidth K state, which is the same as the low-bandwidth J state. It is followed by a brief pause, during which hubs enable their low-bandwidth outputs, already idling in the J state. Then a low-bandwidth packet follows, beginning with a sync sequence and PID byte, and ending with a brief period of SE0. Full-bandwidth devices other than hubs can simply ignore the PRE packet and its low-bandwidth contents, until the final SE0 indicates that a new packet follows.
Comparisons with other connection methods.
FireWire.
At first, USB was considered a complement to IEEE 1394 (FireWire) technology, which was designed as a high-bandwidth serial bus that efficiently interconnects peripherals such as disk drives, audio interfaces, and video equipment. In the initial design, USB operated at a far lower data rate and used less sophisticated hardware. It was suitable for small peripherals such as keyboards and pointing devices.
The most significant technical differences between FireWire and USB include:
These and other differences reflect the differing design goals of the two buses: USB was designed for simplicity and low cost, while FireWire was designed for high performance, particularly in time-sensitive applications such as audio and video. Although similar in theoretical maximum transfer rate, FireWire 400 is faster than USB 2.0 Hi-Bandwidth in real-use, especially in high-bandwidth use such as external hard-drives. The newer FireWire 800 standard is twice as fast as FireWire 400 and faster than USB 2.0 Hi-Bandwidth both theoretically and practically. However, Firewire's speed advantages rely on low-level techniques such as direct memory access (DMA), which in turn have created opportunities for security exploits such as the DMA attack.
The chipset and drivers used to implement USB and FireWire have a crucial impact on how much of the bandwidth prescribed by the specification is achieved in the real world, along with compatibility with peripherals.
Ethernet.
The IEEE 802.3af Power over Ethernet (PoE) standard specifies a more elaborate power negotiation scheme than powered USB. It operates at 48 V DC and can supply more power (up to 12.95 W, PoE+ 25.5 W) over a cable up to 100 meters compared to USB 2.0, which provides 2.5 W with a maximum cable length of 5 meters. This has made PoE popular for VoIP telephones, security cameras, wireless access points and other networked devices within buildings. However, USB is cheaper than PoE provided that the distance is short, and power demand is low.
Ethernet standards require electrical isolation between the networked device (computer, phone, etc.) and the network cable up to 1500 V AC or 2250 V DC for 60 seconds. USB has no such requirement as it was designed for peripherals closely associated with a host computer, and in fact it connects the peripheral and host grounds. This gives Ethernet a significant safety advantage over USB with peripherals such as cable and DSL modems connected to external wiring that can assume hazardous voltages under certain fault conditions.
MIDI.
Digital musical instruments are another example where USB is competitive in low-cost devices. However Power over Ethernet and the MIDI plug standard have an advantage in high-end devices that may have long cables. USB can cause ground loop problems between equipment, because it connects ground references on both transceivers. By contrast, the MIDI plug standard and Ethernet have built-in isolation to or more.
eSATA/eSATAp.
The eSATA connector is a more robust SATA connector, intended for connection to external hard drives and SSDs. eSATA's transfer rate (up to 6 Gbit/s) is similar to that of USB 3.0 (up to 5 Gbit/s on current devices; 10 Gbit/s speeds via USB 3.1, announced on 31 July 2013). A device connected by eSATA appears as an ordinary SATA device, giving both full performance and full compatibility associated with internal drives.
eSATA does not supply power to external devices. This is an increasing disadvantage compared to USB. Even though USB 3.0's 4.5 W is sometimes insufficient to power external hard drives, technology is advancing and external drives gradually need less power, diminishing the eSATA advantage. eSATAp (power over eSATA; aka ESATA/USB) is a connector introduced in 2009 that supplies power to attached devices using a new, backward compatible, connector. On a notebook eSATAp usually supplies only 5 V to power a 2.5-inch HDD/SSD; on a desktop workstation it can additionally supply 12 V to power larger devices including 3.5-inch HDD/SSD and 5.25-inch optical drives.
eSATAp support can be added to a desktop machine in the form of a bracket connecting to motherboard SATA, power, and USB resources.
eSATA, like USB, supports hot plugging, although this might be limited by OS drivers and device firmware.
Thunderbolt.
Thunderbolt combines PCI Express and Mini DisplayPort into a new serial data interface. Current Thunderbolt implementations have two channels, each with a transfer speed of 10 Gbit/s, resulting in an aggregate unidirectional bandwidth of 20 Gbit/s.
Interoperability.
Various protocol converters that convert USB data signals to and from other communications standards.
Related standards.
The USB Implementers Forum is working on a wireless networking standard based on the USB protocol. Wireless USB is intended as a cable-replacement technology, and uses ultra-wideband wireless technology for data rates of up to 480 Mbit/s.
USB 2.0 High Speed Inter Chip (HSIC) is a chip-to-chip variant of USB 2.0 that eliminates the conventional analog transceivers found in normal USB. It was adopted as a standard by the USB Implementers Forum in 2007. The HSIC physical layer uses about 50% less power and 75% less board area compared to traditional USB 2.0. HSIC uses two signals at 1.2 V and has a throughput of 480 Mbit/s using 240 MHz DDR signaling. Maximum PCB trace length for HSIC is 10 cm. It does not have low enough latency to support RAM memory sharing between two chips.

</doc>
<doc id="32152" url="http://en.wikipedia.org/wiki?curid=32152" title="Ural Mountains">
Ural Mountains

The Ural Mountains (Russian: Ура́льские го́ры, "Uralskiye gory"; ]; Bashkir: Урал тауҙары), or simply the Urals, are a mountain range that runs approximately from north to south through western Russia, from the coast of the Arctic Ocean to the Ural River and northwestern Kazakhstan. The mountain range forms the natural boundary between Europe and Asia. Vaygach Island and the islands of Novaya Zemlya form a further continuation of the chain to the north into the Arctic.
The mountains lie within the Ural geographical region and significantly overlap with the Ural Federal District and Ural economic region. They are rich in various deposits, including metal ores, coal, precious and semi-precious stones. Since the 18th century the mountains have been a major mineral base of Russia.
Etymology.
As attested by Sigismund von Herberstein, in the 16th century Russians called the range by a variety of names derived from the Russian words for rock (stone) and belt. The modern Russian name for the Urals (Урал, "Ural"), first appearing in the 16th–17th century when the Russian conquest of Siberia was in its heroic phase, was initially applied to its southern parts and gained currency as the name of the entire range during the 18th century. It might have been a borrowing from either Turkic (Bashkir, where the same name is used for the range), or Ob-Ugric. From the 13th century, in Bashkortostan there has been a legend about a hero named Ural. He sacrificed his life for the sake of his people and they poured a stone pile over his grave, which later turned into the Ural Mountains.
History.
As Middle-Eastern merchants traded with the Bashkirs and other people living on the western slopes of the Urals as far North as Great Perm, since at least the 10th century medieval mideastern geographers had been aware of the existence of the mountain range in its entirety, stretching as far as to the Arctic Ocean in the north. The first Russian mention of the mountains to the east of the East European Plain is provided by the Primary Chronicle, when it describes the Novgorodian expedition to the upper reaches of the Pechora in 1096. During the next few centuries Novgorodians engaged in fur trading with the local population and collected tribute from Yugra and Great Perm, slowly expanding southwards. The rivers Chusovaya and Belaya were first mentioned in the chronicles of 1396 and 1468, respectively. In 1430 the town of Solikamsk (Kama Salt) was founded on the Kama at the foothills of the Urals, where salt was produced in open pans. Ivan III of Moscow captured Perm, Pechora and Yugra from the declining Novgorod Republic in 1472. With the excursions of 1483 and 1499–1500 across the Urals Moscow managed to subjugate Yugra completely.
Nevertheless, around that time early 16th century Polish geographer Maciej of Miechów in his influential "Tractatus de duabus Sarmatiis" (1517) argued that there were no mountains in Eastern Europe at all, challenging the point of view of some authors of Classical antiquity, popular during the Renaissance. Only after Sigismund von Herberstein in his Notes on Muscovite Affairs (1549) had reported, following Russian sources, that there are mountains behind the Pechora and identified them with the Ripheans and Hyperboreans of ancient authors, did the existence of the Urals, or at least of its northern part, become firmly established in the Western geography. The Middle and Southern Urals were still largely unavailable and unknown to the Russian or Western European geographers.
In the 1550s, after the Tsardom of Russia had defeated the Khanate of Kazan and proceeded to gradually annex the lands of the Bashkirs, the Russians finally reached the southern part of the mountain chain. In 1574 they founded Ufa. The upper reaches of the Kama and Chusovaya in the Middle Urals, still unexplored, as well as parts of Transuralia still held by the hostile Siberian Khanate, were granted to the Stroganovs by several decrees of the tsar in 1558–1574. The Stroganovs' land provided the staging ground for Yermak's incursion into Siberia. Yermak crossed the Urals from the Chusovaya to the Tagil around 1581. In 1597 Babinov's road was built across the Urals from Solikamsk to the valley of the Tura, where the town of Verkhoturye (Upper Tura) was founded in 1598. Customs was established in Verkhoturye shortly thereafter and the road was made the only legal connection between European Russia and Siberia for a long time. In 1648 the town of Kungur was founded at the western foothills of the Middle Urals. During the 17th century the first deposits of iron and copper ores, mica, gemstones and other minerals were discovered in the Urals.
Iron and copper smelting works emerged. They multiplied particularly quickly during the reign of Peter I of Russia. In 1720–1722 he commissioned Vasily Tatishchev to oversee and develop the mining and smelting works in the Urals. Tatishchev proposed a new copper smelting factory in Yegoshikha, which would eventually become the core of the city of Perm and a new iron smelting factory on the Iset, which would become the largest in the world at the time of construction and give birth to the city of Yekaterinburg. Both factories were actually founded by Tatishchev's successor, Georg Wilhelm de Gennin, in 1723. Tatishchev returned to the Urals on the order of Empress Anna to succeed de Gennin in 1734–1737. Transportation of the output of the smelting works to the markets of European Russia necessitated the construction of the Siberian Route from Yekaterinburg across the Urals to Kungur and Yegoshikha (Perm) and further to Moscow, which was completed in 1763 and rendered Babinov's road obsolete. In 1745 gold was discovered in the Urals at Beryozovskoye and later at other deposits. It has been mined since 1747.
The first railway across the Urals had been built by 1878 and linked Perm to Yekaterinburg via Chusovoy, Kushva and Nizhny Tagil. In 1890 a railway linked Ufa and Chelyabinsk via Zlatoust. In 1896 this section became a part of the Trans-Siberian Railway. In 1909 yet another railway connecting Perm and Yekaterinburg passed through Kungur by the way of the Siberian Route. It has eventually replaced the Ufa – Chelyabinsk section as the main trunk of the Trans-Siberian railway.
The highest peak of the Urals, Mount Narodnaya, (elevation 1,895 m (6,217 ft)) was identified in 1927.
During the Soviet industrialization in the 1930s the city of Magnitogorsk was founded in the southeastern Urals as a center of iron smelting and steelmaking. During the German invasion of the Soviet Union in 1941–1942, the mountains became a key element in Nazi planning for the territories which they expected to conquer in the USSR. Faced with the threat of having a significant part of the Soviet territories occupied by the enemy, the government evacuated many of the industrial enterprises of European Russia and Ukraine to the eastern foothills of the Urals, considered a safe place out of reach of the German bombers and troops. Three giant tank factories were established at the Uralmash in Sverdlovsk (as Yekaterinburg used to be known), Uralvagonzavod in Nizhny Tagil, and Chelyabinsk Tractor Plant in Chelyabinsk. After the war, in 1947–1948, Chum – Labytnangi railway, built with the forced labor of Gulag inmates, crossed the Polar Urals.
The first ample geographic survey of the Ural Mountains was completed in the early 18th century by the Russian historian and geographer Vasily Tatishchev under the orders of Peter I. Earlier, in the 17th century, rich ore deposits were discovered in the mountains and their systematic extraction began in the early 18th century, eventually turning the region into the largest mineral base of Russia.
One of the first scientific descriptions of the mountains was published in 1770–71. Over the next century, the region was studied by scientists from a number of countries, including Russia (geologist Alexander Karpinsky, botanist Porfiry Krylov and zoologist Leonid Sabaneyev), England (geologist Sir Roderick Murchison), France (paleontologist Edouard de Verneuil), and Germany (naturalist Alexander von Humboldt, geologist Alexander Keyserling). In 1845, Murchison, who had according to "Encyclopædia Britannica" "compiled the first geologic map of the Urals in 1841", published "The Geology of Russia in Europe and the Ural Mountains" with de Verneuil and Keyserling.
Mayak, 150 km southeast of Yekaterinburg, was a center of the Soviet nuclear industry and site of the Kyshtym disaster.
Geography and topography.
The Ural Mountains extend about 2500 km from the Kara Sea to the Kazakh Steppe along the northern border of Kazakhstan. Vaygach Island and the island of Novaya Zemlya form a further continuation of the chain on the north. Geographically this range marks the northern part of the border between the continents of Europe and Asia. Its highest peak is Mount Narodnaya, approximately 1895 m in elevation.
By topography and other natural features, the Urals are divided, from north to south, into the Polar (or Arctic), Nether-Polar (or Sub-Arctic), Northern, Central and Southern parts. The Polar Urals extend for about 385 km from Mount Konstantinov Kamen in the north to the Khulga River in the south; they have an area of about 25000 km2 and a strongly dissected relief. The maximum height is 1499 m at Payer Mountain and the average height is 1000 to. The mountains of the Polar Urals have exposed rock with sharp ridges, though flattened or rounded tops are also found.
The Nether-Polar Urals are up to 150 km wider and higher than the Polar Urals. They include the highest peaks of the range: Mount Narodnaya (1895 m), Mount Karpinsky (1878 m) and Manaraga (1662 m). They extend for more than 225 km south to the Shchugor River. The many ridges are sawtooth shaped and dissected by river valleys. Both Polar and Nether-Polar Urals are typically Alpine; they bear traces of Pleistocene glaciation, along with permafrost and extensive modern glaciation, including 143 extant glaciers.
The Northern Urals consist of a series of parallel ridges up to 1000 – in height and longitudinal depressions. They are elongated from north to south and stretch for about 560 km from the Usa River. Most of the tops are flattened, but those of the highest mountains, such as Telposiz, 1617 m and Konzhakovsky Stone, 1569 m have a dissected topography. Intensive weathering has produced vast areas of eroded stone on the mountain slopes and summits of the northern areas.
The Central Urals are the lowest part of the Urals, with smooth mountain tops, the highest mountain being 994 m (Basegi); they extend south from the Ufa River.
The relief of the Southern Urals is more complex, with numerous valleys and parallel ridges directed south-west and meridionally. The range includes the Ilmensky Mountains separated from the main ridges by the Miass River. The maximum height is 1640 m (Mount Yamantau) and the width reaches 250 km. Other notable peaks lie along the Iremel mountain ridge (Bolshoy Iremel and Maly Iremel). The Southern Urals extend some 550 km up to the sharp westward bend of the Ural River and terminate in the wide Mughalzhar Hills.
Geology.
The Urals are among the world's oldest extant mountain ranges. For its age of 250 to 300 million years, the elevation of the mountains is unusually high. They were formed during the Uralian orogeny due to the collision of the eastern edge of the supercontinent Laurussia with the young and rheologically weak continent of Kazakhstania, which now underlies much of Kazakhstan and West Siberia west of the Irtysh, and intervening island arcs. The collision lasted nearly 90 million years in the late Carboniferous – early Triassic. Unlike the other major orogens of the Paleozoic (Appalachians, Caledonides, Variscides), the Urals have not undergone post-orogenic extensional collapse and are unusually well preserved for their age, being underlaid by a pronounced crustal root. East and south of the Urals much of the orogen is buried beneath later Mesozoic and Cenozoic sediments. The adjacent Pay-Khoy to the north is not a part of the Uralian orogen and formed later.
Many deformed and metamorphosed rocks, mostly of Paleozoic period, surface within the Urals. The sedimentary and volcanic layers are folded and broken, and form meridional bands. The sediments to the west of the Ural Mountains are formed by limestone, dolomite and sandstone left from ancient shallow seas. The eastern side is dominated by basalts similar to the rocks of the bottom of the modern oceans.
The western slope of the Ural Mountains has predominantly karst topography, especially in the basin of the Sylva River, which is a tributary of the Chusovaya River. It is composed of severely eroded sedimentary rocks (sandstones and limestones) that are about 350 million years old. There are many caves, karst sinks and underground streams. The karst topography is much less developed on the eastern slopes. They are relatively flat, with some hills and rocky outcrops and contain alternating volcanic and sedimentary layers dated to the middle Paleozoic period. Most high mountains consist of weather-resistant rocks such as quartzite, schist and gabbro that are between 570 and 395 million years old. The river valleys are laid with limestone.
The Ural Mountains contain about 48 species of economically valuable ores and minerals. Eastern regions are rich in chalcopyrite, nickel oxide, gold, platinum, chromite and magnetite ores, as well as in coal (Chelyabinsk Oblast), bauxite, talc, fireclay and abrasives. The Western Urals contain deposits of coal, oil, natural gas (Ishimbay and Krasnokamsk areas) and potassium salts. Both slopes are rich in bituminous coal and lignite, and the largest deposit of bituminous coal is in the north (Pechora field). The specialty of the Urals is precious and semi-precious stones, such as emerald, amethyst, aquamarine, jasper, rhodonite, malachite and diamond. Some of the deposits, such as the magnetite ores at Magnitogorsk, are already nearly depleted.
Rivers and lakes.
Many rivers originate in the Ural Mountains. The western slopes south of the border between the Komi Republic and Perm Krai and the eastern slopes south of approximately 54°30'N drain into the Caspian Sea via the Kama and Ural River basins. The tributaries of the Kama include the Vishera, Chusovaya, and Belaya and originate on both the eastern and western slopes. The rest of the Urals drain into the Arctic Ocean, mainly via the Pechora basin in the west, which includes the Ilych, Shchugor, and the Usa, and via the Ob basin in the east, which includes the Tobol, Tavda, Iset, Tura and Severnaya Sosva. The rivers are frozen for more than half the year. Generally, the western rivers have higher flow volume than the eastern ones, especially in the Northern and Nether-Polar regions. Rivers are slower in the Southern Urals. This is because of low precipitation and the relatively warm climate resulting in less snow and more evaporation.
The mountains contain a number of deep lakes. The eastern slopes of the Southern and Central Urals have most of these, among the largest of which are the Uvildy, Itkul, Turgoyak, and Tavatuy lakes. The lakes found on the western slopes are less numerous and also smaller. Lake Bolshoye Shchuchye, the deepest lake in the Polar Urals, is 136 m deep. Other lakes, too, are found in the glacial valleys of this region. Spas and sanatoriums have been built to take advantage of the medicinal muds found in some of the mountain lakes.
Climate.
The climate of the Urals is continental. The mountain ridges, elongated from north to south, effectively absorb sunlight thereby increasing the temperature. The areas west of the Ural Mountains are 1 – warmer in winter than the eastern regions because the former are warmed by Atlantic winds whereas the eastern slopes are chilled by Siberian air masses. The average January temperatures increase in the western areas from -20 C in the Polar to -15 C in the Southern Urals and the corresponding temperatures in July are 10 C and 20 C. The western areas also receive more rainfall than the eastern ones by 150 – per year. This is because the mountains trap clouds from the Atlantic Ocean. The highest precipitation, approximately 1000 mm, is in the Northern Urals with up to 1000 cm snow. The eastern areas receive from 500 – in the north to 300 – in the south. Maximum precipitation occurs in the summer: the winter is dry because of the Siberian High.
Flora.
The landscapes of the Urals vary with both latitude and longitude and are dominated by forests and steppes. The southern area of the Mughalzhar Hills is a semidesert. Steppes lie mostly in the southern and especially south-eastern Urals. Meadow steppes have developed on the lower parts of mountain slopes and are covered with zigzag and mountain clovers, "Serratula gmelinii", dropwort, meadow-grass and "Bromus inermis", reaching the height of 60–80 cm. Much of the land is cultivated. To the south, the meadow steppes become more sparse, dry and low. The steep gravelly slopes of the mountains and hills of the eastern slopes of the Southern Urals are mostly covered with rocky steppes. River valleys contain willow, poplar and caragana shrubs.
Forest landscapes of the Urals are diverse, especially in the southern part. The western areas are dominated by dark coniferous taiga forests which change to mixed and deciduous forests in the south. The eastern mountain slopes have light coniferous taiga forests. The Northern Urals are dominated by conifers, namely Siberian fir, Siberian pine, Scots pine, Siberian spruce, Norway spruce and Siberian larch, as well as by Silver and downy birches. The forests are much sparser in the Polar Urals. Whereas in other parts of the Ural Mountains they grow up to the heights of 1 km, in the Polar Urals the tree line is at 250–400 m. The polar forests are low and are mixed with swamps, lichens, bogs and shrubs. Dwarf birch, mosses and berries (blueberry, cloudberry, black crowberry, etc.) are abundant. The forests of the Southern Urals are the most diverse in composition: here, together with coniferous forests are also abundant broadleaf tree species such as English oak, Norway maple and elm. The Virgin Komi Forests in the northern Urals are recognized as a World Heritage site.
Fauna.
The Ural forests are inhabited by animals typical of Siberia, such as elk, brown bear, fox, wolf, wolverine, lynx, squirrel and sable (north only). Because of the easy accessibility of the mountains there are no specifically mountainous species. In the Middle Urals, one can see a rare mixture of sable and pine marten named kidus. In the Southern Urals, badger and black polecat are common. Reptiles and amphibians live mostly in the Southern and Central Ural and are represented by the common viper, lizards and grass snakes. Bird species are represented by capercaillie, black grouse, hazel grouse, spotted nutcracker and cuckoos. In summers, the South and Middle Urals are visited by songbirds, such as nightingale and redstart.
The steppes of the Southern Urals are dominated by hares and rodents such as gophers, susliks and jerboa. There are many birds of prey such as lesser kestrel and buzzards. The animals of the Polar Urals are few and are characteristic of the tundra; they include Arctic fox, tundra partridge, lemming and reindeer. The birds of these areas include rough-legged buzzard, snowy owl and rock ptarmigan.
Ecology.
The continuous and intensive economic development of the last centuries has affected the fauna, and wildlife is much diminished around all industrial centers. During World War II, hundreds of factories were evacuated from Western Russia before the German occupation, flooding the Urals with industry. The conservation measures include establishing national wildlife parks. There are nine strict nature reserves in the Urals: the Ilmen, the oldest one, mineralogical reserve founded in 1920 in Chelyabinsk Oblast, Pechora-Ilych in the Komi Republic, Bashkir and its former branch Shulgan-Tash in Bashkortostan, Visim in Sverdlovsk Oblast, South Urals in Bashkortostan, Basegi in Perm Krai, Vishera in Perm Krai and Denezhkin Kamen in Sverdlovsk Oblast.
The area has also been severely damaged by the plutonium-producing facility Mayak opened in Chelyabinsk-40 (later called Chelyabinsk-65, Ozyorsk), in the Southern Urals, after World War II. Its plants went into operation in 1948 and, for the first ten years, dumped unfiltered radioactive waste into the Techa River and Lake Karachay. In 1990, efforts were underway to contain the radiation in one of the lakes, which was estimated at the time to expose visitors to 500 millirem per day. As of 2006, 500 mrem in the natural environment was the upper limit of exposure considered safe for a member of the general public in an entire year (though workplace exposure over a year could exceed that by a factor of 10). Over 23000 km2 of land were contaminated in 1957 from a storage tank explosion, only one of several serious accidents that further polluted the region. The 1957 accident expelled 20 million curies of radioactive material, 90% of which settled into the land immediately around the facility. Although some reactors of Mayak were shut down in 1987 and 1990, the facility keeps producing plutonium.
Cultural significance.
The Urals have been viewed by Russians as a "treasure box" of mineral resources, which were the basis for its extensive industrial development. In addition to iron and copper the Urals were a source of gold, malachite, alexandrite, and other gems such as those used by the court jeweller Fabergé. As Russians in other regions gather mushrooms or berries, Uralians gather mineral specimens and gems. Dmitry Mamin-Sibiryak (1852–1912) Pavel Bazhov (1879–1950), as well as Aleksey Ivanov and Olga Slavnikova, post-Soviet writers, have written of the region.
The region served as a "military stronghold" during Peter the Great's Great Northern War with Sweden, during Stalin's rule when the Magnitogorsk Metallurgical Complex was built and Russian industry relocated to the Urals during the Nazi advance at the beginning of World War II, and as the center of the Soviet nuclear industry during the Cold War. Extreme levels of air, water, and radiological contamination and pollution by industrial wastes resulted. Population exodus resulted, and economic depression at the time of the collapse of the Soviet Union, but in post-Soviet times additional mineral exploration, particularly in the northern Urals, has been productive and the region has attracted industrial investment.

</doc>
<doc id="32202" url="http://en.wikipedia.org/wiki?curid=32202" title="UIC franc">
UIC franc

The UIC Franc (code: XFU) is a virtual currency unit used by the International Union of Railways.
A related currency for the airline ticketing industry is the Neutral Unit of Construction (NUC).

</doc>
<doc id="32203" url="http://en.wikipedia.org/wiki?curid=32203" title="Unification Church">
Unification Church

</table>
</table>
The Family Federation for World Peace and Unification, founded as the Holy Spirit Association for the Unification of World Christianity, and commonly called the Unification Church or Unificationism, is a new religious movement founded in South Korea in 1954 by Sun Myung Moon. Since its founding, the church has expanded throughout the world with most members living in Korea, Japan, the Philippines, and other nations in East Asia. It has sponsored other organizations and projects over the years; including businesses, news media, projects in education and the arts, and political and social activism. The church was led by Moon until his death on September 3, 2012. Since then, his widow Hak Ja Han has assumed the leadership of the church.
Unificationist beliefs are derived from the Christian Bible and are explained in the church's textbook, the "Divine Principle". It teaches that God is the Creator and Heavenly Parent, whose dual nature combines both masculinity and femininity and whose center is true love. The Blessing ceremony of the Unification Church, a wedding or marriage rededication ceremony, is a church tradition which has attracted wide public attention. The church has engaged in interfaith activities with other religions, including mainstream Christianity and Islam, despite theological differences.
The Unification Church has been the subject of controversy over several issues. Its beliefs, which differ from traditional Jewish and Christian interpretations of the Bible, have been called heretical and antisemitic by some critics. It has also been criticized for its involvement in politics, especially in support of the government of South Korea for which it was investigated by a committee of the United States Congress in 1977. The church has also been accused of brainwashing its members, which led to some being subject to deprogramming. Other controversial events include Moon's 1982 conviction in the United States of filing false federal income tax returns and conspiracy and the 2002 wedding of Roman Catholic archbishop Emmanuel Milingo to a Unification Church member in a ceremony presided over by Moon.
Terminology.
"Moonie" is a colloquial term sometimes used to refer to members of the Unification Church. This is derived from the name of the church's founder Sun Myung Moon, and was first used in 1974 by the American media. Church members have used the word "Moonie", including Moon himself, President of the Unification Theological Seminary David Kim, and Moon's aide and president of The Little Angels Children's Folk Ballet of Korea Bo Hi Pak. In the 1980s and 1990s the Unification Church of the United States undertook an extensive public relations campaign against the use of the word by the news media. Journalistic authorities, including the "New York Times" and Reuters, now discourage its use in news reporting.
History.
Origins in Korea.
Unification Church members believe that Jesus Christ appeared to Mun Yong-myong (his birth name) when Rev. Moon was 16 years old on Easter morning of 1935 (April 17) and asked him to accomplish the work left unfinished because of his crucifixion. After a period of prayer and consideration, Moon accepted the mission, later changing his name to Mun Son-myong (Sun Myung Moon).
The church's official teachings, the "Divine Principle," was first published as "Wolli Wonbon" (Original Text of the Divine Principle) in 1945. However, the earliest manuscript was lost in North Korea during the Korean War. A second, expanded version, "Wolli Hesol", or "Explanation of the Divine Principle", was published in 1957. Finally, its most propagated text, the "Exposition of the Divine Principle" was published in 1966.
Sun Myung Moon preached in northern Korea after the end of World War II and in 1946 was imprisoned by the communist regime in North Korea. He was released from prison by the advance of American and United Nations forces during the Korean War, and moved south along with many other North Koreans. He built his first church from mud and cardboard boxes as a refugee in Pusan.
Moon formally founded the Unification Church in Busan on May 1, 1954, calling it "The Holy Spirit(ual) Association for the Unification of World Christianity." The church expanded rapidly in South Korea and by the end of 1955 had 30 church centers throughout the nation. In its early days, the church was known as "the wailing church" or "the church of tears" because of the passionate sermons given by the Rev. Sun Myung Moon and his followers learning the way of service and sacrifice for God and humanity.
International expansion and controversy.
In 1958, Moon sent missionaries to Japan, and in 1959, to America. Moon moved to the United States in 1971 (although he remained a citizen of the Republic of Korea). Missionary work took place in Washington, D.C., New York, and California. UC missionaries found success in the San Francisco Bay Area, where the church expanded in Oakland, Berkeley, and San Francisco. By 1971, the Unification Church of the United States had about 500 members. By 1973, it had some presence in all 50 states and a few thousand members. In the 1970s American Unification Church members were noted for their enthusiasm and dedication, which often included raising money for church projects on so-called "mobile fundraising teams."
The church also sent missionaries to Europe. The church entered Czechoslovakia in 1968 and remained underground until the 1990s. Unification Church activity in South America began in the 1970s with missionary work. Later, the church made large investments in civic organizations and business projects, including an international newspaper.
In the 1970s, Moon gave a series of public speeches in the United States, including one in Madison Square Garden in New York City in 1974 and two in 1976: in Yankee Stadium in New York City, and on the grounds of the Washington Monument in Washington, D.C., where Moon spoke on "God's Hope for America" to 300,000 people. In 1975, the Unification Church held one of the largest peaceful gatherings in history, with 1.2 million people in Yoido, South Korea.
In the 1970s the Unification Church was accused of "brainwashing" by the newly-active anti-cult movement, which included Steven Hassan and some other former church members. Some sociologists of religion tend to argue that these accusations were based on theories that for the most part have not gained acceptance among scholars. Other scholars, including some psychologists and psychiatrists, argue that brainwashing theories are widely endorsed within the academy at large. Eileen Barker, a sociologist of religion and the founder of INFORM (Information Network Focus on Religious Movements), argues that the Unification Church and other new religious movements of that time "demonstrably did not have access to the irresistible or irreversible techniques they were reputedly wielding".
Members of the Unification Church reported that they were forcibly "deprogrammed" by those who wanted to pull them out of it. "Hostage to Heaven" by Barbara and Betty Underwood is a first-person account of a former church member, and the reflections of her mother who fought to rescue her. In 1977, the Unification Church won a lawsuit in the United States against deprogrammers, as did some other groups about the same time.
Starting in 1972, the Unification Church sponsored the International Conference on the Unity of the Sciences, a series of scientific conferences. The first conference had 20 participants, while the largest conference in Seoul in 1982, had 808 participants from over 100 countries. Participants included Nobel laureates John Eccles (Physiology or Medicine 1963, who chaired the 1976 conference),
Eugene Wigner (Physics 1963).
In 1974 Moon founded the Unification Theological Seminary, in Barrytown, New York, partly in order to improve relations of the Unification Church with other churches. Professors from other denominations, including a Methodist minister, a Presbyterian, and a Roman Catholic priest, as well as a rabbi, were hired to teach Unificationist students.
In the 1970s and 1980s The Unification Church became noted for its political activities, especially its support for United States president Richard Nixon during the Watergate scandal, its support for anti-communism during the Cold War, and its ownership of various news media outlets through News World Communications, an international news media conglomerate which publishes "The Washington Times" newspaper in Washington, D.C., and newspapers in South Korea, Japan, and South America, which tend to support conservatism. The political activities of the Unification Church were opposed by some leftists. In 1976 members of the Youth International Party staged a marijuana "smoke-in" in the middle of a UC sponsored rally in Washington D.C.
In 1977 the Subcommittee on International Organizations of the Committee on International Relations, of the United States House of Representatives, while investigating the Koreagate scandal found that the South Korean intelligence agency(KCIA) had worked with the Unification Church to gain political influence within the United States, with some members working as volunteers in Congressional offices. Together they founded the Korean Cultural Freedom Foundation, a nonprofit organization which undertook public diplomacy for the Republic of Korea. The committee also investigated possible KCIA influence on the Unification Church's campaign in support of Nixon.
In 1977 representatives from the American Jewish Committee, the National Council of Churches of
Christ in the U.S.A., and the Roman Catholic Church of New York City held a press conference to say that the "Divine Principle" contains antisemitic references and heresy. In the 1980s church leaders Mose Durst, Peter Ross, and Andrew Wilson expressed regret over some members' misunderstanding of Judaism and urged better relations with the Jewish community. (see also: Unification Church and Judaism)
In 1980 Moon asked church members to found CAUSA International, an anti-communist educational organization based in New York. In August 1985, six years before the fall of Soviet Union, the Professors World Peace Academy, an organization founded by Moon, sponsored a conference in Geneva to debate the theme "The situation in the world after the fall of the communist empire." The conference was chaired by professors Morton Kaplan and Aleksandras Štromas.
In the 1980s Moon instructed church members to take part in a program called "Home Church" in which they reached out to neighbors and community members through public service. In 1982 the first large scale Blessing ceremony held outside of Korea took place in Madison Square Garden in New York City with 2075 couples. In 1988, Moon matched 2,500 Korean members with Japanese members for a Blessing ceremony held in Korea, partly in order to promote unity between the two nations.
In 1982, Moon was convicted in the United States of filing false federal income tax returns and conspiracy. (see: United States v. Sun Myung Moon) His conviction was upheld on appeal in a split decision. Moon was given an 18-month sentence and a $15,000 fine. He served 13 months of the sentence at the Federal Correctional Institution, Danbury before being released on good behavior to a halfway house. The case was the center of national freedom of religion and free speech debates. Prof. Laurence H. Tribe of the Harvard University Law School argued that the trial by jury had "doomed (Moon) to conviction based on religious prejudice." The American Baptist Churches in the U.S.A, the National Council of Churches, the National Black Catholic Clergy Caucus, and the Southern Christian Leadership Conference filed briefs in support of Moon. Clergy including Jerry Falwell and Joseph Lowery, signed petitions protesting the government's case and spoke out in defense of Moon.
In 1984 Barker published "", based on her seven-year study of Unification Church members in the United Kingdom and the United States. She rejected the then popular "brainwashing" theory as an explanation for conversion to the Unification Church. The book was given the Distinguished Book Award for 1985 by the Society for the Scientific Study of Religion. In 1987, scholars with American Psychological Association rejected the hypotheses of those who accused the Unification Church of brainwashing and coercive persuasion, stating that their "conclusions...cannot be said to be scientific in any meaningful sense".
1990s.
Since 1990, U.S. courts have consistently rejected testimonies about brainwashing (mind control) and manipulation, stating that such theories were not part of accepted mainline science according to the Frye Standard of 1923.
Starting in the 1990s, the Unification Church expanded in Russia and other formerly communist nations. Han made a radio broadcast to the nation from the Kremlin Palace of Congresses. As of 1994, the church had about 5,000 members in Russia. About 500 Russian students had been sent to USA to participate in 40-day workshops.
In 1991 Moon announced that church members should return to their hometowns and undertake apostolic work there. Massimo Introvigne, who studied the Unification Church and other new religious movements, said that this confirmed that full-time membership is no longer considered crucial to church members. On May 1, 1994 (the 40th anniversary of the founding of the Unification Church), Moon declared that the era of the Unification Church had ended and inaugurated a new organization: the Family Federation for World Peace and Unification (FFWPU) would include Unification Church members and members of other religious organizations working toward common goals, especially on issues of sexual morality and reconciliation between people of different religions, nations, and races. The FFWPU co-sponsored Blessing ceremonies in which thousands of non–Unification Church married couples were given the marriage blessing previously given only to Unification Church members.
In 1998 Irving Louis Horowitz, sociologist, questioned the relationship between the Unification Church and scholars whom it paid to conduct research on its behalf.
21st century.
In 2001, the Unification Church came into conflict with the Roman Catholic Church when Catholic archbishop Emmanuel Milingo and Maria Sung, a 43-year-old Korean acupuncturist, married in a Unification Church Blessing ceremony, presided over by Rev. and Mrs. Moon. Following his marriage the Archbishop was called to the Vatican by Pope John Paul II, where he was asked not to see his wife anymore, and to move to a Capuchin monastery. Sung went on a hunger strike to protest their separation. This attracted much media attention. Milingo is now an advocate of the removal of the requirement for celibacy by priests in the Catholic Church. He is the founder of Married Priests Now!.
Since 2003, the church sponsored Middle East Peace Initiative has been organizing group tours of Israel and Palestine to promote understanding, respect, and reconciliation among Jews, Muslims, and Christians.
In 2003, Korean Unification Church members started a political party in South Korea. It was named "The Party for God, Peace, Unification, and Home." In an inauguration declaration, the new party said it would focus on preparing for Korean reunification by educating the public about God and peace. A church official said that similar political parties would be started in Japan and the United States.
Moon was a member of the Honorary Committee of the Unification Ministry of the Republic of Korea. Church member Jae-jung Lee had been once a Unification Minister of the Republic of Korea. Another, Ek Nath Dhakal, is a member of the Nepalese Constituent Assembly, and a first Minister for Co-operatives and Poverty Alleviation Ministry of the Government of Nepal.
In April 2008, Moon, then 88 years old, appointed his youngest son, Hyung Jin Moon, to be the leader of the church and movement, saying, "I hope everyone helps him so that he may fulfil his duty as the successor of the True Parents." At the same time he appointed his daughter In Jin Moon as the president of the Unification Church of the United States. In 2011 in Pyongyang, to mark the 20th anniversary of Sun Myung Moon's visit to the DPRK, de jure President Kim Yong-nam hosted Hyung Jin Moon in the official residence. The latter donated 600 tons of flour to North Korean children of Jeongju Province, the birthplace of Sun Myung Moon. Also, after the 2011 earthquake in Japan, he donated $1.7 million to the Japanese Red Cross.
In 2009, Moon's autobiography, "As a Peace-Loving Global Citizen" (Korean: 평화를 사랑하는 세계인으로), was published by Gimm-Young Publishers in South Korea. The book became a bestseller in Korea and Japan.
In 2010, "Forbes" reported that Moon and Han were living in South Korea while their children took more responsibility for the day-to-day leadership of the Unification Church and its affiliated organizations.
On August 15, 2012, Moon was reported to be gravely ill and was put on a respirator at the intensive care unit of St. Mary’s Hospital at The Catholic University of Korea in Seoul. He died there on September 3, 2012.
Events after Moon's death.
In 2012 Moon was posthumously awarded North Korea's National Reunification Prize. On the first anniversary of Moon's death, North Korean president Kim Jong-un expressed condolences to Han and the family saying: "Kim Jong-un prayed for the repose of Moon, who worked hard for national concord, prosperity and reunification and world peace."
Following Moon's death his widow Hak Ja Han has been considered the spiritual leader of the church. Most church activities have continued, although some unprofitable business projects have been reduced or discontinued. The Family Federation for World Peace and Unification has continuously held its Marriage Blessing tradition each year, their latest one on February 12, 2014. Sociologist Eileen Barker has reported that Unificationists have undergone a transformation in their world view from millennialism to utopianism. Recent church activities have included building projects and a revival tour. In 2014 Sarah M. Lewis wrote that the Unification Church’s greatest present influence comes from church affiliated groups such as the Universal Peace Federation, which include non-church members working for common interests and goals.
In 2012, December 19, the candidate Park Geun-hye, with the support of the Unification Church, became the first female President of South Korea.
Beliefs.
"Divine Principle".
The "Divine Principle" or "Exposition of the Divine Principle" (Korean 원리강론/原理講論, translit. "wonli ganglon") is the main theological textbook of the Unification Church. It was co-written by church founder Sun Myung Moon and early disciple Hyo Won Eu and first published in 1966. A translation entitled "Divine Principle" was published in English in 1973. The book lays out the core of Unification theology, and is held to have the status of scripture by believers. Following the format of systematic theology, it includes (1) God's purpose in creating human beings, (2) the fall of man, and (3) restoration – the process through history by which God is working to remove the ill effects of the fall and restore humanity back to the relationship and position that God originally intended.
God is viewed as the creator, whose nature combines both masculinity and femininity, and is the source of all truth, beauty, and goodness. Human beings and the universe reflect God's personality, nature, and purpose. "Give-and-take action" (reciprocal interaction) and "subject and object position" (initiator and responder) are "key interpretive concepts", and the self is designed to be God's object. The purpose of human existence is to return joy to God. The "four-position foundation" is "another important and interpretive concept", and explains in part the emphasis on the family.
Eugene V. Gallagher commented: "The "Divine Principle's" analysis of the Fall sets the stage for the mission of Rev. Moon, who in the last days brings a revelation that offers humankind the chance to return to an Edenic state. The account in the "Divine Principle" offers Unificationists a comprehensive context for understanding human suffering."
Relations and conflicts with other religions.
The "Divine Principle" includes new interpretations of the Bible not found in Jewish and Christian tradition. From its beginning, the Unification Church claimed to be Christian and promoted its teachings to mainstream Christian churches and organizations . The Unification Church in Korea was labeled as heretical by Protestant churches in South Korea, including Moon’s own Presbyterian Church. In the United States the church was rejected by ecumenical organizations as being non-Christian. The main objections were theological, especially because of the Unification Church’s addition of material to the Bible.
Protestant Christian commentators have also criticized Unification Church teachings as contrary to the Protestant doctrine of salvation by faith alone. In their influential book "The Kingdom of the Cults" (first published in 1965), Walter Ralston Martin and Ravi K. Zacharias disagreed with the "Divine Principle" on the issues of the divinity of Christ, the virgin birth of Jesus, the Unification Church's belief that Jesus should have married, the necessity of the crucifixion of Jesus, and a literal resurrection of Jesus as well as a literal second coming of Jesus.
Jewish commentators, including Rabbi A. James Rudin of the American Jewish Committee in a 1976 report, have stated that "Divine Principle" contains "pejorative language, stereotyped imagery, and accusations of collective sin and guilt." Moon himself has made some controversial statements about the Holocaust, including that its victims were paying indemnity for the crucifixion of Jesus.
In 1977 the Unification Church issued a rebuttal to Rudin's report, stating that it was neither comprehensive nor reconciliatory, but rather had a "hateful tone" and was filled with "sweeping denunciations." It denied that the "Divine Principle" teaches antisemitism and gave detailed responses to 17 specific allegations contained in the AJC's report, stating that allegations were distortions of teaching and obscuration of real passage content or that the passages were accurate summaries of Jewish scripture or New Testament passages.
In 1984 Mose Durst, then the president of the Unification Church of the United States and himself a convert from Judaism, said that the Jewish community had been "hateful" in its response to the growth of the Unification Church, and placed blame both on the community's "insecurity" and on Unification Church members' "youthful zeal and ignorance." Rudin, then the national interreligious affairs director of the American Jewish Committee, said that Durst's remarks were inaccurate and unfair and that "hateful is a harsh word to use." In the same year Durst wrote in his autobiography: "Our relations with the Jewish community have been the most painful to me personally. I say this with a heavy heart, since I was raised in the Jewish faith and am proud of my heritage."
The relationship between the Unification Church and Islam has often been noted, both by scholars and the news media. The "Divine Principle" lists the “Islamic cultural sphere” as one of the world’s four major divisions (the others are the East Asian, the Hindu, and the Christian spheres). In 1997, Louis Farrakhan, the leader of The Nation of Islam, an African American Islamic organization, served as a "co-officiator" at a blessing ceremony presided over by Moon and Han. In 2000 the Church and the Nation of Islam co-sponsored the Million Family March, a rally in Washington, D.C., to celebrate family unity and racial and religious harmony.
In 2009 the Unification Church held an interfaith event in the Peruvian Congress. Former President of the Congress of Peru Marcial Ayaipoma and other notable politicians are "Ambassadors for Peace" of the Unification Church.
In 2010, the church built a large interfaith temple in Seoul. Author Deepak Chopra was the keynote speaker at an interfaith event of the Unification Church co-hosted with the United Nations at the United Nations Headquarters.
In 2011, an interfaith event was held in the National Assembly of Thailand, the President of the National Assembly of Thailand attended the event.
In 2012, the Unification Church-affiliated Universal Peace Federation held an interfaith dialogue in Italy, which was cosponsored by United Nations. That year, Unification Church affiliated Universal Peace Federation held an interfaith program for representatives of 12 various religions and confessions in the United Nations General Assembly Hall. President of the United Nations General Assembly, Deputy Secretary-General of the United Nations, Permanent Observer of the Holy See to the United Nations and other UN officials gave speeches there.
Esotericism.
The Unification Church has been called esoteric, that is making at least some of its beliefs secret from nonmembers. In 1979 Tingle and Fordyce commented: "How different the openness of Christianity is to the attitude of Reverend Moon and his followers who are often reluctant to reveal to the public many of their basic doctrines." Since the 1990s many Unification Church texts that were formerly regarded as esoteric have been posted on the church's official websites.
Spiritualism.
The "Divine Principle" upholds a belief in spiritualism, that is communication with the spirits of deceased persons. Moon and early church members associated with spiritualists, including the famous Arthur Ford.
Resurrection.
Unification Church ascribes to the belief in coming back to life after death. There are two concepts of resurrection detailed in the "Divine Principle". The first is the resurrection of people on earth, who pass from death to life, and the second is the "returning resurrection" which will occur in the Last Days. The Divine Principle reveals the true meaning of these two.
The first resurrection means to pass death to life by living in accordance with God's Will, within the dominion of God's infinite love. The person who was originally separated from God (dead) comes alive by receiving "life elements" from God, which are God's word and God's love. Chapter 5 of "Divine Principle" interprets the Biblical account of Luke 9:60, according to the Unification Church.
The "Divine Principle" posits that departed souls can expiate their sins and achieve spiritual growth by "returning" to earth and cooperating with living people, leading them to fulfill their mission on earth and live in accordance to their conscience. The text cites a scripture justifying the concept: "Apart from us they may not be made perfect".
Unification Church theologian Young Oon Kim explained that returning resurrection is not the same as reincarnation. She emphasized that failure to make the distinction has led many dead people to try to "reincarnate", but wound up only possessing other people - to their mutual detriment.
Indemnity.
Indemnity, in the context of Unification Church beliefs, is a part of the process by which human beings and the world are restored to God's ideal. The concept of indemnity is explained at the start of the second half of the "Divine Principle", "Introduction to Restoration":
The "Divine Principle" goes on to explain three types of indemnity conditions. Equal conditions of indemnity pay back the full value of what was lost. The biblical verse "life for life, eye for eye, tooth for tooth" (Exod.21:23-24) is quoted as an example of an equal indemnity condition. Lesser conditions of indemnity provide a benefit greater than the price that is paid. Faith, baptism, and holy communion are mentioned as examples of lesser indemnity conditions. Greater conditions of indemnity come about when a person fails in a lesser condition. In that case a greater price must be paid to make up for the earlier failure. Abraham's attempted sacrifice of his son Isaac (Gen. 22:1-18) and the Israelites' 40 years of wandering in the wilderness under Moses (Num.14:34) are mentioned as examples of greater indemnity conditions. The "Divine Principle" then explains that an indemnity condition must reverse the course by which the mistake or loss came about. Indemnity, at its core, is required of humans because God is pure, and purity cannot relate directly with impurity. Indemnification is the vehicle that allows a "just and righteous" God to work through mankind. Jesus' statement that God had forsaken him (Matt.27:46) and Christianity's history of martyrdom are mentioned as examples of this. The "Divine Principle" then states that human beings, not God or the angels, are the ones responsible for making indemnity conditions.
In 2005 scholars Daske and Ashcraft explained the Unificationist concept of indemnity: 
Other Christian commentators have criticized the concept of indemnity as being contrary to the Christian doctrine of salvation by faith. Radio and television evangelist Bob Larson said, "Moon's doctrine of sinless perfection by 'indemnity', which can apply even to deceased ancestors, is a denial of the salvation by grace offering through Jesus Christ." Christian historian Ruth Tucker said: "In simple language indemnity is salvation by works." Donald Tingle and Richard Fordyce, ministers with the Christian Church (Disciples of Christ) who debated two Unification Church theologians in 1977, wrote: "In short, indemnity is anything you want to make it, since you establish the conditions. The zeal and enthusiasm of the Unification Church members is not so much based on love for God as it is compulsion to indemnify one's own sins." The Unification Church has also been criticized for saying that the First World War, the Second World War, the Holocaust, and the Cold War served as indemnity conditions to prepare the world for the establishment of the Kingdom of God.
The True Family.
In Unification Church terminology, the True Family is the family of church founder and leader Sun Myung Moon and his wife Hak Ja Han.
Church members regard Moon as the Second Coming of Christ,
and he and his wife as the "True Parents" of humankind, who have realized the ideal of true love as the incarnation of God's Word.
The members of the Unification Movement generally address or refer to Rev. and Mrs. Moon as "Father" and "Mother" or "True Father" and "True Mother."
Their children are known as the "True Children."
Sun Myung and Hak Ja Han are regarded to have achieved the status of True Parents on January 1, 1968, at the end of their "7-year course" of marriage together, representing the perfection of God's masculine and feminine aspects. Unification theology teaches that Jesus achieved this perfection only on the individual level, and that had he not died on the cross he would have married. It further teaches that, having married, he and his wife would have become "True Parents", created a "True Family", and would have saved humanity and perfected the world. Unfortunately Jesus was unable to complete his mission of perfecting the world and went the way of the cross, but his death was not a complete defeat because Jesus died for our sins giving us spiritual salvation.
The primary mission of True Parents is to engraft all people on earth and in the spirit world to the original sinless lineage of God, removing them from the satanic lineage established at the fall of humanity (the original sin in the Garden of Eden).
Sex and marriage.
The Unification Church is well known for its wedding or marriage rededication ceremony. It is given to married (or engaged) couples. Through it, members of the Unification Church believe, the couple is removed from the lineage of sinful humanity and engrafted into God's sinless lineage. The Blessing ceremony was first held in 1961 for 36 couples in Seoul, South Korea by the Moons shortly after their own marriage in 1960. All the couples were members of the church. Rev. Moon matched all of the couples except 12 who were already married to each other before joining the church.
Later Blessing ceremonies were larger in scale but followed the same pattern. All participants were Unification Church members and Rev. Moon matched most of the couples. In 1982 the first large scale Blessing (of 2,000 couples) outside of Korea took place in Madison Square Garden, New York City. In 1988, Moon matched 2,500 Korean members with Japanese members for a Blessing ceremony held in Korea, partly in order to promote unity between the two nations. In 1992 Sun Myung Moon gave the wedding blessing for 30,000 couples at the Seoul Olympic Stadium and for 13,000 at the Yankee Stadium.
In 2013, four months after the death of Sun Myung Moon, the church held a Blessing ceremony for 3,500 couples in South Korea, while another 24,000 followers took part in other countries via video link. This ceremony was presided over by Hak Ja Han.
Several church-related groups are working to promote sexual abstinence until marriage and fidelity in marriage and to prevent child exploitation; they care for victims of Thailand's massive sex trade as well. In 1996, Unification Church gathered 3,500 signatures during its anti-pornography campaign. A church official said, "pornography makes love seem temporal, pure love goes beyond the sexual relationship."
Ceremonies.
The "Family Pledge" of the Unification Church is an eight-part promise of church members to focus on God and His kingdom. Eight verses of the Family Pledge include the phrase "by centering on true love." For the first 40 years of the church's existence, members recited the pledge on Sunday mornings at 5:00 a.m. Now they recite it every 8 days, on "Ahn Shi Il": Day of Settlement and Attendance, which is the Unification Church's equivalent of a Sabbath. The first part says, "Our family, the owner of Cheon Il Guk, pledges to seek our original homeland and build the Kingdom of God on earth and in heaven, the original ideal of creation, by centering on true love."
The Unification Symbol.
According to the current head of the Family Federation for World Peace and Unification (FFWPU), the Tongil ("Unity" or "Unification" in Korean) mark represents the flag of "Cheon Il Guk"—otherwise understood as the Kingdom of Heaven on Earth. It is created with significant meaning and numbers: Its gold color symbolizes an ideal world of peace; the circle in the center represents God and his True Love, True Life and True Lineage; the twelve lines represent 12 months of the year and twelve types of human personalities; the square represents four directions, North, South East & West, and the four position foundation centered on God; and the circle around represents give and receive action between the visible and invisible worlds.
Related organizations.
The Collegiate Association for the Research of Principles (CARP) is a collegiate organization founded by Moon and church members in 1955, which "promotes intercultural, interracial, and international cooperation through the Unification world view." J. Isamu Yamamoto states in "Unification Church": "At times CARP has been very subtle about its association with the Unification Church, however, the link between the two has always been strong, since the purpose of both is to spread Moon's teachings."
The Little Angels Children’s Folk Ballet of Korea is a dance troupe founded in 1962 by Moon and other church members to project a positive image of South Korea to the world. In 1973 they performed at the United Nations Headquarters in New York City. The group’s dances are based on Korean legends and regional dances, and its costumes on traditional Korean styles.
Tongil Group is a South Korean business group (chaebol “Tongil” is Korean for “unification,” the name of the Unification Church in Korean is “Tongilgyo.”), founded in 1963 by Moon as a nonprofit organization which would provide revenue for the church. Its core focus was manufacturing but in the 1970s and 1980s it expanded by founding or acquiring businesses in pharmaceuticals, tourism, and publishing. Among Tongil Group’s chief holdings are: The Ilwha Company, which produces ginseng and related products; Ilshin Stone, building materials; and Tongil Heavy Industries, machine parts including hardware for the South Korean military.
The Unification Theological Seminary (UTS), founded in 1975, is the main seminary of the international Unification Church. It is located in Barrytown, New York and with an Extension Center in midtown Manhattan. Its purpose has been described as training leaders and theologians within the Unification Church. The seminary's professors come from a wide range of faiths, including a Rabbi, a Sheikh, a Methodist minister, a Presbyterian, and a Roman Catholic priest.
News World Communications, is an international news media corporation founded by Moon in 1976. It owns United Press International, "The World and I", "Tiempos del Mundo" (Latin America), "The Segye Ilbo" (South Korea), "The Sekai Nippo" (Japan), the "Zambezi Times" (South Africa), "The Middle East Times" (Egypt). Until 2008 it published the Washington D.C. based newsmagazine "Insight on the News". Until 2010, it owned the "Washington Times". On November 2, 2010, Sun Myung Moon and a group of former "Times" editors purchased the "Times" from News World.
The International Coalition for Religious Freedom is an activist organization based in Virginia, the United States. Founded by the Unification Church in the 1980s, it has been active in protesting what it considers to be threats to religious freedom by governmental agencies.
The Universal Ballet, founded South Korea in 1984, is one of only four professional ballet companies in South Korea. The company performs a repertory that includes many full length classical story ballets, together with shorter contemporary works and original full-length Korean ballets created especially for the company. It is supported by church members with Moon's daughter-in-law Julia H. Moon, who was the company's prima ballerina until 2001, now serving as General Director.
The Women's Federation for World Peace (WFWP) is an organization whose stated purpose is to encourage women to work more actively in promoting peace in their communities and greater society. It was founded in 1992 by Hak Ja Han and is supported by the church. It has members in 143 countries.
Pyeonghwa Motors, founded in 2000, has invested more than $300 million in the automobile industry of the North Korea. Starting in 1992 the church established business ties with communist North Korea and owned an automobile manufacturer (Pyeonghwa Motors), a hotel, and other properties there. In 1998, the Unification Movement launched its operations in North Korea with the approval of the Government of South Korea, which had prohibited business relationships between North and South before.
As of December 1994, the church had invested $150 million in Uruguay. Members own the country's largest hotel, one of its leading banks, the second-largest newspaper and two of the largest printing plants.
The Middle East Peace Initiative sponsors projects to promote peace and understanding including visits by international Christians to Israel and Palestine and dialogues between members of the Israeli Knesset and the Palestinian Parliament.
The Sunmoon Peace Football Foundation founded by the church in 2003 sponsors the Peace Cup, an invitational pre-season friendly football tournament for club teams, currently held every two years. It is contested by the eight clubs from several continents, though 12 teams participated in 2009. The first three competitions were held in South Korea, and the 2009 Peace Cup Andalucia was held in Madrid and Andalusia, Spain. In 1989, Moon had founded Seongnam Ilhwa Chunma Korean football team.
The Sun Hwa International Academy-Philippines (SHIA) an academy founded on March 25, 2002 by the leaders, active parent members and friends of the Unification Movement-Philippines. Drs. Christopher and Julia Kim, Unification Movement International (UM) Continental Directors for Asia led the founding of the institution and provided much support and inspiration to the Academy.

</doc>
<doc id="32241" url="http://en.wikipedia.org/wiki?curid=32241" title="Coast Guard Aviation Association">
Coast Guard Aviation Association

The Coast Guard Aviation Association, formerly known as the Ancient Order of the Pterodactyl (AOP) is a fraternal association founded in 1977. The organization has the purpose of focusing on United States Coast Guard aviation history. On 5 May 2007, the organization was renamed the Coast Guard Aviation Association.
Membership is for former and present officers and enlisted personnel of the United States Coast Guard, and designated pilots of other military services and foreign governments who have piloted Coast Guard aircraft while involved in exchange programs between the Coast Guard and their respective service or government.
Its goals include furthering and preserving the history of Coast Guard aviation and maintaining camaraderie between past and present Coast Guard members. The Coast Guard Aviation Association is headquartered in Chantilly, Virginia.
Activities.
The Order hosts an annual gathering known as the "Roost". A Roost is normally anchored by a Coast Guard Air Station.
List of past Roost locations:

</doc>
<doc id="32242" url="http://en.wikipedia.org/wiki?curid=32242" title="University of Canterbury">
University of Canterbury

The University of Canterbury (Māori: "Te Whare Wānanga o Waitaha"; postnominal abbreviation Cantuar. or Cant. for "Cantuariensis", the Latin name for Canterbury) in Christchurch is New Zealand's second oldest university. Founded in 1873 by professors Charles Cook (Mathematics, St John's College, Cambridge), Alexander Bickerton (Chemistry and Physics, School of Mining, London), and John Macmillan Brown (Balliol College, Oxford), it operates its main campus in the suburb of Ilam. The university offers degrees in Arts, Commerce, Education (physical education), Engineering, Fine Arts, Forestry, Health Sciences, Law, Music, Social Work, Speech and Language Pathology, Science, Sports Coaching and Teaching.
History.
The University originated in 1873 in the centre of Christchurch as Canterbury College, the first constituent college of the University of New Zealand. It became the second institution in New Zealand providing tertiary-level education (following the University of Otago, established in 1869), and the fourth in Australasia.
The Canterbury Museum and Library and Christ's College, dissatisfied with the state of higher education in Canterbury, had both worked towards setting up Canterbury College.
In 1933, the name changed from "Canterbury College" to "Canterbury University College". In 1957 the name changed again to the present "University of Canterbury".
Until 1961, the University formed part of the University of New Zealand (UNZ), and issued degrees in its name. That year saw the dissolution of the federal system of tertiary education in New Zealand, and the University of Canterbury became an independent University awarding its own degrees. Upon the UNZ's demise, Canterbury Agricultural College became a constituent college of the University of Canterbury, as "Lincoln College". Lincoln College became independent in 1990 as a full university in its own right.
Over the period from 1961 to 1974, the university campus relocated from the centre of the city to its much larger current site in the suburb of Ilam. The neo-gothic buildings of the old campus became the site of the Christchurch Arts Centre, a hub for arts, crafts and entertainment in Christchurch.
In 2004, the University underwent restructuring into four Colleges and a School of Law, administering a number of schools and departments (though a number of departments have involvement in cross-teaching in numerous academic faculties). For many years the university worked closely with the Christchurch College of Education, leading to a full merger in 2007, establishing a fifth College.
2010/11 earthquakes.
The James Hight building suffered extensive damage during the 2010 Canterbury earthquake.
Following a magnitude 6.3 earthquake on 22 February 2011, the university was temporarily closed to allow a full safety inspection of all its buildings. A progressive restart of the University began on 14 March with lectures delivered online, off-site, and in tents set up on campus. In September 2011, plans were announced to demolish some University buildings. In the months following the earthquake, the University lost 25 per cent of its first-year students and 8 per cent of continuing students. The number of international students, who pay much higher fees and are a major source of revenue, dropped by 30 per cent. By 2013, the University had lost 22 per cent of its students, leading a former student, visiting the University, to describe the campus as a "ghost town". She commented, "The February 2011 earthquake not only rocked the foundations of many of the campus's buildings, it also knocked the confidence of many of the University's students". However, a record number of 886 PhD students are enrolled at the University of Canterbury as of 2013.
Other New Zealand universities, apparently defying an informal agreement, launched billboard and print advertising campaigns in the earthquake-ravaged city to recruit University of Canterbury students who are finding it difficult to study there. In October 2011, staff were encouraged to take voluntary redundancies as the university scrambled to survive through a financial crisis. The Vice-Chancellor Dr Carr warned "There was 'no doubt' staff who were teaching a smaller number of students, researchers whose outputs were smaller and researchers who were not attracting grants would be at high risk of redundancy". He described possible changes in university courses by stating "What we don't know, and we won't know, is where there are rationalisations of courses within programmes – where we may be able to, instead of having twelve flavours, have eight flavours. We may require staff to teach four courses instead of three courses. But the impact on the actual programmes we offer will be quite modest."
Governance.
The university was first governed by a board of governors (1873–1933), then by a college council (1933–1957), and since 1957 by a university council. The council is chaired by a chancellor. The Council includes representatives from the faculties, students and general staff, as well as local industry, employer and trade union representatives.
The original composition of the board of governors was defined in the Canterbury College Ordinance 1873, which was passed by the Canterbury Provincial Council and named 23 members who might serve for life. Initially, the board was given power to fill their own vacancies, and this power transferred to graduates once their number exceeded 30. At the time, there were discussions about the abolition of provincial government (which did happen in 1876), and the governance structure was set up to give board members "prestige, power and permanence", and "provincial authority and its membership and resources were safely perpetuated, beyond the reach of grasping hands in Wellington."
Original members of the Board of Governors were: Charles Bowen, Rev James Buller, William Patten Cowlishaw, John Davies Enys, Charles Fraser, George Gould Sr, Henry Barnes Gresson, William Habens, John Hall, Henry Harper, John Inglis, Walter Kennaway, Arthur C. Knight, Thomas William Maude, William Montgomery, Thomas Potts, William Rolleston, John Studholme, Henry Tancred, James Somerville Turnbull, Henry Richard Webb, Joshua Williams, and Rev William Wellington Willock.
Professor Roy Sharp assumed the position of Vice-Chancellor on 1 March 2003. In May 2008 he announced his imminent resignation from the position, following his acceptance of the chief executive position at the Tertiary Education Commission (TEC) which he took up on 4 August 2008. The then current Deputy Vice-Chancellor, Professor Ian Town, assumed the role of acting Vice-Chancellor on 1 July 2008. On 15 October 2008 the University announced that Dr Rod Carr, a former banker and the CEO of a local software company, would begin a five-year appointment as Vice-Chancellor on 1 February 2009. Under Carr's leadership, UC's position in the QS World University Rankings has steadily declined by about 30%, as of September 2014 (for detail, see section headed 'League Tables', below).
Council member and former Pro-Chancellor, Rex Williams, became Chancellor in 2009. Council member Dr John Wood became the new Pro-Chancellor. On 1 January 2012, Dr Wood became Chancellor after Williams retired from the role.
Chairmen of the Board of Governors.
Chairmen of the Board of Governors were:
Chairmen of the College Council.
Chairmen of the College Council were:
Chancellors.
The current Chancellor is John Wood. Previous Chancellors were:
Campus.
The University has a main campus of 76 ha at Ilam, a suburb of Christchurch about 5 km from the centre of the city. Adjacent to the main campus stands the University's College of Education, with its own sports-fields and grounds. The University maintains four libraries, with the Central Library (Māori: "Te Puna Mātauraka o Waitaha") housed in the tallest building on campus, the 11-storey James Hight building.
The University's College of Education maintains additional small campuses in Nelson, Tauranga and Timaru, and "teaching centres" in Greymouth, New Plymouth, Rotorua and Timaru. The University has staff in regional information offices in Nelson, Timaru, and Auckland.
Canterbury University has six halls of residence housing around 1800 students. The largest of these are Ilam Apartments and University Hall with 850 residents and 550 residents, respectively. Three of these halls (Ilam Apartments, University Hall and Sonoda Christchurch Campus) are managed by UC Accommodation, a subsidiary of Campus Living Villages, while the university maintains ownership of the property and buildings. Sonoda Christchurch Campus has a close relationship with Sonoda Women's University in Amagasaki, Japan. Bishop Julius, College House and Rochester and Rutherford are run independently.
The six halls of residence are:
The Field Facilities Centre administers four field-stations:
The University and its project-partners also operate an additional field-station in the Nigerian Montane Forests Project – this field station stands on the Ngel Nyaki forest edge in Nigeria.
The Department of Physics and Astronomy runs its own field laboratories:
The Department of Physics and Astronomy also has involvement in the Southern African Large Telescope and is a member of the IceCube collaboration which is installing a neutrino telescope at the South Pole.
Libraries.
There are four libraries on campus each covering different subject areas.
Rankings.
In 2011 QS World University Rankings ranked the University of Canterbury 212th overall in the world, and the third highest ranked university in New Zealand. Its individual global subject rankings were: 226th in Arts & Humanities, 128th in Engineering & IT, 207th in Natural Sciences, and 243rd in Social Sciences. UC's QS ranking has fallen every year since 2008; although the QS rankings of most other New Zealand universities have also declined overall since 2008, some have risen, and only one has fallen more places than UC during this period.
The University was the first in New Zealand to have been granted five stars by QS Stars.
Student association and traditions.
The University of Canterbury Students' Association (UCSA) operates on campus with its own radio station (RDU) and magazine ("Canta"). The Association also runs two bars and several cafes around campus. The popular on-campus bar, "The Foundry", known as "The Common Room" from 2005, has reverted to its former name as promised by 2008 USCA president, Michael Goldstein. Prior to earthquakes in 2010 and 2011, the UCSA also ran the now damaged 430-seat Ngaio Marsh Theatre.
The University has over 100 academic, sporting, recreational and cultural societies and clubs. The most prominent of these include the University of Canterbury Engineering Society (ENSOC), the Law Society (LAWSOC), the Commerce Society (UCom), as well as the largest non-faculty clubs such as Motosoc (Motorsports Society), BYCSOC (Backyard Cricket Society), CUBA (Canterbury University Boardriders' Association), CurrySoc, JSoc, The Gentlemen's Club, and KAOS (Killing As Organised Sport). CUSSC (Canterbury University Snow Sports Club) is the only university club in New Zealand to own a ski field lodge, located at Temple Basin Ski Field the club runs many events to raise funds for maintenance of their lodge. The University of Canterbury Drama Society (Dramasoc) achieved fame for its 1942–1969 Shakespeare productions under Dame Ngaio Marsh, but regularly performs as an active student- and alumni-run arts fixture in the small Christchurch theatre-scene. The Musical Society, MuSoc, engages in comparable activities.
One major student tradition, the Undie 500, involved an annual car-rally from Christchurch to Dunedin run by ENSOC. The rules required only the use of a road-legal car costing under $500 with a sober driver. The 2007 event gained international news coverage (including on CNN and BBC World) when it ended in rioting in the student quarter of Dunedin and in North East Valley. ENSOC cancelled the planned 2008 event. The Undie 500 was replaced by the Roundie 500 in 2011. This event has the same principles but follows a route through rural Canterbury, returning to Christchurch the same day.
Coat of arms.
With the dissolution of the University of New Zealand, the newly independent University of Canterbury devised its own coat of arms, blazoned:
"murrey a fleece argent, in base a plough or, and on a chief wavy or an open book proper bound murrey, edged and clasped or between a pall azure charged with four crosses formy fitchy or and a cross flory azure."
What it means.
In this description, the colour of the shield is the first thing stated. "Murrey" means maroon. This is a colour seldom seen in Heraldry.
Next the objects on the shield and their colour are described. "A fleece" is usually depicted as a whole sheep with a band around its middle and "argent" means silver (or white as it is usually depicted.) "In Base" means at the bottom of the shield, and the object is a hand plough. "Or" means gold so the plough is coloured gold.
A "chief" is a broad stripe across the top of the shield and "wavy" means the line at the base of the chief is like a sine wave. "Or" again means gold so the chief is coloured gold.
The objects on the chief are then described. "An open book" is self-explanatory. "Proper" means the object is depicted in its natural colour(s) - as books normally have white pages, this is how it looks. The book is "bound murrey" which means the covers are in maroon. However, the edges of the pages are in gold ("edged or".) The book also has clasps ("clasped") in maroon. A clasp allows the book to be more securely bound after it has been closed. The "between" indicates that the book is between two other objects; in this case a "pall" which is the Y shaped object. "Azure" means it is blue. "Charged" means that the following objects are placed on the pall. The "four crosses" are Christian crosses but "formy" means the arms of the cross flare at the ends and "fitchy” means that the lower arm has a pointed end. Again, "or" means these crosses are gold. The pall is a link between Canterbury, New Zealand, and Canterbury, England as both the pall and the crosses appear on the arms of the Archbishop of Canterbury. The other object is another type of cross. this time all arms are of equal length. "Flory" means that end of each arm is a fleur-de-lys. "Azure" means that this cross is blue.
This replaced the arms formerly used by Canterbury College – an unofficial, simplified version of the Canterbury Province coat of arms.
The fleece symbolises the pastoral, and the plough at the base the agricultural background of the province of Canterbury. The Bishop's Pall and the cross flory represent Canterbury's ecclesiastical connections, and the open book denotes scholarship.
As it relates to an institution of learning, the University's coat of arms does not have a helmet, crest or mantling on its bearings.
A more detailed history of the arms, including their formal heraldic description, appears on the University .
Awards.
The University was awarded the 2006 Cycle Friendly Award for the best cycle-friendly commitment by a public organisation in New Zealand.
Personnel.
Size and composition.
The number of students enrolled at UC have fallen from 18,783 during 2010 to 14,725 during 2014, though the number of new enrolments increased in 2014. Staff numbers have also declined from 833 full-time equivalent positions in 2010 to 726 in 2014.
Concerns over student racism.
In 2014, one faculty member chosen to receive a teaching award from the University of Canterbury Students’ Association refused to accept the award because of his concerns about student racism and sexism at UC.
Notable alumni.
Honorary doctors.
Since 1962, the University of Canterbury has been awarding honorary doctorates. In many years, no awards were made, but in most years, multiple doctorates were awarded. The highest numbers of honorary doctorates was awarded in 1973, when there were seven recipients.

</doc>
<doc id="32268" url="http://en.wikipedia.org/wiki?curid=32268" title="United Nations Relief and Rehabilitation Administration">
United Nations Relief and Rehabilitation Administration

The United Nations Relief and Rehabilitation Administration (UNRRA) was an international relief agency, largely dominated by the United States but representing 44 nations. Founded in 1943, it became part of the United Nations in 1945, and it largely shut down operations in 1947. Its purpose was to "plan, co-ordinate, administer or arrange for the administration of measures for the relief of victims of war in any area under the control of any of the United Nations through the provision of food, fuel, clothing, shelter and other basic necessities, medical and other essential services".
Its staff of civil servants included 12,000 people, with headquarters in New York. Funding came from many nations, and totaled $3.7 billion, of which the United States contributed $2.7 billion; Britain $625 million and Canada $139 million.Mr.Arunbabu from India contributed in this camp and he spent $400 million. 
UNRRA cooperated closely with dozens of volunteer charitable organizations, who sent hundreds of their own agencies to work alongside UNRRA. In operation only three years, the agency distributed about $4 billion worth of goods, food, medicine, tools, and farm implements at a time of severe global shortages and worldwide transportation difficulties. The recipient nations had been especially hard hit by starvation, dislocation, and political chaos. It played a major role in helping Displaced Persons return to their home countries in Europe in 1945-46. Its UN functions were transferred to several UN agencies, including the International Refugee Organization and the World Health Organization. As an American relief agency, it was largely replaced by the Marshall Plan, which began operations in 1948.
Founding and authority.
U.S. president Franklin Delano Roosevelt proposed the agency in June, 1943, to provide relief to areas liberated from Axis powers after World War II ended. Roosevelt had already obtained the approval of the United Kingdom, the Soviet Union, and China; he later obtained endorsements from 40 other governments to form the first "United Nations" organization.
The Agreement for United Nations Relief and Rehabilitation Administration founding document was signed by 44 countries in the White House in Washington, November 9, 1943. UNRRA was headed by a Director-General, and governed by a Council (composed of representatives of all state parties) with a Central Committee representing the United States, Britain, China, and the Soviet Union. The other countries who signed the agreement included: Australia, Belgium, Bolivia, Brazil, Canada, Chile, Colombia, Costa Rica, Cuba, Czechoslovakia, Dominican Republic, Ecuador, Egypt, El Salvador, Ethiopia, the French Committee of National Liberation, Greece, Guatemala, Haiti, Honduras, Iceland, India, Iran, Iraq, Liberia, Luxembourg, Mexico, Netherlands, New Zealand, Nicaragua, Norway, Panama, Paraguay, Peru, Philippines, Poland, South Africa, Uruguay, Venezuela, and Yugoslavia.
Although the UNRRA was called a "United Nations" agency, it was established prior to the founding of the United Nations. The explanation for this is that the term "United Nations" was used at the time to refer to the Allies of World War II, having been originally coined for that purpose by Roosevelt in 1942.
Although initially restricted by its constitution to render aid only to nationals from the United Nations (the Allies), this was changed late in 1944, in response to pleas from Jewish organizations who were concerned with the fate of surviving Jews of German nationality, to also include "other persons who have been obliged to leave their country or place of origin or former residence or who have been deported therefrom by action of the enemy because of race, religion or activities' in favor of the United Nations."
UNRRA operated in occupied Germany, primarily in camps for Displaced Persons, especially the 11,000,000 non-Germans who had been moved into Germany during the war, but did not render assistance to ethnic Germans.
In Asia the organization provided assistance in China and Taiwan. Allegations of fraud or corruption in UNRRA were rare, but there were allegations of misappropriation of the aid by the Kuomintang (Chinese Nationalist Party).
UNRRA Headquarters was in Washington, D.C., and the European Regional Office was set up in London. The organization was subject to the authority of the Supreme Headquarters of the Allied Expeditionary Forces (SHAEF) in Europe and was directed by three Americans during the four years of its existence. Its first director-general was Herbert Lehman (1 January 1944 to 31 March 1946), former governor of New York. He was succeeded by Fiorello La Guardia (1 April to 31 December 1946), former mayor of New York – who later learned that that his sister, Gemma LaGuardia Gluck, and other relatives had been imprisoned in Nazi concentration camps. LaGuardia was, in turn, followed by Major General Lowell Ward Rooks (1 January 1947 to 30 September 1948).
Operations.
44 nations contributed to funding, supplying, and staffing the agency, of which the United States was the leading donor. The largest recipients of UNRRA commodity aid, in millions of US dollars were China, $518; Poland – $478; Italy – $418; Yugoslavia – $416; Greece – $347; Czechoslovakia – $261; Ukraine (USSR) – $188; and Austria – $136. Hitchcock 2008 concludes that UNRRA was not perfect, for it was troubled by inefficiency, poor planning, shortages of supplies, and some incompetent personnel. On balance however, he argues, it was a major success in terms of delivering aid, food, and medicine, and helping Europe on the path to recovery, especially Eastern and Southern Europe.

</doc>
<doc id="32270" url="http://en.wikipedia.org/wiki?curid=32270" title="UNRWA">
UNRWA

Created in December 1949, the United Nations Relief and Works Agency for Palestine Refugees in the Near East (UNRWA) is a relief and human development agency, originally intended to provide jobs on public works projects and direct relief for 652,000 Arab Palestinians who fled or were expelled from their homes during the fighting that followed the end of the British mandate over the region of Palestine
It also provided relief to Jewish and Arab Palestine refugees inside the state of Israel following the 1948 conflict until the Israeli government took over responsibility for Jewish refugees in 1952.
In the absence of a solution to the Palestine refugee problem, the General Assembly has repeatedly renewed UNRWA's mandate, most recently extending it until 30 June 2017.
Today, UNRWA provides education, health care and social services to the 5 million registered Palestine refugees from the 1948 and 1967 wars, a number which includes all their descendants. Aid is provided to Palestinian refugees living in Jordan, Lebanon and Syria, as well as those in the West Bank and the Gaza Strip.
UNRWA is the only agency dedicated to helping refugees from a specific region or conflict and is separate from UNHCR. Formed in 1950, UNHCR is the UN main Refugee Agency, which is responsible for aiding other refugees all over the world. UNHCR, unlike UNRWA, has a specific mandate to aid its refugees to eliminate their refugee status by local integration in current country, resettlement in a third country or repatriation when possible. UNRWA allows refugee status to be inherited by descendants.
History and mandate.
UNRWA was established following the 1948 Arab-Israeli War by the United Nations General Assembly under This resolution also reaffirmed paragraph 11, concerning refugees, of UN General Assembly Resolution 194 (1948), adopted and passed unopposed, supported by Israel and the Arab states, with only the Soviet bloc and South Africa abstaining. UNRWA is a subsidiary organ of the United Nations General Assembly and its mandate is renewed every three years.
UNRWA has had to develop a working definition of "refugee" to allow it to provide humanitarian assistance. Its definition does not cover final status.
Palestine refugees are defined as “persons whose normal place of residence was Palestine during the period 1 June 1946 to 15 May 1948, and who lost both home and means of livelihood as a result of the 1948 conflict.”
UNRWA services are available to all those living in its area of operations who meet this definition, who are registered with the Agency and who need assistance. The descendants of Palestine refugee males, including adopted children, are also eligible for registration as refugees. When the Agency began operations in 1950, it was responding to the needs of about 750,000 Palestine refugees. Today, some 5 million Palestine refugees are registered as eligible for UNRWA services.
UNRWA provides facilities in 59 recognized refugee camps in Jordan, Lebanon, Syria, the West Bank and the Gaza Strip, and in other areas where large numbers of registered Palestine refugees live outside of recognized camps. For a camp to be recognized by UNRWA, there must be an agreement between the host government and UNRWA governing use of the camp. UNRWA does not itself run camps, has no police powers or administrative role, but simply provides services in the camp. Refugee camps, which developed from tent cities to settlements indistinguishable from their urban surroundings, house around one third of all registered Palestine refugees.
Organisation.
UNRWA is the largest agency of the United Nations, employing over 30,000 staff, 99% of which are locally recruited Palestinians.
UNRWA's headquarters are divided between the Gaza Strip and Amman, Jordan. Its operations are organised into five fields – Jordan, Syria, Lebanon, West Bank and Gaza. UNRWA's Commissioner-General is the Swiss Pierre Krähenbühl, who succeeded Italian national Filippo Grandi on 30 March 2014. The Commissioner-General is responsible for managing UNRWA's overall activities. In each area where UNRWA operates there is a Director in charge of distributing humanitarian aid and overseeing general UNRWA operations. The "public face" of UNRWA is spokesman Chris Gunness.
Advisory Commission.
UN Resolution 302 (IV), adopted on 8 December 1949, created an Advisory Commission tasked with advising and assisting the Commissioner-General of UNRWA in carrying out the Agency’s mandate. It is not a governing body as many other UN Agencies maintain, but serves in a purely advisory capacity. Consisting of five members when it was first created, the Advisory Commission (AdCom) currently has 27 Members and 3 Observers.
The AdCom meets twice a year, usually in June and November, to discuss issues of importance to UNRWA. Members and Observers meet more regularly through Sub-Committee meetings.
Members of the Advisory Commission.
Membership in the Advisory Commission is obtained through resolution of the UN General Assembly. All countries considered as hosts of Palestine refugees (Jordan, Syria, Lebanon) sit on the Advisory Commission, and 24 countries who are major supporters sit on the Commission. In addition, Palestine, the European Union, and the League of Arab States have had Observer status on the Commission since 2005.
The full list of AdCom members, including the year they joined, are: Australia (2005), Belgium (1953), Brazil (2014), Canada (2005), Denmark (2005), Egypt (1949), Finland (2008), France (1949), Germany (2005), Ireland (2008), Italy (2005), Japan (1973), Jordan (1949), Kuwait (2010), Lebanon (1953), Luxembourg (2012), Netherlands (2005), Norway (2005), Saudi Arabia (2005), Spain (2005), Sweden (2005), Switzerland (2005), Syrian Arab Republic (1949), Turkey (1949), United Arab Emirates (2014), United Kingdom (1949), United States of America (1949).
Funding.
Most of UNRWA's funding comes from European countries and the United States.
In 2009, UNRWA’s total budget was US$1.2 billion, for which the agency received US$948 million. In 2009, the retiring Commissioner General spoke of a $200 million shortfall in UNRWA's budgets. Officials in 2009 spoke of a 'dire financial crisis'.
In 2010, the biggest donors for its regular budget were the United States and the European Commission with $248 million and $165 million respectively. Sweden ($47m), the United Kingdom ($45m), Norway ($40m) and the Netherlands ($29m) are also important donors. In addition to its regular budget, UNRWA receives funding for emergency activities and special projects.
In 2011, the United States was the largest single donor with a total contribution of over $239 million, followed by the European Commission’s $175 million contribution.
According to World Bank data, for all countries receiving more than $2 billion international aid in 2012, Gaza and the West Bank received a per capita aid budget over double the next largest recipient, at a rate of $495.
In 2013, of the total of was donated to UNRWA, $294 million was contributed by the United States.
Operations.
Services provided by UNRWA include health care, education, relief and social services and micro-credit loan programmes. In the following, UNRWA's own descriptions of itself are summarized.
Education programme.
UNRWA operates one of the largest school systems in the Middle East. It has been the main provider of basic education to Palestinian refugee children since 1950. The education programme is UNRWA's largest area of activity, accounting for half of its regular budget and 70 per cent of its staff. Basic education is available to all registered refugee children free of charge up to around the age of 15. In the 1960s UNRWA schools became the first in the region to achieve full gender equality.
Half the Palestine refugee population is under 25. Overcrowded classrooms containing 40 or even 50 pupils are common. Almost three quarters run on a double-shift system – where two separate groups of pupils and teachers share the same buildings, thus reducing teaching time. The school year is often interrupted by conflicts and children are often marked by trauma. In the face of these challenges, there are some remarkable achievements.
Key 2014 figures
UNRWA also operates nine vocational and technical training colleges, two educational science faculties and two teacher-training institutes.
Per longstanding agreement, UNRWA schools follow the curriculum of their host countries. This allows UNRWA pupils to progress to further education or employment holding locally recognised qualifications and fits with the sovereignty requirements of countries hosting refugees. Wherever possible, UNRWA students take national exams conducted by the host governments. Pupils at UNRWA schools often out-perform government school pupils in these state exams.
Not all refugee children attend UNRWA schools. In Jordan and Syria children have full access to government schools and many attend those because they are close to where they live.
Relief and social services programme.
In Palestinian refugee society, families without a male bread winner are often very vulnerable. Those headed by a widow, a divorcee or a disabled father often live in dire poverty.
These families are considered "hardship cases", and constitute less than 6% of the people served by UNRWA.
UNRWA provides food aid, cash assistance and help with shelter repairs to these families. In addition children from special hardship case families are given preferential access to the Agency's vocational training centres, while women in such families are encouraged to join UNRWA's women's programme centres. In these centres, training, advice and childcare are available to encourage female refugees’ social development.
UNRWA has created community-based organizations (CBOs) to target women, refugees with disabilities and to look after the needs of children. The CBOs now have their own management committees staffed by volunteers from the community. UNRWA provides them with technical and small amounts of targeted financial assistance, but many have made links of their own with local and international NGOs.
Health programme.
Since 1950, UNRWA has been the main healthcare provider for the Palestinian refugee population. Basic health needs are met through a network of primary care clinics, providing access to secondary treatment in hospitals, food aid to vulnerable groups and environmental health in refugee camps.
Key figures 2014
The health of Palestine refugees has long resembled that of many populations in transition from developing world to developed world status. However, there is now a demographic transition.
People are living longer and developing different needs, particularly those related to non-communicable diseases (NCDs) and chronic conditions that require lifelong care, such as diabetes, hypertension and cancer. A healthy life is a continuum of phases from infancy to old age, each of which has unique, specific needs, and our programme therefore takes a ‘life-cycle approach’ to providing its package of preventive and curative health services.
To address the changing needs of Palestine refugees, we undertook a major reform initiative in 2011. We introduced the Family Health Team (FHT) approach, based on the World Health Organization-indicated values of primary health care, in our primary health facilities (PHFs).
The FHT offers comprehensive primary health care services based on holistic care of the entire family, emphasizing long-term provider-patient relationships and ensuring person-centeredness, comprehensiveness and continuity. Moreover, the FHT helps address cross-cutting issues that impact health, such as diet and physical activity, education, gender-based violence, child protection, poverty and community development.
Medical services include outpatient care, dental treatment and rehabilitation for the physically disabled. Maternal and child healthcare (MCH) is a priority for UNRWA's health programme. School health teams and camp medical officers visit UNRWA schools to examine new pupils to aid early detection of childhood diseases. All UNRWA clinics offer family planning services with counselling that emphasises the importance of birth spacing as a factor in maternal and child health. Agency clinics also supervise the provision of food aid to nursing and pregnant mothers who need it and six clinics in the Gaza Strip have their own maternity units. Infant mortality rates have for some time been lower among refugees than the World Health Organisation's benchmark for the developing world.
UNRWA provides refugees with assistance in meeting the costs of hospitalisation either by partially reimbursing them, or by negotiating contracts with government, NGO and private hospitals.
Environmental health services
The UNRWA Environmental Health programme
controls the quality of drinking water, provides sanitation and carries out vector and rodent control in refugee camps, thus reducing the risk of epidemics.
UNRWA Microfinance Department.
UNRWA's Microfinance Department (MD) aims to alleviate poverty and support economic development in the refugee community by providing capital investment and working capital loans at commercial rates. The programme seeks to be as close to self-supporting as possible. It has a strong record of creating employment, generating income and empowering refugees.
The Microfinance Department is an autonomous financial unit within UNRWA, established in 1991 to provide microfinance services to Palestine refugees, as well as poor or marginal groups living and working in close proximity to them. With operations in three countries, the MD currently has the broadest regional coverage of any microfinance institution in the Middle East. Having begun its operations in the oPt, it remains the largest non-bank financial intermediary in the West Bank and Gaza.
Key Figures - cumulative as of 2014:
Emergency operations.
UNRWA takes a wide variety of action to mitigate the effects of emergencies on the lives of Palestine refugees.
Particularly in the West Bank and Gaza (occupied Palestinian territory (oPt)) there has been ongoing intervention made necessary by e.g. the 1967 war as well as the first and second intifadas, and - not least - the 2014 Gaza war.
Up until this point, the reconstruction work at Nahr el-Bared Palestine refugee camp in Lebanon has been the largest reconstruction project ever undertaken by UNRWA. This work started in 2009, and was made necessary when the camp was destroyed in the fighting between the Lebanese Armed Forces and Fatah al-Islam in 2007.
UNRWA evaluates the ongoing conflict in Syria as one of the most serious challenges ever. UNRWA supports Palestine both refugees displaced within Syria and those who have fled to neighbouring countries within the UNRWA areas of operations.
Services range from supplying temporary shelter, water, food, clothing, blankets - to temporary job-creation - to help for re-building. There is extensive cooperation with other international NGOs and local actors.
Infrastructure and camp/settlement improvement.
About one-third of the 5 million refugees registered with the Agency live in recognized camps/settlements in Jordan, Lebanon, Syria, the West Bank and the Gaza Strip. To date, UNRWA has participated in re-building 5,223
Houses in Nahr el Bared in Northern Lebanon and has initiated a recovery and reconstruction plan for Gaza including clinics, schools, and housing units. Special funding has been provided by Saudi Arabia, Japan, the Netherlands, and the United Arab Emirates.
Praise.
UNRWA has received extensive public expressions of praise and appreciation. This has likely contributed to the agency's mandate continually being renewed, most recently in June 2014, and to its considerable success with fund-raising from a very wide sponsor base.
In the time frame 1998–2009 some of the most notable praise and appreciation has been expressed by the Nobel Peace Laureates Mairéad Corrigan Maguire and Kofi Annan, by the President of the General Assembly of the United Nations, by UN Secretary General Ban Ki-Moon, and by representatives from the European Union, the United States, the Netherlands, Japan, Bangladesh, Cyprus, Jordan, Ghana, and Norway, among others. In 2007, the Permanent Representative of Norway to the United Nations described his country as a "strong supporter" of UNRWA, which acts as "a safety net" for the Palestine refugees, providing them with "immediate relief, basic services and the possibility of a life in dignity." The same day, the Representative of Iceland praised the fact that "despite times of exceptional hardship and suffering in the region, UNRWA has been able to deliver substantial results. On the humanitarian front, UNRWA played a central role in easing the suffering of both refugees and Lebanese civilians during its emergency operations in Lebanon and on the Gaza Strip. Under often life-threatening conditions, UNRWA's staff showed relentless dedication to the Agency's responsibilities."
In 2007 Gershon Kedar, Israel's delegate to the fourth committee, confirmed Israel's support for the UNRWA: "My delegation wishes to inform the Committee that despite our concerns regarding the politicization of UNRWA, Israel supports its humanitarian mission, and will continue to work in a spirit of dialogue and cooperation with the agency under the leadership of its Commissioner-General, Karen Honing AbuZayd."
In 2011 UNRWA agreed to be assessed as a multilateral organisation by The Multilateral Organisation Performance Assessment Network (MOPAN).
MOPAN is network of donor countries with a common interest in assessing the organisational effectiveness of
multilateral organisations. MOPAN was established in 2002 in response to international fora on aid effectiveness and calls for greater donor harmonisation and coordination. Today (2011) MOPAN is made up of 16 donor countries: Australia, Austria, Belgium, Canada, Denmark, Finland, France, Germany, Ireland, The Netherlands, Norway, Republic of Korea, Spain, Sweden, Switzerland and the United Kingdom. ...
MOPAN assessments provide a snapshot of four dimensions of organisational effectiveness (strategic management, operational management, relationship management, and knowledge management). MOPAN does not examine an organisation’s development results.
The MOPAN report evaluated a number of criteria positively. In UNRWA's response, the agency was pleased to note that "many of the challenges highlighted in the report reflect challenges within most, if not all, multilateral organisations."
UNRWA's continuously updated "press room page" refers to many articles and statements supportive to the agency.
All in all, it must be said that UNRWA achievements are considerable despite operating under difficult conditions.
On many occasions UNRWA buildings have been caught in battles between Israeli soldiers and Palestinian militants resulting in the deaths of several employees.
The amount and variety of humanitarian assistance needed has been affected greatly by the tightening of the closure regime since the Second Intifada in the former Israeli-occupied territories, chiefly Gaza.
For many reasons its refugee load has increased much faster than its budget. The 1967 war created additional refugees. In addition, the descendents of refugees are also within UNRWA purview; with all generations included, there are about five million people who are eligible for UNRWA's services.
The number and complexity of tasks have also increased. UNRWA's original mandate does not require resettling refugees or transferring responsibility to the Palestinian Authority. Although the UN mandate is renewed every three years, resettling and transferring have never been added to the mandate.
Criticism and controversies.
After 65 years, UNRWA may have become a nation and sub-culture unto itself. As Emanual Marx and Nitza Nachmias pointed out in 2004:
Most of the criticism concerns the UNRWA showing the pathology of "aging", including symptoms of inflexibility, resistance to adjust to the changing political environment and refusal to phase out and transfer its responsibilities to the Palestinian Authority.
UNRWA has initiated a reform program based on organizational development to improve efficiency in 2007.
As mentioned in another section, UNRWA has participated in a 2011 review of its internal practices. No follow-up can be identified.
In addition to challenges within internal operations, there are some more difficult issues which have been pointed out especially, but not solely, by Israel and pro-Israel groups.
UNRWA has been accused of hiring known militants, perpetuating Palestinian dependency, demonizing Israel, and funneling money from Western governments to line the pockets of the Palestinian Authority and purchasing arms for terrorists.
The mandate itself – including the definition of refugees.
The UNRWA definition is meant solely to determine eligibility for UNRWA assistance. However, some argue it serves to perpetuate the conflict. Under General Assembly Resolution 194 (III), of 11 December 1948, other persons may be eligible for repatriation and/or compensation but are not necessarily eligible for relief under the UNRWA's working definition.
UNRWA and the "right of return".
Critics of Israel say it should allow the refugees to return, which some say is stipulated in United Nations General Assembly Resolution 302 which Israel supported, which would make UNRWA redundant. Defenders of UNRWA respond that it is the stateless status of the Palestinians under British mandate in 1948 that made it necessary to create a definition of refugee based on criteria other than nationality.
Israel responds that settling so many people within Israel – most of whom never lived in that part of Mandate Palestine – would be fatally disruptive to the country.
Compromise proposals stress giving at least some refugees the option of living in Israel or receiving compensation – plus international aid to resettle as many people as possible in the host countries (including the Palestinian Authority).
An interplay with the refugee problem created when some 800,000 Jews were displaced from Arab countries is sometimes discussed. For example,
In 2000, President Bill Clinton stated on Israeli television that he sought to explore "a fund which compensates the Israelis who were made refugees by the war, which occurred after the birth of the State of Israel. Israel is full of people, Jewish people, who lived in predominantly Arab countries who came to Israel because they were made refugees in their own land."
Solutions tying together both streams of refugees may not need to involve much money; many observers stress the important of international recognition of suffering.
Creating dependency rather than resettling refugees.
Although UNRWA's Mandate is only Relief and Works, the Wall Street Journal Europe edition, published an op-ed by Asaf Romirowsky and Alexander H. Joffe in April 2011 saying that despite UNRWA's "purported goal, it is hard to claim that the UNRWA has created any Palestinian institutions that foster a genuinely civil society. Ideally the UNRWA would be disbanded and Palestinians given the freedom – and the responsibility – to build their own society."
The High Commission is mandated to help refugees get on with their lives as quickly as possible, and works to settle them rapidly, most frequently in countries other than those they fled. UNRWA policy, however, states that the Palestinian Arabs who fled from Israel in the course of the 1948 war, plus all their descendants, are to be considered refugees until a just and durable solution can be found by political actors. UNRWA was specifically designed not to proscribe how the outcome of an agreement would take shape.
James G. Lindsay, a former UNRWA general-counsel and fellow researcher for Washington Institute for Near East Policy published a report for WINEP in 2009 in which he criticized UNRWA practices. One of his conclusions was that UNRWA’s failure to match UNHCR’s success in resettling refugees "obviously represents a political decision on the part of the agency" and "seems to favor the strain of Palestinian political thought espoused by those who are intent on a 'return' to the land that is now Israel". However, UNRWA has never been given a mandate by the UN General Assembly to resettle refugees.
In 2010 John Ging, head of UNRWA Gaza, had stated that he was disappointed in Linday's criticisms of UNRWA for failure to resettle refugees. Ging argued that there is "no basis to say that it is UNRWA’s decision because our mandate is given to us. I agree that it is a political failure, but we don’t set up the mandate, we are only the implementers".
In 2006, the UNRWA drew criticism from the US Congressmen Mark Kirk and Steven Rothman. Their letter, sent to the US Secretary of State Condoleezza Rice, stated in part: "After an exhaustive review of the UN's own audit, it is clear UNRWA is wrought by mismanagement, ineffective policies, and failure to secure its finances. We must upgrade UNRWA's financial controls, management and enforcement of US law that bars any taxpayer dollars from supporting terrorists." UNRWA responded by showing the results of its school students in Syria and Jordan, who outperform their peers in host-government schools. UNRWA also mentioned the difficult conditions in which it operates: its refugee load increased much faster than its budget, while the tightening of the closure regime since the Second Intifada deeply affected the humanitarian situation in the former Israeli-occupied territories.
Senator Kit Bond (R-MO) said that UNRWA is an example of a United Nations anti-Israel bias, and that Palestine refugees should be treated the same as all others with refugee status around the world.
In 2011, UNRWA spokesman Chris Gunness wrote Palestinian refugees continue to be refugees because the issues which caused their exile remain outstanding. Only by addressing in a just and durable fashion the underlying causes of conflict – and by doing so in accordance with international law and the rights of refugees – can the refugee issue be laid to rest. This is the responsibility of the parties and international political actors. It is wishful, cynical thinking to suppose that Palestinian refugees can be made to "go away" by dispersing them around the globe or by dissolving the Agency established to protect and assist them pending a just and lasting solution to their plight.—Chris Gunness, UNRWA Jerusalem spokesman, 30 August 2011, http://www.huffingtonpost.com/chris-gunness/unrwa-beyond-the-myths_b_941669.html
In 2014, UNRWA was accused of perpetuating the refugee status of Palestinians by Bassem Eid, the director of the Palestinian Human Rights Monitoring Group. He wrote, 'UNRWA, to continue its operation, depends on death and the visual suffering of five million Palestinians who continue to wallow in and around UNRWA facilities.' He concluded, 'In the eyes of the Palestinians, UNRWA acts a state with its own foreign policy. And that foreign policy does not serve the best interests of the Palestinian refugees.'
His article, however, has been the subject of criticism for its widespread inaccuracies and almost verbatim paraphrasing of another UNRWA-critic. According to McGill University political science professor Rex Brynen: “One could go on picking holes in the Eid op ed for some considerable time. However, what was also striking was the extent to which his piece seemed to simply paraphrase points that were made back in October by perennial anti-UNRWA gadfly David Bedein in the right-wing Arutz Sheva news service.”
Also published in the Jerusalem Post, UNRWA rejected claims that it supports extremism, or is anti-Israeli, and defended its record on the 'effectiveness of our efforts on neutrality'.
Execution-related controversies.
Not being able to protect Palestinian refugees in host countries.
Asem Khalil, Associate Professor of Law at Birzeit University, and Dean of the Faculty of Law and Public Administration, has focused on human rights issues for Palestinians in host countries. After systematically documenting the human rights situation for Palestinians in Egypt, Jordan, Lebanon, and Syria, he concludes:
The point this approach is stresses, I believe, is not that UNRWA is not necessary or that Palestinian refugeehood is not unique and special, but rather that UNRWA is not currently capable of ensuring necessary protection for Palestinian refugees, and that host Arab states cannot use the uniqueness of Palestinian refugeehood to continue upholding discriminatory laws and policies towards Palestinian refugees. ...
The global financial crisis may result in decreasing international funds to UNRWA, and UNRWA may be pushed towards reducing its services. Such a scenario will be felt by Palestinian refugees in particular ways, seeing the absence of alternative sources of income and the restrictive laws and policies that exist in some host countries. UNRWA is a main service provider for Palestinian refugees in host countries. It provides jobs for thousands of refugees, education, health care, and various other services that are extremely valuable and necessary.
... The issue at stake here is that UNRWA is not enough, but the alternative is not the replacement of UNRWA by
UNHCR, rather the enhancement of the protection role of UNRWA, or the extension of protection mandate of UNHCR
to Palestinian refugees besides (not instead) existing agencies dealing with Palestinian refugees ...
Textbooks not preparing Palestinian youth for a future in peace.
In 2005 Nathan Brown, Professor of Political Science at George Washington University, wrote a short but comprehensive review article about textbooks used by Palestinians, focusing especially on changes starting in 1994.
The Oslo agreements resulted in the dismantling of the Israeli office responsible for censorship of textbooks. Administration of the education system for all Palestinian students in the West Bank and Gaza was taken over by the Palestinian Authority (PA). Other Palestinian schools administered by UNRWA in neighboring countries were unaffected. With the end of UNESCO monitoring of the books, UNRWA moved to develop supplementary materials to teach tolerance in the schools it administered.
It is the PA textbooks used in UNRWA schools in the West Bank, Gaza and East Jerusalem that have been most extensively studied. The following discussions cannot be generalized to UNRWA schools elsewhere.
In the beginning, the PA used books from Jordan and Egypt. In 2000 it started issuing its own books. Nathan Brown investigated the differences between the new PA books and the ones being replaced
Regarding the Palestinian Authority's new textbooks, he states:
The new books have removed the anti-Semitism present in the older books while they tell history from a Palestinian point of view, they do not seek to erase Israel, delegitimize it or replace it with the "State of Palestine"; each book contains a foreword describing the West Bank and Gaza as "the two parts of the homeland"; the maps show some awkwardness but do sometimes indicate the 1967 line and take some other measures to avoid indicating borders; in this respect they are actually more forthcoming than Israeli maps; the books avoid treating Israel at length but do indeed mention it by name; the new books must be seen as a tremendous improvement from a Jewish, Israeli, and humanitarian view; they do not compare unfavorably to the material my son was given as a fourth grade student in a school in Tel Aviv".
Brown has pointed out that research into Palestinian textbooks conducted by the Centre for Monitoring the Impact of Peace in 1998 is misleading, because it evaluates the old books; and in 2000, its research mixed old and new books.
In 2002, the United States Congress requested the United States Department of State to commission a reputable NGO to conduct a review of the new Palestinian curriculum. The Israel/Palestine Center for Research and Information (IPCRI) was thereby commissioned by the U.S. Embassy in Tel Aviv and the US Consul General in Jerusalem to review the Palestinian Authority's textbooks. Its report was completed in March 2003 and delivered to the State Department for submission to Congress. Its executive summary states: "The overall orientation of the curriculum is peaceful despite the harsh and violent realities on the ground. It does not openly incite against Israel and the Jews. It does not openly incite hatred and violence. Religious and political tolerance is emphasized in a good number of textbooks and in multiple contexts."
IPCRI's June 2004 follow-up report notes that "except for calls for resisting occupation and oppression, no signs were detected of outright promotion of hatred towards Israel, Judaism, or Zionism" and that "tolerance, as a concept, runs across the new textbooks". The report also stated that "textbooks revealed numerous instances that introduce and promote the universal and religious values and concepts of respect of other cultures, religions, and ethnic groups, peace, human rights, freedom of speech, justice, compassion, diversity, plurality, tolerance, respect of law, and environmental awareness".
However, the IPCRI noted a number of deficiencies in the curriculum.
The practice of 'appropriating' sites, areas, localities, geographic regions, etc. inside the territory of the State of Israel as Palestine/Palestinian observed in our previous review, remains a feature of the newly published textbooks (4th and 9th Grade) laying substantive grounds to the contention that the Palestinian Authority did not in fact recognize Israel as the State of the Jewish people. ...
The Summary also states that the curriculum asserts a historical Arab presence in the region, while
The Jewish connection to the region, in general, and the Holy Land, in particular, is virtually missing. This lack of reference is perceived as tantamount to a denial of such a connection, although no direct evidence is found for such a denial." It also notes that "terms and passages used to describe some historical events are sometimes offensive in nature and could be construed as reflecting hatred of and discrimination against Jews and Judaism."
The US State department has similarly raised concerns about the content of textbooks used in PA schools. In its 2009 Human Rights report, the U.S. State Department wrote that after a 2006 revision of textbooks by the PA Ministry of Education and Higher Education, international academics concluded that books did not incite violence against Jews but showed imbalance, bias, and inaccuracy. The examples given were similar to those given by IPCRI.
The Centre for Monitoring the Impact of Peace was re-constituted as The Institute for Monitoring Peace and Cultural Tolerance in School Education (IMPACT-SE) and seems to have improved the quality of its work. It has published a number of evaluations of PA textbooks The latest evaluation from 2011 concludes that the situation had not significantly improved, and that there were in fact many examples of incitement to hatred and demonization of Israel - conclusions not widely shared by other experts.
In 2013 the results of a rigorous study, which also compared Israeli textbooks to PA textbooks, came out. The study was launched by the Council for Religious Institutions in the Holy Land, an interfaith association of Jewish, Christian, and Muslim leaders in Israel and the Occupied Territories. The study was overseen by an international Scientific Advisory Panel and funded by the United States State Department The Council published a report" "Victims of Our Own Narratives? Portrayal of the 'Other' in Israeli and Palestinian School Books"."
Most books were found to be factually accurate except, for example, through presenting maps that present the area from the river to the sea as either Palestine or Israel. Israeli schoolbooks were deemed superior to Palestinian ones with regard to preparing children for peace, although various depictions of the "other" as enemy occurred in 75% of Israeli, and in 81% of Palestinian textbooks.
The study praised both Israel and the Palestinian Authority for producing textbooks almost completely unblemished by "dehumanizing and demonizing characterizations of the other". Yet many troubling examples were given of both sides failing to represent each other in a positive or even adequate way. And the problem was more pronounced in PA textbooks.
Harsh critics of PA textbooks give similar examples, but weight them more heavily than IPCRI, the U.S. State Department and The Council for Religious Institutions in the Holy Land do. In addition, the critics point to subtle examples not picked up by these studies.
Dr. Arnon Groiss is perhaps the strongest academic critic. He had in the past conducted an independent research of Palestinian, Egyptian, Syrian, Saudi Arabian, Tunisian and Iranian schoolbooks between 2000 and 2010 and was thus appointed to be a member of the Scientific Advisory Panel for the study from The Council For Religious Institutions in the Holy Land. He criticized the study and its results for the following:
Groiss concluded that "the main question, namely, to what extent is this or that party engaged in actual education for peace, if at all, has not been answered by the report itself."
All in all there seems to be broad agreement that there is continual improvement in the textbooks used by UNRWA - but very strong disagreement about whether the improvement is sufficient. In response to a critical report written in 2009 by former UNRWA general-counsel James G. Lindsay, fellow researcher for Washington Institute for Near East Policy John Ging, head of UNRWA Gaza, said: "As for our schools, we use textbooks of the Palestinian Authority. Are they perfect? No, they’re not. I can’t defend the indefensible."
UNRWA has taken many steps since the year 2000 to supplement the PA curriculum with concepts of human rights, nonviolent conflict resolution and tolerance. According to the UNRWA website:
"We have been delivering human rights education in our schools since 2000 to promote non-violence, healthy communication skills, peaceful conflict resolution, human rights, tolerance and good citizenship. In May 2012, the Agency endorsed its new Human Rights, Conflict Resolution and Tolerance (HRCRT) Policy to further strengthen human rights education in UNRWA. This policy builds upon past successes, but also draws from international best practices and paves the way to better integrate human rights education in all our schools. The HRCRT Policy reflects the UNRWA mandate of quality education for Palestine refugees and sets out a common approach among all UNRWA schools for the teaching and learning of human rights, conflict resolution and tolerance. The vision of the policy is to "provide human rights education that empowers Palestine refugee students to enjoy and exercise their rights, uphold human rights values, be proud of their Palestinian identity, and contribute positively to their society and the global community.""
Being compromised by members of Hamas and sympathizers with their agenda.
In April 2013 Palestinian journalist, Hazem Balousha, summed up years of tension between UNRWA and Hamas
Agency in Gaza faces increasing difficulty in carrying out its work, as the Hamas-led government claims some of its activities are not in line with the Strip’s Islamic culture and values ...
Hamas has even gone as far as using violence, threatening UN staff in Gaza, the former UNRWA chief in Gaza, John Ging, having survived two assassination attempts.
Hamas sympathizers employed in and dominating UNRWA unions in Gaza.
Peter Hansen, UNRWA's former Commissioner-General (1996–2005), caused controversy in Canada in October 2004 when he said in an interview with CBC TV:
Oh I am sure that there are Hamas members on the UNRWA payroll and I don't see that as a crime. Hamas as a political organization does not mean that every member is a militant and we do not do political vetting and exclude people from one persuasion as against another.<br>
We demand of our staff, whatever their political persuasion is, that they behave in accordance with UN standards and norms for neutrality.
Hansen later specified that he had been referring not to active Hamas members, but to Hamas sympathizers within UNRWA. In a letter to the Agency's major donors, he said he was attempting to be honest because UNRWA has over 8,200 employees in the Gaza Strip. Given the 30% support to Hamas in Gaza at the time, and UNRWA's workforce of 11,000 Palestinians, at least some Hamas sympathizers were likely to be among UNRWA's employees. The important thing, he wrote, was that UNRWA's strict rules and regulations ensured that its staff remained impartial UN servants. However, he was retired from United Nations service against his will on 31 March 2005.
James G. Lindsay, a former UNRWA general-counsel and fellow researcher for Washington Institute for Near East Policy published a report for WINEP in 2009 in which he criticized UNRWA practices. One of his conclusions was that UNRWA is not ousting terrorists from its ranks:
UNRWA has taken very few steps to detect and eliminate terrorists from the ranks of its staff or its beneficiaries, and no steps at all to prevent members of organizations such as Hamas from joining its staff. UNRWA has no preemployment security checks and does not monitor off-time behavior to ensure compliance with the organization's anti-terrorist rules. No justification exists for millions of dollars in humanitarian aid going to those who can afford to pay for UNRWA services.
In 2013 Lt. Col. (ret.) Jonathan Dahoah-Halevi, senior researcher of the Middle East and radical Islam at the Jerusalem Center for Public Affairs, asserted that 'the UNRWA workers union has been controlled in practice by Hamas for many years'.
Some have made the assertion that Hamas won a teachers union election for UN schools in Gaza in 2009. UNRWA has strongly denied this and notes that “Staff elections are conducted on an individual – not party list – basis for unions that handle normal labour relations – not political – issues. In addition, John Ging, the Gaza head of operations, said in a letter dated 29 March 2009 that employees must not "be under the influence of any political party in the conduct of their work."
It has also been claimed that in 2012, the Hamas "Professional List" again won a Staff Union election in UNRWA. The Professional List is led by alleged senior Hamas activist Suheil Al-Hind. More than 9500 UNRWA employees in the Gaza Strip participated; this represented more than 80% turnout. The professional list won three UNRWA workers groups: the employees’, teachers’, and services’ unions.
Schools.
It has been reported that Hamas has interfered with curriculum and textbooks in UNRWA schools.
For example, in 2009 it caused UNRWA to suspend a decision to introduce Holocaust studies in its schools.
One of UNRWA's flagships has been gender-equality and integration. But Hamas militants have firebombed UNRWA mixed-gender summer camps, and in 2013 Hamas passed a law requiring gender segregation in schools for all pupils nine years of age and older in Gaza. The law does not apply to UNRWA schools.
Elhanen Miller, Arab affairs reporter for The Times of Israel, wrote in February 2014 that Hamas was "bashing" UNRWA's human rights curriculum, saying that it included too many examples and values foreign to Palestinian culture and had too much emphasis on peaceful resistance rather than armed resistance. In this case UNRWA refused to be swayed. Spokesman Chris Gunness:
UNRWA has no plans to change its education programs in Gaza... human rights are taught in all UNRWA schools from grades 1 through 9, discussing the Universal Declaration of Human Rights.
UNRWA's education system takes as its basis the curriculum taught by the PA and so we use PA textbooks in preparing children in Gaza for public examinations. ... In addition, we enrich our education programs in Gaza with an agreed human rights curriculum which has been developed with the communities we serve: with educationalists, parents groups, teachers associations, staff members and others. We have done our utmost in developing these materials to be sensitive to local values while also being true to the universal values that underpin the work of the United Nations.
However, after a few days, UNRWA consented to temporarily suspending the use of only the books used in grades 7-9 (continuing to use the books used in grades 1-6) pending further discussions.
Camps and sports.
Hamas has denounced UNRWA and Ging, accusing them of using their summer camps to corrupt the morals of Palestinian youth. Hamas also advised UNRWA to reexamine its curriculum to ensure its suitability for Palestinian society, due to the mixing of genders at the camps.
In September 2011 it was reported that, under pressure from Hamas, UNRWA has made all its summer camps single-sex.
Hamas has its own network of summer camps and the two organizations are regarded to be vying for influence with Gazan youth. Islamic Jihad has also run summer camps since 2013
UNRWA did not operate its summer camps for summer 2012 and summer 2014 due to a lack of available funding. Hamas has filled this void and now is the direct provider of summer activities for about 100,000 children and youths.
In 2013, UNRWA canceled its planned marathon in Gaza after Hamas rulers prohibited women from participating in the race.
In 2013, Israeli media outlets aired a video documenting UNRWA-funded summer camps where children are being taught to engage in violence with Israelis. The video airs speakers telling campers “With God’s help and our own strength we will wage war. And with education and Jihad we will return to our homes!” A student is also shown on camera describing that “the summer camp teaches us that we have to liberate Palestine.”
UNRWA denies that the video shows UNRWA summer camps and instead shows footage of camps that were not operated by UNRWA. Following the release of the film, UNRWA released a statement that read, in part:
UNRWA has conducted a lengthy and detailed investigation into the film and we categorically reject the allegations it contains. The film is grossly misleading and we regret the damage it has caused to UNRWA and the United Nations.
The film-maker concerned has a history of making baseless claims about UNRWA, all of which we have investigated and demonstrated to be patently false. It has long been the practice of the film-maker to show non-UNRWA activities and portray them as activities of UNRWA. He has done this again and we again reject his allegations. Our repeated rejection of his falsehoods is a matter of public record.
The main accusation in the film is that incitement is promoted during UNRWA 'summer camps’. The 'summer camp’ shown in the West Bank was not affiliated with or organized by UNRWA. The only UNRWA summer activities actually depicted are those shot in Gaza. However, our investigation of the film has revealed that absolutely nothing anti-Semitic or inflammatory was done or said in the scenes filmed in Gaza.
In addition, those interviewed in the film are presented with captions that identify them as UNRWA staff members. However, only one of those interviewed is an UNRWA staff member. The comment she makes does not violate UNRWA’s neutrality policy.
UNRWA is committed to fostering human rights and tolerance, and teaches these values through the curriculum in its schools. UNRWA is one of the few organizations that has implemented human rights and conflict resolution training for millions of Palestine refugee children in the complex political environment of the Middle East for over 12 years.
UNRWA facilities being abused by Hamas militants.
In 2003, Israel released to newspapers what the "New York Times" called a "damning intelligence report". Citing interrogations of suspected militants, the document claims that UNRWA operations being used as cover for Palestinian terrorists, including smuggling arms in UN ambulances and hosting meetings of Tanzim in UN buildings. UN officials responded, according to the NY Times, by saying that it is Israel that has "lost its objectivity and begun regarding anyone who extends a hand to a Palestinian as an enemy."
The Israel Defense Forces released a video from May 2004, in which armed Palestinian militants carry an injured colleague into an UNRWA ambulance, before boarding with him. The ambulance driver requested that the armed men leave, but was threatened and told to drive to a hospital. UNRWA issued a plea to all parties to respect the neutrality of its ambulances.
On 1 October 2004, Israel again lodged accusations against UNRWA. The video documentation was not convincing, and the Israeli military changed some of its earlier statements and conceded the possibility that the object could have indeed been a stretcher, but did not offer the apology Hansen had demanded.
The Israeli Army circulates footage taken on 29 October 2007 showing three militants firing mortars from UNRWA boys’ school in Beit Hanoun, Gaza. The militants were able to enter due to the fact that the school was evacuated at the time because of the war.
According to former Israeli ambassador to the United Nations Dore Gold:
Although education was one of the fields in which UNRWA was supposed to provide aid, the agency did nothing to alter Palestinian educational texts that glorified violence and continuing war against Israel. This has been found to be false in several US State Department reports such as Brown and PICRI cited above.
On 4 February 2009, UNRWA halted aid shipments into the Gaza Strip after it accused Hamas of breaking into a UN warehouse and stealing tonnes of blankets and food which had been earmarked for needy families. A few days later, the UN resumed aid after the missing supplies had been returned.
On 5 August 2009, the IDF accused Hamas of stealing three ambulances that had just been transferred through Israel to the UNRWA. The UNRWA spokesman denied the claim. A week later, Hamas confirmed it confiscated the ambulances due to bureaucratic reasons. A UNRWA spokesman also confirmed this but soon retracted this admission and denied the incident, even publicizing a photo it claimed was of one its officials with the ambulances.
Al-Fakhura violence.
On 7 January 2009, UNRWA officials alleged that the prior day, in the course of the Gaza War, the Israel Defense Forces shelled the area outside a UNRWA school in Jabalya, Gaza, killing more than forty people. The IDF initially claimed it was responding to an attack by Hamas gunman hiding in the compound, but upon reexamination, said that an "errant shell had hit the school." Maxwell Gaylord, the UN humanitarian coordinator, stated that the UN "would like to clarify that the shelling, and all of the fatalities, took place outside rather than inside the school.
UNRWA has consistently rejected the allegation that militants used the Agency's installations during the Gaza war in 2008–2009. These accusations have been published by some media outlets, although they are sometimes retracted. In 2012 when on two occasions, Israel Channel Two TV, the most popular network in Israel apologised and issued a retraction of these allegations.
During the 2014 Israel–Gaza conflict, UNRWA spokesmen reported in July that weapons were found in three vacant UNRWA schools which had been closed for the summer. UNRWA strongly condemned the activity as a "flagrant violation of the inviolability of its premises under international law" and UNRWA staff were withdrawn from the premises. It appears, however, that UNRWA returned weapons to the local government – meaning Hamas.
In July 2014, three Israeli soldiers died from a booby-trap in a clinic. Initially it was reported to be an UNRWA clinic but the IDF shortly thereafter retracted the claim thought it was noted that it had an UNRWA sign on it. The UNRWA sign on the building therefore may have been stolen and placed there by someone, perhaps seeking to protect the building.
Even though the claim of the booby-trapped UNRWA clinic proved to be false, it has been repeated on several occasions by vocal UNRWA opponents, including at an official hearing of the U.S. House Committee on Foreign Affairs on September 9, 2014. During the hearing, “Hamas’ Benefactors: A Network of Terror,” Jonathan Schanzer from the Foundation for Defense of Democracies told the Committee that UNRWA was “allowing for the building of tunnels, these commando tunnels, underneath their facilities in my opinion very much needs to be investigated.” It is unclear whether Schanzer knew he was misleading the Committee, though he also repeated the assertion at an event hosted by the Foundation for Defense of Democracies entitled “The Israeli-Palestinian Conflict” on August 13, 2014, where he stated there was “at least one booby-trapped tunnel under one of its facilities.”
Investigations and calls for accountability and/or reform.
Many critics of UNRWA recognize that the refugees registered with UNRWA need massive help. They realize that the agency needs support—with more oversight and coordination with the Palestinian Authority. And they believe that disbanding UNRWA at this time or in the very near future is not a relevant goal.
Writing in the Middle East Monitor in April 2012, Karen Koning AbuZayd - a former Commissioner-General of the UNRWA (2005–2009) argued that "UNRWA needs support not brickbats". She concludes
... even those who scrutinise it most closely and challenge it most severely are those who also ensure that its programmes receive adequate funding. They, like others who view the agency more positively, realise that UNRWA makes a major contribution to stability in the Middle East.
Writing in the "Times of Israel" on 31 July 2014, David Horovits concludes in his analysis that although Israel has many complaints against UNRWA, it is not interested in abolishing it. Already at that point, less than four weeks into the 2014 war, there were 225,000 displaced persons within Gaza—and nobody to help them in a major way except UNRWA. And Horovits notes:
Israel takes into account that UNRWA, like all international organizations operating in Gaza, is being closely watched by Hamas for signs that it is not sufficiently critical of Israel.
Credible calls for accountability and/or reform tend to follow this line of reasoning, as exemplified below.
2004 Investigation by the United States Congress
The United States government financed a programme of "Operations Support Officers", part of whose job is to make random and unannounced inspections of UNRWA facilities to ensure their sanctity from militant operations. In 2004 the US Congress asked the General Accounting Office to investigate media claims that taxpayer's dollars given to UNRWA had been used to support individuals involved in militant activities. During its investigation, the GAO discovered several irregularities in its processing and employment history.
James G. Lindsay
On the basis of his 2009 analyses for WINEP, referred to in previous sections, former UNRWA general-counsel James G. Lindsay and fellow researcher for Washington Institute for Near East Policy made the following suggestions for improvement:
UNRWA should make the following operational changes: halt its one-sided political statements and limit itself to comments on humanitarian issues; take additional steps to ensure the agency is not employing or providing benefits to terrorists and criminals; and allow the UN Educational, Scientific and Cultural Organization (UNESCO), or some other neutral entity, to provide balanced and discrimination-free textbooks for UNRWA schools.
Andrew Whitley, director of the UNRWA representative office at UN headquarters in New York, said: "The agency is disappointed by the findings of the study, found it to be tendentious and partial, and regrets in particular the narrow range of sources used".
UNRWA's Jerusalem spokesman Chris Gunness stated that UNRWA rejects Lindsay's report and its findings and claimed that the study was inaccurate and misleading, since it "makes selective use of source material and fails to paint a truthful portrait of UNRWA and its operations today".
In response to the criticism of his report from UNRWA, Lindsay writes:
Despite repeated requests from the author, the agency declined to identify the alleged weaknesses on the grounds that “our views—and understanding—of UNRWA’s role, the refugees and even U.S. policy are too far apart for us to take time (time that we do not have) to enter into an exchange with little likelihood of influencing a narrative which so substantially differs from our own.” Thus, the paper has not benefited from any input by UNRWA, whether a discussion of policy or even correction of alleged errors.
Canadian redirection of funds from UNRWA to specific PA projects
In January 2010, the Government of Canada announced that it was redirecting aid previously earmarked to UNRWA "to specific projects in the Palestinian Authority that will ensure accountability and foster democracy in the PA." Victor Toews, the president of Canada's Treasury Board, stated, "Overall, Canada is not reducing the amount of money given to the PA, but it is now being redirected in accordance with Canadian values. This "will ensure accountability and foster democracy in the PA." Previously, Canada provided UNRWA with 11 percent of its budget at $10 million (Canadian) annually. The decision came despite positive internal evaluations of the Agency by CIDA officials. The Canadian decision put it very much at odds with the US and EU, which maintained or increased their levels of funding. Some suggested that the decision also cost Canada international support in its failed October 2010 effort to obtain a seat on the UN Security Council.
Documents obtained from the Canadian International Development Agency revealed that even the government of Israel opposed the Canadian move, and had asked Ottawa to resume contributions to UNRWA's General Fund.
UNRWA Reform Initiative
An initiative to reform UNRWA was announced by the Center for Near East Policy Research in March 2014.
The Center carries out research and (through its "Israel Resource News Agency") investigative journalism and research in cooperation with a wide variety of organisations and researchers, such as The Middle East Forum, which has published an entire issue of Middle East Quarterly discussing the challenges facing UNRWA.
The main thrust of the UNRWA Reform Initiative is to present documentation of problems with UNRWA to sponsor nations and organisations with the aim of increasing sponsor demands for accountability. UNRWA has stated on multiple occasions that the head of this initiative, David Bedein, fabricates the information he publishes.
2014 call for US investigation
In August 2014 US Senators demanded an impartial investigation into UNRWA's alleged participation in the 2014 Gaza-Israel conflict, accusing UNRWA of being complicit with Hamas.
Members of the United States Senate are demanding an independent investigation into the role of the United Nations Relief and Works Agency during Israel's most recent war in Gaza with Hamas.
... While the letter does not call on the State Department to cut aid, the senators write that the American taxpayers "deserve to know if UNRWA is fulfilling its mission or taking sides in this tragic conflict."
... Responding to the letter, a State Department spokesman said that the UN is taking "proactive steps to address this problem," including deploying munitions experts to the strip in search of more weapons caches.
"The international community cannot accept a situation where the United Nations– its facilities, staff and those it is protecting—are used as shields for militants and terrorist groups," State Department spokesman Edgar Vasquez told "The Jerusalem Post". "We remain in intensive consultations with UN leadership about the UN’s response."
"There are few good solutions given the exceptionally difficult situation in Gaza," Vasquez continued, "but nonetheless we are in contact with the United Nations, other UNRWA donors and concerned parties— including Israel—on identifying better options for protecting the neutrality of UN facilities and ensuring that weapons discovered are handled appropriately and do not find their way back to Hamas or other terrorist groups."
Relations with Israel.
After Israel captured the West Bank and Gaza in the June 1967 Six-Day War, Israel requested that the UNRWA continue its operations there, and agreed to facilitate them. Since then the relationship has been characterized as "an uneasy marriage of convenience between two unlikely bedfellows that has helped perpetuate the problem both have allegedly sought to resolve."
Immediately following the Six Day War, on June 14th UNRWA Commissioner-General Dr. Lawrence Michelmore and Political Advisor to the Israeli Foreign Minister Michael Comay exchanged letters that has since served as much of the basis for the relationship between Israel and UNRWA. Commonly referred to the Comay-Michelmore Exchange of Letters, the initial letter from Michelmore reiterates a verbal conversation between the two, stating that:
at the request of the Israel Government, UNRWA would continue its assistance to the Palestine refugees, with the full co-operation of the Israel authorities, in the West Bank and Gaza Strip areas. For its part, the Israel Government will facilitate the task of UNRWA to the best of its ability, subject only to regulations or arrangements which may be necessitated by considerations of military security. 
In his responding letter, Comay wrote:
I agree that your letter and this reply constitute a provisional agreement between UNRWA and the Government of Israel, to remain in force until replaced or cancelled.
UNRWA has been criticised by the Israeli government and politicians for alleged involvement with Palestinian militant groups, such as Hamas. Israel has stated that Peter Hansen, UNRWA's former Commissioner-General (1996–2005) "consistently adopted a trenchant anti-Israel line" which resulted in biased and exaggerated reports against Israel.
UNRWA has also lodged complaints, for example:
Al-Aqsa Intifada 2000– allegations of Israeli interference with UNRWA operations
During the Al-Aqsa Intifada, which started in late 2000, UNRWA often complained that Israeli road closures, curfews and checkpoints in the West Bank and Gaza have interfered with its ability to carry out its humanitarian mandate. The Agency has also complained that large scale house demolitions in the Gaza Strip have left over 30,000 people homeless. Israel justifies the demolitions as anti-terrorism measures.
November 2002 allegation that an Israeli sniper killed UNRWA employee
In November 2002 Iain Hook, a British employee of UNRWA, was shot and killed by an Israeli military sniper while working in the Jenin refugee camp, during an operation to locate a Palestinian militant suspected of masterminding a suicide bombing which had killed 14 people earlier in 2002. Peter Hansen, the head of UNRWA at the time criticized the killing: "Israeli snipers had sights. They would have known who the two internationals (non-Palestinians) were. They did not dress like Palestinians."
2014 Israel–Gaza conflict
During the 2014 Israel–Gaza conflict there have been many accusations by Israel, and many rebuttals by UNRWA. For example Israel's Channel 2 claimed in a report that an UNRWA ambulance was used to transport militants. It later retracted that claim, after being confronted with "incontrovertible evidence", in the words of UNRWA.
Israel damaged or destroyed a number of UNRWA facilities claiming that they were used for war purposes and thus legitimate targets. Pending investigations are expected to reveal the extent to which this was justified.
UNRWA schools and personnel were in the line of fire during the war - even when 290,000 people were staying in UNRWA schools being used as shelters.
During one of the many ceasefires in the war, UNRWA announced nine UNRWA staff members have been killed by Israeli raids on UNRWA schools.
However, three times Hamas weaponry was found in UNRWA schools, which in turn would legitimate them as military targets. UN Sec Gen Ban Ki Moon condemned the use of shelters as a weapons depot.

</doc>
<doc id="32293" url="http://en.wikipedia.org/wiki?curid=32293" title="United States Secretary of State">
United States Secretary of State

The Secretary of State is a senior official of the federal government of the United States of America heading the U.S. Department of State, principally concerned with foreign affairs and is considered to be the U.S. government's equivalent of a Minister for Foreign Affairs.
The Secretary of State, appointed by the President with the advice and consent of the Senate, is a member of the President's Cabinet, the National Security Council, and is the highest-ranking appointed executive branch official both in the presidential line of succession and the order of precedence.
The Secretary of State along with the Secretary of the Treasury, the Secretary of Defense, and the Attorney General are generally regarded as the four most important cabinet members because of the importance of their respective departments. Secretary of State is a Level I position in the Executive Schedule and thus earns the salary prescribed for that level.
The current Secretary of State is John Kerry, the 68th person to hold the office since its creation in 1789.
Duties and responsibilities.
The specific duties of the Secretary of State include:
The original duties of the Secretary of State include some domestic duties, such as:
Most of the domestic functions of the Department of State have been transferred to other agencies. Those that remain include storage and use of the Great Seal of the United States, performance of protocol functions for the White House, and the drafting of certain proclamations. The Secretary also negotiates with the individual States over the extradition of fugitives to foreign countries. Under Federal Law, the resignation of a President or of a Vice-President is only valid if declared in writing, in an instrument delivered to the office of the Secretary of State. Accordingly, the resignations of President Nixon and of Vice-President Spiro Agnew, domestic issues, were formalized in instruments delivered to the Secretary of State.
As the highest-ranking member of the cabinet, the Secretary of State is the third-highest official of the executive branch of the Federal Government of the United States, after the President and Vice President and is fourth in line to succeed the Presidency, coming after the Vice President, the Speaker of the House of Representatives, and the President pro tempore of the Senate. Six Secretaries of State have gone on to be elected President. Others, including Kerry, Hillary Clinton, William Seward, and Henry Clay have been unsuccessful presidential candidates, either before or after their term of office as Secretary of State.
What are the Qualifications of a Secretary of State? He ought to be a Man of universal Reading in Laws, Governments, History. Our whole terrestrial Universe ought to be summarily comprehended in his Mind.
”
 —John Adams
As the head of the United States Foreign Service, the Secretary of State is responsible for management of the diplomatic service of the United States. The foreign service employs about 12,000 people domestically and internationally, and supports 265 United States diplomatic missions around the world, including ambassadors to various nations.
The nature of the position means that Secretaries of State engage in travel around the world. The record for most countries visited in a secretary's tenure is 112, by Hillary Rodham Clinton. Second is Madeleine Albright with 96. The record for most air miles traveled in a secretary's tenure is 1,059,207, by Condoleezza Rice. Second is Clinton's 956,733 miles.
When there is a vacancy in the office of Secretary of State, the duties are exercised either by another member of the cabinet, or, in more recent times, by a high-ranking official of the State Department until the President appoints, and the United States Senate confirms, a new Secretary.
External links.
Listen to this article ()
This audio file was created from a revision of the "United States Secretary of State" article dated 2010-02-23, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="32306" url="http://en.wikipedia.org/wiki?curid=32306" title="UPN">
UPN

The United Paramount Network (UPN) was an American broadcast television network that launched on January 16, 1995. The network was originally owned by Chris-Craft Industries/United Television; Viacom (through its Paramount Television unit, which produced most of the network's series) turned the network into a joint venture in 1996 after acquiring a 50% stake in the network, and then purchased Chris-Craft's remaining stake in 2000; UPN was spun off to CBS Corporation in December 2005, when CBS and Viacom split up into two separate companies.
UPN shut down on September 15, 2006, with some of its programs moving three days later to The CW – a joint venture between CBS Corporation and Time Warner (majority owner of The WB, itself shutting down two days later).
History.
Origins (1949–1993).
Paramount Pictures had played a pivotal role in the development of network television. It was a partner in the DuMont Television Network, and the Paramount Theaters chain, which was spun off from the corporate/studio parent, merged with ABC in a deal that helped cement that network's status as a major network. The Paramount Television Network was launched in 1949, but dissolved in the 1950s.
In the wake of the successful Universal Studios "ad hoc" syndication package "Operation Prime Time", which first featured a miniseries adaptation of John Jakes' novel "The Bastard" and went on to air several more productions, Paramount had earlier contemplated its own television network with the Paramount Television Service. Set to launch in early 1978, it would have run its programming for only one night a week. Thirty "Movies of the Week" would have followed ' on Saturday nights. Plans for the new network were scrapped when sufficient advertising slots could not be sold, though Paramount would contribute some programs to "Operation Prime Time", such as the mini-series "A Woman Called Golda", and the weekly pop music program, "Solid Gold". "Star Trek: Phase II" was reworked as the theatrical film, ', absorbing the costs already incurred from the aborted television series.
Paramount, and its eventual parent Viacom (which bought the studio's then-parent, Paramount Communications, in 1994), continued to consider launching their own television network. Independent stations, even more than network affiliates, were feeling the growing pressure of audience erosion to cable television in the 1980s and 1990s; there were unaffiliated commercial television stations in most of the major television markets, even after the foundation of Fox in 1986. Meanwhile, Paramount, which had long been successful in syndication with repeats of ', launched several first-run syndicated series by the 1990s, including "Entertainment Tonight", "The Arsenio Hall Show", ', "War of the Worlds", ' and '.
In 1993, Time Warner and Chris-Craft Industries entered into a joint venture to distribute programs via a prime time programming service, the Prime Time Entertainment Network (PTEN). PTEN can be seen as the ancestor of both UPN and The WB: Chris-Craft later became a partner in UPN, and Time Warner launched The WB in a joint venture with the Tribune Company at roughly the same time.
Launch (1994–2000).
Paramount formed the Paramount Stations Group when it purchased the assets of the TVX Broadcast Group, which owned several independent stations in major markets, in 1991. This was not unlike the purchase of the Metromedia stations by News Corporation five years earlier, which were used as the nuclei for Fox. In another parallel, 20th Century Fox (the News Corporation subsidiary behind the Fox network, which was spun off with the company's other entertainment assets to 21st Century Fox in July 2013), like Paramount, had long been a powerhouse in television syndication. All indicators suggested that Paramount was about to launch a network of its own. In late 1994, Paramount announced the formation of the United Paramount Network. The new network would be owned by Chris-Craft Industries, while most of its shows were to be produced by Paramount Television. The "U" in UPN stood for Chris-Craft subsidiary United Television, which owned the network's two largest stations, WWOR-TV in New York City and KCOP-TV in Los Angeles; the "P" represented Paramount Television, the studio that formed a programming partnership with Chris-Craft to create the network. Chris-Craft and Paramount/Viacom each owned independent stations in several large and mid-sized U.S. cities, and these stations formed the nuclei of the new network.
UPN launched on January 16, 1995, initially carrying programming only on Monday and Tuesday nights from 8:00 to 10:00 p.m. Eastern and Pacific Time. The first telecast, the of "", was an auspiciously widely viewed start – being watched by 21.3 million viewers; however, "Voyager" would never achieve such viewership levels again, nor would any of the series debuting on UPN's second night of broadcasting survive the season. In contrast, The WB debuted one week earlier, on January 11, with four series – only one of which, "Muscle", would not survive its first season. The first comedy series to debut on UPN were "Platypus Man", starring Richard Jeni, and "Pig Sty", with both shows airing Monday nights in the 9:00 p.m. hour; both received mixed reviews, and neither lasted long.
Other early UPN programs included the action series "Nowhere Man", starring Bruce Greenwood and "Marker", starring Richard Grieco; the comic western "Legend" starring Richard Dean Anderson; the science-fiction themed action series, "The Sentinel"; and "Moesha", a sitcom starring Brandy Norwood. Of the network's early offerings, only "Star Trek: Voyager", "Moesha" and "The Sentinel" would last longer than one season. As a result of the lack of viewership, UPN operated on a loss and had lost $800 million by 2000.
Within nearly two years of the network's launch, on December 8, 1996, Paramount/Viacom purchased a 50% stake in UPN from Chris-Craft. Like Fox had done nine years earlier, UPN started with a few nights of programming each week, with additional nights of primetime shows gradually being added over the course of several seasons. Because of this, UPN's affiliates were basically independent stations for all intents and purposes during the network's early years, with these stations airing either syndicated programs or movies during primetime on nights when the network did not provide programming. The first expansion of its primetime lineup came with the addition of programming on Wednesday nights in the 1996–1997 season. Thursday and Friday nights were the last to be added to the network's primetime slate, beginning with the 1998–1999 season.
Viacom era and decline (2000–2006).
In March 2000, Viacom exercised a contractual clause that would force Chris-Craft to either buy Viacom out of UPN, or have the former sell its ownership stake in the network to Viacom. Chris-Craft was unable to find a suitable partner and allowed Viacom to buy out its 50% stake, giving Viacom full control of the network. This gave UPN the rare distinction of being one of the only broadcast networks to not have had owned-and-operated stations (O&O) in the three largest media markets, New York City, Los Angeles and Chicago (with The WB – the only network never to have had an O&O – being the only other, as minority owner Tribune Broadcasting owned most of its charter affiliates including those in all three markets, while majority owner Time Warner only owned an independent station that originated then-superstation TBS). With Viacom taking full ownership control of UPN, KCOP-TV and WWOR-TV lost their statuses as O&Os and automatically became affiliates of the network. In addition, neither Chris-Craft or Viacom had ever held ownership of Chicago affiliate WPWR-TV, which had been the largest UPN station that was not owned-and-operated by the network prior to the Viacom buyout. As a result of Viacom assuming Chris-Craft's interest, the network's largest owned-and-operated station became Philadelphia outlet WPSG.
Shortly afterward, Viacom shortened the network's official name from the "United Paramount Network" to the three-letter initialism, "UPN". Viacom also proposed a rebranding of UPN into the "Paramount Network", using a prototype logo based on Paramount Pictures' mountain logo, which served as the basis for the "P" triangle in the network's original logo that was used until September 2002. This idea was abandoned after many affiliates protested, citing that the rebranding might confuse viewers and result in ratings declines.
A few months before, Viacom bought CBS (merging that network's owned-and-operated stations into Viacom's Paramount Stations Group unit), creating duopolies between CBS and UPN stations in Philadelphia (KYW-TV and WPSG), Boston (WBZ-TV and WSBK-TV), Miami (WFOR-TV and WBFS-TV), Dallas–Fort Worth (KTVT and KTXA), Detroit (WWJ-TV and WKBD-TV) and Pittsburgh (KDKA-TV and WNPA). Viacom's purchase of CBS was said to be the "death knell" for the Federal Communications Commission's longtime ban on television station duopolies. Further transactions added San Francisco (KPIX-TV and KBHK, the latter of which was traded to Viacom/CBS by Fox Television Stations) and Sacramento (KOVR and KMAX-TV, the former of which was sold to Viacom/CBS by the Sinclair Broadcast Group) to the mix.
At the time of UPN's launch, the network's flagship station was Chris-Craft-owned WWOR-TV in Secaucus, New Jersey (which serves the New York City market). Even after Chris-Craft sold its share in the network to Viacom, WWOR was still commonly regarded as the flagship of the network since it had long been common practice for this status to be associated with a network's New York station. For this reason, some doubt was cast on UPN's future after Fox Television Stations bought most of Chris-Craft's television stations on August 12, 2000, which included several UPN affiliates (including WWOR and West Coast flagship KCOP). Fox later bought the third-largest UPN affiliate, Chicago's WPWR-TV, through a separate deal with Newsweb Corporation.
In 2001, UPN entered into a public bidding war to acquire two series from The WB, "Buffy the Vampire Slayer" and "Roswell", from producing studio 20th Century Fox Television. UPN eventually outbid The WB for the shows and aired them together on Tuesday nights until "Roswell" ended its run in 2002, "Buffy" ended its run the following year. New shows began to breathe life into the network starting in the fall of 2003 with "America's Next Top Model", followed up by the fall 2004 premieres of the sitcom "All of Us" (which was produced by Will and Jada Pinkett Smith) and the mystery series "Veronica Mars", and then by the Chris Rock-produced and narrated sitcom "Everybody Hates Chris" in 2005.
On June 14, 2005, Viacom announced that it would be split into two companies due to declining performance of the company's stock; both the original Viacom – which was renamed CBS Corporation – and a new company that took the Viacom name would be controlled by the original Viacom's parent National Amusements (controlled by Sumner Redstone). UPN became part of CBS Corporation, while the new Viacom kept Paramount Pictures among other holdings each company acquired in the deal.
Network closure.
On January 24, 2006, UPN parent CBS Corporation and Time Warner, the majority owner of The WB, announced that they would shut down the two respective networks and launch a new broadcast network that would be operated as a joint venture between both companies, The CW, which incorporated UPN and The WB's higher-rated programs with newer series produced exclusively for The CW. The new network immediately signed 10-year affiliation agreements with 16 stations affiliated with The WB (out of 19 stations that were affiliated with the network) that were owned by that network's part-owner, the Tribune Company – including stations in the coveted markets of New York City, Los Angeles and Chicago – and 11 UPN stations that were owned by CBS Corporation. Fox Television Stations' nine UPN affiliates were passed over for affiliations as a result, and the company responded two days later by removing all UPN branding from those stations and ceasing promotion of the network's programs. One month later on February 22, Fox announced the formation of MyNetworkTV, a new network that would also debut in September 2006 that would use the company's soon-to-be former UPN affiliates as the nuclei. Over the next nine months, determinations were made as to which shows from the two networks would cross over to The CW, as well as which of UPN and The WB's affiliate stations would be selected to become affiliates of the new network. Programming-wise, six UPN shows – "America's Next Top Model" (which is currently the last surviving series from UPN that remains on The CW's schedule as of 2014), "Veronica Mars", "Everybody Hates Chris", "Girlfriends", "All of Us" and "WWE Smackdown" – were chosen to move to The CW for its inaugural 2006–07 fall schedule.
UPN quietly went off the air on September 15, 2006 – three days prior to the launch of The CW; "WWE SmackDown" was the last official program broadcast on UPN, ending the network's existence after 11 years (although some affiliates aired the network's optional weekend encore block). However, the Fox-owned UPN stations disaffiliated from the network on August 31; as a result, UPN's last two weeks of programming did not air in ten markets where Fox owned a UPN affiliate that was set to become an owned-and-operated station of MyNetworkTV, when that network launched on September 5. "WWE SmackDown", however, aired in those markets on Tribune's WB stations, including those that would join The CW shortly afterward. With the exception of "SmackDown", all of the programs that aired during the network's final three months were reruns. After the network's official closure, UPN's website was redirected to The CW website, and then to CBS's website.
Programming.
At the time of its shutdown, UPN ran only two hours of primetime network programming on Monday through Fridays (compared to the three primetime hours on Monday through Saturdays and four hours on Sundays offered by the Big Three networks, ABC, NBC and CBS). UPN never carried any weekend primetime programming throughout the network's run (though it did offer children's programming on weekend mornings until 2003, and a movie package to its affiliates on weekend afternoons until 2000, when the latter was replaced with a two-hour repeat block of UPN programs); as a result, affiliates held the responsibility of programming their Saturday and Sunday evening schedules with syndicated programs, sports, movies or network programs that were preempted from earlier in the week due to special programming, in the 8:00–10:00 p.m. (Eastern and Pacific Time) time period. This primetime scheduling allowed for many of the network's affiliates to air local newscasts during the 10:00–11:00 p.m. (Eastern and Pacific Time) time period.
Most of UPN's programming through the years was produced by Paramount Television or a Viacom-owned sister company (Viacom Productions, Big Ticket Entertainment, Spelling Television or CBS Productions). UPN's first official program was "Star Trek: Voyager", with the first comedy shows to debut being two short-lived series: Richard Jeni starring vehicle "Platypus Man", and "Pig Sty".
Other notable UPN programs during the network's existence included "The Sentinel", "Moesha", "", "WWE SmackDown", "America's Next Top Model", "Girlfriends", the "Moesha" spin-off "The Parkers", "Veronica Mars", "Everybody Hates Chris", and "Dilbert". In the summer of 2005, UPN aired "R U the Girl", in which R&B group TLC searched for a woman to join them on a new song. The network also produced some special programs, including 2001's "Iron Chef USA". Much of UPN's comedy programming between 1996 and 2006 (particularly those that aired as part of the network's Monday evening lineup) was largely aimed at African American audiences, with minor exceptions in shows such as "Clueless", "DiResta" and "Head Over Heels".
UPN occasionally acquired series cancelled by the other broadcast networks, including former WB series "Buffy the Vampire Slayer" and "Roswell" (both of which moved to UPN in 2001, "Buffy" was picked up after The WB chose not to renew it due to skyrocketing license fees while "Roswell" joined UPN after that same network also cancelled the series), and former ABC series "Clueless" and "The Hughleys". The first program that UPN acquired from another network was "In the House", which moved to the network from NBC (which cancelled the LL Cool J sitcom after its second season) in 1996. In its later years, as part of the network's desire to maintain its own identity with its own unique shows, UPN instituted a policy of "not picking up other networks' scraps", which was a strong argument when fan pressure was generated in 2004 for them to pick up "Angel", the spin-off of "Buffy the Vampire Slayer" which had been dropped from The WB.
UPN aired only one regular sports event program: the much-hyped XFL in 2001, as part of a package from co-creator and WWE founder Vince McMahon, which also included what was then "WWF SmackDown!". UPN had planned to air a second season of the XFL in 2002, but it also demanded that "SmackDown!" be reduced by 30 minutes; McMahon did not agree to the change and the football league folded not long afterward.
News programming.
Like The WB, UPN never aired national morning or evening newscasts; however, several of its affiliates and owned-and-operated stations did produce their own local news programs. Several UPN affiliates ran a local newscast in the 10:00–11:00 p.m. Eastern and Pacific (9:00–10:00 p.m. Central and Mountain Time) timeslot at some point during or throughout their affiliations with the network; there were also a few stations that produced a weekday morning newscast, although early evening newscasts were largely absent on most of these stations. The UPN affiliate body had fewer news-producing stations in comparison to stations aligned with the Big Three television networks (NBC, ABC and CBS) and considerably fewer than Fox and especially The WB. In several markets, the local UPN affiliate either outsourced news programming to an NBC, ABC or CBS station in the market (either due to insufficient funds or studio space for production of their own newscasts, or in later years after the FCC permitted duopolies in markets with at least eight unique station owners in 2000, the station being operated through a legal duopoly or management agreement with a major network affiliate); other affiliates opted to carry syndicated programming in the hour following UPN's primetime programming lineup.
When the network launched in January 1995, UPN automatically gained six affiliates with functioning news departments through Chris-Craft/United Television and Viacom's respective affiliation deals with the network, all of those stations started their news operations as either independent stations or during prior affiliations with other networks: WWOR-TV/Secaucus, New Jersey (New York City), KCOP-TV/Los Angeles, WKBD-TV/Detroit, KPTV/Portland, Oregon, KMSP-TV/Minneapolis and WTOG/Tampa, Florida. Two more stations would join them later on: KSTW/Seattle, also owned by Viacom at the time, after it affiliated with UPN in 1997 through the reversal of a 1995 affiliation switch with CBS affiliate KIRO-TV (which also kept its news department as a UPN affiliate), and KMAX-TV/Sacramento, which joined UPN after being acquired by Viacom in 1998 and began producing newscasts shortly after its 1995 affiliation with The WB. KSTW and WTOG's news departments were shut down in 1998 due to cost-cutting measures mandated by Viacom.
Not all of UPN's news-producing stations were owned by the two companies that formed the nuclei of the network's affiliate group; WUAB/Cleveland, which started its news department in 1988, also continued its 10:00 p.m. newscast as a UPN affiliate (it would begin producing newscasts for sister station WOIO-TV in February 1995, after that station became a CBS affiliate; though WOIO eventually took over production of the newscast by 2002). Harrisburg affiliate WLYH-TV briefly continued its newscasts after switching to UPN from CBS in 1995, until WHP-TV began operating the station under a local marketing agreement that fall. WFTC/Minneapolis continued to produce a late evening newscast after Fox Television Stations (which acquired KMSP-TV through the Chris-Craft purchase, and converted it into a Fox O&O) acquired the station from Clear Channel Communications and switched the station to UPN – it was moved to 10:00 p.m. to avoid competing with KMSP's 9:00 p.m. newscast until the WFTC newscast was cancelled in 2006.
Outside of KPTV and KMSP, which are both now Fox stations, none of the former UPN affiliates that produced newscasts during their affiliation with the network continue to maintain an independent news department (despite license requirements imposed by the station's 1983 transfer of its license to Secaucus, New Jersey from New York City to cover New Jersey issues, WWOR-TV – which continued to produce news programming after coming under common ownership with Fox O&O WNYW – shut down its news department in July 2013 and replaced its lone 10:00 p.m. newscast with an outside produced program called "Chasing New Jersey", a move that resulted in calls by state politicians for the FCC to revoke Fox's license to operate the station; KTTV took over production of sister station KCOP's newscasts in 2007, before discontinuing news programming on that station in 2013; KMAX's news department has since been merged with that of KOVR although it still produces a morning newscast separate from that station; and WKBD shut down its news department in 2003, with its 10:00 p.m. newscast being replaced by a short-lived program produced by ABC affiliate WXYZ-TV).
Children's programming.
When the network launched in January 1995, UPN debuted a weekend morning cartoon block called "UPN Kids" (later called "The UPN Kids Action Zone" during the 1998–99 season). In 1997, UPN added two teen-oriented series to the lineup with reruns of the syndicated "Sweet Valley High" (based on the young adult book series by Francine Pascal) and a new series, "Breaker High" (which co-starred a then-unknown Ryan Gosling); both shows filled the weekday morning block for the 1997–98 season, while they were also included alongside the animated series on Sunday mornings. Unlike other networks, UPN gave its affiliates the option of running its weekend children's program block on either Saturdays or Sundays. In January 1998, the network entered into a deal with Saban Entertainment to program the Sunday morning block (with shows such as "The Incredible Hulk", "X-Men" and "Spider-Man" joining the lineup).
In 1999, UPN contracted the rights to the network's children's programming lineup to The Walt Disney Company; the teen-oriented and animated series were replaced with a new block called "Disney's One Too", which debuted on September 6, 1999 and featured select programs seen on ABC's "Disney's One Saturday Morning" lineup (such as "Recess" and "Sabrina, the Animated Series"). Many UPN affiliates at the network's launch were already airing The Disney Afternoon, a block supplied by Disney-owned syndication distributor Buena Vista Television; however, that block would be discontinued in August 1997. The addition of "Disney's One Too" expanded UPN's children's program block back to two hours, running on Sunday mornings and weekday afternoons. In 2002, "" moved to UPN from Fox Kids, due to Disney's acquisition of Fox's children's program inventory as well as the Fox Family Channel, which was renamed ABC Family the previous year.
By 2003, the "One Too" branding was dropped from the block due to the rebranding of ABC's Saturday morning lineup from "One Saturday Morning" to "ABC Kids" (though the block was unofficially referred to as "Disney's Animation Weekdays" outside of the network). UPN chose not to renew its contract with Disney, with the network dropping all children's programming on August 29, 2003, having no children's programming from Nickelodeon after it ended. (despite being owned by Viacom during that time), This left UPN as one of only two major broadcast networks that did not air a children's programming block (the other being Pax TV, which discontinued its "Pax Kids" lineup in 2000, before reviving children's programming as Ion Television through the 2007 launch of Qubo). Incidentally, UPN's successor The CW carried over the Kids' WB Saturday morning lineup from fellow successor The WB, resulting in UPN affiliates that joined The CW in September 2006 carrying network-supplied children's programming for the first time since the "One Too" block ended.
Some Fox stations that declined to carry 4Kids TV passed on that block to an affiliate of UPN or The WB, or an independent station, in order for the Fox affiliate to air general entertainment programming or local newscasts on Saturday mornings (for example, WFLD in Chicago moved the 4Kids TV schedule to co-owned then-UPN affiliate WPWR-TV, while WFLD aired news, and children's programming that fulfilled the Federal Communications Commission's E/I obligations for broadcast stations in place of the 4Kids lineup). In other cases, some UPN stations aired their own blocks of syndicated children's programs designed to meet the minimal three hours of E/I programming required by the FCC.
Television movies.
During the late 1990s, UPN produced a number of television movies under the umbrella brand "Blockbuster Shockwave Cinema", in conjunction with sponsor (and then-sister company) Blockbuster Video, almost all of which were science fiction films.
From UPN's inception until 2000, the network also offered a hosted movie series called the "UPN Movie Trailer" to its stations. The weekend block featured mostly older theatrically released action and comedy films, often those from the Paramount Pictures film library. The "Movie Trailer" block was discontinued in 2000 to give stations that opted for them room for a two-hour block of select UPN series that aired in primetime during the previous week. There were also three Paramount-branded blocks that aired on Viacom's UPN owned-and-operated stations between 1995 and 2000: the "Paramount Teleplex" as the main brand for movies at any given timeslot, the "Paramount Prime Movie" for primetime features, and the "Paramount Late Movie" for films airing in late night timeslots.
Affiliates.
Although it was considered a major network by Nielsen for ratings purposes, UPN was not available in every American television market. In some areas, UPN programming was shown off-pattern by affiliates of other networks (airing immediately after programming from their primary network on some Fox and WB stations, or during overnight timeslots on major network affiliates) or by otherwise independent stations, such as in the case of KIKU-TV in Honolulu, Hawaii. Some affiliates were also known to extensively preempt network programming in order to broadcast local sporting events.
By 2003, UPN had an estimated audience reach of 85.98% of all U.S. households (equivalent to 91,689,290 households with at least one television set). In contrast, The WB was viewable in 91.66% of all U.S. television homes. This is mainly because The WB operated The WB 100+ Station Group, a group of cable-only stations in areas below the top 100 Nielsen-designated media markets (prior to the September 1998 launch of The WB 100+, The WB's programming was carried over the superstation feed of the network's Chicago affiliate WGN-TV), while UPN did not have such a service (the network reportedly denied Advance Entertainment Corporation permission to distribute the network's programming over the superstation feed of New York City O&O WWOR-TV). UPN did have one cable-only affiliate in its station body, WNFM-TV in Fort Myers, Florida, which joined the network in 1998. UPN had approximately 143 full-power owned-and-operated or primary affiliate stations in the U.S. and another 65 stations aired some UPN programming as secondary affiliates.
In markets where Viacom had a CBS/UPN duopoly after its 2000 merger with CBS, the UPN station was used to air CBS network programs in the event that local sporting events or extended breaking news coverage would air on the CBS station, sometimes resulting in UPN programs being pre-empted outright, as the CBS-owned outlets were usually the senior partner in the duopolies (the only exception being Detroit, where WKBD-TV is considered the senior partner to WWJ-TV due to WKBD being longer-established). One such event occurred on September 26, 2004, when Hurricane Jeanne forced a scheduled NFL game between the Pittsburgh Steelers and Miami Dolphins in Miami to be postponed from its scheduled start time of 1:00 p.m. to 8:30 p.m. Eastern Time; the game aired locally on KDKA-TV and WFOR-TV while their respective UPN sister stations, WNPA-TV and WBFS-TV, aired CBS's regular Sunday night programming instead.
These factors led to the network struggling in the ratings over much of UPN's existence, with its later "Star Trek" franchise, "", perhaps suffering the most and ultimately being cancelled by the network in a controversial decision in February 2005. The most consistent ratings performer for the network was "WWE SmackDown". During the 2004-2005 season, the network was getting consistently better ratings than The WB, much of this thanks to its carriage of the WWE.
Station standardization.
When the network launched, UPN began having most of its stations branded using a combination of "UPN" or "Paramount" (the latter having been used only by the network's Viacom-owned stations, some of whom adopted the "Paramount" branding prior to UPN's launch), and the affiliated station's channel number. By the late 1990s, affiliates were simply branded under the "UPN (channel number or city)" scheme (for example, Chicago affiliate WPWR-TV called itself "UPN Chicago" and New York City O&O-turned-affiliate WWOR-TV was referred to as "UPN 9", until The CW's launch was announced in January 2006).
However, most of the UPN owned-and-operated stations under Viacom/CBS Corporation branded themselves by the network/city conventions (for example, KBHK/San Francisco was branded as "UPN Bay Area," WKBD/Detroit was branded as "UPN Detroit" and WUPL/New Orleans was branded as "UPN New Orleans"). That type of branding did not always apply though, as for example, WSBK-TV/Boston was branded "UPN 38" and KMAX-TV/Sacramento was branded "UPN 31". WNPA/Pittsburgh originally branded itself as "UPN 19", but rebranded itself as "UPN Pittsburgh" soon after the network introduced its second and final logo in September 2002, making it one of the few that had carried both standardization styles. Many UPN-affiliated stations followed the same branding scheme (for example, KFVE/Honolulu used the brand "UPN Hawaii").
This would be a continuation of the trend of networks using such naming schemes, which originated at Fox (and even earlier by CBC Television in Canada), and was also predominately used at CBS (which has most of its owned-and-operated stations, with a few exceptions, brand using a combination of the network's name and over-the-air channel number) and The WB (with the exception of its Tribune Broadcasting-owned affiliates in Los Angeles and Chicago, and certain other stations); NBC and ABC also use similar branding schemes, but not to the same universal level outside of their O&Os. While the "Big Three" networks do not require their affiliates to have such naming schemes (though some affiliates choose to adopt it anyway) and only on the network's owned-and-operated stations is the style required, UPN mandated it on all stations – though in one case, Milwaukee affiliate WCGV branded as "Channel 24" from 1998 to 2001, excluding UPN imagery from its station branding (WCGV, which previously branded as "UPN 24", had disaffiliated from the network for eight months in 1998 due to a compensation dispute).
Humor references.
The cast of ABC's hit Thursday night comedy show Whose Line Is It Anyway? frequently mocked UPN in their jokes.

</doc>
<doc id="32313" url="http://en.wikipedia.org/wiki?curid=32313" title="Unobtainium">
Unobtainium

In fiction, engineering, and thought experiments, unobtainium is any fictional, extremely rare, costly, or impossible material, or (less commonly) device needed to fulfill a given design for a given application. The properties of any particular unobtainium depend on the intended use. For example, a pulley made of unobtainium might be massless and frictionless; however, if used in a nuclear rocket, unobtainium would be light, strong at high temperatures, and resistant to radiation damage. The concept of unobtainium is often applied flippantly or humorously.
The word "unobtainium" is derived from "unobtainable" + "-ium" (the suffix for a number of elements). It pre-dates the similar-sounding IUPAC systematic element names, such as Ununoctium. An alternative spelling, unobtanium is sometimes used (for example, for the crypto-currency Unobtanium), based on the spelling of metals such as titanium.
Engineering origin.
Since the late 1950s, aerospace engineers have used the term "unobtainium" when referring to unusual or costly materials, or when theoretically considering a material perfect for their needs in all respects, except that it does not exist. By the 1990s, the term was in wide use, even in formal engineering papers such as "Towards unobtainium [new composite materials for space applications]."
The word "unobtainium" may well have been coined in the aerospace industry to refer to materials capable of withstanding the extreme temperatures expected in re-entry. Aerospace engineers are frequently tempted to design aircraft which require parts with strength or resilience beyond that of currently available materials.
Later, "unobtainium" became an engineering term for practical materials that really exist, but are difficult to get. For example, during the development of the SR-71 Blackbird spy plane, Lockheed engineers at the "Skunk Works" under Clarence "Kelly" Johnson used "unobtainium" as a dysphemism for "titanium." Titanium allowed a higher strength-to-weight ratio at the high temperatures the Blackbird would reach, but the Soviet Union controlled its supply and was trying to deprive the US armed forces of this valuable resource.
In the 1970s, bicycle magazines, such as "Bike World," sometimes referred to exotic lightweight bicycle parts as being made of unobtainium, which, while expensive, were commercially obtainable. In the same period, driver and engineer Mark Donohue claimed unobtainium was used in the construction of Penske race cars.
Contemporary popularization.
By 2010, the term had diffused beyond engineering, and now can appear in the headlines of mainstream newspapers, especially to describe the commercially useful rare earth elements (particularly terbium, erbium, dysprosium, yttrium, and neodymium). These are essential to the performance of consumer electronics and green technology, but the projected demand for them so outstrips their current supply that they are called "unobtainiums" within the ore industry, and by commentators on the US Congressional hearings into the "supply security" of rare-earths.
"Unobtainium" has come to be used as a synonym for "unobtainable" among people who are neither science fiction fans nor engineers to denote an object that actually exists, but which is very hard to obtain either because of high price (sometimes referred to as "unaffordium") or limited availability. It usually refers to a very high-end and desirable product; for instance, in the mountain biking community, "These titanium hubs are unobtainium, man!" Old-car enthusiasts use "unobtainium" to describe parts that are vanishingly rare or no longer available.
In maintaining old equipment, "unobtainium" refers to replacement parts that are no longer made, such as parts for reel-to-reel audio-tape recorders, or rare vacuum tubes that cost more than the equipment they are fitted to (especially true of certain vacuum tubes, such as the 1L6, used almost exclusively in American battery-powered shortwave radios or the WD-11 used in certain early 1920s radios). Similarly, parts for classic & vintage Ferraris are made by a company actually named Unobtainium Supply Co.
There have been repeated attempts to attribute the name to a real material. Because of the long-standing usage of the term "unobtainium" within the space elevator research community to describe a material with the necessary characteristics, LiftPort Group President Michael Laine has advocated assigning the term as the generic name for cables woven of carbon nanotube fibers, which seem to satisfy the requirements for this application. Since he claimed that sufficiently long nanotube cables will be prohibitively expensive to develop without inexpensive access to microgravity, these cables would still be close enough to unobtainable to meet the definition. However, this usage does not seem to have become widespread. The eyewear and fashion wear company Oakley, Inc. also frequently denotes the material used for many of their eyeglass nosepieces and earpieces, which has the unusual property of increasing tackiness and thus grip when wet, as unobtanium.
Frequent Sunday night/Monday morning host of "Coast to Coast AM" George Knapp usually opens his show mentioning unobtainium. As a play on the word, "Obtainium" is an album by Skeleton Key, released in 2002 by Ipecac Recordings.
Science fiction.
"Unobtainium" can refer to any substance needed to build some device critical to the plot of a science fiction story, but which does not exist in the universe as we know it. A hull material that gets stronger with pressure in the film "The Core" was nicknamed "unobtainium", but the concept under different names can be seen in the anti-gravity material cavorite from H. G. Wells' 1901 novel "The First Men in the Moon", as well as the super-strong material scrith from Larry Niven's novel "Ringworld", which requires a tensile strength on the order of the forces binding an atomic nucleus.
More recently, the term was used in James Cameron's 2009 movie "Avatar", as a substance that was named (in the film's dialog) "unobtanium" (note the slightly different spelling). In the film, it was mined on the fictional moon Pandora, and was described as key to human space exploration and survival.
"Unobtainium" can also refer to any rare but desirable material used to motivate a conflict over its possession, making it a MacGuffin (it appears in the story as something to obtain, not something that is significantly used).
"Unobtainium" can be used in a disparaging context (e.g., "That idea is silly; you'd need unobtainium wires to hold the planet up!") or a hypothetical one ("If one were to build an unobtainium shell around a black hole's event horizon, what would happen to the material piling up on it?").
In real life.
Element 66 is named dysprosium, from the Greek word dysprositos meaning hard to get.
A cryptocurrency is named "Unobtanium", which uses Bitcoin's source code with some modifications to the monetary policy.
Similar terms.
The term handwavium (suggesting handwaving) is another term for this hypothetical material, as are buzzwordium, impossibrium, hardtofindium, flangium, and, less commonly, phlebotinum.
The term eludium (also spelled with variants such as illudium) has been used to describe a material which has "eluded" attempts to develop it. This was mentioned in several Looney Tunes cartoons, where Marvin Martian tried (unsuccessfully) to use his "Eludium Q-36 Explosive Space Modulator" to blow up the Earth.
Another largely synonymous term is wishalloy, although the sense is often subtly different in that a wishalloy usually does not exist at all, whereas unobtainium may merely be unavailable.
A similar conceptual material in alchemy is the philosopher's stone, a mythical substance with the ability to turn lead into gold, or bestow immortality and youth. While the search to find such a substance was not successful, it did lead to discovery of a new substance: phosphorus.

</doc>
<doc id="32321" url="http://en.wikipedia.org/wiki?curid=32321" title="Universal Networking Language">
Universal Networking Language

Universal Networking Language (UNL) is a declarative formal language specifically designed to represent semantic data extracted from natural language texts. It can be used as a pivot language in interlingual machine translation systems or as a knowledge representation language in information retrieval applications. 
Scope and goals.
UNL is designed to establish a simple foundation for representing the most central aspects of information and meaning in a machine- and human-language-independent form. As a language-independent formalism, UNL aims to code, store, disseminate and retrieve information independently of the original language in which it was expressed. In this sense, UNL seeks to provide tools for overcoming the language barrier in a systematic way.
At first glance, UNL seems to be a kind of interlingua, into which source texts are converted before being translated into target languages. It can, in fact, be used for this purpose, and very efficiently, too. However, its real strength is knowledge representation and its primary objective is to provide an infrastructure for handling knowledge that already exists or can exist in any given language.
Nevertheless, it is important to note that at present it would be foolish to claim to represent the “full” meaning of any word, sentence, or text for any language. Subtleties of intention and interpretation make the “full meaning,” however we might conceive it, too variable and subjective for any systematic treatment. Thus UNL avoids the pitfalls of trying to represent the “full meaning” of sentences or texts, targeting instead the “core” or “consensual” meaning most often attributed to them. In this sense, much of the subtlety of poetry, metaphor, figurative language, innuendo, and other complex, indirect communicative behaviors is beyond the current scope and goals of UNL. Instead, UNL targets direct communicative behavior and literal meaning as a tangible, concrete basis for most human communication in practical, day-to-day settings.
Structure.
In the UNL approach, information conveyed by natural language is represented sentence by sentence as a hypergraph composed of a set of directed binary labeled links (referred to as relations) between nodes or hypernodes (the Universal Words, or simply UWs), which stand for concepts. UWs can also be annotated with attributes representing context information. 
As an example, the English sentence ‘The sky was blue?!’ can be represented in UNL as follows:
In the example above, "sky(icl>natural world)" and "blue(icl>color)", which represent individual concepts, are UWs; "aoj" (= attribute of an object) is a directed binary semantic relation linking the two UWs; and "@def", "@interrogative", "@past", "@exclamation" and "@entry" are attributes modifying UWs.
UWs are intended to represent universal concepts, but are expressed in English words or in any other natural language in order to be humanly readable. They consist of a "headword" (the UW root) and a "constraint list" (the UW suffix between parentheses), where the constraints are used to disambiguate the general concept conveyed by the headword. The set of UWs is organized in the UNL Ontology, in which high-level concepts are related to lower-level ones through the relations "icl" (= is a kind of), "iof" (= is an instance of) and "equ" (= is equal to). 
Relations are intended to represent semantic links between words in every existing language. They can be ontological (such as "icl" and "iof," referred to above), logical (such as "and" and "or"), and thematic (such as "agt" = agent, "ins" = instrument, "tim" = time, "plc" = place, etc.). There are currently 46 relations in the UNL Specs. They jointly define the UNL syntax.
Attributes represent information that cannot be conveyed by UWs and relations. Normally, they represent information concerning time ("@past", "@future", etc.), reference ("@def", "@indef", etc.), modality ("@can", "@must", etc.), focus ("@topic", "@focus", etc.), and so on.
Within the UNL Program, the process of representing natural language sentences in UNL graphs is called UNLization, and the process of generating natural language sentences out of UNL graphs is called NLization. UNLization, which involves natural language analysis and understanding, is intended to be carried out semi-automatically (i.e., by humans with computer aids); and NLization is intended to be carried out fully automatically.
History.
The UNL Programme started in 1996, as an initiative of the of the in Tokyo, Japan. In January 2001, the United Nations University set up an autonomous organization, the , to be responsible for the development and management of the UNL Programme. The Foundation, a non-profit international organisation, has an independent identity from the United Nations University, although it has special links with the UN. It inherited from the UNU/IAS the mandate of implementing the UNL Programme so that it can fulfil its mission.
The Programme has already crossed important milestones. The overall architecture of the UNL System has been developed with a set of basic software and tools necessary for its functioning. These are being tested and improved. A vast amount of linguistic resources from the various native languages already under development, as well as from the UNL expression, has been accumulated in the last few years. Moreover, the technical infrastructure for expanding these resources is already in place, thus facilitating the participation of many more languages in the UNL system from now on. A growing number of scientific papers and academic dissertations on the UNL are being published every year. 
The most visible accomplishment so far is the recognition by the Patent Co-operation Treaty (PCT) of the innovative character and industrial applicability of the UNL, which was obtained in May 2002 through the World Intellectual Property Organisation (WIPO). Acquiring the patents (US patents 6,704,700 and 7,107,206) for the UNL is a completely novel achievement within the United Nations.

</doc>
<doc id="32323" url="http://en.wikipedia.org/wiki?curid=32323" title="Urea breath test">
Urea breath test

The urea breath test is a rapid diagnostic procedure used to identify infections by "Helicobacter pylori", a spiral bacterium implicated in gastritis, gastric ulcer, and peptic ulcer disease. It is based upon the ability of "H. pylori" to convert urea to ammonia and carbon dioxide. Urea breath tests are recommended in leading society guidelines as a preferred non-invasive choice for detecting "H. pylori" before and after treatment.
Principles and mechanism.
Patients swallow urea labelled with an uncommon isotope, either radioactive carbon-14 or non-radioactive carbon-13. In the subsequent 10–30 minutes, the detection of isotope-labelled carbon dioxide in exhaled breath indicates that the urea was split; this indicates that urease (the enzyme that "H. pylori" uses to metabolize urea) is present in the stomach, and hence that "H. pylori" bacteria are present.
For the two different forms of urea, different instrumentation is required. Carbon-14 is normally measured by scintillation; whereas, carbon-13 can be detected by isotope ratio mass spectrometry or by mass correlation spectrometry. For each of these methods, a baseline breath sample is required before taking the isotope-labeled urea, for comparison with the post-urea sample, with a 20 to 30-minute duration between them. Samples may be sent to a reference laboratory for analysis. Alternatively, mass correlation spectrometry can be performed as an office-based test since breath samples are continuously collected, and results are provided immediately within minutes.
The difference between the pre and post urea measurements is used to determine infection. This value is compared to a cut-off value. Results below the value are assumed to be negative, those above positive. The cut-off value itself is determined by comparing the results of patients with two or more different detection methods. The value is chosen that gives the best combination of sensitivity and specificity.
The test measures active "H. pylori" infection. If antibiotics are depressing the amount of "H. pylori" present, or the stomach conditions are less acidic than normal, the amount of urease present will be lessened.
Accordingly the test should only be performed 14 days after stopping acid reducing medication (proton pump inhibitors, PPI) or 28 days after stopping antibiotic treatment. Some clinicians believe that a reservoir of "H. pylori" in dental plaque can affect the result.

</doc>
<doc id="32327" url="http://en.wikipedia.org/wiki?curid=32327" title="USS Triton">
USS Triton

USS "Triton" may refer to:

</doc>
<doc id="32332" url="http://en.wikipedia.org/wiki?curid=32332" title="UEFA">
UEFA

The Union of European Football Associations (UEFA, ; French: "Union des Associations Européennes de Football"; German: "Vereinigung Europäischer Fußballverbände") is the administrative body for association football in Europe, although several members have territory which is all or partially in Africa or Asia. It is one of six continental confederations of world football's governing body FIFA. UEFA consists of 54 national association members.
UEFA represents the national football associations of Europe, runs nation and club competitions including the UEFA European Championship, UEFA Champions League, UEFA Europa League, and UEFA Super Cup, and controls the prize money, regulations, and media rights to those competitions.
Until 1959 the main headquarters were located in Paris, and later in Bern. In 1995, UEFA headquarters transferred to Nyon, Switzerland. Henri Delaunay was the first general secretary and Ebbe Schwartz the first president. The current president is Michel Platini.
History and membership.
UEFA was founded on 15 June 1954 in Basel, Switzerland after consultation between the Italian, French, and Belgian associations. Initially, the European football union consisted of 25 members which number doubled by the early 1990s. UEFA membership coincides for the most part with recognition as a sovereign country in Europe, although there some exceptions. Some micro states, (e.g. the Vatican City) are not members. Some UEFA members are not sovereign states, but form part of a larger recognised sovereign state in the context of international law. Examples include England and Scotland, (part of the United Kingdom) or the Faroe Islands, (part of Denmark) however in the context of these countries government functions concerning sport tend to be carried at the territorial level coterminous with the UEFA member entity. Some UEFA members are transcontinental states, (e.g. Turkey and Russia). Several Asian countries were also admitted to the European football association, particularly Israel and Kazakhstan, which had been members of the Asian Football Confederation. Additionally some UEFA member associations allow teams from outside their association's main territory to take part in their "domestic" competition. Monaco, for example, takes part in the French League (though a separate sovereign entity); Welsh clubs Cardiff City and Swansea City participate in the English League; Berwick Rangers, situated in England, play in the Scottish Professional Football League and Derry City, situated in Northern Ireland, play in the Republic of Ireland-based League of Ireland.
Competitions.
UEFA runs official international competitions in Europe and some countries of Northern, Southwestern and Central regions of Asia for national teams and professional clubs, known as "UEFA competitions", some of which are regarded as the world's most prestigious tournaments.
International.
The main competition for men's national teams is the UEFA European Football Championship, started in 1958, with the first finals in 1960, and known as the European Nations Cup until 1964. It is also called UEFA or the EURO. UEFA also runs national competitions at Under-21, Under-19 and Under-17 levels. For women's national teams, UEFA operates the UEFA Women's Championship for senior national sides as well as Women's Under-19 and Women's Under-17 Championships.
UEFA also organized the UEFA-CAF Meridian Cup with CAF for youth teams in an effort to boost youth soccer. UEFA launched the UEFA Regions' Cup, for semi-professional teams representing their local region, in 1999. In futsal there is the UEFA Futsal Championship and UEFA Futsal Under-21 Championship.
The Italian, German, Spanish and French men's national teams are the sole teams to have won the European football championship in all categories.
Club.
The top-ranked UEFA competition is the UEFA Champions League, which started in the 1992/93 season and gathers the top 1–4 teams of each country's league (the number of teams depend on that country's ranking and can be upgraded or downgraded); this competition was re-structured from a previous one that only gathered the top team of each country (held from 1955–92 and known as the European Champion Clubs' Cup or simply the European Cup).
A second, lower-ranked competition is the UEFA Europa League. This competition, for national knockout cup winners and high-placed league teams, was launched by UEFA in 1971 as a successor of both the former UEFA Cup and the Inter-Cities Fairs Cup (also begun in 1955). A third competition, the Cup Winners' Cup, which had started in 1960, was absorbed into the UEFA Cup (now UEFA Europa League) in 1999.
In women's football UEFA also conducts the UEFA Women's Champions League for club teams. The competition was first held in 2001, and known as the UEFA Women's Cup until 2009.
The UEFA Super Cup pits the winners of the Champions League against the winners of the UEFA Europa League (previously the winners of the Cup Winners' Cup), and came into being in 1973.
The UEFA Intertoto Cup was a summer competition, previously operated by several Central European football associations, which was relaunched and recognized as official UEFA club competition by UEFA in 1995. The last Intertoto Cup took place in 2008.
The Intercontinental Cup was jointly organised with CONMEBOL between the Champions League and the Copa Libertadores winners.
Only four teams (Juventus, Ajax, Bayern Munich and Chelsea) have won each of the three main competitions (European Cup/UEFA Champions League, UEFA Cup Winner's Cup and UEFA Cup/Europa League), a feat that is no longer possible for any team that did not win the Cup Winners' Cup. There are currently nine teams throughout Europe that have won two of the three trophies; all but one have won the Cup Winners Cup, four require a win in the Champions League and five require a UEFA Europa League win.
Juventus of Italy was the first team in Europe—remaining the only one to date (2015)—to win all UEFA's official championships and cups and, in commemoration of achieving that feat, have received "The UEFA Plaque" by the Union of European Football Associations on 12 July 1988.
UEFA's premier futsal competition is the UEFA Futsal Cup, a tournament started in 2001 which replaced the former Futsal European Clubs Championship. This event, despite enjoying a long and well-established tradition in the European futsal community, dating back to 1984, was never recognized as official by UEFA.
Sponsors.
The UEFA Champions League current main sponsors are:
The UEFA Europa League current main sponsors are:
Adidas is a secondary sponsor and supplies the official match ball and referee uniform for all UEFA competitions.
Konami's "Pro Evolution Soccer" is also a secondary sponsor as the official Champions League video game.
League revenues.
Annual revenue comparison. All figures in Euros.
"Source is the Deloitte 2013 annual report, which uses 2011–12 figures."
World Cup participation and results.
Men.
Notes

</doc>
<doc id="32398" url="http://en.wikipedia.org/wiki?curid=32398" title="Fighting game">
Fighting game

Fighting game is a video game genre in which the player controls an on-screen character and engages in close combat with an opponent. These characters tend to be of equal power and fight matches consisting of several rounds, which take place in an arena. Players must master techniques such as blocking, counter-attacking, and chaining together sequences of attacks known as "combos". Since the early 1990s, most fighting games allow the player to execute special attacks by performing specific button combinations. The genre is related to but distinct from beat 'em ups, which involve large numbers of antagonists.
The first game to feature fist fighting was arcade game "Heavyweight Champ" in 1976, but it was "Karate Champ" which popularized one-on-one martial arts games in arcades in 1984. In 1985, "Yie Ar Kung-Fu" featured antagonists with differing fighting styles, while "The Way of the Exploding Fist" further popularized the genre on home systems. In 1987, "Street Fighter" introduced hidden special attacks. In 1991, Capcom's highly successful "Street Fighter II" refined and popularized many of the conventions of the genre. The fighting game subsequently became the preeminent genre for competitive video gaming in the early to mid-1990s, especially in arcades. This period spawned numerous popular fighting games in addition to "Street Fighter", including successful and long running franchises like "Mortal Kombat", "Virtua Fighter", "Tekken" and "WWE 2K"
The genre's popularity stagnated as games became more complicated and as arcades began to lose their audience to increasingly powerful home consoles near the end of the 1990s, though new franchises such as "Dead or Alive" and the "Soul" series achieved success. In the new millennium, the genre remains popular but retains a much smaller proportion of enthusiasts than it once did, due to the increasing popularity of other genres and internet multiplayer gaming.
Fighting Games are almost always asymmetric games as each player fights in a different way.
Definition.
Fighting games are a type of action game where on-screen characters fight each other. These games typically feature special moves that are triggered using rapid sequences of carefully timed button presses and joystick movements. Games traditionally show fighters from a side-view, even as the genre has progressed from two-dimensional (2D) to three-dimensional (3D) graphics. "Street Fighter II", though not the first fighting game, popularized and standardized the conventions of the genre, and similar games released prior to "Street Fighter II" have since been more explicitly classified as fighting games. Fighting games typically involve hand-to-hand combat, but may also feature melee weapons.
This genre is distinct from beat 'em ups, another action genre involving combat, where the player character must fight many weaker enemies at the same time. During the 1980s publications used the terms "fighting game" and "beat 'em up" interchangeably, along with other terms such as "martial arts simulation" (or more specific terms such as "judo simulator"). With hindsight, critics have argued that the two types of game gradually became dichotomous as they evolved, though the two terms may still be conflated. Fighting games are sometimes grouped with games that feature boxing or wrestling. Serious boxing games belong more to the sports game genre than the action game genre, as they aim for a more realistic model of boxing techniques, whereas moves in fighting games tend to be highly exaggerated models of Asian martial arts techniques. As such, boxing games and wrestling games are often described as distinct genres, without comparison to fighting games and belong more into the Sports game genre.
Game design.
Fighting games involve combat between pairs of fighters using highly exaggerated martial arts moves. They typically revolve around primarily brawling or combat sport, though some variations feature weaponry. Games usually display on-screen fighters from a side view, and even 3D fighting games play largely within a 2D plane of motion. Games usually confine characters to moving left and right and jumping, although some games such as "" allow players to move between parallel planes of movement. Recent games tend to be rendered in three dimensions and allow side-stepping, but otherwise play like those rendered in two dimensions.
Aside from moving around a restricted space, fighting games limit the player's actions to different offensive and defensive maneuvers. Players must learn which attacks and defenses are effective against each other, often by trial and error. Blocking is a basic technique that allows a player to defend against attacks. Some games feature more advanced blocking techniques: for example, Capcom's "Street Fighter III" features a move termed "parrying" which causes the attacker to become momentarily incapacitated (a similar state is termed "just defended" in SNK's ""). In addition to blows such as punches and kicks, players can utilize throwing or "grappling" to circumvent "blocks". Predicting opponents' moves and counter-attacking, known as "countering", is a common element of gameplay. Fighting games also emphasize the difference between the height of blows, ranging from low to jumping attacks. Thus, strategy becomes important as players attempt to predict each other's moves, similar to rock-paper-scissors.
Special attacks.
An integral feature of fighting games includes the use of "special attacks", also called "secret moves", that employ complex combinations of button presses to perform a particular move beyond basic punching and kicking. Combos, in which several attacks are chained together using basic punches and kicks, are another common feature in fighting games and have been fundamental to the genre since the release of "Street Fighter II". Some fighting games display a "combo meter" that displays the player's progress through a combo. The effectiveness of such moves often relate to the difficulty of execution and the degree of risk. These moves are often beyond the ability of a casual gamer and require a player to have both a strong memory and excellent timing. Taunting is another feature of some fighting games and was originally introduced by Japanese company SNK in their game "Art of Fighting". It is used to add humor to games, but can also have an effect on gameplay such as improving the strength of other attacks. Sometimes, a character can even be noted especially for taunting (for example, Dan Hibiki from "Street Fighter Alpha").
Matches and rounds.
Fighting game matches generally consist of several rounds (typically "best of three"); the player who wins the most rounds wins the match. Fighting games widely feature life bars, which are depleted as characters sustain blows. Each successful attack will deplete a character's health, and the game continues until a fighter's energy reaches zero. Hence, the main goal is to completely deplete the life bar of one's opponent, thus achieving a "knockout". Beginning with Midway's "Mortal Kombat" released in 1992, the "Mortal Kombat" series introduced "Fatalities" in which the victor kills a knocked-out opponent in a gruesome manner. Games such as "Virtua Fighter" also allow a character to be defeated by forcing them outside of the fighting arena, awarding a "ring-out" to the victor. Round decisions can also be determined by time over (if a timer is present), which judges players based on remaining vitality to declare a winner. Fighting games often include a single player campaign or tournament, where the player must defeat a sequence of several computer controlled opponents. Winning the tournament often reveals a special story–ending cutscene, and some games also grant access to hidden characters or special features upon victory.
Character selection.
In most fighting games, players may select from a variety of characters who have unique fighting styles and special moves. This became a strong convention for the genre with the release of "Street Fighter II", and these character choices have led to deeper game strategy and replay value. Although fighting games offer female characters, their image tends to be hypersexualized, and they have even been featured as pin-up girls in game magazines. Male characters in fighting games tend to have extra-broad chests and shoulders, huge muscles, and prominent jaws. Custom creation, or "create–a–fighter", is a feature of some fighting games which allows a player to customize the appearance and move set of their own character. "Super Fire Pro Wrestling X Premium" was the first game to include such a feature, and later fighting games such as "Fighter Maker", "Soulcalibur III", ', and ' adopted the concept.
Multiplayer modes.
Fighting games may also offer a multiplayer mode in which players fight each other, sometimes by letting a second player challenge the first at any moment during a single player match. A few titles allow up to four players to compete simultaneously. Several games have also featured modes that involve teams of characters; players form "tag teams" to fight matches in which combat is one-on-one, but a character may leave the arena to be replaced by a team mate. Some fighting games have also offered the challenge of fighting against multiple opponents in succession, testing the player's endurance. Newer titles take advantage of online gaming services, although lag created by slow data transmission can disrupt the split-second timing involved in fighting games. The impact of lag in some fighting games has been reduced by using technology such as GGPO, which keeps the players' games in sync by quickly rolling back to the most recent accurate game state, correcting errors, and then jumping back to the current frame. Games using this technology include "Skullgirls" and "".
History.
Late 1970s to 1980s.
Fighting games find their origin in boxing games but evolved towards battles between characters with fantastic abilities and complex special maneuvers. Sega's black and white boxing game "Heavyweight Champ", which was released in 1976, is considered the first video game to feature fist fighting. 1979's "Warrior" is another title sometimes credited as one of the first fighting games. In contrast to "Heavyweight Champ" and most later titles, "Warrior" was based on sword fighting duels and used a bird's eye view. In 1983, Sega released another boxing game "Champion Boxing", which was Yu Suzuki's debut title at Sega. However, Data East and its related developer Technōs Japan's "Karate Champ" from 1984 is credited with establishing and popularizing the one-on-one fighting game genre. In it, a variety of moves could be performed using the dual-joystick controls, it used a best-of-three matches format like later fighting games, and it featured training bonus stages. It went on to influence Konami's "Yie Ar Kung Fu", released in January 1985, which expanded on "Karate Champ" by pitting the player against a variety of opponents, each with a unique appearance and fighting style. The player could also perform up to sixteen different moves, including projectile attacks. The martial arts game "The Way of the Exploding Fist", released in June 1985, achieved critical success and subsequently afforded the burgeoning genre further popularity on home systems. Numerous other game developers tried to imitate the financial successes of "Karate Champ", "Yie Ar Kung-Fu" and "The Way of the Exploding Fist" with similar games; Data East took unsuccessful legal action against Epyx over the computer game "International Karate".
Both "Karate Champ" and "Yie Ar Kung Fu" later provided a template for Capcom's "Street Fighter" in 1987. "Street Fighter" found its own niche in the gaming world, partially because many arcade game developers in the 1980s focused more on producing beat-em-ups and shoot 'em ups. Part of the game's appeal was the use of special moves that could only be discovered by experimenting with the game controls, which created a sense of mystique and invited players to practice the game, although similar controller motions used for grappling maneuvers in the earlier "Brian Jacks Uchi Mata" were deemed too difficult. Following "Street Fighter's" lead, the use of command-based hidden moves began to pervade other games in the rising fighting game genre. "Street Fighter" also introduced other staples of the genre, including the blocking technique as well as the ability for a challenger to jump in and initiate a match against a player at any time. The game also introduced pressure-sensitive controls that determine the strength of an attack, though due to causing damaged arcade cabinets, Capcom replaced it soon after with a six-button control scheme offering light, medium and hard punches and kicks, which became another staple of the genre. In 1988, Home Data released "Reikai Dōshi: Chinese Exorcist", also known as "Last Apostle Puppet Show", the first fighting game to use digitized sprites and motion capture animation. Meanwhile, home game consoles largely ignored the genre. "" was one of few releases for the Sega Genesis but was not as popular as games in other genres. Technical challenges limited the popularity of early fighting games. Programmers had difficulty producing a game that could recognize the fast motions of a joystick, and so players had difficulty executing special moves with any accuracy.
Early 1990s.
The release of "Street Fighter II" in 1991 is often considered a revolutionary moment in the fighting game genre. Yoshiki Okamoto's team developed the most accurate joystick and button scanning routine in the genre thus far. This allowed players to reliably execute multi-button special moves, which had previously required an element of luck. The game was also highly successful because its graphics took advantage of Capcom's CPS arcade chipset, with highly detailed characters and stages. Whereas previous games allowed players to combat a variety of computer-controlled fighters, "Street Fighter II" allowed players to play against each other. The popularity of "Street Fighter II" surprised the gaming industry, as arcade owners bought more machines to keep up with demand. "Street Fighter II" was also responsible for popularizing the combo mechanic, which came about when skilled players learned that they could combine several attacks that left no time for the opponent to recover if they timed them correctly.
SNK released "" a few months before "Street Fighter II". It was designed by Takashi Nishiyama, the creator of the original "Street Fighter", which it was envisioned as a spiritual successor to. "Fatal Fury" placed more emphasis on storytelling and the timing of special moves, and added a two-plane system where characters could step into the foreground or background. Meanwhile, Sega experimented with "Dark Edge", an early attempt at a 3D fighting game where characters could move in all directions. Sega however, never released the game outside of Japan because it felt that "unrestrained" 3D fighting games were unenjoyable. Sega also attempted to introduced 3-D holographic technology to the genre with "Holosseum" in 1992, though it was unsuccessful. Several fighting games achieved greater commercial success, including SNK's "Art of Fighting" and "Samurai Shodown" as well as Sega's "Eternal Champions". Nevertheless, "Street Fighter II" remained the most popular, spawning a special "Champion Edition" that improved game balance and allowed players to use additional characters. The popularity of "Street Fighter II" led it to be released for home game consoles and allowed it to define the template for fighting games. Fighting games soon became the dominant genre in the arcade game industry of the early 1990s.
Many American developers tried to capitalize on the template established by "Street Fighter II", but it was Chicago's Midway Games who achieved unprecedented notoriety when they released "Mortal Kombat" in 1992. The game featured digital characters drawn from real actors, numerous secrets, and a "Fatality" system of finishing maneuvers with which the player's character kills their opponent. The game earned a reputation for its gratuitous violence, and was eventually adapted for home game consoles. The home version of "Mortal Kombat" was released on September 13, 1993, a day that was promoted as "Mortal Monday". The advertising resulted in line-ups to purchase the game and a subsequent backlash from politicians concerned about the game's violence. The "Mortal Kombat" franchise would ultimately achieve iconic status similar to that of "Street Fighter" with several sequels as well as movies, television series, and extensive merchandising. Numerous other game developers tried to imitate "Street Fighter II" and "Mortal Kombat"'s financial success with similar games; Capcom USA took unsuccessful legal action against Data East over the 1993 arcade game "Fighter's History". Data East's largest objection in court was that their 1984 arcade game "Karate Champ" was the true originator of the competitive fighting game genre, which predated the original "Street Fighter" by three years.
Sega AM2's first attempt in the genre was the 1993 arcade game "Burning Rival", but began to attract attention with the release of "Virtua Fighter" for the same platform the same year. It was the first fighting game with 3D polygon graphics and a viewpoint that zoomed and rotated with the action. Despite the graphics, players were confined to back and forth motion as seen in other fighting games. With only three buttons, it was easier to learn than "Street Fighter" and "Mortal Kombat", having six and five buttons respectively. By the time the game was released for the Sega Saturn in Japan, the game and system were selling at almost a one-to-one ratio. Meanwhile, the 1993 title "Mortal Kombat II" captivated Western audiences, and a 2008 review considered the best "Mortal Kombat" game in retrospect.
The 1994 PlayStation launch title "Battle Arena Toshinden" is credited for taking the genre into "true 3-D" due to its introduction of the sidestep maneuver, which IGN described as "one little move" that "changed the fighter forever." The same year, SNK released "The King of Fighters '94" in arcades, where players choose from teams of three characters to eliminate each other one by one. Eventually, Capcom released further updates to "Street Fighter II", including "Super Street Fighter II" and "Super Street Fighter II Turbo". These games featured more characters and new moves, some of which were a response to people who had hacked the original "Street Fighter II" game to add new features themselves. However, criticism of these updates grew as players demanded a true sequel. By 1995, the dominant franchises were the "Mortal Kombat" series in America and "Virtua Fighter" series in Japan, with "Street Fighter Alpha: Warriors' Dreams" unable to match the popularity of "Street Fighter II". Throughout this period, the fighting game was the dominant genre in competitive video gaming, with enthusiasts popularly attending arcades in order to find human opponents.
Late 1990s.
In the latter part of the 1990s, the fighting game genre began to decline in popularity, with specific franchises falling into difficulty. "Electronic Gaming Monthly" awarded the excess of fighting games the "Most Appalling Trend" award of 1995. Although the release of "Street Fighter EX" introduced 3D graphics to the series and continued the success of "Street Fighter II" and "Street Fighter Alpha", the ' arcade game was regarded as a failure. "Street Fighter: The Movie" used digitized images from the "Street Fighter" film. While a home video game also titled ' was released for the PlayStation and Sega Saturn, it is not a port but a separately produced game based on the same premise. Capcom later released "Street Fighter III" in 1997 which featured improved visuals and character depth, but was also unable to match the impact of "Street Fighter II". Despite excitement in Japan over "Virtua Fighter 3" in arcades, the limited hardware capabilities of the Sega Saturn led Sega to delay a console release. Sega eventually released the game for its Dreamcast console, but the company became unprofitable and was forced to discontinue the console. Meanwhile, SNK released several fighting games on their Neo-Geo platform, including "Samurai Shodown II" in 1994, "Real Bout Fatal Fury" in 1995, "The Last Blade" in 1997, and annual updates to their "The King of Fighters" franchise. "" from 1999 was considered one of SNK's last great games, and the company announced that it would close its doors in 2001.
In retrospect, multiple developers attribute the decline of the fighting genre to its increasing complexity and specialization. This complexity shut out casual players, and the market for fighting games became smaller and more specialized. Furthermore, arcades gradually became less profitable throughout the 1990s due to the increased technical power and popularity of home consoles. Even as popularity dwindled, the fighting game genre continued to evolve; several strong 3D fighting games also emerged in the late 1990s. Namco's "Tekken" (released in arcades in 1994 and on the PlayStation in 1995) proved critical to the PlayStation's early success, with its sequels also becoming some of the console's most important titles. The "Soul" series of weapon-based fighting games also achieved considerable critical success, beginning with 1995's "Soul Edge" (known as "Soul Blade" outside of Japan) to "Soulcalibur V" in 2012. Tecmo released "Dead or Alive" in Japanese arcades in 1996, porting it for the PlayStation in 1998. It spawned a long running franchise, known for its fast paced control system and innovative counterattacks. The series again included titles important to the success of their respective consoles, including "Dead or Alive 4" for the Xbox 360. In 1998, "Bushido Blade", published by Square, introduced a realistic fighting engine that featured three-dimensional environments while abandoning time limits and health bars in favour of an innovative Body Damage System, where a sword strike to a certain body part can amputate a limb or decapitate the head.
Video game enthusiasts took an interest in gaming crossovers which feature characters from multiple franchises in a particular game. An early example of this type of fighting game was the 1998 arcade release "", featuring comic book superheroes as well as "Street Fighter" characters. In 1999, Nintendo released the first game in the "Super Smash Bros." series, which allowed match-ups such as Pikachu versus Mario.
Early 2000s.
The early part of the decade saw the rise of major international fighting game tournaments such as Tougeki – Super Battle Opera and Evolution Championship Series, and famous players such as Daigo Umehara. Several more fighting game crossovers were released in the new millennium. The two most prolific developers of 2D fighting games, Capcom and SNK, combined intellectual property to produce "SNK vs. Capcom" games. SNK released the first game of this type, "," for its Neo Geo Pocket Color handheld at the end of 1999. GameSpot regarded the game as "perhaps the most highly anticipated fighter ever" and called it the best fighting game ever to be released for a handheld console. Capcom released ' for arcades and the Dreamcast in 2000, followed by sequels in subsequent years. Though none matched the critical success of the handheld version, "Capcom vs. SNK 2 EO" was noted as the first game of the genre to successfully utilize internet competition. Other crossovers from 2008 included ' and "Mortal Kombat vs. DC Universe". The most successful crossover, however, was "Super Smash Bros. Brawl," also released in 2008 for the Wii. Featuring characters from Nintendo's various franchises, the game was a runaway commercial success in addition to being lavished with critical praise.
In the new millennium, fighting games became less popular and plentiful than in the mid-1990s, with multiplayer competition shifting towards other genres. However, SNK reappeared in 2003 as SNK Playmore and continued to release games. Arc System Works received critical acclaim for releasing "Guilty Gear X" in 2001, as well as its sequel "Guilty Gear XX", as both were 2D fighting games featuring striking anime inspired graphics. The fighting game is currently a popular genre for amateur and doujin developers in Japan. The 2002 title "Melty Blood" was developed by then amateur developer French-Bread and achieved cult success on the PC. It became highly popular in arcades following its 2005 release, and a version was released for the PlayStation 2 the following year. While the genre became generally far less popular than it once was, arcades and their attendant fighting games remained reasonably popular in Japan in this time period, and still remain so even today. "Virtua Fighter 5" lacked an online mode but still achieved success both on home consoles and in arcades; players practiced at home and went to arcades to compete face-to-face with opponents. In addition to "Virtua Fighter" and "Tekken", the "Soul" and "Dead or Alive" franchises continued to release installments. Classic "Street Fighter" and "Mortal Kombat" games were re-released on PlayStation Network and Xbox Live Arcade, allowing internet play, and in some cases, HD graphics.
Late 2000s to present.
"Street Fighter IV", which incorporated online multiplayer modes, was released in early 2009 to critical acclaim, having garnered praise since its release at Japanese arcades in 2008. The console versions of the game as well as "Super Street Fighter IV" sold more than 6 million copies in total. "Street Fighter's" successful revival has sparked a renaissance for the genre, introducing new players to the genre and with the increased audience allowing other fighting game franchises to achieve successful revivals of their own. "Tekken 6" was positively received, selling more than 3 million copies worldwide as of August 6, 2010. Other successful titles that followed include ', ', "Mortal Kombat" (2011), "Street Fighter X Tekken", and "The King of Fighters XIII".
Despite the critically acclaimed "Virtua Fighter 5" releasing to very little fanfare in 2007, its update "Virtua Fighter 5: Final Showdown" has received much more attention in 2012 due to the renewed interest in fighting games. "Dead or Alive 5", is notable for making extensive use of destructible environments. Other titles following in the wake of the fighting game renaissance include "Persona 4 Arena", "Tekken Tag Tournament 2", "Soulcalibur V", and crossover titles such as "PlayStation All-Stars Battle Royale" and "Tekken X Street Fighter".

</doc>
<doc id="32405" url="http://en.wikipedia.org/wiki?curid=32405" title="Vegetable farming">
Vegetable farming

Vegetable farming is the growing of vegetables for human consumption.
Traditionally it was done in the soil in small rows or blocks, often primarily for consumption on the farm, with the excess sold in nearby towns. Later, farms on the edge of large communities could specialize in vegetable production, with the short distance allowing the farmer to get his produce to market while still fresh. The three sisters method used by Native Americans (specifically the Haudenosaunee/Iroquois) grew squash, beans and corn together so that the plants enhanced each other's growth. Planting in long rows allows machinery to cultivate the fields, increasing efficiency and output; however, the diversity of vegetable crops requires a number of techniques to be used to optimize the growth of each type of plant. Some farms, therefore, specialize in one vegetable; others grow a large variety. Due to the needs to market vegetables while fresh, vegetable gardening has high labor demands. Some farms avoid this by running u-pick operations where the customers pick their own produce. The development of ripening technologies and refrigeration has reduced the problems with getting produce to market in good condition.
Over the past 100 years a new technique has emerged—raised bed gardening, which has increased yields from small plots of soil without the need for commercial, energy-intensive fertilizers. Modern hydroponic farming produces very high yields in greenhouses without using any soil.
Several economic models exist for vegetable farms: farms may grow large quantities of a few vegetables and sell them in bulk to major markets or middlemen, which requires large growing operations; farms may produce for local customers, which requires a larger distribution effort; farms may produce a variety of vegetables for sale through on-farm stalls, local farmer's markets, u-pick operations. This is quite different from commodity farm products like wheat and maize which do not have the ripeness problems and are sold off in bulk to the local granary. Large cities often have a central produce market which handles vegetables in a commodity-like manner, and manages distribution to most supermarkets and restaurants.
In America, vegetable farms are in some regions known as truck farms; "truck" is a noun for which its more common meaning overshadows its historically separate use as a term for "vegetables grown for market". Such farms are sometimes called muck farms, after the dark black soil in which vegetables grow well.
Common vegetable crops.
Vegetables which are farmed include:

</doc>
<doc id="32406" url="http://en.wikipedia.org/wiki?curid=32406" title="Vinaigrette (disambiguation)">
Vinaigrette (disambiguation)

The word vinaigrette or vinegarette (from the diminutive of the French term "vinaigre" meaning vinegar) can refer to:

</doc>
<doc id="32407" url="http://en.wikipedia.org/wiki?curid=32407" title="Virgo (constellation)">
Virgo (constellation)

Virgo is one of the constellations of the zodiac. Its name is Latin for virgin, and its symbol is ♍. Lying between Leo to the west and Libra to the east, it is the second largest constellation in the sky (after Hydra). It can be easily found through its brightest star, Spica.
Location.
The bright Spica makes it easy to locate Virgo, as it can be found by following the curve of the Big Dipper/Plough to Arcturus in Boötes and continuing from there in the same curve ("follow the arc to Arcturus and speed on to Spica").
Due to the effects of precession, the First Point of Libra, (also known as "the autumn equinox point") lies within the boundaries of Virgo very close to β Virginis. This is one of the two points in the sky where the celestial equator crosses the ecliptic (the other being the First Point of Aries, now in the constellation of Pisces.) This point will pass into the neighbouring constellation of Leo around the year 2440.
Notable features.
Stars.
Besides Spica, other bright stars in Virgo include β Virginis (Zavijava), γ Vir (Porrima), δ Virginis (Auva) and ε Virginis (Vindemiatrix). Other fainter stars that were also given names are ζ Virginis (Heze), η Virginis (Zaniah), ι Virginis (Syrma) and μ Virginis (Rijl al Awwa).
The star 70 Virginis has one of the first known extrasolar planetary systems with one confirmed planet 7.5 times the mass of Jupiter.
The star Chi Virginis has one of the most massive planets ever detected, at a mass of 11.1 times that of Jupiter.
The sun-like star 61 Virginis has three planets: one is a super-Earth and two are Neptune-mass planets.
SS Virginis is a variable star with a noticeable red color. It varies in magnitude from a minimum of 9.6 to a maximum of 6.0 over a period of approximately one year.
Exoplanets.
There are 35 verified exoplanets orbiting 29 stars in Virgo, including PSR B1257+12 (three planets), 70 Virginis (one planet), Chi Virginis (one planet), 61 Virginis (three planets), NY Virginis (two planets), and 59 Virginis (one planet).
Deep-sky objects.
Because of the presence of a galaxy cluster (consequently called the Virgo cluster) within its borders 5° to 12° west of ε Vir (Vindemiatrix), this constellation is especially rich in galaxies.
Some examples are Messier 49 (elliptical), Messier 58 (spiral), Messier 59 (elliptical), Messier 60 (elliptical), Messier 61 (spiral), Messier 84 (lenticular), Messier 86 (lenticular), Messier 87 (elliptical and a famous radio source), Messier 89 (elliptical) and Messier 90 (spiral). A noted galaxy that is not part of the cluster is the Sombrero Galaxy (M104), an unusual spiral galaxy. It is located about 10° due west of Spica.
NGC 4639 is a face-on barred spiral galaxy located from Earth (redshift 0.0034). Its outer arms have a high number of Cepheid variables, which are used as standard candles to determine astronomical distances. Because of this, astronomers used several Cepheid variables in NGC 4639 to calibrate type 1a supernovae as standard candles for more distant galaxies.
Virgo possesses several galaxy clusters, one of which is HCG 62. A Hickson Compact Group, HCG 62 is at a distance of from Earth (redshift 0.0137) and possesses a large central elliptical galaxy. It has a heterogeneous halo of extremely hot gas, posited to be due to the active galactic nucleus at the core of the central elliptical galaxy.
M87 is the largest galaxy in the Virgo cluster, and is at a distance of from Earth (redshift 0.0035). It is a major radio source, partially due to its jet of electrons being flung out of the galaxy by its central supermassive black hole. Because this jet is visible in several different wavelengths, it is of interest to astronomers who wish to observe black holes in a unique galaxy.
M84 is another elliptical radio galaxy in the constellation of Virgo; it is at a distance of (redshift 0.0035) as well. Astronomers have surmised that the speed of the gas clouds orbiting the core (approximately ) indicates the presence of an object with a mass 300 million times that of the sun, which is most likely a black hole.
The Sombrero Galaxy, M104, is an edge-on spiral galaxy located 28 million light-years from Earth (redshift 0.0034). It has a bulge at its center made up of older stars that is larger than normal. It is surrounded by large, bright globular clusters and has a very prominent dust lane made up of polycyclic aromatic hydrocarbons.
NGC 4438 is a peculiar galaxy with an active galactic nucleus, at a distance of from Earth (redshift 0.0035). Its supermassive black hole is ejecting jets of matter, creating bubbles with a diameter of up to .
NGC 4261 also has a black hole from its center with a mass of 1.2 billion solar masses. It is located at a distance of from Earth (redshift 0.0075), and has an unusually dusty disk with a diameter of . Along with M84 and M87, NGC 4261 has strong emissions in the radio spectrum.
IC 1101 is a supergiant elliptical galaxy in the Abell 2029 galaxy cluster located about from Earth. At the diameter of 5.5 million light years, or more than 50 times the size of the Milky Way, it was the largest known galaxy in the universe.
Virgo is also home to the quasar 3C 273 which was the first quasar ever to be identified. With a magnitude of ~12.9 it is also the optically brightest quasar in the sky.
Mythology.
According to the Babylonian Mul.Apin, which dates from 1000–686 BCE, this constellation was known as "The Furrow", representing the goddess Shala's ear of grain. One star in this constellation, Spica, retains this tradition as it is Latin for "ear of grain", one of the major products of the Mesopotamian furrow. The constellation was also known as "AB.SIN" and "absinnu". For this reason the constellation became associated with fertility. According to the figure of Virgo corresponds to two Babylonian constellations: the "Furrow" in the eastern sector of Virgo and the "Frond of Erua" in the western sector. The Frond of Erua was depicted as a goddess holding a palm-frond – a motif that still occasionally appears in much later depictions of Virgo.
The Greeks and Romans associated Virgo with their goddess of wheat/agriculture, Demeter-Ceres who is the mother of Persephone-Proserpina. Alternatively, she was sometimes identified as the virgin goddess "Iustitia" or Astraea, holding the scales of justice in her hand as the constellation Libra. Another myth identifies Virgo as Erigone, the daughter of Icarius of Athens. Icarius, who had been favoured by Dionysus, was killed by his shepherds while they were intoxicated and Erigone hanged herself in grief; Dionysus placed the father and daughter in the stars as Boötes and Virgo respectively. In the Middle Ages, Virgo was sometimes associated with the Blessed Virgin Mary.
Astrology.
s of 2002[ [update]], the Sun appears in the constellation Virgo from September 17 to October 30. In tropical astrology, the Sun is considered to be in the sign Virgo from August 23 to September 22, and in sidereal astrology, from September 16 to October 15.
Visualizations.
Virgo is often portrayed carrying two sheaves of wheat, one of which is marked by the bright star Spica.
H.A. Rey has suggested an alternative way to visualize Virgo, which graphically shows the virgin lying down in a supine position. The stars γ Vir, η Vir, β Vir, ν Vir, and ο Vir form the virgin's head. The stars γ Vir, δ Vir, ζ Vir, α Vir (Spica), and θ Vir form the virgin's blouse. The stars α Vir, ζ Vir, τ Vir, ι Vir, and κ Vir form the virgin's skirt. The stars 109 Vir and μ Vir represent the Virgin's feet, and the star ε Vir represents the virgin's hand.
External links.
Coordinates: 

</doc>
<doc id="32427" url="http://en.wikipedia.org/wiki?curid=32427" title="Violin">
Violin

The violin, also known as a fiddle, is a string instrument, usually with four strings tuned in perfect fifths. It is the smallest, highest-pitched member of the violin family of string instruments, which also includes the viola, and the cello. The modern word is derived from the Italian word "violino", literally meaning 'small viola'.
Someone who plays the violin is called a violinist or a fiddler. The violinist produces sound by drawing a bow across one or more strings (which may be stopped by the fingers of the other hand to produce a full range of pitches), by plucking the strings (with either hand), or by a variety of other techniques. The violin is played by musicians in a wide variety of musical genres, including Baroque music, classical, jazz, country music, bluegrass music, folk music, metal, rock and roll, and soft rock. The violin has come to be played in many non-Western music cultures all over the world.The violin is sometimes informally called a fiddle, regardless of the type of music played on it.
The violin is first known in 16th-century Italy, with some further modifications occurring in the 18th and 19th centuries. Violinists and collectors particularly prize the instruments made by the Stradivari, Guarneri and Amati families from the 16th to the 18th century in Brescia and Cremona and by Jacob Stainer in Austria. According to their reputation, the quality of their sound has defied attempts to explain or equal it, though this belief is disputed. Great numbers of instruments have come from the hands of "lesser" makers, as well as still greater numbers of mass-produced commercial "trade violins" coming from cottage industries in places such as Saxony, Bohemia, and Mirecourt. Many of these trade instruments were formerly sold by Sears, Roebuck and Co. and other mass merchandisers.
A person who makes or repairs violins is called a luthier. The parts of a violin are usually made from different types of wood (although electric violins may not be made of wood at all, since their sound may not be dependent on specific acoustic characteristics of the instrument's construction), and it is usually strung with gut, Perlon or other synthetic, or steel strings.
History.
The earliest stringed instruments were mostly plucked (e.g. the Greek lyre). Bowed instruments may have originated in the equestrian cultures of Central Asia, an example being the Tanbur originated in modern-day Uzbekistan or Kobyz (Kazakh: қобыз) (kyl-kobyz) - an ancient Turkic, Kazakh string instrument or Mongolian instrument Morin huur:
It is believed that these instruments eventually spread to China, India, the Byzantine Empire and the Middle East, where they developed into instruments such as the erhu in China, the rebab in the Middle East, the lyra in the Byzantine Empire, and the esraj in India. The violin in its present form emerged in early 16th-Century Northern Italy, perhaps from the popular medieval European vielle, also called a "fidel" or "viuola," which was itself derived from the aforementioned Byzantine lyra.
The modern European violin evolved from various bowed stringed instruments from the Middle East and the Byzantine Empire. It is most likely that the first makers of violins borrowed from three types of current instruments: the rebec, in use since the 10th century (itself derived from the Byzantine lyra and the Arabic "rebab"), the Renaissance fiddle, and the "lira da braccio" (both derived from the "Byzantine lira").
The earliest pictures of violins, albeit with three strings, are seen in northern Italy around 1530, at around the same time as the words "violino" and "vyollon" are seen in Italian and French documents. One of the earliest explicit descriptions of the instrument, including its tuning, was in the "Epitome musical" by Jambe de Fer, published in Lyon in 1556. By this time, the violin had already begun to spread throughout Europe.
The violin immediately became very popular, both among street musicians and the nobility, illustrated by the fact that the French king Charles IX ordered Andrea Amati to construct 24 violins for him in 1560. One of these instruments, now called the "Charles IX", is the oldest surviving violin. The finest Renaissance carved and decorated violin in the world is the Gasparo da Salò (1574 c.) owned by Ferdinand II, Archduke of Austria and later, from 1841, by the Norwegian virtuoso Ole Bull, who used it for forty years and thousands of concerts, for his very powerful and beautiful tone, similar to those of a Guarneri. It is now in the Vestlandske Kustindustrimuseum in Bergen (Norway). "The Messiah" or "Le Messie" (also known as the "Salabue") made by Antonio Stradivari in 1716 remains pristine. It is now located in the Ashmolean Museum of Oxford.
The most famous (luthiers) between the 16th century and the 18th century include:
Significant changes occurred in the construction of the violin in the 18th century, particularly in the length and angle of the neck, as well as a heavier bass bar. The majority of old instruments have undergone these modifications, and hence are in a significantly different state than when they left the hands of their makers, doubtless with differences in sound and response. But these instruments in their present condition set the standard for perfection in violin craftsmanship and sound, and violin makers all over the world try to come as close to this ideal as possible.
To this day, instruments from the so-called Golden Age of violin making, especially those made by Stradivari, Guarneri del Gesù and Montagnana are the most sought-after instruments by both collectors and performers. The current record amount paid for a Stradivari violin is £9.8 million (US$15.9 million), when the instrument known as the Lady Blunt was sold by Tarisio Auctions in an online auction on June 20, 2011.
Construction and mechanics.
A violin generally consists of a spruce top (the soundboard, also known as the "top plate", "table", or "belly"), maple ribs and back, two endblocks, a neck, a bridge, a soundpost, four strings, and various fittings, optionally including a chinrest, which may attach directly over, or to the left of, the tailpiece. A distinctive feature of a violin body is its hourglass-like shape and the arching of its top and back. The hourglass shape comprises two upper bouts, two lower bouts, and two concave C-bouts at the "waist", providing clearance for the bow.
The voice of a violin depends on its shape, the wood it is made from, the graduation (the thickness profile) of both the top and back, and the varnish that coats its outside surface. The varnish and especially the wood continue to improve with age, making the fixed supply of old violins much sought-after.
The very great majority of glued joints in the instrument use animal hide glue for a number of reasons: it is capable of making a thinner joint than most other glues, it is reversible (brittle enough to crack with carefully applied force, and removable with warm water) when disassembly is needed, and since fresh hide glue sticks to old hide glue, more original wood can be preserved when repairing a joint. (More modern glues must be cleaned off entirely for the new joint to be sound, which generally involves scraping off some wood along with the old glue.) Weaker, diluted glue is usually used to fasten the top to the ribs, and the nut to the fingerboard, since common repairs involve removing these parts.
The purfling running around the edge of the spruce top provides some protection against cracks originating at the edge. It also allows the top to flex more independently of the rib structure. Painted-on faux purfling on the top is usually a sign of an inferior instrument. The back and ribs are typically made of maple, most often with a matching striped figure, referred to as "flame", "fiddleback", or "tiger stripe".
The neck is usually maple with a flamed figure compatible with that of the ribs and back. It carries the fingerboard, typically made of ebony, but often some other wood stained or painted black. Ebony is the preferred material because of its hardness, beauty, and superior resistance to wear. Fingerboards are dressed to a particular transverse curve, and have a small lengthwise "scoop," or concavity, slightly more pronounced on the lower strings, especially when meant for gut or synthetic strings.
Some old violins (and some made to appear old) have a grafted scroll, evidenced by a glue joint between the pegbox and neck. Many authentic old instruments have had their necks reset to a slightly increased angle, and lengthened by about a centimeter. The neck graft allows the original scroll to be kept with a Baroque violin when bringing its neck into conformance with modern standards.
The bridge is a precisely cut piece of maple that forms the lower anchor point of the vibrating length of the strings and transmits the vibration of the strings to the body of the instrument. Its top curve holds the strings at the proper height from the fingerboard in an arc, allowing each to be sounded separately by the bow. The sound post, or "soul post", fits precisely inside the instrument between the back and top, below the treble foot of the bridge, which it helps support. It also transmits vibrations between the top and the back of the instrument.
The tailpiece anchors the strings to the lower bout of the violin by means of the tailgut, which loops around an ebony button called the tailpin (sometimes confusingly called the "endpin", like the cello's spike), which fits into a tapered hole in the bottom block. Very often the E string will have a fine tuning lever worked by a small screw turned by the fingers. Fine tuners may also be applied to the other strings, especially on a student instrument, and are sometimes built into the tailpiece.
At the scroll end, the strings wind around the tuning pegs in the pegbox. Strings usually have a colored silk wrapping at both ends, for identification and to provide friction against the pegs. The tapered pegs allow friction to be increased or decreased by the player applying appropriate pressure along the axis of the peg while turning it.
Strings.
Strings were first made of sheep gut (commonly known as catgut), or simply gut, which was stretched, dried, and twisted. In the early years of the 20th century, strings were made of either gut, silk, aluminum, or steel. Modern strings may be gut, solid steel, stranded steel, or various synthetic materials, wound with various metals, and sometimes plated with silver. Most E strings are unwound, either plain or gold-plated steel. Currently, violin strings are not made with gut as much, but many performers use them to achieve a specific sound especially in historically informed performance.
Strings have a limited lifetime. Apart from obvious things, such as the winding of a string coming undone from wear, players generally change a string when it no longer plays true, losing the desired tone. String longevity depends on string quality and playing intensity.
Pitch range.
The lowest note of a violin, tuned normally, is G3, or G below middle C. (On rare occasions, the lowest string may be tuned down by as much as a fourth, to D3.) The highest note is less well defined: E7, the E two octaves above the open string (which is tuned to E5) may be considered a practical limit for orchestral violin parts, but it is often possible to play higher, depending on the length of the fingerboard and the skill of the violinist. Yet higher notes (up to C8) can be sounded using harmonics, either natural or artificial.
Acoustics.
The arched shape, the thickness of the wood, and its physical qualities govern the sound of a violin. Patterns of the node made by sand or glitter sprinkled on the plates with the plate vibrated at certain frequencies, called "Chladni patterns", are occasionally used by luthiers to verify their work before assembling the instrument.
Sizes.
The history of small violins is not well documented. Small violins were made at least during the late Renaissance Period and quite probably into the Baroque period that were a fourth higher in pitch than standard violins. These violins could be used either by children, or by musicians who had parts that were then outside of the range of standard violins. It is important to remember that the chin rest was a relatively recent invention. Without the chin rest, shifting into upper positions or back down from higher positions often resulted in the musician losing control of the violin. Additionally, some people have speculated that these fractional violins could have been used instead of Dancing master's violins (also called "kits" or "pochettes"). These early fractional violins are easily confused with children-sized violins, but, if confirmed by an expert, are highly sought by collectors and museums. During the later part of the 19th century and early part of the 20th century, makers in Saxony produced many of these fractional violins.
Apart from the standard, "full" (4/4) size, violins are also made in so-called "fractional" sizes of 7/8, 3/4, 1/2, 1/4, 1/8, 1/10, 1/16, 1/32 and even 1/64. These smaller instruments are commonly used by young players, whose fingers are not long enough to reach the correct positions on full-sized instruments.
While related in some sense to the dimensions of the instruments, the fractional sizes are not intended to be literal descriptions of relative proportions. For example, a 3/4-sized instrument is "not" three-quarters the length of a full size instrument. The body length (not including the neck) of a full-size, or 4/4, violin is 356 mm (about 14 inches), smaller in some 17th-century models. A 3/4 violin's body length is 335 mm (about 13 inches), and a 1/2 size is 310 mm (about 12 inches). With the violin's closest family member, the viola, size is specified as body length in inches or centimeters rather than fractional sizes. A full-size viola averages 16 inches (40 cm).
Occasionally, an adult with a small frame may use a so-called 7/8 size violin instead of a full-size instrument. Sometimes called a "lady's violin", these instruments are slightly shorter than a full size violin, but tend to be high-quality instruments capable of producing a sound that is comparable to that of fine full size violins.
Mezzo violin.
The instrument which corresponds to the violin in the violin octet is the mezzo violin, tuned the same as a violin but slightly longer; the strings of the mezzo violin are however of the same length as those of the standard violin.
Tuning.
Violins are tuned by turning the pegs in the pegbox under the scroll, or by adjusting the "fine tuner" screws at the tailpiece. All violins have pegs; fine tuners (also called "fine adjusters") are optional. Most fine tuners consist of a metal screw that moves a lever attached to the string end. They permit very small pitch adjustments much more easily than the pegs. By turning one clockwise, the pitch becomes sharper and turning one counterclockwise, the pitch becomes flatter.
Fine tuners on all four of the strings are a practical necessity for playing steel-core strings, and some players use them with synthetic strings as well. Since modern E strings are steel, a fine tuner is typically fitted for that string. Fine tuners are not used with gut strings, which are more elastic than steel or synthetic-core strings and do not respond adequately to the very small movements of fine tuners.
To tune a violin, the A string is first tuned to a standard pitch (usually 440 Hz). (When accompanying a fixed-pitch instrument such as a piano or accordion, the violin tunes to it.) The other strings are then tuned against each other in intervals of perfect fifths by bowing them in pairs. A minutely higher tuning is sometimes employed for solo playing to give the instrument a brighter sound; conversely, Baroque music is sometimes played using lower tunings to make the violin's sound more gentle. After tuning, the instrument's bridge may be examined to ensure that it is standing straight and centered between the inner nicks of the f-holes; a crooked bridge may significantly affect the sound of an otherwise well-made violin.
The tuning G-D-A-E is used for most violin music. Other tunings are occasionally employed; the G string, for example, can be tuned up to A. The use of nonstandard tunings in classical music is known as "scordatura"; in some folk styles, it is called "cross tuning". One famous example of scordatura in classical music is Saint-Saëns' "Danse Macabre", where the solo violin's E string is tuned down to E flat to impart an eerie dissonance to the composition. Another example is in the third movement of "Contrasts", by Béla Bartók, where the E string is tuned down to E flat and the G tuned to a G sharp, or the set of pieces called the Mystery Sonatas by Biber.
In Indian classical music and Indian light music, the violin is likely to be tuned to D♯-A♯-D♯-A♯ in the South Indian style. As there is no concept of absolute pitch in Indian classical music, any convenient tuning maintaining these relative pitch intervals between the strings can be used. Another prevalent tuning with these intervals is B♭-F-B♭-F, which corresponds to Sa-Pa-Sa-Pa in the Indian carnatic classical music style. In the North Indian Hindustani style, the tuning is usually Pa-Sa-Pa-Sa instead of Sa-Pa-Sa-Pa. This could correspond to F-B♭-F-B♭, for instance.
In Iranian classical music and Iranian light music,the violin ls different tunings in any Dastgah, the violin is likely to be tuned (E-A-E-A) in Dastgah-h Esfahan or in Dastgāh-e Šur is (E-A-D-E) and (E-A-E-E), in Dastgāh-e Māhur is (E-A-D-A).
In Arabic classical music, the A and E strings are lowered by a whole step i.e. G-D-G-D. This is to ease playing Arabic maqams, especially those containing quarter tones.
While most violins have four strings, there are violins with as many as seven strings. The extra strings on such violins typically are lower in pitch than the G-string; these strings are usually tuned to C, F, and B flat. If the instrument's playing length, or string length from nut to bridge, is equal to that of an ordinary full-scale violin; i.e., a bit less than 13 in, then it may be properly termed a violin. Some such instruments are somewhat longer and should be regarded as violas. Violins with five strings or more are typically used in jazz or folk music.
Bows.
A violin is usually played using a bow consisting of a stick with a ribbon of horsehair strung between the tip and frog (or nut, or heel) at opposite ends. A typical violin bow may be 75 cm (29 inches) overall, and weigh about 60 g. Viola bows may be about 5 mm shorter and 10 g heavier.
At the frog end, a screw adjuster tightens or loosens the hair. Just forward of the frog, a leather thumb cushion and winding protect the stick and provide a strong grip for the player's hand. The winding may be wire (often silver or plated silver), silk, or whalebone (now imitated by alternating strips of tan and black plastic.) Some student bows (particularly the ones made of solid fiberglass) substitute a plastic sleeve for grip and winding.
The hair of the bow traditionally comes from the tail of a grey male horse (which has predominantly white hair), though some cheaper bows use synthetic fiber. Occasional rubbing with rosin makes the hair grip the strings intermittently, causing them to vibrate. Originally the stick was made out of snakewood, but modern day bows are now traditionally made of brazilwood, although a stick made from a more select quality (and more expensive) brazilwood is called pernambuco. Both types come from the same tree species. Some student bows are made of fiberglass or various inexpensive woods. Some recent bow design innovations use carbon fiber for the stick, at all levels of craftsmanship.
Playing.
Posture.
Western.
The violin is played either seated or standing up. Solo players (whether playing alone, with a piano or with an orchestra) play mostly standing up (unless prevented by a physical handicap such as in the case of Itzhak Perlman), while in the orchestra and in chamber music it is played seated. Recently some orchestras performing Baroque music (such as the Freiburg Baroque Orchestra) have had all of their violins and violas, solo and ensemble, perform standing up.
The standard way of holding the violin is with the left side of the jaw resting on the chinrest of the violin, and supported by the left shoulder, often assisted by a shoulder rest (or a sponge and an elastic band for younger players who struggle with shoulder rests). The jaw and the shoulder must hold the violin firmly enough to allow it to remain stable when the left hand goes from a high position to a low one. (In the Indian posture the stability of the violin is guaranteed by its scroll resting on the side of the foot).
While all authorities insist on the vital importance of good posture both for the sake of the quality of the playing and to reduce the chance of repetitive strain injury,
advice as to what good posture is and how to achieve it differs in details. However all insist on the importance of a natural relaxed position without tension or rigidity. Things which are almost universally recommended is keeping the left wrist straight (or very nearly so) to allow the fingers of the left hand to move freely and to reduce the chance of injury and keeping either shoulder in a natural relaxed position and avoiding raising either of them in an exaggerated manner. This, like any other unwarranted tension, would limit freedom of motion, and increase the risk of injury.
Left hand and pitch production.
The left hand regulates the sounding length of the string by stopping it against the fingerboard with the fingertips, producing different pitches.
As the violin has no frets to stop the strings, the player must know exactly where to place the fingers on the strings to play with good intonation. Through practice and ear training, the violinist's left hand finds the notes intuitively by muscle memory. Beginners sometimes rely on tapes placed on the fingerboard for proper left hand finger placement, but usually abandon the tapes quickly as they advance. Another commonly used marking technique uses dots of white-out on the fingerboard, which wear off in a few weeks of regular practice. This practice, unfortunately, is used sometimes in lieu of adequate ear-training, guiding the placement of fingers by eye and not by ear. Especially in the early stages of learning to play, the so-called ringing tones are useful. There are nine such notes in first position, where a stopped note sounds a unison or octave with another (open) string, causing it to resonate sympathetically.
Since violins are tuned in perfect fifths, and each subsequent note is stopped at a pitch the player perceives as the most harmonious, "when unaccompanied, [a violinist] does not play consistently in either the tempered or the natural [just] scale, but tends on the whole to conform with the Pythagorean scale." When playing with an instrument tuned to equal temperament, such as a piano, skilled violinists adjust their tuning to avoid discordant notes.
The fingers are conventionally numbered 1 (index) through 4 (little finger). Especially in instructional editions of violin music, numbers over the notes may indicate which finger to use, with "0" indicating an open string. The chart to the right shows the arrangement of notes reachable in first position. Not shown on this chart is the way the spacing between note positions becomes closer as the fingers move up (in pitch) from the nut. The bars at the sides of the chart represent the usual possibilities for beginners' tape placements, at 1st, high 2nd, 3rd, and 4th fingers.
Positions.
The placement of the left hand on the fingerboard is characterized by "positions". First position, where most beginners start (although some methods start in third position), is the most commonly used position in string music. The lowest note available in this position in standard tuning is an open G; the highest note in first position is played with the fourth finger on the E-string, sounding a B, or reaching up a half step (also known as the "extended fourth finger") to the C two octaves above middle C.
Moving the hand up the neck, so the first finger takes the place of the second finger, brings the player into "second position". Letting the first finger take the first-position place of the third finger brings the player to "third position", and so on. The upper limit of the violin's range is largely determined by the skill of the player, who may easily play more than two octaves on a single string, and four octaves on the instrument as a whole, although when a violinist has progressed to the point of being able to use the entire range of the instrument, references to particular positions become less common. Position names are mostly used for the lower positions and in method books; for this reason, it is uncommon to hear references to anything higher than seventh position. The lowest position on a violin is half-position, where the first finger is a half-step away from the nut. This position is less frequently used. The highest position, practically speaking, is 15th position.
Moving between positions is called "shifting". The player moves from position to position by typically using a guide finger. For example, when a player shifts from first to fourth position, they will use the last finger they used in first position as the guide finger. Then, the player moves their entire hand to fourth position, but with the last finger used in first position guiding the hand. The guide finger should not press on the string during the shift; it should only glide down the string. This guide finger moves to its respective spot in fourth position, but does not press down on the string. Then, the finger that plays the note after the shift should be pressed onto the string and the bow is moved to sound the note.
The same note may sound different, depending on which string is used to play it. Sometimes a composer or arranger specifies the string to use for a particular tone quality. This is indicated in the music by the marking, for example, "sul G", meaning to play on the G string. For example, playing very high up on the lower strings gives a distinctive quality to the sound. Otherwise, moving into different positions is usually done for ease of playing.
Open strings.
Bowing or plucking an "open string" (that is, a string played without any finger stopping it) gives a different sound from a stopped string, since the string vibrates more freely at the nut than under a finger. Other than the low G (which can be played in no other way), open strings are generally avoided in some styles of classical playing. This is because they have a somewhat harsher sound (especially open E) and it is not possible to directly use vibrato on an open string. However, this can be partially compensated by applying vibrato on a note that is an octave higher than the open string.
In some cases playing an open string is called for by the composer (and explicitly marked in the music) for special effect, decided upon by the musician for artistic reasons (common in earlier works such as Bach), or played in a fast passage, where they usually cannot be distinguished.
Playing an open string simultaneously with a stopped note on an adjacent string produces a bagpipe-like drone, often used by composers in imitation of folk music. Sometimes the two notes are identical (for instance, playing a fingered A on the D string against the open A string), giving a ringing sort of "fiddling" sound. Playing an open string simultaneously with an identical stopped note can also be called for when more volume is required, especially in orchestral playing.
Double stops and drones.
Double stopping is when two separate strings are stopped by the fingers, and bowed simultaneously, producing a sixth, third, fifth, etc. harmony. Sometimes moving to a higher position is necessary for the left hand to be able to reach both notes at once. Sounding an open string alongside a fingered note is another way to get a partial chord. While sometimes also called a double stop, it is more properly called a drone, as the drone note may be sustained for a passage of different notes played on the adjacent string. Three or four notes can also be played at one time (triple and quadruple stops, respectively), and, according to the style of music, the notes might all be played simultaneously or might be played as two successive double stops, favoring the higher notes.
Vibrato.
Vibrato is a technique of the left hand and arm in which the pitch of a note varies in a pulsating rhythm. While various parts of the hand or arm may be involved in the motion, the end result is a movement of the fingertip bringing about a slight change in vibrating string length. Some violinists oscillate backwards, or lower in pitch from the actual note when using vibrato, since it is believed that perception favors the highest pitch in a varying sound. Vibrato does little, if anything, to disguise an out-of-tune note; in other words, misapplied vibrato is a poor substitute for good intonation. Scales and other exercises meant to work on intonation are typically played without vibrato to make the work easier and more effective. Music students are often taught that unless otherwise marked in music, vibrato is assumed. This can be an obstacle to a classically trained violinist wishing to play in a style that uses little or no vibrato at all, such as baroque music played in period style and many traditional fiddling styles.
Vibrato can be produced by a proper combination of finger, wrist and arm motions. One method, called "hand vibrato", involves rocking the hand back at the wrist to achieve oscillation, while another method, "arm vibrato", modulates the pitch by rocking at the elbow. A combination of these techniques allows a player to produce a large variety of tonal effects.
The "when" and "what for" of violin vibrato are artistic matters of style and taste. For example if you overdo the variation of the note's tone it may become very distracting and overwhelm the piece. In acoustic terms, the interest that vibrato adds to the sound has to do with the way that the overtone mix (or tone color, or timbre) and the directional pattern of sound projection change with changes in pitch. By "pointing" the sound at different parts of the room in a rhythmic way, vibrato adds a "shimmer" or "liveliness" to the sound of a well-made violin. Vibrato is, in a large part, left to the discretion of the violinist. Different types of vibrato will bring different moods to the piece, and the varying degrees and styles of vibrato are often characteristics that stand out in well-known violinists.
Vibrato trill.
Vibrato can also be used for a fast trill. A trill initiated from just hammering the finger up and down on the fingerboard will create a harsher quality than with a vibrato trill. For example, if trilling on the first finger, the second finger is placed very slightly off the string and vibrato is implemented. The second finger will lightly touch the string above the first finger causing the pitch to change. This has a softer quality and many think it is nicer-sounding than a hammered trill. Note - this trill technique only works well for semi-tonal trills, it is far more difficult to vibrato trill for an interval of a tone or more.
Harmonics.
Lightly touching the string with a fingertip at a harmonic node creates harmonics. Instead of the normal tone, a higher pitched note sounds. Each node is at an integer division of the string, for example half-way or one-third along the length of the string. A responsive instrument will sound numerous possible harmonic nodes along the length of the string. Harmonics are marked in music either with a little circle above the note that determines the pitch of the harmonic, or by diamond-shaped note heads. There are two types of harmonics: natural harmonics and artificial harmonics (also known as "false harmonics").
Natural harmonics are played on an open string. The pitch of the open string is called the fundamental frequency. Harmonics are also called "overtones". They occur at whole-number multiples of the fundamental, which is called the first harmonic. The second harmonic is the first overtone, the third harmonic is the second overtone, and so on. The second harmonic is in the middle of the string and sounds an octave higher than the string's pitch. The third harmonic breaks the string into thirds and sounds an octave and a fifth above the fundamental, and the fourth harmonic breaks the string into quarters sounding two octaves above the first. The sound of the second harmonic is the clearest of them all, because it is a common node with all the succeeding even-numbered harmonics (4th, 6th, etc.). The third and succeeding odd-numbered harmonics are harder to play because they break the string into an odd number of vibrating parts and do not share as many nodes with other harmonics.
Artificial harmonics are more difficult to produce than natural harmonics, as they involve both stopping the string and playing a harmonic on the stopped note. Using the "octave frame" (the normal distance between the first and fourth fingers in any given position) with the fourth finger just touching the string a fourth higher than the stopped note produces the fourth harmonic, two octaves above the stopped note. Finger placement and pressure, as well as bow speed, pressure, and sounding point are all essential in getting the desired harmonic to sound. And to add to the challenge, in passages with different notes played as false harmonics, the distance between stopping finger and harmonic finger must constantly change, since the spacing between notes changes along the length of the string.
The "harmonic finger" can also touch at a major third above the pressed note (the fifth harmonic), or a fifth higher (a third harmonic). These harmonics are less commonly used; in the case of the major third, both the stopped note and touched note must be played slightly sharp otherwise the harmonic does not speak as readily. In the case of the fifth, the stretch is greater than is comfortable for many violinists. In the general repertoire fractions smaller than a sixth are not used. However, divisions up to an eighth are sometimes used and, given a good instrument and a skilled player, divisions as small as a twelfth are possible.
There are a few books dedicated solely to the study of violin harmonics. Two comprehensive works are Henryk Heller's seven-volume "Theory of Harmonics", published by Simrock in 1928, and Michelangelo Abbado's five-volume "Tecnica dei suoni armonici" published by Ricordi in 1934.
Elaborate passages in artificial harmonics can be found in virtuoso violin literature, especially of the 19th and early 20th centuries. Two notable examples of this are an entire section of Vittorio Monti's "Csárdás" and a passage towards the middle of the third movement of Pyotr Ilyich Tchaikovsky's Violin Concerto. A section of the third movement of Violin Concerto No. 1 (Paganini) consists of double-stop (!) thirds in harmonics.
Right hand and tone color.
The strings may be sounded by drawing the hair of the bow held by the right hand across them "(arco)" or by plucking them "(pizzicato)" most often with the right hand.
The right arm, hand, and bow are responsible for tone quality, rhythm, dynamics, articulation, and most (but not all) changes in timbre.
Bowing techniques.
The most essential part of bowing technique is the bow grip. It is usually with the thumb bent in the small area between the frog and the winding of the bow. The other fingers are spread somewhat evenly across the top part of the bow. The pinky finger is curled with the tip of the finger placed on the wood next to the screw.
The violin produces louder notes with greater bow speed or more weight on the string. The two methods are not equivalent, because they produce different timbres; pressing down on the string tends to produce a harsher, more intense sound. One can also achieve a louder sound by placing the bow closer to the bridge.
The sounding point where the bow intersects the string also influences timbre. Playing close to the bridge ("sul ponticello") gives a more intense sound than usual, emphasizing the higher harmonics; and playing with the bow over the end of the fingerboard ("sul tasto") makes for a delicate, ethereal sound, emphasizing the fundamental frequency. Dr. Suzuki referred to the sounding point as the "Kreisler highway"; one may think of different sounding points as "lanes" in the highway.
Various methods of attack with the bow produce different articulations. There are many bowing techniques that allow for every range of playing style and many teachers, players, and orchestras spend a lot of time developing techniques and creating a unified technique within the group. These techniques include legato-style bowing, collé, ricochet, sautillé, martelé, spiccato, and staccato.
Pizzicato.
A note marked "pizz." (abbreviation for "pizzicato") in the written music is to be played by plucking the string with a finger of the right hand rather than by bowing. (The index finger is most commonly used here.) Sometimes in virtuoso solo music where the bow hand is occupied (or for show-off effect), "left-hand pizzicato" will be indicated by a "+" (plus sign) below or above the note. In left-hand pizzicato, two fingers are put on the string; one (usually the index or middle finger) is put on the correct note, and the other (usually the ring finger or little finger) is put above the note. The higher finger then plucks the string while the lower one stays on, thus producing the correct pitch. By increasing the force of the pluck, one can increase the volume of the note that the string is producing.
Col legno.
A marking of "col legno" (Italian for "with the wood") in the written music calls for striking the string(s) with the stick of the bow, rather than by drawing the hair of the bow across the strings. This bowing technique is somewhat rarely used, and results in a muted percussive sound. The eerie quality of a violin section playing "col legno" is exploited in some symphonic pieces, notably the "Witches' Dance" of the last movement of Berlioz's Symphonie Fantastique. Saint-Saëns's symphonic poem "Danse Macabre" includes the string section using the "col legno" technique to imitate the sound of dancing skeletons. "Mars" from Gustav Holst's "The Planets" uses "col legno" to play a repeated rhythm in 5/4 time signature. Benjamin Britten's "The Young Person's Guide to the Orchestra" demands its use in the "Percussion" Variation. Dmitri Shostakovich uses it in his Fourteenth Symphony in the movement 'At the Sante Jail'. Some violinists, however, object to this style of playing as it can damage the finish and impair the value of a fine bow, but most of such will compromise by using a cheap bow for at least the duration of the passage in question.
Martelé.
Literally "hammered", a strongly accented effect produced by releasing each bowstroke forcefully and suddenly. Martelé can be played in any part of the bow. It is sometimes indicated in written music by an arrowhead.
Tremolo.
Very rapid repetition (typically of a single note, but occasionally of multiple notes), usually played at the tip of the bow. Tremolo is marked with three short, slanted lines across the stem of the note.
Mute or "sordino".
Attaching a small metal, rubber, leather, or wooden device called a "mute", or "sordino", to the bridge of the violin gives a softer, more mellow tone, with fewer audible overtones; the sound of an entire orchestral string section playing with mutes has a hushed quality. The conventional Italian markings for mute usage are "con sord.", or "con sordina", meaning 'with mute'; and "senza sord.", meaning 'without mute'; or "via sord.", meaning 'mute off'. Larger metal, rubber, or wooden mutes are widely available, known as "practice mutes" or "hotel mutes". Such mutes are generally not used in performance, but are used to deaden the sound of the violin in practice areas such as hotel rooms. (For practicing purposes there is also the mute violin, a violin without a sound box.) Some composers have used practice mutes for special effect, for example, at the end of Luciano Berio's "Sequenza VIII" for solo violin.
Musical styles.
Classical music.
Since the Baroque era, the violin has been one of the most important of all instruments in classical music, for several reasons. The tone of the violin stands out above other instruments, making it appropriate for playing a melody line. In the hands of a good player, the violin is extremely agile, and can execute rapid and difficult sequences of notes.
Violins make up a large part of an orchestra, and are usually divided into two sections, known as the first and second violins. Composers often assign the melody to the first violins, typically a more difficult part using higher positions, while second violins play harmony, accompaniment patterns or the melody an octave lower than the first violins. A string quartet similarly has parts for first and second violins, as well as a viola part, and a bass instrument, such as the cello or, rarely, the double bass.
Jazz.
The earliest references to jazz performance using the violin as a solo instrument are documented during the first decades of the 20th century. Joe Venuti, one of the first jazz violinists, is known for his work with guitarist Eddie Lang during the 1920s. Since that time there have been many improvising violinists including Stéphane Grappelli, Stuff Smith, Eddie South, Regina Carter, Johnny Frigo, John Blake, Adam Taubitz and Jean-Luc Ponty. While not primarily jazz violinists, Darol Anger and Mark O'Connor have spent significant parts of their careers playing jazz.
Violins also appear in ensembles supplying orchestral backgrounds to many jazz recordings.
Popular music.
Up through at least the 1970s, most types of popular music used bowed strings. They were extensively used in popular music throughout the 1920s and early 1930s. With the rise of swing music, however, from 1935 to 1945, the string sound was deemed inappropriate to the improvised style of swing music, and violin usage declined significantly. Following the swing era, from the late 1940s to the mid-1950s, strings began to be revived in traditional pop music. This trend accelerated in the late 1960s, with a significant revival of the use of strings, especially in soul music. Popular Motown recordings of the late 1960s and 1970s relied heavily on strings as part of their trademark texture. The rise of disco music in the 1970s continued this trend with the heavy use of string instruments in popular disco orchestras (e.g., Love Unlimited Orchestra, Biddu Orchestra, Monster Orchestra, Salsoul Orchestra, MFSB).
With the rise of electronically created music in the 1980s, violins declined in use, as synthesized string sections took their place. However, while the violin has had very little usage in mainstream rock music, it has some history in progressive rock (e.g., Electric Light Orchestra, King Crimson, Kansas, Gentle Giant). The 1973 album "" by Italy's RDM plays violins off against synthesizers at its finale ("La grande fuga").
The instrument has a stronger place in modern fusion bands, notably The Corrs. The fiddle has also always been a part of British folk-rock music, as exemplified by the likes of Fairport Convention and Steeleye Span.
The popularity of crossover music beginning in the last years of the 20th century has brought the violin back into the popular music arena, with both electric and acoustic violins being used by popular bands. Vanessa Mae uses classical music with her electric violin. Dave Matthews Band features violinist Boyd Tinsley. The Flock featured violinist Jerry Goodman who later joined the jazz-rock fusion band, The Mahavishnu Orchestra. James' Saul Davies, who is also a guitarist, was enlisted by the band as a violinist. For their first three albums and related singles, the British group No-Man made extensive use of electric and acoustic solo violin as played by band member Ben Coleman (who played violin exclusively).
Pop-Punk band Yellowcard has made a mainstay of violin in its music. Violinist Sean Mackin has been a member of the band since 1997. Los Salvadores also combine punk and ska influences with a violin.
Doom metal band My Dying Bride have used violin as a part of their line-up throughout many of their albums.
The violin appears prominently in the music of Spanish folk metal group Mägo de Oz (for example, in their 1998 hit "Molinos de viento"). The violinist (Carlos Prieto aka "Mohamed") has been one of the group's most popular members with fans since 1992.
The instrument is also used often in Symphonic Metal, particularly by bands such as Therion, Nightwish, Within Temptation, Haggard, and Epica, although it can also be found in Gothic Metal bands such as Tristania and Theater of Tragedy.
The alternative rock band Hurt's vocalist plays violin for the band, making them one of few rock bands to feature violin without hiring a session worker.
Independent artists, such as Owen Pallett, The Shondes, and Andrew Bird, have also spurred increased interest in the instrument. Indie bands have often embraced new and unusual arrangements, allowing them more freedom to feature the violin than many mainstream musical artists. It has been used in the post-rock genre by bands such as A Genuine Freakshow, Sigur Rós, Zox, Broken Social Scene, and A Silver Mt. Zion. The electric violin has even been used by bands like The Crüxshadows within the context of keyboard based music.
Lindsey Stirling, one of the 2010 "America's Got Talent" quarter-finalists and popular YouTube artist, has played the violin in conjunction with dubstep/trance rifts.
Eric Stanley, popular YouTube artist, has remixed hip hop and rap songs on the violin by artists such as Eminem and Drake.
The successful Indie Rock and Baroque Pop band Arcade Fire use violins extensively in their arrangements.
Indian, Pakistani, Turkish, and Arabic pop music is filled with the sound of violins, both soloists and ensembles.
Folk music and fiddling.
Like many other instruments used in classical music, the violin descends from remote ancestors that were used for folk music. Following a stage of intensive development in the late Renaissance, largely in Italy, the violin had improved (in volume, tone, and agility), to the point that it not only became a very important instrument in art music, but proved highly appealing to folk musicians as well, ultimately spreading very widely, sometimes displacing earlier bowed instruments. Ethnomusicologists have observed its widespread use in Europe, Asia, and the Americas.
In many traditions of folk music, the tunes are not written but are memorized by successive generations of musicians and passed on, in what is known as the oral tradition.
Arabic music.
As well as the Arabic rababah, the violin has been used in Arabic music.
Fiddle.
When played as a folk instrument, the violin is usually referred to in English as a "fiddle" (although the term "fiddle" can be used informally no matter what the genre of music.) A fiddle is essentially the same as a classical violin. Many old-time pieces call for cross-tuning the instrument out of standard GDAE tuning. Some players of American styles of folk fiddling (such as bluegrass or old-time) have their bridge's top edge cut to a slightly flatter curve, making techniques such as a "double shuffle" less taxing on the bow arm. A flatter bridge top reduces the range of motion needed for alternating between double stops on different string pairs. Fiddle players who use solid steel core strings may prefer to use a tailpiece with fine tuners on all four strings, instead of the single fine tuner on the E string used by many classical players.
Electric violins.
Electric violins have a magnetic or piezoelectric pickup that converts string vibration to an electric signal. A cable or transmitter sends the signal to an amplifier. Electric violins are usually constructed as such, but a pickup can be added to a conventional acoustic violin.
An electric violin with a resonating body that produces listening-level sound independently of the electric elements can be called an "electro-acoustic violin". To be effective as an acoustic violin, electro-acoustic violins retain much of the resonating body of the violin, and often resemble an acoustic violin or fiddle. The body may be finished in bright colors and made from alternative materials to wood. These violins may need to be hooked up to an amplifier. Some types come with a silent option that allows the player to use headphones that are hooked up to the violin. The first specially built electric violins date back to 1928 and were made by Victor Pfeil, Oskar Vierling, George Eisenberg, Benjamin Miessner, George Beauchamp, Hugo Benioff and Fredray Kislingbury. These violins can create many acoustical effects much like a guitar, including distortion and delay.
Since electric violins do not rely on string tension and resonance to amplify their sound they can have more strings. For example five stringed electric violins are available from several manufacturers, and a seven string electric violin (with three lower strings encompassing the cello's range) is also available. The majority of the first electric violinists were musicians playing jazz and popular music.
Violin authentication.
Violin authentication is the process of determining the maker and manufacture date of a violin. This process is similar to that used to determine the provenance of art works. As significant value may be attached to violins made either by specific makers or at specific times and locations, forgery and other methods of fraudulent misrepresentation can be used to inflate the value of an instrument.
References.
</dl>

</doc>
<doc id="32430" url="http://en.wikipedia.org/wiki?curid=32430" title="Vieille Montagne">
Vieille Montagne

Vieille Montagne is the name of a former zinc mine in Kelmis (La Calamine), a town in Belgium between Liège and Aachen. The mine's name is French for "the old mountain", and this is also reflected in its German name, "Altenberg" (earlier, "Alten Galmei-Berg"). The mine was once a bone of contention between Germany on the one side and the village that became the neutral territory of Moresnet.
The mine was first opened in 1805 and continued its operation until the end of the nineteenth century, when a workforce of 300 produced 8,500 t of crude zinc annually.
The company opened a second zinc mine in Zinkgruvan in Sweden, which is still in operation. It also ran a harbour in Åmmeberg to ship the zinc. The ore was shipped to another affiliate in Balen, Belgium.
The company also had mines in the UK in Nenthead, a village in Cumbria, which were worked from 1896 until 1949. In the department of Ariège in France the VM company took the lease on the zinc mines at Bentaillou in the Pyrenees, also after World War II.
The name became synonymous with zinc oxide and with rolled zinc, especially for building applications. The company was the world's oldest and also largest zinc producer, producing at its peak at least 149,000 tonnes per year. In 1989 Vieille Montagne was merged into the Union Minière group, based in Belgium, which became Umicore in 2003. The group continues its rolled zinc activity under the brand VMZinc which still refers back to the historical link with Vieille Montagne.
Strike in Balen.
In 1971 it turned out the workmen in the other Belgian plants had a much higher salary for similar work. The workers in Balen wanted a raise. This led to a dispute between the workmen, the trade unions and the directors board. The 1500 workmen in Balen went on a strike, but this was not accepted by the trade unions. As a result, the workmen did not earn any money at all. The strike went on for 9 weeks. Many charity was set up by inhabitants of Mol and Balen. Jef Sleeckx, a politician, convinced the banks to cancel payments temporarily for affected workmen. Houselords did not ask rent during the strike. All kind of shops gave food for free or gave the impacted workmen a temporary job. At the end, the directors board agreed and the workmen even got a higher loan than requested. The events in Balen were written in the theatre play and novel "Groenten uit Balen" by Walter van den Broeck. The book was filmed in 2012. All events regarding the strike did happen, all referenced places do/did exist, and only the family Debreucker is a fictive one.

</doc>
<doc id="32431" url="http://en.wikipedia.org/wiki?curid=32431" title="Vanadium">
Vanadium

Vanadium is a chemical element with symbol V and atomic number 23. It is a hard, silvery gray, ductile and malleable transition metal. The element is found only in chemically combined form in nature, but once isolated artificially, the formation of an oxide layer stabilizes the free metal somewhat against further oxidation.
Andrés Manuel del Río discovered compounds of vanadium in 1801 in Mexico by analyzing a new lead-bearing mineral he called "brown lead," and presumed its qualities were due to the presence of a new element, which he named "erythronium" (Greek for "red") since, upon heating, most of its salts turned from their initial color to red. Four years later, however, he was (erroneously) convinced by other scientists that erythronium was identical to chromium. Chlorides of vanadium were generated in 1830 by Nils Gabriel Sefström who thereby proved that a new element was involved, which he named "vanadium" after the Scandinavian goddess of beauty and fertility, Vanadís (Freyja). Both names were attributed to the wide range of colors found in vanadium compounds. Del Rio's lead mineral was later renamed vanadinite for its vanadium content. In 1867 Henry Enfield Roscoe obtained the pure element.
Vanadium occurs naturally in about 65 different minerals and in fossil fuel deposits. It is produced in China and Russia from steel smelter slag; other countries produce it either from the flue dust of heavy oil, or as a byproduct of uranium mining. It is mainly used to produce specialty steel alloys such as high-speed tool steels. The most important industrial vanadium compound, vanadium pentoxide, is used as a catalyst for the production of sulfuric acid.
Large amounts of vanadium ions are found in a few organisms, possibly as a toxin. The oxide and some other salts of vanadium have moderate toxicity. Particularly in the ocean, vanadium is used by some life forms as an active center of enzymes, such as the vanadium bromoperoxidase of some ocean algae. Vanadium is probably a micronutrient in mammals, including humans, but its precise role in this regard is unknown.
History.
Vanadium was discovered by Andrés Manuel del Río, a Spanish-Mexican mineralogist, in 1801. Del Río extracted the element from a sample of Mexican "brown lead" ore, later named vanadinite. He found that its salts exhibit a wide variety of colors, and as a result he named the element "panchromium" (Greek: παγχρώμιο "all colors"). Later, Del Río renamed the element "erythronium" (Greek: ερυθρός "red") as most of its salts turned red upon heating. In 1805, the French chemist Hippolyte Victor Collet-Descotils, backed by del Río's friend Baron Alexander von Humboldt, incorrectly declared that del Río's new element was only an impure sample of chromium. Del Río accepted Collet-Descotils' statement and retracted his claim.
In 1831, the Swedish chemist Nils Gabriel Sefström rediscovered the element in a new oxide he found while working with iron ores. Later that same year, Friedrich Wöhler confirmed del Río's earlier work. Sefström chose a name beginning with V, which had not been assigned to any element yet. He called the element "vanadium" after Old Norse "Vanadís" (another name for the Norse Vanr goddess Freyja, whose facets include connections to beauty and fertility), because of the many beautifully colored chemical compounds it produces. In 1831, the geologist George William Featherstonhaugh suggested that vanadium should be renamed "rionium" after del Río, but this suggestion was not followed.
The isolation of vanadium metal proved difficult. In 1831, Berzelius reported the production of the metal, but Henry Enfield Roscoe showed that Berzelius had in fact produced the nitride, vanadium nitride (VN). Roscoe eventually produced the metal in 1867 by reduction of vanadium(II) chloride, VCl2, with hydrogen. In 1927, pure vanadium was produced by reducing vanadium pentoxide with calcium. The first large-scale industrial use of vanadium in steels was found in the chassis of the Ford Model T, inspired by French race cars. Vanadium steel allowed for reduced weight while simultaneously increasing tensile strength.
German chemist Martin Henze discovered vanadium in the blood cells (or coelomic cells) of Ascidiacea (sea squirts) in 1911.
Characteristics.
Vanadium is a medium-hard, ductile, steel-blue metal. Some sources describe vanadium as "soft", perhaps because it is ductile, malleable and not brittle. Vanadium is harder than most metals and steels (see Hardnesses of the elements (data page) and iron). It has good resistance to corrosion and it is stable against alkalis and sulfuric and hydrochloric acids. It is oxidized in air at about 933 K (660 °C, 1220 °F), although an oxide layer forms even at room temperature.
Isotopes.
Naturally occurring vanadium is composed of one stable isotope, 51V, and one radioactive isotope, 50V. The latter has a half-life of 1.5×1017 years and a natural abundance of 0.25%. 51V has a nuclear spin of 7/2, which is useful for NMR spectroscopy. Twenty-four artificial radioisotopes have been characterized, ranging in mass number from 40 to 65. The most stable of these isotopes are 49V, with a half-life of 330 days, and 48V, with a half-life of 16.0 days. The remaining radioactive isotopes have half-lives shorter than an hour, most below 10 seconds. At least four isotopes have metastable excited states. Electron capture is the main decay mode for isotopes lighter than 51V. For the heavier ones, the most common mode is beta decay. The electron capture reactions lead to the formation of element 22 (titanium) isotopes, while beta decay leads to element 24 (chromium) isotopes.
Compounds.
See also: .
The chemistry of vanadium is noteworthy for the accessibility of the four adjacent oxidation states 2-5. In aqueous solution, vanadium forms metal aquo complexes the colours are lilac [V(H2O)6]2+, green [V(H2O)6]3+, blue [VO(H2O)5]2+, yellow VO3−. Vanadium(II) compounds are reducing agents, and vanadium(V) compounds are oxidizing agents. Vanadium(IV) compounds often exist as vanadyl derivatives which contain the VO2+ center.
Ammonium vanadate(V) (NH4VO3) can be successively reduced with elemental zinc to obtain the different colors of vanadium in these four oxidation states. Lower oxidation states occur in compounds such as V(CO)6, [V(CO)6]- and substituted derivatives.
The most commercially important compound is vanadium pentoxide. It is used as a catalyst for the production of sulfuric acid. This compound oxidizes sulfur dioxide (SO2) to the trioxide (SO3). In this redox reaction, sulfur is oxidized from +4 to +6, and vanadium is reduced from +5 to +4:
The catalyst is regenerated by oxidation with air:
Similar oxidations are used in the production of maleic anhydride, phthalic anhydride, and several other bulk organic compounds.
Oxyanions.
In aqueous solution, vanadium(V) forms an extensive family of oxyanions. The interrelationships within this family are described by the predominance diagram, shows at least 11 species, depending on pH and concentration. The tetrahedral orthovanadate ion, VO43−, is the principal species present at pH 12-14. Analogies exist between orthovanadate and orthophosphate owing to the similarity in size and charge of phosphorus(V) and vanadium(V). Orthovanadate VO43− is used in protein crystallography to study the biochemistry of phosphate. The tetrathiovanadate [VS4]3− is analogous to the orthovanadate ion.
At lower pH's, the monomer [HVO4]2− and dimer [V2O7]− are formed, with the monomer predominant at vanadium concentration of less than ca. 10−2M (pV > 2; pV is equal to minus the logarithm of the total vanadium concentration/M). The formation of the divanadate ion is analogous to the formation of the dichromate ion. As the pH is reduced, further protonation and condensation to polyvanadates occur: at pH 4-6 [H2VO4]− is predominant at pV greater than ca. 4, while at higher concentrations trimers and tetramers are formed. Between pH 2-4 decavanadate predominates, its formation from orthovanadate is represented by this condensation reaction:
In decavanadate, each V(V) center is surrounded by six oxide ligands. Vanadic acid, H3VO4 exists only a very low concentrations because protonation of the tetrahedral species [H2VO4]− results in the preferential formation of the octahedral [VO2(H2O)4]+ species. In strongly acidic solutions, pH<2. [VO2(H2O)4]+ is the predominant species, while the oxide V2O5 precipitates from solution at high concentrations. The oxide is formally the inorganic anhydride of vanadic acid. The structures of many vanadate compounds have been characterized by X-ray crystallography.
The Pourbaix diagram for vanadium in water, which shows the redox potentials between various vanadium species in different oxidation states is also complex.
Vanadium(V) also forms various peroxo complexes, most notably in the active site of the vanadium-containing bromoperoxidase enzymes. The species VO(O)2(H2O)4+ is stable in acidic solutions. In alkaline solutions species with 2, 3 and 4 peroxide groups are known; the last forms violet salts with the formula M3V(O2)4 nH2O (M = Li, Na, etc.), in which the vanadium has an 8-coordinate dodecahedral structure.
Halide derivatives.
Twelve binary halides, compounds with the formula VXn, are known. VI4, VCl5, VBr5, and VI5 do not exist or are extremely unstable. In combination with other reagents, VCl4 is used as a catalyst for polymerization of dienes. Like all binary halides, those of vanadium are Lewis acidic, especially those of V(IV) and V(V). Many of the halides form octahedral complexes with the formula VX"n"L6−"n" (X = halide; L = other ligand).
Many vanadium oxyhalides (formula VOmXn) are known. The oxytrichloride and oxytrifluoride, VOF3) and VOCl3) are the most widely studied. Akin to POCl3, they are volatile, adopt tetrahedral structures in the gas phase, and are Lewis acidic.
Coordination compounds.
Complexes of vanadium(II) and (III) are relatively exchange inert and reducing. Those of V(IV) and V(V) are oxidants. Vanadium ion is rather large and some complexes achieve coordination numbers greater than 6, as is the case in [V(CN)7]4−. The coordination chemistry of V4+ is dominated by the vanadyl center, VO2+, which binds four other ligands strongly and one weakly (the one trans to the vanadyl center). An example is vanadyl acetylacetonate (V(O)(O2C5H7)2). In this complex, the vanadium is 5-coordinate, square pyramidal, meaning that a sixth ligand, such as pyridine, may be attached, though the association constant of this process is small. Many 5-coordinate vanadyl complexes have a trigonal bypyramidal geometry, such as VOCl2(NMe3)2. The coordination chemistry of V5+ is dominated by the polyoxovanadates, such as decavanadate.
Organometallic compounds.
Organometallic chemistry of vanadium is well developed, although they are mainly only academic significance. Vanadocene dichloride is a versatile starting reagent and even finds some applications in organic chemistry. Vanadium carbonyl, V(CO)6, is a rare example of a paramagnetic metal carbonyl. Reduction yields V(CO)6− (isoelectronic with Cr(CO)6), which may be further reduced with sodium in liquid ammonia to yield V(CO)53− (isoelectronic with Fe(CO)5).
Occurrence.
See also: .
Metallic vanadium is not found in nature, but vanadium compounds occur naturally in about 65 different minerals. Economically significant examples include patronite (VS4), vanadinite (Pb5(VO4)3Cl), and carnotite (K2(UO2)2(VO4)2·3H2O). Much of the world's vanadium production is sourced from vanadium-bearing magnetite found in ultramafic gabbro bodies. Vanadium is mined mostly in South Africa, north-western China, and eastern Russia. In 2010 these three countries mined more than 98% of the 56,000 tonnes of produced vanadium.
Vanadium is also present in bauxite and in fossil fuel deposits such as crude oil, coal, oil shale and tar sands. In crude oil, concentrations up to 1200 ppm have been reported. When such oil products are burned, the traces of vanadium may initiate corrosion in motors and boilers. An estimated 110,000 tonnes of vanadium per year are released into the atmosphere by burning fossil fuels. Vanadium has also been detected spectroscopically in light from the Sun and some other stars.
Production.
Most vanadium is used as an alloy called ferrovanadium as an additive to improve steels. Ferrovanadium is produced directly by reducing a mixture of vanadium oxide, iron oxides and iron in an electric furnace. The vanadium ends up in pig iron produced from vanadium bearing magnetite. Depending on the ore used, the slag contains up to 25% of vanadium.
Vanadium metal is obtained via a multistep process that begins with the roasting of crushed ore with NaCl or Na2CO3 at about 850 °C to give sodium metavanadate (NaVO3). An aqueous extract of this solid is acidified to give "red cake", a polyvanadate salt, which is reduced with calcium metal. As an alternative for small-scale production, vanadium pentoxide is reduced with hydrogen or magnesium. Many other methods are also in use, in all of which vanadium is produced as a byproduct of other processes. Purification of vanadium is possible by the crystal bar process developed by Anton Eduard van Arkel and Jan Hendrik de Boer in 1925. It involves the formation of the metal iodide, in this example vanadium(III) iodide, and the subsequent decomposition to yield pure metal.
Applications.
Alloys.
Approximately 85% of vanadium produced is used as ferrovanadium or as a steel additive. The considerable increase of strength in steel containing small amounts of vanadium was discovered in the beginning of the 20th century. Vanadium forms stable nitrides and carbides, resulting in a significant increase in the strength of the steel. From that time on vanadium steel was used for applications in axles, bicycle frames, crankshafts, gears, and other critical components. There are two groups of vanadium containing steel alloy groups. Vanadium high-carbon steel alloys contain 0.15% to 0.25% vanadium and high speed tool steels (HSS) have a vanadium content of 1% to 5%. For high speed tool steels, a hardness above HRC 60 can be achieved. HSS steel is used in surgical instruments and tools. Some powder metallurgic alloys can contain up to 18% percent vanadium. The high content of vanadium carbides in those alloys increases the wear resistivity significantly. One application for those alloys are tools and knives.
Vanadium stabilizes the beta form of titanium and increases the strength and temperature stability of titanium. Mixed with aluminium in titanium alloys it is used in jet engines, high-speed airframes and dental implants. One of the common alloys is Titanium 6AL-4V, a titanium alloy with 6% aluminium and 4% vanadium.
Other uses.
Vanadium is compatible with iron and titanium, therefore vanadium foil is used in cladding titanium to steel. The moderate thermal neutron-capture cross-section and the short half-life of the isotopes produced by neutron capture makes vanadium a suitable material for the inner structure of a fusion reactor. Several vanadium alloys show superconducting behavior. The first A15 phase superconductor was a vanadium compound, V3Si, which was discovered in 1952. Vanadium-gallium tape is used in superconducting magnets (17.5 teslas or 175,000 gauss). The structure of the superconducting A15 phase of V3Ga is similar to that of the more common Nb3Sn and Nb3Ti.
The most common oxide of vanadium, vanadium pentoxide V2O5, is used as a catalyst in manufacturing sulfuric acid by the contact process and as an oxidizer in maleic anhydride production. Vanadium pentoxide is also used in making ceramics. Another oxide of vanadium, vanadium dioxide VO2, is used in the production of glass coatings, which blocks infrared radiation (and not visible light) at a specific temperature. Vanadium oxide can be used to induce color centers in corundum to create simulated alexandrite jewelry, although alexandrite in nature is a chrysoberyl.
The possibility to use vanadium redox couples in both half-cells, thereby eliminating the problem of cross contamination by diffusion of ions across the membrane is the advantage of vanadium redox rechargeable batteries. Vanadate can be used for protecting steel against rust and corrosion by electrochemical conversion coating. Lithium vanadium oxide has been proposed for use as a high energy density anode for lithium ion batteries, at 745 Wh/L when paired with a lithium cobalt oxide cathode. It has been proposed by some researchers that a small amount, 40 to 270 ppm, of vanadium in Wootz steel and Damascus steel, significantly improves the strength of the material, although it is unclear what the source of the vanadium was. Lithium vanadium phosphate has been proposed for a new battery as well,and is very commercially applicable because phosphates are inexpensive and vanadium makes the battery very energy dense.
Biological role.
Vanadium plays a very limited role in human biology. It is more important in marine environments than terrestrial ones.
Vanadoenzymes.
A number of species of marine algae produce vanadium-containing vanadium bromoperoxidase as well as the closely related chloroperoxidase (which may use a heme or vanadium cofactor) and iodoperoxidases. The bromoperoxidase produces an estimated 1–2 million tons of bromoform and 56,000 tons of bromomethane annually. Most naturally occurring organobromine compounds, accounting arise by the action of this enzyme. They catalyse the following reaction (R-H is hydrocarbon substrate):
A vanadium nitrogenase is used by some nitrogen-fixing micro-organisms, such as "Azotobacter". In this role vanadium replaces more common molybdenum or iron, and gives the nitrogenase slightly different properties.
Vanadium accumulation in tunicates and ascidians.
Vanadium is essential to ascidians and tunicates, where it is stored in the highly acidified vacuoles of certain blood cell types, designated vanadocytes. Vanabins (vanadium binding proteins) have been identified in the cytoplasm of such cells. The concentration of vanadium in these ascidians' blood is up to ten million times higher than the concentration of vanadium in surrounding seawater, which normally contains 1 to 2 µg/l. The function of this vanadium concentration system, and these vanadium-containing proteins, is still unknown but the vanadocytes are later deposited just under the outer surface of the tunic where their presence may deter predation.
Fungi.
Several species of macrofungi, namely "Amanita muscaria" and related species, accumulate vanadium (up to 500 mg/kg in dry weight). Vanadium is present in the coordination complex amavadin, in fungal fruit-bodies. However, the biological importance of the accumulation process is unknown. Toxin functions or peroxidase enzyme functions have been suggested.
Mammals and birds.
Deficiencies in vanadium result in reduced growth and impaired reproduction in rats and chickens. Vanadium is a relatively controversial dietary supplement, used primarily for increasing insulin sensitivity and body-building. Whether it works for the latter purpose has not been proven; some evidence suggests that athletes who take it are merely experiencing a placebo effect. Vanadyl sulfate may improve glucose control in people with type 2 diabetes. Decavanadate and oxovanadates appear to play a role in a variety of biochemical processes, such as those relating to oxidative stress.
Safety.
All vanadium compounds should be considered toxic. Tetravalent VOSO4 has been reported to be over 5 times more toxic than trivalent V2O3. The Occupational Safety and Health Administration (OSHA) has set an exposure limit of 0.05 mg/m3 for vanadium pentoxide dust and 0.1 mg/m3 for vanadium pentoxide fumes in workplace air for an 8-hour workday, 40-hour work week. The National Institute for Occupational Safety and Health (NIOSH) has recommended that 35 mg/m3 of vanadium be considered immediately dangerous to life and health. This is the exposure level of a chemical that is likely to cause permanent health problems or death.
Vanadium compounds are poorly absorbed through the gastrointestinal system. Inhalation exposures to vanadium and vanadium compounds result primarily in adverse effects on the respiratory system. Quantitative data are, however, insufficient to derive a subchronic or chronic inhalation reference dose. Other effects have been reported after oral or inhalation exposures on blood parameters, on liver, on neurological development in rats, and other organs.
There is little evidence that vanadium or vanadium compounds are reproductive toxins or teratogens. Vanadium pentoxide was reported to be carcinogenic in male rats and male and female mice by inhalation in an NTP study, although the interpretation of the results has recently been disputed. Vanadium has not been classified as to carcinogenicity by the United States Environmental Protection Agency.
Vanadium traces in diesel fuels present a corrosion hazard; it is the main fuel component influencing high temperature corrosion. During combustion, it oxidizes and reacts with sodium and sulfur, yielding vanadate compounds with melting points down to 530 °C, which attack the passivation layer on steel, rendering it susceptible to corrosion. The solid vanadium compounds also cause abrasion of engine components.

</doc>
<doc id="32628" url="http://en.wikipedia.org/wiki?curid=32628" title="VisiCalc">
VisiCalc

VisiCalc was the first spreadsheet computer program for personal computers, originally released for the Apple II. It is often considered the application that turned the microcomputer from a hobby for computer enthusiasts into a serious business tool, and is considered the Apple II's killer app. VisiCalc sold over 700,000 copies in six years, and as many as 1 million copies over its history.
VisiCalc was ported to numerous platforms, both 8-bit and some of the early 16-bit systems. In order to do this, the company developed porting platforms that produced bug compatible versions. The company took the same approach when the IBM PC was launched, producing a product that was essentially identical to the original 8-bit Apple II version. Sales were initially brisk, with about 300,000 copies sold.
VisiCalc used the A1 notation in formulas.
When Lotus 1-2-3 was launched in 1983, taking full advantage of the expanded memory and screen of the PC, VisiCalc sales practically ended overnight. Sales imploded so rapidly that the company was soon insolvent. Lotus Development purchased the company in 1985, and immediately ended sales of VisiCalc and the company's other products.
History.
VISICALC represented a new idea of a way to use a computer and a new way of thinking about the world. Where conventional programming was thought of as a sequence of steps, this new thing was no longer sequential in effect: When you made a change in one place, all other things changed instantly and automatically. 
— Ted Nelson
VisiCalc traces its history to a presentation that Dan Bricklin was watching while attending Harvard Business School. The professor was creating a financial model on a blackboard that was ruled with lines to create a table, and formulas and data were being written into the cells. When the professor found an error or wanted to change a parameter, he had to erase and rewrite a number of sequential entries in the table. Bricklin realized that he could replicate the process on a computer using an "electronic spreadsheet" to view results of underlying formulae.
Bricklin was joined by Bob Frankston, and the pair worked on VisiCalc for two months during the winter of 1978–79, forming Software Arts. Bricklin wrote, "[W]ith the years of experience we had at the time we created VisiCalc, we were familiar with many row/column financial programs. In fact, Bob had worked since the 1960s at Interactive Data Corporation, a major timesharing utility that was used for some of them and I was exposed to some at Harvard Business School in one of the classes." Bricklin is referring to the variety of report generators that were in use at that time, including Business Planning Language (BPL) from International Timesharing Corporation (ITS) and Foresight, from Foresight Systems. However, these earlier timesharing programs were not completely interactive, nor did they run on personal computers.
Frankston described VisiCalc as a "magic sheet of paper that can perform calculations and recalculations", which "allows the user to just solve the problem using familiar tools and concepts". Personal Software began selling it in mid-1979 for under $100, after a demonstration at the fourth West Coast Computer Faire and an official launch on June 4 at the National Computer Conference. It required an Apple II with 32K, and supported saving to cassette or disk.
VisiCalc was unusually easy to use and came with excellent documentation; Apple's developer documentation cited the software as an example of one with a simple user interface. Observers immediately noticed its power. Ben Rosen speculated in July 1979 that "VisiCalc could someday become the software tail that wags (and sells) the personal computer dog". For the first 12 months it was only available for the Apple II, and became that platform's killer app. Many bought $2000 Apples to run the $100 software, even if they already owned computers. Apple's rival Tandy Corporation used VisiCalc on its own Apple IIs. Other software supported its Data Interchange Format (DIF) to share data.
Bricklin and Frankston's original intention was to fit the program into 16k, but this proved impossible and 32k became necessary. Some additional features they wanted like a split text/graphics screen still had to be omitted for space reasons. However, Apple eventually began shipping all Apple IIs with 48k following a drop in RAM prices and so this was no longer an issue. The initial release supported cassette storage, but that was quickly dropped.
At its release Personal Software promised ports to other computers, starting with those using the 6502 CPU, and versions appeared for the Atari 800 and Commodore PET, both of which could be done easily because those computers used the same processor as the Apple II, and large portions of code could be reused. The PET version, which contained two separate executables for 40 and 80-column models, was widely criticized for having a very small amount of worksheet space due to the developers' insistence on including their own custom DOS which used a large amount of memory (the PET only had 32k versus the Apple II's 48k).
Other ports followed for the Zilog Z-80-based Tandy TRS-80 Model I, Model II, Model III, and Model 4. The TRS-80 Model I port is the only version of VisiCalc without copy protection. On most versions, this was disk-based, but the PET VisiCalc comes with a ROM chip that the user had to install in one of the motherboard's expansion ROM sockets. Another port was made to the IBM PC, and it was one of the first commercial packages available when it shipped in 1981. It quickly became a best-seller on this platform, in spite of being severely limited to be compatible with the versions from the 8-bit platforms. It is estimated that 300,000 copies were sold on the PC, bringing total sales to about 1 million copies.
By 1982 VisiCalc's price had risen from $100 to $250. Several competitors appeared in the market, notably SuperCalc and Multiplan, each of which added more features and corrected deficiencies in Visicalc, but could not overcome its market dominance. A more dramatic change occurred with the 1983 launch of Lotus Development Corporation's Lotus 1-2-3, written by a former VisiCalc employee. Unlike the PC version of VisiCalc, 1-2-3 was written to take full advantage of the PC's increased memory, screen and performance. Yet it deliberately attempted to remain as compatible as possible with VisiCalc, including copying its menu structure as far as possible to allow VisiCalc users to easily migrate to 1-2-3.
1-2-3 was almost immediately successful, and by 1984 "InfoWorld" wrote that sales of VisiCalc were "rapidly declining", stating that it was "the first successful software product to have gone through a complete life cycle, from conception in 1978 to introduction in 1979 to peak success in 1982 to decline in 1983 to a probable death, according to industry insiders, in 1984." The magazine added that the company was slow to upgrade the software, only releasing an Advanced Version of VisiCalc for the Apple II in 1983 and announcing one for the IBM PC in 1984. By 1985 VisiCorp was insolvent, and Lotus Development acquired Software Arts. Lotus immediately ended sales of the application.
Reception.
In 1983 "Softline" readers named VisiCalc tenth overall, and the highest non-game, on the magazine's Top Thirty list of Atari 8-bit programs by popularity. "II Computing" listed it second on the magazine's list of top Apple II software as of late 1985, based on sales and market-share data.
In its 1980 review, "BYTE" wrote "The most exciting and influential piece of software that has been written for any microcomputer application is VisiCalc". It concluded, "VisiCalc is the first program available on a microcomputer that has been responsible for sales of entire systems". "Creative Computing"‍ '​s review the same year similarly concluded, "for almost anyone in business, education, or any science-related field it is ... reason enough to purchase a small computer system in the first place". "Compute!" reported, "Every Visicalc user knows of someone who purchased an Apple just to be able to use Visicalc". "Antic" wrote in 1984, "VisiCalc isn't as easy to use as prepackaged home accounting programs, because you're required to design both the layout and the formulas used by the program. Because it is not pre-packaged, however, it's infinitely more powerful and flexible than such programs. You can use VisiCalc to balance your checkbook, keep track of credit card purchases, calculate your net worth, do your taxes—the possibilities are practically limitless." "The Addison-Wesley Book of Atari Software 1984" gave the application an overall A+ rating, praising its documentation and calling it "indispensible ... a straight 'A' classic".
In 2006, Charles Babcock of "InformationWeek" wrote that, in retrospect, "VisiCalc was flawed and clunky, and couldn't do many things users wanted it to do."

</doc>
<doc id="32644" url="http://en.wikipedia.org/wiki?curid=32644" title="Viol">
Viol

The viol or viola da gamba is any one of a family of bowed, fretted and stringed instruments that first appeared in Spain in the mid to late 15th century and was most popular in the Renaissance and Baroque periods. Early ancestors include the Arabic "rebab" and the medieval European vielle, but later, more direct possible ancestors include the Venetian "viole" and the 15th- and 16th-century Spanish "vihuela", a 6-course plucked instrument tuned like a lute (and also like a present-day viol) that looked like but was quite distinct from the (at that time) 4-course guitar (an earlier chordophone).
Although bass viols superficially resemble cellos, viols are different in numerous respects from instruments of the violin family: the viol family has flat rather than curved backs, sloped rather than squarely rounded shoulders, c holes rather than f holes, and five to seven rather than four strings; some of the many additional differences are tuning strategy (in fourths with a third in the middle—exactly like a lute—rather than in fifths), the presence of frets, and underhand rather than overhand bow grip.
All members of the viol family are played upright between the legs like a modern cello, hence the Italian name "viola da gamba" (lit. viol of the leg). This distinguishes the viol from the modern violin family, the "viola da braccio" (lit. viol of the arm). A player of the viol is commonly known as a "gambist".
History.
Vihuelists began playing their flat-edged instruments with a bow in the second half of the 15th century. Within two or three decades, this led to the evolution of an entirely new and dedicated bowed string instrument that retained many of the features of the original plucked vihuela: a flat back, sharp waist-cuts, frets, thin ribs (initially), and an identical tuning—hence its original name, vihuela de arco; "arco" is Spanish for "bow".
An influence in the playing posture has been credited to the example of Moorish "rabab" players.
The viol is unrelated to the much older Hebrew stringed instrument called a "viol" (literally, "skin"). This ancient harp-like instrument was similar to the kinnor or nabla.
Stefano Pio argues that a re-examination of documents in the light of newly collected data indicates an origin different from the vihuela de arco from Aragon:according to Pio the viol (viola da gamba) had its origins and evolved independently in Venice. According to Pio, it is implausible that the vihuela de arco, which possibly arrived in Rome and Naples after 1483-1487, since Johannes Tinctoris does not mention it prior to this time, underwent such a rapid evolution by Italian instrument makers – not Venetian (circumstances specifically excluded by Lorenzo da Pavia), nor Mantuan or Ferrarese (as evidenced by Isabella and Alfonso I d' Este's orders from luthiers from other cities) – so that a ten-year span witnessed the birth and diffusion in Italy of a new family of instruments (viole da gamba or viols) which comprised instruments of different size, some as large as the famous "violoni" as ‘big as a man’ mentioned by Prospero Bernardino in 1493.
Pio also notes that both in the manuscript of the early 15th-century music theorist Antonius de Leno and in the treatises of the Venetian Silvestro Ganassi dal Fontego and Giovanni Maria Lanfranco (), the fifth string of the viola da gamba is uniquely called a "bordone" (drone), although it is not actually a drone and is played the same as the other strings. Pio argues that this inconsistency is justifiable only assuming the invention, during the last part of the fifteenth century, of a larger instrument derived from the medieval "violetta", to which were gradually added other strings to allow a greater extension to the low register that resulted from its increased size. The fifth string, already present in some specimens of these violette as a drone ("bordone"), was incorporated into the neck when they were expanded in size. This was then surpassed by a sixth string, named "basso", which fixed the lower sound produced by the instrument. In Pio's view, the origin of the viola da gamba is tied to the evolution of the smaller the medieval violetta or vielle, that was originally fitted with a fifth string drone, where the name remained unchanged even though it ceased to perform this function.
Ian Woodfield, in his "The Early History of the Viol", points to evidence that the viol does in fact start with the vihuela but that Italian makers of the instrument immediately began to apply their own highly developed instrument-making traditions to the early version of the instrument when it was introduced into Italy.
Initially the family of "viole" ("viols") shared common characteristics but differed in the way they were played. The increase in the dimensions of the "viola" determined the birth of the viol and the definitive change in the manner the instrument was held, as musicians found it easier to play it vertically. The first consort of viols formed by four players was documented at the end of the fifteenth century in the courts of Mantua and Ferrara, but was also present in popular Venetian music ambience, noted at the Scuola Grande di San Marco, 1499; Venetian culture remained independent of Spanish influence and consequently unfamiliar with the instruments of those lands, such as the bowed "vihuela de arco". Groups of viol players, generally called "violoni", were established in the Venetian Scuole Grandi around 1530/40, but the highly traditional environment of these institutions suggests that these groups would have already been active in the general urban context during the previous two decades (1510-1520). Some of these players were known to have traveled to distant lands, including Vienna, the Duchy of Bavaria or the Kingdom of England where they were welcomed at the court of the Tudors and subsequently influenced England’s local instrumental production.
Construction.
Viols most commonly had six strings, although many 16th-century instruments had five or even four strings. Viols were (and are) strung with gut strings of lower tension than on the members of the violin family, let alone the steel strings mostly used in those instruments today. Gut strings produce a sonority far different from steel, generally described as softer and sweeter. Around 1660, gut or silk core strings overspun with copper wire first became available; these were then used for the lowest-pitched bass strings on viols, and on many other string instruments as well.
Viols are fretted in a manner similar to early guitars or lutes, by means of movable wrapped-around and tied-on gut frets. A low seventh string was supposedly added in France to the bass viol by Monsieur de Sainte-Colombe (c. 1640–1690), whose students included the French gamba virtuoso and composer Marin Marais. Also, the painting (1618) by Domenichino (1581–1641) shows what may be a seven-string viol.
Unlike members of the violin family, which are tuned in fifths, viols are usually tuned in fourths with a major third in the middle, mirroring the tuning employed on the vihuela de mano and lute during the 16th century and similar to that of the modern six-string guitar.
Viols were first constructed much like the "vihuela de mano", with all surfaces, top, back, and sides made from flat slabs or pieces of joined wood, bent or curved as required. However, some viols, both early and later, had carved tops, similar to those more commonly associated with instruments of the violin family. The ribs or sides of early viols were usually quite shallow, reflecting more the construction of their plucked vihuela counterparts. Rib depth increased during the course of the 16th century, finally coming to resemble the greater depth of the classic 17th-century pattern.
The flat backs of most viols have a sharply angled break or "canted" bend in their surface close to where the neck meets the body. This serves to taper the back (and overall body depth) at its upper end to meet the back of the neck joint flush with its heel. Traditional construction uses animal glue, and internal joints are often reinforced with strips of either linen or vellum soaked in hot animal glue—a practice also employed in early plucked vihuela construction. The peg boxes of viols (which hold the tuning pegs) were typically decorated either with elaborate carved heads of animals or people or with the now familiar spiral "scroll" finial.
The earliest vihuelas and viols, both plucked and bowed, all had sharp cuts to their waists, similar to the profile of a modern violin. This is a key and new feature—first appearing in the mid-15th century—and from then on, it was employed on many different types of string instruments. This feature is also key in seeing and understanding the connection between the plucked and bowed versions of early vihuelas. If one were to go searching for very early viols with smooth-curved figure-eight bodies, like those found on the only slightly later plucked vihuelas and the modern guitar, they would be out of luck. By the mid-16th century, however, "guitar-shaped" viols were fairly common, and a few of them survive.
The earliest viols had flat, glued-down bridges just like their plucked counterpart vihuelas. Soon after, however, viols adopted the wider and high-arched bridge that facilitated the bowing of single strings. The earliest of viols would also have had the ends of their fretboards flat on the deck, level with or resting upon the top or sound board. Once the end of their fretboards were elevated above the top of the instrument's face, the entire top could vibrate freely. Early viols did not have sound posts, either (again reflecting their plucked vihuela siblings). This reduced damping again meant that their tops could vibrate more freely, contributing to the characteristic "humming" sound of viols; yet the absence of a sound post also resulted in a quieter and softer voice overall.
It is commonly believed that C-holes (a type and shape of pierced sound port visible on the top face or belly of string instruments) are a definitive feature of viols, a feature used to distinguish viols from instruments in the violin family, which typically had F-shaped holes. This generality, however, renders an incomplete picture. The earliest viols had either large, open, round, sound holes (or even round pierced rosettes like those found on lutes and vihuelas), or they had some kind of C-holes. Viols sometimes had as many as four small C-holes—one placed in each corner of the bouts—but more commonly, they had two. The two C-holes might be placed in the upper bouts, centrally, or in the lower bouts. In the formative years, C-holes were most often placed facing each other or turned inwards.
In addition to round or C-holes, however, and as early as the first quarter of the 16th century, some viols adopted S-shaped holes, again facing inward. By the mid-16th century, S-holes morphed into the classic F-shaped holes, which were then used by viols and members of the violin family alike. By the mid- to late 16th century, the viol's C-holes facing direction was reversed, becoming outward facing. That configuration then became a standard feature of what we today call the “classic” 17th-century pattern. Yet another style of sound holes found on some viols was a pair of flame-shaped Arabesques placed left and right. The lute and vihuelalike round or oval ports or rosettes became a standard feature of German and Austrian viols and was retained to the very end. That feature or “genetic marker” was exclusively unique to viols and reminded one always of the viol's more ancient plucked vihuela roots, the "luteness" of viols.
Historians, makers, and players generally distinguish between "Renaissance" and "Baroque" viols. The latter are more heavily constructed and are fitted with a bass bar and sound post, like modern stringed instruments.
Viol bows.
The bow is held underhand (palm up), similar to a German double bass bow grip, but away from the frog towards the balance point. The stick's curvature is generally convex as were violin bows of the period, rather than concave like a modern violin bow. The "frog" (which holds the bowhair and adjusts its tension) is also different from that of modern bows: whereas a violin bow frog has a "slide" (often made of mother of pearl), which pinches the hair and holds it flat and stationary across the frog, viol bows have an open frog that allows more movement of the hair. This facilitates a traditional playing technique where the performer uses one or two fingers of the bow hand to press the hair away from the bow stick. This dynamically increases bow hair tension to control articulation and inflection.
Versions.
Gambas (as the name is often abbreviated) come in seven sizes: "pardessus de viole" (which is relatively rare, exclusively French and did not exist before the 18th century), treble (in French "dessus"), alto, tenor (in French "taille"), bass, and "two" sizes of contrabass (also known as a violone), the smaller one tuned an octave below the tenor (violone in G, sometimes called "great bass" or in French "grande basse") and the larger one tuned an octave below the bass (violone in D).
Their tuning (see next section) alternates G and D instruments: pardessus in G, treble in D, tenor in G, bass in D (the seven string bass was in A), small violone in G, large violone in D. The alto, between the treble and the tenor, does not fit in this scheme. The treble has a size similar to a viola but with a deeper body; the typical bass is about the size of a cello. The pardessus and the treble were held vertically in the lap. The English made smaller basses known as division viols, and the still-smaller Lyra viol. The viola bastarda was a similar type of viol used in Italy for a virtuosic style of viol repertoire and performance. German consort basses were larger than the French instruments designed for continuo.
Those instruments were not all equally common. The typical Elizabethan consort of viols was composed of six instruments: two basses, two tenors and two trebles, or one bass, three tenors and two trebles (see Chest of viols). Thus the bass, tenor and treble were the central members of the family as far as music written specifically for viols is concerned. Besides consort playing the bass could also be used as a solo instrument (there were also smaller basses designed especially for a virtuosic solo role, see above "division viol", "lyra viol", "viola bastarda"). And the bass viol could also serve as a continuo bass. The pardessus was a French 18th century instrument that was introduced to allow ladies to play mostly violin or flute music but eventually acquired its own repertoire. The alto was a relatively rare smaller version of the tenor. The violones were never part of the consort of viols but functioned as the contrabass of all kinds of instrumental combinations.
Tuning.
The standard tuning of the viol is in fourths, with a major third in the middle (like the standard Renaissance lute tuning). For bass viols, the notes would be (from the lowest) D-G-c-e-a-d', with an additional low AA for seven-string bass viols. For the tenor viol, the tuning is G-c-f-a-d'-g' (a smaller tenor, or "alto" viol, is usually tuned A-d-g-b-e'-a'). The treble viol is one octave higher than the bass.
Alternate tunings (called scordatura) were often employed, particularly in the solo lyra viol style of playing, which also made use of many techniques such as chords and "pizzicato", not generally used in consort playing. An unusual style of pizzicato was known as a thump. Lyra viol music was also commonly written in tablature. There is a vast repertoire of this music, some by well-known composers and much by anonymous ones.
Much viol music predates the adoption of equal temperament tuning by musicians. The movable nature of the tied-on frets permits the viol player to make adjustments to the tempering of the instrument, and some players and consorts adopt meantone temperaments, which are arguably more suited to Renaissance music. Several fretting schemes involve frets that are spaced unevenly to produce "better-sounding" chords in a limited number of keys. In some of these schemes, the two strands of gut that form the fret are separated so that the player can finger a slightly sharper or flatter version of a note (for example G sharp vs. A flat) to suit different circumstances.
Treatises.
Descriptions and illustrations of viols are found in numerous early 16th-century musical treatises, including those authored by:
Both Agricola's and Gerle's works were published in various editions.
There were then several important treatises concerning or devoted to the viol. The first was by Silvestro Ganassi dal Fontego: "Regola Rubertina & Lettione Seconda" (1542/3). Diego Ortiz published "Trattado de Glosas" (Rome, 1553), an important book of music for the viol with both examples of ornamentation and pieces called "Recercadas". In England, Christopher Simpson wrote the most important treatise, with the second edition being published in 1667 in parallel text (English and Latin). This has divisions at the back that are very worthwhile repertoire. A little later, in England, Thomas Mace wrote "Musick's Monument", which deals more with the lute but has an important section on the viol. After this, the French treatises by Machy (1685), Rousseau (1687), Danoville (1687), and Etienne Loulie (1700) show further developments in playing technique.
Popularity.
Viols were second in popularity only to the lute (although this is disputed), and like lutes, were very often played by amateurs. Affluent homes might have a so-called "chest of viols", which would contain one or more instruments of each size. Gamba ensembles, called "consorts", were common in the 16th and 17th centuries, when they performed vocal music (consort songs or verse anthems) as well as that written specifically for instruments. Only the treble, tenor, and bass sizes were regular members of the viol consort, which consisted of three, four, five, or six instruments. Music for consorts was very popular in England in Elizabethan times, with composers such as William Byrd and John Dowland, and, during the reign of King Charles I, John Jenkins and William Lawes. The last music for viol consorts before their modern revival was probably written in the early 1680s by Henry Purcell.
Perhaps even more common than the pure consort of viols was the mixed or broken consort (also called Morley consort). Broken consorts combined a mixture of different instruments—a small band, essentially—usually comprising a gathering of social amateurs and typically including such instruments as a bass viol, a lute or orpharion (a wire-strung lute, metal-fretted, flat-backed, and festoon-shaped), a cittern, a treble viol (or violin, as time progressed), sometimes an early keyboard instrument (virginal, spinet, or harpsichord), and whatever other instruments or players (or singers) might be available at the moment. The single most common and ubiquitous pairing of all was always and everywhere the lute and bass viol: for centuries, the inseparable duo.
The bass viola da gamba continued to be used into the 18th century as a solo instrument (and to complement the harpsichord in basso continuo). It was a favorite instrument of Louis XIV and acquired associations of both courtliness and "Frenchness" (in contrast to the Italianate violin). Composers such as Marin Marais, Johann Sebastian Bach, Johannes Schenck, Antoine Forqueray, and Carl Friedrich Abel wrote virtuoso music for it. However, viols fell out of use as concert halls grew larger and the louder and more penetrating tone of the violin family became more popular. In the 20th century, the viola da gamba and its repertoire were revived by early music enthusiasts, an early proponent being Arnold Dolmetsch.
The treble viol in d and the even smaller pardessus de viole in g (often with only five strings) were also popular instruments in the 18th century, specially in France. Composers like Jean-Baptise Barrière, Georg Phillipp Telemann and Marin Marais wrote solo- and ensemble pieces for treble or pardessus. It was also common to play music for violins or flutes or unspecified top parts on small viols.
Historic viols survive in relatively great number, though very few remain in original condition. They can often be found in collections of historic musical instruments at museums and universities. Here are some of the extant historic viols at The Metropolitan Museum of Art:
Modern era.
Today, the viol is attracting ever more interest, particularly among amateur players. This may be due to the increased availability of reasonably priced instruments from companies using more automated production techniques, coupled with the greater accessibility of music editions. The viol is also regarded as a suitable instrument for adult learners; Percy Scholes wrote that the viol repertoire "...belongs to an age that demanded musicianship more often than virtuosity."
There are now many societies for people with an interest in the viol. The first was , which was established in the United Kingdom in 1948 and has a worldwide membership. Since then, similar societies have been organized in several other nations. In the 1970s, the now defunct Guitar and Lute Workshop in Honolulu, Hawaii generated resurgent interest in the viol and traditional luthierie methods within the western United States.
A notable youth viol group is the Gateshead Viol Ensemble. It consists of young players between the ages of 7 and 18 and is quite well known in the north east of England. It gives young people the opportunity to learn the viol and gives concerts in the North East and abroad. Ensembles like these show that the viol is making a comeback.
A living museum of historical musical instruments was created at the University of Vienna as a center for the revival of the instrument. More than 100 instruments, including approximately 50 historical viola da gambas in playable condition, are the property of this new concept of museum: the Orpheon Foundation Museum of Historical Instruments. All the instruments of this museum are played by the Orpheon Baroque Orchestra, the Orpheon consort, or by musicians who receive an instrument for a permanent loan. The instruments can be seen during temporary exhibitions . They are studied and copied by violin makers, contributing to the extension of the general knowledge we have on the viola da gamba, its forms, and the different techniques used for its manufacture.
The 1991 feature film "Tous les matins du monde" (All the Mornings of the World) by Alain Corneau, based on the lives of Monsieur de Sainte-Colombe and Marin Marais, prominently featured these composers' music for the viola da gamba and brought viol music to new audiences. The film's bestselling soundtrack features performances by Jordi Savall, one of the best-known modern viola da gamba players.
Among the foremost modern players of the viol are Alison Crum, Vittorio Ghielmi, Wieland Kuijken, Paolo Pandolfo, Hille Perl and Jordi Savall. Many fine modern viol consorts (ensembles) are also recording and performing, among them the groups Fretwork, the Rose Consort of Viols, Le Consort de violes des Voix humanes, and Phantasm. The Baltimore Consort specializes in Renaissance song (mostly English) with broken consort (including viols).
New compositions for viol.
A number of contemporary composers have written for viol, and a number of soloists and ensembles have commissioned new music for viol. Fretwork has been most active in this regard, commissioning George Benjamin, Michael Nyman, Elvis Costello, Sir John Tavener, Orlando Gough, John Woolrich, Tan Dun, Alexander Goehr, Fabrice Fitch, Andrew Keeling, Thea Musgrave, Sally Beamish, Peter Sculthorpe, Gavin Bryars, Barrington Pheloung, Simon Bainbridge, Duncan Druce, Poul Ruders, Ivan Moody, and Barry Guy; many of these compositions may be heard on their 1997 CD "Sit Fast". The Yukimi Kambe Viol Consort has commissioned and recorded many works by David Loeb, and the New York Consort of Viols has commissioned Bülent Arel, David Loeb, Daniel Pinkham, Tison Street, Frank Russo, Seymour Barab, William Presser, and Will Ayton, many of these compositions appearing on their 1993 CD "Illicita Cosa".
The Palazzo Strozzi in Florence commissioned composer Bruce Adolphe to create a work based on Bronzino poems, and the piece, "Of Art and Onions: Homage to Bronzino", features a prominent viola da gamba part. has also written works for the Yukimi Kambe Viol Consort, Les Voix Humaines, and Elliot Z. Levine, among others. Other composers for viols include Moondog, Kevin Volans, Roy Whelden, Toyohiko Satoh, Roman Turovsky, Giorgio Pacchioni, Michael Starke, Emily Doolittle, and . Composer Henry Vega has written pieces for the Viol: "Ssolo," developed at the Institute for Sonology and performed by Karin Preslmayr, as well as for Netherlands based ensemble The Roentgen Connection in 2011 with "Slow slower" for recorder, viola da gamba, harpsichord and computer.
Electric viols.
Since the late 1980s, numerous instrument makers, including Eric Jensen, Francois Danger, Jan Goorissen, and Jonathan Wilson, have experimented with the design and construction of electric viols. Their range of approaches, from Danger's minimally electrified acoustic/electric Altra line to Eric Jensen's solid-body brace-mounted design, have met with varying degrees of ergonomic and musical success.
In the early 21st century, the Ruby Gamba, a seven-string electric viola da gamba, was developed by Ruby Instruments of Arnhem, the Netherlands. It has 21 tied nylon (adjustable) frets in keeping with the adjustable (tied gut) frets on traditional viols and has an effective playing range of more than six octaves.
Electric viols have been adopted by such contemporary gambists as Gilles Zimmermann, Loren Ludwig, Jay Elfenbein, Paolo Pandolfo, Tina Chancey, Fahmi Alqhai and Tony Overwater.
Similar names and common confusions.
The viola da gamba is occasionally confused with the viola, the alto member of the modern violin family and a standard member of both the symphony orchestra and string quartet. In the 15th century, the Italian word "viola" was a generic term used to refer to any bowed instrument, or fiddle. It is important to note that the word "viola" existed in Italy before the vihuela, or first viol, was brought from Spain. In Italy, "viola" was first applied to "a braccio" precursor to the modern violin, as described by Tinctoris ("De inventione et usu musice", c. 1481–3), and then was later used to describe the first Italian viols as well.
Depending on the context, the unmodified "viola da braccio" most regularly denoted either an instrument from the violin family, or specifically the viola (whose specific name was "alto de viola da braccio"). When Monteverdi called simply for "viole da braccio" in "Orfeo", the composer was requesting violas as well as treble and bass instruments.
The full name of the viola namely "alto de viola da braccio" was finally shortened to "viola" in some languages (e.g. English, Italian, Spanish) once viols became less common, while other languages picked some other part of the phrase to designate the instrument, e.g. "alto" in French and "Bratsche" (which comes from the Italian "braccio") in German.
Other instruments which include the word "viola" in their name but have nothing to do specifically with and are not a member of the "viola da gamba" family are the viola d'amore and the viola pomposa. Although the baryton doesn't have "viola" in its name it is sometimes included, wrongly, in the viol family.
The names "viola" (Italy) and "vihuela" (Spain) were essentially synonymous and interchangeable. According to viol historian Ian Woodfield, there is little evidence that the "vihuela de arco" was introduced to Italy before the 1490s. The use of the term "viola" was never used exclusively for viols in the 15th or 16th centuries. In 16th century Italy, both ""violas","—the early viols and violins—developed somewhat simultaneously. While the violins such as those of Amati achieved their classic form before the first half of the century, the viol's form would be standardized later in the century by instrument makers in England.
Viola da gamba, "viola cum arculo", and "vihuela de arco" are some (true) alternative names for viols. Both "vihuela" and "viola" were originally used in a fairly generic way, having included even early violins ("viola da braccio") under their umbrella. It is common enough (and justifiable) today for modern players of the viola da gamba to call their instruments "violas" and likewise to call themselves "violists". That the "alto violin" eventually became known simply as the "viola" is not without historical context, yet the ambiguity of the name tends to cause some confusion. The violin, or "violino", was originally the soprano "viola da braccio", or "violino da braccio". Due to the popularity of the soprano violin, the entire consort eventually took on the name "violin family".
Some other names for viols include "viole" or "violle" (French). In Elizabethan English, the word "gambo" (for gamba) appears in many permutations; e.g., "viola de gambo", "gambo violl", "viol de gambo", or "viole de gambo", used by such notables as Tobias Hume, John Dowland, and William Shakespeare in "Twelfth Night".
"Viola da Gamba" also appears as a name appended to a spoof letter-to-the-editor in the first issue of "National Lampoon" magazine (April 1970).
Viol da Gamba and Gamba also appear as string family stops on the pipe organ.

</doc>
<doc id="32659" url="http://en.wikipedia.org/wiki?curid=32659" title="Varney the Vampire">
Varney the Vampire

Varney the Vampire; or, the Feast of Blood was a Victorian era serialized gothic horror story by James Malcolm Rymer (alternatively attributed to Thomas Preskett Prest). It first appeared in 1845–47 as a series of cheap pamphlets of the kind then known as "penny dreadfuls". The story was published in book form in 1847. It is of epic length: the original edition ran to 876 double-columned pages divided into 220 chapters. Altogether it totals nearly 667,000 words. Despite its inconsistencies, "Varney the Vampire" is more or less a cohesive whole. It is the tale of the vampire Sir Francis Varney, and introduced many of the tropes present in vampire fiction recognizable to modern audiences to this day.
The story.
Setting.
The story has a confused setting. While ostensibly set in the early eighteenth century, there are references to the Napoleonic Wars and other indicators that the story is contemporary to the time of its writing in the mid-nineteenth century. Varney's adventures also occur in various locations including London, Bath, Winchester, Naples and Venice.
Human characters.
The plot concerns the troubles that Sir Francis Varney inflicts upon the Bannerworths, a formerly wealthy family driven to ruin by their recently deceased father. Initially the Bannerworths consist of Mrs. Bannerworth and her adult children Henry, George and Flora. (George is never mentioned after the thirty-sixth chapter.) A family friend, Mr. Marchdale, lives with the Bannerworths in early chapters. Later Flora's fiancé Charles Holland and his seafaring uncle Admiral Bell along with his assistant, the extremely humorous Jack Pringle, also take residence with the Bannerworths.
The character of Varney.
Though the earliest chapters give the standard motives of blood sustenance for Varney's actions toward the family, later ones suggest that Varney is motivated by monetary interests. The story is at times inconsistent and confusing, as if the author did not know whether to make Varney a literal vampire or simply a human who acts like one. Varney bears a strong resemblance to a portrait in Bannerworth Hall, and the implication throughout is that he is actually Marmaduke Bannerworth (a.k.a. Sir Runnagate Bannerworth in a classic naming confusion), but that connection is never cleared up. He is portrayed as loathing his condition, and at one point he turns Clara Crofton, a member of another family he terrorizes, into a vampire for revenge.
Over the course of the book, Varney is presented with increasing sympathy as a victim of circumstances. He tries to save himself, but is unable to do so. He ultimately commits suicide by throwing himself into Mount Vesuvius, after having left a written account of his origin with a sympathetic priest. According to Varney, he was cursed with vampirism after he had betrayed a royalist to Oliver Cromwell and accidentally killed his own son afterwards in a fit of anger, although he "dies" and is revived several times in the course of his career. This afforded the author a variety of origin stories. In one of these, a medical student named Dr. Chillingworth applies galvanism to Varney's hanged corpse and revives him. This sub-plot has an obvious similarity to the story of "Frankenstein" by Mary Shelley and even more so, perhaps, to much later film adaptations of the novel (the novel itself does not present electricity as Dr. Frankenstein's means of creating the monster.)
Legacy.
"Varney" was a major influence on later vampire fiction, most notably the renowned novel "Dracula" (1897) by Bram Stoker. Many of today's standard vampire tropes originated in "Varney": Varney has fangs, leaves two puncture wounds on the necks of his victims, has hypnotic powers, and has superhuman strength. Unlike later fictional vampires, he is able to go about in daylight and has no particular fear of either crosses or garlic. He can eat and drink in human fashion as a form of disguise, but he points out that human food and drink do not agree with him. His vampirism seems to be a fit that comes on him when his vital energy begins to run low; he is a regular, normally functioning person between feedings.
This is also the first example of the "sympathetic vampire," a vampire who despises his condition but is nonetheless a slave to it. This archetype has been widely exemplified, notably by such characters as Countess Zaleska in the 1936 film "Dracula's Daughter", Barnabas Collins in the TV soap opera "Dark Shadows", Mick St. John in the TV show "Moonlight", Louis de Pointe du Lac in Anne Rice's "Interview with the Vampire", Kain in the "Legacy Of Kain" video games, Marvel Comics character Morbius, the Living Vampire, Nick Knight in the TV series "Forever Knight", Angel from the "Buffy the Vampire Slayer" universe, and Bill Compton in Charlaine Harris' "The Southern Vampire Mysteries".
The makers of Marvel Comics were also influenced by this story. In the Marvel Universe, "Varnae" is the name of the first vampire, created by the people of Atlantis before it sank.
The sole member of the German darkwave band Sopor Aeternus & The Ensemble of Shadows, Anna-Varney Cantodea, adopted her name from Varney the vampire.
"The Shadow" magazine story "The Vampire Murders" published in 1942 features "Varney Haldrew" who lives in Haldrew Hall. Varney is named after the original Varney The Vampire and spends his nights sleeping in a coffin.
Varney appears in an episode of the Canadian television show "", played by Sam Malkin. He appears in the episode "Bad Blood", and treats Dracula after he is poisoned by a rare blood type.
Sir Francis Varney is the Viceroy of India in Kim Newman's "Anno Dracula".
In the TV series "Penny Dreadful" (2014), Dr. Van Helsing gives a copy of "Varney the Vampire" to Victor Frankenstein, explaining that the story is more truth than fiction and that the mysterious creature the series' characters are pursuing is a vampire.

</doc>
<doc id="32660" url="http://en.wikipedia.org/wiki?curid=32660" title="V12 engine">
V12 engine

A V12 engine is a V engine with 12 cylinders mounted on the crankcase in two banks of six cylinders, usually but not always at a 60° angle to each other, with all 12 pistons driving a common crankshaft. Since each cylinder bank is essentially a straight-6 which is by itself in both primary and secondary balance, a V12 is automatically in primary and secondary balance no matter which V angle is used, and therefore it needs no balance shafts. A four-stroke 12 cylinder engine has an even firing order if cylinders fire every 60° of crankshaft rotation, so a V12 with cylinder banks at a multiples of 60° (60°, 120°, or 180°) will have even firing intervals without using split crankpins. By using split crankpins or just ignoring minor vibrations, any V angle is possible. The 180° configuration is usually referred to as a Flat-twelve engine or even a boxer although it is in reality a 180° V since the pistons can and normally do use shared crankpins.
Applications.
V12 engines deliver power pulses more often than engines with six or eight cylinders, and the power pulses have triple overlap (at any time three cylinders are on different stages of the same power stroke) which eliminates gaps between power pulses and allows for greater refinement and smoothness in a luxury car engine, at the expense of much greater cost. In a racing car engine, the rotating parts of a V12 can be made much lighter than a V8 with a crossplane crankshaft because there is no need to use heavy counterweights on the crankshaft and less need for the inertial mass in a flywheel to smooth out the power delivery. Exhaust system tuning is also much more difficult on a crossplane V8 than a V12, so racing cars with V8 engines often use a complicated bundle of snakes exhaust system, or a flat-plane crankshaft which causes severe engine vibration and noise. This is not important in a race car if all-out performance is the only goal. Since cost and fuel economy are usually important even in luxury and racing cars, the V12 has been largely phased out in favor of engines with fewer cylinders.
In a large displacement, high-power engine, a 60° V12 fits into a longer and narrower space than a V8 and most other V configurations, which is a problem in modern cars, but less so in heavy trucks, and seldom a problem in large stationary engines. The V12 is common in locomotive and battle tank engines, where high power is required, but the width of the engine is constrained by tight railway clearances or street widths, while the length of the vehicle is more flexible. It is often used in marine engines where great power is required, and the hull width is limited, but a longer vessel allows faster hull speed. In twin-propeller boats, two V12 engines can be narrow enough to sit side-by-side, while three V12 engines are sometimes used in high-speed three-propeller configurations. Large, fast cruise ships can have six or more V12 engines. In historic piston-engine fighter and bomber aircraft, the long, narrow V12 configuration used in high-performance aircraft made them more streamlined than other engines, particularly the short, wide radial engine. During World War II the power of fighter engines was stepped up to extreme levels using multi-speed superchargers and ultra-high octane gasoline, so the extreme smoothness of the V12 prevented the powerful engines from tearing apart the light airframes of fighters (often made out of balsa wood and/or canvas rather than aluminum). After World War II, the compact, more powerful, and vibration-free turboprop and turbojet engines replaced the V12 in aircraft applications.
Early (pre WW1) V12 engines.
Marine engines.
The first V-type engine (a 2-cylinder vee twin) was built in 1889 by Daimler, to a design by Wilhelm Maybach. By 1903 V8 engines were being produced for motor boat racing by the Société Antoinette to designs by Léon Levavasseur, building on experience gained with in-line four-cylinder engines. In 1904, the Putney Motor Works completed a new V12 marine racing engine – the first V12 engine produced for any purpose. Known as the ‘Craig-Dörwald' engine after Putney's founding partners, the engine mounted pairs of L-head cylinders at a 90 degree included angle on an aluminium crankcase, using the same cylinder pairs that powered the company's standard 2-cylinder car. A single camshaft mounted in the central vee operated the valves directly. As in many marine engines, the camshaft could be slid longitudinally to engage a second set of cams, giving valve timing that reversed the engine's rotation to achieve astern propulsion. "Starting is by pumping a charge into each cylinder and switching on the trembler coils. A sliding camshaft gave direct reversing. The camshaft has fluted webs and main bearings in graduated thickness from the largest at the flywheel end." Displacing 1120 cuin (bore and stroke of 4.875 x), the engine weighed 950 lbs and developed 150 bhp. Little is known of the engine's achievements in the 40-foot hull for which it was intended, while using the engine to power heavy freight vehicles was not realized. One V12 Dörwald marine engine was found still running in a Hong Kong junk in the late-1960s.
Two more V12s appeared in the 1909-10 motor boat racing season. The Lamb Boat & Engine Company of Clinton, Iowa built a 1559 cuin (5.25 x) engine for the company's 32-foot Lamb IV. It weighed in at 2,114 lbs. No weight is known for the massive 3464 cuin (7 x) F-head engine built by the Orleans Motor Company. Output is quoted as “nearly 400 bhp”.
By 1914, when Panhard built two 2356 cuin (5 x) engines with four-valve cylinder heads the V12 was well established in motor boat racing.
Motor car engines.
In October 1913, Louis Coatalen, chief engineer of the Sunbeam car company entered a V12 powered car in the Brooklands short and long handicap races. The engine displaced 9 L, with bore and stroke of 80 x 150 mm. An aluminum crankcase carried two blocks of three cylinders along each side, with a 60 degree included angle. The cylinders were of iron, with integral cylinder heads with L-shaped combustion chambers. Inlet and exhaust valves were operated by a central camshaft in the vee. Valve clearance was set by grinding the relevant parts, the engine lacking any easy means of adjustment. This pointed to Coatalen's ultimate aim of using the new V12 as an aero engine, where any adjustment method that could go wrong in flight was to be avoided. As initially built, the V12 was rated at 200 bhp at 2,400 rpm, weighing about 750 lbs. The engine powered the car (named ‘Toodles V' (for Coatalen's wife Olive's pet name) to several records in 1913 and 1914.
Early aero engines.
In 1909 Renault pioneered aero V12s with a 60 degree air-cooled engine with individual finned cylinders and F-head valve arrangement, driven by single camshaft in the crankcase. This was developed to a 12.2 L unit (96 x 140 mm) which weighed 772 lbs and produced 138 bhp at 1,800 rpm. The propeller was driven from the nose of the camshaft in the central vee, rather than from the crankshaft, thus providing an automatic half-speed reduction, improving propeller efficiency.
Renault's designs were closely followed in Britain by the Royal Aircraft Factory. Its RAF 4 engine displaced 13.2 L (100 x 140 mm), produced 140 bhp at 1,800 rpm, for a weight of 637 lbs. Its RAF 4a derivative was produced in substantial numbers during the war.
By 1912 ABC Motors were offering a water-cooled engine of 17.4 L, claimed to produce 170 bhp at 1,400 rpm and weigh 390 – with radiator and coolant.
In March 1914 Sunbeam exhibited an airborne version of Toodles V's engine at Olympia. Racing in 1913 had helped to prove the design, and encouraged a 10 mm increase in bore to 90 mm, the stroke remaining at 150 mm. Its rated output was 225 bhp at 2,000 rpm. Named the ‘Mohawk', the engine was the most powerful available to British aviation at the outbreak of World War I. During the war further enlargement to 100 x 150 mm created the 240 bhp 'Gurkha'.
Later V12s in aviation.
By the end of World War I, V12s were well established in aviation, powering some of the newest and largest fighters and bombers and being produced by companies such as Renault and Sunbeam. Many Zeppelins had 12-cylinder engines from German manufacturers Maybach and Daimler.
Austro Daimler of the Austro-Hungarian empire, produced also V12, designed by Ferdinand Porsche, with first 300 and later 345 hp, used with the big flying boats of the A-H Naval Air force.
Various U.S. companies produced the Liberty L-12. Soon after the end of WW1 V12 engines powered the first trans-atlantic crossings by the Curtiss NC Flying boats (4 x Liberty L-12), the first non-stop crossing by Alcock and Brown in a Vickers Vimy (2× Rolls-Royce Eagles and the first airship crossing by HM airship R-34 (5× Sunbeam Maori).
V12 engines reached their apogee during World War II. Fighters and bombers used V12 engines such as the British Rolls-Royce Merlin and Griffon, the Soviet Klimov VK-107 and Mikulin AM-38, the American Allison V-1710, or the German Daimler-Benz DB 600 series and Junkers Jumo. These engines generated about 1000 hp at the beginning of the war and above 1500 hp at their ultimate evolution stage. The German DB 605DC engine reached 2000 hp with methanol/water injection called MW 50-equipment. In contrast to most Allied V12s, the engines built in Germany by Daimler-Benz, Junkers-Jumo, and Argus (As 410 and As 411) were primarily inverted, which had the advantages of lower centers of gravity and improved pilot visibility for single-engined designs. Only the pre-war origin BMW VI V12 of Germany was an "upright" engine. The United States had the experimental Continental IV-1430 inverted V12 engine under development, with a higher power-to-weight ratio than any of the initial versions of the German WW II inverted V12s, but was never developed to production status, with only 23 examples of the Continental inverted V12 ever being built. The only American-design inverted V12 engine of any type to see even limited service in World War II was the air-cooled Ranger V-770, which found use in stateside-based training aircraft like the Fairchild AT-21 Gunner twin-engined "advanced" trainer.
The Rolls-Royce Merlin V12 powered the Hawker Hurricane and Supermarine Spitfire fighters that played a vital role in Britain's victory in the Battle of Britain. The long, narrow configuration of the V12 contributed to good aerodynamics, while its smoothness allowed its use with relatively light and fragile airframes. The Merlin was also used in the Avro Lancaster and de Havilland Mosquito bombers. In the United States the Packard Motor company was licensed by Rolls-Royce to produce the Merlin as the Packard V-1650 for use in the North American P-51 Mustang. It was also incorporated into some models of the Curtiss P-40, specifically the P-40F and P-40L. Packard Merlins powered Canadian-built Hurricane, Lancaster, and Mosquito aircraft, as well as the UK-built Spitfire Mark XVI, which was otherwise the same as the Mark IX with its British-built Merlin.
The Allison V-1710 was the only indigenous U.S.-developed V12 liquid-cooled engine to see service during World War II. A sturdy design, it lacked an advanced mechanical supercharger until 1943. Although versions with a turbosupercharger provided excellent performance at high altitude in the Lockheed P-38 Lightning, the turbosupercharger and its ductwork were too bulky to fit into typical single-engine fighters. While a good performer at low altitudes, without adequate supercharging, the Allison's high-altitude performance was lacking.
After World War II, V12 engines became generally obsolete in aircraft due to the introduction of turbojet and turboprop engines that had more power for their weight, and fewer complications.
V12 road cars.
In automobiles, V12 engines have not been common due to their complexity and cost. They are used almost exclusively in expensive sports and luxury cars because of their power, smoother operation and distinctive sound.
Prior to World War II, 12-cylinder engines were found in many luxury models, including cars from: Packard 1916 to 1923 and again from 1932 to 1939, Daimler 1926 to 1937, Hispano-Suiza 1931, Cadillac 1931, Auburn 1932, Franklin 1932, Lincoln 1932 to 1942 (continuing after the war through 1948), Rolls-Royce 1936, and Pierce-Arrow also 1936.
Vehicles with 8-, 12-, and 16-cylinders provide higher levels of refinement compared to those with fewer cylinders, especially important prior to the general adoption of vibration isolating engine mounts in the 1930s.
Packard's 1916 "Twin Six" is widely regarded as the first production V12 automobile engine. With a list price of US$1,000, the Auburn was the lowest priced V12 car ever (unadjusted for inflation). Production cost savings were achieved by using horizontal valves which, however, did not result in an efficient and powerful combustion chamber. Between 1916 and 1921, there was a vogue of V12s, during which National (Indianapolis) copied the Packard engine, and Weidely Motors (also of Indianapolis) offered a proprietary engine. Soon after the end of World War I, Lancia offered a 22° V12, Fiat had a 60° model 520 (1921-2), British truck manufacturer Ensign announced a V12 that did not materialize, and in 1926, Daimler (Britain) offered the first of a full range of sleeve valve Double Sixes, 7,136 cc, 3,744 cc, 5,296 cc and 6,511 cc versions remaining available until 1937. In 1927 more entered the market from Cadillac, Franklin, Hispano-Suiza, Horch, Lagonda, Maybach, Packard, Rolls, Tatra, Voisin, and Walter offering V12 engines. Cadillac (from 1930 to 1940) and Marmon (1931–33) even developed V16 engines.
Improvements in combustion chamber design and piston form enabled lighter V8 engines to surpass the V12 in power starting from the 1930s; only the smaller, H-Series Lincoln V-12 remained after WWII and it was replaced by a V8 in 1949. Similarly, as they seemed excessive for the postwar market in Europe, production of V12-engined-cars was very limited until the 1960s.
Ferrari has traditionally reserved their top V12 engine for their top-of-the line luxury sports coupes since 1949. Ferrari's closest rival, Lamborghini has also used the V12 configuration for many of its road cars since the company's inception in 1963.
In 1972, Jaguar introduced the XJ12, equipped with a 5.3 litre V12, which continued (after revisions in 1993) until the 1996 model year, after which the marque discontinued the twelve-cylinder engine.
BMW returned to V12 designs for its 7-Series sedan in model year 1986, forcing Mercedes-Benz to follow suit in 1991. While BMW sells far fewer V12-engined 7-Series vehicles than V8 versions, the V12 is marketed in the U.S., China , and Russia. The BMW-designed V12 is used in Rolls-Royce cars, while the Mercedes engine was installed in Maybach cars.
Mercedes S65 AMG, CL65 AMG, and SL65 AMG are powered by V12 Bi Turbo making 463 kW and 1000 Nm at 2,300-4,300 rpm. The CL65 AMG has a significant higher cost, but the V12 engine makes them a status symbol.
The V12 engine is generally smoother than the other 12-cylinder configuration, the W12, although the W12 is more compact.
In 1997, Toyota equipped their Century Limousine with a 5.0 L DOHC V12 (model # 1GZ-FE), making it the first and only Japanese production passenger car so equipped.
TVR made and tested a 7.7 L V12 called the Speed Twelve, but the project ended. The only British marques currently using a V12 configuration are Aston Martin — whose Cosworth-developed engine was authorized during the company's ownership by Ford Motor Company — and Rolls-Royce.
In 2009, China FAW Group equipped their Hongqi HQE with a 6.0 L DOHC V12 (model # CA12VG), making it the first and only Chinese production passenger car so equipped.
Most production V12 engines in road cars have an even firing order, with the uneven-firing exceptions such as Aston Martin 5.9L V12 and Mercedes-Benz M275 AMG V12s.
In 2008, Audi launched their Q7 model with a 5.9-litre V12 twin-turbo diesel engine, making it the first production passenger car so equipped. The engine also appeared in the R8 V12 TDI concept car.
Postwar V12 production cars.
This is a list of V12-engined production road cars produced since 1945, sorted alphabetically by make (and sub-sorted by year of introduction):
Some tuner companies, such as Brabus, also sell V12 versions of the Mercedes-Benz E-Class and CLS, which were the fastest street-legal sedans upon their respective introductions.
Heavy trucks.
Tatra used a 17.6 L air-cooled naturally aspirated V12 diesel engine in many of their trucks; for instance, the Tatra T813 and uses 19 L air-cooled naturally aspirated or turbo V12 diesel engine in Tatra T815. Some large trucks have been fitted with twin V12s that drive a common shaft, although this is often advertised as a V24.
GMC produced a large gasoline-burning V12 from 1960 to 1965 for trucks, the "Twin-Six"; it was basically GMC's large-capacity truck 351 V6, doubled, with four rocker covers and four exhaust manifolds. Fifty-six major parts are interchangeable between the Twin-Six and all other GMC V6 engines to provide greater parts availability and standardization. Its engine displacement was 702 cuin, and while power was not too impressive at 250 hp, torque was 585 lbft. For firetrucks the rev limiter was increased to produce 299 hp at 3000 rpm and torque was increased to 630 lbft at 1600–1900 rpm. It was possibly the last gasoline engine used in heavy trucks in the US.
Detroit Diesel produced their Series 53, 71, 92, and 149 engines as V-12s, among other configurations.
Auto racing.
V12 engines used to be common in Formula One and endurance racing. From 1965 to 1980, Ferrari, Weslake, Honda, BRM, Maserati, Matra, Delahaye, Peugeot, Delage, Alfa Romeo, Lamborghini, and Tecno used 12-cylinder engines in Formula One, either V12 or flat-12. The last V12 engine used in Formula One was the Ferrari 044, on the Ferrari 412T2 cars driven by Jean Alesi and Gerhard Berger in 1995.
In the late 1960s Nissan used a V12 in the Japanese Grand Prix and again in the early-1990s Group C races.
At the Paris motor show 2006 Peugeot presented a new racing car, as well as a luxury saloon concept car, both called 908 HDi FAP and 908 RC and fitted with a V12 Diesel engine producing around or even surpassing 700 PS. This took part in the 24 Hours of Le Mans 2007 race, coming in second place after the similarly conceived Audi R10 TDI V12 Diesel originally developed for the 2006 season.
Large diesel engines.
V12 is a common configuration for large diesel engines; most are available with differing numbers of cylinders in V configuration to offer a range of power ratings. Many diesel locomotives have V12 engines. Examples include the 3200 hp 12-710 from Electro-Motive Diesel and the 4400 hp GEVO-12 from GE Transportation.
Large V12 engines are also common in ships. For example, Wärtsilä offers V12 engines with various cylinder bore diameters between 26 and with power output ranging from 4080 to. These engines are commonly used especially in cruise ships, which may have up to six such main engines. The largest medium-speed diesel engine, Wärtsilä 64, was also offered in V configuration, and a single 12V64 prototype with an output of 23,280 kW was produced for an experimental power plant in the late 1990s.
Railway V12 diesel engines specs.
Railway Diesel engines with 12 cylinder developing 500 KW and more:
Tanks and other AFVs.
The V12 is a common configuration for tank and other armoured fighting vehicles (AFVs). Some examples are:

</doc>
<doc id="32665" url="http://en.wikipedia.org/wiki?curid=32665" title="Voice-over">
Voice-over

Voice-over (also known as off-camera or off-stage commentary) is a production technique where a voice—that is not part of the narrative (non-diegetic)—is used in a radio, television production, filmmaking, theatre, or other presentations. The voice-over is read from a script and may be spoken by someone who appears elsewhere in the production or by a specialist voice actor. It is usually pre-recorded and placed over the top of a film or video and commonly used in documentaries or news reports to explain information. It may also be read live for events such as award presentations.
Techniques.
Character device.
In Herman Melville's "Moby Dick" (1956), Ishmael (Richard Basehart) narrates the story and sometimes comments on the action in voice-over, as does Joe Gillis (William Holden) in "Sunset Boulevard (1950)" and Eric Erickson (William Holden) in "The Counterfeit Traitor (1962)"; adult Pip (John Mills) in "Great Expectations (1946)" and Michael York in a television remake (1974).
Voice-over technique is likewise used to give voices and personalities to animated characters. Noteworthy and versatile voice actors include Mel Blanc, Daws Butler, Don Messick, Paul Frees, and June Foray.
Creative device.
In film, the film-maker places the sound of a human voice (or voices) over images shown on the screen that may or may not be related to the words that are being spoken. Consequently, voice-overs are sometimes used to create ironic counterpoint. Also, sometimes they can be random voices not directly connected to the people seen on the screen. In works of fiction, the voice-over is often by a character reflecting on his or her past, or by a person external to the story who usually has a more complete knowledge of the events in the film than the other characters.
Voice-overs are often used to create the effect of storytelling by a character/omniscient narrator. For example, in "The Usual Suspects", the character of Roger "Verbal" Kint has voice-over segments as he is recounting details of a crime. Classic voice-overs in cinema history can be heard in "Citizen Kane" and "The Naked City". Other examples of storytelling voice-overs can be heard in "Annie Hall", "Gattaca", "Fight Club", "Megamind", "Ratatouille", "", "Tangled", "The Magic School Bus", "The Emperor's New Groove", "Kronk's New Groove", "Blade Runner", "The Rugrats Movie", "The Shawshank Redemption", "Big Fish", "How to Train Your Dragon", "Moulin Rouge!", "The Postman Always Rings Twice", "Young Winston", "Rugrats", "Raising Arizona", "Goodfellas", and "Clash of the Titans".
Sometimes, voice-over can be used to aid continuity in edited versions of films, in order for the audience to gain a better understanding of what has gone on between scenes. This was done when the 1948 "Joan of Arc," starring Ingrid Bergman, turned out to be far from the box-office and critical hit that was expected, and was edited down from 145 minutes to 100 minutes for its second run in theatres. The edited version, which circulated for years, used narration to conceal the fact that large chunks of the film had been cut. In the full-length version, restored in 1998 and released on DVD in 2004, the voice-over narration is heard only at the beginning of the film.
Film noir is especially associated with the voice-over technique. The golden Age of first person narration was in the 1940s. Typically used male voice-over narration but there are a few rare female voice-over narration in noir.
In radio, voice-overs are an integral part of the creation of the radio programme. The voice-over artist might be used to remind listeners of the station name or as characters to enhance or develop show content. In the 1980s UK broadcasters Steve Wright and Kenny Everett used voice-over artists to create a virtual "posse" or studio crew that contributed to the programmes. It is believed that this principle was in play long before that time. USA radio broadcaster Howard Stern has also used voice-overs in this way.
Educational or descriptive device.
The voice-over has many applications in non-fiction as well. Television news is often presented as a series of video clips of newsworthy events, with voice-over by the reporters describing the significance of the scenes being presented; these are interspersed with straight video of the news anchors describing stories for which video is not shown.
Television networks such as The History Channel and the Discovery Channel make extensive use of voice-overs. On NBC, the television show "Starting Over" used Sylvia Villagran as the voice-over narrator to tell a story.
Live sports broadcasts are usually shown as extensive voice-overs by expert announcers over video of the sporting event.
Game shows formerly made extensive use of voice-overs to introduce contestants and describe available or awarded prizes, but this technique has diminished as shows have moved toward predominantly cash prizes. The most prolific have included Don Pardo, Johnny Olson, John Harlan, Jay Stewart, Gene Wood and Johnny Gilbert.
Voice-over commentary by a leading critic, historian, or by the production personnel themselves is often a prominent feature of the release of feature films or documentaries on DVDs.
Commercial device.
The commercial use of voice-over in television advertising has been popular since the beginning of radio broadcasting.
In the early years, before effective sound recording and mixing, announcements were produced "live" and at-once in a studio with the entire cast, crew and, usually, orchestra. A corporate sponsor hired a producer, who hired writers and voice actors to perform comedy or drama.
The industry expanded very rapidly with the advent of television in the 1950s and the age of highly produced serial radio shows ended. The ability to record high-quality sound on magnetic tape also created opportunities, as has the proliferation of home computers capable of recording, often using inexpensive even free software and a microphone of reasonable quality.
Manufacturers will often use a distinctive voice to help them with brand messaging, often retaining talent to a long term exclusive contract.
Translation.
In some countries, such as Russia and Poland, a voice-over provided by a single artist is commonly used on television programs as a language localization technique, as an alternative to full dub localization.
In Bulgaria, voice-over translation is also common, but each film (or episode) is normally voiced by at least four actors. The voice artists try to match the original voice and preserve the intonation. The main reason for the use of this type of translation is that unlike synchronized voice translation, it takes a relatively short time to produce as there is no need to synchronize the voices with the character's lip movements, which is compensated by the quieted original audio. When there is no speaking in the film for some time, the original sound is turned up. Recently, as more films are distributed with separate voice and noises-and-music tracks, some voice-over translations in Bulgaria are produced by only turning down the voice track, in this way not affecting the other sounds. One actor always reads the translation crew's names over the show's ending credits (except for when there are dialogs over the credits).

</doc>
<doc id="32669" url="http://en.wikipedia.org/wiki?curid=32669" title="West African Vodun">
West African Vodun

Vodun (meaning "spirit" in the Fon and Ewe languages, ] with a nasal high-tone "u"; also spelled Vodon, Vodoun, Vodou, Voudou, Voodoo, etc.) is practiced by the Ewe people of eastern and southern Ghana, and southern and central Togo; and the Kabye people, Mina people, and Fon people of southern and central Togo, southern and central Benin. It's also practiced by some Gun (Gbe ) people of Lagos and Ogun in south west Nigeria.
It is distinct from the various African traditional religions in the interiors of these countries and is the main source of religions with similar names found among the African Diaspora in the New World such as Haitian "Vodou"; Puerto Rican "Vodú"; Cuban "Vodú"; Dominican "Vudú"; Brazilian "Vodum"; and Louisiana "Voodoo". All of these closely related faiths are syncretized with Christianity to various degrees and with the traditional beliefs of the Kongo people and Indigenous American traditions.
Theology and practice.
Vodun cosmology centers around the "vodun" spirits and other elements of divine essence that govern the Earth, a hierarchy that range in power from major deities governing the forces of nature and human society to the spirits of individual streams, trees, and rocks, as well as dozens of ethnic vodun, defenders of a certain clan, tribe, or nation. The "vodun" are the center of religious life, similarly in many ways to doctrines such as the intercession of saints and angels that made Vodun appear compatible with Christianity, especially Catholicism, and produced syncretic religions such as Haitian Vodou. Adherents also emphasize ancestor worship and hold that the spirits of the dead live side by side with the world of the living, each family of spirits having its own female priesthood, sometimes hereditary when is from mother to blood daughter.
Patterns of worship follow various dialects, spirits, practices, songs, and rituals. A divine Creator, called variously "Mawu" is a female being who in one tradition bore seven children and gave each rule over a realm of nature - animals, earth, and sea - or else these children are inter-ethnic and related to natural phenomena or to historical or mythical individuals. The Creator embodies a dual cosmogonic principle of which "Mawu" the moon and "Lisa" the sun are respectively the female and male aspects, often portrayed as the twin children of the Creator.
In other traditions, Legba is represented as Mawu's masculine counterpart, thus being represented as a phallus or as a man with a prominent phallus. Dan, who is Mawu's androgynous son, is represented as a rainbow serpent, and was to remain with her and act as a go-between with her other creations. As the mediator between the spirits and the living, Dan maintains balance, order, peace and communication.
All creation is considered divine and therefore contains the power of the divine. This is how medicines such as herbal remedies are understood, and explains the ubiquitous use of mundane objects in religious ritual. Vodun talismans, called "fetishes", are objects such as statues or dried animal parts that are sold for their healing and spiritually rejuvenating properties.
Priests.
Often described as queen mother is the first daughter of a patriarchal lineage of a family collective. She holds the right to lead the ceremonies incumbent to the clan: marriages, baptisms and funerals. She is considered one of the most important members of community. She will lead the women of a village when her family collective is the ruling one. Her dominant role has often been confused or associated to that of a high priestess which she is not. They take part in the organisation and the running of markets and are also responsible for their upkeep, which is vitally important because marketplaces are the focal points for gatherings and social centres in their communities. In the past when the men of the villages would go to war, the Queen Mothers would lead prayer ceremonies in which all the women attended every morning to ensure the safe return of their menfolk.
The High priestess is, on the other hand, the woman chosen by the oracle to care for the convent.
Priestesses, like priests, receive a calling from an oracle, which may come at any moment during their lives. They will then join their clan's convent to pursue spiritual instruction. It is also an oracle that will designate the future high priest and high priestess among the new recruits, establishing an order of succession within the convent. Only blood relatives were allowed in the family convent; strangers are forbidden.
In modern days, however, some of the rules have been changed, enabling non family members to enter what is described as the first circle of worship. Strangers are allowed to worship only the spirits of the standard pantheon.
Relationship to Bò.
Confusion and an amalgam are often made between Vodun and Bò also called O bò or Juju in Yoruba. Bò is an occult science whose priest are called Bòkônon or Bòkôtônon in opposition to Vodunsi (Vodun female priestess) and Vodunon (Vodun male priest). 
Contrary to popular beliefs, in West African Vodun, spells are not cast upon someone. Vodun is a religion in which an important part is devoted to the cult of the ancestors. Even if the origin of humanity and the world are explained in Vodun mythology, it is not a centered question of the faith. The followers believe that the answer to such question is beyond human reach. Priority is given to the ancestors with them interceding on behalf of their families and descendant towards the Almighty. While an Almighty creator is recognized in Vodun pantheon, the believers do not address themselves to that particular deity. Only the Loas, the messengers with the help of the dead have that access. In order to communicate and pray every clan and sometimes each family root have their own Vodun sometimes called Assanyì as Vodun can also be translated as "The spirit of those who have passed before us". The family Vodun is often associated with a known higher spirit of the standard pantheon, but is distinctive to each family (clan). This distinctiveness is the Clan Vodun is also an assertion of identity and origin with cult and worshiping process specific to a family collective.
The occult science of Bô is not Vodun, although it often summons spirits issued from the Vodun pantheon in its process. The amalgam probably occurred through foreign observation and explanation of the rituals of Vodun. It is due to the fact that Vodun elements can be seen in the rituals of Bò.
The general perception of west African Vodun today is based on a perception of Bò (Juju in Yoruba), European witchcraft and misunderstanding of the practice.
Demographics.
About 17% of the population of Benin, some 1.6 million people, follow Vodun. (This does not count other traditional religions in Benin.) In addition, many of the 41.5% of the population that refer to themselves as "Christian" practice a syncretized religion, not dissimilar from Haitian Vodou or Brazilian Candomblé; indeed, many of them are descended from freed Brazilian slaves who settled on the coast near Ouidah.
In Togo, about half the population practices indigenous religions, of which Vodun is by far the largest, with some 2.5 million followers; there may be another million Vodunists among the Ewe of Ghana, as a 13% of the total Ghana population of 20 million are Ewe and 38% of Ghanaians practise traditional religion. According to census data, about 14 million people practise traditional religion in Nigeria, most of whom are Yoruba practising Ifá, but no specific breakdown is available.
European colonialism, followed by some of the totalitarian regimes in West Africa, have tried to suppress Vodun as well as other African indigenous religions. However, because the vodun deities are born to each clan, tribe, and nation, and their clergy are central to maintaining the moral, social and political order and ancestral foundation of its village, these efforts have not been successful. Recently there have been moves to restore the place of Vodun in national society, such as an annual International Vodun Conference held in the city of Ouidah in Benin that has been held since 1991.
Further reading.
</dl>

</doc>
<doc id="32678" url="http://en.wikipedia.org/wiki?curid=32678" title="Vocoder">
Vocoder

A vocoder (, short for "voice encoder") is an analysis and synthesis system, used to reproduce human speech. The vocoder was originally developed as a speech coder for telecommunications applications in the 1930s, the idea being to code speech for transmission.
In the encoder, the input is passed through a multiband filter, each band is passed through an envelope follower, and the control signals from the envelope followers are communicated to the decoder. The decoder applies these (amplitude) control signals to corresponding filters in the synthesizer. Since the control signals change only slowly compared to the original speech waveform, the bandwidth required to transmit speech can be reduced. This allows more speech channels to share a radio circuit or submarine cable.
By encrypting the control signals, voice transmission can be secured against interception. Its primary use in this fashion is for secure radio communication. The advantage of this method of encryption is that none of the original signal is sent, but rather envelopes of the bandpass filters. The receiving unit needs to be set up in the same filter configuration to resynthesize a version of the original signal spectrum.
The vocoder has also been used extensively as an electronic musical instrument. The synthesis portion of the vocoder, called a voder, can be used independently for speech synthesis.
Theory.
The human voice consists of sounds generated by the opening and closing of the glottis by the vocal cords, which produces a periodic waveform with many harmonics. This basic sound is then filtered by the nose and throat (a complicated resonant piping system) to produce differences in harmonic content (formants) in a controlled way, creating the wide variety of sounds used in speech. There is another set of sounds, known as the unvoiced and plosive sounds, which are created or modified by the mouth in different fashions.
The vocoder examines speech by measuring how its spectral characteristics change over time. This results in a series of signals representing these modified frequencies at any particular time as the user speaks. In simple terms, the signal is split into a number of frequency bands (the larger this number, the more accurate the analysis) and the level of signal present at each frequency band gives the instantaneous representation of the spectral energy content.
Thus, the vocoder dramatically reduces the amount of information needed to store speech, from a complete recording to a series of numbers. To recreate speech, the vocoder simply reverses the process, processing a broadband noise source by passing it through a stage that filters the frequency content based on the originally recorded series of numbers.
Information about the instantaneous frequency (as distinct from spectral characteristic) of the original voice signal is discarded; it wasn't important to preserve this for the purposes of the vocoder's original use as an encryption aid, and it is this "dehumanizing" quality of the vocoding process that has made it useful in creating special voice effects in popular music and audio entertainment.
Since the vocoder process sends only the parameters of the vocal model over the communication link, instead of a point by point recreation of the waveform, it allows a significant reduction in the bandwidth required to transmit speech.
Analog vocoders typically analyze an incoming signal by splitting the signal into a number of tuned frequency bands or ranges. A modulator and carrier signal are sent through a series of these tuned bandpass filters. In the example of a typical robot voice the modulator is a microphone and the carrier is noise or a sawtooth waveform. There are usually between 8 and 20 bands.
The amplitude of the modulator for each of the individual analysis bands generates a voltage that is used to control amplifiers for each of the corresponding carrier bands. The result is that frequency components of the modulating signal are mapped onto the carrier signal as discrete amplitude changes in each of the frequency bands.
Often there is an unvoiced band or sibilance channel. This is for frequencies outside of analysis bands for typical speech but still important in speech. Examples are words that start with the letters s, f, ch or any other sibilant sound. These can be mixed with the carrier output to increase clarity. The result is recognizable speech, although somewhat "mechanical" sounding. Vocoders also often include a second system for generating unvoiced sounds, using a noise generator instead of the fundamental frequency.
History.
The first experiments with a vocoder were conducted in 1928 by Bell Labs engineer Homer Dudley, who was granted a patent for it on March 21, 1939.
The Voder (Voice Operating Demonstrator), was introduced to the public at the AT&T building at the 1939–1940 New York World's Fair. The Voder consisted of a series of manually controlled oscillators, filters, and a noise source. The filters were controlled by a set of keys and a foot pedal to convert the hisses and tones into vowels, consonants, and inflections. This was a complex machine to operate, but with a skilled operator could produce recognizable speech.
Dudley's vocoder was used in the SIGSALY system, which was built by Bell Labs engineers in 1943. SIGSALY was used for encrypted high-level voice communications during World War II. Later work in this field has been conducted by James Flanagan.
Modern implementations.
Even with the need to record several frequencies, and additional unvoiced sounds, the compression of vocoder systems is impressive. Standard speech-recording systems capture frequencies from about 500 Hz to 3400 Hz, where most of the frequencies used in speech lie, typically using a sampling rate of 8 kHz (slightly greater than the Nyquist rate). The sampling resolution is typically at least 12 or more bits per sample resolution (16 is standard), for a final data rate in the range of 96–128 kbit/s, but a good vocoder can provide a reasonably good simulation of voice with as little as 2.4 kbit/s of data.
'Toll Quality' voice coders, such as ITU G.729, are used in many telephone networks. G.729 in particular has a final data rate of 8 kbit/s with superb voice quality. G.723 achieves slightly worse quality at data rates of 5.3 kbit/s and 6.4 kbit/s. Many voice vocoder systems use lower data rates, but below 5 kbit/s voice quality begins to drop rapidly.
Several vocoder systems are used in NSA encryption systems:
Vocoders are also currently used in developing psychophysics, linguistics, computational neuroscience and cochlear implant research.
Modern vocoders that are used in communication equipment and in voice storage devices today are based on the following algorithms:
Linear prediction-based.
Since the late 1970s, most non-musical vocoders have been implemented using linear prediction, whereby the target signal's spectral envelope (formant) is estimated by an all-pole IIR filter. In linear prediction coding, the all-pole filter replaces the bandpass filter bank of its predecessor and is used at the encoder to "whiten" the signal (i.e., flatten the spectrum) and again at the decoder to re-apply the spectral shape of the target speech signal.
One advantage of this type of filtering is that the location of the linear predictor's spectral peaks is entirely determined by the target signal, and can be as precise as allowed by the time period to be filtered. This is in contrast with vocoders realized using fixed-width filter banks, where spectral peaks can generally only be determined to be within the scope of a given frequency band. LP filtering also has disadvantages in that signals with a large number of constituent frequencies may exceed the number of frequencies that can be represented by the linear prediction filter. This restriction is the primary reason that LP coding is almost always used in tandem with other methods in high-compression voice coders.
Waveform-Interpolative.
Waveform-Interpolative (WI) vocoder was developed in AT&T Bell Laboratories around 1995 by W.B. Kleijn, and subsequently a low- complexity version was developed by AT&T for the DoD secure vocoder competition. Notable enhancements to the WI coder were made at the University of California, Santa Barbara. AT&T holds the core patents related to WI, and other institutes hold additional patents. Using these patents as a part of WI coder implementation requires licensing from all IPR holders.
Artistic effects.
Uses in music.
For musical applications, a source of musical sounds is used as the carrier, instead of extracting the fundamental frequency. For instance, one could use the sound of a synthesizer as the input to the filter bank, a technique that became popular in the 1970s.
Werner Meyer-Eppler, a German scientist with a special interest in electronic voice synthesis, published a thesis in 1948 on electronic music and speech synthesis from the viewpoint of sound synthesis, and later he was instrumental in the founding in 1951 of the Studio for Electronic Music of WDR in Cologne.
One of the first attempt to divert a vocoder to create music may be a “Siemens Synthesizer” at the Siemens Studio for Electronic Music, developed between 1956 and 1959.
In 1968, Robert Moog developed one of the first solid-state musical vocoder for the electronic music studio of University at Buffalo.
In 1968, Bruce Haack built a prototype vocoder, named "Farad" after Michael Faraday, and it was first featured on "The Electronic Record For Children" released in 1969 and then on his rock album "The Electric Lucifer" released in 1970.
In 1970 Wendy Carlos and Robert Moog built another musical vocoder, a 10-band device inspired by the vocoder designs of Homer Dudley. It was originally called a spectrum encoder-decoder, and later referred to simply as a vocoder. The carrier signal came from a Moog modular synthesizer, and the modulator from a microphone input. The output of the 10-band vocoder was fairly intelligible, but relied on specially articulated speech. Later improved vocoders use a high-pass filter to let some sibilance through from the microphone; this ruins the device for its original speech-coding application, but it makes the "talking synthesizer" effect much more intelligible.
Carlos and Moog's vocoder was featured in several recordings, including the soundtrack to Stanley Kubrick's "A Clockwork Orange" in which the vocoder sang the vocal part of Beethoven's "Ninth Symphony". Also featured in the soundtrack was a piece called "Timesteps," which featured the vocoder in two sections. "Timesteps" was originally intended as merely an introduction to vocoders for the "timid listener", but Kubrick chose to include the piece on the soundtrack, much to the surprise of Wendy Carlos.
In 1972, Isao Tomita's first electronic music album "Electric Samurai: Switched on Rock" was an early attempt at applying speech synthesis technique in electronic rock and pop music. The album featured electronic renditions of contemporary rock and pop songs, while utilizing synthesized voices in place of human voices. In 1974, he utilized synthesized voices again in his popular classical music album "Snowflakes are Dancing", which became a worldwide success and helped popularize electronic music. Emerson Lake and Palmer used it for the album Brain Salad Surgery 1973. 
Kraftwerk's "Autobahn" (1974) was one of the first successful albums to feature vocoder vocals. Another of the early songs to feature a vocoder was "The Raven" on the 1976 album "Tales of Mystery and Imagination" by progressive rock band The Alan Parsons Project; the vocoder also was used on later albums such as "I Robot". Following Alan Parsons' example, vocoders began to appear in pop music in the late 1970s, for example, on disco recordings. Jeff Lynne of Electric Light Orchestra used the vocoder in several albums such as "Time" (featuring the Roland VP-330 Plus MkI). ELO songs such as "Mr. Blue Sky" and "Sweet Talkin' Woman" both from "Out of the Blue" (1977) use the vocoder extensively as does "The Diary of Horace Wimp" from the album "Discovery" (1979). Featured on the album are the EMS Vocoder 2000W MkI, and the EMS Vocoder (-System) 2000 (W or B, MkI or II).
Giorgio Moroder made extensive use of the vocoder on the 1975 album "Einzelganger" and on the 1977 album "From Here to Eternity".
Another example is Pink Floyd's song "Dogs", from their album "Animals" (1977), where the band put the sound of a barking dog through the device.
A vocoder was used by Jo Partridge to produce the Martian's unearthly exultations of "Ulla" in the 1978 Concept album Jeff Wayne's Musical Version of The War of the Worlds.
The vocoder has been used at the start and end of the Main Street Electrical Parade at Disneyland and Walt Disney World since 1979.
Phil Collins used a vocoder to provide a vocal effect for his 1981 international hit single "In the Air Tonight".
Vocoders are often used to create the sound of a robot talking, as in the Styx song "Mr. Roboto" (1983).
Roger Taylor of Queen used the Vocoder on two songs on Queen's eleventh studio album "The Works", "Radio Ga Ga" and "Machines (Or 'Back to Humans')".
Vocoders have appeared on pop recordings from time to time ever since, most often simply as a special effect rather than a featured aspect of the work. However, many experimental electronic artists of the new-age music genre often utilize vocoder in a more comprehensive manner in specific works, such as Jean Michel Jarre (on "Zoolook", 1984) and Mike Oldfield (on "QE2", 1980 and "Five Miles Out", 1982). There are also some artists who have made vocoders an essential part of their music, overall or during an extended phase. Examples include the German synthpop group Kraftwerk, Stevie Wonder ("Send One Your Love", "A Seed's a Star") and jazz/fusion keyboardist Herbie Hancock during his late 1970s period. In 1982 Neil Young used a Sennheiser Vocoder VSM201 on six of the nine tracks on "Trans". Tommy James used a Vocoder in the production of his group's (the Shondells) 1968 number one hit 'Crimson and Clover'. 
Perhaps the most heard, yet often unrecognized, example of the use of a vocoder in popular music, is on Michael Jackson's 1982 album "Thriller", in the song "P.Y.T. (Pretty Young Thing)". During the first few seconds of the song, the background voicings "ooh-ooh, ooh, ooh", behind his spoken words, exemplify the heavily modulated sound of his voice through a Vocoder. The bridge also features a vocoder as well ("Pretty young thing/You make me sing"), courtesy of session musician Michael Boddicker.
Coldplay have used a vocoder in some of their songs. For example in "Major Minus" and "Hurts Like Heaven", both from the album "Mylo Xyloto" (2011), Chris Martin's vocals are mostly vocoder-processed. "Midnight", from "Ghost Stories" (2014), also features Martin singing through a vocoder; in "O", from the same album, Martin can be heard repeating "Don't ever let go" into a vocoder.
Noisecore band Atari Teenage Riot have used Vocoders in variety of their songs and live performances such as Live at the Brixton Academy (2002) alongside other digital audio technology both old and new.
Among the most consistent uses of vocoder in emulating the human voice are Daft Punk, who have used this instrument from their first album "Homework" (1997) to their latest work "Random Access Memories" (2013) and consider the convergence of technological and human voice "the identity of their musical project". For instance, the lyrics of "Around the World" (1997) are integrally vocoder-processed, "Get Lucky" (2013) features a mix of natural and processed human voices, and "Instant Crush" (2013) features Julian Casablancas singing into a vocoder.
Voice effects in other arts.
"Robot voices" became a recurring element in popular music during the 20th century. Apart from vocoders, several other methods of producing variations on this effect include: the Sonovox, Talk box, and Auto-Tune, linear prediction vocoders, speech synthesis, ring modulation and comb filter.
Vocoders are used in television production, filmmaking and games, usually for robots or talking computers.
The robot voices of the Cylons in "Battlestar Galactica" were created with an EMS Vocoder 2000. The 1980 version of the "Doctor Who" theme, as arranged and recorded by Peter Howell, has a section of the main melody generated by a Roland SVC-350 Vocoder. A vocoder was also used to create the iconic voice of Soundwave, a character from the Transformers series.
In 1967 the Supermarionation series "Captain Scarlet and the Mysterons" to supply the deep, eerie threatening voice of the disembodied Mysterons and well as the bass tones for the Spectrum agent Captain Black when he is seized under their telepathic control. It was also used in the closing credits theme, of the first 13 episodes to provide the synthetic repetition of the words "Captain Scarlet".

</doc>
<doc id="32708" url="http://en.wikipedia.org/wiki?curid=32708" title="Sarasvati River">
Sarasvati River

The Sarasvati River (Sanskrit: सरस्वती नदी "sárasvatī nadī") is one of the main Rigvedic rivers mentioned in the Rig Veda and later Vedic and post-Vedic texts. It plays an important role in Hinduism, since Vedic Sanskrit and the first part of the Rig Veda are regarded to have originated when the Vedic people lived on its banks, 2nd millennium BCE . The goddess Sarasvati was originally a personification of this river, but later developed an independent identity.
The Nadistuti hymn in the Rigveda (10.75) mentions the Sarasvati between the Yamuna in the east and the Sutlej in the west, and later Vedic texts like Tandya and Jaiminiya Brahmanas as well as the Mahabharata mention that the Sarasvati dried up in a desert.
Many scholars have identified the Vedic Sarasvati River with the Ghaggar-Hakra River, which flows in northwestern India and Pakistan. This was proposed by several scholars in the 19th and early 20th century. Satellite images in possession of the ISRO and ONGC have confirmed that the major course of a river ran through the present day Ghaggar River. Another theory suggests that the Helmand River of southern Afghanistan corresponds to the Sarasvati River, while other scholars have argued that the Sarasvati was a mythical river, signifying the Milky Way.
For centuries, Sarasvati is regarded to exist as "subtle or mythic" and said to form a confluence with the physical sacred rivers Ganges and Yamuna at the Triveni Sangam, in an invisible form.
Etymology.
"Sarasvatī" is the devi feminine of an adjective "sarasvant-" (which occurs in the Rigveda as the name of the keeper of the celestial waters), derived from Proto-Indo-Iranian *"sáras-vat-ī" (and earlier, PIE "*séles-u̯n̥t-ih₂"), meaning ‘marshy, full of pools’.
Sanskrit "sáras" means ‘pool, pond’; the feminine "sarasī́" means ‘stagnant pool, swamp’. Like its cognates Welsh "hêl, heledd" ‘river meadow’ and Greek ἕλος ("hélos") ‘swamp’, the Rigvedic term refers mostly to stagnant waters, and Mayrhofer considers unlikely a connection with the root *"sar-" ‘run, flow’.
"Sarasvatī" is an exact cognate with Avestan "Haraxvatī", perhaps originally referring to Arədvī Sūrā Anāhitā (modern "Ardwisur Anahid"), the Zoroastrian mythological world river, which would point to a common Indo-Iranian myth of a cosmic or mystical "Sáras-vat-ī" river. In the younger Avesta, "Haraxvatī" is Arachosia, a region described to be rich in rivers, and its Old Persian cognate "Harauvati", which gave its name to the present-day Hārūt River in Afghanistan, may have referred to the entire Helmand drainage basin (the center of Arachosia).
Importance.
The Saraswati river was revered and considered important for Hindus because it is believed that it was on its banks in the Vedic state of Brahmavarta, that Vedic Sanskriti saw the light, and important Vedic scriptures like Manusmriti, initial part of Rigveda and several Upanishads were composed by Vedic seers after the great floods, some 10,000 years ago.
In the Rigveda.
The Sarasvati River is mentioned in all but the fourth book of the Rigveda. The most important hymns related to Sarasvati are RV 6.61, RV 7.95 and RV 7.96.
As a goddess.
The Sarasvati is mentioned some fifty times in the hymns of the Rig Veda. it is mentioned in thirteen hymns of the late books (1 and 10) of the Rigveda. Only two of these references are unambiguously to the river: 10.64.9, calling for the aid of three "great rivers", Sindhu, Sarasvati and Sarayu; and 10.75.5, the geographical list of the Nadistuti sukta. The others invoke Sarasvati as a goddess without direct connection to a specific river.
In 10.30.12, her origin as a river goddess may explain her invocation as a protective deity in a hymn to the celestial waters. In 10.135.5, as Indra drinks Soma he is described as refreshed by Sarasvati. The invocations in 10.17 address Sarasvati as a goddess of the forefathers as well as of the present generation. In 1.13, 1.89, 10.85, 10.66 and 10.141, she is listed with other gods and goddesses, not with rivers. In 10.65, she is invoked together with "holy thoughts" ("dhī") and "munificence" ("puraṃdhi"), consistent with her role as a goddess of both knowledge and fertility. Let us try to view another aspect of this goddess in the light of Sri Aurobindo's symbolic interpretation. Sri Aurobindo categorically states,"The symbolism of the Veda betrays itself to the greatest clearness in the figure of the goddess Sarasvati...She is, plainly and clearly, the goddess of the Word, the goddess of a divine inspiration... ".
Though Sarasvati initially emerged as a river goddess in the Vedic scriptures, in later Hinduism of the Puranas, she was rarely associated with the river and emerged as an independent goddess of knowledge, music, arts, wisdom and learning.
Other Vedic texts.
In post-Rigvedic literature, the disappearance of the Sarasvati is mentioned. Also the origin of the Sarasvati is identified as Plaksa Prasravana.
In a supplementary chapter of the Vajasaneyi-Samhita of the Yajurveda (34.11), Sarasvati is mentioned in a context apparently meaning the Sindhu: "Five rivers flowing on their way speed onward to Sarasvati, but then become Sarasvati a fivefold river in the land." According to the medieval commentator Uvata, the five tributaries of the Sarasvati were the Punjab rivers Drishadvati, Satudri (Sutlej), Chandrabhaga (Chenab), Vipasa (Beas) and the Iravati (Ravi).
The first reference to the disapparance of the lower course of the Sarasvati is from the Brahmanas, texts that are composed in Vedic Sanskrit, but dating to a later date than the Veda Samhitas. The Jaiminiya Brahmana (2.297) speaks of the 'diving under (upamajjana) of the Sarasvati', and the Tandya Brahmana (or Pancavimsa Br.) calls this the 'disappearance' (vinasana). The same text (25.10.11-16) records that the Sarasvati is 'so to say meandering' (kubjimati) as it could not sustain heaven which it had propped up.
The Plaksa Prasravana (place of appearance/source of the river) may refer to a spring in the Siwalik mountains. The distance between the source and the Vinasana (place of disappearance of the river) is said to be 44 asvina (between several hundred and 1600 miles) (Tandya Br. 25.10.16; cf. Av. 6.131.3; Pancavimsa Br.)
In the Latyayana Srautasutra (10.15-19) the Sarasvati seems to be a perennial river up to the Vinasana, which is west of its confluence with the Drshadvati (Chautang). The Drshadvati is described as a seasonal stream (10.17), meaning it was not from Himalayas. Bhargava has identified Drashadwati river as present day Sahibi river originating from Jaipur hills in Rajasthan. The Asvalayana Srautasutra and Sankhayana Srautasutra contain verses that are similar to the Latyayana Srautasutra.
Post-Vedic texts.
According to the Mahabharata, the Sarasvati dried up in a desert (at a place named Vinasana or Adarsana); after having disappeared in the desert, reappears in some places; and joins the sea "impetuously". MB.3.81.115 locates Kurukshetra to the south of the Sarasvati and north of the Drishadvati. The dried up seasonal Ghaggar River in Rajasthan and Haryana reflects the same geographical view described in the Mahabharata.
According to Hindu scriptures, a journey was made during the Mahabharata by Balrama along the banks of the Saraswati from Dwarka to Mathura. There were ancient kingdoms too (the era of the Mahajanapads) that lay in parts of north Rajasthan and that were named on the Saraswati River.
Several Puranas describe the Sarasvati River, and also record that the river separated into a number of lakes ("saras").
In the Skanda Purana, the Sarasvati originates from the water pot of Brahma and flows from Plaksa on the Himalayas. It then turns west at Kedara and also flows underground. Five distributaries of the Sarasvati are mentioned. The text regards Sarasvati as a form of Brahma's consort Brahmi. According to the Vamana Purana 32.1-4, the Sarasvati rose from the Plaksa tree (Pipal tree).
Identification theories.
Attempts have been made to identify the mythical Sarasvati of the Vedas with concrete rivers. Many think that the Vedic Sarasvati river once flowed east of the Indus (Sindhu) river. Scientists, geologists as well as scholars have identified the Sarasvati with many present-day or now defunct rivers.
Two theories are popular in the attempts to identify the Sarasvati. Several scholars have identified the river with the present-day Ghaggar-Hakra River or dried up part of it, which is located in Northwestern India and Pakistan. A second popular theory associates the river with the Helmand river or an ancient river in the present Helmand Valley in Afghanistan. Others consider Sarasvati a mythical river.
Ghaggar-Hakra River.
The Ghaggar-Hakra River is an intermittent river in India and Pakistan that flows only during the monsoon season.
Identification with the Sarasvati.
Many scholars as well as geologists have identified the Sarasvati river with the present-day Ghaggar-Hakra River, or the dried up part of it. The main arguments are the supposed position east of the Indus, which corresponds with the Ghaggar-Hakra riverbed; the actual absence of a "mighty river" east of the Indus, which may be explained by the drying up of the historical Ghaggar-Hakra river; and the resemblance between the "diving under" of the Puranic Sarasvati, and the ending of the present-day Ghaggar-Hakra river in a desert.
The identification of the Vedic Sarasvati River with the Ghaggar-Hakra River was proposed by some scholars in the 19th and early 20th century, including Christian Lassen, Max Müller, Marc Aurel Stein, C.F. Oldham and Jane Macintosh. Danino notes that "the 1500 km-long bed of the Sarasvati" was "rediscovered" in the 19th century. According to Danino, "most Indologists" were convinced in the 19th century that "the bed of the Ghaggar-Hakra was the relic of the Sarasvati."
Romila Thapar terms the identification "controversial" and dismisses it, noticing that the descriptions of Sarasvati flowing through the "high mountains" does not tally with Ghaggar's course and suggests that Sarasvati is Haraxvati of Afghanistan. Wilke suggests that the identification is problematic since the Ghaggar-Hakra river was already dried up at the time of the composition of the Vedas, let alone the migration of the Vedic people into northern India.
Course of the historical Ghaggar-Hakra River.
The historical Ghaggar-Hakra river, identified with the Sarasvati, flowed down the present Ghaggar-Hakra River channel, and that of the Nara in Sindh. Satellite images in possession of the ISRO and ONGC have confirmed that the major course of a river ran through the present day Ghaggar River.
The full flow of the paleo-Ghaggar-Hakra River was not present during the Holocene. According to Clift et al. and Giosan et al. the Yamuna and Sutlej were lost during the Pleistocene, and the Ghaggar-Hakra River was a much smaller river, fed entirely by monsoon rains rather than glacial streams, during the mid-late Holocene (including the Vedic period).
Drying-up of the Ghaggar-Hakra system.
Late in the 2nd millennium BCE the Ghaggar-Hakra fluvial system dried up, which affected the Harappan civilisation.
Giosan et al., in their study "Fluvial landscapes of the Harappan civilisation", make clear that the Ghaggar-Hakra fluvial system was not a large glacierfed Himalayan river, but a monsoonal-fed river.[#endnote_] They concluded that the Indus Valley Civilisation died out because the monsoons, which fed the rivers that supported the civilisation, migrated to the east. With the rivers drying out as a result, the civilisation diminished some 4000 years ago. This particular effected the Ghaggar-Hakra system, which became ephemeral and was largely abandoned. The Indus Valley Civilisation had the option to migrate east toward the more humid regions of the Indo-Gangetic Plain, where the decentralized late Harappan phase took place.
Painted Grey Ware sites (ca. 1000 BCE) have been found in the bed and not on the banks of the Ghaggar-Hakra river, suggesting that the river had dried up before this period.
Other scenarios suppose that geological changes diverted the Sutlej towards the Indus and the Yamuna towards the Ganges, following which the river did not have enough water to reach the sea any more and dried up in the Thar desert. Active faults are present in the region, and lateral and vertical tectonic movements have frequently diverted streams in the past. The Saraswati may have migrated westward due to such uplift of the Aravallis. According to geologists Puri and Verma a major seismic activity in the Himalayan region caused the rising of the Bata-Markanda Divide. This resulted in the blockage of the westward flow of Sarasvati forcing the water back. Since the Yamunā Tear opening was not far off, the blocked water exited from the opening into the Yamunā system.
Apart from the above reasons, the following can be the possible reasons for the drying up of the river:
Identification with the Indus Valley Civilisation.
The Indus Valley Civilisation (Harrapan Civilisation), which is named after the Indus, was largely located on the banks of and in the proximity of the Ghaggar-Hakra fluvial system. Kalyanaraman concludes that the drying-up of the Ghaggar-Hakra resulted in the abandonment of the valley by the Mature Harappans. They moved into the region between the upper reaches of Gangā and Yamunā going in the north-eastwards direction. This is supported by the evidence of the occurrence of a very few Mature Harappan sites but Late Harappan sites in that region.
The Indus Valley Civilisation is sometimes called the "Sarasvati culture", the "Sarasvati Civilization", the "Indus-Sarasvati Civilization" or the "Sindhu-Sarasvati Civilization", as it is theorized that the civilisation flourished on banks of the Sarasvati river, along with the Indus. Danino notes that the dating of the Vedas to the third millennium BCE coincides with the mature phase of the Indus Valley civilisation, and that it is "tempting" to equate the Indus Valley and Vedic cultures.
Helmand river.
Suggestions for the identity of the early Rigvedic Sarasvati River include the Helmand River in Afghanistan, separated from the watershed of the Indus by the Sanglakh Range. The Helmand historically besides Avestan "Haetumant" bore the name "Haraxvaiti", which is the Avestan form cognate to Sanskrit "Sarasvati". The Avesta extols the Helmand in similar terms to those used in the Rigveda with respect to the Sarasvati: "the bountiful, glorious Haetumant swelling its white waves rolling down its copious flood".
Kocchar (1999) argues that the Helmand is identical to the early Rigvedic Sarasvati of suktas 2.41, 7.36 etc., and that the Nadistuti sukta (10.75) was composed centuries later, after an eastward migration of the bearers of the Rigvedic culture to the western Gangetic plain some 600 km to the east. The Sarasvati by this time had become a mythical "disappeared" river, and the name was transferred to the Ghaggar which disappeared in the desert.
The identification of the Helmand with the early Rig Vedic Sarasvati is not without difficulties. However, the geographic situation of the Sarasvati and the Helmand rivers are similar. Both flow into a terminal lakes: the Helmand into a swamp in the Iranian plateau (the extended wetland and lake system of Hamun-i-Helmand). This matches the Rigvedic description of the Sarasvati flowing to the "samudra", which at that time meant 'confluence', 'lake', 'heavenly lake, ocean'; the current meaning of 'terrestrial ocean' was not even felt in the Pali Canon. In post-Rig Vedic texts (Brahmanas) the Sarasvati ("she who has (many) lakes"), is said to disappear ("dive under") in the desert.
Because the Nadi Sukta of the Rig Veda (10.75.5) place the Sarasvati between the Yamuna and the Ghaggar, the Helmand is ruled out as being the historical Sarasvati since there are no rivers in Afghanistan by the names Yamuna and Ghaggar.
Also because the Rig Veda (10.92.2) mentions that Sarasvati rose from the mountains and fell into the ocean, the Helmand is ruled out as being the Sarasvati because Helmand does not flow into the ocean.
Mythical river.
According to Michael Witzel, the Vedic "Sarasvati is not an earthly river, but the Milky Way that is seen as a road to immortality and heavenly after-life." The description of the Sarasvati as the river of heavens, is interpreted to suggest its mythical nature.
Ashoke Mukherjee (2001) is critical of the attempts to identify the Rigvedic Sarasvati. Mukherjee notes that many historians and archaeologists, both Indian and foreign, concluded that the word "Sarasvati" (literally "being full of water") is not a noun, a specific "thing". However, Mukherjee believes that "Sarasvati" is initially used by the Rig Vedic people as an adjective to the Indus as a large river and later evolved into a "noun". Mukherjee concludes that the Vedic poets had not seen the palaeo-Sarasvati, and that what they described in the Vedic verses refers to something else. He also suggests that in the post-Vedic and Puranic tradition the "disappearance" of Sarasvati, which to refers to "[going] under [the] ground in the sands", was created as a complementary myth to explain the visible non-existence of the river. Suggesting a political angle, he accuses "the BJP-led Governments at the centre and in some states to boost up Hindu religious sentiments and prejudices over some of the sensitive areas of Indian history."
Drying-up and dating of the Vedas.
The Vedic and Puarnic statements about the drying-up and diving-under of the Sarasvati have been used as a reference point for the dating of the Harappan civilisation and the Vedic culture. Some see these texts as evidence for an earlier dating of the Rig Veda, identifying the Sarasvati with the Ghaggar-Hakra River, rejecting the Indo-Aryan migrations theory, which postulates a migration at 1500 BCE.
Danino places the composition of the Vedas in the third millennium BCE, a century earlier than the conventional dates. Danino notes that accepting the Rig Veda accounts as factual descriptions, and dating the drying up late in the third millennium, are incompatible. According to Danino, this suggests that the Vedic people were present in northern India in the third millennium BCE, a conclusion which is drawn by some Indian archaeologists, but not by Western archaeologists. Danino states that there is an absence of "any intrusive material culture in the Northwest during the second millennium BCE," a biological continuity in the skeletal remains, and a cultural continuity. Danino then states that if the "testimony of the Sarasvati is added to this,"
[T]he simplest and most natural conclusion is that the Vedic culture was present in the region in the third millennium.
Danino acknowledges that this asks for "studying its tentacular ramifications into linguistics, archaeoastronomy, anthropology and genetics, besides a few other fields".
Annette Wilke notes that the "historical river" Sarasvati was a "topographically tangible mythogeme", which was already reduced to a "small, sorry tickle in the desert", by the time of composition of the Hindu epics. These post-Vedic texts regularly talk about drying up of the river, and start associating the goddess Sarasvati with language, rather than the river.
Michael Witzel also notes that the Rig Veda indicates that the Sarsvati "had already lost its main source of water supply and must have ended in a terminal lake (samudra)."
Contemporary religious meaning.
Diana Eck notes that the power and significance of the Sarasvati for present-day India is in the persistent symbolic presence at the confluence of rivers all over India. Although "materially missing", she is the third river, which emerges to join in the meeting of rivers, thereby making the waters triple holy.
After the Vedic Sarasvati dried, new myths about the rivers arose. Sarasvati is described to flow in the underworld and rise to the surface at some places. For centuries, the Sarasvati river existed in a "subtle or mythic" form, since it corresponds with none of the major rivers of present-day South Asia. The flowing together of the Ganges and Yamuna rivers at Triveni Sangam, Allahabad, converging with the unseen Sarasvati river, which is believed to flow underground. The "Padma Purana" proclaims:
One who bathes and drinks there where the Gangā, Yamunā and Sarasvati join enjoys liberation. Of this there is no doubt."
The Kumbh Mela, a mass bathing festival is held at Triveni Sangam, literally "confluence of the three rivers", every 12 years. The belief of Sarasvati joining at the confluence of the Ganges and Yamuna originates from the Puranic scriptures and denotes the "powerful legacy" the Vedic river left after her disappearance. The belief is interpreted as "symbolic". The three rivers Sarasvati, Yamuna, Ganga are considered consorts of the Hindu Trinity (Trimurti) Brahma, Vishnu (as Krishna) and Shiva respectively.
In lesser known configuration, Sarasvati is said to form the "Triveni" confluence with rivers Hiranya and Kapila at Somnath. There are several other "Triveni"s in India where two physical rivers are joined by the "unseen" Sarasvati, which adds to the sanctity of the confluence.
Romila Thapar notes that "once the river had been mythologized through invoking the memory of the earlier river, its name - Sarasvati - could be applied to many rivers, which is what happened in various parts of the [Indian] subcontinent."
Several present-day rivers are also named Sarasvati, after the Vedic Sarasvati:
Sources.
</dl>

</doc>
<doc id="32755" url="http://en.wikipedia.org/wiki?curid=32755" title="Videos and audio recordings of Osama bin Laden">
Videos and audio recordings of Osama bin Laden

There were several video and audio recordings released by Osama bin Laden between 2001 and 2011. Most of the tapes were released directly (by mail or messenger) to Arabic language satellite television networks, primarily al Jazeera.
Audio or video.
September 16, 2001.
Osama bin Laden denied any involvement with the September 11, 2001 attacks by reading a statement which was broadcast by the al Jazeera satellite channel: "I stress that I have not carried out this act, which appears to have been carried out by individuals with their own motivation." This denial was broadcast on U.S. news networks and worldwide.
October 7, 2001.
Just after US and NATO forces launched strikes in Afghanistan, bin Laden released a video tape, stating, "America has been hit by Allah at its most vulnerable point, destroying, thank God, its most prestigious buildings," referencing the September 11 attacks on the US. bin Laden did not claim responsibility for the attacks in the recording.
November 3, 2001.
Bin Laden released another video tape, excoriating the West, the United Nations, and Israel, and explaining all of the unfolding events as fundamentally a religious war.
December 13, 2001.
On November 9, 2001, U.S. military forces in Jalalabad found a video tape of bin Laden.
On December 13, 2001, the United States State Department released a video tape apparently showing bin Laden speaking with Khaled al-Harbi and other associates, somewhere in Afghanistan, before the U.S. invasion had driven the Taliban regime from Kandahar. The State Department stated that the tape was captured by U.S. forces in Afghanistan during a raid on a house in Jalalabad. The tape was aired with an accompanying English translation. In this translation, Osama bin Laden displays knowledge of the timing of the actual attack a few days in advance; the translation attributes the following lines to bin Laden:
On December 20, 2001, German TV channel "Das Erste" broadcast an analysis of the White House's translation of the videotape. On the program "Monitor", two independent translators and an expert on oriental studies found the White House's translation to be both inaccurate and manipulative stating "At the most important places where it is held to prove the guilt of bin Laden, it is not identical with the Arabic" and that the words used that indicate foreknowledge can not be heard at all in the original. Prof. Gernot Rotter, professor of Islamic and Arabic Studies at the Asia-Africa Institute at the University of Hamburg said "The American translators who listened to the tapes and transcribed them apparently wrote a lot of things in that they wanted to hear but that cannot be heard on the tape no matter how many times you listen to it."
The American transcript and video is located at http://youtube.com/watch?v=x0FVeqCX6z8
December 27, 2001.
Bin Laden released another video tape, stating, "Terrorism against America deserves to be praised because it was a response to injustice, aimed at forcing America to stop its support for Israel, which kills our people." In the recording, bin Laden describes attacks by the United States against Islamic people. He describes his message as a review of events following 9/11, and in this statement, he neither admits nor denies responsibility for the 9/11 attacks.
November 12, 2002.
Al Jazeera broadcast the unauthenticated bin Laden recording. US law enforcement officials reported that bin Laden's probably the speaker on the tape. Most US experts who've heard or processed the tape have usually supported that conclusion, but the result of Idiap Research Institute shows that "there is serious room for doubt" that the speaker is bin Laden. The study concluded that the sample set of bin Laden recordings is too small to allow any researcher draw a conclusion regarding the videos validity.
February 12, 2003.
Colin Powell told a United States Senate panel that he'd reviewed a transcript of a message from bin Laden stating that he was in "partnership with Iraq", which was to be broadcast on al Jazeera. Al Jazeera initially denied having the tape, but subsequently located it.
September 10, 2003.
Al Jazeera broadcast footage of bin Laden and Ayman al-Zawahiri hiking in the mountains together.
October 29, 2004.
Shortly before the U.S. presidential election on October 29, 2004, Arab television network al Jazeera broadcast an 18-minute video tape of Osama bin Laden, addressed to citizens of the United States. According to the English translation distributed by the BBC and other media outlets, he tells viewers he personally directed the 19 hijackers, and describes his motivation:
I will explain to you the reasons behind these events, and I will tell you the truth about the moments when this decision was taken, so that you can reflect on it. God knows that the plan of striking the towers had not occurred to us, but the idea came to me when things went just too far with the American-Israeli alliance's oppression and atrocities against our people in Palestine and Lebanon.
Bin Laden claimed he was inspired to destroy the World Trade Center after watching the destruction of towers in Lebanon by Israel during the 1982 Lebanon War.
January 19, 2006.
Al Jazeera broadcast an audiotape of bin Laden addressing citizens of the United States again.
April 23, 2006.
Al Jazeera broadcast parts of an audiotape. On this tape, bin Laden accuses the Western world of waging a Zionist crusade against Islam. He comments on Hamas, Darfur and the situation in Iraq.
May 23, 2006.
Al Jazeera broadcast a 5-minute audiotape from bin Laden, where he claims he, alone, assigned the hijackers to perform the September 11 attacks. bin Laden comments on the trial of Zacarias Moussaoui, the prisoners in Guantanamo Bay, and the imprisoned journalists Sami al-Hajj and Tayssir Alouni, denying that any of them (apart from a few Guantanamo Bay prisoners) were connected with al-Qaeda.
bin Laden tells viewers that Zacarias Moussaoui "had no connection at all with September 11. I am the one in charge of the 19 brothers and I never assigned brother Zacarias to be with them in that mission. I am certain of what I say because I was responsible for entrusting the 19 brothers... with the raids."
June 30, 2006.
An Islamist website posted a recording, in which bin Laden praised Abu Musab al-Zarqawi as a "lion of Jihad". The 19-minute video shows a still picture of bin Laden next to video celebrating al-Zarqawi. US officials said that the tape was authentic.
July 14, 2007.
A few days after the Lal Masjid siege by the Pakistani government (and after a video released by al-Zawahiri), bin Laden released a tape. He was shown with a fatigued expression. He came out in a video clip less than a minute long on a militant website. BBC News reports that it was "undated and correspondents say it may be re-run footage. Its authenticity also cannot be independently verified."
September 11, 2007.
A second video appeared purportedly featuring a eulogy by bin Laden to 9/11 hijacker Waleed al-Shehri. In the video, a voice identified as bin Laden's delivers the 14-minute introduction. The voice is heard over a still picture of bin Laden, dressed and groomed as he appears in the September 7, 2007 video. The 33-minute balance of the recording is a video will read by al-Shehri.
September 20, 2007.
An audio tape appeared with bin Laden's voice over previously released footage of him. In the audio tape, bin Laden called on Pakistanis to overthrow President Pervez Musharraf, promising what he called retaliation for the storming of the Red Mosque in the capital, Islamabad, in July.
November 29, 2007.
An audio tape appeared purportedly by bin Laden has urged European countries to end their military involvement in Afghanistan.
December 29, 2007.
In an audiotape Osama bin Laden warned Iraq's Sunni Arabs against fighting al-Qaeda and vowed to expand the group's Jihad to Israel, threatening "blood for blood, destruction for destruction." 
The title of this message is "The Way To Frustrate The Conspiracies." Its transcript is located at 
March 19, 2008.
On March 19, 2008, an audio tape appeared purportedly by al-Qaeda leader Osama bin Laden "threatens" the EU over the re-printing of a cartoon offensive to Muslims.
The title of this message is "May our Mothers be bereaved of us if we fail to help our Prophet." Its full transcript and tape is located at 
March 20, 2008.
On March 20, 2008, a second audio tape appeared purportedly by al-Qaeda leader Osama bin Laden urging Palestinian militants to join the insurgency in Iraq.
May 16, 2008.
On May 16, 2008, an audio tape appeared purportedly by al-Qaeda leader Osama bin Laden issued to the Western People: "Reasons of the struggle on the occasion of the 60th anniversary of the founding of the occupying state of Israel" The message is 22 minutes, 41 seconds long. The message addresses the 60th anniversary of the founding of the state of Israel.
May 18, 2008.
On May 18, 2008, a second audio tape appeared purportedly by al-Qaeda leader Osama bin Laden issued 'A very strong statement to the Islamic nation and Muslims'. The second in three days, once again is an audio message, with no new imagery of bin Laden seen. A video version of the message was released showing an older still image of bin Laden while an audio message plays in the background.
January 14, 2009.
On January 14, 2009, a new audio recording purportedly by bin Laden posted on Islamist websites called for a new jihad over the Israeli offensive in Gaza and said the global financial crisis has exposed the waning influence of the United States in world affairs and that it will in turn weaken its ally Israel. The recording was accompanied by a still photo of Bin Laden superimposed over the al-Aqsa mosque in Jerusalem.
June 3, 2009.
According to a recording aired by Al Jazeera on June 3, Bin Laden made a statement against President Barack Obama.
September 13, 2009.
On September 13, 2009, an audio recording purportedly by Osama Bin Laden was released on an Islamist website.
For a transcript, see .
September 25, 2009.
On September 25, 2009, a forum participant posted to a jihadist website links to download an audio statement from al-Qaeda leader Osama Bin Ladin. The statement, entitled "A Message From Shaykh Osama Bin Ladin to the People of Europe," is 4 minutes, 47 seconds, and in Arabic with German and English subtitles available. The video shows a still photograph of Bin Laden. 
For a transcript, see 
January 24, 2010.
New audio tape released to al-Jazeera news features a voice claiming to be Osama bin Laden taking credit for the attempted bombing by Umar Farouk Abdulmutallab. The authenticity of the tape has not yet been verified.
January 29, 2010.
A tape allegedly (remains unverified) from bin Laden was released by al-Jazeera, in which the leader of al-Qaeda excoriated the U.S. for its reluctance to address climate change. Bin Laden called for a worldwide boycott of American goods and the United States dollar in order to have a huge impact on the American economy.
March 25, 2010.
Another new audio tape allegedly from bin Laden was released by al-Jazeera. In this tape, the leader of al-Qaeda threatened that the execution of Khalid Sheikh Mohammed would result in al-Qaeda indefinitely murdering Americans in the event of their "falling into our hands".
October 1, 2010.
An audiotape allegedly featuring the voice of bin Laden surfaced on the Internet and addresses the flooding in Pakistan. The 11-minute tape, posted on militant websites Friday, focused on relief efforts and what can be done to prevent future natural disasters. In the recording, bin Laden reportedly urges a change in how governments execute relief work and calls for the creation of a relief group to study Muslim regions located near rivers and low-lying areas. He also calls for a greater investment in agriculture. The U.S.-based SITE Intelligence Group, which monitors terrorist forums, says the latest message is heard in a video featuring a photograph of bin Laden superimposed over images of aid distribution.
October 2, 2010.
The audio of a follow up speech allegedly by Osama bin Laden was released onto the internet. The title of the speech was "Help Your Pakistani Brothers" in which he expressed his feeling that the response of Arab and Muslim leaders to the Pakistani's affected by the flooding "did not match the level of the disaster" and that more people are affected by climate change than wars. The authenticity of the tape has not been independently verified.
October 27, 2010.
In a new audio tape from Osama bin Laden he is demanding that France withdraw its troops from Afghanistan.
January 21, 2011.
On January 21, 2011, Osama bin Laden said the release of French hostages depends on a pullout of France's soldiers in Afghanistan and warned Paris of a "high price" for its policies, in a tape broadcast on Friday. "We repeat the same message to you: The release of your prisoners in the hands of our brothers is linked to the withdrawal of your soldiers from our country," said the speaker on the audiotape broadcast on Al-Jazeera television. This was the final tape released by Bin Laden prior to the confirmation of his death on May 2, 2011. A new audio message that Osama Bin Laden recorded before his death was released on May 7, 2011.
May 19, 2011.
A recording purported to have been made by Osama shortly before he died has been released by al-Qaeda. In the message, he praised the revolutions in Tunisia and Egypt and speaks of a "rare historic opportunity" for Muslims to rise up. The 12-minute audio message came out on a video posted on Islamist websites, and has been translated by the SITE Institute.

</doc>
<doc id="32769" url="http://en.wikipedia.org/wiki?curid=32769" title="Vannevar Bush Award">
Vannevar Bush Award

The National Science Board established the Vannevar Bush Award ( ) in 1980 to honor Dr. Vannevar Bush's unique contributions to public service. The annual award recognizes an individual who, through public service activities in science and technology, has made an outstanding "contribution toward the welfare of mankind and the Nation." The recipient of the award receives a bronze medal struck in the memory of Dr. Bush. 
Dr. Bush (1890–1974) was a prominent scientist, adviser to Presidents, and the force behind the establishment of the National Science Foundation. In 1945, at the request of President Franklin D. Roosevelt, he recommended that a foundation be established by Congress to serve as a focal point for the Federal Government's support and encouragement of research and education in science and technology as well as the development of a national science policy. The legislation creating the National Science Foundation was signed by President Harry S. Truman on May 10, 1950
List of winners.
Source: 

</doc>
<doc id="32777" url="http://en.wikipedia.org/wiki?curid=32777" title="Victor of Aveyron">
Victor of Aveyron

Victor of Aveyron (c. 1788 – 1828) was a French feral child who was found in 1800 after apparently spending the majority of his childhood alone in the woods. Upon his discovery, his case was taken up by a young physician, Jean Marc Gaspard Itard, who worked with the boy for five years and gave him his name, Victor. Itard was interested in determining what Victor could learn. He devised procedures to teach the boy words and recorded his progress. Based on his work with Victor, Itard broke new ground in the education of the developmentally delayed.
Early life.
Victor is estimated to have been born around 1788. He was prepubescent when he was captured in 1800, but advanced to puberty within a year or two. It is not known when or how he came to live in the woods near Saint-Sernin-sur-Rance, though he was reportedly seen there around 1794. In 1797 he was spotted by three hunters; he ran from them but they were able to catch him when he tried to climb a tree. They brought him to a nearby town where he was cared for by a widow. However, he soon escaped and returned to the woods; he was periodically spotted in 1798 and 1799. On January 8, 1800, he emerged from the forests on his own. His age was unknown, but citizens of the village estimated his age to be that of around twelve. His lack of speech, as well as his food preferences and the numerous scars on his body, suggested to some that he had been in the wild for most of his life.
Study.
Shortly after Victor was found, a local abbot and biology professor, Pierre Joseph Bonnaterre, examined him. He removed the boy's clothing and led him outside into the snow, where, far from being upset, Victor began to frolic about in the nude, showing Bonnaterre that he was clearly accustomed to exposure and cold. The local government commissioner, Constans-Saint-Esteve, also observed the boy and wrote there was "something extraordinary in his behavior, which makes him seem close to the state of wild animals".:9 The boy was eventually taken to Rodez, where two men traveled to discover whether or not he was their missing son. Both men had lost their sons during the French Revolution, but neither claimed the boy as his son. There were other rumors regarding the boy's origins. For example, one rumor insisted the boy was the illegitimate son of a "notaire" abandoned at a young age because he was mute.:17 Itard believed Victor had "lived in an absolute solitude from his fourth or fifth almost to his twelfth year, which is the age he may have been when he was taken in the Caune woods." That means he presumably lived for seven years in the wilderness.:10
It was clear that Victor could hear, but he was taken to the National Institute of the Deaf in Paris for the purpose of being studied by the renowned Roch-Ambroise Cucurron Sicard. Sicard and other members of the Society of Observers of Man believed that by studying, as well as educating the boy, they would gain the proof they needed for the recently popularized empiricist theory of knowledge.:5 In the context of the Enlightenment, when many were debating what exactly distinguished man from animal, one of the most significant factors was the ability to learn language. By studying the boy, they would also be able to explain the relationship between man and society.
Influence of the Enlightenment.
The Enlightenment caused many thinkers, including naturalists and philosophers, to believe human nature was a subject that needed to be redefined and looked at from a completely different angle. Because of the French Revolution and new developments in science and philosophy, man was looked at as not special, but as characteristic of his place in nature.:42 It was hoped that by studying the wild boy, this idea would gain support. He became a case study in the Enlightenment debate about the differences between humans and other animals.
At that time, the scientific category "Juvenis averionensis" was used, as a special case of the "Homo ferus", described by Carl von Linné in "Systema Naturae". Linnaeus and his discoveries, then, forced people to ask the question, what makes us men? Another developing idea that was prevalent during the Enlightenment was the idea of the noble savage. Some believed a man, existing in the pure state of nature, would be "gentle, innocent, a lover of solitude, ignorant of evil and incapable of causing intentional harm."
Philosophies proposed by the likes of Rousseau, Locke and Descartes were evolving around the time when the boy was discovered in France in 1800. These philosophies invariably had an influence on how the boy was looked at, and eventually, how his education would be constructed by Itard.
Influence of colonialism.
Simpson points out there was a "direct link between the discourse of colonialism abroad and internal regulation of deviants back home." The same way in which Europeans viewed the "Other" in colonies and other exotic locations was how the French people saw the Wild Boy of Aveyron. To lack reason and understanding during the Enlightenment was to be uncivilized. The attitudes that Europeans extended toward the Other were paralleled by Victor, as he too was considered "uncivilized" because of his lack of language and, therefore, reason. These characteristics defined mankind for Victor's contemporaries.
Education.
It was said that even though he had been exposed to society and education, he had made little progress at the Institution under Sicard. Many people questioned his ability to learn because of his initial state, and as Yousef explains, "it is one thing to say that the man of nature is not yet fully human; it is quite another thing to say that the man of nature cannot become fully human." After Sicard became frustrated with the lack of progress made by the boy, he was left to roam the institution by himself, until Itard decided to take the boy into his home to keep reports and monitor his development.
Jean Marc Gaspard Itard.
Jean Marc Gaspard Itard, a young medical student, effectively adopted Victor into his home and published reports on his progress. Itard believed two things separated humans from animals: empathy and language. He wanted to civilize Victor with the objectives of teaching him to speak and to communicate human emotion. Victor showed significant early progress in understanding language and reading simple words, but failed to progress beyond a rudimentary level. Itard wrote, "Under these circumstances his ear was not an organ for the appreciation of sounds, their articulations and their combinations; it was nothing but a simple means of self-preservation which warned of the approach of a dangerous animal or the fall of wild fruit.":26
The only two phrases Victor ever actually learned to spell out were "lait" (milk) and "Oh, Dieu" (Oh, God). It would seem, however, that Itard implemented more contemporary views when he was educating Victor. Rousseau appears to have believed "that natural association is based on reciprocally free and equal respect between people." This notion of how to educate and to teach was something that although did not produce the effects hoped for, did prove to be a step towards new systems of pedagogy. By attempting to learn about the boy who lived in nature, education could be restructured and characterized.
Itard has been recognized as the founder of "oral education of the deaf; the field of otolaryngology; the use of behavior modification with severely impaired children; and special education for the mentally and physically handicapped."
While Victor did not learn to speak the language that Itard tried to teach him, it seems that Victor did make progress in his behavior towards other people. At the Itard home, housekeeper Madame Guérin was setting the table one evening while crying over the loss of her husband. Victor stopped what he was doing and displayed consoling behavior towards her. Itard reported on this progress.
Language.
When looking at the association between language and intellect, French society considered one with the other. Unless cared for by friends or family, most people considered "dumb" ended up in horrible, ghastly conditions. However, around 1750, something different was happening in Paris. A French priest, Charles-Michel de l'Épée, created a school to educate deaf-mutes. His institution was made into a National Institute in 1790.:61 This new interest and moral obligation towards deaf-mutes inspired Itard to nurture and attempt to teach Victor language. "He had Locke's and Condiallac's theory that we are born with empty heads and that our ideas arise from what we perceive and experience. Having experienced almost nothing of society, the boy remained a savage.":73
Throughout the years Itard spent working with Victor, he made some gradual progress. Victor understood the meaning of actions and used what Shattuck describes as "action language", which Itard regarded as a kind of primitive form of communication.:98 However, Itard still could not get Victor to speak. He wondered why Victor would choose to remain mute when he had already proved that he was not, in fact, deaf. Victor also did not understand tones of voice. Itard proclaimed "Victor was the mental and psychological equivalent of a born deaf-mute. There would be little point in trying to teach him to speak by the normal means of repeating sounds if he didn't really hear them.":139–140
Shattuck critiques Itard's process of education, wondering why he never attempted to teach Victor to use sign language. Regardless, today there are certain hypotheses that Shattuck applies to Victor. "One is that the Wild Boy, though born normal, developed a serious mental or psychological disturbance before his abandonment. Precocious schizophrenia, infantile psychosis, autism—a number of technical terms have been applied to his position. Several psychiatrists I have consulted favor this approach. It provides both a motivation for abandonment and an explanation for his partial recovery under Itard's treatment.":169
Victor died in Paris in 1828 in the home of Madame Guérin.
In Popular Culture.
Victor's life was dramatized in François Truffaut's 1970 film "L'Enfant sauvage" (marketed in the UK as "The Wild Boy" and in the US as "The Wild Child").
Victor's story was retold through dramatizations in a fourth-season episode of "In Search Of...", titled "Wild Children", in 1980.
Victor's story as well as the 1970's film helped inspire the 2012 album L'Enfant Sauvage by French metal band Gojira
In literature.
Victor's life has been fictionalized in the 2003 novel "Wild Boy" by Jill Dawson, and in the title novella of the 2010 collection "Wild Child and Other Stories" by T. C. Boyle. Also in Mordicai Gerstein's novel titled "Victor: A Novel Based in the Life of the Savage of Aveyron." The story is retold as children's nonfiction in Mary Losure's "Wild Boy: The Real Life of the Savage of Aveyron".
Recent commentary.
Professor Uta Frith has stated she believes Victor displayed signs of autism. Serge Aroles, in his book "L'énigme des enfants-loups" ("The Mystery of the Wolf-Children"), also believes that surviving accounts of his behavior point to "an average degree of autism" ("autisme moderé") in Victor's case. He notes that he showed characteristic signs of mental derangement, such as grinding of the teeth, incessant rocking back and forth, and spasmodic movements.
In March 2008, following the disclosure that Misha Defonseca's best-selling book, later turned into film "Survivre Avec les Loups" (Survival with Wolves) was a hoax, there was a debate in the French media (newspapers, radio and television) concerning the numerous false cases of feral children uncritically believed. Although there are numerous books on this subject, almost none of them have been based on archives, the authors using rather dubious second or third-hand, printed information. According to French surgeon Serge Aroles, author of a general study of the phenomenon of feral children based on archives, almost all of these cases are fakes. In his judgment, Victor of Aveyron was not a genuine feral child: "Don't forget that Truffaut's movie is... a movie!" According to Aroles, the scars on the body of Victor were not the consequences of a wild life in the forests, but rather of physical abuse at the hands of his parents or whoever initially raised him. Humans need to be nurtured at least until the age of 5 or 6; it is inconceivable that any child, including Victor, could survive on his own, in the wild, younger than that. The fact that he could not speak a word at the time of his capture, even though he must have been around humans into early childhood, and never learned to speak thereafter despite Itard's intensive tutelage, suggests that he was mentally disabled—again, a diagnosis of autism seems to be gaining favor. This could also explain why he was abused, perhaps treated like an animal, in his earliest years.

</doc>
<doc id="32783" url="http://en.wikipedia.org/wiki?curid=32783" title="Antisemitism and the New Testament">
Antisemitism and the New Testament

It has been argued that the New Testament contributed toward subsequent antisemitism in the Christian community. A. Roy Eckardt has asserted that the foundation of antisemitism and responsibility for the Holocaust lies ultimately in the New Testament.
The New Testament and Christian antisemitism.
A. Roy Eckardt, a pioneer in the field of Jewish-Christian relations, asserted that the foundation of antisemitism and responsibility for the Holocaust lies ultimately in the New Testament. Eckardt insisted that Christian repentance must include a reexamination of basic theological attitudes toward Jews and the New Testament in order to deal effectively with antisemitism.
According to Rabbi Michael J. Cook, Professor of Intertestamental and Early Christian Literature at the Hebrew Union College, there are ten themes in the New Testament that are the greatest sources of anxiety for Jews concerning Christian antisemitism.
Cook believes that both contemporary Jews and contemporary Christians need to reexamine the history of early Christianity, and the transformation of Christianity from a Jewish sect consisting of followers of a Jewish Jesus, to a separate religion often dependent on the tolerance of Rome while proselytizing among Gentiles loyal to the Roman empire, to understand how the story of Jesus came to be recast in an anti-Jewish form as the Gospels took their final form.
Some scholars assert that critical verses in the New Testament have been used to incite prejudice and violence against Jewish people. Professor Lillian C. Freudmann, author of "Antisemitism in the New Testament" (University Press of America, 1994) has published a study of such verses and the effects that they have had in the Christian community throughout history. Similar studies have been made by both Christian and Jewish scholars, including, Professors Clark Williamsom (Christian Theological Seminary), Hyam Maccoby (The Leo Baeck Institute), Norman A. Beck (Texas Lutheran College), and Michael Berenbaum (Georgetown University).
Jewish-Christian conflict in the New Testament.
There are some verses in the New Testament that describe Jews in a positive way, attributing to them salvation or divine love. In the story of the crucifixion, meanwhile, Jews prompt Jesus' execution and say "His blood be on us, and on our children", referred to as the blood curse. In the Book of John, Jesus calls certain Pharisees "children of the devil".
According to the New Testament Gospels, Jesus, on his fateful entry into Jerusalem before Passover, was received by a great crowd of people. Jesus was arrested and tried by the Sanhedrin. After the trial, Jesus was handed over to Pontius Pilate, who duly tried him again and, at the urging of the people, had him crucified.
The New Testament records that Jesus' disciple Judas Iscariot, the Roman governor Pontius Pilate along with Roman forces and the leaders and people of Jerusalem were (to varying degrees) responsible for the death of Jesus.
Gospel of Mark.
According to the Gospel of Mark, Jesus' crucifixion was authorized by Roman authorities at the insistence of leading Jews (Judeans) from the Sanhedrin.
Paul H. Jones writes:
Although Mark depicts all of the Jewish groups united in their opposition to Jesus, his passion narratives are not "overtly" anti-Jewish, since they can be interpreted as falling within the range of "acceptable" intra-Jewish disputes. To some readers, the "cleansing of the Temple" scene (11:15-19) framed by the "withered fig tree" pericopes confirms God's judgment against the Jews and their Temple. Most likely, however, the story explains for this small sect of Jesus followers that survived the Roman-Jewish War why God permitted the destruction of the Temple. It is an in-house interpretation and, therefore, not anti-Jewish. Likewise, the parable of the vineyard (12:1-12), by which the traditional allegorical interpretation casts the tenants as the Jews, the murdered heir as Jesus, and the owner as God, must be set within the context of an intra-Jewish dispute.
Gospel of Matthew.
As Matthew's narrative marches toward the passion, the anti-Jewish rhetoric increases. In chapter 21, the parable of the vineyard is followed by the great "stone" text, an early Christological interpretation of : "The stone that the builders rejected has become the cornerstone". Then, in chapters 23 and 24, three successive hostile pericopes are recorded. First, a series of "woes" are pronounced against the Pharisees:
you testify against yourselves that you are descendants of those who murdered the prophets...You snakes, you brood of vipers! How can you escape being sentenced to hell?—
Then, Jesus laments over the capital: "Jerusalem, Jerusalem, the city that kills the prophets and stones those who are sent to it...See, your house is left to you, desolate" (). And finally, Jesus predicts the demise of the Temple: "Truly I tell you, not one stone will be left here upon another; all will be thrown down" ().
The culmination of this rhetoric, and arguably the one verse that has caused more Jewish suffering than any other second Testament passage, is the uniquely Matthean attribution to the Jewish people: "His [Jesus's] blood be on us and on our children!" (). This so-called "blood guilt" text has been interpreted to mean that all Jews, of Jesus' time and forever afterward, accept responsibility for the death of Jesus.
Shelly Matthews writes:
In Matthew, as in many books of the New Testament, the idea that Christ followers are persecuted is pervasive. Blessings are pronounced on those who are persecuted for righteousness sake in the Sermon on the Mount; the woes against the Pharisees in Matthew 23 culminate in predictions that they will "kill and crucify, flog in synagogues, and pursue from town to town;" the parable of the banquet in Matthew 22 implies that servants of the king will be killed by those to whom they are sent.
Douglas Hare noted that the Gospel of Matthew avoids sociological explanations for persecution:
Only the theological cause, the obduracy of Israel is of interest to the author. Nor is the mystery of Israel's sin probed, whether in terms of dualistic categories or in terms of predestinarianism. Israel's sin is a fact of history which requires no explanation.
The term "Jews" in the Gospel of Matthew is applied to those who deny the resurrection of Jesus and believe that the disciples stole Jesus's corpse.
Gospel of John.
The Gospel of John collectively describes the enemies of Jesus as "the Jews". In none of the other gospels do "the Jews" demand, en masse, the death of Jesus; instead, the plot to put him to death is always presented as coming from a small group of priests and rulers, the Sadducees. John's gospel is thus the primary source of the image of "the Jews" acting collectively as the enemy of Jesus, which later became fixed in the Christian mind.
For example, in Jesus moves around in Galilee but avoids Judea, because "the Jews" were looking for a chance to kill him. In some said "he is a good man" whereas others said he deceives the people, but these were all "whispers", no one would speak publicly for "fear of the Jews". Jewish rejection is also recorded in , , , and . says many did believe, but they kept it private, for fear the Pharisees would exclude them from the Synagogue. After the crucifixion, has the disciples hiding behind locked doors, "for fear of the Jews".
In several places John's gospel also associates "the Jews" with darkness and with the devil. In John 8:37-39; 44-47, Jesus says, speaking to a group of Pharisees:
I know that you are descendants of Abraham; yet you seek to kill me, because my word finds no place in you. I speak of what I have seen with my Father, and you do what you have heard from your father. They answered him, "Abraham is our father." Jesus said to them, "If you were Abraham's children, you would do what Abraham did. ... You are of your father the devil, and your will is to do your father's desires. He was a murderer from the beginning, and has nothing to do with the truth, because there is no truth in him. When he lies, he speaks according to his own nature, for he is a liar and the father of lies. But, because I tell the truth, you do not believe me. Which of you convicts me of sin? If I tell the truth, why do you not believe me? He who is of God hears the words of God; the reason why you do not hear them is you are not of God.
John's use of the term 'Jews' is a complex and debated area of biblical scholarship. Some scholars argue that the author most likely considered himself Jewish and was probably speaking to a largely Jewish community. New Testament scholar J.G. Dunn writes:
The Fourth Evangelist is still operating within a context of intra-Jewish factional dispute, although the boundaries and definitions themselves are part of that dispute. It is clear beyond doubt that once the Fourth Gospel is removed from that context, and the constraints of that context, it was all too easily read as an anti-Jewish polemic and became a tool of anti-semitism. But it is highly questionable whether the Fourth Evangelist himself can fairly be indicted for either anti-Judaism or anti-semitism.
Because of this controversy some modern English translations, such as Today's New International Version, remove the term "Jews" and replace it with more specific terms to avoid anti-Semitic connotations. For example, the Jesus Seminar translates this as "Judeans", i.e. residents of Judea, in contrast to residents of Galilee. Most critics of these translations, while conceding this point, argue that the context (since it is obvious that Jesus, John himself, and the other disciples were all Jews) makes John's true meaning sufficiently clear, and that a literal translation is preferred.
Paul Jones writes:
The Gospel of John has the dubious distinction of being both the most popular Gospel (considered the most "spiritual" of the canonical Gospels) and the most anti-Jewish. The term "the Jews" (Ioudaios) in the Gospel functions as a "hostile collective stereotype" and is identified with "evil" and the "devil." Yet the Gospel of John is intimately connected with Judaism. Jesus is thoroughly Jewish in this Gospel. His life revolves around the Jewish festivals, and his identity as the Messiah is confirmed by the Jewish scriptures. According to John 20:31, the book was written so "that you may come to believe that Jesus is the Messiah, the Son of God." Christology, therefore, is the key to understanding both the theology of the Gospel and its strained relationship with the larger Pharisaic Jewish tradition.
First Epistle to the Thessalonians.
According to Pieter Willem van der Horst, there is an instance of antisemitic statements in one of the Pauline epistles; Paul writes in 1 Thessalonians 2:14-16 as follows:
For you, brothers and sisters, became imitators of God’s churches in Judea, which are in Christ Jesus: You suffered from your own people the same things those churches suffered from the Jews who killed the Lord Jesus and the prophets and also drove us out. They displease God and are hostile to everyone in their effort to keep us from speaking to the Gentiles so that they may be saved. In this way they always heap up their sins to the limit. The wrath of God has come upon them at last.
Book of Revelation.
In Revelation 2:9 and 3:9 Jews appear to be called a synagogue of Satan. The idea of a Jewish Antichrist developed from these verses.
Later commentary.
Successive generations of Christians read in the Gospel of John the collective guilt of Jews, universally and in all generations, in the death of Christ. John's use of the collective expression "the Jews" is likely explained by the historical circumstances in which and audience for which he wrote. After the destruction of the Temple in 70 AD, the Jewish priesthood, and thus the class of the Sadducees, no longer existed. As John wrote his Gospel after these events, for a gentile audience, he spoke generically of Jews, rather than specifying a group within Judaism that no longer existed and that would have been unfamiliar to his readers.
Christian responses.
The Catholic Church has denounced antisemitic views held by Christians in the past with a series of statements beginning in 1937 (cf. Mit brennender Sorge of Pope Pius XI). In the decree "Nostra aetate", Pope Paul VI in Council declared that:
Norman Beck, professor of theology and classical languages at Texas Lutheran University, has proposed that Christian lectionaries remove what he calls "… the specific texts identified as most problematic …". Beck identifies what he deems to be offensive passages in the New Testament and indicates the instances in which these texts or portions thereof are included in major lectionary series.
Daniel Goldhagen, former Associate Professor of Political Science at Harvard University, also suggested in his book "A Moral Reckoning" that the Roman Catholic Church should change its doctrine and the accepted Biblical canon to excise statements he labels as antisemitic, to indicate that "The Jews' way to God is as legitimate as the Christian way".

</doc>
<doc id="32797" url="http://en.wikipedia.org/wiki?curid=32797" title="Villains and Vigilantes">
Villains and Vigilantes

Villains and Vigilantes (abbreviated as V&V) is a superhero-themed role-playing game which competed primarily with "Champions" and "Superworld" in the early to mid-1980s.
Origin.
"Villains and Vigilantes" was the first role-playing game designed by Jack Herman and Jeff Dee and featuring illustrations by Dee. Fantasy Games Unlimited published the first edition of "Villains and Vigilantes" in 1979. The second edition of "Villains and Vigilantes" was published in 1982 with significant rule revisions. In 2010, Monkey House Games published a new edition of the game although the lawsuit filed in U.S. Federal court, Arizona District, (Case no. 2:2011-cv-02036) asserts that Monkey House Games had no legal right to do so.
Mechanics.
Characters in "Villains and Vigilantes" reflected the unique nature of the rules. First, instead of playing a completely fictional character, players were encouraged to start the character-creation process with a version of themselves (presumably as the superhero's "secret identity.") V&V then used random die rolls for the origins of superpowers (i.e., mutant, space alien, etc.) number and type, sometimes resulting in odd combinations. A further quirk of the system was that while players advanced in levels and hit points, superpowers did not, lending a different feel to characters at low, middle and high power levels.
Another notable feature of the system was its approach to combat: a table outlined the effectiveness of the attacker's superpower (for example, an energy blast) against all of the defender's powers, reflecting in theory the interplay of attack and defense powers.
Publication history.
The first edition was created by Jeff Dee and Jack Herman and published by Fantasy Games Unlimited in 1979. This book was followed by an adventure published in 1981, "Break In at Three Kilometer Island", and a pair of adventures in 1982 designed specifically to be playable with the original ruleset, but to begin to introduce players to the revised second edition which was published later the same year. These two adventures are particularly notable for being authored by Bill Willingham, most recently famous as the creator of the Fables comic book series. Willingham's adventures also used characters that would later appear in his Comico comic book series, "Elementals".
The second edition of Villains & Vigilantes, created by the same authorship team and also published by Fantasy Games Unlimited, was released in 1982 in two formats, both as a single rulebook and in a box set format that contained the revised rulebook, GM's screen, dice and character sheets. These were followed by a line of adventure module and rule supplements published through the 80's until the final supplement published in 1987, "For the Greater Good".
In 2004 Fantasy Games Unlimited began republishing the original V&V rules and supplements as a series of electronic supplements via the DriveThruRPG online store, continuing to add supplements until nearly the entire back catalog is now available.
In 2010, the original creators of the game released a new revised version, version 2.1, published through Monkey House Games. This has caused some ongoing legal disputes over the years that have followed. That same year, Fantasy Games Unlimited began releasing new supplements for 2nd Edition Villains and Vigilantes, for the first time since 1987, while Monkey House Games continued to release supplements of its own. Both companies continue to sell their products, pending resolution of the rights dispute.
Adventures.
"Crisis at Crusader Citadel" was an introductory adventure, a V&V supplement published in 1982 by Fantasy Games Unlimited, written and illustrated by Dee and Herman. The scenario begins with the players controlling neophyte superheroes, based on themselves, who are looking to apply for membership in the established super-hero team called the Crusaders. During the adventure, the player-heroes have to stop a crime wave being carried out by the Crusaders' opposite numbers, a villain team called the Crushers.
Four years after the adventure booklet was published, the setting of the first adventure was used as the basis of a "Villains and Vigilantes" comic book mini-series by Dee and Herman published by Eclipse Comics. Each issue included character sheets for new heroes and villains and updated material for the existing ones for use with the game.
Two early adventures for the game by Bill Willingham, "Death Duel with the Destroyers" and "The Island of Dr. Apocalypse", used characters that would later appear in his Comico comic book series, "Elementals". Similarly, a "Villains and Vigilantes" character, The Dark, later appeared in a series of comic books by the independent publisher Continüm Comics.
2011 legal dispute.
As of 2011, the game's creators are involved in a legal dispute with Fantasy Games Unlimited. They claim that Fantasy Games Unlimited, Inc. ceased to exist in 1991, and that their contract specified that in such an event the contracted publication rights would revert to them, and that therefore the current Fantasy Games Unlimited (which they claim is a separate legal entity, although with the same owner) has no right to sell V&V material. A court case filed in U.S. Federal court, Arizona district (REFERENCE: case no. 2:2011-cv-02036) supports the claim by Fantasy Games Unlimited that the owner properly followed procedure to continue with the obligations of the New York business entity in continued operations as the Arizona entity. The case also asserts further that at the time of its dissolution in New York, FGU was current on all obligations with that state and in good standing which then devolved into the entity currently operating simply as Fantasy Games Unlimited.
A judgment given on July 11, 2012, on the first two counts of case no. 2:2011-cv-02036 in U.S. Federal court, Arizona district ruled in favor siding with Scott Bizar resulting in Jeff Dee and Jack Herman being found guilty of defamation and unfair business practice causing unspecified damages to the plaintiff. More judgments on other counts were still pending as of that date. The judgment ordered (not withstanding any other punitive measures to be determined for damages on all counts) the defendants within 30 days to post conspicuously in every place on the internet a retraction/corrective measure where their false statements have been posted.
In January 2013, the U.S. District Court of Arizona found that Jeff Dee and Jack Herman own all copyrights to Villains and Vigilantes, including the previously contracted to Fantasy Games Unlimited. Additionally the court found that Fantasy Games Unlimited had been using Dee's and Herman's (the game creators') copyrighted material without permission by selling merchandise like T-shirts, comic books, and video games. Finally, court found that Fantasy Games Unlimited had legally abandoned its trademark rights to the title "Villains and Vigilantes" due to disuse.
On February 19, Scott Bizar filed an appeal with the 9th Circuit Court of Appeals.

</doc>
<doc id="32798" url="http://en.wikipedia.org/wiki?curid=32798" title="Victorian era">
Victorian era

 
The Victorian era of British history (and that of the British Empire) was the period of Queen Victoria's reign from 20 June 1837 until her death, on 22 January 1901. It was a long period of peace, prosperity, refined sensibilities and national self-confidence for Britain. Some scholars date the beginning of the period in terms of sensibilities and political concerns to the passage of the Reform Act 1832.
Within the fields of social history and literature, Victorianism refers to the study of late-Victorian attitudes and culture with a focus on the highly moralistic, straitlaced language and behaviour of Victorian morality. The era followed the Georgian period and preceded the Edwardian period. The later half of the Victorian age roughly coincided with the first portion of the "Belle Époque" era of continental Europe and the "Gilded Age" of the United States.
Culturally there was a transition away from the rationalism of the Georgian period and toward romanticism and mysticism with regard to religion, social values, and arts. In international relations the era was a long period of peace, known as the "Pax Britannica", and economic, colonial, and industrial consolidation, temporarily disrupted by the Crimean War in 1854. The end of the period saw the Boer War. Domestically, the agenda was increasingly liberal with a number of shifts in the direction of gradual political reform, industrial reform and the widening of the voting franchise.
Two especially important figures in this period of British history are the prime ministers Gladstone and Disraeli, whose contrasting views changed the course of history. Disraeli, favoured by the queen, was a gregarious Tory. His rival Gladstone, a Liberal distrusted by the Queen, served more terms and oversaw much of the overall legislative development of the era.
The population of England and Wales almost doubled from 16.8 million in 1851 to 30.5 million in 1901. Scotland's population also rose rapidly, from 2.8 million in 1851 to 4.4 million in 1901. Ireland's population however decreased sharply, from 8.2 million in 1841 to less than 4.5 million in 1901, mostly due to the Great Famine. At the same time, around 15 million emigrants left the United Kingdom in the Victorian era, settling mostly in the United States, Canada, New Zealand and Australia.
During the early part of the era, the House of Commons was headed by the two parties, the Whigs and the Conservatives. From the late 1850s onwards, the Whigs became the Liberals. These parties were led by many prominent statesmen including Lord Melbourne, Sir Robert Peel, Lord Derby, Lord Palmerston, William Ewart Gladstone, Benjamin Disraeli, and Lord Salisbury. The unsolved problems relating to Irish Home Rule played a great part in politics in the later Victorian era, particularly in view of Gladstone's determination to achieve a political settlement. Southern Ireland achieved independence in 1922.
Population in the Victorian era.
The Victorian era was a time of unprecedented demographic increase in Britain. The population rose from 13.9 million in 1831 to 32.5 million in 1901. Two major factors affecting population growth are fertility rates and mortality rates. Britain was the first country to undergo the Demographic transition and the Agricultural and Industrial Revolutions.
Britain had the lead in rapid economic and population growth. At the time, Thomas Malthus believed this lack of growth outside Britain was due to the 'Malthusian trap'. That is, the tendency of a population to expand geometrically while resources grew more slowly, reaching a crisis (such as famine, war, or epidemic) which would reduce the population to a sustainable size. Britain escaped the 'Malthusian trap' because the Industrial Revolution had a positive impact on living standards. People had more money and could improve their standards; therefore, a population increase was sustainable.
Fertility rates.
In the Victorian era, fertility rates increased in every decade until 1901, when the rates started evening out. There are several reasons for the increase in birth rates. One is biological: with improving living standards, the percentage of women who were able to have children increased. Another possible explanation is social. In the 19th century, the marriage rate increased, and people were getting married at a very young age until the end of the century, when the average age of marriage started to increase again slowly. The reasons why people got married younger and more frequently are uncertain. One theory is that greater prosperity allowed people to finance marriage and new households earlier than previously possible. With more births within marriage, it seems inevitable that marriage rates and birth rates would rise together.
Birth rates were originally measured by the 'Crude birth rate' – births per year in population per every thousand people. This is thought not to be accurate enough, as key groups and their fertility rates are not clear. It also does not take into account population changes, e.g., same number of births in a smaller population (if men go to war, etc.). It was then changed to the 'Net Reproduction Rate,' which only measured the fertility rate of women who were capable of giving birth.
The evening out of fertility rates at the beginning of the 20th century was mainly the result of a few big changes: availability of forms of birth control, and changes in people's attitude towards sex.
Mortality rates.
The mortality rates in England changed greatly through the 19th century. There was no catastrophic epidemic or famine in England or Scotland in the 19th century – it was the first century in which a major epidemic did not occur throughout the whole country, with deaths per 1000 of population per year in England and Wales dropping from 21.9 from 1848–54 to 17 in 1901 (contrasting with, for instance, 5.4 in 1971). Class had a significant effect on mortality rates as the upper classes had a lower rate of premature death early in the 19th century than poorer classes did.
Environmental and health standards rose throughout the Victorian era; improvements in nutrition may also have played a role, although the importance of this is debated. Sewage works were improved as was the quality of drinking water. With a healthier environment, diseases were caught less easily and did not spread as much. Technology was also improving because the population had more money to spend on medical technology (for example, techniques to prevent death in childbirth so more women and children survived), which also led to a greater number of cures for diseases. However, a cholera epidemic took place in London in 1848–49 killing 14,137, and subsequently in 1853 killing 10,738. This anomaly was attributed to the closure and replacement of cesspits by the modern London sewerage system.
Culture.
Gothic Revival architecture became increasingly significant during the period, leading to the Battle of the Styles between Gothic and Classical ideals. Charles Barry's architecture for the new Palace of Westminster, which had been badly damaged in an 1834 fire, was built in the medieval style of Westminster Hall, the surviving part of the building. It constructed a narrative of cultural continuity, set in opposition to the violent disjunctions of Revolutionary France, a comparison common to the period, as expressed in Thomas Carlyle's "" and Charles Dickens' "Great Expectations" and "A Tale of Two Cities". Gothic was also supported by critic John Ruskin, who argued that it epitomised communal and inclusive social values, as opposed to Classicism, which he considered to epitomise mechanical standardisation.
The middle of the 19th century saw The Great Exhibition of 1851, the first World's Fair, which showcased the greatest innovations of the century. At its centre was the Crystal Palace, a modular glass and iron structure – the first of its kind. It was condemned by Ruskin as the very model of mechanical dehumanisation in design, but later came to be presented as the prototype of Modern architecture. The emergence of photography, showcased at the Great Exhibition, resulted in significant changes in Victorian art with Queen Victoria being the first British monarch to be photographed. John Everett Millais was influenced by photography (notably in his portrait of Ruskin) as were other Pre-Raphaelite artists. It later became associated with the Impressionistic and Social Realist techniques that would dominate the later years of the period in the work of artists such as Walter Sickert and Frank Holl.
Industrialization brought with it a burgeoning middle class whose increase in numbers had a significant effect on the social strata itself: cultural norms, lifestyle, values and morality. Identifiable characteristics came to define, in particular, the middle class home. Previously, in town and city, residential space was adjacent to or incorporated into the work site, virtually occupying the same geographical space. The difference between private life and commerce was a fluid one distinguished by an informal demarcation of function. In the Victorian era, English family life increasingly became compartmentalised, the home a self-contained structure housing a nuclear family extended according to need and circumstance to include blood relations. The concept of "privacy" became a hallmark of the middle class life. "... The English home closed up and darkened over the decade (1850s), the cult of domesticity matched by a cult of privacy." Bourgeois existence was a world of interior space, heavily curtained off and wary of intrusion, and opened only by invitation for viewing on occasions such as parties or teas. "The essential, unknowability of each individual, and society's collaboration in the maintenance of a façade behind which lurked innumerable mysteries, were the themes which preoccupied many mid-century novelists." 
Entertainment.
Popular forms of entertainment varied by social class. Victorian Britain, like the periods before it, was interested in literature (see Charles Dickens, Arthur Conan Doyle, Charlotte Brontë and her sisters and William Makepeace Thackeray), theatre and the arts (see Aesthetic movement and Pre-Raphaelite Brotherhood), and music, drama, and opera were widely attended. Michael Balfe was the most popular British grand opera composer of the period, while the most popular musical theatre was a series of fourteen comic operas by Gilbert and Sullivan, although there was also musical burlesque and the beginning of Edwardian musical comedy in the 1890s. Drama ranged from low comedy to Shakespeare (see Henry Irving). There were, however, other forms of entertainment. Gentlemen went to dining clubs, like the Beefsteak club or the Savage club. Gambling at cards in establishments popularly called casinos was wildly popular during the period: so much so that evangelical and reform movements specifically targeted such establishments in their efforts to stop gambling, drinking, and prostitution.
Brass bands and 'The Bandstand' became popular in the Victorian era. The band stand was a simple construction that not only created an ornamental focal point, but also served acoustic requirements whilst providing shelter from the changeable British weather. It was common to hear the sound of a brass band whilst strolling through parklands. At this time musical recording was still very much a novelty.
The Victorian era marked the golden age of the British circus.Astley's Amphitheatre in Lambeth, London, featuring equestrian acts in a 42-foot wide circus ring, was the epicentre of the 19th century circus. The permanent structure sustained three fires but as an institution lasted a full century, with Andrew Ducrow and William Batty managing the theatre in the middle part of the century. William Batty would also build his own 14,000-person arena, known commonly as Batty's Hippodrome, in Kensington Gardens and draw crowds from the Crystal Palace Exhibition. Travelling circuses, like Pablo Fanque's, dominated the British provinces, Scotland, and Ireland (Fanque would enjoy fame again in the 20th century when John Lennon would buy an 1843 poster advertising his circus and adapt the lyrics for The Beatles song, "Being for the Benefit of Mr. Kite!"). Fanque also stands out as a black man who achieved great success and enjoyed great admiration among the British public only a few decades after Britain had abolished slavery.
Another form of entertainment involved 'spectacles' where paranormal events, such as mesmerism, communication with the dead (by way of mediumship or channelling), ghost conjuring and the like, were carried out to the delight of crowds and participants. Such activities were more popular at this time than in other periods of recent Western history.
Natural history became increasingly an "amateur" activity. Particularly in Britain and the United States, this grew into specialist hobbies such as the study of birds, butterflies, seashells (malacology/conchology), beetles and wild flowers. Amateur collectors and natural history entrepreneurs played an important role in building the large natural history collections of the nineteenth and early twentieth centuries.
Middle-class Victorians used the train services to visit the seaside, helped by the Bank Holiday Act of 1871, which created a number of fixed holidays. Large numbers travelling to quiet fishing villages such as Worthing, Brighton, Morecambe and Scarborough began turning them into major tourist centres, and people like Thomas Cook saw tourism and even overseas travel as viable businesses.
Technology and engineering.
The Victorians were impressed by science and progress, and felt that they could improve society in the same way as they were improving technology. Britain was the leading world center for advanced engineering and technology. Its engineering firms were in world wide demand for designing and constructing railways.
A central development during the Victorian era was the improvement of communication. The new railways all allowed goods, raw materials and people to be moved about, rapidly facilitating trade and industry. The financing of railways became an important specialty of London's financiers. Trains became important factor ordering society, with "railway time" being the standard by which clocks were set throughout Britain, and with the complex railway system setting the standard for technological advances and efficiency.. Steam ships such as the SS Great Britain and SS Great Western made international travel more common but also advanced trade, so that in Britain it was not just the luxury goods of earlier times that were imported into the country but essentials and raw materials such as corn and cotton from the United States and meat and wool from Australia. One more important innovation in communications was the Penny Black, the first postage stamp, which standardised postage to a flat price regardless of distance sent.
Even later communication methods such as electric power, telegraph, and telephones, had an impact. Photography was realised in 1839 by Louis Daguerre in France and William Fox Talbot in Britain. By 1889, hand-held cameras were available.
Similar sanitation reforms, prompted by the Public Health Acts 1848 and 1869, were made in the crowded, dirty streets of the existing cities, and soap was the main product shown in the relatively new phenomenon of advertising. A great engineering feat in the Victorian Era was the sewage system in London. It was designed by Joseph Bazalgette in 1858. He proposed to build 82 mi of sewer system linked with over 1000 mi of street sewers. Many problems were encountered but the sewers were completed. After this, Bazalgette designed the Thames Embankment which housed sewers, water pipes and the London Underground. During the same period London's water supply network was expanded and improved, and a gas network for lighting and heating was introduced in the 1880s.
The model town of Saltaire was founded, along with others, as a planned environment with good sanitation and many civic, educational and recreational facilities, although it lacked a pub, which was regarded as a focus of dissent. During the Victorian era, science grew into the discipline it is today. In addition to the increasing professionalism of university science, many Victorian gentlemen devoted their time to the study of natural history. This study of natural history was most powerfully advanced by Charles Darwin and his theory of evolution first published in his book "On the Origin of Species" in 1859.
Although initially developed in the early years of the 19th century, gas lighting became widespread during the Victorian era in industry, homes, public buildings and the streets. The invention of the incandescent gas mantle in the 1890s greatly improved light output and ensured its survival as late as the 1960s. Hundreds of gasworks were constructed in cities and towns across the country. In 1882, incandescent electric lights were introduced to London streets, although it took many years before they were installed everywhere.
Sport.
The Victorian Era saw the introduction and development of many modern sports. Cricket, cycling, croquet, horse-riding, and water activities are examples of some of the popular sports in the Victorian Era. The modern game of tennis originated in Birmingham, England, between 1859 and 1865. The world's oldest tennis tournament, the Wimbledon championships, were first played in London in 1877. The first Olympic Games held under the auspices of the IOC were hosted in Athens in 1896. The Games brought together 12 nations and 241 athletes who competed in 43 events.
Football.
The first football league in the world was established in 1888 by Aston Villa director William McGregor. Aston Villa were the most successful English club of the Victorian era, winning five League titles and three FA Cups by the end of Queen Victoria's reign. Other prominent clubs of the era were Blackburn Rovers, Sunderland and Preston North End. The end of the 19th century saw Britain being swept by football mania, attracting huge crowds of largely working class men.
Health and medicine.
Medicine progressed during Queen Victoria's reign. Although nitrous oxide, or laughing gas, had been proposed as an anaesthetic as far back as 1799 by Humphry Davy, it wasn't until 1846 when an American dentist named William Morton started using ether on his patients that anaesthetics became common in the medical profession. In 1847 chloroform was introduced as an anaesthetic by James Young Simpson. Chloroform was favoured by doctors and hospital staff because it is much less flammable than ether, but critics complained that it could cause the patient to have a heart attack. Chloroform gained in popularity in England and Germany after Dr. John Snow gave Queen Victoria chloroform for the birth of her eighth child (Prince Leopold). By 1920, chloroform was used in 80 to 95% of all narcoses performed in the UK and German-speaking countries.
Anaesthetics made painless dentistry possible. At the same time the European diet grew a great deal sweeter as the use of sugar became more widespread. As a result, more and more people were having teeth pulled and needed replacements. This gave rise to "Waterloo Teeth", which were real human teeth set into hand-carved pieces of ivory from hippopotamus or walrus jaws. The teeth were obtained from executed criminals, victims of battlefields, from grave-robbers, and were even bought directly from the desperately impoverished.
Medicine also benefited from the introduction of antiseptics by Joseph Lister in 1867 in the form of carbolic acid (phenol). He instructed the hospital staff to wear gloves and wash their hands, instruments, and dressings with a phenol solution and, in 1869, he invented a machine that would spray carbolic acid in the operating theatre during surgery.
Poverty.
19th century Britain saw a huge population increase accompanied by rapid urbanisation stimulated by the Industrial Revolution. The large numbers of skilled and unskilled people looking for work kept wages down to a barely subsistence level. Available housing was scarce and expensive, resulting in overcrowding. These problems were magnified in London, where the population grew at record rates. Large houses were turned into flats and tenements, and as landlords failed to maintain these dwellings, slum housing developed. Kellow Chesney described the situation as follows: "Hideous slums, some of them acres wide, some no more than crannies of obscure misery, make up a substantial part of the metropolis... In big, once handsome houses, thirty or more people of all ages may inhabit a single room." Significant changes happened in the British Poor Law system in England and Wales, Scotland, and Ireland. These included significant expansions in workhouses (or poorhouses in Scotland), although with changing populations during the era.
The British Library evanion catalogue has an advertisement for Allen & Sons Cocoa Chocolate and Confectionery Works, c. 1880. The advertisement shows one family in poverty and another which is middle class. This drastic image was normal during the Victorian age when there was a very drastic divide between the classes.
Child labour.
The Victorian era became notorious for the employment of young children in factories and mines and as chimney sweeps. Child labour, often brought about by economic hardship, played an important role in the Industrial Revolution from its outset: Charles Dickens, for example, worked at the age of 12 in a blacking factory, with his family in a debtors' prison. In 1840 only about 20 percent of the children in London had any schooling. By 1860 about half of the children between 5 and 15 were in school (including Sunday school).
The children of the poor were expected to help towards the family budget, often working long hours in dangerous jobs for low wages. Agile boys were employed by the chimney sweeps; small children were employed to scramble under machinery to retrieve cotton bobbins; and children were also employed to work in coal mines, crawling through tunnels too narrow and low for adults. Children also worked as errand boys, crossing sweepers, shoe blacks, or sold matches, flowers, and other cheap goods. Some children undertook work as apprentices to respectable trades, such as building, or as domestic servants (there were over 120,000 domestic servants in London in the mid 18th century). Working hours were long: builders might work 64 hours a week in summer and 52 in winter, while domestic servants worked 80 hour weeks. Many young people worked as prostitutes (the majority of prostitutes in London were between 15 and 22 years of age).
"Mother bides at home, she is troubled with bad breath, and is sair weak in her body from early labour. I am wrought with sister and brother, it is very sore work; cannot say how many rakes or journeys I make from pit's bottom to wall face and back, thinks about 30 or 25 on the average; the distance varies from 100 to 250 fathom. I carry about 1 cwt. and a quarter on my back; have to stoop much and creep through water, which is frequently up to the calves of my legs." (Isabella Read, 12 years old, coal-bearer, testimony gathered by Ashley's Mines Commission 1842)
"My father has been dead about a year; my mother is living and has ten children, five lads and five lasses; the oldest is about thirty, the youngest is four; three lasses go to mill; all the lads are colliers, two getters and three hurriers; one lives at home and does nothing; mother does nought but look after home.
All my sisters have been hurriers, but three went to the mill. Alice went because her legs swelled from hurrying in cold water when she was hot. I never went to day-school; I go to Sunday-school, but I cannot read or write; I go to pit at five o'clock in the morning and come out at five in the evening; I get my breakfast of porridge and milk first; I take my dinner with me, a cake, and eat it as I go; I do not stop or rest any time for the purpose; I get nothing else until I get home, and then have potatoes and meat, not every day meat. I hurry in the clothes I have now got on, trousers and ragged jacket; the bald place upon my head is made by thrusting the corves; my legs have never swelled, but sisters' did when they went to mill; I hurry the corves a mile and more under ground and back; they weigh 300 cwt.; I hurry 11 a-day; I wear a belt and chain at the workings, to get the corves out;" (Patience Kershaw, 17 years old, coal-bearer, testimony gathered by Ashley's Mines Commission 1842)
Children as young as 4 were put to work. In coal mines, children began work at the age of 5 and generally died before the age of 25. Many children (and adults) worked 16-hour days. As early as 1802 and 1819, Factory Acts were passed to limit the working hours of workhouse children in factories and cotton mills to 12 hours per day. These acts were largely ineffective and after radical agitation, by for example the "Short Time Committees" in 1831, a Royal Commission recommended in 1833 that children aged 11–18 should work a maximum of 12 hours per day, children aged 9–11 a maximum of eight hours, and children under the age of nine should no longer be permitted to work. This act, however, only applied to the textile industry, and further agitation led to another act in 1847 limiting both adults and children to 10-hour working days.
Prostitution.
Beginning in the late 1840s, major news organisations, clergymen, and single women became increasingly concerned about prostitution, which came to be known as "The Great Social Evil". Estimates of the number of prostitutes in London in the 1850s vary widely (in his landmark study, "Prostitution", William Acton reported that the police estimated there were 8,600 in London alone in 1857). When the United Kingdom Census 1851 publicly revealed a 4% demographic imbalance in favour of women (i.e., 4% more women than men), the problem of prostitution began to shift from a moral/religious cause to a socio-economic one. The 1851 census showed that the population of Great Britain was roughly 18 million; this meant that roughly 750,000 women would remain unmarried simply because there were not enough men. These women came to be referred to as "superfluous women" or "redundant women", and many essays were published discussing what, precisely, ought to be done with them.
While the Magdalene Asylums had been "reforming" prostitutes since the mid-18th century, the years between 1848 and 1870 saw a veritable explosion in the number of institutions working to "reclaim" these "fallen women" from the streets and retrain them for entry into respectable society — usually for work as domestic servants. The theme of prostitution and the "fallen woman" (any woman who has had sexual intercourse out of marriage) became a staple feature of mid-Victorian literature and politics. In the writings of Henry Mayhew, Charles Booth, Charles Dickens and others, prostitution began to be seen as a social problem.
When Parliament passed the first of the Contagious Diseases Acts in 1864 (which allowed the local constabulary to force any woman suspected of venereal disease to submit to its inspection), Josephine Butler's crusade to repeal the CD Acts yoked the anti-prostitution cause with the emergent feminist movement. Butler attacked the long-established double standard of sexual morality.
Prostitutes were often presented as victims in sentimental literature such as Thomas Hood's poem "The Bridge of Sighs", Elizabeth Gaskell's novel "Mary Barton", and Dickens' novel "Oliver Twist". The emphasis on the purity of women found in such works as Coventry Patmore's "The Angel in the House" led to the portrayal of the prostitute and fallen woman as soiled, corrupted, and in need of cleansing.
This emphasis on female purity was allied to the stress on the homemaking role of women, who helped to create a space free from the pollution and corruption of the city. In this respect, the prostitute came to have symbolic significance as the embodiment of the violation of that divide. The double standard remained in force. Divorce legislation introduced in 1857 allowed for a man to divorce his wife for adultery, but a woman could only divorce if adultery were accompanied by cruelty. The anonymity of the city led to a large increase in prostitution and unsanctioned sexual relationships. Dickens and other writers associated prostitution with the mechanisation and industrialisation of modern life, portraying prostitutes as human commodities consumed and thrown away like refuse when they were used up. Moral reform movements attempted to close down brothels, something that has sometimes been argued to have been a factor in the concentration of street-prostitution in Whitechapel (where the Jack the Ripper prostitute murders took place), part of the East End of London, by the 1880s (near the end of the 19th century).

</doc>
<doc id="32821" url="http://en.wikipedia.org/wiki?curid=32821" title="V-1 flying bomb">
V-1 flying bomb

The V-1 flying bomb (German: "Vergeltungswaffe 1",)—also known to the Allies as the buzz bomb, or doodlebug, and in Germany as Kirschkern (cherrystone) or Maikäfer (maybug)—was an early pulsejet-powered predecessor of the cruise missile.
The V-1 was developed at Peenemünde Army Research Center by the Nazi German "Luftwaffe" during the Second World War. During initial development it was known by the codename "Cherry Stone". The first of the so-called "Vergeltungswaffen" series designed for terror bombing of London, the V-1 was fired from launch facilities along the French (Pas-de-Calais) and Dutch coasts. The first V-1 was launched at London on 13 June 1944), one week after (and prompted by) the successful Allied landing in Europe. At its peak, more than one hundred V-1s a day were fired at south-east England, 9,521 in total, decreasing in number as sites were overrun until October 1944, when the last V-1 site in range of Britain was overrun by Allied forces. After this, the V-1s were directed at the port of Antwerp and other targets in Belgium, with 2,448 V-1s being launched. The attacks stopped when the last launch site was overrun on 29 March 1945.
The British operated an arrangement of air defences (including anti-aircraft guns and fighter aircraft) to intercept the bombs before they reached their targets as part of Operation Crossbow, while the launch sites and underground V-1 storage depots were targets of strategic bombing.
Design and development.
In late 1936, while employed by the "Argus Motoren" company, Fritz Gosslau began work on the further development of remote-controlled aircraft; Argus had already developed a remote-controlled surveillance aircraft, the AS 292 (military designation FZG 43).
On 9 November 1939, a proposal for a remote-controlled aircraft carrying a payload of 1000 kg over a distance of 500 km was forwarded to the RLM (German Air Ministry). Argus joined with Lorentz AG and Arado Flugzeugwerke to develop the project as a private venture, and in April 1940, Gosslau presented an improved study of Project ""Fernfeuer" to the RLM, as Project P 35 "Erfurt"".
On 31 May, Rudolf Bree of the RLM commented that he saw no chance that the projectile could be deployed in combat conditions, as the proposed remote control system was seen as a design weakness. Heinrich Koppenberg, the director of Argus, met with Ernst Udet on 6 January 1941 to try to convince him that the development should be continued, but Udet opted to cancel it.
Despite this, Gosslau was convinced that the basic idea was sound and proceeded to simplify the design. As an aircraft engine manufacturer, Argus lacked the capability to produce a fuselage for the project and Koppenberg sought the assistance of Robert Lusser, chief designer and technical director at Heinkel. On 22 January 1942, Lusser took up a position with the Fieseler aircraft company. He met with Koppenberg on 27 February and was informed of Gosslau's project. Gosslau's design used two pulse jet engines; Lusser improved the design to use a single engine.
A final proposal for the project was submitted to the Technical Office of the RLM on 5 June and the project was renamed Fi 103, as Fieseler was to be the chief contractor. On 19 June, "Generalfeldmarschall" Erhard Milch gave Fi 103 production high priority, and development was undertaken at the Luftwaffe's "Erprobungsstelle" coastal test centre at Karlshagen, part of the Peenemünde-West facility.
By 30 August, Fieseler had completed the first fuselage, and the first flight of the Fi 103 V7 took place on 10 December, when it was airdropped by a Fw 200.
The V-1 was named by "The Reich" journalist Hans Schwarz Van Berkl in June 1944 with Hitler's approval.
Description.
The V-1 was designed under the codename "Kirschkern" (cherry stone) by Lusser and Gosslau, with a fuselage constructed mainly of welded sheet steel and wings built of plywood. The simple, Argus-built pulsejet engine pulsed 50 times per second, and the characteristic buzzing sound gave rise to the colloquial names "buzz bomb" or "doodlebug" (a common name for a wide variety of insects). It was known briefly in Germany (on Hitler's orders) as "Maikäfer" (May bug) and "Krähe" (crow).
Power plant.
Ignition of the Argus pulse jet was accomplished using an automotive type spark plug located about 76 cm behind the intake shutters, with current supplied from a portable starting unit. Three air nozzles in the front of the pulse jet were at the same time connected to an external high pressure air source which was used to start the engine. Acetylene gas was typically used for starting, and very often a panel of wood or similar material was held across the end of the tailpipe to prevent the fuel from diffusing and escaping before ignition.The V-1 was fueled by 625 l of 75 octane gasoline.
Once the engine had been started and the temperature had risen to the minimum operating level, the external air hose and connectors were removed and the engine's resonant design kept it firing without any further need for the electrical ignition system, which was used only to ignite the engine when starting.
It is a common myth that the V-1's Argus As 014 pulsejet engine needed a minimum airspeed of 240 km/h to operate. The Argus As 014 (also known as a resonant jet) could in fact operate at zero airspeed due to the nature of its intake shutters and its acoustically tuned resonant combustion chamber. Contemporary film footage of the V-1 always shows the distinctive pulsating exhaust of a fully running engine before the catapult system is triggered and the missile launched. The origin of this myth may lie in the fact that due to the low static thrust of the pulse jet engine and the very high stall speed of the small wings, the V-1 could not take off under its own power in a practically short distance, and thus required to either be launched by aircraft catapult or be airlaunched from a modified bomber aircraft such as the Heinkel He-111. Ground-launched V-1s were typically propelled up an inclined launch ramp by an apparatus known as a "Dampferzeuger" ("steam generator") which used stabilized hydrogen peroxide and potassium permanganate ("T-Stoff" and "Z-Stoff"). Takeoff speed was 580 km/h.
Beginning in January 1941, the V-1's pulse jet engine was also tested on a variety of craft, including automobiles and an experimental attack boat known as the "Tornado". The unsuccessful prototype was a version of a "Sprengboot", in which a boat loaded with explosives was steered towards a target ship and the pilot would leap out of the back at the last moment. The Tornado was assembled from surplus seaplane hulls connected in catamaran fashion with a small pilot cabin on the cross beams. The Tornado prototype was a noisy underperformer and was abandoned in favour of more conventional piston engined craft.
The engine made its first flight aboard a Gotha Go 145 on 30 April 1941.
Guidance system.
The V-1 guidance system used a simple autopilot to regulate altitude and airspeed, developed by Askania in Berlin. (The RLM at first planned to use a radio control system with the V-1 for precision attacks, but the government decided instead to use the missile against London.) A weighted pendulum system provided fore-and-aft attitude measurement to control pitch (damped by a gyrocompass, which it also stabilized). Operating power for the gyroscope platform and the flight control actuators was provided by two large spherical compressed air tanks which also pressurized the fuel tank. These air tanks were charged to 150 atm before launch. With the counter determining how far the missile would fly, it was only necessary to launch the V-1 with the ramp pointing in the approximate direction, and the autopilot controlled the flight.
There was a more sophisticated interaction between yaw, roll, and other sensors: a gyrocompass (set by swinging in a hangar before launch) gave feedback to control the dynamics of pitch and roll, but it was angled away from the horizontal so that controlling these degrees of freedom interacted: the gyroscope remained true on the basis of feedback received from a magnetic compass, and from the fore and aft pendulum. This interaction meant that rudder control was sufficient for steering and no banking mechanism was needed. In a V-1 which landed in March 1945 without detonating between Tilburg and Goirle, Netherlands, several rolled issues of the German wartime propaganda magazine "Signal" were found inserted into the left wing's tubular steel spar, used for weight to preset the missile's static equilibrium before launching. It is also known that several of the first buzz bombs to be launched were provided with a small radio transmitter (using a triode valve marked 'S3' but being equivalent to a then-current power valve, type RL 2,4T1), to check the general direction of flight related to the launching place's and the target's grid coordinates by radio bearing (navigation).
An odometer driven by a vane anemometer on the nose determined when the target area had been reached, accurately enough for area bombing. Before launch, the counter was set to a value that would reach zero upon arrival at the target in the prevailing wind conditions. As the missile flew, the airflow turned the propeller, and every 30 rotations of the propeller counted down one number on the counter. This counter triggered the arming of the warhead after about 60 km. When the count reached zero, two detonating bolts were fired. Two spoilers on the elevator were released, the linkage between the elevator and servo was jammed and a guillotine device cut off the control hoses to the rudder servo, setting the rudder in neutral. These actions put the V-1 into a steep dive. While this was originally intended to be a power dive, in practice the dive caused the fuel flow to cease, which stopped the engine. The sudden silence after the buzzing alerted listeners of the impending impact. The fuel problem was quickly fixed, and when the last V-1s fell, the majority hit under power.
Operation and effectiveness.
The first complete V-1 airframe was delivered 30 August 1942, and after the first complete As.109-014 was delivered in September, the first glide test flight was 28 October 1942 at Peenemünde, from under a Focke-Wulf Fw 200. The first powered trial was 10 December, launched from beneath an He-111.
A myth arose that early guidance and stabilisation problems were resolved by a daring test flight by Hanna Reitsch in a V-1 modified for manned operation. The myth entered popular consciousness from Reitsch's fictional exploits in the film "Operation Crossbow".
The conventional launch sites could theoretically launch about 15 V-1s per day, but this rate was difficult to achieve on a consistent basis; the maximum rate achieved was 18. Overall, only about 25% of the V-1s hit their targets, the majority being lost because of a combination of defensive measures, mechanical unreliability, or guidance errors. With the capture or destruction of the launch facilities used to attack England, the V-1s were employed in attacks against strategic points in Belgium, primarily the port of Antwerp.
Launches against Britain were met by a variety of countermeasures, including barrage balloons, Hawker Tempests, and even Gloster Meteors. These measures were so successful, by August 1944, about 80% of V-1s were being destroyed. (The Meteors, though fast enough to catch the V-1s, suffered frequent cannon failures, and accounted for only 13.) In all, about 1,000 V-1s were destroyed by aircraft.
The intended operational altitude was originally set at 2750 m. However, repeated failures of a barometric fuel-pressure regulator led to it being changed in May 1944, halving the operational height, thereby bringing V-1s into range of the Bofors guns commonly used by Allied AA units.
The trial versions of the V-1 were air-launched. Most operational V-1s were launched from static sites on land, but from July 1944 to January 1945, the "Luftwaffe" launched approximately 1,176 from modified Heinkel He 111 H-22s of the "Luftwaffe"‍ '​s Kampfgeschwader 3 (3rd Bomber Wing, the so-called "Blitz Wing") flying over the North Sea. Apart from the obvious motive of permitting the bombardment campaign to continue after static ground sites on the French coast were lost, air-launching gave the "Luftwaffe" the opportunity to outflank the increasingly effective ground and air defences put up by the British against the missile. To minimise the associated risks (primarily radar detection), the aircrews developed a tactic called "lo-hi-lo": the He 111s would, upon leaving their airbases and crossing the coast, descend to an exceptionally low altitude. When the launch point was neared, the bombers would swiftly ascend, fire their V-1s, and then rapidly descend again to the previous 'wave-top' level for the return flight. Research after the war estimated a 40% failure rate of air-launched V-1s, and the He-111s used in this role were extremely vulnerable to night fighter attack, as the launch lit up the area around the aircraft for several seconds. The combat potential of air-launched V-1s dwindled as 1944 progressed at about the same rate as that of the ground-launched missiles, as the British gradually took the measure of the weapon and developed increasingly effective defence tactics. For example, during Operation "Martha", KG 3's He 111H-22s fired 45 V-1s at Britain in a single concerted strike on Christmas Eve 1944, with just one missile getting through to hit a target.
Experimental and long-range variants.
Late in the war, several air-launched piloted V-1s, known as "Reichenbergs", were built, but never used in combat. Hanna Reitsch made some flights in the modified V-1 Fieseler "Reichenberg" when she was asked to find out why test pilots were unable to land it and had died as a result. She discovered, after simulated landing attempts at high altitude where there was air space to recover, that the craft had an extremely high stall speed and the previous pilots with little high speed experience had attempted their approaches much too slowly. Her recommendation of much higher landing speeds was then introduced in training new "Reichenberg" volunteer pilots. The "Reichenberg"s were air-launched rather than fired from a catapult ramp as erroneously portrayed in "Operation Crossbow".
There were plans, not put into practice, to use the Arado Ar 234 jet bomber to launch V-1s either by towing them aloft or by launching them from a "piggy back" position (in the manner of the "Mistel", but in reverse) atop the aircraft. In the latter configuration, a pilot-controlled, hydraulically operated dorsal trapeze mechanism would elevate the missile on the trapeze's launch cradle some eight feet clear of the 234's upper fuselage. This was necessary to avoid damaging the mother craft's fuselage and tail surfaces when the pulse jet ignited, as well as to ensure a 'clean' airflow for the Argus motor's intake. A somewhat less ambitious project undertaken was the adaptation of the missile as a 'flying fuel tank' "(Deichselschlepp)" for the Messerschmitt Me 262 jet fighter, which was initially test-towed behind an He 177A "Greif" bomber. The pulse-jet, internal systems and warhead of the missile were removed, leaving only the wings and basic fuselage, now containing a single large fuel tank. A small cylindrical module, similar in shape to a finless dart, was placed atop the vertical stabilizer at the rear of the tank, acting as a centre of gravity balance and attachment point for a variety of equipment sets. A rigid tow-bar with a pitch pivot at the forward end connected the flying tank to the Me 262. The operational procedure for this unusual configuration saw the tank resting on a wheeled trolley for take-off. The trolley was dropped once the combination was airborne, and explosive bolts separated the towbar from the fighter upon exhaustion of the tank's fuel supply. A number of test flights were conducted in 1944 with this set-up, but inflight "porpoising" of the tank, with the instability transferred to the fighter, meant the system was too unreliable to be used. An identical utilisation of the V-1 flying tank for the Ar 234 bomber was also investigated, with the same conclusions reached. Some of the "flying fuel tanks" used in trials utilised a cumbersome fixed and spatted undercarriage arrangement, which (along with being pointless) merely increased the drag and stability problems already inherent in the design.
One variant of the basic Fi 103 design did see operational use. The progressive loss of French launch sites as 1944 proceeded and the area of territory under German control shrinking meant that soon the V-1 would lack the range to hit targets in England. Air-launching was one alternative utilised, but the most obvious solution was to extend the missile's range. Thus the F-1 version developed. The weapon's fuel tank was increased in size, with a corresponding reduction in the capacity of the warhead. Additionally, the nose-cones and wings of the F-1 models were made of wood, affording a considerable weight saving. With these modifications, the V-1 could be fired at London and nearby urban centres from prospective ground sites in the Netherlands. Frantic efforts were made to construct a sufficient number of F-1s in order to allow a large-scale bombardment campaign to coincide with the Ardennes Offensive, but numerous factors (bombing of the factories producing the missiles, shortages of steel and rail transport, the chaotic tactical situation Germany was facing at this point in the war etc.) delayed the delivery of these long-range V-1s until February/March 1945. Beginning on March 2, 1945, slightly more than three weeks before the V-1 campaign ended for good, several hundred F-1s were launched at Britain from Dutch sites under Operation 'Zeppelin'.
There was also a jet-propelled variant proposed.
Almost 30,000 V-1s were made; by March 1944, they were produced in 350 hours (including 120 for the autopilot), at a cost of just 4% of a V-2, which delivered a comparable payload. Approximately 10,000 were fired at England; 2,419 reached London, killing about 6,184 people and injuring 17,981. The greatest density of hits were received by Croydon, on the south-east fringe of London. Antwerp, Belgium was hit by 2,448 V-1s from October 1944 to March 1945.
Intelligence reports.
The codename ""Flakzielgerät" 76"—"Flak aiming apparatus" helped to hide the nature of the device, and it was some time before references to FZG 76 were linked to the V-83 pilotless aircraft (an experimental V-1) that had crashed on Bornholm in the Baltic and to reports from agents of a flying bomb capable of being used against London. Importantly, the Polish Home Army intelligence contributed information on V-1 construction and a place of development (Peenemünde). Initially, British experts were sceptical of the V-1 because they had considered only solid fuel rockets, which could not attain the stated range of 1000 kg: 130 miles (209 km). However they later considered other types of engine, and by the time German scientists had achieved the needed accuracy to deploy the V-1 as a weapon, British intelligence had a very accurate assessment of it.
Countermeasures.
Anti-aircraft guns.
The British defence against the German long-range weapons was Operation Crossbow. Anti-aircraft guns were redeployed in several movements: first in mid-June 1944 from positions on the North Downs to the south coast of England, then a cordon closing the Thames Estuary to attacks from the east. In September 1944, a new linear defence line was formed on the coast of East Anglia, and finally in December there was a further layout along the Lincolnshire-Yorkshire coast. The deployments were prompted by changes to the approach tracks of the V-1 as launch sites were overrun by the Allies' advance.
On the first night of sustained bombardment, the anti-aircraft crews around Croydon were jubilant – suddenly they were downing unprecedented numbers of German bombers; most of their targets burst into flames and fell when their engines cut out. There was great disappointment when the truth was announced. Anti-aircraft gunners soon found that such small fast-moving targets were, in fact, very difficult to hit. The cruising altitude of the V-1, between 600 to, was just above the effective range of light anti-aircraft guns, and just below the optimum engagement height of heavier guns. The altitude and speed were more than the rate of traverse of the standard British QF 3.7-inch mobile gun could cope with, and faster-traversing static gun emplacements had to be built at great cost.
The development of the proximity fuze and of centimetric, 3 gigahertz frequency gun-laying radars based on the cavity magnetron helped to counter the V-1's high speed and small size. In 1944, Bell Labs started delivery of an anti-aircraft predictor fire-control system based on an analog computer, just in time for the Allied invasion of Europe.
These electronic aids arrived in quantity from June 1944, just as the guns reached their firing positions on the coast. Seventeen percent of all flying bombs entering the coastal 'gun belt' were destroyed by guns in their first week on the coast. This rose to 60% by 23 August and 74% in the last week of the month, when on one day 82% were shot down. The rate improved from one V-1 destroyed for every 2,500 shells fired initially, to one for every 100. This still did not end the threat. V-1 attacks continued until all launch sites were captured by ground forces.
Barrage balloons.
Eventually some 2,000 barrage balloons were deployed, in the hope that V-1s would be destroyed when they struck the balloons' tethering cables. The leading edges of the V-1's wings were fitted with cable cutters, and fewer than 300 V-1s are known to have been brought down by barrage balloons.
Interceptors.
The Defence Committee expressed some doubt as to the ability of the Royal Observer Corps to adequately deal with this new threat, but the ROC's Commandant Air Commodore Finlay Crerar assured the committee that the ROC could again rise to the occasion and prove its alertness and flexibility. He oversaw plans for handling the new threat, codenamed by the RAF and ROC as "Operation Totter".
Observers at the coast post of Dymchurch identified the very first of these weapons and within seconds of their report the anti-aircraft defences were in action. This new weapon gave the ROC much additional work both at posts and operations rooms. Eventually RAF controllers actually took their radio equipment to the two closest ROC operations rooms at Horsham and Maidstone, and vectored fighters direct from the ROC's plotting tables. The critics who had said that the Corps would be unable to handle the fast-flying jet aircraft were answered when these aircraft on their first operation were actually controlled entirely by using ROC information both on the coast and at inland.
The average speed of V-1s was 550 km/h and their average altitude was 1000 m to 1200 m. Fighter aircraft required excellent low altitude performance to intercept them and enough firepower to ensure that they were destroyed in the air rather than crashing to earth and detonating. Most aircraft were too slow to catch a V-1 unless they had a height advantage, allowing them to gain speed by diving on their target.
When V-1 attacks began in mid-June 1944, the only aircraft with the low-altitude speed to be effective against it was the Hawker Tempest. Fewer than 30 Tempests were available. They were assigned to No. 150 Wing RAF. Early attempts to intercept and destroy V-1s often failed, but improved techniques soon emerged. These included using the airflow over an interceptor's wing to raise one wing of the V-1, by sliding the wingtip to within 6 in of the lower surface of the V-1's wing. If properly executed, this manoeuvre would tip the V-1's wing up, overriding the gyros and sending the V-1 into an out-of-control dive. At least sixteen V-1s were destroyed this way (a P-51 piloted by Major R. E. Turner of 356th Fighter Squadron being the first on 18 June). It could be seen that the aerodynamic flip method was actually effective when V-1s could be seen over southern parts of the Netherlands headed due eastwards at low altitude, the engine quenched. In early 1945 such a missile soared below clouds over Tilburg to gently alight eastwards of the city in open fields.
The Tempest fleet was built up to over 100 aircraft by September. Specially modified P-47M Thunderbolts (half their fuel tanks, half their 0.5in {12.7 mm} machine guns, all external fittings, and all their armour plate removed) were also pressed into service against the V-1 menace. Also, North American P-51 Mustangs and Griffon-engined Supermarine Spitfire Mk XIVs were tuned to make them almost fast enough, and during the short summer nights the Tempests shared defensive duty with de Havilland Mosquitoes. There was no need for airborne radar; at night the V-1's engine could be heard from 10 mi away or more, and the exhaust plume was visible from a long distance. Wing Commander Roland Beamont had the 20 mm cannon on his Tempest adjusted to converge at 300 yd ahead. This was so successful that all other aircraft in 150 Wing were thus modified.
The anti-V-1 sorties by fighters were known as "Diver patrols" (after "Diver", the codename used by the Royal Observer Corps for V-1 sightings). Attacking a V-1 was dangerous: machine guns had little effect on the V-1's sheet steel structure, and if a cannon shell detonated the warhead, the explosion could destroy the attacker.
In daylight, V-1 chases were chaotic and often unsuccessful until a special defence zone was declared between London and the coast, in which only the fastest fighters were permitted. The first interception of a V-1 was by F/L JG Musgrave with a No. 605 Squadron RAF Mosquito night fighter on the night of 14/15 June 1944. Between June and 5 September 1944, a handful of 150 Wing Tempests shot down 638 flying bombs, with No. 3 Squadron RAF alone claiming 305. One Tempest pilot, Squadron Leader Joseph Berry (501 Squadron), shot down 59 V-1s, the Belgian ace Squadron Leader Remy Van Lierde (164 Squadron) destroyed 44 (with a further nine shared) and W/C Roland Beamont (see above) destroyed 31.
The next most successful interceptors were the Mosquito (623 victories), Spitfire XIV (303), and Mustang (232). All other types combined added 158. Even though it was not fully operational, the jet-powered Gloster Meteor was rushed into service with No. 616 Squadron RAF to fight the V-1s. It had ample speed but its cannon were prone to jamming, and it shot down only 13 V-1s.
In late 1944 a radar-equipped Vickers Wellington bomber was modified for use by the RAF's Fighter Interception Unit as an Airborne Early Warning and Control aircraft. Flying at an altitude of 4000 ft over the North Sea, it directed Mosquito fighters charged with intercepting He 111s from Dutch airbases that sought to launch V-1s from the air.
Disposal.
The first bomb disposal officer to defuse an unexploded V1 flying bomb was John Pilkington Hudson in 1944.
Deception.
To adjust and correct settings in the V-1 guidance system, the Germans needed to know where the V-1s were landing. Therefore, German intelligence was requested to obtain this impact data from their agents in Britain. However, all German agents in Britain had been turned, and were acting as double agents under British control.
On 16 June 1944, British double agent "Garbo" (Juan Pujol) was requested by his German controllers to give information on the sites and times of V-1 impacts, with similar requests made to the other German agents in Britain, "Brutus" (Roman Czerniawski) and "Tate" (Wulf Schmidt). If given this data, the Germans would be able to adjust their aim and correct any shortfall. However, there was no plausible reason why the double agents could not supply accurate data; the impacts would be common knowledge amongst Londoners and very likely reported in the press, which the Germans had ready access to through the neutral nations. In addition, as John Cecil Masterman, chairman of the Twenty Committee, commented, "If, for example, St Paul's Cathedral were hit, it was useless and harmful to report that the bomb had descended upon a cinema in Islington, since the truth would inevitably get through to Germany ..."
While the British decided how to react, Pujol played for time. On 18 June it was decided that the double agents would report the damage caused by V-1s fairly accurately and minimise the effect they had on civilian morale. It was also decided that Pujol should avoid giving the times of impacts, and should mostly report on those which occurred in the north west of London, to give the impression to the Germans that they were overshooting the target area.
While Pujol downplayed the extent of V-1 damage, trouble came from "Ostro", an "Abwehr" agent in Lisbon who pretended to have agents reporting from London. He told the Germans that London had been devastated and had been mostly evacuated due to enormous casualties. The Germans could not perform aerial reconnaissance of London, and believed his damage reports in preference to Pujol's. They thought that the Allies would make every effort to destroy the V-1 launch sites in France. They also accepted "Ostro"‍ '​s impact reports. Due to Ultra however, the Allies read his messages and adjusted for them.
A certain number of the V-1s fired had been fitted with radio transmitters, which had clearly demonstrated a tendency for the V-1 to fall short. "Oberst" Max Wachtel, commander of Flak Regiment 155(W), which was responsible for the V-1 offensive, compared the data gathered by the transmitters with the reports obtained through the double agents. He concluded, when faced with the discrepancy between the two sets of data, that there must be a fault with the radio transmitters, as he had been assured that the agents were completely reliable. It was later calculated that if Wachtel had disregarded the agents' reports and relied on the radio data, he would have made the correct adjustments to the V-1's guidance, and casualties might have increased by 50% or more.
The policy of diverting V-1 impacts away from central London was initially controversial. The War Cabinet refused to authorise a measure which would increase casualties in any area, even if it reduced casualties elsewhere by greater amounts. It was thought that Churchill would reverse this decision later (he was then away at a conference); but the delay in starting the reports to Germans might be fatal to the deception. So Sir Findlater Stewart of Home Defence Executive took responsibility for starting the deception programme immediately. His action was approved by Churchill when he returned.
End of the V-1 attacks.
By September 1944, the V-1 threat to England was temporarily halted when the launch sites on the French coast were overrun by the advancing Allied armies. 4,261 V-1s had been destroyed by fighters, anti-aircraft fire and barrage balloons. The last enemy action of any kind on British soil occurred on 29 March 1945, when a V-1 struck Datchworth in Hertfordshire.
Assessment.
Unlike the V-2, the V-1 was a cost-effective weapon for the Germans as it forced the Allies to spend heavily on defensive measures and divert bombers from other targets. More than 25% of Combined Bomber Offensive's bombs in July and August 1944 were used against V-weapon sites, often ineffectively. In early December 1944, American General Clayton Bissell wrote a paper which argued strongly in favour of the V-1 compared to conventional bombers.
The following is a table he produced.
The statistic of this report, however, have been the subject of some dispute. The V-1 missiles were often prone to exploding prematurely, occasionally resulting in the loss of the aircraft from which they were dropped. The Luftwaffe lost 77 aircraft as a result of the launch of these sorties.
Japanese developments.
In 1943, an Argus pulse jet engine was shipped to Japan by German submarine. The Aeronautical Institute of Tokyo Imperial University and the Kawanishi Aircraft Company conducted a joint study of the feasibility of mounting a similar engine on a piloted plane. The resulting design was named "Baika" ("plum blossom") but bore no more than a superficial resemblance to the Fi 103. "Baika" never left the design stage but technical drawings and notes suggest that several versions were considered: an air-launched version with the engine under the fuselage, a ground-launched version that could take off without a ramp and a submarine launched version with the engine moved forwards.
Post-war.
After the war, the armed forces of France, the Soviet Union and the United States experimented with the V-1.
France.
The French produced copies of the V-1 for use as target drones. These were called the CT-10 and were smaller than the V-1 with twin tail surfaces. The CT 10 could be ground launched using a rocket booster or from an aircraft. Some CT-10s were sold to the UK and the US.
Soviet Union.
The Soviet Union captured V-1s when they overran the Blizna test range in Poland, as well as from the Mittelwerk. The 10Kh was their copy of the V-1, later called Izdeliye 10. Initial tests began in March 1945 at a test range in Tashkent, with further launches from ground sites and from aircraft of improved versions continuing into the late 1940s. The inaccuracy of the guidance system compared to new methods such as beam-riding and TV guidance saw development end in the early 1950s.
The Soviets also worked on a piloted attack aircraft based on the Argus pulse jet engine of the V-1 which began as a German project, the Junkers EF 126 "Lilli", in the latter stages of the war. The Soviet development of the "Lilli" ended in 1946 after a crash that killed the test pilot.
United States.
The United States reverse-engineered the V-1 in 1944 from salvaged parts recovered in England during June. By 8 September, the first of thirteen complete prototype Republic-Ford JB-2 Loons, were assembled at Republic Aviation. The United States JB-2 was different from the German V-1 in only the smallest of dimensions. The wing span was only 2.5 in wider and the length was extended less than 2 ft. The difference gave the JB-2 60.7 sqft of wing area versus 55 sqft for the V-1.
A navalized version, designated KGW-1, was developed to be launched from LSTs (Landing Ship, Tank) as well as escort carriers (CVEs) and long-range 4-engine reconnaissance aircraft. Waterproof carriers for the KGW-1 were developed for launches of the missile from surfaced submarines. Both the USAAF JB-2 and Navy KGW-1 were put into production and were planned to be used in the Allied invasion of Japan (Operation Downfall). However, the atomic bombings of Japan obviated the need for its use. After World War II, the JB-2/KGW-1 played a significant role in the development of more advanced surface-to-surface tactical missile systems such as the MGM-1 Matador and later MGM-13 Mace.
References.
Notes
Citations
Bibliography
</dl>

</doc>
<doc id="32921" url="http://en.wikipedia.org/wiki?curid=32921" title="Watermark">
Watermark

A watermark is an identifying image or pattern in paper that appears as various shades of lightness/darkness when viewed by transmitted light (or when viewed by reflected light, atop a dark background), caused by thickness or density variations in the paper.
Watermarks have been used on postage stamps, currency, and other government documents to discourage counterfeiting. There are two main ways of producing watermarks in paper; the "dandy roll process", and the more complex "cylinder mould process".
Watermarks vary greatly in their visibility; while some are obvious on casual inspection, others require some study to pick out. Various aids have been developed, such as "watermark fluid" that wets the paper without damaging it. Watermarks are often used as security features of banknotes, passports, postage stamps, and other documents to prevent counterfeiting (see security paper).
A watermark is very useful in the examination of paper because it can be used for dating, identifying sizes, mill trademarks and locations, and determining the quality of a sheet of paper.
Encoding an identifying code into digitized music, video, picture, or other file is known as a digital watermark.
Dandy roll process.
A watermark is made by impressing a water-coated metal stamp or "dandy roll" onto the paper during manufacturing. While watermarks were first introduced in Fabriano, Italy, in 1282, the invention of the dandy roll in 1826 by John Marshall revolutionised the watermark process and made it easier for producers to watermark their paper.
The "dandy roll" is a light roller covered by material similar to window screen that is embossed with a pattern. Faint lines are made by "laid wires" that run parallel to the axis of the dandy roll, and the bold lines are made by "chain wires" that run around the circumference to secure the laid wires to the roll from the outside. Because the chain wires are located on the outside of the laid wires, they have a greater influence on the impression in the pulp, hence their bolder appearance than the laid wire lines.
This embossing is transferred to the pulp fibres, compressing and reducing their thickness in that area. Because the patterned portion of the page is thinner, it transmits more light through and therefore has a lighter appearance than the surrounding paper. If these lines are distinct and parallel, and/or there is a watermark, then the paper is termed "laid paper". If the lines appear as a mesh or are indiscernible, and/or there is no watermark, then it is called "wove paper". This method is called "line drawing watermarks."
Cylinder mould process.
Another type of watermark is called the "cylinder mould watermark". A shaded watermark, first used in 1848, incorporates tonal depth and creates a greyscale image. Instead of using a wire covering for the dandy roll, the shaded watermark is created by areas of relief on the roll's own surface. Once dry, the paper may then be rolled again to produce a watermark of even thickness but with varying density. The resulting watermark is generally much clearer and more detailed than those made by the Dandy Roll process, and as such Cylinder Mould Watermark Paper is the preferred type of watermarked paper for banknotes, passports, motor vehicle titles, and other documents where it is an important anti-counterfeiting measure.
Watermarks on postage stamps and stationery.
In philately, the watermark is a key feature of a stamp, and often constitutes the difference between a common and a rare stamp. Collectors who encounter two otherwise identical stamps with different watermarks consider each stamp to be a separate identifiable issue. The "classic" stamp watermark is a small crown or other national symbol, appearing either once on each stamp or a continuous pattern. Watermarks were nearly universal on stamps in the 19th and early 20th centuries, but generally fell out of use and are not commonly used on modern U.S. issues, but some countries continue to use them.
Some types of embossing, such as that used to make the "cross on oval" design on early stamps of Switzerland, resemble a watermark in that the paper is thinner, but can be distinguished by having sharper edges than is usual for a normal watermark. Stamp paper watermarks also show various designs, letters, numbers and pictorial elements.
The process of bringing out the stamp watermark is fairly simple. Sometimes a watermark in stamp paper can be seen just by looking at the unprinted back side of a stamp. More often, the collector must use a few basic items to get a good look at the watermark. For example, watermark fluid may be applied to the back of a stamp to temporarily reveal the watermark.
Even using the simple watermarking method described, it can be difficult to distinguish some watermarks. Watermarks on stamps printed in yellow and orange can be particularly difficult to see. A few mechanical devices are also are used by collectors to detect watermarks on stamps such as the Morley-Bright watermark detector and the more expensive Safe Signoscope. Such devices can be very useful for they can be used without the application of watermark fluid and also allow the collector to look at the watermark for a longer period of time to more easily detect the watermark.

</doc>
<doc id="32923" url="http://en.wikipedia.org/wiki?curid=32923" title="Western canon">
Western canon

The term "Western canon" denotes a body of books and, more broadly, music and art that have been traditionally accepted by Western scholars as the most important and influential in shaping Western culture. As such, it includes the "greatest works of artistic merit". Such a canon is important to the theory of educational perennialism and the development of "high culture". The idea of a Canon has been used to address the question "What is Art?"; according to this approach, a work is art by comparison to the works in the canon, or conversely, any aesthetic law to be valid should not rule out any of the works included in the canon. The concept has become challenged by advocates of multiculturalism and critics who charge that it has been influenced by race, gender, and other biases.
Origins.
The process of listmaking—defining the boundaries of the canon—is endless. The philosopher John Searle has said, "In my experience there never was, in fact, a fixed 'canon'; there was rather a certain set of tentative judgments about what had importance and quality. Such judgments are always subject to revision, and in fact they were constantly being revised."
One of the notable attempts at compiling an authoritative canon in the English-speaking world was the "Great Books of the Western World" program. This program, developed in the middle third of the 20th century, grew out of the curriculum at the University of Chicago. University president Robert Maynard Hutchins and his collaborator Mortimer Adler developed a program that offered reading lists, books, and organizational strategies for reading clubs to the general public.
An earlier attempt, the Harvard Classics (1909), was promulgated by Harvard University president Charles W. Eliot, whose thesis was the same as Carlyle's:
... The greatest university of all is a collection of books.—Thomas Carlyle
Debate.
There has been an ongoing debate, motivated by politics and social agendas, over the nature and status of the canon since at least the 1960s, much of which is rooted in critical theory, feminism, critical race theory, and Marxist attacks against capitalism and classical liberal principles. In the United States, in particular, the canon has been attacked as a compendium of books written mainly by "dead European men", that does not represent the viewpoints of many in contemporary societies around the world. Allan Bloom in his 1987 book "The Closing of the American Mind", has disagreed strongly. Yale University Professor of Humanities Harold Bloom (no relation to Allan) has also argued strongly in favor of the canon, and in general the canon remains as a represented idea in many institutions, though its implications continue to be debated.
Defenders maintain that those who undermine the canon do so out of primarily political interests, and that such criticisms are misguided and/or disingenuous. As John Searle has written:
There is a certain irony in this [i.e., politicized objections to the canon] in that earlier student generations, my own for example, found the critical tradition that runs from Socrates through the "Federalist Papers", through the writings of Mill and Marx, down to the twentieth century, to be liberating from the stuffy conventions of traditional American politics and pieties. Precisely by inculcating a critical attitude, the "canon" served to demythologize the conventional pieties of the American bourgeoisie and provided the student with a perspective from which to critically analyze American culture and institutions. Ironically, the same tradition is now regarded as oppressive. The texts once served an unmasking function; now we are told that it is the texts which must be unmasked.
One of the main objections to a canon of literature is the question of authority—who should have the power to determine what works are worth reading and teaching? Searle's rebuttal suggests that "one obvious difficulty with it [i.e., arguments against hierarchical ranking of books] is that if it were valid, it would argue against any set of required readings whatever; indeed, any list you care to make about anything automatically creates two categories, those that are on the list and those that are not."
Although there is debate among theorists, individuals such as teachers and students would consider the works within the canon to be those which are the most appropriate in exploiting social and historical contexts from selected time periods. Additionally, the works are usually considered a craft and are commonly used as a guide or rule, particularly for senior students, when reading and writing. (1983) supports this stating canons are "an institutional form for exposing people to a range of idealized attitudes." It is with this notion considered that work may be removed from the canon over time in order to reflect the contextual relevance and thoughts of society.
Works.
Works which are commonly included in the canon include works of fiction such as some epic poems, poetry, music, drama, novels, and other assorted forms of literature from the many diverse Western (and more recently non-Western) cultures. Many non-fiction works are also listed, primarily from the areas of religion, mythology, science, philosophy, psychology, economics, politics, and history.
Works which directly address the canon (both "for" and "against"):
Examples.
Examples of shorter canonical lists of most important works include the following:
University reading lists reflect the Western canon:
More comprehensive collections that include large parts of the Western canon include the following:
Chronological brackets:

</doc>
<doc id="32961" url="http://en.wikipedia.org/wiki?curid=32961" title="Wine">
Wine

Wine is an alcoholic beverage made from fermented grapes or other fruits. Due to the natural chemical balance, grapes ferment without the addition of sugars, acids, enzymes, water, or other nutrients. Yeast consumes the sugar in the grapes and converts it to ethanol and carbon dioxide. Different varieties of grapes and strains of yeasts produce different styles of wine. The well-known variations result from the very complex interactions between the biochemical development of the fruit, reactions involved in fermentation, terroir and subsequent appellation, along with human intervention in the overall process.
Wine has been produced for thousands of years. The earliest evidence of wine to date was found in the Republic of Georgia, where 8,000-year-old wine jars were uncovered. Traces of wine have also been found in Iran with 7,000-year-old wine jars and in Armenia with the 6,100-year-old Areni-1 winery, which is by far considered to be the earliest known winery. The earliest form of grape-based fermented drink however, was found in northern China, where archaeologists discovered 9,000-year-old pottery jars. Wine had reached the Balkans by c. 4500 BC and was consumed and celebrated in ancient Greece and Rome. It has been consumed for its intoxicating effects throughout history and the psychoactive effects are evident at normal serving sizes.
Wines made from produce besides grapes include rice wine, pomegranate wine, apple wine and elderberry wine and are generically called fruit wine.
Wine has played an important role in religion. Red wine was associated with blood by the ancient Egyptians and was used by both the Greek cult of Dionysus and the Romans in their Bacchanalia; Judaism also incorporates it in the Kiddush and Christianity in the Eucharist.
Etymology.
The English word "wine" comes from the Proto-Germanic "*winam", an early borrowing from the Latin "vinum", "wine" or "(grape) vine", itself derived from the Proto-Indo-European stem *"win-o-" (cf. Armenian: գինի , "gini"; Ancient Greek: οἶνος "oinos"; Aeolic Greek: ϝοῖνος "woinos"; Hittite: "wiyana"; Lycian: "oino").
The earliest attested terms referring to wine are the Mycenaean Greek 𐀕𐀶𐀺𐄀𐀚𐀺 "me-tu-wo ne-wo" (*μέθυϝος νέϝῳ), meaning "in (the month)" or "(festival) of the new wine", and 𐀺𐀜𐀷𐀴𐀯 "wo-no-wa-ti-si", meaning "wine garden", written in Linear B inscriptions. Linear B also includes, inter alia, an ideogram for wine, i.e. 𐂖.
Some scholars have noted the similarities between the words for wine in Indo-European languages (e.g. Armenian "gini", Latin "vinum", Ancient Greek οἶνος, Russian вино ]), Kartvelian (e.g. Georgian ღვინო ]), and Semitic ("*wayn"; Hebrew יין ]), pointing to the possibility of a common origin of the word denoting "wine" in these language families. The Georgian word goes back to Proto-Kartvelian *"ɣwino"-, which is generally believed to be a borrowing from Proto-Indo-European. Another hypothesis is that the lexeme was borrowed from Proto-Armenian *"ɣʷeinyo"-, whence Armenian "gini", but this is disputed. On the other hand, Fähnrich considers *"ɣwino"- a native Kartvelian word derived from the verbal root *"ɣun"- ('to bend'). See *"ɣwino"- for more.
Wines from other fruits, such as apples and berries, are usually named after the fruit from which they are produced combined with the word "wine" (for example, apple wine and elderberry wine) and are generically called fruit wine or country wine (not to be confused with the French term "vin de pays"). Besides the grape varieties traditionally used for winemaking, most fruits naturally lack either a high amount of fermentable sugars, relatively low acidity, yeast nutrients needed to promote or maintain fermentation or a combination of these three characteristics. This is probably one of the main reasons why wine derived from grapes has historically been more prevalent by far than other types and why specific types of fruit wine have generally been confined to regions in which the fruits were native or introduced for other reasons.
Other wines, such as barley wine and rice wine (e.g. sake), are made from starch-based materials and resemble beer more than wine, while ginger wine is fortified with brandy. In these latter cases, the term "wine" refers to the similarity in alcohol content rather than to the production process. The commercial use of the English word "wine" (and its equivalent in other languages) is protected by law in many jurisdictions.
History.
Archaeological evidence has established the earliest-known production of wine from fermenting grapes during the late Neolithic or early Chalcolithic in the Caucasus and the northern edge of the Middle East. The earliest chemically attested grape wine in the world was discovered at Hajji Firuz in the northwestern Zagros Mountains of Iran, ca. 5400 BC. Both archaeological and genetic evidence suggest that the earliest production of wine may slightly predate this, and the earliest wine making likely have taken place in Trans-Caucasia (including Armenia, Azerbaijan, Georgia), through the region between Eastern Turkey, and North West Iran.
The earliest form of grape-based fermented drink was found in northern China, where archaeologists discovered 9,000-year-old pottery jars, while the earliest archaeological evidence of wine particles found has been in Georgia, where archaeologists discovered evidence of wine residue inside ceramic jars that were dated back some 8000 years and Iran (c. 5000 BC). The earliest evidence of wine production was discovered in Armenia within the Areni-1 winery in 2007 and is at least 6,100 years old, making it the oldest winery in the world. The development of a winery implies wine had started being produced much earlier.
A 2003 report by archaeologists indicates a possibility that grapes were mixed with rice to produce mixed fermented beverages in China in the early years of the seventh millennium BC. Pottery jars from the Neolithic site of Jiahu, Henan, contained traces of tartaric acid and other organic compounds commonly found in wine. However, other fruits indigenous to the region, such as hawthorn, cannot be ruled out. If these beverages, which seem to be the precursors of rice wine, included grapes rather than other fruits, they would have been any of the several dozen indigenous wild species in China, rather than "Vitis vinifera", which was introduced there 6,000 years later.
The spread of wine culture westwards was most probably due to the Phoenicians who spread outward from a base of city-states along the Lebanese and Israeli coast. The wines of Byblos were exported to Egypt during the Old Kingdom and then throughout the Mediterranean. Evidence includes two Phoenician shipwrecks from 750 BC discovered by Robert Ballard, whose cargo of wine was still intact. As the first great traders in wine ("cherem"), the Phoenicians seem to have protected it from oxidation with a layer of olive oil, followed by a seal of pinewood and resin, again similar to retsina.
Literary references to wine are abundant in Homer (8th century BC, but possibly relating earlier compositions), Alkman (7th century BC), and others. In ancient Egypt, six of 36 wine amphoras were found in the tomb of King Tutankhamun bearing the name "Kha'y", a royal chief vintner. Five of these amphoras were designated as originating from the king's personal estate, with the sixth from the estate of the royal house of Aten. Traces of wine have also been found in central Asian Xinjiang in modern-day China, dating from the second and first millennia BC.
The first known mention of grape-based wines in India is from the late 4th-century BC writings of Chanakya, the chief minister of Emperor Chandragupta Maurya. In his writings, Chanakya condemns the use of alcohol while chronicling the emperor and his court's frequent indulgence of a style of wine known as "madhu".
The ancient Romans planted vineyards near garrison towns so wine could be produced locally rather than shipped over long distances. Some of these areas are now world renowned for wine production. The Romans discovered that burning sulfur candles inside empty wine vessels keeps them fresh and free from a vinegar smell. In medieval Europe, the Roman Catholic Church supported wine because the clergy required it for the Mass. Monks in France made wine for years, aging it in caves. An old English recipe that survived in various forms until the 19th century calls for refining white wine from bastard—bad or tainted "bastardo" wine.
Grape varieties.
Wine is usually made from one or more varieties of the European species "Vitis vinifera", such as Pinot noir, Chardonnay, Cabernet Sauvignon, Gamay and Merlot. When one of these varieties is used as the predominant grape (usually defined by law as minimums of 75% to 85%), the result is a "varietal" as opposed to a "blended" wine. Blended wines are not considered inferior to varietal wines, rather they are a different style of winemaking; some of the world's most highly regarded wines, from regions like Bordeaux and the Rhone Valley, are blended from different grape varieties.
Wine can also be made from other species of grape or from hybrids, created by the genetic crossing of two species. "V. labrusca" (of which the Concord grape is a cultivar), "V. aestivalis", "V. ruprestris", "V. rotundifolia" and "V. riparia" are native North American grapes usually grown to eat fresh or for grape juice, jam, or jelly, and only occasionally made into wine.
Hybridization is different from grafting. Most of the world's vineyards are planted with European "V. vinifera" vines that have been grafted onto North American species' rootstock, a common practice due to their resistance to phylloxera, a root louse that eventually kills the vine. In the late 19th century, most of Europe's vineyards (excluding some of the driest in the south) were devastated by the infestation, leading to widespread vine deaths and eventual replanting. Grafting is done in every wine-producing region in the world except in Argentina, the Canary Islands and Chile—the only places not yet exposed to the insect.
In the context of wine production, "terroir" is a concept that encompasses the varieties of grapes used, elevation and shape of the vineyard, type and chemistry of soil, climate and seasonal conditions, and the local yeast cultures. The range of possible combinations of these factors can result in great differences among wines, influencing the fermentation, finishing, and aging processes as well. Many wineries use growing and production methods that preserve or accentuate the aroma and taste influences of their unique "terroir". However, flavor differences are less desirable for producers of mass-market table wine or other cheaper wines, where consistency takes precedence. Such producers try to minimize differences in sources of grapes through production techniques such as micro-oxygenation, tannin filtration, cross-flow filtration, thin-film evaporation,
and spinning cones.
Classification.
Regulations govern the classification and sale of wine in many regions of the world. European wines tend to be classified by region (e.g. Bordeaux, Rioja and Chianti), while non-European wines are most often classified by grape (e.g. Pinot noir and Merlot). Market recognition of particular regions has recently been leading to their increased prominence on non-European wine labels. Examples of recognized non-European locales include Napa Valley, Santa Clara Valley and Sonoma Valley in California; Willamette Valley and Rogue Valley in Oregon; Columbia Valley in Washington; Barossa Valley in South Australia and Hunter Valley in New South Wales; Luján de Cuyo in Argentina; Central Valley in Chile; Vale dos Vinhedos in Brazil; Hawke's Bay and Marlborough in New Zealand; and Okanagan Valley and Niagara Peninsula in Canada.
Some blended wine names are marketing terms whose use is governed by trademark law rather than by specific wine laws. For example, Meritage (sounds like "heritage") is generally a Bordeaux-style blend of Cabernet Sauvignon and Merlot, but may also include Cabernet Franc, Petit Verdot, and Malbec. Commercial use of the term Meritage is allowed only via licensing agreements with the Meritage Association.
European classifications.
France has various appellation systems based on the concept of "terroir", with classifications ranging from "Vin de Table" ("table wine") at the bottom, through "Vin de Pays" and "Appellation d'Origine Vin Délimité de Qualité Supérieure" (AOVDQS), up to "Appellation d'Origine Contrôlée" (AOC) or similar, depending on the region. Portugal has developed a system resembling that of France and, in fact, pioneered this concept in 1756 with a royal charter creating the Demarcated Douro Region and regulating the production and trade of wine. Germany created a similar scheme in 2002, although it has not yet achieved the authority of the other countries' classification systems. Spain, Greece and Italy have classifications based on a dual system of region of origin and product quality.
Beyond Europe.
New World wines—those made outside the traditional wine regions of Europe—are usually classified by grape rather than by "terroir" or region of origin, although there have been unofficial attempts to classify them by quality.
Vintages.
In the United States, for a wine to be vintage-dated and labeled with a country of origin or American Viticultural Area (AVA) (e.g. Sonoma Valley), 95% of its volume must be from grapes harvested in that year. If a wine is not labeled with a country of origin or AVA the percentage requirement is lowered to 85%.
Vintage wines are generally bottled in a single batch so that each bottle will have a similar taste. Climate's impact on the character of a wine can be significant enough to cause different vintages from the same vineyard to vary dramatically in flavor and quality. Thus, vintage wines are produced to be individually characteristic of the particular vintage and to serve as the flagship wines of the producer. Superior vintages from reputable producers and regions will often command much higher prices than their average ones. Some vintage wines (e.g. Brunello), are only made in better-than-average years.
For consistency, non-vintage wines can be blended from more than one vintage, which helps winemakers sustain a reliable market image and maintain sales even in bad years. One recent study suggests that for the average wine drinker, the vintage year may not be as significant for perceived quality as had been thought, although wine connoisseurs continue to place great importance on it.
Tasting.
Wine tasting is the sensory examination and evaluation of wine. Wines contain many chemical compounds similar or identical to those in fruits, vegetables, and spices. The sweetness of wine is determined by the amount of residual sugar in the wine after fermentation, relative to the acidity present in the wine. Dry wine, for example, has only a small amount of residual sugar.
Some wine labels suggest opening the bottle and letting the wine "breathe" for a couple of hours before serving, while others recommend drinking it immediately. Decanting (the act of pouring a wine into a special container just for breathing) is a controversial subject among wine enthusiasts. In addition to aeration, decanting with a filter allows the removal of bitter sediments that may have formed in the wine. Sediment is more common in older bottles, but aeration may benefit younger wines.
During aeration, a younger wine's exposure to air often "relaxes" the drink, making it smoother and better integrated in aroma, texture, and flavor. Older wines generally "fade" (lose their character and flavor intensity) with extended aeration. Despite these general rules, breathing does not necessarily benefit all wines. Wine may be tasted as soon as the bottle is opened to determine how long it should be aerated, if at all.
When tasting wine, individual flavors may also be detected, due to the complex mix of organic molecules (e.g. esters and terpenes) that grape juice and wine can contain. Experienced tasters can distinguish between flavors characteristic of a specific grape and flavors that result from other factors in winemaking. Typical intentional flavor elements in wine—chocolate, vanilla, or coffee—are those imparted by aging in oak casks rather than the grape itself.
Vertical and horizontal tasting involves a range of vintages within the same grape and vineyard, or the latter in which there is one vintage from multiple vineyards.
Banana flavors (isoamyl acetate) are the product of yeast metabolism, as are spoilage aromas such as sweaty, barnyard, band-aid (4-ethylphenol and 4-ethylguaiacol), and rotten egg (hydrogen sulfide). Some varieties can also exhibit a mineral flavor due to the presence of water-soluble salts as a result of limestone's presence in the vineyard's soil.
Wine aroma comes from volatile compounds released into the air. Vaporization of these compounds can be accelerated by twirling the wine glass or serving at room temperature. Many drinkers prefer to chill red wines that are already highly aromatic, like Chinon and Beaujolais.
The ideal temperature for serving a particular wine is a matter of debate, but some broad guidelines have emerged that will generally enhance the experience of tasting certain common wines. A white wine should foster a sense of coolness, achieved by serving at "cellar temperature" (13 C). Light red wines drunk young should also be brought to the table at this temperature, where they will quickly rise a few degrees. Red wines are generally perceived best when served "chambré" ("at room temperature"). However, this does not mean the temperature of the dining room—often around (21 C)—but rather the coolest room in the house and, therefore, always slightly cooler than the dining room itself. Pinot noir should be brought to the table for serving at (16 C) and will reach its full bouquet at (18 C). Cabernet Sauvignon, zinfandel, and Rhone varieties should be served at (18 C) and allowed to warm on the table to 21 C for best aroma.
Collecting.
Outstanding vintages from the best vineyards may sell for thousands of dollars per bottle, though the broader term "fine wine" covers those typically retailing in excess of US$30–50. "Investment wines" are considered by some to be Veblen goods: those for which demand increases rather than decreases as their prices rise.
Particular selections have higher value, such as "Verticals", in which a range of vintages of a specific grape and vineyard, are offered. The most notable was a Chateau d'Yquem 135 year vertical containing every vintage from 1860 to 2003 sold for $1.5 million.
The most common wines purchased for investment include those from Bordeaux and Burgundy; cult wines from Europe and elsewhere; and vintage port. Characteristics of highly collectible wines include:
Investment in fine wine has attracted those who take advantage of their victims' relative ignorance of this wine market sector. Such wine fraudsters often profit by charging excessively high prices for off-vintage or lower-status wines from well-known wine regions, while claiming that they are offering a sound investment unaffected by economic cycles. As with any investment, thorough research is essential to making an informed decision.
Production.
Wine grapes grow almost exclusively between 30 and 50 degrees latitude north and south of the equator. The world's southernmost vineyards are in the Central Otago region of New Zealand's South Island near the 45th parallel south, and the northernmost are in Flen, Sweden, just north of the 59th parallel north.
Exporting countries.
The UK was the world's largest importer of wine in 2007.
Wine production in the European Union in 2005 and 2006.
2005 Estimate (thousands of hectoliters)
2006 Estimate (thousands of hectoliters)
World production in 2003.
In 2003, world wine production had reached 269 millions of hectoliters. The world's main 15 wine producers were:
In the USA.
The Wine Institute, a nonprofit group, tracked that the U.S. has been the largest wine consuming nation in the world since 2010, with residents individually consuming a total of 2.82 gallons a year in 2013, up from under 2 gallons in 1979. Wine shipments within the U.S. from California alone were 215 million cases in 2013, up 3% from the previous year, with an estimated retail value of $23.1 billion, up 5%. All American regions produce wine however, 95% comes from four regions with California as the most prolific producer, followed by Washington, Oregon and New York.
Consumption.
Wine-consumption data from a list of countries by alcohol consumption measured in liters of pure ethyl alcohol consumed per capita in a given year, according to the most recent data from the World Health Organization. The methodology includes persons 15 years of age or older.
Uses.
Wine is a popular and important beverage that accompanies and enhances a wide range of cuisines, from the simple and traditional to the most sophisticated and complex. Wine is important in cuisine not just for its value as a beverage, but as a flavor agent, primarily in stocks and braising, since its acidity lends balance to rich savory or sweet dishes. Wine sauce is an example of a culinary sauce that uses wine as a primary ingredient. Natural wines may exhibit a broad range of alcohol content, from below 9% to above 16% ABV, with most wines being in the 12.5%–14.5% range. Fortified wines (usually with brandy) may contain 20% alcohol or more.
Religious significance.
Ancient religions.
The use of wine in ancient Near Eastern and Ancient Egyptian religious ceremonies was common. Libations often included wine, and the religious mysteries of Dionysus used wine as a sacramental entheogen to induce a mind-altering state.
Judaism.
Wine is an integral part of Jewish laws and traditions. The "Kiddush" is a blessing recited over wine or grape juice to sanctify the Shabbat. On Pesach (Passover) during the Seder, it is a Rabbinic obligation of adults to drink four cups of wine. In the Tabernacle and in the Temple in Jerusalem, the libation of wine was part of the sacrificial service. Note that this does not mean that wine is a symbol of blood, a common misconception that contributes to the Christian myth of the blood libel.
"It has been one of history's cruel ironies that the blood libel—accusations against Jews using the blood of murdered gentile children for the making of wine and matzot—became the false pretext for numerous pogroms. And due to the danger, those who live in a place where blood libels occur are halachically exempted from using red wine, lest it be seized as "evidence" against them."
Christianity.
In Christianity, wine is used in a sacred rite called the Eucharist, which originates in the Gospel account of the Last Supper (Gospel of Luke 22:19) describing Jesus sharing bread and wine with his disciples and commanding them to "do this in remembrance of me." Beliefs about the nature of the Eucharist vary among denominations (see Eucharistic theologies contrasted).
While some Christians consider the use of wine from the grape as essential for the validity of the sacrament, many Protestants also allow (or require) pasteurized grape juice as a substitute. Wine was used in Eucharistic rites by all Protestant groups until an alternative arose in the late 19th century. Methodist dentist and prohibitionist Thomas Bramwell Welch applied new pasteurization techniques to stop the natural fermentation process of grape juice. Some Christians who were part of the growing temperance movement pressed for a switch from wine to grape juice, and the substitution spread quickly over much of the United States, as well as to other countries to a lesser degree. There remains an ongoing debate between some American Protestant denominations as to whether wine can and should be used for the Eucharist or allowed as an ordinary beverage, with Catholics and some mainline Protestants allowing wine drinking in moderation, and some conservative Protestant groups opposing consumption of alcohol altogether.
Islam.
Alcoholic beverages, including wine, are forbidden under most interpretations of Islamic law. Iran had previously had a thriving wine industry that disappeared after the Islamic Revolution in 1979. In Greater Persia, "mey" (Persian wine) was a central theme of poetry for more than a thousand years, long before the advent of Islam. Some Alevi sects use wine in their religious services.
Certain exceptions to the ban on alcohol apply. Alcohol derived from a source other than the grape (or its byproducts) and the date is allowed in "very small quantities" (loosely defined as a quantity that does not cause intoxication) under the Sunni Hanafi "madhab", for specific purposes (such as medicines), where the goal is not intoxication. However, modern Hanafi scholars regard alcohol consumption as totally forbidden.
Health effects.
Studies of the health effects of wine have focused on cardiovascular health, cancer, Alzheimer's disease, alcoholism, cirrhosis of the liver, and oral bacteria. Although excessive alcohol consumption has adverse health effects, epidemiological studies have consistently demonstrated that moderate consumption of alcohol and wine is statistically associated with a decrease in cardiovascular illness such as heart failure. Additional news reports on the French paradox also back the relationship.
This paradox concerns the comparatively low incidence of coronary heart disease in France despite relatively high levels of saturated fat in the traditional French diet. Some epidemiologists suspect that this is due to higher wine consumption by the French, but the scientific evidence for this theory is limited. Because the average moderate wine drinker is likely to exercise more often, to be more health conscious, and to be from a higher educational and socioeconomic background, the association between moderate wine drinking and better health may be related to confounding factors or represent a correlation rather than cause and effect.
Population studies have observed a J-curve correlation between wine consumption and the prevalence of heart disease: heavy drinkers have an elevated prevalence, while moderate drinkers (up to 20g of alcohol per day, approximately 200 ml of 12.7% ABV wine) have a lower prevalence than non-drinkers. Studies have also found that moderate consumption of other alcoholic beverages is correlated with decreased mortality from cardiovascular causes, although the association is stronger for wine. Additionally, some studies have found a greater correlation of health benefits with red than white wine, though other studies have found no difference. Red wine contains more polyphenols than white wine, and these could be protective against cardiovascular disease.
A chemical in grapes, red wine, peanuts and blueberries called resveratrol has been shown to have both cardioprotective and chemoprotective effects in animal studies. Low doses of resveratrol in the diet of middle-aged mice has a widespread influence on the genetic factors related to aging and may confer special protection on the heart. Specifically, low doses of resveratrol mimic the effects of caloric restriction—diets with 20–30% fewer calories than a typical diet. Resveratrol is produced naturally by grape skins in response to fungal infection, including exposure to yeast during fermentation. As white wine has minimal contact with grape skins during this process, it generally contains lower levels of the chemical. Beneficial compounds in wine also include other polyphenols, antioxidants, and flavonoids.
Sipping slowly when drinking may result in optimal absorption of the resveratrol in wine. Due to inactivation in the gut and liver, most of the resveratrol consumed while drinking red wine does not reach the blood circulation. However, when sipping slowly, absorption via the mucous membranes in the mouth can result in up to 100 times the blood levels of resveratrol, according to Stephen Taylor, Ph.D.
Red wines from the south of France and from Sardinia in Italy have the highest levels of procyanidins, compounds in grape seeds which could be responsible for red wine's heart benefits. Red wines from these areas contain between two and four times as much procyanidins as other red wines tested. Procyanidins suppress the synthesis of a peptide called endothelin-1 that constricts blood vessels.
A 2007 study found that both red and white wines are effective antibacterial agents against strains of "Streptococcus". In addition, a report in the October 2008 issue of "Cancer Epidemiology, Biomarkers and Prevention" posits that moderate consumption of red wine may decrease the risk of lung cancer in men.
While evidence from laboratory and epidemiological (observational) studies suggest a cardioprotective effect, no controlled studies have been completed on the effect of alcoholic beverages on the risk of developing heart disease or stroke. Excessive consumption of alcohol can cause cirrhosis of the liver and alcoholism; the American Heart Association states that "the American Heart Association cautions people NOT to start drinking ... if they do not already drink alcohol. Consult your doctor on the benefits and risks of consuming alcohol in moderation."
Wine's effect on the brain is also under study. One study concluded that wine made from the Cabernet Sauvignon grape reduces the risk of Alzheimer's Disease. Another study found that among alcoholics, wine damages the hippocampus, a brain area involved in memory processes, to a greater degree than other alcoholic beverages.
Sulfites in wine can cause some people, particularly those with asthma, to have adverse reactions. Sulfites are present in all wines and are formed as a natural product of the fermentation process; many winemakers add sulfur dioxide in order to help preserve wine. Sulfur dioxide is also added to foods such as dried apricots and orange juice. The level of added sulfites varies; some wines have been marketed with low sulfite content.
A study of women in the United Kingdom, called The Million Women Study, concluded that moderate alcohol consumption can increase the risk of certain cancers, including breast, pharynx and liver cancer. Lead author of the study, Professor Valerie Beral, asserted that there is scant evidence that any positive health effects of red wine outweigh the risk of cancer. She said, "It's an absolute myth that red wine is good for you." Professor Roger Corder, author of the bestselling book"The Red Wine Diet", countered that two small glasses of a very tannic, procyanidin-rich wine would confer a benefit, although "most supermarket wines are low-procyanidin and high-alcohol." No professional medical association recommends that people who are nondrinkers should start drinking wine.
Forgery and manipulation of wines.
Incidents of fraud, such as mislabeling the origin or quality of wines, have resulted in regulations on labeling. "Wine scandals" that have received media attention include:
Packaging.
Most wines are sold in glass bottles and sealed with corks (50% of which come from Portugal). An increasing number of wine producers have been using alternative closures such as screwcaps and synthetic plastic "corks". Although alternative closures are less expensive and prevent cork taint, they have been blamed for such problems as excessive reduction.
Some wines are packaged in thick plastic bags within corrugated fiberboard boxes, and are called "box wines", or "cask wine". Tucked inside the package is a tap affixed to the bag in box, or bladder, that is later extended by the consumer for serving the contents. Box wine can stay acceptably fresh for up to a month after opening because the bladder collapses as wine is dispensed, limiting contact with air and, thus, slowing the rate of oxidation. In contrast, bottled wine oxidizes more rapidly after opening because of the increasing ratio of air to wine as the contents are dispensed; it can degrade considerably in a few days.
Environmental considerations of wine packaging reveal benefits and drawbacks of both bottled and box wines. The glass used to make bottles is a nontoxic, naturally occurring substance that is completely recyclable, whereas the plastics used for box-wine containers are typically much less environmentally friendly. However, wine-bottle manufacturers have been cited for Clean Air Act violations. A "New York Times" editorial suggested that box wine, being lighter in package weight, has a reduced carbon footprint from its distribution; however, box-wine plastics, even though possibly recyclable, can be more labor-intensive (and therefore expensive) to process than glass bottles. In addition, while a wine box is recyclable, its plastic bladder most likely is not.
Storage.
Wine cellars, or wine rooms, if they are above-ground, are places designed specifically for the storage and aging of wine. In an "active" wine cellar, temperature and humidity are maintained by a climate-control system. "Passive" wine cellars are not climate-controlled, and so must be carefully located. Because wine is a natural, perishable food product, all types—including red, white, sparkling, and fortified—can spoil when exposed to heat, light, vibration or fluctuations in temperature and humidity. When properly stored, wines can maintain their quality and in some cases improve in aroma, flavor, and complexity as they age. Some wine experts contend that the optimal temperature for aging wine is 13 C, others 15 C.
Wine refrigerators offer an alternative to wine cellars and are available in capacities ranging from small, 16-bottle units to furniture-quality pieces that can contain 400 bottles. Wine refrigerators are not ideal for aging, but rather serve to chill wine to the perfect temperature for drinking. These refrigerators keep the humidity low (usually under 50%), below the optimal humidity of 50% to 70%. Lower humidity levels can dry out corks over time, allowing oxygen to enter the bottle, which reduces the wine's quality through oxidation.
Further reading.
</dl>

</doc>
<doc id="33116" url="http://en.wikipedia.org/wiki?curid=33116" title="World Series of Poker">
World Series of Poker

The World Series of Poker (WSOP) is a series of poker tournaments held annually in Las Vegas and, since 2005, sponsored by Caesars Entertainment (known as Harrah's Entertainment until 2010). It dates its origins to 1970, when Benny Binion invited seven of the best-known poker players to the Horseshoe Casino for a single tournament, with a set start and stop time, and a winner determined by secret ballot.
The winner of each event receives a World Series of Poker bracelet and a monetary prize based on the number of entrants and buy-in amounts. Over the years, the tournament has grown in both the number of events and in the number of participants. Each year, the WSOP culminates with the $10,000 no-limit hold'em "Main Event," which, since 2004, has attracted entrants numbering in the thousands. The victor receives a multi-million dollar cash prize and a bracelet, which has become the most coveted award a poker player can win. The winner of the World Series of Poker Main Event is considered to be the World Champion of Poker.
As of 2014, the WSOP consists of 65 events, with most major poker variants featured. However, in recent years, over half of the events have been variants of Texas hold 'em. Events traditionally take place during one day or over several consecutive days during the series in June and July. However, starting in 2008, the Main Event final table was delayed until November. The 2012 final table commenced in October because of the United States presidential election.
Format.
Since 1971, all WSOP events have been tournaments with cash prizes. In 1973 a five-card stud event was added. Since then, new events have been added and removed. Since 1976, a bracelet has been awarded to the winner of every event at the annual WSOP; later on, the winners of pre-1976 events were retroactively given bracelets.
The tournament grew slowly for over a decade, reaching 52 participants in 1982. In the early 1980s, satellite tournaments were introduced, allowing people to win their way into the various events. By 1987, there were over 2,100 entrants in the entire series.
At the 2006 World Series of Poker, there were 45 events, covering the majority of poker variants. Participation in the Main Event peaked that year, with 8,773 players.
Currently, Texas hold 'em, Omaha hold 'em and Seven-card stud and their lowball variants are played. H.O.R.S.E. has been played in the past and returned in 2006. Also, S.H.O.E. has been played in the past, and returned in 2007. Other events played in the past include Chinese poker, Five card stud, and many others. Like most tournaments, the sponsoring casino takes an entry fee (a percentage between 6% and 10%, depending on the buy-in) and distributes the rest, hence the prize money increasing with more players. In the 2005 Main Event, US$52,818,610 in prize money was distributed among 560 players, with US$7.5 million as the first prize. The 2006 Main Event, won by Jamie Gold, is the largest single poker tournament by prize pool or by entrant numbers in history; Gold pocketed US$12 million for his victory. In July 2010, it was announced that the winner of the 2010 Main Event would receive just under US$9 million.
On June 2, 2011, the World Series of Poker and Cirque du Soleil founder Guy Laliberté announced plans for an officially sanctioned special fundraising event, known as The Big One for One Drop, starting on July 1, 2012 with a record US$1 million entry fee. 11% of the money (more precisely, $111,111 from each buy-in) went to Laliberté's charity, the One Drop Foundation, and the WSOP waived its normal 10% rake of the entry fees. At the time of the original announcement, 15 of the maximum 48 seats had been taken. By early December 2011, the field size had increased to 22, the minimum required for an official bracelet tournament. Among those who committed early to the event were Johnny Chan, Daniel Negreanu, Jonathan Duhamel, Tom Dwan, Laliberté, billionaire businessman Phil Ruffin and Erik Seidel. On April 12, 2012, the WSOP announced that 30 players had committed to the tournament, which brought the first prize to $12.3 million, exceeding the record amount won by Jamie Gold. In the end, all 48 seats were filled, resulting in a first prize of $18.3 million. Poker professional Antonio Esfandiari won the event, also receiving a special platinum WSOP bracelet.
Highlights.
The number of participants in the WSOP grew every year from 2000 until 2006. Following 2006, new online gambling legislation restricted the number of online qualifiers to the event. 2007 was the first dip in numbers in this century while in 2008 more people participated than the previous year. In 2000 there were 4,780 entrants in the various events, but in 2005, the number rose to over 23,000 players. In the main event alone, the number of participants grew from 839 in 2003 to 8,773 in 2006. Phil Hellmuth has won the most bracelets with 13 followed by Doyle Brunson, Johnny Chan, and Phil Ivey with ten bracelets each. Crandell Addington is the only player to place in the top ten of the World Series of Poker Main Event eight times, albeit in earlier years with small fields compared to modern times. Four players have won the Main Event multiple times: Johnny Moss (1970, 1971, and 1974), Doyle Brunson (1976 and 1977), Stu Ungar (1980, 1981, and 1997) and Johnny Chan (1987 and 1988). Bracelet winners who first achieved fame in other fields include French actor/singer Patrick Bruel (in 1998), Danish soccer player Jan Vang Sørensen (in 2002) and American actress Jennifer Tilly (in 2005). In recent years, there have been non-bracelet events at the WSOP; two of the most notable are the "World Series of Rock Paper Scissors" and "Ante Up for Africa."
History.
The idea of a World Series of Poker began in 1969 with an event called the Texas Gambling Reunion. It was an invitational event sponsored by Tom Moore of San Antonio, Texas, and held at the Holiday Hotel and Casino in Reno. This inaugural event was won by Crandell Addington. The set of tournaments that the World Series of Poker (WSOP) would evolve into was the brainchild of Las Vegas casino owner and poker player Benny Binion. In 1970, the first WSOP at Binion's Horseshoe took place as a series of cash games that included five-card stud, deuce to seven low-ball draw, razz, seven-card stud, and Texas hold 'em. The format for the Main Event as a freeze-out Texas hold 'em game came the next year. The winner in 1970, Johnny Moss, was elected by his peers as the first "World Champion of Poker" and received a silver cup as a prize.
Acquisition by Harrah's.
In 2004, Harrah's Entertainment (now Caesars Entertainment) purchased Binion's Horseshoe, retained the rights to the Horseshoe and World Series of Poker brands, sold the hotel and casino to MTR Gaming Group, and announced that the 2005 Series events would be held at the Harrah's-owned Rio Hotel and Casino, located just off the Las Vegas Strip. The final two days of the main event in 2005 were held downtown at what is now the MTR-operated "Binion's" in celebration of the centennial of the founding of Las Vegas. It also added a made-for-television $2 million "freeroll" invitational Tournament of Champions (TOC) event first won by Annie Duke as a "winner-take-all" event.
Starting in 2005, the WSOP began a tournament "circuit" at Harrah's-owned properties in the United States where, in addition to the $10,000 buy-in tournament at each site, qualifying players became eligible for a revamped Tournament of Champions. The 2005 TOC, made up of the top twenty qualifying players at each circuit event, along with the final table from the 2005 Main Event and the winners of nine or more bracelets (Johnny Chan, Doyle Brunson, and Phil Hellmuth) would participate in the revamped TOC at Caesars Palace. Mike Matusow won the first prize of $1 million (US), and all the players at the final table were guaranteed a minimum of $25,000 for the eighth and ninth place finishers. During a break in the final table of the 2005 Main Event on July 16, Harrah's announced that eleven properties — including the recently added Bally's and Caesar's properties — would host 2005–06 WSOP Circuit events that started on August 11 in Tunica, Mississippi. One event that was scheduled for Biloxi, Mississippi, was canceled after the Grand Casino Biloxi, which was scheduled to host the event, suffered major damage from Hurricane Katrina. The Rio also hosted the 2006 World Series of Poker, which began on June 25 with satellite events and formally began the day after with the annual Casino Employee event, won in 2006 by Chris Gros. 2006 featured the Tournament of Champions on June 25 and 26, won by Mike Sexton. Various events led up to the main event, which was held from July 28 until August 10. The first prize of $12 million was awarded to Jamie Gold.
Main Event.
Since 1972, the Main Event of the WSOP has been the $10,000 buy-in no-limit Texas Hold 'Em (NLHE) tournament (in 1971 the buy-in was $5,000 and the inaugural 1970 event was an invitational with winner determined by a vote). Winners of the event not only get the largest prize of the tournament and a gold bracelet, but additionally their picture is placed in the "Gallery of Champions" at Binion's. The winner of the Main Event has traditionally been given the unofficial title of World Champion. However, some believe that no-limit hold 'em is not the optimal structure for determining a champion poker player. In 2002, Daniel Negreanu argued that the Main Event should switch to pot-limit hold 'em, believing that pot-limit required a more complete set of poker skills than no-limit, although he admitted that such a change would likely never be made. However, many of the game's top professionals, including Negreanu, have since stated that the recently added $50,000 H.O.R.S.E./Poker Player's Championship event is the one which ultimately decides the world's best player. The $50,000 buy-in, being five times larger than the buy-in for the Main Event, has thus far tended to deter amateurs from playing in this event, and the variety of games played require a broader knowledge of poker. The first $50,000 event, conducted as a H.O.R.S.E. tournament, was won by Chip Reese in 2006. In 2010, the $50,000 event changed from H.O.R.S.E. to an "8-game" format, adding no-limit hold 'em, pot-limit Omaha, and 2–7 triple draw to the mix, and was rechristened The Poker Player's Championship, with Michael Mizrachi winning the first edition of the revamped event. Since Reese's death in December 2007, the winner of this event receives the David 'Chip' Reese Memorial Trophy in addition to the bracelet and the prize money.
There have been many memorable moments during the main events, including Jack Straus's 1982 comeback win after discovering he had one $500 chip left when he thought he was out of the tournament. The end of the 1988 main event was featured in the movie "Rounders". Chris Moneymaker and Greg Raymer, the winners in 2003 and 2004, both qualified for the main event through satellite tournaments at the PokerStars online card room. Jerry Yang, the winner in 2007, had only been playing poker for two years prior to his victory. He won his seat at a $225 satellite tournament at Pechanga Resort & Casino, in California. With passage of the Unlawful Internet Gambling Enforcement Act (UIGEA) of 2006 online poker sites have been barred from purchasing entrance directly for their users.
Players.
Records.
Since its inception, Stu Ungar and Johnny Moss are the only players to have won the Main Event three times. However, Moss' first victory came in a different format, as he was elected winner by vote of his fellow players at the conclusion of what was then a timed event. Moss (if the first time win by vote is counted), Ungar, Doyle Brunson, and Johnny Chan are the only people who have won the Main Event in consecutive years. Johnny Chan's second victory in 1988 was featured in the 1998 film "Rounders".
Phil Hellmuth holds multiple WSOP records including most bracelets, most WSOP cashes, and most WSOP final tables. He is also the only player to have won the Main Events of both the WSOP and WSOP Europe.
In recent years, the prize pool for the WSOP Main Event has become so large that the winner instantly becomes one of the top money winners of WSOP and even in tournament poker history. Before July 2012, the top seven players on the all-time WSOP Earnings list were Main Event champions from 2005 to 2011, among whom Jamie Gold topped those seven, he won the 2006 Main Event, which had then the biggest first prize for a single tournament, and still is the largest poker tournament by prize pool in history. However, the all-time leader is currently Antonio Esfandiari, who has not won a Main Event. He collected a record-breaking first prize of $18.3 million in July 2012 when he won The Big One for One Drop, a charitable WSOP event with a $1 million buy-in. The players in second and third place on the all-time earnings list, Daniel Colman and Daniel Negreanu, have also yet to win a Main Event in Las Vegas, although Negreanu won the inaugural WSOP Asia Pacific Main Event in 2013. They finished in the top two places in the 2014 Big One for Big Drop, respectively winning $15.3 million and $8.3 million.
The list below includes the WSOP Europe and WSOP Asia-Pacific, but excludes WSOP Circuit events and other non-bracelet events. The results are updated through the 2014 WSOP APAC.
Player of the Year.
Since 2004, a Player of the Year (POY) award has been given to the player with the most points accumulated throughout the WSOP. As of 2013, nine different players have won the ten awards, with Daniel Negreanu the only repeat (2-time) winner. 
Only "open" events in which all players can participate count in the standings; this eliminates the Seniors, Ladies, and Casino Employee events. Beginning with the 2006 World Series of Poker, the Main Event and the $50,000 H.O.R.S.E. competition had no effect on the outcome of the winner of the Player of the Year award. In the 2008 World Series of Poker, the $50,000 H.O.R.S.E. event counted toward the Player of the Year award, but the Main Event did not. Since 2009, all open events, including the Main Event, count towards Player of the Year. The Player of the Year standings were based upon performance solely at the WSOP in Las Vegas up until 2010, but beginning in 2011 have also taken the World Series of Poker Europe into account, and starting in 2013 also include events in the World Series of Poker Asia Pacific. The 2011 WSOP Player of the Year organized by "Bluff Magazine" used a different scoring system which took into account field sizes and buy-in amounts when calculating points earned. This scoring system has been used ever since.
Poker Hall of Fame.
Since its inception in 1979, the WSOP Poker Hall of Fame has honored 42 individuals. Selection criteria for players include having competed against acknowledged top competition, played for high stakes and played consistently well to gain the respect of their peers. For non-players, selection is based on positive and lasting contributions to the overall growth and success of poker.
Expansion.
2007 - World Series of Poker Europe.
The World Series of Poker Europe (WSOPE) is the first expansion of the World Series of Poker. Since 1970, the event has occurred every year in Las Vegas. In September 2007, the first WSOP championship events outside of Las Vegas, complete with bracelets, were held. The inaugural WSOPE consisted of three events held in London from September 6–17, 2007. The main event, a GBP 10,000 buy-in no-limit hold 'em tournament, was won by Norwegian online prodigy Annette Obrestad on the day before her 19th birthday. This made her the youngest person ever to win a WSOP bracelet, a record that cannot be broken in the Las Vegas WSOP under current laws because the minimum legal age for casino gaming in Nevada is 21. Obrestad could play in the WSOPE because the minimum age for casino gaming in the United Kingdom is 18. While no definitive plans have been announced, WSOP Commissioner Jeffrey Pollack has indicated that in the next one to three years that other venues may start holding WSOP events. Two locations that have been mentioned as possible expansion sites are Egypt and South Africa, and the World Series of Poker Africa was ultimately launched in South Africa in 2010. However, it is currently treated as a WSOP Circuit event, with no bracelets awarded. The next expansion of the WSOP that included bracelet events was ultimately to Australia.
The WSOPE moved from London to Cannes, France in 2011. At that time, the buy-ins and payouts changed from being fixed in pounds to euros. The event moved again in 2013, this time to the Paris suburb of Enghien-les-Bains.
In November 2013, it was announced that the WSOPE would in the future alternate with the World Series of Poker Asia Pacific (WSOP APAC), which launched in 2013. The WSOPE will now be held only in odd-numbered years, with WSOP APAC conducted in even-numbered years.
2010 World Series of Poker - Africa.
In 2010, the WSOP expanded overseas once again, only this time to Gauteng, South Africa. While the WSOPE awarded bracelets, the World Series of Poker Africa Although the 2010 event was part of the WSOP Circuit, winners did not earn a gold ring or standing for the WSOP Circuit National Championship, both of which were common for other circuit events. This policy changed in 2012. The WSOPA did not occur in 2011, but resumed in 2012.
2013 World Series of Poker Asia-Pacific.
On April 30, 2012, the WSOP and Australian casino Crown Melbourne jointly announced the creation of the World Series of Poker Asia-Pacific (WSOP APAC). The first edition of the event was held at Crown's Melbourne casino from April 4–15, 2013 and featured five bracelet events.
WSOP television coverage.
1970s–1980s.
The earliest filming of the World Series was a special produced by Binion's Horseshoe in 1973 and narrated by Jimmy "The Greek" Snyder. CBS began covering the World Series in the late 1970s. In the early 1980s, the event was again broadcast as specials. In the late 1980s, the World Series returned to television as ESPN took over broadcasting. Initially, coverage consisted of just a single one-hour taped-delay broadcast of the main event.
1990s.
ESPN Classic currently airs many of the old broadcasts from the mid-1990s and beyond. Since no "pocket cam" existed, very few hole cards were actually shown to television viewers. Generally, ESPN used poker-playing actors such as Dick Van Patten, Vince Van Patten, and Gabe Kaplan, with either the tournament director (usually Jim Albrecht) or a poker professional like Phil Hellmuth joining the team. Unlike today's coverage, ESPN featured no pre-taped interviews or profiles on the players. In addition, the commentators were generally on the casino floor itself.
In the 1994 coverage of the final hand of the main event shows Hugh Vincent holding 8c 5h . The flop was 8c 2s 6d, indicating that there were two 8c in the deck. The tournament director announces that Hugh Vincent needed two running spades to win. The likely hand for Hugh Vincent was 8s 5s, but there is no known video of the actual hand turned over by Hugh Vincent.
2000s.
From 1999 to 2001, the World Series of Poker was broadcast by The Discovery Channel. These hour-long programs presented more of an overview or recap of the WSOP as opposed to broadcasting an actual live event with play-by-play analysis and color commentary. The Discovery Channel's broadcast also featured final table players interviews interlaced throughout the show. ESPN would resume coverage the following year.
ESPN's coverage in 2002 was typical of their coverage in the 1990s (recorded in video, little or no post-production commentary or player profiles, no card cams). However, the final table broadcast was expanded over two one-hour episodes. The 2002 WSOP was the first with the "sneak peek" (later called the pocket cam, or hole cam).
In 2003, Fred Christenson secured the long-term rights acquisition for ESPN, and the channel expanded their coverage to new heights with their coverage of the WSOP. They included coverage of the entire tournament, with a "Featured Table". At this table, the viewers could see the player's hole cards and subsequent strategy. The action was also broadcast as if live, though on tape-delay. 2003 was the first year that the broadcast covered action preceding the final table. Since then, ESPN has greatly expanded its coverage to include many of the preliminary events of the WSOP, especially Texas Hold 'Em. Also, their coverage of the main event now typically includes at least one hour program on each day. For the first two years of its existence, ESPN was broadcasting one hour programs of the "circuit" events that the WSOP has at various Harrah's-owned casinos, but ESPN did not renew these events. ESPN's coverage now includes many of the trappings of sports coverage, such as lighter segments (called "The Nuts") and interviews. ESPN's coverage has been largely driven by Matt Maranz, Executive Producer for the WSOP telecasts. Maranz leads 441 Productions, which produces the telecast under contract to ESPN's unit ESPN Original Entertainment (EOE). Maranz has significant sports production experience, having previously worked on ESPN's football pre-game show, and has also produced taped segments for NBC's Olympic coverage.
Coverage would increase in 2004 and 2005 to include preliminary events from the WSOP, in addition to the "Main Event". ESPN has expanded poker to all-new levels, especially with their coverage of the 2006 WSOP, including providing the entire final table of the 2006 Main Event via pay-per-view airing. In 2008, ESPN experimented with the idea of a delayed final table. This idea presented greater sponsorship opportunities and notoriety, culminating in a recap of the Main Event and the conclusion of the 2008 Main Event final table. In 2009, ESPN announced they would again move the final table to November 2009. The WSOP also decided there would be no rebuy events in 2009. The decision was reached because of complaints that rebuy events provided an unfair advantage to professionals with no limitation on how much money they can spend for an event. There were 57 bracelet events that year. The 2010 WSOP had the same number of bracelet events as in 2009, again with no rebuy events.
With 58 bracelet events and no rebuy events, the 2011 WSOP featured unprecedented "nearly live" coverage, with broadcasts being delayed by much smaller amounts of time while still satisfying Nevada Gaming Commission regulators. Caesars Entertainment, via WSOP.com, streamed final-table coverage of all bracelet events on a 5-minute delay, although without pocket cams. The ESPN family of networks aired 36 hours of Main Event coverage leading up to the November Nine on a 30-minute delay, showing the hole cards of all players who voluntarily entered the pot once the hand ended. The Main Event final table was broadcast on a 15-minute delay with the same policy regarding hole cards. The first day of the final table was aired on ESPN2 and the final day on ESPN, with both days also streamed on ESPN3 and WSOP.com.
Marketing.
The WSOP has corporate sponsors and licensed products which pay fees to market themselves as an official sponsors and/or licensees and exclusively use the WSOP insignia and cross-promote with their events. Besides the Harrah's properties and ESPN, major sponsors have included Jack Links Beef Jerky, Miller Brewing's "Milwaukee's Best" brand of beers, Pepsi's SoBe Adrenaline Rush energy drink (sponsors of the 2005 TOC), Helene Curtis' Degree brand of anti-perspirant/deodorant, United States Playing Card's Bicycle Pro Cards, Bluff Magazine, GlaxoSmithKline/Bayer's Levitra erectile dysfunction medicine, and The Hershey Company. Licensees include Glu Mobile, Activision (video games for different platforms such as Nintendo's GameCube, Microsoft's Xbox, Sony's PlayStation 2, and PC, featuring computer-generated versions of stars like Ferguson), and products made by different companies ranging from chip sets, playing cards, hand-held games, and clothing like caps and shirts. The official playing cards and chips are manufactured by Excalibur Electronics, Inc. which is based out of Miami, Florida and has been the main chip licensee since 2005. The fees and licenses bring in more than a million dollars to Harrah's.
DVD releases.
In 2003 and 2004 DVD sets were released by ESPN of the Main Event.
Video games.
In 2005, a video game based on the tournament, titled "World Series of Poker", was released for several consoles and the computer. A sequel called ' came out in 2006. In 2007, ' was released. WSOP video poker machines now appear at some Harrah's casinos; the machines are standard video poker machines, but have a bonus feature which allows a player to play a modified game of Texas Hold 'em against the machine.
WSOP Poker Academy.
Beginning in 2007, Harrah's announced the creation of the World Series of Poker Academy, a poker school aimed at providing poker players with the skills needed to win a WSOP Bracelet. The instructors for the Academy include Annie Duke, Phil Hellmuth, Jr., Greg Raymer, Scott Fischman, Mark Kroon, Mark Seif, Alex Outhred, and former FBI interrogator Joe Navarro. Initial academies were launched in Tunica, Mississippi, Indiana, and Las Vegas.
WSOP online.
In September 2009 Harrah's signed an agreement with Dragonfish, the B2B arm of 888 Holdings, to provide its online gaming services. The offering went live in the UK later that year, allowing UK users to play for real money. Real money online poker is available in the United States, but only in Nevada and New Jersey.
WSOP Arizona Lottery.
In December 2010, the Arizona Lottery issued Game Number 739: World Series of Poker $5 Scratchers(sm) with a $50,000.00 top prize. Played on two tables, the game included a second chance drawing for non-winning tickets to win one of two Grand Prize Trip Packages that included a seat at the 2011 World Series of Poker Main Event or one of eight WSOP Poker Party Prize Packs.

</doc>
<doc id="33804" url="http://en.wikipedia.org/wiki?curid=33804" title="Wellington">
Wellington

Wellington () is the capital city and second most populous urban area of New Zealand, with 393,600 residents. It is located at the south-western tip of the North Island, between Cook Strait and the Rimutaka Range. It is the major population centre of the southern North Island, and is the administrative centre of the Wellington Region, which also includes the Kapiti Coast and Wairarapa. Wellington is the world's southernmost capital city of a sovereign state.
The Wellington urban area comprises four cities: Wellington city, on the peninsula between Cook Strait and Wellington Harbour, contains the central business district and about half the population; Porirua on Porirua Harbour to the north is notable for its large Māori and Pacific Island communities; Lower Hutt and Upper Hutt are largely suburban areas to the northeast, together known as the Hutt Valley.
The 2014 Mercer Quality of Living Survey ranked Wellington 12th in the world. In 2011 Lonely Planet Best in Travel 2011 named Wellington as fourth in its Top 10 Cities to Visit in 2011, referring to it as the "coolest little capital in the world".
Etymology.
Wellington takes its name from Arthur Wellesley (1769-1852), the first Duke of Wellington and victor of the Battle of Waterloo (1815): his title comes from the town of Wellington in the English county of Somerset. It was named in November 1840 by the original settlers of the New Zealand Company on the suggestion of the directors of the same, in recognition of the Duke's strong support for the company's principles of colonisation and his "strenuous and successful defence against its enemies of the measure for colonising South Australia". One of the founders of the settlement, Edward Jerningham Wakefield, reported that the settlers "took up the views of the directors with great cordiality and the new name was at once adopted".
In Māori, Wellington has three names. Te Whanga-nui-a-Tara refers to Wellington Harbour and means "the great harbour of Tara"; "Pōneke" is a transliteration of "Port Nick", short for "Port Nicholson" (the city's central marae, the community supporting it and its "kapa haka" have the pseudo-tribal name of Ngāti Pōneke); "Te Upoko-o-te-Ika-a-Māui", meaning 'The Head of the Fish of Māui' (often shortened to "Te Upoko-o-te-Ika"), a traditional name for the southernmost part of the North Island, deriving from the legend of the fishing up of the island by the demi-god Māui.
In New Zealand Sign Language, the name is signed by raising the index, middle and ring fingers of one hand, palm forward, to form a "W", and shaking it slightly from side to side twice while mouthing "Wellington".
The city's location close to the mouth of the narrow Cook Strait leads to its vulnerability to strong gales, leading to the city's nickname of "Windy Wellington".
History.
Settlement.
Legends recount that Kupe discovered and explored the district in about the 10th century. The earliest date with hard evidence for Maori living in New Zealand is about 1280.
European settlement began with the arrival of an advance party of the New Zealand Company on the ship "Tory" on 20 September 1839, followed by 150 settlers on the "Aurora" on 22 January 1840. The settlers constructed their first homes at Petone (which they called Britannia for a time) on the flat area at the mouth of the Hutt River. When that proved swampy and flood prone they transplanted the plans, which had been drawn without regard for the hilly terrain.
National capital.
In 1865, Wellington became the capital city in place of Auckland, which William Hobson had made the capital in 1841. The Parliament of New Zealand had first met in Wellington on 7 July 1862, on a temporary basis; in November 1863, the Prime Minister of New Zealand, Alfred Domett, placed a resolution before Parliament in Auckland that "... it has become necessary that the seat of government ... should be transferred to some suitable locality in Cook Strait [region]." Apparently there had been some concerns that the more populous South Island (where the goldfields were located) would choose to form a separate colony in the British Empire. Several Commissioners invited from Australia, chosen for their neutral status, declared that Wellington was a suitable location because of its central location in New Zealand and good harbour. Parliament officially met in Wellington for the first time on 26 July 1865. At that time, the population of Wellington was just 4,900. Wellington's status as capital is by constitutional convention rather than statute.
Wellington is the location of the highest court, the Supreme Court of New Zealand, and the historic former High Court building has been enlarged and restored for its use. Government House, the official residence of the Governor-General, is in Newtown, opposite the Basin Reserve. Premier House, the official residence of the Prime Minister, is in Thorndon on Tinakori Road.
Importance.
Wellington is New Zealand's political centre, housing Parliament, the head offices of all Government Ministries and Departments and the bulk of the foreign diplomatic missions. It is an important centre of the film and theatre industry, and second to Auckland in terms of numbers of screen industry businesses. Te Papa Tongarewa (the Museum of New Zealand), the New Zealand Symphony Orchestra, the Royal New Zealand Ballet, Museum of Wellington City & Sea and the biennial New Zealand International Arts Festival are all sited there.
Wellington had the 12th best quality of living in the world in 2014, a ranking up from 13th place in 2012, according to a 2014 study by consulting company Mercer. Of cities with English as the primary language, it ranked fourth in 2007. Of cities in the Asia Pacific region, it ranked third (2014) behind Auckland and Sydney. It became much more affordable in terms of cost of living relative to cities worldwide, with its ranking moving from 93rd (more expensive) to 139th (less expensive) in 2009, probably as a result of currency fluctuations during the global economic downturn from March 2008 to March 2009. "Foreigners get more bang for their buck in Wellington, which is among the cheapest cities in the world to live", according to a 2009 article, which reported that currency fluctuations make New Zealand cities affordable for multinational firms to do business: "New Zealand cities were now more affordable for expatriates and were competitive places for overseas companies to develop business links and send employees".
Lonely Planet named Wellington 'the coolest little capital in the world' in its 'Best In Travel 2011' guide book. It is home to Weta Workshop, associated with Peter Jackson, behind critically acclaimed films like "The Lord of the Rings", "King Kong" and "Avatar".
Geography.
Wellington is at the south-western tip of the North Island on Cook Strait, separating the North and South Islands. On a clear day the snowcapped Kaikoura Ranges are visible to the south across the strait. To the north stretch the golden beaches of the Kapiti Coast. On the east the Rimutaka Range divides Wellington from the broad plains of the Wairarapa, a wine region of national notability. With a latitude of 41° 17' South, Wellington is the southernmost capital city in the world. It is also the most remote capital city, the farthest away from any other capital. It is more densely populated than most other cities in New Zealand due to the restricted amount of land that is available between its harbour and the surrounding hills. It has very few open areas in which to expand, and this has brought about the development of the suburban towns. Because of its location in the Roaring Forties and its exposure to the winds blowing through Cook Strait, Wellington is known as "Windy Wellington".
Wellington's scenic natural harbour and green hillsides adorned with tiered suburbs of colonial villas are popular with tourists. The CBD is close to Lambton Harbour, an arm of Wellington Harbour, which lies along an active geological fault, clearly evident on its straight western shore. The land to the west of this rises abruptly, meaning that many suburbs sit high above the centre of the city. There is a network of bush walks and reserves maintained by the Wellington City Council and local volunteers. These include Otari-Wilton's Bush, dedicated to the protection and propagation of native plants. The Wellington region has 500 km2 of regional parks and forests. In the east is the Miramar Peninsula, connected to the rest of the city by a low-lying isthmus at Rongotai, the site of Wellington International Airport.
The narrow entrance to the harbour is to the east of the Miramar Peninsula, and contains the dangerous shallows of Barrett Reef, where many ships have been wrecked (notably the inter-island ferry in 1968). The harbour has three islands: Matiu/Somes Island, Makaro/Ward Island and Mokopuna Island. Only Matiu/Somes Island is large enough for habitation. It has been used as a quarantine station for people and animals, and was an internment camp during World War I and World War II. It is a conservation island, providing refuge for endangered species, much like Kapiti Island farther up the coast. There is access during daylight hours by the .
Wellington is primarily surrounded by water, but some of near-by locations are listed below. 
Suburbs.
The urban area stretches across the areas administered by the city councils of Wellington, Hutt (covering Lower Hutt), Upper Hutt and Porirua.
Climate.
The city averages 2,050 hours of sunshine per year. The climate is a temperate marine one, (Köppen: "Cfb ") is generally moderate all year round, and rarely sees temperatures above 25 °C or below 4 °C. The hottest recorded temperature is 31.1 °C, while -1.9 °C is the coldest. The city is notorious for its southerly blasts in winter, which may make the temperature feel much colder. It is generally very windy all year round with high rainfall; average annual rainfall is 1249 mm, June and July being the wettest months. Frosts are quite common in the hill suburbs and the Hutt Valley between May and September. Snow is very rare at low altitudes, although snow fell on the city and many other parts of the Wellington region during separate events in July and August 2011.
Earthquakes.
Wellington suffered serious damage in a series of earthquakes in 1848 and from another earthquake in 1855. The 1855 Wairarapa earthquake occurred on the Wairarapa Fault to the north and east of Wellington. It was probably the most powerful earthquake in recorded New Zealand history, with an estimated magnitude of at least 8.2 on the Moment magnitude scale. It caused vertical movements of two to three metres over a large area, including raising land out of the harbour and turning it into a tidal swamp. Much of this land was subsequently reclaimed and is now part of the central business district. For this reason the street named Lambton Quay is 100 to 200 metres (325 to 650 ft) from the harbour – plaques set into the footpath mark the shoreline in 1840, indicating the extent of reclamation.
The area has high seismic activity even by New Zealand standards, with a major fault line running through the centre of the city, and several others nearby. Several hundred minor fault lines have been identified within the urban area. Inhabitants, particularly in high-rise buildings, typically notice several earthquakes every year. For many years after the 1855 earthquake, the majority of buildings were made entirely from wood. The 1996-restored Government Buildings near Parliament is the largest wooden building in the Southern Hemisphere. While masonry and structural steel have subsequently been used in building construction, especially for office buildings, timber framing remains the primary structural component of almost all residential construction. Residents place their confidence in good building regulations, which became more stringent in the 20th century.
Since the Canterbury earthquakes of 2010 and 2011, earthquake readiness has become even more of an issue, with buildings declared by Wellington City Council to be earthquake-prone, and the costs of meeting new standards.
Every five years a year-long slow quake occurs beneath Wellington, stretching from Kapiti to the Marlborough Sounds. It was first measured in 2003, and reappeared in 2008 and 2013. It releases as much energy as a magnitude 7 quake, but as it happens slowly there is no damage. During July 2013 there were many earthquakes, mostly in Cook Strait near Seddon. On 21 July 2013 a magnitude 6.5 earthquake hit the city, but no tsunami report was confirmed nor any major damage. On 16 August 2013 at 2:31 pm another earthquake struck, this time magnitude 6.6, but again no major damage occurred, though many buildings were evacuated. On 20 January 2014 at 3:52 pm a rolling 6.2 magnitude earthquake struck the lower North Island 15 km east of Eketahuna and was felt in Wellington, but little damage was reported initially, except at Wellington Airport where one of the two giant eagle sculptures commemorating The Hobbit became detached from the ceiling.
Demographics.
The four cities comprising Wellington had a total population of (June 2014 estimate), and the urban area contained 99% of that population. The remaining areas are largely mountainous and sparsely farmed or parkland and are outside the urban area boundary. More than most cities, life is dominated by its central business district (CBD). Approximately 62,000 people work in the CBD, only 4,000 fewer than work in Auckland's CBD, despite that city having four times the population.
Another major population area is the Kapiti Coast, north of Porirua and including the towns of Paraparaumu, Waikanae, Raumati and Otaki. The population was . The beach and garden zones of these townships attract life-stylers and retired people: 24.6% were aged 65+ as at : "See" Waikanae River and Otaki Beach."
Counts from the 2013 census gave totals by area, gender, and age. Wellington City had the largest population of the four cities with 190,956 people, followed by Lower Hutt, Porirua and Upper Hutt. Women outnumbered men in all four areas.
Source: (2013 Census)
An increasing number of Wellingtonians profess no religious belief, with the most recent census in 2013 showing 44% in that category. The largest religious group was Christians at 39%. The latter figure represented a significant decline from seven years earlier at the previous census, when over 50% of the population identified as Christian.
At the 2013 Census, just over 27% of Wellington's population was born overseas. The most common overseas birthplace is the United Kingdom, place of origin of 7.1% of the urban area's population. The next most-common countries of origin were Samoa (2.0%), India (1.8%), China (1.7%), Australia (1.6%), the Philippines (1.2%), South Africa (1.1%), Fiji (1.0%), the United States (0.8%) and Malaysia (0.6%).
Age distribution.
Age distributions for the four cities are given (see table below). The age structure closely matches the national distribution.
The relative lack of older people in Wellington is less marked when Kapiti Coast District is included - nearly 7% of Kapiti Coast residents are over 80.
Source:Statistics New Zealand (2006 Census)
Architecture.
Wellington showcases a variety of architectural styles from the past 150 years – 19th-century wooden cottages, such as the Italianate Katherine Mansfield Birthplace in Thorndon; streamlined Art Deco structures such as the old Wellington Free Ambulance headquarters, the Central Fire Station, Fountain Court Apartments, the City Gallery, and the former Post and Telegraph Building; and the curves and vibrant colours of post-modern architecture in the CBD.
The oldest building is the 1858 Colonial Cottage in Mount Cook. The tallest building is the Majestic Centre on Willis Street at 116 metres high, the second tallest being the structural expressionist State Insurance Building at 103 metres.
For a full list see: List of tallest buildings in Wellington.
Futuna Chapel in Karori was the first bicultural building in New Zealand, and is considered one of the most significant New Zealand buildings of the 20th century.
Old St Paul's is an example of 19th-century Gothic Revival architecture adapted to colonial conditions and materials, as is St Mary of the Angels. Sacred Heart Cathedral is a Palladian Revival Basilica with the Portico of a Roman or Greek temple. The Museum of Wellington City & Sea in the Bond Store is in the Second French Empire style, and the Wellington Harbour Board Wharf Office Building is in a late English Classical style. There are several restored theatre buildings: the St James Theatre, the Opera House and the Embassy Theatre.
Civic Square is surrounded by the Town Hall and council offices, the Michael Fowler Centre, the Wellington Central Library, Capital E (home of the National Theatre for Children), the City-to-Sea Bridge, and the City Gallery.
As the capital city, there are many notable government buildings. The conical Executive Wing of New Zealand Parliament Buildings, on the corner of Lambton Quay and Molesworth Street, was constructed between 1969 and 1981 and is commonly referred to as the Beehive. Across the road is the largest wooden building in the Southern Hemisphere, part of the old Government Buildings which now houses part of Victoria University of Wellington's Law Faculty.
The Museum of New Zealand Te Papa Tongarewa is on the waterfront.
Other notable buildings include Wellington Town Hall, Wellington Railway Station, Dominion Museum (now Massey University), State Insurance Building, Westpac Stadium, and Wellington Airport at Rongotai. Leading architects include Frederick Thatcher, Frederick de Jersey Clere, W. Gray Young, Bill Alington, Ian Athfield, Roger Walker and Pynenburg and Collins.
Wellington contains many iconic sculptures and structures, such as the Bucket Fountain in Cuba Street and "Invisible City" by Anton Parsons on Lambton Quay. Kinetic sculptures have been commissioned, such as the Zephyrometer. This 26-metre orange spike built for movement by artist Phil Price has been described as "tall, soaring and elegantly simple", which "reflects the swaying of the yacht masts in the Evans Bay Marina behind it" and "moves like the needle on the dial of a nautical instrument, measuring the speed of the sea or wind or vessel."
Housing and real estate.
Wellington experienced a real estate boom in the early 2000s and the effects of the international property bust at the start of 2007. In 2005, the market was described as "robust". By 2008, property values had declined by about 9.3% over a 12-month period, according to one estimate. More expensive properties declined more steeply, sometimes by as much as 20%. "From 2004 to early 2007, rental yields were eroded and positive cash flow property investments disappeared as house values climbed faster than rents. Then that trend reversed and yields slowly began improving," according to two "New Zealand Herald" reporters writing in May 2009. In the middle of 2009 house prices had dropped, interest rates were low, and buy-to-let property investment was again looking attractive, particularly in the Lambton precinct, according to these two reporters.
A Wellington City Council survey conducted in March 2009 found the typical central city apartment dweller was a New Zealand native aged 24 to 35 with a professional job in the downtown area, with household income higher than surrounding areas. Three quarters (73%) walked to work or university, 13% travelled by car, 6% by bus, 2% bicycled (although 31% own bicycles), and did not travel very far since 73% worked or studied in the central city. The large majority (88%) did not have children in their apartments; 39% were couples without children; 32% were single-person households; 15% were groups of people flatting together. Most (56%) owned their apartment; 42% rented (of renters, 16% paid NZ$351 to NZ$450 per week, 13% paid less and 15% paid more – only 3% paid more than NZ$651 per week). The report continued: "The four most important reasons for living in an apartment were given as lifestyle and city living (23%), close to work (20%), close to shops and cafes (11%) and low maintenance (11%) ... City noise and noise from neighbours were the main turnoffs for apartment dwellers (27%), followed by a lack of outdoor space (17%), living close to neighbours (9%) and apartment size and a lack of storage space (8%)."
Households are primarily one-family, making up 66.9% of households, followed by single-person households (24.7%); there were fewer multiperson households and even fewer households containing two or more families. These counts are from the 2013 census for the Wellington region (which includes the surrounding area in addition to the four cities).
Economy.
The Government sector has long been a mainstay of the economy, which has typically risen and fallen with it. Traditionally, its central location meant it was the location of many head offices of various sectors – particularly finance, technology and heavy industry – many of which have since relocated to Auckland following economic deregulation and privatisation.
In recent years, tourism, arts and culture, film, and ICT have played a bigger role in the economy. Wellington's median income is well above the average in New Zealand, and the highest of all New Zealand cities. It has a much higher proportion of people with tertiary qualifications than the national average. Major companies headquartered in Wellington include:
Tourism.
Tourism is a major contributor to the economy, injecting approximately NZ$1.3 billion into the region annually and accounting for 9% of total FTE employment. The city is consistently named as New Zealanders’ favourite destination in the quarterly FlyBuys Colmar Brunton Mood of the Traveller survey and it was fourth in Lonely Planet Best in Travel 2011’s Top 10 Cities to Visit in 2011. New Zealanders make up the largest visitor market, with 3.6 million visits each year. New Zealand visitors spend on average NZ$2.4 million a day. There are approximately 540,000 international visitors each year, who spend 3.7 million nights and NZ$436 million. The largest international visitor market is Australia, with over 210,000 visitors spending approximately NZ$334 million annually.
Wellington is marketed as the 'coolest little capital in the world' by Positively Wellington Tourism, an award-winning regional tourism organisation set up as a council controlled organisation by Wellington City Council in 1997. The organisation’s council funding comes through the Downtown Levy commercial rate. In the decade to 2010, the city saw growth of over 60% in commercial guest nights. It has been promoted through a variety of campaigns and taglines, starting with the iconic Absolutely Positively Wellington advertisements. The long-term domestic marketing strategy was a finalist in the 2011 CAANZ Media Awards.
Popular tourist attractions include Museum of Wellington City & Sea, Wellington Zoo, Zealandia and Wellington Cable Car. Cruise tourism is experiencing a major boom in line with nationwide development. The 2010/11 season saw 125,000 passengers and crew visit on 60 liners. There were 80 vessels booked for visits in the 2011/12 season – estimated to inject more than NZ$31 million into the economy and representing a 74% increase in the space of two years.
Wellington is a popular conference tourism destination due to its compact nature, cultural attractions, award-winning restaurants and access to government agencies. In the year ending March 2011, there were 6495 conference events involving nearly 800,000 delegate days; this injected approximately NZ$100 million into the economy.
Arts and culture.
Museums and cultural institutions.
Wellington is home to Te Papa (the Museum of New Zealand), the National Library of New Zealand, Archives New Zealand, the Museum of Wellington City & Sea, the Katherine Mansfield Birthplace Museum, Colonial Cottage, the New Zealand Cricket Museum, the Cable Car Museum, Old St Paul's, and the Wellington City Art Gallery.
Festivals.
Wellington is home to many high-profile events and cultural celebrations, including the biennial New Zealand International Arts Festival, biennial Wellington Jazz Festival, biennial Capital E National Arts Festival for Children and major events such as Brancott Estate World of Wearable Art, Cuba Street Carnival, Visa Wellington On a Plate, New Zealand Fringe Festival, New Zealand International Comedy Festival (also hosted in Auckland), Summer City, The Wellington Folk Festival (in Wainuiomata), New Zealand Affordable Art Show, the New Zealand Sevens Weekend and Parade, Out in the Square, Vodafone Homegrown, the Couch Soup theatre festival, Camp A Low Hum and numerous film festivals.
The annual children's Artsplash Festival brings together hundreds of students from across the region. The week-long festival includes music and dance performances and the presentation of visual arts.
Film.
Filmmakers Sir Peter Jackson, Sir Richard Taylor and a growing team of creative professionals have turned the eastern suburb of Miramar into a film-making, post-production and special effects infrastructure centre, giving rise to the moniker 'Wellywood'. Jackson's companies include Weta Workshop, Weta Digital, Camperdown Studios, post-production house Park Road Post, and Stone Street Studios near Wellington Airport. Recent films shot partly or wholly in Wellington include the Lord of The Rings trilogy, King Kong and Avatar. Jackson described Wellington: "Well, it's windy. But it's actually a lovely place, where you're pretty much surrounded by water and the bay. The city itself is quite small, but the surrounding areas are very reminiscent of the hills up in northern California, like Marin County near San Francisco and the Bay Area climate and some of the architecture. Kind of a cross between that and Hawaii."
Sometime Wellington directors Jane Campion and Geoff Murphy have reached the world's screens with their independent spirit. Emerging Kiwi film-makers, like Robert Sarkies, Taika Waititi, Costa Botes and Jennifer Bush-Daumec, are extending the Wellington-based lineage and cinematic scope. There are agencies to assist film-makers with tasks such as securing permits and scouting locations.
Wellington has a large number of independent cinemas, including , , the Roxy and , which participate in film festivals throughout the year. Wellington has one of the country's highest turn-outs for the annual "New Zealand International Film Festival".
Music.
The music scene has produced bands such as The Warratahs, The Phoenix Foundation, Shihad, Beastwars, Fly My Pretties, Rhian Sheehan, Birchville Cat Motel, Black Boned Angel, Fat Freddy's Drop, The Black Seeds, Fur Patrol, Flight of the Conchords, Connan and the Mockasins, Rhombus and Module. The New Zealand School of Music was established in 2005 through a merger of the conservatory and theory programmes at Massey University and Victoria University of Wellington. New Zealand Symphony Orchestra, Nevine String Quartet and Chamber Music New Zealand are based in Wellington. The city is also home to the New Zealand Symphony Orchestra and the Internationally renowned men's A Cappella chorus Vocal FX.
Theatre and the dramatic arts.
Wellington is home to Downstage Theatre, BATS Theatre, Circa Theatre, the National Maori Theatre company Taki Rua, Whitireia Performance Centre, National Dance & Drama School Toi Whakaari and the National Theatre for Children at Capital E in Civic Square.
Wellington is home to groups that perform Improvised Theatre and Improvisational comedy, including Wellington Improvisation Troupe (WIT) an Improvisors and youth group, Joe Improv. Te Whaea National Dance & Drama Centre, houses New Zealand's University-level school of Dance and Drama, Toi Whakaari: NZ Drama School & New Zealand School of Dance, and Whitiriea Performing Arts Centre. These are separate entities that share the building's facilities.
St James' Theatre on Courtenay Place is a popular venue for artistic performances.
Dance.
Wellington is the home for the Royal New Zealand Ballet, the New Zealand School of Dance and contemporary dance company Footnote.
Comedy.
Many of New Zealand's prominent comedians have either come from Wellington or got their start there, such as Ginette McDonald ("Lynn of Tawa"), Raybon Kan, Dai Henwood, Ben Hurley, Steve Wrigley, Guy Williams, the Flight of the Conchords and the satirist John Clarke ("Fred Dagg").
The comedy group Breaking the 5th Wall operated out of Wellington and regularly did shows around the city, performing a mix of sketch comedy and semi-improvised theatre. In 2012 the group disbanded when some of its members moved to Australia.
Wellington is home to groups that perform improvised theatre and improvisational comedy, including Wellington Improvisation Troupe (WIT), The Improvisors and youth group Joe Improv.
Wellington hosts shows in the annual New Zealand International Comedy Festival. The NZ International Comedy Fest 2010 featured over 250 local and international comedy acts and was a first in incorporating an iPhone application for the Festival.
Visual arts.
From 1936 to 1992 Wellington was home to the National Art Gallery of New Zealand, when it was amalgamated into Museum of New Zealand Te Papa Tongarewa. Wellington is home to the New Zealand Academy of Fine Arts and the Arts Foundation of New Zealand. The city's arts centre, Toi Poneke, is a nexus of creative projects, collaborations, and multi-disciplinary production. Arts Programmes and Services Manager Eric Vaughn Holowacz and a small team based in the Abel Smith Street facility have produced ambitious initiatives such as Opening Notes, Drive by Art, and public art projects. The city is home to experimental arts publication "White Fungus". The Learning Connexion provides art classes. Other visual art galleries include the .
Cuisine.
Café culture is prominent. Restaurants are either licensed to sell alcohol, BYO (bring your own), or unlicensed (no alcohol). Restaurants offer cuisines including from Europe, Asia and Polynesia. "For dishes that have a distinctly New Zealand style, there are lamb, pork and cervena (venison), salmon, crayfish (lobster), Bluff oysters, pāua (abalone), mussels, scallops, pipis and tuatua (both New Zealand shellfish); kumara (sweet potato); kiwifruit and tamarillo; and pavlova, the national dessert," recommends one tourism website.
Sport.
Wellington is the home to:
Sporting events include:
Education.
Wellington offers a variety of college and university programs for students.
Victoria University of Wellington has four campuses and works with a three-trimester system (beginning March, July, and November). It enrolled 21,380 students in 2008; of these, 16,609 were full-time students. Of all students, 56% were female and 44% male. While the student body was primarily New Zealanders of European descent, 1,713 were Maori, 1,024 were Pacific students, 2,765 were international students. 5,751 degrees, diplomas and certificates were awarded. The university has 1,930 full-time employees.
Massey University has a Wellington campus known as the "creative campus" and offers courses in communication and business, engineering and technology, health and well-being, and creative arts. Its school of design was established in 1886, and has research centres for studying public health, sleep, Maori health, small & medium enterprises, disasters, and tertiary teaching excellence. It combined with Victoria University to create the New Zealand School of Music.
The University of Otago has a Wellington branch with its Wellington School of Medicine and Health.
Whitireia New Zealand has large campuses in Porirua, Wellington and Kapiti; the Wellington Institute of Technology and New Zealand's National Drama school, Toi Whakaari. For further information, see List of universities in New Zealand. The Wellington area has numerous primary and secondary schools.
Transport.
Wellington is served by State Highway 1 in the west and State Highway 2 in the east, meeting at the Ngauranga Interchange north of the city centre, where SH 1 runs through the city to the airport. Road access into the capital is lower in grade than most other cities in New Zealand – between Wellington and the Kapiti Coast, SH 1 travels along the Centennial Highway, a narrow section of road, and between Wellington and Wairarapa SH 2 transverses the Rimutaka Ranges on a similar narrow accident-prone road. Wellington has two short motorways, both part of SH 1: the Johnsonville–Porirua Motorway and the Wellington Urban Motorway, which in combination with a small non-motorway section in the Ngauranga Gorge connect Porirua with Wellington city.
Bus transport in Wellington is supplied by several different operators under the banner of Metlink. Buses serve almost every part of Wellington city, with most of them running along the "Golden Mile" from Wellington Railway Station to Courtenay Place. Most of the buses run on diesel, but nine routes use trolleybuses – the only remaining public system in Oceania.
Wellington lies at the southern end of the North Island Main Trunk Railway (NIMT) and the Wairarapa Line, converging on Wellington Railway Station at the northern end of central Wellington. Two long-distance services leave from Wellington: the Capital Connection, for commuters from Palmerston North, and the Northern Explorer to Auckland.
Four electrified suburban lines radiate from Wellington Railway Station to the outer suburbs – the Johnsonville Line through the hillside suburbs north of central Wellington; the Kapiti Line along the NIMT to Waikanae on the Kapiti Coast via Porirua and Paraparaumu; the Melling Line to Lower Hutt via Petone; and the Hutt Valley Line along the Wairarapa Line via Waterloo and Taita to Upper Hutt. A diesel-hauled carriage service, the Wairarapa Connection, connects several times daily to Masterton in the Wairarapa via the 8.8 km long Rimutaka Tunnel. Combined, these five services carry 11.64 million passengers per year.
Wellington is the North Island port for Cook Strait ferries to Picton in the South Island, provided by state-owned Interislander and private Bluebridge. Local ferries connect Wellington city centre with Eastbourne, Seatoun and Petone.
Wellington International Airport is 6 km south-east of the city centre. It is serviced by flights from across New Zealand, and to Australia. Flights to other international destinations require a transfer at another airport, as larger aircraft cannot use Wellington's short (2081 m) runway, which has become an issue in recent years in regards to the Wellington region's economic performance. The airport is the base for Wellington Aero Club, a private not-for-profit flight school.
Infrastructure.
Electric power.
The maximum electricity demand is forecast to grow on average by 1.4% annually over the next 15 years, from 756 MW in 2012 to 934 MW by 2027, slightly lower than the national average demand growth of 1.7% per annum. The largest source of generation in the region is Meridian Energy's West Wind wind farm, with a maximum output of 143 MW. It is a few kilometres west of Wellington's central business district, on Quartz Hill and Terawhiti Station. There are some other small generators in the region, but the total peak generation is only 165 MW.
Peak demand greatly exceeds local generation, and power supply is highly dependent on the National Grid operated by Transpower. Four 220 kV transmission circuits from Bunnythorpe, near Palmerston North, provide the main connections with the national grid. The region is also supplied by the North Island terminal of the HVDC link at Haywards substation, on State Highway 58 above the Hutt Valley. A major upgrade of the HVDC link commissioned in 2013 increased the capacity of the link from 700 MW to 1,000 MW from 2012, and 1,200 MW from 2014.
The local power distribution network is owned and managed by Wellington Electricity.
The main power supplies to the central business district come from Transpower grid exit point substations at Central Park and Wilton. The Central Park substation is the largest grid exit point in the region, with a peak demand of over 170 MW (forecast to grow to 200 MW by 2020). There are constraints and limitations with this substation, and alternative investment solutions are being developed to improve security of supply.
Strong winds, advantageous for wind farms, have been known to damage power lines. In May 2009, one windstorm left about 2500 residents without power for a several hours. Lightning strikes and occasional faults in the electric power system sometimes cause power outages.

</doc>
<doc id="33879" url="http://en.wikipedia.org/wiki?curid=33879" title="Windows XP">
Windows XP

Windows XP (codenamed Whistler) is a personal computer operating system produced by Microsoft as part of the Windows NT family of operating systems. The operating system was released to manufacturing on August 24, 2001, and generally released for retail sale on October 25, 2001.
Development of XP began in the late 1990s as "Neptune", an operating system built on the Windows NT kernel which was intended specifically for mainstream consumer use—an updated version of Windows 2000 was also originally planned for the business market. However, in January 2000, both projects were shelved in favor of a single OS codenamed "Whistler", which would serve as a single OS platform for both consumer and business markets. Windows XP was a major advance from the MS-DOS based versions of Windows in security, stability and efficiency due to its use of Windows NT underpinnings. It introduced a significantly redesigned graphical user interface and was the first version of Windows to use product activation in an effort to reduce software piracy.
Upon its release Windows XP received generally positive reviews, with critics noting increased performance (especially in comparison to Windows ME), a more intuitive user interface, improved hardware support, and its expanded multimedia capabilities. Despite some initial concerns over the new licensing model and product activation system, Windows XP eventually proved to be popular and widely used. It is estimated that at least 400 million copies of Windows XP were sold globally within its first five years of availability, and at least one billion copies were sold by April 2014.
Windows XP remained popular even after the release of newer versions, particularly due to the poorly received release of its successor Windows Vista. Vista's 2009 successor, Windows 7, only overtook XP in total market share at the end of 2011.
Sales of Windows XP licenses to original equipment manufacturers (OEMs) ceased on June 30, 2008, but continued for netbooks until October 2010. Extended support for Windows XP ended on April 8, 2014, after which the operating system ceased receiving further support or security updates to most users.
Development.
As "Neptune" and "Odyssey".
In the late 1990s, initial development of what would become Windows XP was focused on two individual products; "Odyssey", which was reportedly intended to succeed the future Windows 2000, and "Neptune", which was intended to succeed the MS-DOS-based Windows 98 with a Windows NT-based product designed for consumers. Based on 2000's NT 5.0 kernel, Neptune primarily focused on offering a simplified, task-based interface based around a concept known internally as "activity centers". A number of activity centers were planned, serving as hubs for communications (i.e. email), playing music, managing or viewing photos, searching the internet, and viewing recently used content. A single build of Neptune, 5111 (which was otherwise based on, and still carried the branding of Windows 2000 in places), revealed early work on the activity center concept, with an updated user account interface and graphical login screen, common functions (such as recently used programs) being accessible from a customizable "Starting Places" page (which could be used as either a separate window, or a full-screen desktop replacement).
However, the project, at its current state, proved to be too ambitious. Microsoft would ultimately shelve Bill Gates' 1998 promise that Windows 98 would be the final MS-DOS based version of Windows; at the WinHEC conference on April 7, 1999, Steve Ballmer announced an updated version of 98 known as Windows Millennium. Microsoft also planned to push back Neptune in favor of an interim, but consumer-oriented NT-based OS codenamed "Asteroid". Concepts introduced by Neptune would influence future Windows products; in Windows ME, the activity center concept was used for System Restore and Help and Support (which both combined Win32 code with an interface rendered using Internet Explorer's layout engine), the hub concept would be expanded on Windows Phone, and Windows 8 would similarly use a simplified user interface running atop the existing Windows shell.
As "Whistler".
In January 2000, shortly prior to the official release of Windows 2000, technology writer Paul Thurrott reported that Microsoft had shelved both Neptune and Odyssey in favor of a new product codenamed Whistler, after Whistler, British Columbia, as many Microsoft employees skied at the Whistler-Blackcomb ski resort. The goal of Whistler was to unify both the consumer and business-oriented Windows lines under a single, Windows NT platform, further stating that: "Neptune became a black hole when all the features that were cut from [Windows ME] were simply re-tagged as Neptune features. And since Neptune and Odyssey would be based on the same code-base anyway, it made sense to combine them into a single project". At WinHEC in April 2000, Microsoft officially announced and presented an early build of Whistler, focusing on a new modularized architecture, built-in CD burning, fast user switching, and updated versions of the digital media features introduced by ME. Windows general manager Carl Stork stated that Whistler would be released in both consumer- and business-oriented versions built atop the same architecture, and that there were plans to update the Windows interface to make it "warmer and more friendly".
In June 2000, Microsoft began the technical beta testing process; Whistler was expected to be made available in "Personal", "Professional", "Server", "Advanced Server", and "Datacenter" editions. At PDC on July 13, 2000, Microsoft announced that Whistler would be released during the second half of 2001, and also released the first preview build, 2250. The build notably introduced an early version of a new visual styles system along with an interim theme known as "Professional" (later renamed "Watercolor"), and contained a hidden "Start page" (a full-screen page similar to Neptune's "Starting Places"), and a hidden, early version of a two-column Start menu design. Build 2257 featured further refinements to the Watercolor theme, along with the official introduction of the two-column Start menu, and the addition of an early version of Windows Firewall.
Beta releases.
Microsoft released Whistler Beta 1, build 2296, on October 31, 2000. Build 2410 in January 2001 introduced Internet Explorer 6.0 (previously branded as 5.6) and the Windows Product Activation system. Bill Gates dedicated a portion of his keynote at Consumer Electronics Show to discuss Whistler, explaining that the OS would bring "[the] dependability of our highest end corporate desktop, and total dependability, to the home", but also "move it in the direction of making it very consumer-oriented. Making it very friendly for the home user to use." Alongside Beta 1, it was also announced that Microsoft would prioritize the release of the consumer-oriented versions of Whistler over the server-oriented versions in order to gauge reaction, but that they would be both generally available during the second half of 2001 (Whistler Server would ultimately be delayed into 2003). Builds 2416 and 2419 added the File and Transfer Settings Wizard and began to introduce elements of the operating system's final appearance (such as its near-final Windows Setup design, and the addition of new default wallpapers, such as "Bliss").
On February 5, 2001, Microsoft officially announced that Whistler would be known as Windows XP, short for "experience". As a complement, the next version of Microsoft Office was also announced as Office XP. Microsoft stated that the name "[symbolizes] the rich and extended user experiences Windows and Office can offer by embracing Web services that span a broad range of devices." In a press event at EMP Museum in Seattle on February 13, 2001, Microsoft publicly unveiled the new "Luna" user interface of Windows XP. Windows XP Beta 2, build 2462a (which among other improvements, introduced the Luna style), was launched at WinHEC on March 25, 2001.
In April 2001, Microsoft controversially announced that XP would not integrate support for Bluetooth or USB 2.0 on launch, requiring the use of third-party drivers. Critics felt that in the case of the latter, Microsoft's decision had delivered a potential blow to the adoption of USB 2.0, as XP was to provide support for the competing, Apple-developed FireWire standard instead. A representative stated that the company had "[recognized] the importance of USB 2.0 as a newly emerging standard and is evaluating the best mechanism for making it available to Windows XP users after the initial release." The builds prior to and following Release Candidate 1 (build 2505), released on July 5, 2001, and Release Candidate 2 (build 2526, released on July 27, 2001), focused on fixing bugs, acknowledging user feedback, and other final tweaks before the RTM build.
RTM and release.
On August 24, 2001, Windows XP build 2600 was released to manufacturing. During a ceremonial media event at Microsoft Redmond Campus, copies of the RTM build were given to representatives of several major PC manufacturers in briefcases, who then flew off on XP-branded helicopters. While PC manufacturers would be able to release devices running XP beginning on September 24, 2001, XP was expected to reach general, retail availability on October 25, 2001. On the same day, Microsoft also announced the final retail pricing of XP's two main editions, "Home" and "Professional".
In June 2001, Microsoft indicated that it was planning to, in conjunction with Intel and other PC makers, spend at least US$1 billion on marketing and promoting Windows XP, among these efforts included a U.S. television commercial featuring Madonna's song "Ray of Light".
New and updated features.
User interface.
While retaining some similarities to previous versions, Windows XP's interface was overhauled with a new visual appearance, with an increased use of alpha compositing effects, drop shadows, and "visual styles", which completely change the appearance of the operating system. The amount of effects enabled are determined by the operating system by the computer's processing power, and can be enabled or disabled on a case-by-case basis. XP also added ClearType, a new subpixel rendering system designed to improve the appearance of fonts on LCD displays. A new set of system icons were also introduced. The default wallpaper, "Bliss", is a photo of a landscape in the Napa Valley outside Napa, California, with rolling green hills and a blue sky with stratocumulus and cirrus clouds.
The Start menu received its first major overhaul on XP, switching to a two-column layout with the ability to list, pin, and display frequently used applications, recently opened documents, and the traditional cascading "All Programs" menu. The taskbar can now group windows opened by a single application into one taskbar button, with a popup menu listing the individual windows. The notification area also hides "inactive" icons by default. The taskbar can also be "locked" to prevent accidental moving or other changes. A "common tasks" list was added Windows Explorer's sidebar was updated to use a new task-based designs with lists of common actions; the tasks displayed are contextually relevant to the type of content in a folder (i.e. a folder with music displays offers to play all the files in the folder, or burn them to a CD).
The "task grouping" feature introduced in Windows XP showing both grouped and individual items
Fast user switching allows additional users to log into a Windows XP machine without existing users having to close their programs and logging out. Although only one user at the time can use the console (i.e. monitor, keyboard and mouse), previous users can resume their session once they regained control of the console.
Infrastructure.
In an effort to prevent software piracy, XP also introduced Windows Product Activation, which requires that each Windows license be "activated" and tied to a unique ID generated using information from the computer hardware.
Windows XP uses prefetcher to improve startup and application launch times. It also became possible to revert the installation of an updated device driver, should one not produce desirable results.
Numerous improvements were also made to system administration tools such as Windows Installer, Windows Script Host, Disk Defragmenter, Windows Task Manager, Group Policy, CHKDSK, NTBackup, Microsoft Management Console, Shadow Copy, Registry Editor, Sysprep and WMI.
Networking and internet functionality.
Windows XP was originally bundled with Internet Explorer 6, Outlook Express 6, Windows Messenger, and MSN Explorer. New networking features were also added to XP, including Internet Connection Firewall, Internet Connection Sharing integration with UPnP, NAT traversal APIs, Quality of Service features, IPv6 and Teredo tunneling, Background Intelligent Transfer Service, extended fax features, network bridging, peer to peer networking, support for most DSL modems, IEEE 802.11 (Wi-Fi) connections with auto configuration and roaming, TAPI 3.1, and networking over FireWire. Remote Assistance and Remote Desktop were also added, which allow users to connect to a computer running Windows XP from across a network or the Internet and access their applications, files, printers, and devices or request help. Improvements were also made to "IntelliMirror" features such as Offline Files, Roaming user profiles and Folder redirection.
Other features.
Users in British schools observed the improved ease of use and advanced capabilities—comparing the former to RISC OS and Mac OS, and the latter to Unix.
Removed features.
Some of the programs and features that were part of the previous versions of Windows did not make it to Windows XP. CD Player, DVD Player and Imaging for Windows are removed as Windows Picture and Fax Viewer, Windows Media Player and Windows shell take over their duties. NetBEUI and NetDDE are deprecated and are not installed by default. DLC and AppleTalk network protocols are removed. Plug-and-play–incompatible communication devices (like modems and network interface cards) are no longer supported.
Service Pack 2 and Service Pack 3 also remove features from Windows XP but to a less noticeable extent. For instance, Program Manager and support for TCP half-open connections are removed in Service Pack 2. Energy Star logo and the address bar on taskbar are removed in Service Pack 3.
Editions.
Windows XP was released in two major editions on launch; "Home Edition", and "Professional". Both editions were made available at retail as pre-loaded software on new computers, and in boxed copies. Boxed copies were sold as "Upgrade" or "Full" licenses; the "Upgrade" versions were slightly cheaper, but require an existing version of Windows to install. The "Full" version could be installed on systems without an operating system or existing version of Windows. Both versions of XP were aimed towards different markets; "Home Edition" is explicitly intended for consumer use and disables or removes certain advanced and enterprise-oriented features present on "Professional", such as the ability to join a Windows domain, Internet Information Services, and Multilingual User Interface. Additionally, users could not directly upgrade to XP Home Edition from Windows NT 4.0 or 2000, although users could upgrade to either variant of XP from Windows 98 or ME. Windows' software license agreement for pre-loaded licenses allows the software to be "returned" to the OEM for a refund if the user does not wish to use it. Despite the refusal of some manufacturers to honor the entitlement, it has been enforced by courts in some countries.
Two specialized variants of XP were introduced in 2002 for certain types of hardware, exclusively through OEM channels as pre-loaded software. "Windows XP Media Center Edition" was initially designed for high-end home theater PCs with TV tuners (marketed under the term "Media Center PC"), offering expanded multimedia functionality, an electronic program guide, and digital video recorder (DVR) support through the Windows Media Center application. Microsoft also unveiled "Windows XP Tablet PC Edition", which contains additional pen input features, and is optimized for mobile devices meeting its Tablet PC specifications. Two different 64-bit editions of XP were made available; the first, "Windows XP 64-Bit Edition", was intended for IA-64 (Itanium) systems; as IA-64 usage declined on workstations in favor of AMD's x86-64 architecture (which was supported by the later "Windows XP Professional x64 Edition"), the Itanium version was discontinued in 2005.
Microsoft also targeted emerging markets with the 2004 introduction of "Windows XP Starter Edition", a special variant of "Home Edition" intended for low-cost PC's. The OS is primarily aimed at first-time computer owners (particularly in developing countries); containing heavy localization (including wallpapers and screen savers incorporating images of local landmarks), and a "My Support" area which contains video tutorials on basic computing tasks. It also removes certain "complex" features, and does not allow users to run more than three applications at a time. After a pilot program in India and Thailand, "Starter" was released in other emerging markets throughout 2005. In 2006, Microsoft also unveiled the FlexGo initiative, which would also target emerging markets with subsidized PCs on a pre-paid, subscription basis.
As the result of unfair competition lawsuits in Europe and South Korea, which both alleged that Microsoft had improperly leveraged its status in the PC market to favor its own software, Microsoft was forced to release special versions of XP in these markets that excluded certain applications. In March 2004, after the European Commission fined Microsoft €497 million (US$603 million), Microsoft was forced to release "N" versions of XP that excluded Windows Media Player, encouraging users to pick and download their own media player software. As it is sold at the same price as the version with Windows Media Player included, certain OEMs (such as Dell, who offered it for a short period, along with Hewlett-Packard, Lenovo and Fujitsu Siemens) chose not to offer it. Consumer interest has been low, with roughly 1,500 units shipped to OEMs, and no reported sales to consumers. In December 2005, the Korean Fair Trade Commission ordered Microsoft to make available editions of Windows XP and Windows Server 2003 that do not contain Windows Media Player or Windows Messenger. The "K" and "KN" editions of Windows XP were released in August 2006, and are only available in English and Korean, and also contain links to third-party instant messenger and media player software.
Service packs.
Three service packs were released for Windows XP, containing various bug fixes and the addition of certain features. Each service pack is a superset of all previous service packs and patches so that only the latest service pack needs to be installed, and also includes new revisions.
Service Pack 1.
Service Pack 1 (SP1) for Windows XP was released on September 9, 2002. It contained over 300 minor, post-RTM bug fixes, along with all security patches released since the original release of XP. SP1 also added USB 2.0 support, Microsoft Java Virtual Machine, .NET Framework support, and support for technologies used by the then-upcoming "Media Center" and "Tablet PC" editions of XP. The most significant change on SP1 was the addition of "Set Program Access and Defaults", a settings page which allows users and OEMs to set default programs for certain types of activities (such as media players or web browsers) and disable access to bundled, Microsoft programs (such as Internet Explorer or Windows Media Player). This was added as part of the company's settlement in "United States v. Microsoft Corp.", which stated that Microsoft would allow users to "enable or remove access to each Microsoft Middleware Product or Non-Microsoft Middleware Product by displaying or removing icons, shortcuts, or menu entries on the desktop or Start menu, or anywhere else in a Windows Operating System Product where a list of icons, shortcuts, or menu entries for applications are generally displayed."
On February 3, 2003, Microsoft released Service Pack 1a (SP1a). This release removed Microsoft Java Virtual Machine as a result of a lawsuit with Sun Microsystems.
Service Pack 2.
Service Pack 2 (SP2) was released on August 25, 2004, SP2 added new functionality to Windows XP, such as WPA encryption compatibility and improved Wi-Fi support (with a wizard utility), a pop-up ad blocker for Internet Explorer 6, and partial Bluetooth support.
Service Pack 2 also added new security enhancements (codenamed "Springboard"), which included a major revision to the included firewall (renamed Windows Firewall, and now enabled by default), Data Execution Prevention gained hardware support in the NX bit that can stop some forms of buffer overflow attacks. Raw socket support is removed (which supposedly limits the damage done by zombie machines) and the Windows Messenger service became disabled by default, which was an attack vector for pop-up advertisements to be displayed as system messages without a web browser or any additional software. Additionally, security-related improvements were made to e-mail and web browsing. Service Pack 2 also added Security Center, an interface which provides a general overview of the system's security status, including the state of the firewall and automatic updates. Third-party firewall and antivirus software can also be monitored from Security Center.
In August 2006, Microsoft released updated installation media for Windows XP and Windows Server 2003 SP2 (SP2b) to contain a patch that requires ActiveX controls to require manual activation in accordance with a patent held by Eolas. Microsoft has since licensed the patent, and released a patch reverting the change in April 2008. In September 2007, another minor revision known as SP2c was released for XP Professional, extending the number of available product keys for the operating system to "support the continued availability of Windows XP Professional through the scheduled system builder channel end-of-life (EOL) date of January 31, 2009."
Service Pack 3.
Windows XP Service Pack 3 (SP3) was released to manufacturing on April 21, 2008, and to the public via both the Microsoft Download Center and Windows Update on May 6, 2008.
It began being automatically pushed out to "Automatic Updates" users on July 10, 2008. A feature set overview which details new features available separately as stand-alone updates to Windows XP, as well as backported features from Windows Vista, has been posted by Microsoft. A total of 1,174 fixes have been included in SP3. Service Pack 3 can be installed on systems with Internet Explorer versions 6, 7, or 8.
Internet Explorer 7 and 8 are not included as part of SP3. Service Pack 3 is not available for the 64 bit version of Windows XP, which is based on Windows Server 2003 kernel.
Previously released updates.
Service Pack 3 also incorporated several previously released key updates for Windows XP, which were not included up to SP2, including:
Service Pack 3 contains updates to the operating system components of Windows XP Media Center Edition (MCE) and Windows XP Tablet PC Edition, and security updates for .NET Framework version 1.0, which is included in these editions. However, it does not include update rollups for the Windows Media Center application in Windows XP MCE 2005. SP3 also omits security updates for Windows Media Player 10, although the player is included in Windows XP MCE 2005. The Address Bar DeskBand on the Taskbar is no longer included due to antitrust violation concerns.
System requirements.
System requirements for Windows XP are as follows:
Notes:
</dl>
Physical memory limits.
The maximum amount of RAM that Windows XP can support varies depending on the product edition and the processor architecture, as shown in the following table.
Processor limits.
Windows XP Professional supports up to two physical processors
(CPU sockets);
Windows XP Home Edition is limited to one.
Windows XP supports a greater number of logical processors.
A logical processor is either: 1) One of the two handlers of threads of instructions in one of the cores of a physical processor with support for hyper-threading present and enabled; or 2) one of the cores of one of the physical processors without enabled support for hyper-threading.
Windows XP 32-bit editions support up to 32 logical processors;
64-bit editions support up to 64 logical processors.
Support lifecycle.
Support for Windows XP without a service pack ended on September 30, 2005. Windows XP Service Pack 1 and 1a were retired on October 10, 2006, and Windows XP Service Pack 2 reached end of support on July 13, 2010, almost six years after its general availability. The company stopped general licensing of Windows XP to OEMs and terminated retail sales of the operating system on June 30, 2008, 17 months after the release of Windows Vista. However, an exception was announced on April 3, 2008, for OEMs producing what it defined as "ultra low-cost personal computers", particularly netbooks, until one year after the availability of Windows 7 (October 22, 2010). Analysts felt that the move was primarily intended to combat the threat of Linux-based netbooks, although Microsoft's Kevin Hutz stated that the decision was due to apparent market demand for low-end computers with Windows.
Variants of Windows XP for embedded systems have different support policies: Windows XP Embedded SP3 will be supported until January 2016. Windows Embedded Standard 2009 and Windows Embedded POSReady 2009, will be supported through January and April 2019, respectively.
End of support.
On April 14, 2009, Windows XP exited mainstream support and entered the Extended Support phase; Microsoft continued to provide security updates every month for Windows XP; however, free technical support, warranty claims, and design changes were no longer being offered. Extended support ended on April 8, 2014, over 12 years since the release of XP; normally Microsoft products have a support life cycle of only 10 years. Beyond the final security updates released on April 8, no more security patches or support information are provided for XP free-of-charge; "critical patches" will still be created, and made available only to customers subscribing to a paid "Custom Support" plan. As it is a Windows component, all versions of Internet Explorer for Windows XP also became unsupported.
Microsoft will continue to provide Security Essentials virus definitions and updates for its Malicious Software Removal Tool (MSRT) for XP until July 14, 2015. As the end of extended support approached, Microsoft began to increasingly urge XP customers to migrate to newer versions such as Windows 7 or 8 in the interest of security, suggesting that attackers could reverse engineer security patches for newer versions of Windows and use them to target equivalent vulnerabilities in XP. On March 8, 2014, Microsoft deployed an update for XP that, on the 8th of each month, displays a pop-up notification to remind users about the end of support—these notifications may be disabled by the user. Microsoft also partnered with Laplink to provide a special "express" version of its PCmover software to help users migrate files and settings from XP to a computer with a newer version of Windows.
Despite the approaching end of support, there have still been notable holdouts who have not migrated past XP; many users elected to remain on XP because of the poor reception of Windows Vista, sales of newer PCs with newer versions of Windows declined due to the Great Recession and the effects of Vista, and deployments of new versions of Windows in enterprise environments require a large amount of planning, which includes testing applications for compatibility (especially those that are dependent on Internet Explorer 6, which is not compatible with newer versions of Windows). Major security software vendors (including Microsoft itself) plan to continue offering support and definitions for Windows XP past the end of support to varying extents, along with the developers of Google Chrome, Mozilla Firefox, and Opera web browsers; despite these measures, critics similarly argued that users should eventually migrate from XP to a supported platform.
As of January 2014, at least 49% of all computers in China still ran XP. These holdouts have been influenced by several factors; prices of genuine copies of Windows in the country are high, while Ni Guangnan of the Chinese Academy of Sciences warned that Windows 8 could allegedly expose users to surveillance by the United States government, and the Chinese government would ban the purchase of Windows 8 products for government use in May 2014 in protest of Microsoft's inability to provide "guaranteed" support. The government also had concerns that the impending end of support could affect their anti-piracy initiatives with Microsoft, as users would simply pirate newer versions rather than purchasing them legally. As such, government officials formally requested that Microsoft extend the support period for XP for these reasons. While Microsoft did not comply with their requests, a number of major Chinese software developers, such as Lenovo, Kingsoft and Tencent, will provide free support and resources for Chinese users migrating from XP. Several governments, in particular the Netherlands and the United Kingdom, elected to negotiate "Custom Support" plans with Microsoft for their continued, internal use of Windows XP; the British government's deal lasts for a year, also covers support for Office 2003 (which reached end-of-life the same day) and cost £5.5 million.
In January 2014, it was estimated that more than 95% of the 3 million automated teller machines in the world were still running Windows XP (which largely replaced IBM's OS/2 as the predominant operating system on ATMs); ATMs have an average lifecycle of between seven and ten years, but some have had lifecycles as long as 15. Plans were being made by several ATM vendors and their customers to migrate to Windows 7-based systems over the course of 2014, while vendors have also considered the possibility of using Linux-based platforms in the future to give them more flexibility for support lifecycles. However, ATMs typically run the embedded variant of Windows XP, which is supported through January 2016. Similarly specialized devices that run XP, particularly medical devices, must have any revisions to their software—even security updates for the underlying operating system—approved by relevant regulators before they can be released. For the same reason, manufacturers of medical devices had historically refused to provide, or even allow the installation of any Windows updates for these devices, leaving them open to security exploits and malware.
On May 1, 2014, despite the end of support for the operating system, Microsoft released an emergency patch to correct a major, recently discovered security exploit in the Internet Explorer browser on all versions of Windows (including Windows XP).
Reception.
On release, Windows XP received mostly positive reviews. CNET described the operating system as being "worth the hype", considering the new interface to be "spiffier" and more intuitive than previous versions, but feeling that it may "annoy" experienced users with its "hand-holding". XP's expanded multimedia support and CD burning functionality was also noted, along with its streamlined networking tools. The performance improvements of XP in comparison to 2000 and ME were also praised, along with its increased number of built-in device drivers in comparison to 2000. The software compatibility tools were also praised, although it was noted that some programs, particularly older MS-DOS software, may not work correctly on XP due to its differing architecture. They panned Windows XP's new licensing model and product activation system, considering it to be a "slightly annoying roadblock", but acknowledged Microsoft's intent for the changes. "PC Magazine" provided similar praise, although noting that a number of its online features were designed to promote Microsoft-owned services, and that aside from quicker boot times, XP's overall performance showed little difference over Windows 2000.
Market share.
According to web analytics data generated by Net Applications, Windows XP was the most widely used operating system until August 2012, when Windows 7 overtook it. In January 2014, Net Applications reported a market share of 29.23% for XP, while W3Schools reported a share of 11.0%.
According to web analytics data generated by StatOwl, Windows XP had a 27.82% market share as of November 2012, having dropped to second place in October 2011.
According to web analytics data generated by W3Schools, from September 2003 to July 2011, Windows XP was the most widely used operating system for accessing the w3schools website, which they claim is consistent with statistics from other websites. s of 2014[ [update]], Windows XP market share was at 7.3% after having peaked at 76.1% in January 2007.
According to Net Applications, Windows XP market share was 17.18% as of October 2014.
According to Net Applications, as per the gathered statistics of February 2015, Windows XP holds 19.15% of the market share. 

</doc>
<doc id="33941" url="http://en.wikipedia.org/wiki?curid=33941" title="Windows 2000">
Windows 2000

Windows 2000 is an operating system for use on both client and server computers. It was produced by Microsoft and released to manufacturing on December 15, 1999 and launched to retail on February 17, 2000. It is the successor to Windows NT 4.0, and is the last version of Microsoft Windows to display the "Windows NT" designation. It is succeeded by Windows XP (released in October 2001) and Windows Server 2003 (released in April 2003). During development, Windows 2000 was known as Windows NT 5.0.
Four editions of Windows 2000 were released: "Professional", "Server", "Advanced Server", and "Datacenter Server"; the latter was both released to manufacturing and launched months after the other editions. While each edition of Windows 2000 was targeted at a different market, they shared a core set of features, including many system utilities such as the Microsoft Management Console and standard system administration applications.
Support for people with disabilities was improved over Windows NT 4.0 with a number of new assistive technologies, and Microsoft increased support for different languages and locale information.
All versions of the operating system support NTFS 3.0, Encrypting File System, as well as basic and dynamic disk storage. The Windows 2000 Server family has additional features, including the ability to provide Active Directory services (a hierarchical framework of resources), Distributed File System (a file system that supports sharing of files) and fault-redundant storage volumes. Windows 2000 can be installed through either a manual or unattended installation. Unattended installations rely on the use of answer files to fill in installation information, and can be performed through a bootable CD using Microsoft Systems Management Server, by the System Preparation Tool.
Microsoft marketed Windows 2000 as the most secure Windows version ever at the time; however, it became the target of a number of high-profile virus attacks such as Code Red and Nimda. For ten years after its release, it continued to receive patches for security vulnerabilities nearly every month until reaching the end of its lifecycle on July 13, 2010.
History.
Windows 2000 is a continuation of the Microsoft Windows NT family of operating systems, replacing Windows NT 4.0. The original name for the operating system was Windows NT 5.0. Beta 1 of NT 5.0 was released in September 1997, followed by Beta 2 in August 1998. On October 27, 1998, Microsoft announced that the name of the final version of the operating system would be Windows 2000, a name which referred to its projected release date. Windows 2000 Beta 3 was released in January 1999. NT 5.0 Beta 1 was similar to NT 4.0, including a very similar themed logo. NT 5.0 Beta 2 introduced a new 'mini' boot screen, and removed the 'dark space' theme in the logo. The NT 5.0 betas had very long startup and shutdown sounds, though these were changed in the early Windows 2000 beta, but during Beta 3, a new piano-made startup and shutdown sounds were made, featured in the final version as well as for Windows ME. The new login prompt from the final version made its first appearance in Beta 3 build 1946 (the first build of Beta 3). The new, updated icons (for "My Computer", "Recycle Bin" etc.) first appeared in Beta 3 build 1976. The Windows 2000 boot screen in the final version first appeared in Beta 3 build 1994. Windows 2000 did not have a codename because, according to Dave Thompson of Windows NT team, "Jim Allchin didn't like codenames".
Windows 2000 Service Pack 1 was codenamed "Asteroid" and Windows 2000 64-bit was codenamed "Janus." During development, there was a build for the Alpha which was abandoned some time after RC1 after Compaq announced they had dropped support for Windows NT on Alpha. From here, Microsoft issued three release candidates between July and November 1999, and finally released the operating system to partners on December 12, 1999. The public could buy the full version of Windows 2000 on February 17, 2000. Three days before this event, which Microsoft advertised as "a standard in reliability," a leaked memo from Microsoft reported on by Mary Jo Foley revealed that Windows 2000 had "over 63,000 potential known defects." After Foley's article was published, she claimed that Microsoft blacklisted her for a considerable time. However, Abraham Silberschatz et al. claim in their computer science textbook that "Windows 2000 was the most reliable, stable operating system Microsoft had ever shipped to that point. Much of this reliability came from maturity in the source code, extensive stress testing of the system, and automatic detection of many serious errors in drivers." InformationWeek summarized the release "our tests show the successor to NT 4.0 is everything we hoped it would be. Of course, it isn't perfect either." Wired News later described the results of the February launch as "lackluster." Novell criticized Microsoft's Active Directory, the new directory service architecture, as less scalable or reliable than its own Novell Directory Services (NDS) alternative.
Windows 2000 was first planned to replace both Windows 98 and Windows NT 4.0. However, that changed later. Instead, an updated version of Windows 98 called Windows 98 Second Edition was released in 1999 and Windows ME was released in late 2000.
On or shortly before February 12, 2004, "portions of the Microsoft Windows 2000 and Windows NT 4.0 source code were illegally made available on the Internet." The source of the leak remains unannounced. Microsoft issued the following statement:
 "Microsoft source code is both copyrighted and protected as a trade secret. As such, it is illegal to post it, make it available to others, download it or use it." 
Despite the warnings, the archive containing the leaked code spread widely on the file-sharing networks. On February 16, 2004, an exploit "allegedly discovered by an individual studying the leaked source code" for certain versions of Microsoft Internet Explorer was reported.
Microsoft planned to release a 64-bit version of Windows 2000, which would run on 64-bit Intel Itanium microprocessors, in 2000. However, the first officially-released 64-bit editions of Windows were "Windows Datacenter Server Limited Edition" and later "Windows Advanced Server Limited Edition", which were based on the pre-release Windows Server 2003 (then known as "Windows .NET Server") codebase. These editions were released in 2002, were shortly available through the OEM channel and then were superseded by the final versions of Windows Server 2003.
New and updated features.
Windows 2000 introduced many of the new features of Windows 98 and Windows 98 SE into the NT line, such as the Windows Desktop Update, Internet Explorer 5 (Internet Explorer 6, which came in 2001, is also available for Windows 2000), Outlook Express, NetMeeting, FAT32 support, Windows Driver Model, Internet Connection Sharing, Windows Media Player, WebDAV support etc. Certain new features are common across all editions of Windows 2000, among them NTFS 3.0, the Microsoft Management Console (MMC), UDF support, the Encrypting File System (EFS), Logical Disk Manager, Image Color Management 2.0, support for PostScript 3-based printers, OpenType (.OTF) and Type 1 PostScript (.PFB) font support (including a new font—Palatino Linotype—to showcase some OpenType features), the Data protection API (DPAPI), an LDAP/Active Directory-enabled Address Book, usability enhancements and multi-language and locale support. Windows 2000 also introduced USB device class drivers for USB printers, Mass storage class devices, and improved FireWire SBP-2 support for printers and scanners, along with a "Safe removal" applet for storage devices. Windows 2000 is also the first Windows version to support hibernation at the operating system level (OS-controlled ACPI S4 sleep state) unlike Windows 98 which required special drivers from the hardware manufacturer or driver developer.
A new capability designed to protect critical system files called Windows File Protection was introduced. This protects critical Windows system files by preventing programs other than Microsoft's operating system update mechanisms such as the "Package Installer", Windows Installer and other update components from modifying them. The System File Checker utility provides users the ability to perform a manual scan the integrity of all protected system files, and optionally repair them, either by restoring from a cache stored in a separate "DLLCACHE" directory, or from the original install media.
Microsoft recognized that a serious error or a stop error could cause problems for servers that needed to be constantly running and so provided a system setting that would allow the server to automatically reboot when a stop error occurred. Also included is an option to dump any of the first 64 KB of memory to disk (the smallest amount of memory that is useful for debugging purposes, also known as a minidump), a dump of only the kernel's memory, or a dump of the entire contents of memory to disk, as well as write that this event happened to the Windows 2000 event log. In order to improve performance on servers running Windows 2000, Microsoft gave administrators the choice of optimizing the operating system's memory and processor usage patterns for background services or for applications. Windows 2000 also introduced core system administration and management features as the Windows Installer, Windows Management Instrumentation and Event Tracing for Windows (ETW) into the operating system.
Plug and Play and hardware support improvements.
The most notable improvement from Windows NT 4.0 is the addition of Plug and Play with full ACPI and Windows Driver Model support. Similar to Windows 9x, Windows 2000 supports automatic recognition of installed hardware, hardware resource allocation, loading of appropriate drivers, PnP APIs and device notification events. The addition of the kernel PnP Manager along with the Power Manager are two significant subsystems added in Windows 2000.
Windows 2000 introduced version 3 print drivers (user mode printer drivers). Generic support for 5-button mice is also included as standard and installing IntelliPoint allows reassigning the programmable buttons. Windows 98 lacked generic support. Driver Verifier was introduced to stress test and catch device driver bugs.
Shell.
Windows 2000 introduces layered windows that allow for transparency, translucency and various transition effects like shadows, gradient fills and alpha blended GUI elements to top-level windows. Menus support a new "Fade" transition effect.
The Start Menu in Windows 2000 introduces "personalized menus", expandable special folders and the ability to launch multiple programs without closing the menu by holding down the codice_1 key. A "Re-sort" button forces the entire Start Menu to be sorted by name. The Taskbar introduces support for balloon notifications which can also be used by application developers. Windows 2000 Explorer introduces customizable Windows Explorer toolbars, auto-complete in Windows Explorer address bar and Run box, advanced file type association features, displaying comments in shortcuts as tooltips, extensible columns in Details view (IColumnProvider interface), icon overlays, integrated search pane in Windows Explorer, sort by name function for menus, and "Places bar" in common dialogs for "Open" and "Save".
Windows Explorer has been enhanced in several ways in Windows 2000. It is the first Windows NT release to include Active Desktop, first introduced as a part of Internet Explorer 4.0 (specifically Windows Desktop Update), and only pre-installed in Windows 98 by that time. It allowed users to customize the way folders look and behave by using HTML templates, having the file extension HTT. This feature was abused by computer viruses that employed malicious scripts, Java applets, or ActiveX controls in folder template files as their infection vector. Two such viruses are VBS/Roor-C<ref name="VBS/Roor-C">"Sophos", . Retrieved August 26, 2007.</ref> and VBS.Redlof.a.
The "Web-style" folders view, with the left Explorer pane displaying details for the object currently selected, is turned on by default in Windows 2000. For certain file types, such as pictures and media files, the preview is also displayed in the left pane. Until the dedicated interactive preview pane appeared in Windows Vista, Windows 2000 had been the only Windows release to feature an interactive media player as the previewer for sound and video files, enabled by default. However, such a previewer can be enabled in previous versions of Windows with the Windows Desktop Update installed through the use of folder customization templates. The default file tooltip displays file title, author, subject and comments; this metadata may be read from a special NTFS stream, if the file is on an NTFS volume, or from an OLE structured storage stream, if the file is a structured storage document. All Microsoft Office documents since Office 4.0 make use of structured storage, so their metadata is displayable in the Windows 2000 Explorer default tooltip. File shortcuts can also store comments which are displayed as a tooltip when the mouse hovers over the shortcut. The shell introduces extensibility support through metadata handlers, icon overlay handlers and column handlers in Explorer "Details view".
The right pane of Windows 2000 Explorer, which usually just lists files and folders, can also be customized. For example, the contents of the system folders aren't displayed by default, instead showing in the right pane a warning to the user that modifying the contents of the system folders could harm their computer. It's possible to define additional Explorer panes by using DIV elements in folder template files. This degree of customizability is new to Windows 2000; neither Windows 98 nor the Desktop Update could provide it. The new DHTML-based search pane is integrated into Windows 2000 Explorer, unlike the separate search dialog found in all previous Explorer versions. The Indexing Service has also been integrated into the operating system and the search pane built into Explorer allows searching files indexed by its database.
NTFS 3.0.
Microsoft released the version 3.0 of NTFS (sometimes incorrectly called NTFS 5 in relation to the kernel version number) as part of Windows 2000; this introduced disk quotas (provided by QuotaAdvisor), file-system-level encryption, sparse files and reparse points. Sparse files allow for the efficient storage of data sets that are very large yet contain many areas that only have zeros. Reparse points allow the object manager to reset a file namespace lookup and let file system drivers implement changed functionality in a transparent manner. Reparse points are used to implement volume mount points, junctions, Hierarchical Storage Management, Native Structured Storage and Single Instance Storage. Volume mount points and directory junctions allow for a file to be transparently referred from one file or directory location to another.
Windows 2000 also introduces a "Distributed Link Tracking" service to ensure file shortcuts remain working even if the target is moved or renamed. The target object's unique identifier is stored in the shortcut file on NTFS 3.0 and Windows can use the Distributed Link Tracking service for tracking the targets of shortcuts, so that the shortcut file may be silently updated if the target moves, even to another hard drive.
Encrypting File System.
The Encrypting File System (EFS) introduced strong file system-level encryption to Windows. It allows any folder or drive on an NTFS volume to be encrypted transparently by the user. EFS works together with the EFS service, Microsoft's CryptoAPI and the EFS File System Runtime Library (FSRTL). To date, its encryption has not been compromised.
EFS works by encrypting a file with a bulk symmetric key (also known as the File Encryption Key, or FEK), which is used because it takes less time to encrypt and decrypt large amounts of data than if an asymmetric key cipher were used. The symmetric key used to encrypt the file is then encrypted with a public key associated with the user who encrypted the file, and this encrypted data is stored in the header of the encrypted file. To decrypt the file, the file system uses the private key of the user to decrypt the symmetric key stored in the file header. It then uses the symmetric key to decrypt the file. Because this is done at the file system level, it is transparent to the user.
For a user losing access to their key, support for recovery agents that can decrypt files is built into EFS. A Recovery Agent is a user who is authorized by a public key recovery certificate to decrypt files belonging to other users using a special "private key". By default, local administrators are "recovery agents" however they can be customized using Group Policy.
Basic and dynamic disk storage.
Windows 2000 introduced the Logical Disk Manager and the diskpart command line tool for dynamic storage. All versions of Windows 2000 support three types of dynamic disk volumes (along with basic disks): "simple volumes", "spanned volumes" and "striped volumes":
In addition to these disk volumes, Windows 2000 Server, Windows 2000 Advanced Server, and Windows 2000 Datacenter Server support "mirrored volumes" and "striped volumes with parity":
Accessibility.
With Windows 2000, Microsoft introduced the Windows 9x accessibility features for people with visual and auditory impairments and other disabilities into the NT-line of operating systems. These included:
Additionally, Windows 2000 introduced the following new accessibility features:
Languages and locales.
Windows 2000 introduced the Multilingual User Interface (MUI). Besides English, Windows 2000 incorporates support for Arabic, Armenian, Baltic, Central European, Cyrillic, Georgian, Greek, Hebrew, Indic, Japanese, Korean, Simplified Chinese, Thai, Traditional Chinese, Turkic, Vietnamese and Western European languages. It also has support for many different locales.
Games.
Windows 2000 included version 7.0 of the DirectX API, commonly used by game developers on Windows 98. The last version of DirectX that Windows 2000 supports is DirectX 9.0c (Shader Model 3.0), that shipped with Windows XP Service Pack 2. Microsoft published quarterly updates to DirectX 9.0c through the February 2010 release after which support was dropped in the June 2010 SDK. These updates contain bug fixes to the core runtime and some additional libraries such as D3DX, XAudio 2, XInput and Managed DirectX components. The majority of games written for versions of DirectX 9.0c (up to the February 2010 release) can therefore run on Windows 2000.
Windows 2000 included the same games as Windows NT 4.0 did: FreeCell, Minesweeper, Pinball, and Solitaire.
System utilities.
Windows 2000 introduced the Microsoft Management Console (MMC), which is used to create, save, and open administrative tools. Each of these is called a "console", and most allow an administrator to administer other Windows 2000 computers from one centralised computer. Each console can contain one or many specific administrative tools, called "snap-ins". These can be either standalone (with one function), or an extension (adding functions to an existing snap-in). In order to provide the ability to control what snap-ins can be seen in a console, the MMC allows consoles to be created in "author mode" or "user mode". Author mode allows snap-ins to be added, new windows to be created, all portions of the console tree to be displayed and consoles to be saved. User mode allows consoles to be distributed with restrictions applied. User mode consoles can grant full access to the user for any change, or they can grant limited access, preventing users from adding snapins to the console though they can view multiple windows in a console. Alternatively users can be granted limited access, preventing them from adding to the console and stopping them from viewing multiple windows in a single console.
The main tools that come with Windows 2000 can be found in the "Computer Management" console (in Administrative Tools in the Control Panel). This contains the Event Viewer—a means of seeing events and the Windows equivalent of a log file, a system information utility, a backup utility, Task Scheduler and management consoles to view open shared folders and shared folder sessions, configure and manage COM+ applications, configure Group Policy, manage all the local users and user groups, and a device manager. It contains "Disk Management" and "Removable Storage" snap-ins, a disk defragmenter as well as a performance diagnostic console, which displays graphs of system performance and configures data logs and alerts. It also contains a service configuration console, which allows users to view all installed services and to stop and start them, as well as configure what those services should do when the computer starts. CHKDSK has significant performance improvements.
Windows 2000 comes with two utilities to edit the Windows registry, "REGEDIT.EXE" and "REGEDT32.EXE". REGEDIT has been directly ported from Windows 98, and therefore does not support editing registry permissions. REGEDT32 has the older multiple document interface (MDI) and can edit registry permissions in the same manner that Windows NT's REGEDT32 program could. REGEDIT has a left-side tree view of the Windows registry, lists all loaded hives and represents the three components of a value (its name, type, and data) as separate columns of a table. REGEDT32 has a left-side tree view, but each hive has its own window, so the tree displays only keys and it represents values as a list of strings. REGEDIT supports right-clicking of entries in a tree view to adjust properties and other settings. REGEDT32 requires all actions to be performed from the top menu bar. Windows XP is the first system to integrate these two programs into a single utility, adopting the REGEDIT behavior with the additional NT features.
The System File Checker (SFC) also comes with Windows 2000. It is a command line utility that scans system files and verifies whether they were signed by Microsoft and works in conjunction with the Windows File Protection mechanism. It can also repopulate and repair all the files in the "Dllcache" folder.
Recovery Console.
The Recovery Console is run from outside the installed copy of Windows to perform maintenance tasks that can neither be run from within it nor feasibly be run from another computer or copy of Windows 2000. It is usually used to recover the system from problems that cause booting to fail, which would render other tools useless, like Safe Mode or Last Known Good Configuration, or chkdsk. It includes commands like 'fixmbr', which are not present in MS-DOS.
It has a simple command line interface, used to check and repair the hard drive(s), repair boot information (including NTLDR), replace corrupted system files with fresh copies from the CD, or enable/disable services and drivers for the next boot.
The console can be accessed in either of the two ways:
Windows Scripting Host 2.0.
Windows 2000 introduced Windows Script Host 2.0 which included an expanded object model and support for logon and logoff scripts.
Server family features.
The Windows 2000 server family consists of Windows 2000 Server, Windows 2000 Advanced Server, and Windows 2000 Datacenter Server.
All editions of Windows 2000 Server have the following services and features built in:
The Server editions include more features and components, including the Microsoft Distributed File System (DFS), Active Directory support and fault-tolerant storage.
Distributed File System.
The Distributed File System (DFS) allows shares in multiple different locations to be logically grouped under one folder, or "DFS root". When users try to access a network share off the DFS root, the user is really looking at a "DFS link" and the DFS server transparently redirects them to the correct file server and share. A DFS root can only exist on a Windows 2000 version that is part of the server family, and only one DFS root can exist on that server.
There can be two ways of implementing a DFS namespace on Windows 2000: either through a standalone DFS root or a domain-based DFS root. Standalone DFS allows for only DFS roots on the local computer, and thus does not use Active Directory. Domain-based DFS roots exist within Active Directory and can have their information distributed to other domain controllers within the domain – this provides fault tolerance to DFS. DFS roots that exist on a domain must be hosted on a domain controller or on a domain member server. The file and root information is replicated via the Microsoft File Replication Service (FRS).
Active Directory.
A new way of organizing Windows network domains, or groups of resources, called Active Directory, is introduced with Windows 2000 to replace Windows NT's earlier domain model. Active Directory's hierarchical nature allowed administrators a built-in way to manage user and computer policies and user accounts, and to automatically deploy programs and updates with a greater degree of scalability and centralization than provided in previous Windows versions. User information stored in Active Directory also provided a convenient phone book-like function to end users. Active Directory domains can vary from small installations with a few hundred objects, to large installations with millions. Active Directory can organise and link groups of domains into a contiguous domain name space to form "trees". Groups of trees outside of the same namespace can be linked together to form "forests."
Active Directory services could always be installed on a Windows 2000 Server, Advanced Server, or Datacenter Server computer, and cannot be installed on a Windows 2000 Professional computer. However, Windows 2000 Professional is the first client operating system able to exploit Active Directory's new features. As part of an organization's migration, Windows NT clients continued to function until all clients were upgraded to Windows 2000 Professional, at which point the Active Directory domain could be switched to native mode and maximum functionality achieved.
Active Directory requires a DNS server that supports SRV resource records, or that an organization's existing DNS infrastructure be upgraded to support this. There should be one or more domain controllers to hold the Active Directory database and provide Active Directory directory services.
Volume fault tolerance.
Along with support for simple, spanned and striped volumes, the server family of Windows 2000 also supports fault-tolerant volume types. The types supported are "mirrored volumes" and "RAID-5 volumes":
Deployment.
Windows 2000 can be deployed to a site via various methods. It can be installed onto servers via traditional media (such as CD) or via distribution folders that reside on a shared folder. Installations can be attended or unattended. During a manual installation, the administrator must specify configuration options. Unattended installations are scripted via an answer file, or a predefined script in the form of an INI file that has all the options filled in. An answer file can be created manually or using the graphical "Setup manager". The Winnt.exe or Winnt32.exe program then uses that answer file to automate the installation. Unattended installations can be performed via a bootable CD, using Microsoft Systems Management Server (SMS), via the System Preparation Tool (Sysprep), via the Winnt32.exe program using the /syspart switch or via Remote Installation Services (RIS). The ability to slipstream a service pack into the original operating system setup files is also introduced in Windows 2000.
The Sysprep method is started on a standardized reference computer – though the hardware need not be similar – and it copies the required installation files from the reference computer to the target computers. The hard drive does not need to be in the target computer and may be swapped out to it at any time, with the hardware configured later. The Winnt.exe program must also be passed a /unattend switch that points to a valid answer file and a /s file that points to one or more valid installation sources.
Sysprep allows the duplication of a disk image on an existing Windows 2000 Server installation to multiple servers. This means that all applications and system configuration settings will be copied across to the new installations, and thus, the reference and target computers must have the same HALs, ACPI support, and mass storage devices – though Windows 2000 automatically detects "plug and play" devices. The primary reason for using Sysprep is to quickly deploy Windows 2000 to a site that has multiple computers with standard hardware. (If a system had different HALs, mass storage devices or ACPI support, then multiple images would need to be maintained.)
Systems Management Server can be used to upgrade multiple computers to Windows 2000. These must be running Windows NT 3.51, Windows NT 4.0, Windows 98 or Windows 95 OSR2.x along with the SMS client agent that can receive software installation operations. Using SMS allows installations over a wide area and provides centralised control over upgrades to systems.
Remote Installation Services (RIS) are a means to automatically install Windows 2000 Professional (and not Windows 2000 Server) to a local computer over a network from a central server. Images do not have to support specific hardware configurations and the security settings can be configured after the computer reboots as the service generates a new unique security ID (SID) for the machine. This is required so that local accounts are given the right identifier and do not clash with other Windows 2000 Professional computers on a network.
RIS requires that client computers are able to boot over the network via either a network interface card that has a Pre-Boot Execution Environment (PXE) boot ROM installed or that the client computer has a network card installed that is supported by the remote boot disk generator. The remote computer must also meet the Net PC specification. The server that RIS runs on must be Windows 2000 Server and it must be able to access a network DNS Service, a DHCP service and the Active Directory services.
Editions.
Microsoft released various editions of Windows 2000 for different markets and business needs: Professional, Server, Advanced Server and Datacenter Server. Each was packaged separately.
Windows 2000 Professional was designed as the desktop operating system for businesses and power users. It is the client version of Windows 2000. It offers greater security and stability than many of the previous Windows desktop operating systems. It supports up to two processors, and can address up to 4 GB of RAM. The system requirements are a Pentium processor (or equivalent) of 133 MHz or greater, at least 32 MB of RAM, 650 MB of hard drive space, and a CD-ROM drive (recommended: Pentium II, 128 MB of RAM, 2 GB of hard drive space, and CD-ROM drive).
Windows 2000 Server shares the same user interface with Windows 2000 Professional, but contains additional components for the computer to perform server roles and run infrastructure and application software. A significant new component introduced in the server versions is Active Directory, which is an enterprise-wide directory service based on LDAP (Lightweight Directory Access Protocol). Additionally, Microsoft integrated Kerberos network authentication, replacing the often-criticised NTLM (NT LAN Manager) authentication system used in previous versions. This also provided a purely transitive-trust relationship between Windows 2000 domains in a "forest" (a collection of one or more Windows 2000 domains that share a common schema, configuration, and global catalog, being linked with two-way transitive trusts). Furthermore, Windows 2000 introduced a Domain Name Server which allows dynamic registration of IP addresses. Windows 2000 Server supports up to 4 processors and 4GB of RAM, with a minimum requirement of 128 MB of RAM and 1 GB hard disk space, however requirements may be higher depending on installed components.
Windows 2000 Advanced Server is a variant of Windows 2000 Server operating system designed for medium-to-large businesses. It offers clustering infrastructure for high availability and scalability of applications and services, including support for up to 8 CPUs, a main memory amount of up to 8 gigabytes (GB) on Physical Address Extension (PAE) systems and the ability to do 8-way SMP. It supports TCP/IP load balancing and enhanced two-node server clusters based on the Microsoft Cluster Server (MSCS) in Windows NT Server 4.0 Enterprise Edition. System requirements are similar to those of Windows 2000 Server, however they may need to be higher to scale to larger infrastructure.
Windows 2000 Datacenter Server is a variant of Windows 2000 Server designed for large businesses that move large quantities of confidential or sensitive data frequently via a central server. Like Advanced Server, it supports clustering, failover and load balancing. Its minimum system requirements are normal, but it was designed to be capable of handing advanced, fault-tolerant and scalable hardware—for instance computers with up to 32 CPUs and 32 GBs RAM, with rigorous system testing and qualification, hardware partitioning, coordinated maintenance and change control. System requirements are similar to those of Windows 2000 Advanced Server, however they may need to be higher to scale to larger infrastructure. Windows 2000 Datacenter Server was released to manufacturing on August 11, 2000 and launched on September 26, 2000. This edition was based on Windows 2000 with Service Pack 1 and was not available at retail.
Service packs.
Windows 2000 has received four full service packs and one rollup update package following SP4, which is the last service pack. These were: SP1 on August 15, 2000, SP2 on May 16, 2001, SP3 on August 29, 2002 and SP4 on June 26, 2003. Microsoft phased out all development of its Java Virtual Machine (JVM) from Windows 2000 in SP3. Internet Explorer 5.01 has also been upgraded to the corresponding service pack level.
Microsoft had originally intended to release a fifth service pack for Windows 2000, but Microsoft cancelled this project early in its development, and instead released Update Rollup 1 for SP4, a collection of all the security-related hotfixes and some other significant issues. The Update Rollup does not include all non-security related hotfixes and is not subjected to the same extensive regression testing as a full service pack. Microsoft states that this update will meet customers' needs better than a whole new service pack, and will still help Windows 2000 customers secure their PCs, reduce support costs, and support existing computer hardware.
Security.
During the Windows 2000 period, the nature of attacks on Windows servers changed: more attacks came from remote sources via the Internet. This has led to an overwhelming number of malicious programs exploiting the IIS services – specifically a notorious buffer overflow tendency. This tendency is not operating-system-version specific, but rather configuration-specific: it depends on the services that are enabled. Following this, a common complaint is that "by default, Windows 2000 installations contain numerous potential security problems. Many unneeded services are installed and enabled, and there is no active local security policy." In addition to insecure defaults, according to the SANS Institute, the most common flaws discovered are remotely exploitable buffer overflow vulnerabilities. Other criticized flaws include the use of vulnerable encryption techniques.
Code Red and Code Red II were famous (and much discussed) worms that exploited vulnerabilities of the Windows Indexing Service of Windows 2000's Internet Information Services (IIS). In August 2003, two major worms called Sobig and Blaster began to attack millions of Microsoft Windows computers, resulting in the largest downtime and clean-up cost to that date. The 2005 Zotob worm was blamed for security compromises on Windows 2000 machines at the U.S. Department of Homeland Security, the New York Times Company, ABC and CNN.
On September 8, 2009, Microsoft skipped patching two of the five security flaws that were addressed in the monthly security update, saying that patching one of the critical security flaws was "infeasible." According to Microsoft Security Bulletin MS09-048: "The architecture to properly support TCP/IP protection does not exist on Microsoft Windows 2000 systems, making it infeasible to build the fix for Microsoft Windows 2000 Service Pack 4 to eliminate the vulnerability. To do so would require re-architecting a very significant amount of the Microsoft Windows 2000 Service Pack 4 operating system, [...] there would be no assurance that applications designed to run on Microsoft Windows 2000 Service Pack 4 would continue to operate on the updated system." No patches for this flaw were however released for the newer Windows XP (32-bit) and Windows XP Professional x64 Edition either, despite both also being affected.
Support lifecycle.
Windows 2000 was superseded by newer Microsoft operating systems: Windows 2000 Server products by Windows Server 2003, and Windows 2000 Professional by Windows XP Professional.
The Windows 2000 family of operating systems moved from mainstream support to the extended support phase on June 30, 2005. Microsoft says that this marks the progression of Windows 2000 through the Windows lifecycle policy. Under mainstream support, Microsoft freely provides design changes if any, service packs and non-security related updates in addition to security updates, whereas in extended support, service packs are not provided and non-security updates require contacting the support personnel by e-mail or phone. Under the extended support phase, Microsoft continued to provide critical security updates every month for all components of Windows 2000 (including Internet Explorer 5.0 SP4) and paid per-incident support for technical issues. Because of Windows 2000's age, updated versions of components such as Windows Media Player 11 and Internet Explorer 7 have not been released for it. In the case of Internet Explorer, Microsoft said in 2005 that, "some of the security work in IE 7 relies on operating system functionality in XP SP2 that is non-trivial to port back to Windows 2000."
While users of Windows 2000 Professional and Server are eligible to receive the upgrade license for Windows Vista Business or Windows Server 2008, neither of these operating systems can directly perform an upgrade installation from Windows 2000; a clean installation must be performed instead or a two-step upgrade through XP/2003. Microsoft has dropped the upgrade path from Windows 2000 (and earlier) to Windows 7. Users of Windows 2000 must buy a full Windows 7 license.
Although Windows 2000 is the last NT-based version of Microsoft Windows which does not include product activation, Microsoft has introduced Windows Genuine Advantage for certain downloads and non-critical updates from the Download Center for Windows 2000.
Windows 2000 reached the end of its lifecycle on July 13, 2010. It will not receive new security updates and new security-related hotfixes after this date. In Japan, over 130,000 servers and 500,000 PCs in local governments are affected; many local governments said that they will not update as they do not have funds to cover a replacement.
As of 2011 Windows Update still supports the Windows 2000 updates available on Patch Tuesday in July 2010, e.g., if older optional Windows 2000 features are enabled later. Microsoft Office products under Windows 2000 have their own product lifecycles. While Internet Explorer 6 for Windows XP did receive security patches up until it lost support, this is not the case for IE6 under Windows 2000. The Windows Malicious Software Removal Tool installed monthly by Windows Update for XP and later versions can be still downloaded manually for Windows 2000.
Total cost of ownership.
In October 2002, Microsoft commissioned IDC to determine the total cost of ownership (TCO) for enterprise applications on Windows 2000 versus the TCO of the same applications on Linux. IDC's report is based on telephone interviews of IT executives and managers of 104 North American companies in which they determined what they were using for a specific workload for file, print, security and networking services.
IDC determined that the four areas where Windows 2000 had a better TCO than Linux – over a period of five years for an average organization of 100 employees – were file, print, network infrastructure and security infrastructure. They determined, however, that Linux had a better TCO than Windows 2000 for web serving. The report also found that the greatest cost was not in the procurement of software and hardware, but in staffing costs and downtime. While the report applied a 40% productivity factor during IT infrastructure downtime, recognizing that employees are not entirely unproductive, it did not consider the impact of downtime on the profitability of the business. The report stated that Linux servers had less unplanned downtime than Windows 2000 servers. It found that most Linux servers ran less workload per server than Windows 2000 servers and also that none of the businesses interviewed used 4-way SMP Linux computers. The report also did not take into account specific application servers – servers that need low maintenance and are provided by a specific vendor. The report did emphasize that TCO was only one factor in considering whether to use a particular IT platform, and also noted that as management and server software improved and became better packaged the overall picture shown could change.

</doc>
<doc id="33974" url="http://en.wikipedia.org/wiki?curid=33974" title="Will Eisner">
Will Eisner

William Erwin "Will" Eisner (March 6, 1917 – January 3, 2005) was an American cartoonist, writer, and entrepreneur. He was one of the earliest cartoonists to work in the American comic book industry, and his series "The Spirit" (1940–1952) was noted for its experiments in content and form. In 1978, he popularized the term "graphic novel" with the publication of his book "A Contract with God". He was an early contributor to formal comics studies with his book "Comics and Sequential Art" (1985). The Eisner Award was named in his honor, and is given to recognize achievements each year in the comics medium; he was one of the three inaugural inductees to the Will Eisner Comic Book Hall of Fame.
Biography.
Family background.
Eisner's father Shmuel "Samuel" Eisner was born March 6, 1886, in Kolomyia, Austria-Hungary, and was one of eleven children. He aspired to be an artist, and as a teenager painted murals for rich patrons and Catholic churches in Vienna. To avoid conscription in the army, he moved to New York before the outbreak of World War I. There he found getting work difficult as his English skills were poor. He made what living he could painting backdrops for vaudeville and the Jewish theater.
Eisner's mother, Fannie Ingber, was born to Jewish parents from Romania April 25, 1891, on a ship bound for the US. Her mother died on her tenth birthday, and was quickly followed by her father. An older stepsister thereafter raised her and kept her so busy with chores that she had little time for socializing or schooling; she did what she could later in life to keep knowledge of her illiteracy from her children.
Family introduced Shmuel and Fannie, who were distant relatives. They had three children: son Will Erwin, born on his father's birthday in 1917; son Julian, born February 3, 1921; and daughter Rhoda, born November 2, 1929.
Early life.
Eisner was born in Brooklyn, New York City. He grew up poor, and the family moved frequently. Young Eisner often got into physical confrontations when subject to antisemitism from his schoolmates. His family were not orthodox followers of Judaism; Eisner himself, while he prided his cultural background, turned against the religion when his family was denied entry to a synagogue over lack of money for admission.
Young Eisner was tall and of sturdy build, but lacked athletic skills. He was a voracious consumer of pulp magazines and film, including avant-garde films such as those by Man Ray. To his mother's disappointment, Eisner had his father's interest in art, and his father encouraged him by buying him art supplies.
Eisner's mother frequently berated his father for not providing the family a better income, as he went from one job to another. Without success he also tried his hand at such ventures as a furniture retailer and a coat factory. The family situation was especially dire following the Wall Street Crash of 1929 that marked the beginning of the Great Depression. In 1930, the situation was so desperate that Eisner's mother demanded that he, at thirteen, find some way to contribute to the family's income. He entered working life selling newspapers on street corners, a competitive job where the toughest boys fought for the best locations.
Eisner attended DeWitt Clinton High School. With influences that included the early 20th-century commercial artist J. C. Leyendecker, he drew for the school newspaper ("The Clintonian"), the literary magazine ("The Magpie") and the yearbook, and did stage design, leading him to consider doing that kind of work for theater. Upon graduation, he studied under Canadian artist George Brandt Bridgman (1864–1943) for a year at the Art Students League of New York. Contacts made there led to a position as an advertising writer-cartoonist for the "New York American" newspaper. Eisner also drew $10-a-page illustrations for pulp magazines, including "Western Sheriffs and Outlaws".
In 1936, high-school friend and fellow cartoonist Bob Kane, of future Batman fame, suggested that the 19-year-old Eisner try selling cartoons to the new comic book "Wow, What A Magazine!" "Comic books" at the time were tabloid-sized collections of comic strip reprints in color. In 1935, they had begun to include occasional new comic strip-like material. "Wow" editor Jerry Iger bought an Eisner adventure strip called "Captain Scott Dalton", an H. Rider Haggard-styled hero who traveled the world after rare artifacts. Eisner subsequently wrote and drew the pirate strip "The Flame" and the secret agent strip "Harry Karry" for "Wow" as well.
Eisner said that on one occasion a man who Eisner described as "a Mob type straight out of Damon Runyon, complete with pinkie ring, broken nose, black shirt, and white tie, who claimed to have 'exclusive distribution rights for all Brooklyn" asked Eisner to draw Tijuana bibles for $3 a page. Eisner said that he declined the offer; he described the decision as "one of the most difficult moral decisions of my life".
Eisner & Iger.
"Wow" lasted four issues (cover-dated July–September and November 1936). After it ended, Eisner and Iger worked together producing and selling original comics material, anticipating that the well of available reprints would soon run dry, though their accounts of how their partnership was founded differ. One of the first such comic-book "packagers", their partnership was an immediate success, and the two soon had a stable of comics creators supplying work to Fox Comics, Fiction House, Quality Comics (for whom Eisner co-created such characters as Doll Man and Blackhawk), and others.
Turning a profit of $1.50 a page, Eisner claimed that he "got very rich before I was 22," later detailing that in Depression-era 1939 alone, he and Iger "had split $25,000 between us", a considerable amount for the time.
Among the studio's products was a self-syndicated Sunday comic strip, "Hawks of the Sea", that initially reprinted Eisner's old strip "Wow, What A Magazine!" feature "The Flame" and then continued it with new material. Eisner's original work even crossed the Atlantic, with Eisner drawing the new cover of the October 16, 1937 issue of Boardman Books' comic-strip reprint tabloid "Okay Comics Weekly."
In 1939, Eisner was commissioned to create Wonder Man for Victor Fox, an accountant who had previously worked at DC Comics and was becoming a comic book publisher himself. Following Fox's instructions to create a Superman-type character, and using the pen name Willis, Eisner wrote and drew the first issue of "Wonder Comics." Eisner said in interviews throughout his later life that he had protested the derivative nature of the character and story, and that when subpoenaed after National Periodical Publications, the company that would evolve into DC Comics, sued Fox, alleging Wonder Man was an illegal copy of Superman, Eisner testified that this was so, undermining Fox's case; Eisner even depicts himself doing so in his semi-autobiographical graphic novel "The Dreamer". However, a transcript of the proceeding, uncovered by comics historian Ken Quattro in 2010, indicates Eisner in fact supported Fox and claimed Wonder Man as an original Eisner creation.
"The Spirit".
In "late '39, just before Christmas time," Eisner recalled in 1979, Quality Comics publisher Everett M. "Busy" Arnold "came to me and said that the Sunday newspapers were looking for a way of getting into this comic book boom," In a 2004 interview, he elaborated on that meeting:
"Busy" invited me up for lunch one day and introduced me to Henry Martin [sales manager of The Des Moines Register and Tribune Syndicate, who] said, "The newspapers in this country, particularly the Sunday papers, are looking to compete with comics books, and they would like to get a comic-book insert into the newspapers." ... Martin asked if I could do it. ... It meant that I'd have to leave Eisner & Iger [which] was making money; we were very profitable at that time and things were going very well. A hard decision. Anyway, I agreed to do the Sunday comic book and we started discussing the deal [which] was that we'd be partners in the 'Comic Book Section,' as they called it at that time. And also, I would produce two other magazines in partnership with Arnold.
Eisner negotiated an agreement with the syndicate in which Arnold would copyright "The Spirit," but, "Written down in the contract I had with 'Busy' Arnold — and this contract exists today as the basis for my copyright ownership — Arnold agreed that it was my property. They agreed that if we had a split-up in any way, the property would revert to me on that day that happened. My attorney went to 'Busy' Arnold and his family, and they all signed a release agreeing that they would not pursue the question of ownership" This would include the eventual backup features "Mr. Mystic" and "Lady Luck".
Selling his share of their firm to Iger, who would continue to package comics as the S. M. Iger Studio and as Phoenix Features through 1955, for $20,000, Eisner left to create "The Spirit." "They gave me an adult audience", Eisner said in 1997, "and I wanted to write better things than superheroes. Comic books were a ghetto. I sold my part of the enterprise to my associate and then began The Spirit. They wanted an heroic character, a costumed character. They asked me if he'd have a costume. And I put a mask on him and said, 'Yes, he has a costume!'"
"The Spirit", an initially eight- and later seven-page urban-crimefighter series, ran with the initial backup features "Mr. Mystic" and "Lady Luck" in a 16-page Sunday supplement (colloquially called "The Spirit Section") that was eventually distributed in 20 newspapers with a combined circulation of as many as five million copies. It premiered June 2, 1940, and continued through 1952. Eisner has cited the Spirit story "Gerhard Shnobble" as a particular favorite, as it was one of his first attempts at injecting his personal point of view into the series.
World War II and "Joe Dope".
Eisner was drafted into the U.S. Army in "late '41, early '42" and then "had about another half-year which the government gave me to clean up my affairs before going off" to fight in World War II. He was assigned to the camp newspaper in Aberdeen, where "there was also a big training program there, so I got involved in the use of comics for training. ... I finally became a warrant officer, which involved taking a test – that way you didn't have to go through Officer Candidate School."
En route to Washington, D.C., he stopped at the Holabird Depot in Baltimore, Maryland, where a mimeographed publication titled "Army Motors" was put together. "Together with the people there ... I helped develop its format. I began doing cartoons – and we began fashioning a magazine that had the ability to talk to the G.I.s in their language. So I began to use comics as a teaching tool, and when I got to Washington, they assigned me to the business of teaching – or selling – preventive maintenance."
Eisner then created the educational comic strip and titular character "Joe Dope" for "Army Motors", and spent four years working in The Pentagon editing the ordnance magazine "Firepower" and doing "all the general illustrations – that is, cartoons" for "Army Motors". He continued to work on that and its 1950 successor magazine, "PS, The Preventive Maintenance Monthly" until 1971. Eisner also illustrated an official Army pamphlet in 1968 and 1969 called "The M16A1 Rifle" specifically for troops in Vietnam. Eisner's style helped to popularize these officially-distributed works in order to better educate soldiers on equipment maintenance.
While Eisner's later graphic novels were entirely his own work, he had a studio working under his supervision on "The Spirit". In particular, letterer Abe Kanegson came up with the distinctive lettering style which Eisner himself would later imitate in his book-length works, and Kanegson would often rewrite Eisner's dialogue.
Eisner's most trusted assistant on "The Spirit," however, was Jules Feiffer, later a renowned cartoonist, playwright and screenwriter in his own right. Eisner later said of their working methods "You should hear me and Jules Feiffer going at it in a room. 'No, you designed the splash page for this one, then you wrote the ending — I came up with the idea for the story, and you did it up to this point, then I did the next page and this sequence here and...' And I'll be swearing up and down that 'he' wrote the ending on that one. We never agree".
So trusted were Eisner's assistants that Eisner allowed them to "ghost" "The Spirit" from the time that he was drafted into the U.S. Army in 1942 until his return to civilian life in 1945. The primary wartime artists were the uncredited Lou Fine and Jack Cole, with future "Kid Colt, Outlaw" artist Jack Keller drawing backgrounds. Ghost writers included Manly Wade Wellman and William Woolfolk. The wartime ghosted stories have been reprinted in DC Comics' hardcover collections "The Spirit Archives" Vols. 5 to 11 (2001–2003), spanning July 1942 – December 1944.
On Eisner's return from service and resumption of his role in the studio, he created the bulk of the "Spirit" stories on which his reputation was solidified. The post-war years also saw him attempt to launch the comic-strip/comic-book series "Baseball," "John Law," "Kewpies," and "Nubbin the Shoeshine Boy;" none succeeded, but some material was recycled into "The Spirit".
American Visuals Corporation.
During his World War II military service, Eisner had introduced the use of comics for training personnel in the publication "Army Motors", for which he created the cautionary bumbling soldier Joe Dope, who illustrated various methods of preventive maintenance of various military equipment and weapons. In 1948, while continuing to do "The Spirit" and seeing television and other post-war trends eat at the readership base of newspapers, he formed the American Visuals Corporation in order to produce instructional materials for the government, related agencies, and businesses.
One of his longest-running jobs was "PS, The Preventive Maintenance Monthly," a digest sized magazine with comic book elements that he started for the Army in 1951 and continued to work on until the 1970s with Klaus Nordling, Mike Ploog, and other artists. In addition, Eisner produced other military publications such as the graphic manual in 1969, "", which was distributed along with cleaning kits to address serious reliability concerns with the M16 Rifle during the Vietnam War.
Other clients of his Connecticut-based company included RCA Records, the Baltimore Colts NFL football team, and New York Telephone.
Graphic novels.
In the late 1970s, Eisner turned his attention to longer storytelling forms. "A Contract with God, and Other Tenement Stories" (Baronet Books, October 1978) is an early example of an American graphic novel, combining thematically linked short stories into a single square-bound volume. Eisner continued with a string of graphic novels that tell the history of New York's immigrant communities, particularly Jews, including "The Building", "A Life Force", "Dropsie Avenue" and "To the Heart of the Storm". He continued producing new books into his seventies and eighties, at an average rate of nearly one a year. Each of these books was done twice — once as a rough version to show editor Dave Schreiner, then as a second, finished version incorporating suggested changes.
Some of his last work was the retelling in sequential art of novels and myths, including "Moby-Dick". In 2002, at the age of 85, he published "Sundiata", based on the part-historical, part-mythical stories of a West African king, "The Lion of Mali". "Fagin the Jew" is an account of the life of Dickens' character Fagin, in which Eisner tries to get past the stereotyped portrait of Fagin in "Oliver Twist".
His last graphic novel, "The Plot: The Secret Story of The Protocols of the Elders of Zion", an account of the making of the anti-semitic hoax "The Protocols of the Learned Elders of Zion", was completed shortly before his death and published in 2005.
Teaching.
In his later years especially, Eisner was a frequent lecturer about the craft and uses of sequential art. He taught at the School of Visual Arts in New York City, where he published "Will Eisner's Gallery", a collection of work by his students and wrote two books based on these lectures, "Comics and Sequential Art" and "Graphic Storytelling and Visual Narrative", which are widely used by students of cartooning. In 2002, Eisner participated in the Will Eisner Symposium of the 2002 University of Florida Conference on Comics and Graphic Novels.
Death.
Eisner died January 3, 2005, in Lauderdale Lakes, Florida, of complications from a quadruple bypass surgery performed December 22, 2004. DC Comics held a memorial service in Manhattan's Lower East Side, a neighborhood Eisner often visited in his work, at the Angel Orensanz Foundation on Norfolk Street.
Eisner was survived by his wife, Ann Weingarten Eisner, and their son, John. In the introduction to the 2001 reissue of "A Contract with God", Eisner revealed that the inspiration for the title story grew out of the 1970 death of his leukemia-stricken teenaged daughter, Alice, next to whom he is buried. Until then, only Eisner's closest friends were aware of his daughter's life and death.
Awards and honors.
Eisner has been recognized for his work with the National Cartoonists Society Comic Book Award for 1967, 1968, 1969, 1987 and 1988, as well as its Story Comic Book Award in 1979, and its Reuben Award in 1998.
He was inducted into the Academy of Comic Book Arts Hall of Fame in 1971, and the Jack Kirby Hall of Fame in 1987. The following year, the Will Eisner Comic Industry Awards were established in his honor.
He received in 1975 the second Grand Prix de la ville d'Angoulême.
With Jack Kirby, Robert Crumb, Harvey Kurtzman, Gary Panter, and Chris Ware, Eisner was among the artists honored in the exhibition "Masters of American Comics" at the Jewish Museum in New York City, from September 16, 2006 to January 28, 2007.
On the 94th anniversary of Eisner's birth, in 2011, Google used an image featuring the Spirit as its logo.
References.
Works cited.
</dl>

</doc>
<doc id="34001" url="http://en.wikipedia.org/wiki?curid=34001" title="Warmia">
Warmia

Warmia (Polish: "Warmia", Latin: "Varmia", German:   ) is a historical region in northern Poland. 
It is nowadays the core of the Warmian-Masurian Voivodeship. It has about 4,500 km2 and 350,000 inhabitants. Its biggest city is Olsztyn, while the historical capital was Lidzbark Warmiński; another large town is Braniewo. Important landmarks include the cathedral in Frombork, where Mikołaj Kopernik elaborated the heliocentric theory, and sanctuary in Gietrzwałd, a site of Marian apparitions and miracles. It is an area of many lakes; it lies at the upper Łyna river and on the right bank of Pasłęka, stretching in the northwest to the Vistula Bay. Warmia is part of the historical province of Prussia and has traditionally strong connections with Masuria (the southern part was Polish-speaking too, while the rest has been German), but unlike it, remained Catholic and belonged to Poland before 1772. Warmia has been under the dominion of various states over the course of its history, most notably the Old Prussians, the Teutonic Knights, the Kingdom of Poland, and the Kingdom of Prussia. The history of the region is closely connected to that of the Archbishopric of Warmia (formerly, Duchy of Warmia). The region is associated with the Prussian tribe, the Warmians, who settled in an approximate area. According to folk etymology, Warmia is named after the legendary Prussian chief Warmo, whereas the name Ermland derives from his widow Erma.
History.
Early times.
The first traces of human settlement in the region come from roughly 14 to 15 thousand years ago: traces of settlements made by the Lusatian culture (thirteenth—fifth century BC), including above-ground water housings and artificial islands. By the early Middle Ages the Warmians, an Old Prussian tribe, inhabited the area.
The beginning of the Northern Crusades.
In the 13th century the area became a battleground in the Northern Crusades. Having failed to gather an expedition against Palestine, Pope Innocent III resolved in 1207 to organize a new crusade; beginning in 1209, he called for crusades against the Albigenses, against the Almohad dynasty of Spain (1213), and, also around that time, against the pagans of Prussia. The first Bishop of Prussia, Christian of Oliva, was commissioned in 1209 to convert the Prussians, at the request of Konrad I of Masovia (duke from 1194 to 1247).
The Teutonic Order.
In 1226 Duke Konrad I of Masovia invited the Teutonic Knights to Christianize the pagan Prussians. He supplied the Teutonic Order and allowed the usage of Chełmno Land ("Culmerland") as a base for the knights. They had the task of establishing secure borders between Masovia and the Prussians, with the assumption that conquered territories would become part of Masovia. The Order waited until they received official authorisation from the Empire, which Emperor Frederick II granted by issuing the Golden Bull of Rimini (March 1226). The papal Golden Bull of Rieti from Pope Gregory IX in 1234 confirmed the grant, although Konrad of Masovia never recognized the rights of the Order to rule Prussia. Later, the Knights were accused of forging these land grants. 
By the end of the 13th century the Teutonic Order had conquered and Christianized most of the Prussian region, including Warmia. The new régime reduced many of the native Prussians to the status of serfs and gradually Germanized them. Over several centuries the colonists, native Prussians and immigrants gradually intermingled.
In 1242 the papal legate William of Modena set up four dioceses, including the Archbishopric of Warmia. From the 13th century new colonists, mainly Germans, settled in the Monastic State of the Teutonic Knights (with Warmia) (the Duchy of Prussia, Lutheran from 1525 onwards, granted refuge to Protestant Poles, Lithuanians, Scots and Salzburgers). The bishopric was exempt and was governed by a prince-bishop, confirmed by Emperor Charles IV. The Bishops of Warmia were usually Germans or Poles, although Enea Silvio Piccolomini, the later Pope Pius II, served as an Italian bishop of the diocese. 
After the 1410 Battle of Grunwald, Bishop Heinrich Vogelsang of Warmia surrendered to King Władysław II Jagiełło of Poland, and later with Bishop Henry of Sambia gave homage to the Polish king at the Polish camp during the siege of Marienburg Castle (Malbork). After the Polish army moved out of Warmia, the new Grand Master of the Teutonic Knights, Heinrich von Plauen the Elder, accused the bishop of treachery and reconquered the region.
Polish Crown.
The Second Peace of Thorn (1466) removed Warmia from the control of the Teutonic Knights and placed it under the sovereignty of the Crown of Poland as part of the province of Royal Prussia, although with several privileges.
Soon after, in 1467, the Cathedral Chapter elected Nicolas von Tüngen against the wish of the Polish king. The Estates of Royal Prussia did not take the side of the Cathedral Chapter. Nicholas von Tüngen allied himself with the Teutonic Order and with King Matthias Corvinus of Hungary. The feud, known as the War of the Priests, was a low scale affair, affecting mainly Warmia. In 1478 Braniewo (Braunsberg) withstood a Polish siege which was ended in an agreement in which the Polish king recognized von Tüngen as bishop and the right of the Cathedral Chapter to elect future bishops, which however would have to be accepted by the king, and the bishop as well as Cathedral Chapter swore an oath to the Polish king. Later in the Treaty of Piotrków Trybunalski (December 7, 1512), conceded to the king of Poland a limited right to determine the election of bishops by choosing four candidates from Royal Prussia.
After the Union of Lublin in 1569 Duchy of Warmia was officially directly included as part of the Polish crown within the Polish-Lithuanian Commonwealth. At the same time the territory continued to enjoy substantial autonomy, with many legal differences from neighbouring lands. For example, the bishops were by law members of Polish Senat and the land elected MP's to the Sejmik resp. Landtag of Royal Prussia as well as MP's to the Sejm of Poland.
Warmia was under the Church jurisdiction of the Archbishopric of Riga until 1512, when Prince-Bishop Lucas Watzenrode received exempt status, placing Warmia directly under the authority of the Pope (in terms of church jurisdiction), which remained until the resolution of the Holy Roman Empire in 1806.
Prussia.
By the First Partition of Poland in 1772, Warmia was merged with the surrounding parts of East Prussia and annexed by the Kingdom of Prussia; the properties of the Archbishopric of Warmia were secularized by the Prussian state. Ignacy Krasicki, the last prince-bishop of Warmia as well as Enlightenment Polish poet, friend of Frederick the Great (whom he did not give homage as his new king), was nominated to the Archbishopric of Gnesen (Gniezno) in 1795. After the last partition of Poland and during his tenure as Archbishop of Poland and Prussian subject he was ordered by Pope Pius VI to teach his Catholic Poles to 'stay obedient, faithful, and loving to their new kings', Papal brief of 1795. The Prussian census in 1772 showed a total population of 96,547, including an urban population of 24,612 in 12 towns. 17,749 houses were listed and the biggest city was Braunsberg (Braniewo).
From 1772-1945 Warmia was part of Lutheran East Prussia, with the exception that the people of Warmia remained largely Catholic. Most of the German population of Warmia spoke High Prussian, while a small area in the north spoke Low Prussian; southern Warmia was mostly populated by Polish-speaking Warmiaks. Warmia became part of the German Empire in 1871.
In 1873 according to a regulation of the Imperial German government, school lessons at public schools inside Germany had to be held in German, as a result the Polish language was forbidden in all schools in Warmia, including Polish schools already founded in the sixteenth century. In 1900 Warmia's population was 240,000. In the jingoistic climate after World War I, Warmian Poles were subject to persecution by the German government. Polish children speaking their language were punished in schools and often had to wear signs with insulting names, such as "Pollack".
After the First World War in the aftermath of the East Prussian plebiscite, carried when Red Army was marching on Warsaw - Polish–Soviet War in 1920, the region remained in Germany, as in the Warmian district of Allenstein (Olsztyn) 86,53% and in the district of Rössel (Reszel) 97,9% voted for Germany. The persecutions of the German governments and militias worsened in the late 1930s when Hitler was in power, and the Poles in Warmia were subject to harsh persecution by German authorities and militias, such as attacks on schools and centers. During World War II Germany sought to suppress all elements of social and political life of the Polish minority in Germany by interning and murdering Polish activists and leaders, including the ones in Warmia. Unlike the rest of Protestant East Prussia, Warmia retained its Catholicism and Catholic-related folk customs, all the way through 1945.
Poland.
At the Yalta Conference and Potsdam Conference of 1945, the victorious Allies divided East Prussia into the two parts now known as Oblast Kaliningrad (in Russia) and the Warmian-Masurian Voivodeship (in Poland). The population was evacuated or fled the advancing Red Army in 1945. Prior to the Potsdam Conference, during the Soviet winter 1945 offensive (Vistula–Oder Offensive), the Red Army overran Warmia, killing and raping the local residents, without any regards towards age or ethnic origin. Following the Potsdam Conference agreements the Soviets and then the Polish administration expelled most of the remaining Warmian Germans to the Western part of Germany under the Allied rule, unless in the Polish voivodeship if they declared themselves Polish speakers. 
As a result of World War II, only a small minority of Germans of Poland remained in Warmia. The majority of present inhabitants of Warmia are descendants of Poles who either were Warmiaks or migrated there from other parts of Poland, including the pre-1939 Polish Borderlands, after 1945. Olsztyn is the largest city in Warmia and the capital of the Warmian-Masurian Voivedeship. During 1945-46, Warmia was part of the "Okreg Mazurski" (Masurian District). In 1946 a new voivodeship was created and named the Olsztyn Voivodeship, which encompassed both Warmia and Masurian counties. In 1975 this voivodeship was redistricted and survived in this form until the new redistricting and renaming in 1999 as Warmian-Masurian Voivodeship. The Catholic character of Warmia has been preserved in the architecture of its villages and towns, as well as in folk customs.

</doc>
<doc id="34033" url="http://en.wikipedia.org/wiki?curid=34033" title="Wildebeest">
Wildebeest

The wildebeests, also called gnus, are a genus of antelopes, Connochaetes. They belong to the family Bovidae, which includes antelopes, cattle, goats, sheep and other even-toed horned ungulates. "Connochaetes" includes two species, both native to Africa: the black wildebeest, or white-tailed gnu ("C. gnou"); and the blue wildebeest, or brindled gnu ("C. taurinus"). Fossil records suggest these two species diverged about one million years ago, resulting in a northern and a southern species. The blue wildebeest remained in its original range and changed very little from the ancestral species, while the black wildebeest changed more in order to adapt to its open grassland habitat in the south. The most obvious way of telling the two species apart are the differences in their colouring and in the way their horns are orientated. Their main predators are lions, spotted hyenas, crocodiles, and African wild dogs.
In East Africa, the blue wildebeest is the most abundant big game species and some populations perform an annual migration to new grazing grounds but the black wildebeest is merely nomadic. Breeding in both takes place over a short period of time at the end of the rainy season and the calves are soon active and are able to move with the herd. Nevertheless, some fall prey to large carnivores. Wildebeest often graze in mixed herds with zebra which gives heightened awareness of potential predators. They are also alert to the warning signals emitted by other animals such as baboons. Wildebeest are a tourist attraction but compete with domesticated livestock for pasture and are sometimes blamed by farmers for transferring diseases and parasites to their cattle. Some illegal hunting goes on but the population trend is fairly stable and some populations are in national parks or on private land. The IUCN lists both species as being of "least concern".
Etymology.
The wildebeest ( or , plural wildebeest or wildebeests, wildebeesties (juv)), also called the gnu ( or ) is an antelope of the genus "Connochaetes". "Wildebeest" is Dutch for "wild beast" or "wild cattle" in Afrikaans ("bees" = cattle), while "Connochaetes" derives from the Greek words κόννος, "kónnos", "beard", and χαίτη, "khaítē", "flowing hair", "mane". Some sources claim the name "gnu" originates from the Khoikhoi name for these animals, "t'gnu". Others contend the name and its pronunciation in English go back to the word "!nu:" used for the black wildebeest among the Southern Bushmen, now generally referred to as the San people.
Classification.
Taxonomy and evolution.
The wildebeest, or the genus "Connochaetes", is placed under family Bovidae and subfamily Alcelaphinae, where its closest relatives are the hartebeest ("Alcelaphus" spp.), the hirola ("Beatragus hunteri") and species in the genus "Damaliscus", such as the topi, the tsessebe, the blesbok and the bontebok. The name "Connochaetes" was given by German zoologist Martin Hinrich Carl Lichtenstein in 1814. Wildebeest were first discovered about 1700 by Dutch settlers on their way to the interior of South Africa. Due to their resemblance to wild cattle, these people called them "wild ox" or "wildebeest". The black wildebeest was first known to westerners in the northern part of South Africa a century later, in the 1800s.
In the early twentieth century, one species of the wildebeest, "Connochaetes albojubatus", was identified in eastern Africa. In 1914, two separate races of the wildebeest were introduced, namely "Gorgon a. albojubatus" (Athi white-bearded wildebeest) and "G. a. mearnsi" (Loita white-bearded wildebeest). However, in 1939, the two were once again merged into a single race, "Connochaetes taurinus albojubatus". In the mid-twentieth century, two separate forms were recognised, "Gorgon taurinus hecki" and "G. t. albojubatus". Finally two distinct types of wildebeest - the blue and black wildebeest - were identified. The blue wildebeest was at first placed under a separate genus, "Gorgon", while the black wildebeest belonged to the genus "Connochaetes". Today they are united in the single genus "Connochaetes": the black wildebeest being named ("C. gnou") and the blue wildebeest, ("C. taurinus").
According to an mtDNA analysis, the black wildebeest seem to have diverged from the main lineage during the Middle Pleistocene and became a distinct species around a million years ago. A divergence rate of approximately 2% has been calculated. The split does not seem to have been driven by competition for resources but instead by the fact that each species adopted a different feeding niche and occupied a different trophic level.
Blue wildebeest fossils dating back some two and a half million years ago are common and widespread. They have been found in the fossil bearing caves at the Cradle of Humankind north of Johannesburg. Elsewhere in South Africa they are plentiful at such sites as Elandsfontein, Cornelia and Florisbad. The earliest fossils of the black wildebeest were found in sedimentary rock in Cornelia in the Orange Free State and dated back about eight hundred thousand years. Today, five subspecies of the blue wildebeest are recognised while the black wildebeest has no named subspecies.
Genetics and hybrids.
The diploid number of chromosomes in the wildebeest is 58. Chromosomes were studied in a male and a female wildebeest. In the female, all except a pair of very large submetacentric chromosomes were found to be acrocentric. Metaphases were studied in the male's chromosomes, and very large submetacentric chromosomes were found there as well, similar to those in the female both in size and morphology. Other chromosomes were acrocentric. The X chromosome is a large acrocentric and the Y chromosome a minute one.
The two species of the wildebeest are known to hybridise. Male black wildebeest have been reported to mate with female blue wildebeest and vice versa. The differences in social behaviour and habitats have historically prevented interspecific hybridisation between the species, however hybridisation may occur when they are both confined within the same area. The resulting offspring is usually fertile. A study of these hybrid animals at Spioenkop Dam Nature Reserve in South Africa revealed that many had disadvantageous abnormalities relating to their teeth, horns and the wormian bones in the skull. Another study reported an increase in the size of the hybrid as compared to either of its parents. In some animals the auditory bullae are highly deformed and in others the radius and ulna are fused.
Differences between species.
Both species of wildebeest are even-toed, horned, greyish-brown ungulates resembling cattle. Males are larger than females and both have heavy forequarters compared to their hindquarters. They have broad muzzles, Roman noses, shaggy manes and tails. The most striking morphological differences between the black and blue wildebeest are the orientation and curvature of their horns and the color of their coats. The blue wildebeest is the bigger of the two species. In males, blue wildebeest stand 150 cm tall at the shoulder and weigh around 250 kg, while the black wildebeest stands 111 to 120 cm tall and weighs about 180 kg. In females, blue wildebeest have a shoulder height of 135 cm and weigh 180 kg while black wildebeest females stand 108 cm at the shoulder and weigh 155 kg. The horns of blue wildebeest protrude to the side then curve downwards before curving up back towards the skull, while the horns of the black wildebeest curve forward then downward before curving upwards at the tips. Blue wildebeest tend to be a dark grey color with stripes, but may have a bluish sheen. The black wildebeest has brown-coloured hair, with a mane that ranges in color from cream to black, and a cream-coloured tail. The blue wildebeest lives in a wide variety of habitats, including woodlands and grasslands, while the black wildebeest tends to reside exclusively in open grassland areas. In some areas the blue wildebeest migrates over long distances in the winter, whereas the black wildebeest does not. The milk of the female black wildebeest contains a higher protein, lower fat, and lower lactose content than the milk of the blue wildebeest. Wildebeest can live more than forty years, though their average lifespan is around twenty years.
Distribution and habitat.
Wildebeest inhabit the plains and open woodlands of parts of Africa south of the Sahara. The black wildebeest is native to the southernmost parts of the continent. Its historical range included South Africa, Swaziland and Lesotho, but in the latter two countries it was hunted to extinction in the 19th century. It has now been reintroduced to them and also introduced to Namibia where it has become well established. It inhabits open plains, grasslands and Karoo shrublands in both steep mountainous regions and lower undulating hills at altitudes varying between 1350 and. In the past, it inhabited the highveld temperate grasslands during the dry winter season and the arid Karoo region during the rains. However, as a result of widespread hunting, it no longer occupies its historical range or makes migrations, and is now largely limited to game farms and protected reserves.
The blue wildebeest is native to eastern and southern Africa. Its range includes Kenya, Tanzania, Botswana, Zambia, Zimbabwe, Mozambique, South Africa, Swaziland and Angola. It is no longer found in Malawi but has been successfully reintroduced into Namibia. Blue wildebeest are mainly found in short grass plains bordering bush-covered acacia savannas, thriving in areas that are neither too wet nor too dry. They can be found in habitats that vary from overgrazed areas with dense bush to open woodland floodplains. In East Africa, the blue wildebeest is the most abundant big game species, both in population and biomass. It is a notable feature of the Serengeti National Park in Tanzania, the Masai Mara Game Reserve in Kenya and the Liuwa Plain National Park in Zambia.
Migration.
Not all wildebeest are migratory. Black wildebeest herds are often nomadic or may have a regular home range of 1 km2. Bulls may occupy territories, usually about 100 to apart, but this spacing varies according to the quality of the habitat. In favourable conditions they may be as close as 9 m or as far apart as 1600 m in poor habitat. Female herds have home ranges of about 250 acre in size. Herds of non-territorial batchelor males roam at will and do not seem to have a home range.
In the Masai Mara game reserve, there is a non-migratory population of blue wildebeest which had dwindled from about 119,000 animals in 1977 to about 22,000 in 1997. The reason for the decline is thought to be the increasing competition between cattle and wildebeest for a diminishing area of grazing land as a result of changes in agricultural practices, and possibly fluctuations in rainfall.
Each year, some East African populations of blue wildebeest have a long-distance migration, seemingly timed to coincide with the annual pattern of rainfall and grass growth. The timing of their migrations in both the rainy and dry seasons can vary considerably (by months) from year to year. At the end of the rainy season (May or June in East Africa), wildebeest migrate to dry-season areas in response to a lack of surface (drinking) water. When the rainy season begins again (months later), animals quickly move back to their wet-season ranges. Factors suspected to affect migration include food abundance, surface water availability, predators, and phosphorus content in grasses. Phosphorus is a crucial element for all life forms, particularly for lactating female bovids. As a result during the rainy season, wildebeest select grazing areas that contain particularly high phosphorus levels. One study found, in addition to phosphorus, wildebeest select ranges containing grass with relatively high nitrogen content.
Numerous documentaries feature wildebeest crossing rivers, with many being eaten by crocodiles or drowning in the attempt. While having the appearance of a frenzy, recent research has shown a herd of wildebeest possesses what is known as a "swarm intelligence", whereby the animals systematically explore and overcome the obstacle as one. Major predators that feed on wildebeest include the lion, hyena, cheetah, leopard, and crocodile, which seem to favour the wildebeest. Wildebeest, however, are very strong, and can inflict considerable injury even to a lion. Wildebeest have a maximum running speed of around 80 km/h. The primary defensive tactic is herding, where the young animals are protected by the older, larger ones, while the herd runs as a group. Typically, the predators attempt to cut out a young or ill animal and attack without having to worry about the herd. Wildebeest have developed additional sophisticated cooperative behaviours, such as animals taking turns sleeping while others stand guard against a night attack by invading predators. Wildebeest migrations are closely followed by vultures, as wildebeest carcasses are an important source of food for these scavengers. The vultures consume about 70% of the wildebeest carcasses available. Decreases in the number of migrating wildebeest have also had a negative effect on the vultures. In the Serengeti ecosystem, Tanzania, wildebeest may help facilitate the migration of other, smaller-bodied grazers, such as Thomson's gazelles ("Eudorcas thomsonii"), which eat the new-growth grasses stimulated by wildebeest foraging.
Ecology.
Interactions with nonpredators.
Zebras and wildebeest group together in open savannah environments with high chances of predation. This grouping strategy reduces predation risk because larger groups decrease each individual’s chance of being hunted, and predators are more easily seen in open areas.
Wildebeest can also listen in on the alarm calls of other species, and by doing so can reduce their risk of predation. One study showed, along with other ungulates, wildebeests responded more strongly to the baboon alarm calls compared to the baboon contest calls, though both types of calls had similar patterns, amplitudes, and durations. The alarm calls were a response of the baboons to lions, and the contest calls were recorded when a dispute between two males occurred.
Breeding and reproduction.
Wildebeest do not form permanent pair bonds and during the mating season, or rut, the males establish temporary territories and try to attract females into them. These small territories are about 3000 m2, with up to 300 territories per km2. The males defend these territories from other males while competing for females that are coming into season. The males use grunts and distinctive behaviour to entice females into their territories. Wildebeest usually breed at the end of the rainy season when the animals are well fed and at their peak of fitness. This usually occurs between May and July, and birthing usually takes place between January and March, at the start of the wet season. Wildebeest females breed seasonally and ovulate spontaneously. The estrous cycle is about 23 days and the gestation period lasts 250 to 260 days. The calves weigh about 21 kg at birth and scramble to their feet within minutes, being able to move with the herd in a matter of days. Groups of wildebeest females and young live in the small areas established by the male. When groups of wildebeest join together, the female to male ratio is higher because the females choose to move to the areas held by a smaller number of males. This female-dominated sex ratio may be due to illegal hunting and human disturbance as higher male mortality has been attributed to hunting.
Threats and conservation.
Today many wildebeest populations are experiencing rapid declines. Overland migration as a biological process requires large connected landscapes, which are increasingly difficult to maintain, particularly over the long term, when human demands on the landscape compete, as well. The most acute threat comes from migration barriers, such as fences and roads. In one of the more striking examples of the consequences of fence-building on terrestrial migrations, Botswanan authorities placed thousands of kilometres of fences across the Kalahari that prevented wildebeests from reaching watering holes and grazing grounds, resulting in the deaths of tens of thousands of individuals, reducing the wildebeest population to less than 10% of its previous size. Illegal hunting is a major conservation concern in many areas, along with natural threats posed by main predators (such as lions, leopards, hunting dogs and hyenas). Where the black and blue wildebeest share a common range, the two can hybridise, and this is regarded as a potential threat to the black wildebeest.
The black wildebeest has been classified as of "Least Concern" by the International Union for Conservation of Nature (IUCN), in its Red List of Threatened Species. The populations of this species are on an increase. There are now believed to be more than 18,000 individuals, 7,000 of which are in Namibia, outside its natural range, and where it is farmed. Around 80% of the wildebeest occur in private areas, while the other 20% are confined in protected areas. Its introduction into Namibia has been a success and numbers have increased substantially there from 150 in 1982 to 7,000 in 1992.
The blue wildebeest has also been rated as being of "Least Concern". The population trend is stable, and their numbers are estimated to be around 1,500,000 - mainly due to the increase of the populations in Serengeti National Park (Tanzania) to 1,300,000. However, the numbers of one of the subspecies, the Eastern white-bearded wildebeest ("C. t. albojubatus") have seen a steep decline. Population density ranges from 0.15/km2. in Hwange and Etosha National Parks to 35/km2. in Ngorongoro Crater and Serengeti National Park.
Uses and interaction with humans.
Wildebeest provide several useful animal products. The hide makes good quality leather and the flesh is coarse, dry and rather hard. Wildebeest are killed for food, especially to make biltong in Southern Africa. This dried game meat is a delicacy and an important food item in Africa. The meat of females is more tender than that of males, and is the most tender during the autumn season. Wildebeest are a regular target for illegal meat hunters because their numbers make them easy to find. Cooks preparing the wildebeest carcass usually cut it into 11 pieces. The estimated price for wildebeest meat was about US$0.47 per kilogram around 2008. The silky, flowing tail of the black wildebeest is used to make fly-whisks or "chowries".
The wildebeest benefit the ecosystem by increasing soil fertility with their excreta. Now they are economically important for human beings as they are a major tourist attraction. They also provide important products like leather to humans. The wildebeest, however, can also have a negative impact on humans. Wild individuals can be competitors of commercial livestock, and can transmit fatal diseases like rinderpest and cause epidemics among animals, particularly domestic cattle. They can also spread ticks, lungworms, tapeworms, flies and paramphistome flukes.
The black wildebeest is depicted on the coat of arms of the Province of Natal in South Africa. Over the years the South African authorities have issued several stamps displaying the animal and the South African Mint has struck a two cent piece with a prancing black wildebeest.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="34175" url="http://en.wikipedia.org/wiki?curid=34175" title="Aveh">
Aveh

Aveh may refer to:

</doc>
<doc id="34193" url="http://en.wikipedia.org/wiki?curid=34193" title="XP">
XP

XP may refer to:

</doc>
<doc id="34195" url="http://en.wikipedia.org/wiki?curid=34195" title="XyWrite">
XyWrite

XyWrite is a word processor for DOS and Windows modeled on the mainframe-based ATEX typesetting system. Popular with writers and editors for its speed and degree of customization, XyWrite was in its heyday the house word processor in many editorial offices, including the "New York Times" from 1989 to 1993. XyWrite was developed by David Erickson and marketed by XyQuest from 1982 through 1992, after which it was acquired by The Technology Group. The final version for DOS was 4.18 (1993); for Windows, 4.13.
History and current usage.
XyQuest was founded in June 1982 by former ATEX employees Dave Erickson and John Hild. Its most successful product was XyWrite III Plus, which attracted a devoted following among professional writers.
The turning point for XyWrite came in the form of a disastrous near-partnership with IBM, which was seeking a modern replacement for its venerable DisplayWrite word processor. Working under an agreement signed in June 1990, XyQuest devoted nearly all of its development resources to revising Erickson's XyWrite IV to IBM's specifications, including IBM Common User Access-style menus, mouse support and a graphical user interface. Envisioned as a marriage between XyQuest technology and IBM marketing, the product was to be called Signature.
But on the eve of Signature's release, IBM announced a strategic decision to withdraw completely from the desktop software market, shocking XyQuest and leaving Signature in limbo. When a prospective new alliance with Lotus did not materialize, XyQuest had no alternative but to resticker the ready-to-ship Signature packages as XyWrite 4.0 and attempt to carry on.
However, the changes IBM had insisted on were a liability where the III Plus user base was concerned. Some key reviews (such as in "The Wall Street Journal") were harsh, and there were complaints that 4.0 was buggy and slow. Moreover, in the years since the last major XyWrite release, WordPerfect had cemented its hold on the DOS word processor market. Already financially strained by the long development cycle for Signature, by the end of 1992 XyQuest was bleeding money. The sale to The Technology Group ensued.
While there were a few maintenance releases of 4.0 after the acquisition, The Technology Group's major commitment was to developing XyWrite for Windows. But XyWrite remained a niche product, unable to compete for the business user against Word for Windows, WordPerfect for Windows, and Ami Pro, despite added versatility and customization potential. The Technology Group was dissolved in 2003.
Several versions of XyWrite for DOS and Windows were also localized for use in European countries. For example, the programs were offered in Germany under the name "euroscript" by North American Software GmbH.
A descendant of XyWrite called Nota Bene (word processor) is still being actively developed. Nota Bene, which runs on the XyWrite engine, is popular among academics. Nota Bene for Windows is now in version 10.
Thanks in large part to the work of users of XyWrite, the program is still very usable with Windows (or MS-DOS, and thus Linux). Even on Pentium and similar hardware, it remains noticeably faster than MS Word or OpenOffice.org. Despite these advantages in speed, XyWrite does not have as many features as Word or OpenOffice.org. For example, XyWrite is unaware of Windows ANSI or Unicode character sets and Nota Bene does not support languages (such as Chinese) that require double-byte characters.
Reception.
"BYTE" in 1984 stated "the XyQuest people have done an admirable job porting the editing part of the Atex system" to the IBM PC. While criticizing the documentation, it called XyWrite "extremely fast, powerful, compact, and flexible".

</doc>
<doc id="34197" url="http://en.wikipedia.org/wiki?curid=34197" title="X-ray">
X-ray

X-radiation (composed of X-rays) is a form of electromagnetic radiation. Most X-rays have a wavelength ranging from 0.01 to 10 nanometers, corresponding to frequencies in the range 30 petahertz to 30 exahertz (3×1016 Hz to 3×1019 Hz) and energies in the range 100 eV to 100 keV. X-ray wavelengths are shorter than those of UV rays and typically longer than those of gamma rays. In many languages, X-radiation is referred to with terms meaning Röntgen radiation, after Wilhelm Röntgen, who is usually credited as its discoverer, and who had named it "X-radiation" to signify an unknown type of radiation. Spelling of "X-ray(s)" in the English language includes the variants "x-ray(s)", "xray(s)" and "X ray(s)".
X-rays with photon energies above 5–10 keV (below 0.2–0.1 nm wavelength) are called "hard X-rays", while those with lower energy are called "soft X-rays". Due to their penetrating ability, hard X-rays are widely used to image the inside of objects, e.g., in medical radiography and airport security. As a result, the term "X-ray" is metonymically used to refer to a radiographic image produced using this method, in addition to the method itself. Since the wavelengths of hard X-rays are similar to the size of atoms they are also useful for determining crystal structures by X-ray crystallography. By contrast, soft X-rays are easily absorbed in air and the attenuation length of 600 eV (~2 nm) X-rays in water is less than 1 micrometer.
There is no universal consensus for a definition distinguishing between X-rays and gamma rays. One common practice is to distinguish between the two types of radiation based on their source: X-rays are emitted by electrons, while gamma rays are emitted by the atomic nucleus. This definition has several problems; other processes also can generate these high energy photons, or sometimes the method of generation is not known. One common alternative is to distinguish X- and gamma radiation on the basis of wavelength (or equivalently, frequency or photon energy), with radiation shorter than some arbitrary wavelength, such as 10−11 m (0.1 Å), defined as gamma radiation.
This criterion assigns a photon to an unambiguous category, but is only possible if wavelength is known. (Some measurement techniques do not distinguish between detected wavelengths.) However, these two definitions often coincide since the electromagnetic radiation emitted by X-ray tubes generally has a longer wavelength and lower photon energy than the radiation emitted by radioactive nuclei.
Occasionally, one term or the other is used in specific contexts due to historical precedent, based on measurement (detection) technique, or based on their intended use rather than their wavelength or source.
Thus, gamma-rays generated for medical and industrial uses, for example radiotherapy, in the ranges of 6–20 MeV, can in this context also be referred to as X-rays.
Properties.
X-ray photons carry enough energy to ionize atoms and disrupt molecular bonds. This makes it a type of ionizing radiation, and therefore harmful to living tissue. A very high radiation dose over a short amount of time causes radiation sickness, while lower doses can give an increased risk of radiation-induced cancer. In medical imaging this increased cancer risk is generally greatly outweighed by the benefits of the examination. The ionizing capability of X-rays can be utilized in cancer treatment to kill malignant cells using radiation therapy. It is also used for material characterization using X-ray spectroscopy.
Hard X-rays can traverse relatively thick objects without being much absorbed or scattered. For this reason, X-rays are widely used to image the inside of visually opaque objects. The most often seen applications are in medical radiography and airport security scanners, but similar techniques are also important in industry (e.g. industrial radiography and industrial CT scanning) and research (e.g. small animal CT). The penetration depth varies with several orders of magnitude over the X-ray spectrum. This allows the photon energy to be adjusted for the application so as to give sufficient transmission through the object and at the same time good contrast in the image.
X-rays have much shorter wavelength than visible light, which makes it possible to probe structures much smaller than what can be seen using a normal microscope. This can be used in X-ray microscopy to acquire high resolution images, but also in X-ray crystallography to determine the positions of atoms in crystals.
Interaction with matter.
X-rays interact with matter in three main ways, through photoabsorption, Compton scattering, and Rayleigh scattering. The strength of these interactions depend on the energy of the X-rays and the elemental composition of the material, but not much on chemical properties since the X-ray photon energy is much higher than chemical binding energies. Photoabsorption or photoelectric absorption is the dominant interaction mechanism in the soft X-ray regime and for the lower hard X-ray energies. At higher energies, Compton scattering dominates.
Photoelectric absorption.
The probability of a photoelectric absorption per unit mass is approximately proportional to "Z"3/"E"3, where "Z" is the atomic number and "E" is the energy of the incident photon. This rule is not valid close to inner shell electron binding energies where there are abrupt changes in interaction probability, so called absorption edges. However, the general trend of high absorption coefficients and thus short penetration depths for low photon energies and high atomic numbers is very strong. For soft tissue photoabsorption dominates up to about 26 keV photon energy where Compton scattering takes over. For higher atomic number substances this limit is higher. The high amount of calcium ("Z"=20) in bones together with their high density is what makes them show up so clearly on medical radiographs.
A photoabsorbed photon transfers all its energy to the electron with which it interacts, thus ionizing the atom to which the electron was bound and producing a photoelectron that is likely to ionize more atoms in its path. An outer electron will fill the vacant electron position and the produce either a characteristic photon or an Auger electron. These effects can be used for elemental detection through X-ray spectroscopy or Auger electron spectroscopy.
Compton scattering.
Compton scattering is the predominant interaction between X-rays and soft tissue in medical imaging. Compton scattering is an inelastic scattering of the X-ray photon by an outer shell electron. Part of the energy of the photon is transferred to the scattering electron, thereby ionizing the atom and increasing the wavelength of the X-ray. The scattered photon can go in any direction, but a direction similar to the original direction is a bit more likely, especially for high-energy X-rays. The probability for different scattering angles are described by the Klein–Nishina formula. The transferred energy can be directly obtained from the scattering angle from the conservation of energy and momentum.
Rayleigh scattering.
Rayleigh scattering is the dominant elastic scattering mechanism in the X-ray regime. The inelastic forward scattering is what gives rise to the refractive index, which for X-rays is only slightly below 1.
Production.
Whenever charged particles (electrons or ions) of sufficient energy hit a material, x-rays are produced.
Production by electrons.
X-rays can be generated by an X-ray tube, a vacuum tube that uses a high voltage to accelerate the electrons released by a hot cathode to a high velocity. The high velocity electrons collide with a metal target, the anode, creating the X-rays. In medical X-ray tubes the target is usually tungsten or a more crack-resistant alloy of rhenium (5%) and tungsten (95%), but sometimes molybdenum for more specialized applications, such as when softer X-rays are needed as in mammography. In crystallography, a copper target is most common, with cobalt often being used when fluorescence from iron content in the sample might otherwise present a problem.
The maximum energy of the produced X-ray photon is limited by the energy of the incident electron, which is equal to the voltage on the tube times the electron charge, so an 80 kV tube cannot create X-rays with an energy greater than 80 keV. When the electrons hit the target, X-rays are created by two different atomic processes:
So the resulting output of a tube consists of a continuous bremsstrahlung spectrum falling off to zero at the tube voltage, plus several spikes at the characteristic lines. The voltages used in diagnostic X-ray tubes range from roughly 20 to 150 kV and thus the highest energies of the X-ray photons range from roughly 20 to 150 keV.
Both of these X-ray production processes are inefficient, with a production efficiency of only about one percent, and hence, to produce a usable flux of X-rays, most of the electric power consumed by the tube is released as waste heat. The X-ray tube must be designed to dissipate this excess heat.
Short nanosecond bursts of X-rays peaking at 15-keV in energy may be reliably produced by peeling pressure-sensitive adhesive tape from its backing in a moderate vacuum. This is likely to be the result of recombination of electrical charges produced by triboelectric charging. The intensity of X-ray triboluminescence is sufficient for it to be used as a source for X-ray imaging. Using sources considerably more advanced than sticky tape, at least one startup firm is exploiting tribocharging in the development of highly portable, ultra-miniaturized X-ray devices.
A specialized source of X-rays which is becoming widely used in research is synchrotron radiation, which is generated by particle accelerators. Its unique features are X-ray outputs many orders of magnitude greater than those of X-ray tubes, wide X-ray spectra, excellent collimation, and linear polarization.
Production by fast positive ions.
X-rays can also be produced by fast protons or other positive ions. The Proton-induced X-ray emission or Particle-induced X-ray emission is widely used as an analytical procedure. For high energies, the production cross section is proportional to "Z12Z2−4", where "Z1" refers to the atomic number of the ion, "Z2" to that of the target atom.
An overview of these cross sections is given in the same reference.
Detectors.
X-ray detectors vary in shape and function depending on their purpose. Imaging detectors such as those used for radiography were originally based on photographic plates and later photographic film but are now mostly replaced by various digital detector types such as image plates or flat panel detectors. For radiation protection direct exposure hazard is often evaluated using ionization chambers, while dosimeters are used to measure the radiation dose a person has been exposed to. X-ray spectra can be measured either by energy dispersive or wavelength dispersive spectrometers.
Medical uses.
Since Röntgen's discovery that X-rays can identify bone structures, X-rays have been used for medical imaging. The first medical use was less than a month after his paper on the subject. Up until 2010, 5 billion medical imaging studies have been conducted worldwide. Radiation exposure from medical imaging in 2006 made up about 50% of total ionizing radiation exposure in the United States.
Radiographs.
A radiograph is an X-ray image obtained by placing a part of the patient in front of an X-ray detector and then illuminating it with a short X-ray pulse. Bones contain much calcium, which due to its relatively high atomic number absorbs x-rays efficiently. This reduces the amount of X-rays reaching the detector in the shadow of the bones, making them clearly visible on the radiograph. The lungs and trapped gas also show up clearly because of lower absorption compared to tissue, while differences between tissue types are harder to see.
Radiographs are useful in the detection of pathology of the skeletal system as well as for detecting some disease processes in soft tissue. Some notable examples are the very common chest X-ray, which can be used to identify lung diseases such as pneumonia, lung cancer or pulmonary edema, and the abdominal x-ray, which can detect bowel (or intestinal) obstruction, free air (from visceral perforations) and free fluid (in ascites). X-rays may also be used to detect pathology such as gallstones (which are rarely radiopaque) or kidney stones which are often (but not always) visible. Traditional plain X-rays are less useful in the imaging of soft tissues such as the brain or muscle.
Dental radiography is commonly used in the diagnoses of common oral problems, such as cavities.
In medical diagnostic applications, the low energy (soft) X-rays are unwanted, since they are totally absorbed by the body, increasing the radiation dose without contributing to the image. Hence, a thin metal sheet, often of aluminium, called an X-ray filter, is usually placed over the window of the X-ray tube, absorbing the low energy part in the spectrum. This is called "hardening" the beam since it shifts the center of the spectrum towards higher energy (or harder) x-rays.
To generate an image of the cardiovascular system, including the arteries and veins (angiography) an initial image is taken of the anatomical region of interest. A second image is then taken of the same region after an iodinated contrast agent has been injected into the blood vessels within this area. These two images are then digitally subtracted, leaving an image of only the iodinated contrast outlining the blood vessels. The radiologist or surgeon then compares the image obtained to normal anatomical images to determine if there is any damage or blockage of the vessel.
Computed tomography.
Computed tomography (CT scanning) is a medical imaging modality where tomographic images or slices of specific areas of the body are obtained from a large series of two-dimensional X-ray images taken in different directions. These cross-sectional images can be combined into a three-dimensional image of the inside of the body and used for diagnostic and therapeutic purposes in various medical disciplines.
Fluoroscopy.
Fluoroscopy is an imaging technique commonly used by physicians or radiation therapists to obtain real-time moving images of the internal structures of a patient through the use of a fluoroscope. In its simplest form, a fluoroscope consists of an X-ray source and fluorescent screen between which a patient is placed. However, modern fluoroscopes couple the screen to an X-ray image intensifier and CCD video camera allowing the images to be recorded and played on a monitor. This method may use a contrast material. Examples include cardiac catheterization (to examine for coronary artery blockages) and barium swallow (to examine for esophageal disorders).
Radiotherapy.
The use of X-rays as a treatment is known as radiation therapy and is largely used for the management (including palliation) of cancer; it requires higher radiation doses than those received for imaging alone. X-rays beams are used for treating skin cancers using lower energy x-ray beams while higher energy beams are used for treating cancers within the body such as brain, lung, prostate and breast.
Adverse effects.
Diagnostic X-rays (primarily from CT scans due to the large dose used) increase the risk of developmental problems and cancer in those exposed. X rays are classified as a carcinogen by both the World Health Organization's International Agency for Research on Cancer and the U.S. government. It is estimated that 0.4% of current cancers in the United States are due to computed tomography (CT scans) performed in the past and that this may increase to as high as 1.5-2% with 2007 rates of CT usage.
Experimental and epidemiological data currently do not support the proposition that there is a threshold dose of radiation below which there is no increased risk of cancer. However, this is under increasing doubt. It is estimated that the additional radiation will increase a person's cumulative risk of getting cancer by age 75 by 0.6–1.8%. The amount of absorbed radiation depends upon the type of X-ray test and the body part involved. CT and fluoroscopy entail higher doses of radiation than do plain X-rays.
To place the increased risk in perspective, a plain chest X-ray will expose a person to the same amount from background radiation that we are exposed to (depending upon location) every day over 10 days, while exposure from a dental X-ray is approximately equivalent to 1 day of environmental background radiation. Each such X-ray would add less than 1 per 1,000,000 to the lifetime cancer risk. An abdominal or chest CT would be the equivalent to 2–3 years of background radiation to the whole body, or 4–5 years to the abdomen or chest, increasing the lifetime cancer risk between 1 per 1,000 to 1 per 10,000. This is compared to the roughly 40% chance of a US citizen developing cancer during their lifetime. For instance, the effective dose to the torso from a CT scan of the chest is about 5 mSv, and the absorbed dose is about 14 mGy. A head CT scan (1.5mSv, 64mGy) that is performed once with and once without contrast agent, would be equivalent to 40 years of background radiation to the head. Accurate estimation of effective doses due to CT is difficult with the estimation uncertainty range of about ±19% to ±32% for adult head scans depending upon the method used.
The risk of radiation is greater to unborn babies, so in pregnant patients, the benefits of the investigation (X-ray) should be balanced with the potential hazards to the unborn fetus. In the US, there are an estimated 62 million CT scans performed annually, including more than 4 million on children. Avoiding unnecessary X-rays (especially CT scans) will reduce radiation dose and any associated cancer risk.
Medical X-rays are a significant source of man-made radiation exposure. In 1987, they accounted for 58% of exposure from man-made sources in the United States. Since man-made sources accounted for only 18% of the total radiation exposure, most of which came from natural sources (82%), medical X-rays only accounted for 10% of "total" American radiation exposure; medical procedures as a whole (including nuclear medicine) accounted for 14% of total radiation exposure. By 2006, however, medical procedures in the United States were contributing much more ionizing radiation than was the case in the early 1980s. In 2006, medical exposure constituted nearly half of the total radiation exposure of the U.S. population from all sources. The increase is traceable to the growth in the use of medical imaging procedures, in particular computed tomography (CT), and to the growth in the use of nuclear medicine.
Dosage due to dental X-rays varies significantly depending on the procedure and the technology (film or digital). Depending on the procedure and the technology, a single dental X-ray of a human results in an exposure of 0.5 to 4 mrem. A full mouth series may therefore result in an exposure of up to 6 (digital) to 18 (film) mrem, for a yearly average of up to 40 mrem.
Other uses.
Other notable uses of X-rays include
History.
Discovery.
German physicist Wilhelm Röntgen is usually credited as the discoverer of X-rays in 1895, because he was the first to systematically study them, though he is not the first to have observed their effects. He is also the one who gave them the name "X-rays" (signifying an unknown quantity) though many others referred to these as "Röntgen rays" (and the associated X-ray radiograms as, "Röntgenograms") for several decades after their discovery and even to this day in some languages, including Röntgen's native German.
X-rays were found emanating from Crookes tubes, experimental discharge tubes invented around 1875, by scientists investigating the cathode rays, that is energetic electron beams, that were first created in the tubes. Crookes tubes created free electrons by ionization of the residual air in the tube by a high DC voltage of anywhere between a few kilovolts and 100 kV. This voltage accelerated the electrons coming from the cathode to a high enough velocity that they created X-rays when they struck the anode or the glass wall of the tube. Many of the early Crookes tubes undoubtedly radiated X-rays, because early researchers noticed effects that were attributable to them, as detailed below. Wilhelm Röntgen was the first to systematically study them, in 1895.
Early research.
Both William Crookes (in the 1880s) and German physicist Johann Hittorf, a co-inventor and early researcher of the Crookes tube, found that photographic plates placed near the tube became unaccountably fogged or flawed by shadows. Neither found the cause nor investigated this effect.
In 1877 Ukrainian-born Ivan Pulyui, a lecturer in experimental physics at the University of Vienna, constructed various designs of vacuum discharge tube to investigate their properties. He continued his investigations when appointed professor at the Prague Polytechnic and in 1886 he found that sealed photographic plates became dark when exposed to the emanations from the tubes. Early in 1896, just a few weeks after Röntgen published his first X-ray photograph, Pulyui published high-quality X-ray images in journals in Paris and London. Although Pulyui had studied with Röntgen at the University of Strasbourg in the years 1873–75, his biographer Gaida (1997) asserts that his subsequent research was conducted independently.
X-rays were generated and detected by Fernando Sanford (1854–1948), the foundation Professor of Physics at Stanford University, in 1891. From 1886 to 1888 he had studied in the Hermann Helmholtz laboratory in Berlin, where he became familiar with the cathode rays generated in vacuum tubes when a voltage was applied across separate electrodes, as previously studied by Heinrich Hertz and Philipp Lenard. His letter of January 6, 1893 (describing his discovery as "electric photography") to The Physical Review was duly published and an article entitled "Without Lens or Light, Photographs Taken With Plate and Object in Darkness" appeared in the San Francisco Examiner.
Starting in 1888, Philipp Lenard, a student of Heinrich Hertz, conducted experiments to see whether cathode rays could pass out of the Crookes tube into the air. He built a Crookes tube (later called a "Lenard tube") with a "window" in the end made of thin aluminum, facing the cathode so the cathode rays would strike it. He found that something came through, that would expose photographic plates and cause fluorescence. He measured the penetrating power of these rays through various materials. It has been suggested that at least some of these "Lenard rays" were actually X-rays.
Hermann von Helmholtz formulated mathematical equations for X-rays. He postulated a dispersion theory before Röntgen made his discovery and announcement. It was formed on the basis of the electromagnetic theory of light. However, he did not work with actual X-rays.
In 1894 Nikola Tesla noticed damaged film in his lab that seemed to be associated with Crookes tube experiments and began investigating this "radiant energy of "invisible" kinds". After Röntgen identified the x-ray Tesla began making X-ray images of his own using high voltages and tubes of his own design, as well as Crookes tubes.
Wilhelm Röntgen.
On November 8, 1895, German physics professor Wilhelm Röntgen stumbled on X-rays while experimenting with Lenard and Crookes tubes and began studying them. He wrote an initial report "On a new kind of ray: A preliminary communication" and on December 28, 1895 submitted it to the Würzburg's Physical-Medical Society journal. This was the first paper written on X-rays. Röntgen referred to the radiation as "X", to indicate that it was an unknown type of radiation. The name stuck, although (over Röntgen's great objections) many of his colleagues suggested calling them Röntgen rays. They are still referred to as such in many languages, including German, Danish, Polish, Swedish, Finnish, Estonian, Russian, Japanese, Dutch, and Norwegian. Röntgen received the first Nobel Prize in Physics for his discovery.
There are conflicting accounts of his discovery because Röntgen had his lab notes burned after his death, but this is a likely reconstruction by his biographers: Röntgen was investigating cathode rays using a fluorescent screen painted with barium platinocyanide and a Crookes tube which he had wrapped in black cardboard so the visible light from the tube would not interfere. He noticed a faint green glow from the screen, about 1 meter away. Röntgen realized some invisible rays coming from the tube were passing through the cardboard to make the screen glow. He found they could also pass through books and papers on his desk. Röntgen threw himself into investigating these unknown rays systematically. Two months after his initial discovery, he published his paper.
Röntgen discovered its medical use when he made a picture of his wife's hand on a photographic plate formed due to X-rays. The photograph of his wife's hand was the first photograph of a human body part using X-rays. When she saw the picture, she said "I have seen my death."
Advances in radiology.
In 1895, Thomas Edison investigated materials' ability to fluoresce when exposed to X-rays, and found that calcium tungstate was the most effective substance. Around March 1896, the fluoroscope he developed became the standard for medical X-ray examinations. Nevertheless, Edison dropped X-ray research around 1903, even before the death of Clarence Madison Dally, one of his glassblowers. Dally had a habit of testing X-ray tubes on his hands, and acquired a cancer in them so tenacious that both arms were amputated in a futile attempt to save his life.
In 1901, U.S. President William McKinley was shot twice in an assassination attempt. While one bullet only grazed his sternum, another had lodged somewhere deep inside his abdomen and could not be found. "A worried McKinley aide sent word to inventor Thomas Edison to rush an X-ray machine to Buffalo to find the stray bullet. It arrived but wasn't used." While the shooting itself had not been lethal, "gangrene had developed along the path of the bullet, and McKinley died of septic shock due to bacterial infection" six days later.
The first use of X-rays under clinical conditions was by John Hall-Edwards in Birmingham, England on 11 January 1896, when he radiographed a needle stuck in the hand of an associate. On 14 February 1896 Hall-Edwards was also the first to use X-rays in a surgical operation. In early 1896, several weeks after Röntgen's discovery, Ivan Romanovich Tarkhanov irradiated frogs and insects with X-rays, concluding that the rays "not only photograph, but also affect the living function".
The first medical X-ray made in the United States was obtained using a discharge tube of Pulyui's design. In January 1896, on reading of Röntgen's discovery, Frank Austin of Dartmouth College tested all of the discharge tubes in the physics laboratory and found that only the Pulyui tube produced X-rays. This was a result of Pulyui's inclusion of an oblique "target" of mica, used for holding samples of fluorescent material, within the tube. On 3 February 1896 Gilman Frost, professor of medicine at the college, and his brother Edwin Frost, professor of physics, exposed the wrist of Eddie McCarthy, whom Gilman had treated some weeks earlier for a fracture, to the X-rays and collected the resulting image of the broken bone on gelatin photographic plates obtained from Howard Langill, a local photographer also interested in Röntgen's work.
Dangers.
With the widespread experimentation with x‑rays after their discovery in 1895 by scientists, physicians, and inventors came many stories of burns, hair loss and worse in technical journals of the time. In February 1896 Professor John Daniel and Dr. William Lofland Dudley of Vanderbilt University reported hair loss after Dr. Dudley was X-rayed. In August 1896 Dr. H/D. Hawks, a graduate of Columbia College, suffered severe hand and chest burns in an x-ray demonstration. It was reported in "Electrical Review" and led to many other reports of problems associated with x-rays being sent in to the publication. Many experimenters including Elihu Thomson at Edison's lab, William J. Morton, and Nikola Tesla also reported burns. Elihu Thomson deliberately exposed a finger to an x-ray tube over a period of time and suffered pain, swelling, and blistering. Other effects were sometime blamed for the damage including ultraviolet rays and (according to Tesla) ozone. Many physicians claimed there were no effects from x-ray exposure at all.
20th century and beyond.
The many applications of X-rays immediately generated enormous interest. Workshops began making specialized versions of Crookes tubes for generating X-rays and these first generation cold cathode or Crookes X-ray tubes were used until about 1920.
Crookes tubes were unreliable. They had to contain a small quantity of gas (invariably air) as a current will not flow in such a tube if they are fully evacuated. However, as time passed the X-rays caused the glass to absorb the gas, causing the tube to generate "harder" X-rays until it soon stopped operating. Larger and more frequently used tubes were provided with devices for restoring the air, known as "softeners". These often took the form of a small side tube which contained a small piece of mica: a mineral that traps relatively large quantities of air within its structure. A small electrical heater heated the mica and this caused it to release a small amount of air, thus restoring the tube's efficiency. However, the mica had a limited life, and the restoration process was consequently difficult to control.
In 1904, John Ambrose Fleming invented the thermionic diode, the first kind of a vacuum tube. This used a hot cathode that caused an electric current to flow in a vacuum. This idea was quickly applied to X-ray tubes, and hence heated-cathode X-ray tubes, called "Coolidge tubes", completely replaced the troublesome cold cathode tubes by about 1920.
In about 1906, the physicist Charles Barkla discovered that X-rays could be scattered by gases, and that each element had a characteristic X-ray. He won the 1917 Nobel Prize in Physics for this discovery.
In 1912, Max von Laue, Paul Knipping, and Walter Friedrich first observed the diffraction of X-rays by crystals. This discovery, along with the early work of Paul Peter Ewald, William Henry Bragg, and William Lawrence Bragg, gave birth to the field of X-ray crystallography.
The Coolidge X-ray tube was invented during the following year by William D. Coolidge. It made possible the continuous emissions of X-rays. X-ray tubes similar to this are still in use in 2012.
The use of X-rays for medical purposes (which developed into the field of radiation therapy) was pioneered by Major John Hall-Edwards in Birmingham, England. Then in 1908, he had to have his left arm amputated because of the spread of X-ray dermatitis on his arm.
The X-ray microscope was developed during the 1950s.
The Chandra X-ray Observatory, launched on July 23, 1999, has been allowing the exploration of the very violent processes in the universe which produce X-rays. Unlike visible light, which gives a relatively stable view of the universe, the X-ray universe is unstable. It features stars being torn apart by black holes, galactic collisions, and novae or neutron stars that build up layers of plasma that then explode into space.
An X-ray laser device was proposed as part of the Reagan Administration's Strategic Defense Initiative in the 1980s, but the only test of the device (a sort of laser "blaster", or death ray, powered by a thermonuclear explosion) gave inconclusive results. For technical and political reasons, the overall project (including the X-ray laser) was de-funded (though was later revived by the second Bush Administration as National Missile Defense using different technologies).
Phase-contrast X-ray imaging refers to a variety of techniques that use phase information of a coherent x-ray beam to image soft tissues. It has become an important method for visualizing cellular and histological structures in a wide range of biological and medical studies. There are several technologies being used for x-ray phase-contrast imaging, all utilizing different principles to convert phase variations in the x-rays emerging from an object into intensity variations. These include propagation-based phase contrast, talbot interferometry, refraction-enhanced imaging, and x-ray interferometry. These methods provide higher contrast compared to normal absorption-contrast x-ray imaging, making it possible to see smaller details. A disadvantage is that these methods require more sophisticated equipment, such as synchrotron or microfocus x-ray sources, x-ray optics and high resolution x-ray detectors.
Visibility.
While generally considered invisible to the human eye, in special circumstances X-rays can be visible. Brandes, in an experiment a short time after Röntgen's landmark 1895 paper, reported after dark adaptation and placing his eye close to an X-ray tube, seeing a faint "blue-gray" glow which seemed to originate within the eye itself. Upon hearing this, Röntgen reviewed his record books and found he too had seen the effect. When placing an X-ray tube on the opposite side of a wooden door Röntgen had noted the same blue glow, seeming to emanate from the eye itself, but thought his observations to be spurious because he only saw the effect when he used one type of tube. Later he realized that the tube which had created the effect was the only one powerful enough to make the glow plainly visible and the experiment was thereafter readily repeatable. The knowledge that X-rays are actually faintly visible to the dark-adapted naked eye has largely been forgotten today; this is probably due to the desire not to repeat what would now be seen as a recklessly dangerous and potentially harmful experiment with ionizing radiation. It is not known what exact mechanism in the eye produces the visibility: it could be due to conventional detection (excitation of rhodopsin molecules in the retina), direct excitation of retinal nerve cells, or secondary detection via, for instance, X-ray induction of phosphorescence in the eyeball with conventional retinal detection of the secondarily produced visible light.
Though X-rays are otherwise invisible it is possible to see the ionization of the air molecules if the intensity of the X-ray beam is high enough. The beamline from the wiggler at the at ESRF is one example of such high intensity.
Units of measure and exposure.
The measure of X-rays ionizing ability is called the exposure:
However, the effect of ionizing radiation on matter (especially living tissue) is more closely related to the amount of energy deposited into them rather than the charge generated. This measure of energy absorbed is called the absorbed dose:
The equivalent dose is the measure of the biological effect of radiation on human tissue. For X-rays it is equal to the absorbed dose.

</doc>
<doc id="34225" url="http://en.wikipedia.org/wiki?curid=34225" title="Yahoo (Gulliver's Travels)">
Yahoo (Gulliver's Travels)

A Yahoo is a legendary being in the novel "Gulliver's Travels" (1726) by Jonathan Swift. 
Swift describes them as being filthy and with unpleasant habits, resembling human beings far too closely for the liking of protagonist Lemuel Gulliver, who finds the calm and rational society of intelligent horses, the Houyhnhnms, greatly preferable. The Yahoos are primitive creatures obsessed with "pretty stones" they find by digging in mud, thus representing the distasteful materialism and ignorant elitism Swift encountered in Britain. Hence the term "yahoo" has come to mean "a crude, brutish or obscenely coarse person".
American frontiersman Daniel Boone, who often used terms from "Gulliver's Travels", claimed that he killed a hairy giant that he called a Yahoo.
Yahoos were referred to in a letter sent by serial killer David Berkowitz to New York City police while committing the "Son of Sam" murders in 1976.

</doc>
<doc id="34238" url="http://en.wikipedia.org/wiki?curid=34238" title="Yunus Emre">
Yunus Emre

Yunus Emre (]) (1240?–1320?) was a Turkish poet and Sufi mystic. He has exercised immense influence on Turkish literature, from his own day until the present. Because Yunus Emre is, after Ahmet Yesevi and Sultan Walad, one of the first known poets to have composed works in the spoken Turkish of his own age and region rather than in Persian or Arabic, his diction remains very close to the popular speech of his contemporaries in Central and Western Anatolia. This is also the language of a number of anonymous folk-poets, folk-songs, fairy tales, riddles ("tekerlemeler"), and proverbs. 
Like the Oghuz "Book of Dede Korkut", an older and anonymous Central Asian epic, the Turkish folklore that inspired Yunus Emre in his occasional use of "tekerlemeler" as a poetic device had been handed down orally to him and his contemporaries. This strictly oral tradition continued for a long while.
Following the Mongolian invasion of Anatolia facilitated by the Sultanate of Rûm's defeat at the 1243 Battle of Köse Dağ, Islamic mystic literature thrived in Anatolia, and Yunus Emre became one of its most distinguished poets. Poems of Sultan Yunus Emre — despite being fairly simple on the surface — evidence his skill in describing quite abstruse mystical concepts in a clear way. He remains a popular figure in a number of countries, stretching from Azerbaijan to the Balkans, with seven different and widely dispersed localities disputing the privilege of having his tomb within their boundaries.
His poems, written in the tradition of Anatolian folk poetry, mainly concern divine love as well as human destiny:
Yunus Emre's portrait is depicted on the reverse of the Turkish 200 lira banknote issued in 2009.

</doc>
<doc id="34244" url="http://en.wikipedia.org/wiki?curid=34244" title="Yugoslavia">
Yugoslavia

Yugoslavia (Serbo-Croatian, Macedonian, Slovene: "Jugoslavija", Југославија), once spelled and called "Jugoslavia", was a country in Southeast Europe during most of the 20th century. It came into existence after World War I in 1918 under the name of Kingdom of Serbs, Croats and Slovenes by the merger of the provisional State of Slovenes, Croats and Serbs (itself formed from territories of the former Austro-Hungarian Empire) with the formerly independent Kingdom of Serbia and Kingdom of Montenegro. The Serbian royal House of Karađorđević became the Yugoslav royal dynasty. Yugoslavia gained international recognition on 13 July 1922 at the Conference of Ambassadors in Paris. The country was named after the South Slavic peoples and constituted their first union, following centuries in which the territories had been part of the Ottoman Empire and Austria-Hungary.
Renamed Kingdom of Yugoslavia on 3 October 1929, it was invaded by the Axis powers on 6 April 1941. In 1943, a Democratic Federal Yugoslavia was proclaimed by the Partisan resistance. In 1944, the king recognised it as the legitimate government, but in November 1945 the monarchy was abolished. Yugoslavia was renamed the Federal People's Republic of Yugoslavia in 1946, when a communist government was established. It acquired the territories of Istria, Rijeka, and Zadar from Italy. Partisan leader Josip Broz Tito ruled the country as president until his death in 1980. In 1963, the country was renamed again to the Socialist Federal Republic of Yugoslavia (SFRY).
The constituent six Socialist Republics that made up the country were Socialist Republic of Bosnia and Herzegovina, SR Croatia, SR Macedonia, SR Montenegro, SR Slovenia, and SR Serbia. Serbia contained two Socialist Autonomous Provinces, Vojvodina and Kosovo, which after 1974 were largely equal to the other members of the federation. After an economic and political crisis in the 1980s and the rise of nationalism, Yugoslavia broke up along its republics' borders, at first into five countries, leading to the Yugoslav Wars.
After the breakup, the republics of Serbia and Montenegro formed a reduced federation, the Federal Republic of Yugoslavia (FRY), which aspired to the status of sole legal successor to the SFRY, but those claims were opposed by the other former republics. Eventually, Serbia and Montenegro accepted the opinion of the Badinter Arbitration Committee about shared succession. Serbia and Montenegro themselves broke up in 2006 and became independent states, while Kosovo proclaimed independence in 2008.
Background.
The concept of "Yugoslavia", as a single state for all South Slavic peoples, emerged in the late 17th century and gained prominence through the Illyrian Movement of the 19th century. The name was created by the combination of the Slavic words "jug" (south) and "slaveni" (Slavs). Yugoslavia was the result of the Corfu Declaration, as a project of the Serbian Parliament in exile and the Serbian royal Karađorđević dynasty, who became the Yugoslav royal dynasty.
Kingdom of Yugoslavia.
The country was formed in 1918 immediately after World War I as the Kingdom of Serbs, Croats and Slovenes by union of the State of Slovenes, Croats and Serbs and the Kingdom of Serbia. It was commonly referred to at the time as the "Versailles state". Later, the government renamed the country leading to the first official use of "Yugoslavia" in 1929.
King Alexander.
On 20 June 1928 Serb deputy Puniša Račić shot at five members of the opposition Croatian Peasant Party in the National Assembly resulting in the death of two deputies on the spot and that of leader Stjepan Radić a few weeks later. On 6 January 1929 King Alexander I suspended the constitution, banned national political parties, assumed executive power and renamed the country Yugoslavia. He hoped to curb separatist tendencies and mitigate nationalist passions. He imposed a new constitution and relinquished his dictatorship in 1931. However, Alexander's policies later encountered opposition from other European powers stemming from developments in Italy and Germany, where Fascists and Nazis rose to power, and the Soviet Union, where Joseph Stalin became absolute ruler. None of these three regimes favored the policy pursued by Alexander I. In fact, Italy and Germany wanted to revise the international treaties signed after World War I, and the Soviets were determined to regain their positions in Europe and pursue a more active international policy.
Alexander attempted to create a centralised Yugoslavia. He decided to abolish Yugoslavia's historic regions, and new internal boundaries were drawn for provinces or banovinas. The banovinas were named after rivers. Many politicians were jailed or kept under police surveillance. The effect of Alexander's dictatorship was to further alienate the non-Serbs from the idea of unity. During his reign the flags of Yugoslav nations were banned. Communist ideas were banned also.
The king was assassinated in Marseille during an official visit to France in 1934 by Vlado Chernozemski, an experienced marksman from Ivan Mihailov's Internal Macedonian Revolutionary Organization with the cooperation of the Ustaše, a Croatian fascist revolutionary organisation. Alexander was succeeded by his eleven-year-old son Peter II and a regency council headed by his cousin, Prince Paul.
1934–1941.
The international political scene in the late 1930s was marked by growing intolerance between the principal figures, by the aggressive attitude of the totalitarian regimes and by the certainty that the order set up after World War I was losing its strongholds and its sponsors were losing their strength. Supported and pressured by Fascist Italy and Nazi Germany, Croatian leader Vladko Maček and his party managed the creation of the Banovina of Croatia (Autonomous Region with significant internal self-government) in 1939. The agreement specified that Croatia was to remain part of Yugoslavia, but it was hurriedly building an independent political identity in international relations. The entire kingdom was to be federalised but World War II stopped the fulfillment of those plans.
Prince Paul submitted to the fascist pressure and signed the Tripartite Pact in Vienna on 25 March 1941, hoping to still keep Yugoslavia out of the war. But this was at the expense of popular support for Paul's regency. Senior military officers were also opposed to the treaty and launched a coup d'état when the king returned on 27 March. Army General Dušan Simović seized power, arrested the Vienna delegation, exiled Paul, and ended the regency, giving 17-year-old King Peter full powers. Hitler then decided to attack Yugoslavia on 6 April 1941, followed immediately by an invasion of Greece where Mussolini had previously been repelled.
World War II.
At 5:12 am on 6 April 1941, German, Italian and Hungarian forces invaded Yugoslavia. The German Air Force ("Luftwaffe") bombed Belgrade and other major Yugoslav cities. On 17 April, representatives of Yugoslavia's various regions signed an armistice with Germany in Belgrade, ending 11 days of resistance against the invading German Army ("Wehrmacht Heer"). More than 300,000 Yugoslav officers and soldiers were taken prisoner.
The Axis Powers occupied Yugoslavia and split it up. The Independent State of Croatia was established as a Nazi satellite state, ruled by the fascist militia known as the Ustaše that came into existence in 1929, but was relatively limited in its activities until 1941. German troops occupied Bosnia and Herzegovina as well as part of Serbia and Slovenia, while other parts of the country were occupied by Bulgaria, Hungary, and Italy. From 1941–45, the Croatian Ustaše regime murdered around 500,000 people, 250,000 were expelled, and another 200,000 were forced to convert to Catholicism; the victims were predominantly Serbians but included 37,000 Jews.
From the start, the Yugoslav resistance forces consisted of two factions: the communist-led Yugoslav Partisans and the royalist Chetniks, with the former receiving Allied recognition only at the Tehran conference (1943). The heavily pro-Serbian Chetniks were led by Draža Mihajlović, while the pan-Yugoslav oriented Partisans were led by Josip Broz Tito.
The Partisans initiated a guerrilla campaign that developed into the largest resistance army in occupied Western and Central Europe. The Chetniks were initially supported by the exiled royal government and the Allies, but they soon focused increasingly on combating the Partisans rather than the occupying Axis forces. By the end of the war, the Chetnik movement transformed into a collaborationist Serb nationalist militia completely dependent on Axis supplies. The highly mobile Partisans, however, carried on their guerrilla warfare with great success. Most notable of the victories against the occupying forces were the battles of Neretva and Sutjeska.
On 25 November 1942, the Anti-Fascist Council of National Liberation of Yugoslavia was convened in Bihać, modern day Bosnia and Herzegovina. The council reconvened on 29 November 1943, in Jajce, also in Bosnia and Herzegovina, and established the basis for post-war organisation of the country, establishing a federation (this date was celebrated as Republic Day after the war).
The Yugoslav Partisans were able to expel the Axis from Serbia in 1944 and the rest of Yugoslavia in 1945. The Red Army provided limited assistance with the liberation of Belgrade and withdrew after the war was over. In May 1945, the Partisans met with Allied forces outside former Yugoslav borders, after also taking over Trieste and parts of the southern Austrian provinces of Styria and Carinthia. However, the Partisans withdrew from Trieste in June of the same year under heavy pressure from Stalin, who did not want a confrontation with the other Allies.
Western attempts to reunite the Partisans, who denied the supremacy of the old government of the Kingdom of Yugoslavia, and the émigrés loyal to the king led to the Tito-Šubašić Agreement in June 1944; however, Marshal Josip Broz Tito was in control and was determined to lead an independent communist state, starting as a prime minister. He had the support of Moscow and London and led by far the strongest partisan force with 800,000 men.
The official Yugoslav post-war estimate of victims in Yugoslavia during World War II is 1,704,000. Subsequent data gathering in the 1980s by historians Vladimir Žerjavić and Bogoljub Kočović showed that the actual number of dead was about 1 million.
SFR Yugoslavia.
On 11 November 1945 elections were held with only the Communist-led National Front appearing on the ballot, securing all 354 seats. On 29 November, while still in exile, King Peter II was deposed by Yugoslavia's Constituent Assembly, and the Federal People's Republic of Yugoslavia was declared. However, he refused to abdicate. Marshal Tito was now in full control, and all opposition elements were eliminated.
On 31 January 1946, the new constitution of Socialist Federal Republic of Yugoslavia, modeled after the Soviet Union, established six republics, an autonomous province, and an autonomous district that were part of SR Serbia. The federal capital was Belgrade. The policy focused on a strong central government under the control of the Communist Party, and on recognition of the multiple nationalities.
Tito's regional goal was to expand south and take control of Albania and parts of Greece. In 1947, negotiations between Yugoslavia and Bulgaria led to the Bled agreement, which proposed to form a close relationship between the two Communist countries, and enable Yugoslavia to start a civil war in Greece and use Albania and Bulgaria as bases. Stalin vetoed this agreement and it was never realised. The break between Belgrade and Moscow was now imminent.
Yugoslavia solved the national issue of nations and nationalities (national minorities) in a way that all nations and nationalities had the same rights. The flags of the republics used versions of the red flag or Slavic tricolor, with a red star in the centre or in the canton.
The 1948 Yugoslavia-Soviet split.
The country distanced itself from the Soviets in 1948 (cf. Cominform and Informbiro) and started to build its own way to socialism under the strong political leadership of Josip Broz Tito.
All the Communist European Countries had deferred to Stalin and rejected the Marshall Plan aid in 1947. Tito, at first went along and rejected the Marshall plan. However, in 1948 Tito broke decisively with Stalin on other issues, making Yugoslavia an independent communist state. Yugoslavia requested American aid. American leaders were internally divided, but finally agreed and began sending money on a small scale in 1949, and on a much larger scale 1950-53. The American aid was not part of the Marshall plan.
Tito criticised both Eastern Bloc and NATO nations and, together with India and other countries, started the Non-Aligned Movement in 1961, which remained the official affiliation of the country until it dissolved.
In 1974, the two provinces of Vojvodina and Kosovo-Metohija (for the latter had by then been upgraded to the status of a province), as well as the republics of Bosnia and Herzegovina and Montenegro, were granted greater autonomy to the point that Albanian and Hungarian became nationally recognised minority languages, and the Serbo-Croat of Bosnia and Montenegro altered to a form based on the speech of the local people and not on the standards of Zagreb and Belgrade. In Slovenia the recognized minorities were Hungarians and Italians.
Vojvodina and Kosovo-Metohija formed a part of the Republic of Serbia but those provinces also formed part of the federation, which led to the unique situation that Central Serbia did not have its own assembly but a joint assembly with its provinces represented in it.
Demographics.
Yugoslavia had always been a home to a very diverse population, not only in terms of national affiliation, but also religious affiliation. Of the many religions, Islam, Roman Catholicism, Judaism and Protestantism, as well as various Eastern Orthodox faiths, composed the religions of Yugoslavia, comprising over 40 in all. The religious demographics of Yugoslavia changed dramatically since World War II. A census taken in 1921 and later in 1948 show that 99% of the population appeared to be deeply involved with their religion and practices. With postwar government programs of modernisation and urbanisation, the percentage of religious believers took a dramatic plunge. Connections between religious belief and nationality posed a serious threat to the post-war Communist government's policies on national unity and state structure.
After the rise of communism, a survey taken in 1964 showed that just over 70% of the total population of Yugoslavia considered themselves to be religious believers. The places of highest religious concentration were that of Kosovo with 91% and Bosnia and Herzegovina with 83.8%. The places of lowest religious concentration were Slovenia 65.4%, Serbia with 63.7% and Croatia with 63.6%. Religious differences between Orthodox Serbs, Catholic Croats, Muslim Bosniaks and Albanians alongside the rise of nationalism contributed to the collapse of Yugoslavia in 1991.
Government.
On 7 April 1963, the nation changed its official name to Socialist Federal Republic of Yugoslavia and Josip Broz Tito was named President for life. In the SFRY, each republic and province had its own constitution, supreme court, parliament, president and prime minister. At the top of the Yugoslav government were the President (Tito), the federal Prime Minister, and the federal Parliament (a collective Presidency was formed after Tito's death in 1980). Also important were the Communist Party general secretaries for each republic and province, and the general secretary of Central Committee of the Communist Party.
Tito was the most powerful person in the country, followed by republican and provincial premiers and presidents, and Communist Party presidents. Slobodan Penezić Krcun, Tito's chief of secret police in Serbia, fell victim to a dubious traffic incident after he started to complain about Tito's politics. Minister of the interior Aleksandar Ranković lost all of his titles and rights after a major disagreement with Tito regarding state politics. Some influential ministers in government, such as Edvard Kardelj or Stane Dolanc, were more important than the Prime Minister.
First cracks in the tightly governed system surfaced when students of the University of Belgrade and several other cities joined the worldwide protests of 1968. President Josip Broz Tito gradually stopped the protests by giving in to some of the students' demands and saying that "students are right" during a televised speech. But in the following years, he dealt with the leaders of the protests by sacking them from university and Communist party posts.
A more severe sign of disobedience was so-called Croatian Spring of 1970–1971, when students in Zagreb organised demonstrations for greater civil liberties and greater Croatian autonomy, followed by mass manifestations across Croatia. The regime stifled the public protest and incarcerated the leaders, but many key Croatian representatives in the Party silently supported this cause, lobbying within the Party ranks for a reorganisation of the country. As a result, new Constitution was ratified in 1974, which gave more rights to the individual republics in Yugoslavia and provinces in Serbia.
Ethnic tensions and economic crisis.
The Yugoslav federation was constructed against a double background: an inter-war Yugoslavia which had been dominated by the Serbian ruling class; and a war-time division of the country, as Fascist Italy and Nazi Germany split the country apart and endorsed an extreme Croatian nationalist faction called the Ustaše. A small faction of Bosniak nationalists joined the Axis forces and attacked Serbs while extreme Serb nationalists engaged in attacks on Bosniaks and Croats.
Yugoslav Partisans took over the country at the end of the war and banned nationalism from being publicly promoted. Overall relative peace was retained under Tito's rule, though nationalist protests did occur, but these were usually repressed and nationalist leaders were arrested and some were executed by Yugoslav officials. However one protest in Croatia in the 1970s, called the "Croatian Spring" was backed by large numbers of Croats who claimed that Yugoslavia remained a Serb hegemony and demanded that Serbia's powers be reduced.
Tito, whose home republic was Croatia, was concerned over the stability of the country and responded in a manner to appease both Croats and Serbs, he ordered the arrest of the Croat protestors, while at the same time conceding to some of their demands. In 1974, Serbia's influence in the country was significantly reduced as autonomous provinces were created in ethnic Albanian-majority populated Kosovo and the mixed-populated Vojvodina.
These autonomous provinces held the same voting power as the republics but unlike the republics, they could not legally separate from Yugoslavia. This concession satisfied Croatia and Slovenia, but in Serbia and in the new autonomous province of Kosovo, reaction was different. Serbs saw the new constitution as conceding to Croat and ethnic Albanian nationalists. Ethnic Albanians in Kosovo saw the creation of an autonomous province as not being enough, and demanded that Kosovo become a constituent republic with the right to separate from Yugoslavia. This created tensions within the Communist leadership, particularly among Communist Serb officials who resented the 1974 constitution as weakening Serbia's influence and jeopardising the unity of the country by allowing the republics the right to separate.
An economic crisis erupted in the 1970s which was the product of disastrous errors by Yugoslav governments, such as borrowing vast amounts of Western capital in order to fund growth through exports . Western economies then entered recession, blocked Yugoslav exports and created a huge debt problem. The Yugoslav government then accepted the IMF loan.
In 1989, according to official sources, 248 firms were declared bankrupt or were liquidated and 89,400 workers were laid off. During the first nine months of 1990 directly following the adoption of the IMF programme, another 889 enterprises with a combined work-force of 525,000 workers suffered the same fate. In other words, in less than two years "the trigger mechanism" (under the Financial Operations Act) had led to the lay off of more than 600,000 workers out of a total industrial workforce of the order of 2.7 million. An additional 20% of the work force, or half a million people, were not paid wages during the early months of 1990 as enterprises sought to avoid bankruptcy. The largest concentrations of bankrupt firms and lay-offs were in Serbia, Bosnia and Herzegovina, Macedonia and Kosovo. Real earnings were in a free fall and social programmes had collapsed; creating within the population an atmosphere of social despair and hopelessness. This was a critical turning point in the events to follow.
Breakup.
Though the 1974 Constitution reduced the power of the federal government, Tito's authority substituted for this weakness until his death in 1980.
After Tito's death on 4 May 1980, ethnic tensions grew in Yugoslavia. The legacy of the Constitution of 1974 was used to throw the system of decision-making into a state of paralysis, made all the more hopeless as the conflict of interests had become irreconcilable. The Albanian majority in Kosovo demanded the status of a republic in the 1981 protests in Kosovo while Serbian authorities suppressed this sentiment and proceeded to reduce the province's autonomy.
In 1986, the Serbian Academy of Sciences and Arts drafted a memorandum addressing some burning issues concerning position of Serbs as the most numerous people in Yugoslavia. The largest Yugoslav republic in territory and population, Serbia's influence over the regions of Kosovo and Vojvodina was reduced by the 1974 Constitution. Because its two autonomous provinces had de facto prerogatives of full-fledged republics, Serbia found that its hands were tied, for the republican government was restricted in making and carrying out decisions that would apply to the provinces. Since the provinces had a vote in the Federal Presidency Council (an eight-member council composed of representatives from the six republics and the two autonomous provinces), they sometimes even entered into coalition with other republics, thus outvoting Serbia. Serbia's political impotence made it possible for others to exert pressure on the 2 million Serbs (20% of the total Serbian population) living outside Serbia.
Serbian communist leader Slobodan Milošević sought to restore pre-1974 Serbian sovereignty. Other republics, especially Slovenia and Croatia, denounced this move as a revival of greater Serbian hegemonism. Through a series of moves known as the "anti-bureaucratic revolution", Milošević succeeded in reducing the autonomy of Vojvodina and of Kosovo and Metohija, but both entities retained a vote in the Yugoslav Presidency Council. The very instrument that reduced Serbian influence before was now used to increase it: in the eight-member Council, Serbia could now count on four votes at a minimum – Serbia proper, then-loyal Montenegro, Vojvodina, and Kosovo.
As a result of these events, the ethnic Albanian miners in Kosovo organised the 1989 Kosovo miners' strike, which dovetailed into ethnic conflict between the Albanians and the non-Albanians in the province. At around 80% of the , ethnic-Albanians were the majority. The number of Slavs in Kosovo (mainly Serbs) was quickly declining for several reasons, among them the ever increasing ethnic tensions and subsequent emigration from the area. By 1999 the Slavs formed as little as 10% of the total population in Kosovo.
Meanwhile Slovenia, under the presidency of Milan Kučan, and Croatia supported the Albanian miners and their struggle for formal recognition. Initial strikes turned into widespread demonstrations demanding a Kosovan republic. This angered Serbia's leadership which proceeded to use police force, and later even the Federal Army was sent to the province by the order of the Serbia-held majority in the Yugoslav Presidency Council.
In January 1990, the extraordinary 14th Congress of the League of Communists of Yugoslavia was convened. For most of the time, the Slovenian and Serbian delegations were arguing over the future of the League of Communists and Yugoslavia. The Serbian delegation, led by Milošević, insisted on a policy of "one person, one vote", which would empower the plurality population, the Serbs. In turn, the Slovenes, supported by Croats, sought to reform Yugoslavia by devolving even more power to republics, but were voted down. As a result, the Slovenian and Croatian delegations left the Congress and the all-Yugoslav Communist party was dissolved.
The constitutional crisis that inevitably followed resulted in a rise of nationalism in all republics: Slovenia and Croatia voiced demands for looser ties within the Federation.
Following the fall of communism in the rest of Eastern Europe, each of the republics held multi-party elections in 1990. Slovenia and Croatia held the elections in April since their communist parties chose to cede power peacefully. Other Yugoslav republics – especially Serbia – were more or less dissatisfied with the democratisation in two of the republics and proposed different sanctions (e.g. Serbian "customs tax" for Slovenian products) against the two, but as the year progressed, other republics' communist parties saw the inevitability of the democratisation process and in December as the last member of the federation – Serbia held parliamentary elections which confirmed former communists' rule in this republic.
The unresolved issues however remained. In particular, Slovenia and Croatia elected governments oriented towards greater autonomy of the republics (under Milan Kučan and Franjo Tuđman, respectively), since it became clear that Serbian domination attempts and increasingly different levels of democratic standards were becoming increasingly incompatible. Serbia and Montenegro elected candidates who favoured Yugoslav unity.
The Croat quest for independence led to large Serb communities within Croatia rebelling and trying to secede from the Croat republic. Serbs in Croatia would not accept a status of a national minority in a sovereign Croatia, since they would be demoted from the status of a constituent nation of the entirety of Yugoslavia.
Yugoslav Wars.
The war broke out when the new regimes tried to replace Yugoslav civilian and military forces with secessionist forces. When, in August 1990, Croatia attempted to replace police in the Serb populated Croat Krajina by force, the population first looked for refuge in the Yugoslavian Army barracks, while the army remained passive. The civilians then organised armed resistance. These armed conflicts between the Croatian armed forces ("police") and civilians mark the beginning of the Yugoslav war that inflamed the region. Similarly, the attempt to replace Yugoslav frontier police by Slovenian police forces provoked regional armed conflicts which finished with a minimal number of victims.
A similar attempt in Bosnia and Herzegovina led to a war that lasted more than three years (see below). The results of all these conflicts are almost complete emigration of the Serbs from all three regions, massive displacement of the populations in Bosnia and Herzegovina, and establishment of the three new independent states. The separation of Macedonia was peaceful, although the Yugoslav Army occupied the peak of the Straža mountain on the Macedonian soil.
Serbian uprisings in Croatia began in August 1990 by blocking roads leading from the Dalmatian coast towards the interior almost a year before Croatian leadership made any move towards independence. These uprisings were more or less discretely backed up by the Serb-dominated federal army (JNA). The Serbs in Croatia proclaimed "Serb autonomous areas", later united into the Republic of Serb Krajina. The federal army tried to disarm the territorial defence forces of Slovenia (republics had their local defence forces similar to the Home Guard) in 1990 but was not completely successful. Still, Slovenia began to covertly import arms to replenish its armed forces.
Croatia also embarked upon the illegal import of arms, (following the disarmament of the republics' armed forces by the federal army) mainly from Hungary, and were under constant surveillance which produced a video of a secret meeting between the Croatian Defence minister Martin Špegelj and the two men, filmed by the Yugoslav counter-intelligence ("KOS, Kontra-obavještajna služba"). Špegelj announced that they were at war with the army and gave instructions about arms smuggling as well as methods of dealing with the Yugoslav Army's officers stationed in Croatian cities. Serbia and JNA used this discovery of Croatian rearmament for propaganda purposes.
Guns were also fired from army bases through Croatia. Elsewhere, tensions were running high.
In the same month, the Army leaders met with the Presidency of Yugoslavia in an attempt to get them to declare a state of emergency which would allow for the army to take control of the country. The army was seen as an arm of the Serbian government by that time so the consequence feared by the other republics was to be total Serbian domination of the union. The representatives of Serbia, Montenegro, Kosovo, and Vojvodina voted for the decision, while all other republics, Croatia, Slovenia, Macedonia and Bosnia and Herzegovina, voted against. The tie delayed an escalation of conflicts, but not for long.
Following the first multi-party election results, in the autumn of 1990, the republics of Slovenia and Croatia proposed transforming Yugoslavia into a loose confederation of six republics. By this proposal, republics would have right to self-determination. However Milošević rejected all such proposals, arguing that like Slovenes and Croats, the Serbs (having in mind Croatian Serbs) should also have a right to self-determination.
On 9 March 1991, demonstrations were held against Slobodan Milošević in Belgrade, but the police and the military were deployed in the streets to restore order, killing two people. In late March 1991, the Plitvice Lakes incident was one of the first sparks of open war in Croatia. The Yugoslav People's Army (JNA), whose superior officers were mainly of Serbian ethnicity, maintained an impression of being neutral, but as time went on, they got more and more involved in state politics.
On 25 June 1991, Slovenia and Croatia became the first republics to declare independence from Yugoslavia. The federal customs officers in Slovenia on the border crossings with Italy, Austria, and Hungary mainly just changed uniforms since most of them were local Slovenes. The following day (26 June), the Federal Executive Council specifically ordered the army to take control of the "internationally recognised borders", leading to the Ten-Day War.
The Yugoslav People's Army forces, based in barracks in Slovenia and Croatia, attempted to carry out the task within the next 48 hours. However, because of misinformation given to the Yugoslav Army conscripts that the Federation was under attack by foreign forces and the fact that the majority of them did not wish to engage in a war on the ground where they served their conscription, the Slovene territorial defence forces retook most of the posts within several days with only minimal loss of life on both sides.
There was a suspected incident of a war crime, as the Austrian ORF TV network showed footage of three Yugoslav Army soldiers surrendering to the territorial defence force, before gunfire was heard and the troops were seen falling down. However, none were killed in the incident. There were however numerous cases of destruction of civilian property and civilian life by the Yugoslav People's Army, including houses and a church. A civilian airport, along with a hangar and aircraft inside the hangar, was bombarded; truck drivers on the road from Ljubljana to Zagreb and Austrian journalists at the Ljubljana Airport were killed.
A ceasefire was eventually agreed upon. According to the Brioni Agreement, recognised by representatives of all republics, the international community pressured Slovenia and Croatia to place a three-month moratorium on their independence.
During these three months, the Yugoslav Army completed its pull-out from Slovenia, but in Croatia, a bloody war broke out in the autumn of 1991. Ethnic Serbs, who had created their own state Republic of Serbian Krajina in heavily Serb-populated regions resisted the police forces of the Republic of Croatia who were trying to bring that breakaway region back under Croatian jurisdiction. In some strategic places, the Yugoslav Army acted as a buffer zone; in most others it was protecting or aiding Serbs with resources and even manpower in their confrontation with the new Croatian army and their police force.
In September 1991, the Republic of Macedonia also declared independence, becoming the only former republic to gain sovereignty without resistance from the Belgrade-based Yugoslav authorities. 500 U.S. soldiers were then deployed under the U.N. banner to monitor Macedonia's northern borders with the Republic of Serbia. Macedonia's first president, Kiro Gligorov, maintained good relations with Belgrade and the other breakaway republics and there have to date been no problems between Macedonian and Serbian border police even though small pockets of Kosovo and the Preševo valley complete the northern reaches of the historical region known as Macedonia (Prohor Pčinjski part), which would otherwise create a border dispute if ever Macedonian nationalism should resurface ("see VMRO"). This was despite the fact that the Yugoslav Army refused to abandon its military infrastructure on the top of the Straža Mountain up to the year 2000.
As a result of the conflict, the United Nations Security Council unanimously adopted UN Security Council Resolution 721 on 27 November 1991, which paved the way to the establishment of peacekeeping operations in Yugoslavia.
In Bosnia and Herzegovina in November 1991, the Bosnian Serbs held a referendum which resulted in an overwhelming vote in favour of forming a Serbian republic within the borders of Bosnia and Herzegovina and staying in a common state with Serbia and Montenegro. On 9 January 1992, the self-proclaimed Bosnian Serb assembly proclaimed a separate "Republic of the Serb people of Bosnia and Herzegovina". The referendum and creation of SARs were proclaimed unconstitutional by the government of Bosnia and Herzegovina and declared illegal and invalid. However, in February–March 1992, the government held a national referendum on Bosnian independence from Yugoslavia. That referendum was in turn declared contrary to the BiH and the Federal constitution by the federal Constitutional Court in Belgrade and the newly established Bosnian Serb government.
The referendum was largely boycotted by the Bosnian Serbs. The Federal court in Belgrade did not decide on the matter of the referendum of the Bosnian Serbs. The turnout was somewhere between 64–67% and 98% of the voters voted for independence. It was not clear what the two-thirds majority requirement actually meant and whether it was satisfied. The republic's government declared its independence on 5 April, and the Serbs immediately declared the independence of "Republika Srpska". The war in Bosnia followed shortly thereafter.
Timeline.
Various dates are considered the end of the Socialist Federal Republic of Yugoslavia:
New states.
The successor states to the former Yugoslavia are the following:
Succession, 1992–2003.
As the Yugoslav Wars raged through Croatia and Bosnia, the republics of Serbia and Montenegro, which remained relatively untouched by the war, formed a rump state known as the Federal Republic of Yugoslavia (FRY) in 1992. The Federal Republic of Yugoslavia aspired to be a sole legal successor to the Socialist Federal Republic of Yugoslavia, but those claims were opposed by the other former republics. The United Nations also denied its request to automatically continue the membership of the former state. Eventually, after the overthrow of Slobodan Milošević from power as president of the federation in 2000, the country dropped those aspirations, accepted the opinion of the Badinter Arbitration Committee about shared succession, and reapplied for and gained UN membership on 2 November 2000. (From 1992 to 2000, some countries, including the United States, had referred to the FRY as "Serbia and Montenegro".) In April 2001, the five successor states extant at the time drafted an Agreement on Succession Issues, signing the agreement in June 2001. Marking an important transition in its history, the Federal Republic of Yugoslavia was officially renamed Serbia and Montenegro in 2003.
Succession, 2006–present.
In June 2006, Montenegro became an independent nation after the results of a May 2006 referendum, therefore rendering Serbia and Montenegro no longer existent. After Montenegro's independence, Serbia became the legal successor of Serbia and Montenegro, while Montenegro re-applied for membership in international organisations. In February 2008, the Republic of Kosovo declared independence from Serbia, leading to an ongoing dispute on whether Kosovo is a legally recognised state. However, numerous countries, including the United States and various members of the European Union, have recognised Kosovo as an independent nation.
Yugosphere.
In 2009, "The Economist" coined the term "Yugosphere" to describe the present-day physical areas that formed Yugoslavia, as well as its culture and influence.
The similarity of the languages and the long history of common life have left many ties among the peoples of the new states, even though the individual state policies of the new states favour differentiation, particularly in language. The Serbo-Croatian language is linguistically a single language, with several literary and spoken variants since the language of the government was imposed where other languages dominated (Slovenia, Macedonia). Now, separate sociolinguistic standards exist for the Bosnian, Croatian, Montenegrin and Serbian languages.
Remembrance of the time of the joint state and its perceived positive attributes is referred to as "Yugonostalgia".
Many aspects of Yugonostalgia refer to the socialist system and the sense of social security it provided. There are still people from the former Yugoslavia who self-identify as Yugoslavs; this identifier is commonly seen in demographics relating to ethnicity in today's independent states.

</doc>
<doc id="34253" url="http://en.wikipedia.org/wiki?curid=34253" title="Foreign relations of Yemen">
Foreign relations of Yemen

The foreign relations of Yemen are the relationships and policies that Yemen maintains with other countries. It is a member of the United Nations, the Arab League, and the Organisation of Islamic Cooperation. Yemen participates in the nonaligned movement. The Republic of Yemen accepted responsibility for all treaties and debts of its predecessors, the YAR and the PDRY. Additionally, Yemen has acceded to the Nuclear Non-Proliferation Treaty and has stressed the need to render the Middle East region free of nuclear and other weapons of mass destruction.
History.
North Yemen.
The geography and ruling Imams of North Yemen kept the country isolated from foreign influence before 1962. During the 1920s, the government of Yemen forged relations with the Italian government under Mussolini, which lead to the conclusion of an Italian-Yemeni friendship treaty on September 2, 1926. This gave the Sana'a government diplomatic support vis-a-vis the Saudi government, which had aggressive designs on Yemeni territory. The country's relations with Saudi Arabia were defined by the Treaty of Taif in 1934 which delineated the northernmost part of the border between the two kingdoms and set the framework for commerce and other interactions. The Taif Agreement has been renewed periodically in 20-year increments, and its validity was reaffirmed in 1995. Relations with the British colonial authorities in Aden and the south were usually tense.
The Soviet and Communist Chinese Aid Missions established in 1958 and 1959 were the first important non-Muslim presence in North Yemen. Following the September 1962 revolution, the Yemen Arab Republic became closely allied with and heavily dependent upon Egypt. Saudi Arabia aided the royalists in their attempt to defeat the republicans and did not recognize the Yemen Arab Republic until 1970. Subsequently, Saudi Arabia provided Yemen substantial budgetary and project support. At the same time, Saudi Arabia maintained direct contact with Yemeni tribes, which sometimes strained its official relations with the Yemeni government. Hundreds of thousands of Yemenis found employment in Saudi Arabia during the late 1970s and 1980s.
Saleh's foreign policy as the leader of North Yemen was characterized by the principles of "positive neutrality" and Arab unity. Under Saleh, Yemen cultivated close ties with Saudi Arabia and other pro-West states in the region. He also purchased military equipment from the United States and expanded economic relations with the West. At the same time, Saleh also tried to maintain friendly relations with the then-Soviet Union (which broke apart in 1991). In October 1984, he renewed the treaty of Friendship and Cooperation that was originally signed in 1964 by San'a and Moscow.
In February 1989, North Yemen joined Iraq, Jordan, and Egypt informing the Arab Cooperation Council (ACC), an organization created partly in response to the founding of the Gulf Cooperation Council, and intended to foster closer economic cooperation and integration among its members. After unification, the Republic of Yemen was accepted as a member of the ACC in place of its YAR predecessor. In the wake of the Persian Gulf crisis, the ACC has remained inactive.
South Yemen.
British authorities left South Yemen in November 1967 in the wake of an intense terrorist campaign. The People's Democratic Republic of Yemen, the successor to British colonial rule, had diplomatic relations with many nations, but its major links were with the Soviet Union and other Communist countries. Relations between it and the conservative Arab states of the Arabian Peninsula were strained. There were military clashes with Saudi Arabia in 1969 and 1973, and the PDRY provided active support for the Dhofar Rebellion against the Sultanate of Oman. The PDRY was the only Arab state to vote against admitting new Arab states from the Persian Gulf area to the United Nations and the Arab League. The PDRY provided sanctuary and material support to various international terrorist groups.
Unified Yemen.
The Persian Gulf crisis dramatically affected Yemen's foreign relations. As a member of the UN Security Council (UNSC) for 1990 and 1991, Yemen abstained on a number of UNSC resolutions concerning Iraq and Kuwait and voted against the "use of force resolution". Western and Persian Gulf Arab states reacted by curtailing or canceling aid programs and diplomatic contacts. At least 850,000 Yemenis returned from Saudi Arabia and the Persian Gulf.
After the liberation of Kuwait, Yemen continued to maintain high-level contacts with Iraq. This hampered its efforts to rejoin the Arab mainstream and to mend fences with its immediate neighbors. In 1993, Yemen launched an unsuccessful diplomatic offensive to restore relations with its Persian Gulf neighbors. Some of its aggrieved neighbors actively aided the south during the 1994 civil war. Since the end of that conflict, tangible progress has been made on the diplomatic front in restoring normal relations with Yemen's neighbors. The Omani-Yemeni border has been officially demarcated. In the summer of 2000, Yemen and Saudi Arabia signed an International Border Treaty settling a 50-year-old dispute over the location of the border between the two countries. Yemen settled its dispute with Eritrea over the Hanish Islands in 1998.
By country.
Bangladesh.
It was here that a person named Shah Jalal spread Islam and introduced the religion to the people of Bangladesh back in 1303.
Djibouti.
Relations between Yemen and Djibouti are good, and cooperation takes place on many levels. A causeway between the two countries has been proposed.
Eritrea.
In 1995, there was a conflict between Yemen and Eritrea over the Hanish islands. Eritrea has half, while Yemen has half.
Holy See.
The Holy See and Yemen established diplomatic relations in 1998.
India.
India has an embassy in Sana'a, while Yemen has an embassy in New Delhi.
Iran.
Following the first two decades of the Islamic Revolution, ties between Tehran and Sana'a were never strong, but in recent years the two countries have attempted to settle their differences. One sign of this came on December 2, 2003, when the Yemeni foreign ministry announced that "Yemen welcomes Iran's request to participate in the Arab League as an observer member."
However, relations have also been tense in recent years, particularly for the alleged Iranian support to Houthi rebels in Yemen, as part of the Shia insurgency in Yemen.
Israel.
Yemen does not have diplomatic relations with Israel and relations between the two countries are very tense. People with an Israeli passport or any passport with an Israeli stamp cannot enter Yemen, and Yemen is defined as an enemy state by Israeli law.
Malaysia.
Malaysia has an embassy in Sana'a, and Yemen has an embassy in Kuala Lumpur.
Oman.
Oman and Yemen are generally enjoying good relations. The two countries share a border. Both Oman and Yemen were part of the Persian Empire, and later of Umayyad and Abbasid Caliphates. Yemen has an embassy in Muscat. Oman is represented in Yemen through its embassy in San'a.
Pakistan.
Pakistan has an embassy in Sana'a. Many Pakistani worked in Yemen.
Saudi Arabia.
Although similar in dialect, ethnicity and religion, diplomatic relations between the two countries have been hostile owing to various political and border disputes.
Singapore.
Singapore and Yemen generally have solid, good relations. The governments of Yemen and Singapore are in support of the War on Terror and Singapore is also one of Yemen's biggest trading partners outside the Gulf States
Somalia.
Although relations between the modern-day territories of Somalia and Yemen stretch back to antiquity, the two countries formally established diplomatic ties on December 18, 1960. Both nations are also members of the Arab League.
Following the outbreak of the civil war in Somalia in the 1990s, the Yemeni authorities maintained relations with Somalia's newly established Transitional National Government and its successor the Transitional Federal Government. The subsequent establishment of the Federal Government of Somalia in August 2012 was also welcomed by the Yemeni authorities, who re-affirmed Yemen's continued support for Somalia's government, its territorial integrity and sovereignty.
Additionally, Somalia maintains an embassy in Yemen, with the diplomatic mission led by Ambassador Ismail Qassim Naji. Yemen also has an embassy in Mogadishu.
United Kingdom.
The United Kingdom have one consulate and embassy based in Sana'a. 
United States.
Traditionally, Yemen's relations with the United States have been tepid, as the lack of strong military-to-military ties, commercial relations, and support of Yemeni President Ali Abdullah Saleh has hindered the development of strong bilateral ties. During the early years of the George W. Bush administration, relations improved under the rubric of the war on terror, though Yemen's lax policy toward wanted terrorists has stalled additional American support.
International organization membership.
Yemen is a member of the United Nations (UN) and the following UN affiliates and specialized agencies:
Yemen is also a member of the following organizations:
Yemen was granted observer status at the World Trade Organization (WTO) in 1999 and in 2002 and 2003 submitted necessary documentation for full membership. The WTO working party on Yemen met in 2004 and twice thereafter to discuss Yemen's accession; negotiations are expected to take several years.
Relations with the Gulf Cooperation Council.
Yemen desires to join the 24-year-old Gulf Cooperation Council (GCC), a sub-regional organization which groups Saudi Arabia, Kuwait, Bahrain, Qatar, the United Arab Emirates, and Oman in an economic and security alliance. GCC members have traditionally opposed accession of additional states. Currently, Yemen has partial observer status on some GCC committees, and observers believe that full membership is unlikely. Others assert that it is in the GCC's interest to assist Yemen and prevent it from becoming a failed state, lest its instability spread to neighboring Gulf countries. This has helped Yemen greatly. In November 2006, an international donors' conference was convened in London to raise funds for Yemen's development. Yemen received pledges totaling $4.7 billion, which are to be disbursed over four years (2007–2010) and represent over 85% of the government's estimated external financing needs. Much of these pledges came from Yemen's wealthy Arab neighbors.
The impediments to full GCC membership are steep. Reportedly, Kuwait, still bitter over Yemen's support for Saddam Hussein during the first Gulf War, has blocked further discussion of membership. Meanwhile, Yemen needs to export thousands of its workers each year to the Gulf in order to alleviate economic burdens at home. Foreign remittances are, aside from oil exports, Yemen's primary source of hard currency.
Arab–Israeli conflict.
Yemen has usually followed mainstream Arab positions on Arab–Israel issues, and its geographic distance from the conflict and lack of political clout make it a minor player in the peace process. Yemen has not established any bilateral mechanism for diplomatic or commercial contacts with Israel. The Yemeni Jewish community (300 members) continues to dwindle, as many of its members emigrated to Israel decades ago. On December 11, 2008, Moshe Nahari, a Jewish teacher, was murdered in a market in Raidah, home to one of the last Jewish communities in Yemen. After the attack, President Saleh pledged to relocate Yemeni Jews to the capital.
Yemen supports the Arab Peace Initiative, which calls for Israel's full withdrawal from all occupied territories and the establishment of a Palestinian state in the West Bank and Gaza Strip in exchange for full normalization of relations with all Arab states in the region. In the spring of 2008, President Saleh attempted to broker a reconciliation agreement between the competing Palestinian factions Hamas and Fatah. During a March meeting in Sana'a, Palestinian representatives from both groups signed a declaration (the Sana'a Declaration) calling for the creation of a national unity government, but the talks fell apart over the issue of Hamas's role in a unified Palestinian Authority.
Major international treaties.
Yemen is a signatory to various international agreements on agricultural commodities, commerce, defense, economic and technical cooperation, finance, and postal matters. Yemen is a Non-Annex I country under the United Nations Framework Convention on Climate Change. Yemen is not a signatory to the Kyoto Protocol but has acceded to it, which has the same legal effect as ratification. Yemen is a signatory to the Nuclear Non-Proliferation Treaty, is a party to the Biological Weapons Convention, and has signed and ratified the Chemical Weapons Convention. Yemen is also a party to environmental conventions on Biodiversity, Desertification, Environmental Modification, Hazardous Wastes, Law of the Sea, and Ozone Layer Protection.
2010 embassy closures.
In late December 2009, the U.S. Embassy asked Americans in Yemen to keep watch for any suspicious terrorist activity following a terrorist incident on board a flight to the US that was linked to Yemen. On January 3, 2010, following intelligence and threats from al-Qaeda, the U.S. embassy in Sana'a was closed. A statement issued on the embassy's website said: "The US Embassy in Sana'a is closed today, in response to ongoing threats by Al-Qaeda in the Arabian Peninsula (AQAP) to attack American interests in Yemen". Al Jazeera reported that the closure of the embassy can mean only that "they believe al-Qaeda threat is very serious". No reopening date was given.
On the same day, the United Kingdom withdrew their presence in the country for similar purposes. The following day, France closed its embassy. Although the French Embassy was closed, staff remained inside. The French foreign ministry issued a statement saying, "Our ambassador decided on January 3 not to authorise any public access to the diplomatic mission until further notice." At the Italian Embassy, only those with prior appointments were allowed to enter. Ambassador Mario Boffo noted, though, that "if things remain as they are, then tomorrow or the day after we will return to normality." The embassy of the Czech Republic closed the visa and consular departments "amid fears of terrorist attacks." Japan, Spain and Germany also made changes to their security arrangements and embassy accessibility. In addition to extra security at embassies, Yemen increased security at Sana'a International Airport.
According to the BBC, Yemeni media say the embassy closures come after "six trucks full of weapons and explosives entered the capital, and the security forces lost track of the vehicles." Trucks driven by militants, previously under security surveillance, had entered Sana'a and lost the surveillance at that point.
The French, UK, and US embassies later reopened the following day.
2015 embassy closures.
Following the 2014–15 Yemeni coup d'état, many nations closed their embassies. France, United Kingdom, and United States closed their embassies on 11 February 2015, Germany, Italy, and Saudi Arabia closed their embassies on 13 February, Spain, Turkey, and United Arab Emirates closed their embassies on 14 February, Japan closed its embassy on 16 February, and Egypt closed its embassy on 23 February.

</doc>
<doc id="34310" url="http://en.wikipedia.org/wiki?curid=34310" title="York University">
York University

York University (French: "Université York") is a public research university in Toronto, Ontario, Canada. It is Canada's third-largest university.
York University has approximately 55,000 students, 7,000 faculty and staff, and 275,000 alumni worldwide. It has eleven faculties, namely the Faculty of Liberal Arts & Professional Studies, Faculty of Science, Lassonde School of Engineering, Schulich School of Business, Osgoode Hall Law School, Glendon College, Faculty of Education, Faculty of Health, Faculty of Environmental Studies, Faculty of Graduate Studies, the School of the Arts, Media, Performance and Design (formerly the Faculty of Fine Arts), and 28 research centres.
York University participates in the Canadian Space Program. The Faculty of Science and Lassonde School of Engineering are Canada's primary research facility into Martian exploration, and have designed several space research instruments and applications currently used by NASA. York has pioneered several PhD programs in Canada, including women's studies. The School of Social Work is recognized as having one of the most socially responsive programs in the country. York University's business school and law school have continuously and consistently been ranked among the top schools in Canada and the world.
History.
York University was established in 1959 as a non-denominational institution by the "York University Act", which received Royal Assent in the Legislative Assembly of Ontario on 26 March of that year. Its first class was held in September 1960 in Falconer Hall on the University of Toronto campus with a total of 76 students.
The policy of university education initiated in the 1960s responded to population pressure and the belief that higher education was a key to social justice and economic productivity for individuals and for society. The governance was modelled on the provincial University of Toronto Act of 1906, which established a bicameral system of university government consisting of a senate (faculty), responsible for academic policy, and a board of governors (citizens) exercising exclusive control over financial policy and having formal authority in all other matters. The president, appointed by the board, was to provide a link between the two bodies and to perform institutional leadership.
In the fall of 1961, York moved to its first campus, Glendon College, and began to emphasize liberal arts and part-time adult education. It became independent in 1965, after an initial period of affiliation with the University of Toronto (U of T), under the "York University Act", 1965. Its main campus on the northern outskirts of Toronto opened in 1965.
Murray Ross, who continues to be honoured today at the University in several ways – including the Murray G. Ross Award, was still vice-president of U of T when he was approached to become York University's new president. At the time, York University was envisaged as a feeder campus to U of T, until Ross's powerful vision led it to become a completely separate institution.
In 1965, the university opened a second campus, the Keele Campus, in North York. The Glendon campus became a bilingual liberal arts college led by Escott Reid, who envisaged it as a national institution to educate Canada's future leaders, a vision shared by Prime Minister Lester Pearson, who formally opened Glendon College in 1966.
The first Canadian undergraduate program in dance opened at York University in 1970. In 1972, Canada Post featured the nascent institution on 8¢ stamps, entitled "York University Campus, North York, Ont." The first Canadian PhD. program in Women's Studies opened with five candidates in January 1992.
Its bilingual mandate and focus on the liberal arts continue to shape Glendon's special status within York University. The new Keele Campus was regarded as somewhat isolated, in a generally industrialized part of the city. Petrol storage facilities are still located across the street. Some of the early architecture was unpopular with many, not only for the brutalist designs, but the vast expanses between buildings, which was not viewed as suitable for the climate. In the last two decades, the campus has been intensified with new buildings, including a dedicated student centre and new fine arts, computer science and business administration buildings, a small shopping mall, and a hockey arena. The Rexall Centre tennis stadium, built in 2004, is a perennial host of the Canada Masters tennis tournament. As Toronto has spread further out, York has found itself in a relatively central location within the built-up Greater Toronto Area (GTA), and in particular, near the Jane and Finch neighbourhood. Its master plan envisages a denser on-campus environment commensurate with that location. Students occupied the university's administration offices in March 1997, protesting escalating tuition hikes.
On November 6, 2008, the York University Senate suspended classes because of a strike by CUPE Local 3903. The local represents contract professors, teaching assistants, and graduate assistants. Classes resumed on Monday, February 2, 2009 after back-to-work legislation was passed by the Ontario Legislative Assembly (see: "2008-09 York University Strike")
Academics.
York has educated some of the current and past directors and CEOs of Canada's major banks (Bank of Nova Scotia, Bank of Montreal), the largest and most prominent media networks in Canada (CTV Television Network, Rogers Communications, Canadian Broadcasting Corporation), and numerous judges, diplomats, and senior politicians including a former Chief Justice of the Federal Court of Appeal of Canada, the Minister of Finance of Canada and the former Canadian Ambassador to the United Nations. Astronaut Steve MacLean was educated at York University in the physics department and later taught there before going to work at Stanford University.
York's approximately 2,450 full-time faculty and academic librarians are represented by the York University Faculty Association. Contract faculty, teaching assistants, and graduate assistants are represented by CUPE Local 3903.
Admissions.
For the 2012-2013 academic year, York received 40,943 undergraduate applications; approximately 11,000 students accepted admission offers and were enrolled maintaining a high school entrance average of 81.7%. Admission to the BBA programs requires an average above 90% (A+). For graduate admissions, the average GMAT score for candidates admitted to the Schulich School of Business is 663. 
York University has over 120 undergraduate programs with 17 degree types (BA, iBA, BSc, iBSc, BBA, iBBA, BEng, BES, BDes, BPA, BFA, BAS, BEd, BDEM, BHRM, BScN, BSW) and offers over 170 degree options. They admit to 30 international degrees offering international language study and opportunities to study abroad at more than 100 international universities. In 2012, 8,238 Graduate applications were received and 53,198 Undergraduate applications. Its international students represent over 150 countries around the world. York University is one of the few universities to offer a GAP year for students who wish to travel abroad, study religious development or work and still keep their offer. York also allows students who are having immigration or financial difficulties to defer their offer for up to one year.
Reputation.
The 2014-2015 Times Higher Education World University Rankings placed York 226-250th in the world, placing 9-15th in Canada. The 2014 QS World University Rankings ranked the university 421-430th in the world. According to the 2011 Academic Ranking of World Universities (ARWU) rankings, the university ranked 401-500th in the world. In terms of national rankings, "Maclean's" ranked York 9th in their 2014 comprehensive university rankings. The Higher Education Strategy Associates ranked York 8th nationally in Social Sciences and Humanities.
Several of York's programs had also gained notable recognition both nationally and internationally. In the QS World University Rankings 2013 ranking of history departments, York was place thirty-third in the world, and third in Canada. In the Corporate Knights 2011 ranking of teacher-education programs in Canada, York had placed fifth. Osgoode Hall Law School (a Toronto law school affiliated with the university at New Osgoode Hall on the York Campus) ranked second in Canada, in Maclean's 2012 ranking of Canadian common law schools. In the Corporate Knights ranking of law programs in Canada, Osgoode Hall had also placed second. In the QS ranking of law programs, York had placed 42nd in the world, and fifth in Canada.
The Schulich School of Business had also gained national and international accolades. In the Corporate Knights 2011 rankings of MBA programs in Canada, the school had placed first. In the same year, the Corporate Knights had also ranked the school third in Canada for undergraduate business programs. In Beyond Grey Pinstripes 2011 rankings of MBA programs, York had placed second in the world, and first in Canada. In Bloomberg Businessweek's 2010 rankings of the best business schools outside the United States, Schulich had ranked seventh, the second highest of any Canadian business school. In Forbes 2010 rankings of the best two-year MBA program outside the United States, Schulich had ranked 10th in the world, the highest out of any Canadian business school. In CNN Expansion's ranking of MBA programs, the school had ranked 18th in the world, placing first in Canada. In the Financial Times 2011 ranking of MBA programs, Schulich had ranked 49th in the world, and third in Canada. In The Economist's 2011 full-time MBA rankings, Schulich had ranked ninth in the world, and first in Canada.
Faculties.
York University has eleven faculties, including the Faculties of Liberal Arts and Professional Studies (which was formed in July 2009 by the merger of the Atkinson Faculty of Liberal and Professional Studies and the Faculty of Arts), Science, Health, the Lassonde School of Engineering, Education, the School of the Arts, Media, Performance and Design (formerly the Faculty of Fine Arts), Environmental Studies, the Schulich School of Business, Glendon College, and Graduate Studies. Some faculties' programs overlap: for instance, more than one house separate mathematics departments, although some of these are being merged; the Schulich School of Business offers undergraduate and graduate International Business Administration programmes and the Faculty of Liberal Arts & Professional Studies offers a Bachelor of Administrative Studies with streams in Accounting, Business Research, Finance, Human Resources Management, Management, Management Science, and Marketing, as well as a minor in Business, to be taken in conjunction with a major in another discipline. The Schulich School of Business offers undergraduates with the option of pursuing a BBA or iBBA program with a component of mandatory exchange. Also, the Glendon and Schulich units are offering or are in the processes of preparing to offer degrees in public policy and administration. The University administration has, however, taken steps in some cases to unify departments in separate faculties, in part to support York's efforts to brand itself as a university focused on interdisciplinarity. For example, the Faculty of Health, opened on 1 July 2006, houses the School of Health Policy and Management, School of Kinesiology and Health Science, School of Nursing, and the Department of Psychology.
York University offers the first and largest graphic design program in Ontario York/Sheridan Design (YSDN). It is a four-year University degree delivered jointly by the two leading educational institutions of design in Canada (York University and Sheridan College), and is recognized throughout North America for maintaining the highest academic and professional standards.
The Osgoode Hall Law School moved from a downtown location to the York campus in 1969, following the requirement that every law school affiliate with a university. The law school has offered several flexible degrees available, including the Osgoode-NYU JD/LLB degree in conjunction with New York University School of Law. Osgoode Hall Law School of York University has been ranked the top law school in Canada in "Canadian Lawyer" magazine’s 2008 Law School Survey.
York University's Faculty of Graduate Studies offers graduate degrees in a variety of disciplines, and there are several joint graduate programmes with the University of Toronto and Ryerson University. It is the second largest graduate school in the Province of Ontario.
The Ph.D. students in the Social and Political Thought program have won the award for best PhD thesis in Canada.
The School of Women's Studies at York University offers a large array of courses in the field, some of which are offered in French.
The Canadian Centre for Germanic and European Studies is co-housed at York University and Université de Montréal. The Centre is funded by the German Academic Exchange Service.
Research centres and institutes.
The Art Gallery of York University houses the permanent art collections. The collection of 1500 objects includes Canadian, American, Inuit, and European mixed media, multimedia, installations, painting, photography, prints, drawings, sculpture, sketchbooks, film and video.
The School of the Arts, Media, Performance and Design (AMPD, formerly the Faculty of Fine Arts), offers programmes such as design, ethnomusicology, cultural studies, visual arts, music, dance, and theatre. York's Jazz Department was once overseen by Oscar Peterson. York also has a joint Bachelor of Design program with Sheridan College. York's Departments of Film, Theatre and Creative Writing (which is not affiliated with the Faculty of Fine Arts) offers programmes in film production/directing, acting, and writing respectively, producing many award-winning graduates. The founders of Toronto's Hot Docs International Documentary Film Festival and CineACTION film theory magazine were graduates of York's Faculty of Fine Arts.
York's Dance department was founded by National Ballet of Canada's first choreographer Grant Strate.
York offers a Space and Communication Sciences undergraduate degree. York’s Centre for Vision Research has developed a ‘virtual reality room’ called IVY (Immersive Virtual Environment at York) in order to study spatial orientation and perception of gravity and motion. The Canadian Space Agency and National Space Biomedical Research Institute (NSBRI) use this room to strengthen astronauts’ sense of ‘up’ and ‘down’ in zero-gravity environments. The room is a six-sided immersive environment made of the glass used in the CN Tower’s observation deck and includes walls, ceiling, and a floor made of computer-generated pixel maps. York's Faculty of Science and Engineering most recently took part in the 2007 NASA Phoenix Mars Mission.
York is also the only university in Canada with specialized programs in meteorological sciences at both the undergraduate and graduate level.
Seneca@York.
The Keele campus is host to a satellite facility of Seneca College, and York University offers a number of joint programs with Seneca College:
Libraries.
There are five libraries at York University. Four of these libraries constitute the York University Libraries system: the Scott Library, Steacie Science and Engineering Library, Peter F. Bronfman Business Library, and the Leslie Frost Library (on Glendon Campus). The fifth library, the Osgoode Hall Law Library, reports to the Dean of the Osgoode Hall Law School and is the largest law library in the Commonwealth. The Scott Library also contains within it the Clara Thomas Archives and Special Collections, the Map Library and the Sound and Moving Images Library. Although the Archives of Ontario moved to York's Keele campus in April 2009, it is not affiliated with York University Libraries.
Campuses.
Keele Campus.
York's primary campus ("The Keele Campus") is located in the north of the City of Toronto and bordering York Region. It is the largest post-secondary campus in Canada at 457 acres. Most of the University's faculties reside here, including Liberal Arts, Fine Arts, Business, Law, Environmental Studies, Science and Engineering, Education, and Health. All together, nearly 50,000 students attend classes on the Keele campus.
York has over 200,000 living alumni. Although a large number of alumni live in Ontario, a significant number live in British Columbia, Nova Scotia, Alberta, New York, and Washington, D.C. York also has over 25,000 alumni overseas.
Glendon Campus.
Glendon College is a bilingual liberal arts faculty and separate campus of York University located in the downtown Toronto neighbourhood of Lawrence Park. Glendon is unique amongst York faculties as it possesses autonomy over both its recruitment and admission. The College hosts its own academic programs and facilities. Student services on Campus are provided in both French and English and all Glendon students are required to take courses in both French and English to graduate. Glendon is the only university-level institution in central Southern Ontario that offers university courses in both French and English. Further, it is the only University Campus in Ontario that requires students to take courses in both official languages of Canada. The Campus is made accessible through TTC Routes 124-Sunnybrook and 11-Bayview and a frequent shuttle bus that runs between Glendon and York's Keele Campus. Glendon students are free to take courses and access Libraries and institutional services offered at the Keele campus (and vice versa).
Markham Campus.
In June 2014 York announced that Markham, Ontario will become a satellite site for York Region. The campus will be built near Highway 407, between Kennedy Rd. and Warden Ave. in partnership with Seneca College. The new campus will house approximately 4000 students with the potential to expand to meet future demand. On May 20, 2015, the provincial government announced it will provide financial contribution to this new project. 
Satellite campuses.
While most of the Schulich School of Business and Osgoode Hall Law School programs are offered at the Keele Campus, both of them maintain satellite facilities in downtown Toronto. Schulich operates the Miles S. Nadal Management Centre, while Osgoode Hall has a Professional Development Centre located at 1 Dundas Street West.
Student life.
York University has over 54,000 students: about 48,000 undergraduates (approximately 38,500 of which are full-time and 7,500 part-time) and 5,900 graduate students. Many students come from the Greater Toronto Area, but there is a sizeable population of students from across Canada and abroad, making York one of the most international universities in Canada. To serve this large population, there are 290 student clubs and organizations; six student-run publications and three broadcast programs; six art galleries; 33 on-campus eateries; and a retail mall. Undergraduate students at York are represented by the York Federation of Students, a student-elected body that sponsors most of the clubs, and engages in lobbying with the university administration as well as the provincial and federal governments. 
Colleges and residences.
York has nine undergraduate residential colleges:
Residence Life.
Residence Life at York University is part of the Centre for Student Community and Leadership Development (SCLD), that manages the eight residence buildings on the Keele campus. These Residences are divided into two complexes on either side of campus: Complex 1 includes Winters Residence, Tatham Hall Residence, Vanier Residence, and Founders Residence; Complex 2 includes Bethune Residence, Calumet Residence, Stong Residence, and Pond Residence (the newest residence, and sometimes referred to as Complex 3 grouped with the York Apartments). Each residence is linked to one of the aforementioned colleges, but since 2007 have been managed through SCLD rather than the Master's Offices of each College. Residence Life works closely in partnership with Student Housing Services, a separate campus office. Residence Life provides and manages the student programming and support, while Housing Services manages the facilities, room placements and application processes. The Residence Life Staff is composed of Residence Life Coordinators, Residence Dons, and Night Porters designated to each building. Residence Life also maintains important partnerships with several campus services, including but not limited to Security Services, Food Services, Sport and Recreation, Centre for Human Rights, Office of Student Conflict Resolution, York Federation of Students, and Colleges and College Councils.
The Village at York University.
The Village at York University off-campus student housing area has become a popular area of accommodation for many upper-year and post-graduate students, and the area has had a large amount of attention particularly for large parties hosted by students, including the annual Battle of the Village kegger held in March. There have also been many reports of the level of noise pollution from late-night parties from students living in the area. Safety has also been a pressing issue.
The Village is a residential neighbourhood within the City of Toronto, occupying about 130 acres beyond the south boundary of York University's property. Residential dwellings in The Village are privately owned, and homeowners in this community are City of Toronto taxpayers. York University participates in the Village At York Town and Gown Committee, with representatives of residents of The Village; the York Federation of Students; Toronto Police Service and Toronto Fire Service, as well as other key municipal agencies and the local Councillors' office. This committee addresses concerns of residents, students, the City and the University, such as community safety and by-law enforcement and compliance.
Newspapers and other publications.
"Excalibur" has been the university's autonomous student newspaper since 1966. In 2008, the "YU Free Press" was formed as an alternative campus newspaper.
Colleges and some programs also have individual newspapers or magazines. They include: "The Flying Walrus" (Stong College), "MacMedia" (McLaughlin College), "The Pipe" (Calumet College), "Artichoke" (Winters College), "SOFA" (Spotlight On Fine Arts), "The Lexicon" (Norman Bethune College), "The Vandoo" (Vanier College), "The Phoenix" (Founders College), "Pro Tem" (Glendon College), "Obiter Dicta" (Osgoode Law School), and "The Insider" (Schulich School of Business).
"Existere – Journal of Arts and Literature" (est. 1978) is a national publication with local and international contributors. It is financed by Vanier College Council. The journal publishes short fiction, poetry, non-fiction and art from novices and seasoned veterans. Several major writers got their early start in "Existere".
"YorkU Magazine" (est. 2003) is the official magazine of York University. It is published 3 times a year in both a print and digital format.
Sport.
The University is represented in Canadian Interuniversity Sport by the York Lions. Beginning in 1968 York's sporting teams were known as the "Yeomen", after the Yeomen Warders, the guardians of the fortress and palace at the Tower of London, otherwise known as Beefeaters. Later, the name "Yeowomen" was introduced to encourage women to participate in sports. Popular sentiment ran against this name scheme, however, as many students were fond of noting that a "Yeowoman" was fictitious, neither a real word nor having any historical merit. In 2003, after conducting an extensive internal study, the University replaced both names with the "Lions", as part of a larger renaming effort, and a new logo, now a white and red lion, was brought into line with the university's new visual scheme. The name change also brought York University in line with the 92% of other Canadian universities which use a single name for both sexes' sports teams. Ironically, students often refer to the female Lions teams as the "York Lionesses", even though the name "Lion" is intended to apply to both sexes.
York offers 29 interuniversity sport teams, 12 sport clubs, 35 intramural sport leagues, special events and 10 pick-up sport activities offered daily.
York University has several athletic facilities, some of which are used for major tournaments. These include a football stadium, 4 gymnasia, 5 sport playing fields, 4 softball fields, 9 outdoor tennis courts, 5 squash courts, 3 dance/aerobic studios, 6 ice arenas, a swimming pool, an expanding fitness centre and the new Rexall Centre (home of the Rogers Tennis Cup).
In 2005, plans were made to build a new football and soccer stadium to host the Toronto Argonauts of the Canadian Football League and future football tournaments. These plans were scuttled, however, when a deal was signed by the Argos to remain at the Rogers Centre.
Fight song.
Notable among a number of songs commonly played and sung at various events such as commencement and convocation, and athletic games are: "York Song", sung to the tune "Harvard".
There are also college songs, particularly from the friendly feuds between Stong College and Bethune College, and the infamous Winters College and Vanier College cheer songs.
Fraternities and Sororities.
With both fraternities and sororities operating on campus, York University is home to a vibrant "Greek" community. Though the organizations are not officially recognized by the University, they do provide member students with valuable experiences throughout their post-secondary academic careers to supplement campus life. Many lifelong friendships are forged through these societies, which place a great deal of emphasis on academic achievement, leadership and philanthropy.
Organizations.
Over the years, fraternities and sororities have operated unofficially on campus:
Fraternities:
- Tau Kappa Epsilon (ΤΚΕ) - (Interest Group - active)
Sororities:
- Tau Sigma Phi (ΤΣΦ) - (Colony - active)
Phi Delta Phi (ΦΔΦ) international legal fraternity, at Osgoode Law School, was given special dispensation when the law school became part of the university, as the fraternity's history with the law school dated back to 1896, and is recognized at York.
Transit.
York University is sometimes referred to as a "commuter school". Over 65% of the students and staff have home addresses in the Greater Toronto Area (GTA), particularly in York Region and downtown Toronto. Many students are opting for public transit owing to York's high parking fees. York intends to increase the fees for parking to combat congestion around campus and to support the goal of making Toronto more environmentally friendly.
Close to fourteen hundred buses move people through the campus each day. An extension of the Yonge-University-Spadina line of the Toronto Subway is currently under construction. It will run directly under the campus, creating new stations at Keele Street and Finch Avenue (as Finch West station), at the centre of campus (as York University station), and at Steeles Avenue, interfacing with York Regional Transit (as Black Creek Pioneer Village station).
York University's Glendon and Keele campuses are served by the TTC. The Keele site is also served by York Region Transit buses (both regular and Viva) from the immediate north, GO Transit express buses from several other Toronto suburbs and colleges or universities (such as Sheridan College and UOIT) along with Greyhound buses for regional transportation. Transportation Services operates a shuttle service to GO Transit's York University train station on its Barrie corridor. As of November 20, 2009, express buses on the highly frequent 196 York University Rocket Toronto Transit Commission (TTC) bus route now use the dedicated York University Busway to transport students from Downsview station to York Lanes in about 15 minutes. It consists of bus-only lanes on Allen Road and Dufferin Street, and bus-only roadways through a hydro corridor north of Finch Avenue West, and along the east side of the campus. As of September 20, 2010, the Züm Route 501 provides service from Bramalea Terminal in Brampton to York University.
Campus safety.
York University is physically located in a diverse area of Toronto. York University Department of Security Services provides security services on the university's campuses. York Security Services provide uniformed security staff which consist of campus liaison officers (community services unit) and the Investigative Unit which works closely with the Toronto Police Services. The investigative unit and Toronto Police Investigative Units and 31 Division work jointly on serious investigations. Patrols are conducted on foot, bicycle and vehicle. The security service is a member of the Ontario Association of College and University Security Administrators (OACUSA) and the International Association of Campus Law Enforcement Administrators (IACLEA). The campus community is over 55,000 people.
The department uses marked Ford Crown Victorias which are clearly identifiable with low-profile LED roof lights. The department responds to all calls for service, however incidents of a criminal nature should be reported to Toronto Police. Uniform security staff can be clearly identified by their dark cargo-style pants, red shirt (York University colour) and black exterior body armour (bulletproof vest covers). Residence watch staff are also posted nightly at all undergraduate residence buildings to provide an extra level of protection. As part of the campus safety system a student escort service is available to all members of the community. The escort staff are students hired on a part-time basis. In an emergency escort staff have immediate communication with Security Services.
Incidents.
In June 2008, the university announced it had commissioned an external safety audit after a string of rapes on the university campus. During frosh week 2007, two men entered a campus dormitory and raped two students.
The victims were 17 and 18 years old at the time. A lawsuit filed claims that "the entrance door to the college and to the residential room areas of the college were virtually wide open to the public at large". The next year, in January 2008, another student was sexually assaulted in the stairwell of a campus building. In May 2009, a contract security guard was shot at a York University campus pub, The Underground.
In April 2010, a 20-year-old student was sexually assaulted while walking to her apartment, located minutes away from the main campus. The incident, described by police as "particularly severe" and resulting in "very serious" injuries for the victim, was particularly notable as it occurred just a week after Daniel Katsnelson was sentenced for the 2007 rape on the university campus.
The incident further reignited criticism against the school for continued delays in releasing the campus safety audit, which was first announced nearly 2 years prior. On March 31, 2010, less than a month prior to the April 2010 incident, the campus newspaper, Excalibur published an article lambasting the school administration for delays in releasing the audit. A spokesman for the university responded that the school is still "anxiously awaiting" the report.
In April 2011, campus security was temporarily increased following the murder of a 23-year old Chinese overseas student in the York University Village. The university temporarily hired paid duty Toronto Police Officers to patrol the campus. On August 26, 2011, York University announced that in response to recommendations in the METRAC Safety Audit, the University will be modifying the service delivery model for York Security Services (YSS) personnel. Members will be trained and issued with handcuffs and batons. Cruisers are to also be equipped with "silent partners" for transporting arrested persons.
On November 20, 2012, York University initiated the intervention capable delivery model for security services. Having been certified after undergoing training in August 2012, personnel were equipped with handcuffs and batons, and began wearing a dark grey uniform, replacing the red uniform, to signify the change in role for the community.
York still does not train its security staff to the level of special constables, giving them police powers on campus, like other universities in Ontario including, Western, Waterloo, WLU, Brock, Carleton, U of T, Guelph, Windsor and Fanshawe College.
On March 6, 2014, Toronto Police and EMS responded late at night to a shooting at the York University Student Centre. Two women were found at the scene; one had been shot and suffered non-life-threatening injuries and another received minor (non-gun-related) injuries from the suspect. A suspect was apprehended and subsequently charged. On April 10, 2014 two injured students and six others who witnessed the shooting filed a $20.5 million lawsuit against the University. Toronto Law Firm, Diamond and Diamond filed the suit alleging a pattern of negligence.
Noted alumni and faculty.
Distinguished Research Professors.
The rank of "" is the highest rank a professor can achieve at York University. There are only ever 25 active Distinguished Research Professors at any time. It is awarded to members of the faculty who have made outstanding contributions to the University through their work in research.
Controversies.
Violation of academic freedom.
In the aftermath of an academic conference that took place in 2009, titled "Israel/Palestine: Mapping Models of Statehood and Paths to Peace", which explored the possible models of statehood for Israel/Palestine, including the one state model, concerns were raised about the way the York Administration has handled the political pressure. Some of the organizers accused the York administration, mainly the then Dean of the Osgoode Hall Law School and the Associate Vice President for Research and Innovation, of putting undue pressure on the organizers in order to force them to change the content of the conference, invite or disinvite some speakers. The accusations were supported by documents and emails that were obtained through the Freedom of Information and Privacy Protection Act.
The York administration appointed former Supreme Court of Canada Judge to review the issue, but the Iacobucci Review was problematic, and the terms of reference for the review were seen as an attack on academic freedom. The whole issue is being investigated by the Canadian Association of University Teachers.
In response to the allegations made, a University spokesperson said that the University should be judged the fact that event took place despite the pressure not to hold it, and that there is always internal discussion as part of the planning of all events. He added that "In the end, this conference did go on and we do not feel that academic freedom was breached." Vice President and Provost, Patrick Monahan, said, about these allegation that "Justice Iacobucci has looked at that and he does not see any purpose in conducting further inquiries. Obviously there are a lot of different views about it.” Yet, it seems that the Iaccobucci Report is seen by many faculty members at Osgoode as controversial. In a letter from the Osgoode Hall Faculty Association, the Association said that it "considers the Iacobucci Report to be unsound and unreliable." The Association also said that "the Report both jeopardizes academic freedom and fails to consider the troubling conduct of the York officials."
Intimidation and Harassment of Pro-Israel Groups.
On February 11, 2009, tensions between pro-Israeli students, and their anti-Israeli counterparts broke into physical unilateral conflict.
Some pro-Israeli Speakers at a conference involved with the Drop YFS campaign called for the impeachment of some York Federation of Students (YFS) executives due to their inactivity during the recent CUPE strike at the time, which shut down the university for three months. The 5,000 signatures necessary to do this were gathered as part of a wider initiative to oust the YFS executives. At the time of submission of the petition, approximately 100 pro-Palestinian students including those from SAIA and the local chapter of the Tamil Tigers reacted to that with protests.
According to pro-Israeli sources including the student group that was attacked, the demonstrators, which reportedly consisted of members of the YFS and Students Against Israeli Apartheid (SAIA), shouted “Zionism equals racism!” and “Racists off campus!” One witness stated that “a riot broke out. They [YFS supporters] started banging the door and windows, intimidating Jewish students and screaming anti semitic slurs like “Die Jew,” “Get the hell off campus,” “Go back to Israel,” and “F---ing Jew". The students barricaded themselves inside the Hillel offices, where protesters reportedly banged on the windows and attempted to force their way in. Eventually police were called to escort Jewish students through the protesters. The claims of anti-Semitic slurs were not widely reported by the media that were present at the time, as some believe that this accusation lacks credibility. The slurs were reported in Jewish community papers, The Globe and Mail and the Canadian Huffington Post among others. Roughly a year later a video surfaced on Youtube that showed the incident unfolding from the trapped Jewish students' side, the video is alleged to show the protesters verbally and physically harass dissenters and force them to seek refuge in the Hillel office.
 Interestingly, the anti-Israeli side had a different viewpoint. Krisna Saravanamuttu, York Federation of Students' vice-president of equity, who took part in the protest, denied that the protesters shouted anti-semitic slogans, stating that "That is categorically false. I heard nothing of that nature at all." He did however, confirm that the protesters shouted "racism off campus" and "students united will never be defeated." Some York faculty and students have rejected these claims of anti-Semitism as media spin and exaggeration It should be noted that the petition, if successful would have removed him from his position and that he was spotted as not only being with the protestors during the incident but also according to some accounts leading them.
In May 2009, York adjudicator Professor Janet Mosher, who was then Associate Dean at York’s Osgoode Hall Law School, ruled that two York students, Krisna Saravanamuttu and Jesse Zimmerman, had violated the Student Code of Conduct due to their behaviour at the protest, which she described as “exclusionary and offensive” and which promoted an atmosphere of “hostility, incivility and intimidation.” Mosher noted that both students participated in the protest which pursued a group of Jewish students to Hillel’s lounge in York’s Student Centre, and swarmed outside shouting taunts. On a video of the incident, Saravanamuttu was shown clapping and apparently leading a chant of “Whose campus? Our campus!” as well as participating in a chant of “Racists off campus." Saravanamuttu was fined $150 and both he and Zimmerman were given an official reprimand and human rights training.
In February 2010, the campus group the Christians United for Israel (CUFI) and "My Canada" applied to use university space to host the "Imagine With Us" coalition event consisting of pro-Israel speakers. The University replied that the event could only proceed under certain conditions (which ultimately led to the event's cancellation when the organizers declined to comply with the terms):
These conditions drew criticism because they were not imposed on the organizers of Israel Apartheid Week which was being held on campus the same month. York's decision drew sharp criticism from David Frum who wrote in the "National Post" that "Since the anti-Israel people might use violence, the speech of the pro-Israel people must be limited. On the other hand, since the pro-Israel people do not use violence, the speech of the anti-Israel people can proceed without restraint." A York University spokesman subsequently told Frum that "all student groups that request university space" must meet "precisely same requirements" but that while the “process” and the “protocols” that were the same, a “needs-based assessment” of each particular case is necessary. Frum subsequently criticized the "utterly arbitrary ad hoc decision-making of a fathomlessly cowardly university administration." Frank Dimant, CEO of B'nai Brith Canada also sharply criticized York's justification, arguing that "York’s continued appeasement of anti-Israel agitators at the expense of Zionist Christians and Jews is unacceptable.”
Professor Ed Morgan of the University of Toronto criticized York, citing a 1992 ruling by the U.S. Supreme Court that struck down a county government's increased fee for police protection for a controversial speaker because "speech cannot be financially burdened, any more than it can be punished or banned, simply because it might offend a hostile mob." Regarding the situation at York, Morgan wrote that "It's bad enough that there are "hostile mobs" on our campuses; making others pay for that hostility only rubs salt in our wounded freedoms." Prof. Howard C. Tenenbaum, also of the University of Toronto, wrote that York "has lost all stature as an academic institution whose remit is to provide for full academic discourse, freedom from hatred on campus and freedom of speech, unless of course that freedom only includes unabashed hatred for the State of Israel." David Murrell of the University of New Brunswick wrote that "Everyone has a supposed right to free speech at York University – so long as groups can afford to pay security against leftist intimidation."
In March 2013, the York Federation of Students (YFS) endorsed BDS at the university. It is alleged that no one from Hillel at York or Hasbara @ York, was officially informed of the meeting, the motion, or of the agenda that resulted in this resolution. The groups claim that they only found out about the meeting the day before it was scheduled and only because a member of Hillel at York overheard from the YFS that it would be happening the following day. It is also alleged that there was no official audit of the petition that resulted in said motion.
Recently there has been controversy raised regarding a mural in the student centre that depicts a man cloaked in a scarf bearing the entire state of Israel as Palestine as well as him holding stones behind his back. The main issue behind this is that it depicts a violent act in the making and uses symbols associated with terrorist organizations.
Strikes.
York University has a history of faculty and teaching assistant strikes. In 1997, there was a faculty strike by YUFA that lasted seven weeks. At the time, this was the second longest strike in Canadian University history.
Key issues in the strike included retirement, funding, and institutional governance. In 2001, teaching assistants and contract faculty went on strike for 11 weeks, when the university broke its own record. The central issue in the 2001 disruption was the administration's proposed attempts to remove tuition indexation language.
A strike beginning on November 6, 2008 concerned a variety of institutional grievances, including job security for contract professors, elimination of the Non-Academic Student Code of Conduct, creation of whistleblower protection, and fund indexation. On January 20, 2009, CUPE 3903 defeated a forced ratification vote that would have ended the strike. On January 24, Ontario premier Dalton McGuinty announced a rare Sunday recall of the provincial legislature in order to pass back-to-work legislation mandating an immediate end to the strike. On January 29, the York University Labour Disputes Resolution Act was passed in the provincial parliament on a count of 42–8 ending the long 85-day strike and setting a precedent for future university strikes in Ontario.
An additional strike by teachings assistants, contract faculty, and graduate assistants took place throughout March 2015. When the strike began, on March 2, the university cancelled nearly all classes because about 2/3 of York courses were taught by the striking contract faculty at the time. On March 10, the contract faculty ratified a new agreement, but the teaching assistants and graduate assistants rejected tentative agreements the bargaining team had reached with the university. The contract faculty immediately returned to work and, at that point, the university gradually restarted nearly all courses. The teaching assistants and graduate assistants, however, continued their strike until the end of the month. The union reached a tentative agreement with the university on March 29, 2015 which was ratified on March 31, 2015, thus putting an immediate end to the 29 day strike.
Issuance of parking tickets.
Toronto City Councillor Howard Moscoe claims that York University is illegally issuing parking tickets to persons parking on York University property. Legally the City of Toronto is the only regulatory body allowed to issue parking tickets. York University refuses to surrender student transcripts to students who have failed to pay parking fees. York University spokesman Richard Fisher states "It's identical to when you renew your taxi or driver's licences. Unless you pay your fines, you don't get it [renewed] and that's because that's their last opportunity to actually get you before you depart. And I think we are in the same situation."
Bibliography.
Histories.
</dl>

</doc>
<doc id="34358" url="http://en.wikipedia.org/wiki?curid=34358" title="Yacc">
Yacc

Yacc is a computer program for the Unix operating system. The name is an acronym for "Yet Another Compiler Compiler". It is a LALR parser generator, generating a parser, the part of a compiler that tries to make syntactic sense of the source code, specifically a LALR parser, based on an analytic grammar written in a notation similar to BNF. It was originally developed in the early 1970s by Stephen C. Johnson at AT&T Corporation and written in the B programming language, but soon rewritten in C. It appeared as part of Version 3 Unix, and a full description of Yacc was published in 1975.
Yacc and similar programs (largely reimplementations) have been very popular. Yacc itself used to be available as the default parser generator on most Unix systems, though it has since been supplanted as the default by more recent, largely compatible, programs such as Berkeley Yacc, GNU bison, MKS Yacc and Abraxas PCYACC. An updated version of the original AT&T version is included as part of Sun's OpenSolaris project. Each offers slight improvements and additional features over the original Yacc, but the concept and syntax have remained the same. Yacc has also been rewritten for other languages, including OCaml, Ratfor, ML, Ada, Pascal, Java, Python, Ruby, Go and Common Lisp.
Yacc produces only a parser (phrase analyzer); for full syntactic analysis this requires an external lexical analyzer to perform the first tokenization stage (word analysis), which is then followed by the parsing stage proper. Lexical analyzer generators, such as Lex or Flex are widely available. The IEEE POSIX P1003.2 standard defines the functionality and requirements for both Lex and Yacc.
Some versions of AT&T Yacc have become open source. For example, source code (for different implementations) is available with the standard distributions of Plan 9 and OpenSolaris.

</doc>
<doc id="34360" url="http://en.wikipedia.org/wiki?curid=34360" title="Yahya Khan">
Yahya Khan

Agha Muhammad Yahya Khan (Urdu: آغا محمد یحیی خان, Bengali: আগা মুহাম্মদ ইয়াহিয়া খান 4 February 1917 – 10 August 1980), popularly known as Yahya Khan was a four-star general and statesman who served as the third President of Pakistan from 1969 until the fall of East-Pakistan as a follow-up to Pakistan's defeat in the war with India in 1971. Serving with distinction in World War II as a British Indian Army officer, Yahya opted for Pakistan in 1947 and held critical command assignments.
After helping to conduct military infiltration against India in the 1965 war, Yahya was appointed commander-in-chief of army in 1966– a position he held until the final days of 1971 war. When political upheaval forced President Ayub Khan to resign in 1969, Yahya installed a military government after declaring martial law for the second time in Pakistan's history. After promulgating executive order to disestablishment West-Pakistan, he held the country's first nationwide free and fair general elections in 1970, which witnessed the Awami League led by Mujibur Rahman gaining a majority in East Pakistan. Under pressured from Zulfikar Ali Bhutto whose PPP which had won from the four provinces of Pakistan but had far fewer votes, Yahya delayed handing over power to Awami League. As civil unrest erupted all over East Pakistan, Yahya initiated military operations to quell the rebellion. With reports of genocide by the Pakistan army and their local collaborators against Bengali civilians. During the nine-month-long Bangladesh war for independence, members of the Pakistani military and supporting militias killed between 300,000 and 3,000,000 people.
Tensions escalated with India, which intervened on the side of the Mukti Bahini insurgency in 1971. In the resulting Indo-Pakistani war of 1971, which lasted less than two weeks, Pakistan surrendered its eastern command, with about 93,000 soldiers in East-Pakistan becoming prisoners-of-war. Following the independence of Bangladesh, powerful public demonstrations and mass rallies against Yahya in Pakistan forced him to hand over the power to Zulfikar Bhutto as well as stepping down from the post of commander-in-chief in disgrace. Military decorations and honors were stripped from Yahya and he was placed under house arrest for most of the 1970s. With Bhutto's removal in 1977, Yahya was released by provincial administrator Fazle Haq. He died in 1980 in Rawalpindi. He is viewed largely negatively by Pakistani historians, and is considered among the least successful of the country's leaders. He is also accused to be inept, womanizer and alcoholic.
Early life.
Agha Muhammad Yahya Khan was born in Chakwal, Punjab, British Indian Empire on 4 February 1917, according to the references written by Russian sources . According to sources his family descended from the elite soldier class of Nader Shah of Khorasan.
Few Pakistanis knew anything about Yahya Khan when he was vaulted into the presidency two years ago. The stocky, bushy–browed Pathan had been the army chief of staff since 1966...—Editorial, "Time", 2 August 1971, source
Military career.
Yahya entered in the Indian Military Academy in 1936 at Dehradun and was commissioned into the British Indian Army during World War II in 1938. He performed well and served with distinction during the World War II in North African theater, and was taken as the Prisoner of war (POW) by Italy. He escaped from there on his third attempt. His active service further saw actions as part of the 4th Infantry Division in the North Africa, Middle East, and Mediterranean theatres of the war, including Iraq, Italy, and North Africa.
1965 war and Commander-in-chief.
After the World War II, he decided to join the Pakistan Army in 1947. At the age of 34, he was promoted as Brigadier and commanded the 106th Infantry Brigade that was deployed in LoC ceasefire region in Jammu and Kashmir. He was described as a "hard drinking soldier" who liked both his women and wine; though he was a professional soldier. Yahya also co-founded the Command and Staff College in Quetta, Balochistan. He played a pivotal role in sustaining the support for President Ayub Khan's campaign in 1965 presidential elections against Fatima Jinnah. In recognition, he was promoted as Major-General and made GOC of 7th Infantry Division of Pakistan Army, which he commanded during the 1965 war with India. At this assignment, he was not instrumental in planning and executing the military infiltration operation, the "Grand Slam", which failed miserably due to General Yahya's blunderous delay owing to change of command decision, the Indian Army crossed the intentional border and made a beeline for Lahore.
Despite failure and to utter disgust, Yahya was promoted as Lieutenant-General after his promotion papers were personally approved by President Ayub Khan in 1966, at a stint as an appointed Deputy Army Commander in Chief. He was appointed as commander-in-chief of Pakistan Army in March 1966. At promotion, Yahya Khan superseded two of his seniors: Lieutenant-General Altaf Qadir and Lieutenant-General Bakhtiar Rana.
President of Pakistan.
During the course of 1968, the political pressure exerted by Zulfikar Ali Bhutto had weakened the President Ayub Khan, who had earlier sacked Bhutto after disagreeing with President Ayub's decision to implement on Tashkent Agreement, facilitated by the Soviet Union to end the hostilities with India. To ease the situation, President Ayub tried reaching out to terms with Pakistan Peoples Party (PPP) and Awami League (AL), but remain unsuccessful. In poor health, President Ayub abrogated his own constitution and suddenly resigned from the presidency.
On 24 March 1969, President Ayub directed a letter to General Yahya Khan, inviting him to deal with the situation, as it was "the beyond the capacity of (civil) government to deal with the... Complex situation." On 26 March 1969, General Yahya appeared in national television and announced to enforce a martial law in all over the country. The 1962 Constitution was abrogated, dissolved the parliament, and dismissed the President Ayub's civilian officials. In his first nationwide address, Yahya maintained: "I will not tolerate disorder. Let everyone remain at his post."
On immediate effect, he installed a military government and featured active duty military officials:
Yahya Khan Administration 
National Security Council and LFO.
President Yahya was well aware of this explosive situation and decided to bring changes all over the country. His earlier initiatives directed towards establishing the National Security Council (NSC) with Major-General Ghulam Omar being its first advisor. It was formed to analyze and prepare assessments towards issues relating the political and national security.
Secondly in 1969, President Yahya promulgated the Legal Framework Order No. 1970 which disestablished the One Unit programme where West Pakistan was formed. Instead, LFO No. 1970 hence removed the prefix "West", instead adding Pakistan. The decree has no effect on East Pakistan. Following this, President Yahya announced to held nationwide general elections in 1970, and appointed Judge Abdus Sattar as Chief Election Commissioner of Election Commission of Pakistan. Changes were carried out by President Yahya to reversed the country back towards parliamentary democracy.
Last days of East Pakistan.
1970 general elections.
By 28 July 1969, President Yahya had set a framework for elections that were to be held in December 1970. Finally, the general elections were held in all over the country. In East Pakistan, the Awami League led by Mujibur Rahman held almost all mandate, but no seat in any of four provinces of West Pakistan. The socialist Pakistan Peoples Party (PPP) had won the exclusive mandate in the four provinces of Pakistan, but none in the East-Pakistan. The Pakistan Muslim League (PML) led by Nurul Amin was the only party to have representation from all over the country, though it had failed to gain the mandate to run the government. The Awami League had 160 seats, all won from the East-Pakistan; the socialist PPP had secured 81; the conservative PML had 10 seats in the National Assembly. The general elections's results truly reflected the ugly political reality: the division of the Pakistani electorate along regional lines and political polarization of the country between the two states, East Pakistan and Pakistan.
In political terms, therefore, Pakistan as a nation stood divided as a result. Series of bilateral talks between PPP and Mujibur Rahman produced to results and were unable to come to an agreement of transfer of power from to East-Pakistan's representatives on the basis of the Six-Point programme. In Pakistan, the people had felt that the Six-point agenda was a step towards secession. In recent media reports, it since emerged that Mujib met Indian diplomats in London according to his daughter in 1969 from where he agreed to secede from Pakistan 
Genocide in East-Pakistan.
While, the political deadlock remains between the Awami League, PPP, and the military government after the general elections in 1970. During this time, Yahya began coordinating several meetings with his military strategists over the issue in East Pakistan. On 25 March 1971, President Yahya initiated the "Searchlight" in order to restore the writ of the government. Partially successful, the situation in East-Pakistan worsened and the gulf between the two wings now was too wide to be bridged. Agitation was now transformed into a vicious insurgency as Bengali elements of Pakistan armed forces and Police mutinied and formed Bangladesh Forces along with common people of all classes to launch both unconventional and hit and run operations. 
The "Searchlight" ordered by Yahya was a planned military pacification carried out by the Pakistan Armed Forces to curb the Bengali nationalist movement in erstwhile East Pakistan in March 1971 Ordered by the government in Pakistan, this was seen as the sequel to Operation Blitz which had been launched in November 1970.
The original plan envisioned taking control of the major cities on 26 March 1971, and then eliminating all opposition, political or military, within one month. The prolonged Bengali resistance was not anticipated by Pakistani planners. The main phase of Operation Searchlight ended with the fall of the last major town in Bengali hands in mid May.
The total number of people killed in East Pakistan is not known with any degree of accuracy. Bangladeshi authorities claim that 3 million people were killed, while the Hamoodur Rahman Commission, an official Pakistan Government investigation, put the figure as low as 26,000 civilian casualties. According to Sarmila Bose, between 50,000 and 100,000 combatants and civilians were killed by both sides during the war. A 2008 "British Medical Journal" study by Ziad Obermeyer, Christopher J. L. Murray, and Emmanuela Gakidou estimated that up to 269,000 civilians died as a result of the conflict; the authors note that this is far higher than a previous estimate of 58,000 from Uppsala University and the Peace Research Institute, Oslo. According to Serajur Rahman, the official Bangladeshi estimate of "3 "lahks" (300,000) was wrongly translated into English as 3 million.
Khan arrested Sheikh Mujibur Rahman on charges of Sedition and appointed Brigadier Rahimuddin Khan (later General) to preside over a special tribunal dealing with Mujib's case. Rahimuddin awarded Mujib the death sentence, and President Yahya put the verdict into abeyance. Yahya's crackdown, however, had led to a Bangladesh Liberation War within Pakistan, and eventually drew India into what would extend into the Indo-Pakistani War of 1971. The end result was the establishment of Bangladesh as an independent republic. Khan subsequently apologised for his mistakes and voluntarily stepped down.
The US role.
The United States had been a major sponsor of President Yahya's military government, as noted in a reference written by Gary Bass in the "The Blood Telegram"": "President Nixon liked very few people, but he did like General Yahya Khan." Personal initiatives of President Yahya had helped to establish the communication channel between the United States and the China, which would be used to set up the Nixon's trip in 1972.
Since 1960, Pakistan was perceived in the United States as an integral bulwark against the globalized Communism in the Cold War. The United States cautiously supported Pakistan during 1971 although Congress kept in place an arms embargo. In 1970, India with a heavily socialist economy entered in a formal alliance with the Soviet Union in August 1971. Moreover, noting that India was using the violence committed by all sides during this war as a pretext for a possible military intervention, they suspected that India had aggressive intentions.
Over this period, Henry Kissinger would work to prevent sectarian conflicts in Yemen and Lebanon from devolving into regional wars under President Nixon. The Soviet Union's growing support and influence in the Afghanistan, the Nixon administration used Pakistan to try to deter any further Soviet encroachment in the region. Nixon relayed several written and oral messages to President Yahya, strongly urging him to restrain the use of Pakistan forces. His objective was to prevent a war and safeguard Pakistan's interests, though he feared an Indian invasion of Pakistan that would lead to Indian domination of the subcontinent and strengthen the position of the Soviet Union. Similarly, President Yahya feared that an independent Bangladesh could lead to the disintegration of Pakistan. Indian military support for Bengali guerrillas led to war between India and Pakistan.
In 1971, Richard Nixon met Indian Prime Minister Indira Gandhi and did not believe her assertion that she would not invade Pakistan; Nixon did not trust her and even once referred to her as an "old witch". Witness accounts presented by Kissinger pointed out that Nixon made specific proposals to Prime Minister Gandhi on a solution for the crisis, some of which she heard for the first time, including a mutual withdrawal of troops from the Indo-East Pakistan borders. Nixon also expressed a wish to fix a time limit with Yahya for political accommodation in East Pakistan. Nixon asserted that India could count on US endeavors to ease the crisis within a short time. But, both Kissinger and Gandhi's aide Jayakar maintained, Gandhi did not respond to these proposals. Kissinger noted that she "listened to what was in fact one of Nixon's better presentations with aloof indifference" but "took up none of the points." Jayakar pointed out that Gandhi listened to Nixon "without a single comment, creating an impregnable space so that no real contact was possible." She also refrained from assuring that India would follow Pakistan's suit if it withdrew from India's borders. As a result, the main agenda was "dropped altogether."
On 3 December, Yahya preemptively attacked the Indian Air Force and Gandhi retaliated, pushing into East Pakistan. Nixon issued a statement blaming Pakistan for starting the conflict and blaming India for escalating it because he favored a cease-fire. The United States was secretly encouraging the shipment of military equipment from Iran, Turkey, and Jordan to Pakistan, reimbursing those countries despite Congressional objections. The US used the threat of an aid cut-off to force Pakistan to back down, while its continued military aid to Islamabad prevented India from launching incursions deeper into the country. A cease fire was reached on 16 December, leading to the creation of the independent state of Bangladesh.
Fall from power.
When the news of surrender of East Pakistan reaches through the national television, the spontaneous and overwhelming public anger over Pakistan's defeat by Bangladeshi rebels and the Indian Army, followed by the division of Pakistan into two parts boiled into street demonstrations throughout Pakistan. Rumors of an impending coup d'état by junior military officers against President Yahya to swept the country. Yahya became the highest-ranking casualty of the war: to forestall further unrest, on 20 December 1971 he handed over the presidency and government to Zulfikar Ali Bhutto— the ambitious leader of Pakistan's powerful People's Party.
Within hours of Yahya stepping down, President Bhutto reversed JAG's verdict against Mujibur Rehman and instead releasing him to saw him off to London. President Bhutto also signed orders for Yahya's house confinement, the man who imprisoned Mujib in the first place. Both actions produced headlines round the world.
Death.
Yahya remained under the house arrest orders until 1979 when he was released from the custody by martial law administrator General Fazle Haq. He remained out from the public events and died on 10 August 1980 in Rawalpindi, Punjab, Pakistan.
References.
</dl>

</doc>
<doc id="34415" url="http://en.wikipedia.org/wiki?curid=34415" title="Zambia">
Zambia

The Republic of Zambia is a landlocked country in Southern Africa, neighbouring the Democratic Republic of the Congo to the north, Tanzania to the north-east, Malawi to the east, Mozambique, Zimbabwe, Botswana and Namibia to the south, and Angola to the west. The capital city is Lusaka, in the south-central part of Zambia. The population is concentrated mainly around Lusaka in the south and the Copperbelt Province to the northwest.
Originally inhabited by Khoisan peoples, the region was colonised during the Bantu expansion of the thirteenth century. After visits by European explorers in the eighteenth century, Zambia became the British protectorate of Northern Rhodesia towards the end of the nineteenth century. For most of the colonial period, Zambia was governed by an administration appointed from London with the advice of the British South Africa Company.
On 24 October 1964, Zambia became independent of the United Kingdom and prime minister Kenneth Kaunda became the inaugural president. Kaunda's socialist United National Independence Party (UNIP) maintained power from 1964 until 1991. From 1972 to 1991 Zambia was a single-party state with the UNIP as the sole legal political party under the motto 'One Zambia, One Nation'. Kaunda was succeeded by Frederick Chiluba of the social-democratic Movement for Multi-Party Democracy in 1991, beginning a period of social-economic growth and government decentralisation. Levy Mwanawasa, Chiluba's chosen successor, presided over Zambia from January 2002 until his death in August 2008, and is credited with campaigns to reduce corruption and increase the standard of living. After Mwanawasa's death, Rupiah Banda presided as Acting President before being elected President in 2008. Holding office for only three years, Banda stepped down after his defeat in the 2011 elections by Patriotic Front party leader Michael Sata. Sata died on 28 October 2014, the second Zambian president to die in office. Guy Scott served briefly as interim president until new elections were held on 20 January 2015, in which Edgar Lungu was elected as the sixth President.
In 2010, the World Bank named Zambia one of the world's fastest economically reformed countries. The Common Market for Eastern and Southern Africa (COMESA) is headquartered in Lusaka.
Etymology.
The territory of what is now Zambia was known as Northern Rhodesia from 1911. It was renamed Zambia at independence in 1964.
The new name of Zambia was derived from the Zambezi river (Zambezi may mean "God's river") .
History.
Pre-colonial.
The area of modern Zambia was inhabited by Khoisan until around AD 300, when the migrating Bantu began to settle around these areas. In the 12th century, major waves of Bantu-speaking immigrants arrived during the Bantu expansion. Among them, the Tonga people (also called Ba-Tonga, "Ba-" meaning "men") were the first to settle in Zambia and are believed to have come from the east near the "big sea".
The Nkoya people also arrived early in the expansion, coming from the Luba–Lunda kingdoms located in the southern parts of the modern Democratic Republic of the Congo and northern Angola, followed by a much larger influx, especially between the late 12th and early 13th centuries.
At the end of the 18th century, some of the Mbunda migrated to Barotseland, Mongu upon the migration of among others, the Ciyengele. The Aluyi and their leader, the Litunga Mulambwa, especially valued the Mbunda for their fighting ability.
In the early 19th century, the Nsokolo people settled in the Mbala district of Northern Province. During the 19th century, the Ngoni and Sotho peoples arrived from the south. By the late 19th century, most of the various peoples of Zambia were established in their current areas.
European contact.
The earliest European to visit the area was Portuguese explorer Francisco de Lacerda in the late 18th century. This territory, located between Portuguese Mozambique and Portuguese Angola, was claimed and explored by Portugal in that period. Other European visitors followed in the 19th century. The most prominent of these was David Livingstone, who had a vision of ending the slave trade through the "3 Cs": Christianity, Commerce and Civilization.
He was the first European to see the magnificent waterfalls on the Zambezi River in 1855, naming them the Victoria Falls after Queen Victoria. He described them thus: "Scenes so lovely must have been gazed upon by angels in their flight".
Locally the falls are known as "Mosi-o-Tunya" or "thundering smoke" in the Lozi or Kololo dialect. The town of Livingstone, near the Falls, is named after him. Highly publicised accounts of his journeys motivated a wave of European visitors, missionaries and traders after his death in 1873.
Northern Rhodesia (1888–1964).
In 1888, the British South Africa Company (BSA Company), led by Cecil Rhodes, obtained mineral rights from the Litunga, the Paramount Chief of the Lozi (Ba-rotse) for the area which later became Barotziland-North-Western Rhodesia. To the east, in December 1897 a group of the Angoni or Ngoni (originally from Zululand) rebelled under Tsinco, son of King Mpezeni, but the rebellion was put down, and Mpezeni accepted the Pax Britannica. That part of the country then came to be known as North-Eastern Rhodesia. In 1895, Rhodes asked his American scout Frederick Russell Burnham to look for minerals and ways to improve river navigation in the region, and it was during this trek that Burnham discovered major copper deposits along the Kafue River.
North-Eastern Rhodesia and Barotziland-North-Western Rhodesia were administered as separate units until 1911 when they were merged to form Northern Rhodesia, a British protectorate. In 1923, the BSA Company ceded control of Northern Rhodesia to the British Government after the government decided not to renew the Company's charter.
That same year, Southern Rhodesia (now Zimbabwe), a conquered territory which was also administered by the BSA Company, became a self-governing British colony. In 1924, after negotiations, administration of Northern Rhodesia transferred to the British Colonial Office. In 1953, the creation of the Federation of Rhodesia and Nyasaland grouped together Northern Rhodesia, Southern Rhodesia and Nyasaland (now Malawi) as a single semi-autonomous region. This was undertaken despite opposition from a sizeable minority of the population, who demonstrated against it in 1960–61. Northern Rhodesia was the centre of much of the turmoil and crisis characterising the federation in its last years. Initially, Harry Nkumbula's African National Congress (ANC) led the campaign, which Kenneth Kaunda's United National Independence Party (UNIP) subsequently took up.
A two-stage election held in October and December 1962 resulted in an African majority in the legislative council and an uneasy coalition between the two African nationalist parties. The council passed resolutions calling for Northern Rhodesia's secession from the federation and demanding full internal self-government under a new constitution and a new National Assembly based on a broader, more democratic franchise.
The federation was dissolved on 31 December 1963, and in January 1964, Kaunda won the only election for Prime Minister of Northern Rhodesia. The Colonial Governor, Sir Evelyn Hone, was very close to Kaunda and urged him to stand for the post. Soon after, there was an uprising in the north of the country known as the Lumpa Uprising led by Alice Lenshina – Kaunda's first internal conflict as leader of the nation.
Post-Independence (1964–).
Northern Rhodesia became the Republic of Zambia on 24 October 1964, with Kenneth Kaunda as the first president. At independence, despite its considerable mineral wealth, Zambia faced major challenges. Domestically, there were few trained and educated Zambians capable of running the government, and the economy was largely dependent on foreign expertise. This expertise was provided in part by John Willson CMG There were over 70,000 Europeans resident in Zambia in 1964, and they remained of disproportionate economic significance.
Kaunda's endorsement of Patriotic Front guerrillas conducting raids into neighbouring (Southern) Rhodesia resulted in political tension and a militarisation of the border, leading to its closure in 1973. The Kariba hydroelectric station on the Zambezi River provided sufficient capacity to satisfy the country's requirements for electricity, despite Rhodesian management.
A railway (TAZARA - Tanzania Zambia Railways) to the Tanzanian port of Dar es Salaam, completed in 1975 with Chinese assistance, reduced Zambian dependence on railway lines south to South Africa and west through an increasingly troubled Portuguese Angola. Until the completion of the railway, Zambia's major artery for imports and the critical export of copper was along the TanZam Road, running from Zambia to the port cities in Tanzania. The Tazama oil pipeline was also built from Dar es Salaam to Ndola in Zambia.
By the late 1970s, Mozambique and Angola had attained independence from Portugal. Rhodesia's predominantly white government, which issued a Unilateral Declaration of Independence in 1965, accepted majority rule under the Lancaster House Agreement in 1979.
Civil strife in both Portuguese colonies and a mounting Namibian War of Independence resulted in an influx of refugees and compounded transportation issues. The Benguela railway, which extended west through Angola, was essentially closed to Zambian traffic by the late 1970s. Zambia's support for anti-apartheid movements such as the African National Congress (ANC) also created security problems as the South African Defence Force struck at dissident targets during external raids.
In the mid-1970s, the price of copper, Zambia's principal export, suffered a severe decline worldwide. In Zambia's situation, the cost of transporting the copper great distances to market was an additional strain. Zambia turned to foreign and international lenders for relief, but, as copper prices remained depressed, it became increasingly difficult to service its growing debt. By the mid-1990s, despite limited debt relief, Zambia's per capita foreign debt remained among the highest in the world.
In June 1990 riots against Kaunda accelerated. Many protesters were killed by the regime in breakthrough June 1990 protests. In 1990 Kaunda survived an attempted coup, and in 1991 he agreed to reinstate multiparty democracy, having instituted one party rule under the Choma Commission of 1972. Following multiparty elections, Kaunda was removed from office (see below).
In the 2000s, the economy stabilized, attaining single-digit inflation in 2006–2007, real GDP growth, decreasing interest rates, and increasing levels of trade. Much of its growth is due to foreign investment in mining and to higher world copper prices. All this led to Zambia being courted enthusiastically by aid donors, and saw a surge in investor confidence in the country.
Politics.
Politics in Zambia take place in a framework of a presidential representative democratic republic, whereby the President of Zambia is both head of state and head of government in a pluriform multi-party system. The government exercises executive power, while legislative power is vested in both the government and parliament.
Zambia became a republic immediately upon attaining independence in October 1964. From 2011 to 2014, Zambia's president had been Michael Sata, until Sata died on 28 October 2014.
After Sata's death, Vice President Guy Scott, a Zambian of Scottish descent, became acting President of Zambia. On 24 January 2015 it was announced that Edgar Changwa Lungu had won the election to become the 6th President in a tightly contested race. He won 48.33% of the vote, a lead of 1.66% over his closest rival, Hakainde Hichilema, with 46.67%. 9 other candidates all got less than 1% each.
Foreign relations.
After independence in 1964 the foreign relations of Zambia were mostly focused on supporting liberation movements in other countries in Southern Africa, such as the African National Congress and SWAPO. During the Cold War Zambia was a member of the Non-Aligned Movement.
Zambia is involved in a border dispute concerning the convergence of the boundaries of Botswana, Namibia, Zambia and Zimbabwe. An additional dispute with the Democratic Republic of Congo concerns the Lunchinda-Pweto Enclave.
Military.
The Zambian Defence Force (ZDF) consists of the army, the air force, and the Zambian National Service (ZNS). The ZDF is designed primarily against external threats.
Subdivisions.
Zambia is divided into ten provinces, each administered by an appointed deputy minister. Each province is subdivided into several districts with a grand total of 89 districts.
The provinces are:
Geography.
Zambia is a landlocked country in southern Africa, with a tropical climate, and consists mostly of high plateaus with some hills and mountains, dissected by river valleys. At 752614 km2 it is the 39th-largest country in the world, slightly smaller than Chile. The country lies mostly between latitudes 8° and 18°S, and longitudes 22° and 34°E.
Zambia is drained by two major river basins: the Zambezi/Kafue basin in the centre, west and south covering about three-quarters of the country; and the Congo basin in the north covering about one-quarter of the country. A very small area in the northeast forms part of the internal drainage basin of Lake Rukwa in Tanzania.
In the Zambezi basin, there are a number of major rivers flowing wholly or partially through Zambia: the Kabompo, Lungwebungu, Kafue, Luangwa, and the Zambezi itself, which flows through the country in the west and then forms its southern border with Namibia, Botswana and Zimbabwe. Its source is in Zambia but it diverts into Angola, and a number of its tributaries rise in Angola's central highlands. The edge of the Cuando River floodplain (not its main channel) forms Zambia's southwestern border, and via the Chobe River that river contributes very little water to the Zambezi because most is lost by evaporation.
Two of the Zambezi's longest and largest tributaries, the Kafue and the Luangwa, flow mainly in Zambia. Their confluences with the Zambezi are on the border with Zimbabwe at Chirundu and Luangwa town respectively. Before its confluence, the Luangwa River forms part of Zambia's border with Mozambique. From Luangwa town, the Zambezi leaves Zambia and flows into Mozambique, and eventually into the Mozambique Channel.
The Zambezi falls about 100 m over the 1.6 km wide Victoria Falls, located in the south-west corner of the country, subsequently flowing into Lake Kariba. The Zambezi valley, running along the southern border, is both deep and wide. From Lake Kariba going east it is formed by grabens and like the Luangwa, Mweru-Luapula, Mweru-wa-Ntipa and Lake Tanganyika valleys, is a rift valley.
The north of Zambia is very flat with broad plains. In the west the most notable being the Barotse Floodplain on the Zambezi, which floods from December to June, lagging behind the annual rainy season (typically November to April). The flood dominates the natural environment and the lives, society and culture of the inhabitants and those of other smaller, floodplains throughout the country.
In Eastern Zambia the plateau which extends between the Zambezi and Lake Tanganyika valleys is tilted upwards to the north, and so rises imperceptibly from about 900 m in the south to 1200 m in the centre, reaching 1800 m in the north near Mbala. These plateau areas of northern Zambia have been categorised by the World Wildlife Fund as a large section of the Central Zambezian Miombo woodlands ecoregion.
Eastern Zambia shows great diversity. The Luangwa Valley splits the plateau in a curve north east to south west, extended west into the heart of the plateau by the deep valley of the Lunsemfwa River. Hills and mountains are found by the side of some sections of the valley, notably in its north-east the Nyika Plateau (2200 m) on the Malawi border, which extend into Zambia as the Mafinga Hills, containing the country's highest point, Mafinga Central (2339 m).
The Muchinga Mountains, the watershed between the Zambezi and Congo drainage basins, run parallel to the deep valley of the Luangwa River and form a sharp backdrop to its northern edge, although they are almost everywhere below 1700 m. Their culminating peak Mumpu is at the western end and at 1892 m is the highest point in Zambia away from the eastern border region. The border of the Congo Pedicle was drawn around this mountain.
The southernmost headstream of the Congo River rises in Zambia and flows west through its northern area firstly as the Chambeshi and then, after the Bangweulu Swamps as the Luapula, which forms part of the border with the Democratic Republic of the Congo. The Luapula flows south then west before it turns north until it enters Lake Mweru. The lake's other major tributary is the Kalungwishi River, which flows into it from the east. The Luvua River drains Lake Mweru, flowing out of the northern end to the Lualaba River (Upper Congo River).
Lake Tanganyika is the other major hydrographic feature that belongs to the Congo basin. Its south-eastern end receives water from the Kalambo River, which forms part of Zambia's border with Tanzania. This river has Africa's second highest uninterrupted waterfall, the Kalambo Falls.
Climate.
The climate of Zambia is tropical, modified by elevation. In the Köppen climate classification, most of the country is classified as humid subtropical or tropical wet and dry, with small stretches of semi-arid steppe climate in the south-west and along the Zambezi valley.
There are two main seasons, the rainy season (November to April) corresponding to summer, and the dry season (May/June to October/November), corresponding to winter. The dry season is subdivided into the cool dry season (May/June to August), and the hot dry season (September to October/November). The modifying influence of altitude gives the country pleasant subtropical weather rather than tropical conditions during the cool season of May to August. However, average monthly temperatures remain above 20 °C over most of the country for eight or more months of the year.
Demographics.
Zambia is one of the most highly urbanised countries in sub-Saharan Africa with 44% of the population concentrated in a few urban areas along the major transport corridors, while rural areas are sparsely populated. The fertility rate was 6.2 as of 2007 (6.1 in 1996, 5.9 in 2001–02).
Ethnic groups.
The population comprises approximately 73 ethnic groups, most of which are Bantu-speaking. Almost 90% of Zambians belong to the nine main ethnolinguistic groups: the Nyanja-Chewa, Bemba, Tonga, Tumbuka, Lunda, Luvale, Kaonde, Nkoya and Lozi. In the rural areas, each ethnic group is concentrated in a particular geographic region of the country and many groups are very small and not as well known. However, all the ethnic groups can be found in significant numbers in Lusaka and the Copperbelt.
Expatriates, mostly British or South African, as well as some white Zambian citizens, live mainly in Lusaka and in the Copperbelt in northern Zambia, where they are either employed in mines, financial and related activities or retired. There were 70,000 Europeans in Zambia in 1964, but many have since left the country.
Zambia has a small but economically important Asian population, most of whom are Indians and Chinese. There are 13,000 Indians in Zambia. An estimated 80,000 Chinese are resident in Zambia. In recent years, several hundred dispossessed white farmers have left Zimbabwe at the invitation of the Zambian government, to take up farming in the Southern province.
According to the "World Refugee Survey 2009" published by the US Committee for Refugees and Immigrants, Zambia had a population of refugees and asylum seekers numbering approximately 88,900. The majority of refugees in the country came from the Democratic Republic of Congo (47,300 refugees from the DRC living in Zambia in 2007), Angola (27,100; see Angolans in Zambia), Zimbabwe (5,400) and Rwanda (4,900).
Beginning in May 2008, the number of Zimbabweans in Zambia also began to increase significantly; the influx consisted largely of Zimbabweans formerly living in South Africa who were fleeing xenophobic violence there. Nearly 60,000 refugees live in camps in Zambia, while 50,000 are mixed in with the local populations. Refugees who wish to work in Zambia must apply for official permits which can cost up to $500 per year.
Population centres.
The Europeans in the protectorate numbered 14,000 at the 1931 census and the Africans numbered 1,400,000. Of the Europeans, more than 10,000 had entered the country in the previous ten years, since the census in 1921 (mostly to work on the copper mines). In 1938 there were only eight doctors in the entire country.
Languages.
The official language of Zambia is English, which is used to conduct official business and is the medium of instruction in schools. The main local language, especially in Lusaka, is Nyanja, followed by Bemba. In the Copperbelt Bemba is the main language and Nyanja second. Bemba and Nyanja are spoken in the urban areas in addition to other indigenous languages which are commonly spoken in Zambia. These include Lozi, Kaonde, Tonga, Lunda and Luvale, which feature on the Zambia National Broadcasting Corporation (ZNBC)'s local languages section. The total number of languages spoken in Zambia is 73.
The process of urbanisation has had a dramatic effect on some of the indigenous languages, including the assimilation of words from other indigenous languages and English. Urban dwellers sometimes differentiate between urban and rural dialects of the same language by prefixing the rural languages with 'deep'.
Most will thus speak Bemba and Nyanja in the Copperbelt; Nyanja is dominantly spoken in Lusaka and Eastern Zambia. English is used in official communications and is the language of choice at home among – now common – intertribal families. This continuous evolution of languages has led to Zambian slang which can be heard in daily life throughout Lusaka and other major cities. Portuguese has been introduced into the school curriculum due to the presence of a large Portuguese-speaking Angolan community. French is commonly studied in private schools, while some secondary schools have it as an optional subject. A German course has been introduced at the University of Zambia (UNZA).
Religion.
Religion in Zambia (2010)
  Protestant (68%)   Roman Catholic
 (21%)  Other Christian (9%)  Other or Non Religious (2.5%)
Zambia is officially a Christian nation according to the 1996 constitution, but a wide variety of religious traditions exist. Traditional religious thoughts blend easily with Christian beliefs in many of the country's syncretic churches. Christian denominations include: Roman Catholic, Anglican, Pentecostal, New Apostolic Church, Lutheran, Jehovah's Witnesses, Seventh-day Adventist, The Church of Jesus Christ of Latter-day Saints, Branhamites, and a variety of Evangelical denominations.
These grew, adjusted and prospered from the original missionary settlements (Portuguese and Catholicism in the east from Mozambique) and Anglicanism (British influences) from the south. Except for some technical positions (e.g. physicians), Western missionary roles have been assumed by native believers. After Frederick Chiluba (a Pentecostal Christian) became President in 1991, Pentecostal congregations expanded considerably around the country. Approximately 87% of the population are Christians. It has one of the largest percentage of Seventh-day Adventist per head in the world, about 1 in 18 Zambians.<br>
1 in 11 Zambians is member of the New Apostolic Church. With membership above 1.200.000 the Zambia district is 3rd large after Congo east and East Africa (Nairobi).
The Baha'i population of Zambia is over 160,000, or 1.5% of the population. The William Mmutle Masetlha Foundation run by the Baha'i community is particularly active in areas such as literacy and primary health care. Approximately 1% of the population are Muslims with most living in urban areas and play a large economic role in the country, of which about 500 belong to the Ahmadiyya sect in Islam. There is also a small Jewish community, composed mostly of Ashkenazis.
Economy.
Presently, Zambia averages between $7 billion and $8 billion of exports annually. About 68% of Zambians live below the recognized national poverty line, with rural poverty rates standing at about 78% and urban rates of 53%. Unemployment and underemployment in urban areas are serious problems. Most rural Zambians are subsistence farmers.
Zambia ranked 117th out of 128 countries on the 2007 Global Competitiveness Index, which looks at factors that affect economic growth. Social indicators continue to decline, particularly in measurements of life expectancy at birth (about 40.9 years) and maternal mortality (830 per 100,000 pregnancies). The country's rate of economic growth cannot support rapid population growth or the strain which HIV/AIDS-related issues place on the economy.
Zambia fell into poverty after international copper prices declined in the 1970s. The socialist regime made up for falling revenue with several abortive attempts at International Monetary Fund structural adjustment programmes (SAPs). The policy of not trading through the main supply route and line of rail to the sea – the territory known as Rhodesia (from 1965 to 1979), and now known as Zimbabwe – cost the economy greatly. After the Kaunda regime, (from 1991) successive governments began limited reforms. The economy stagnated until the late 1990s. In 2007 Zambia recorded its ninth consecutive year of economic growth. Inflation was 8.9%, down from 30% in 2000.
Zambia is still dealing with economic reform issues such as the size of the public sector and improving Zambia's social sector delivery systems. Economic regulations and red tape are extensive, and corruption is widespread. The bureaucratic procedures surrounding the process of obtaining licences encourages the widespread use of facilitation payments. Zambia's total foreign debt exceeded $6 billion when the country qualified for Highly Indebted Poor Country Initiative (HIPC) debt relief in 2000, contingent upon meeting certain performance criteria. Initially, Zambia hoped to reach the HIPC completion point, and benefit from substantial debt forgiveness, in late 2003.
In January 2003, the Zambian government informed the International Monetary Fund and World Bank that it wished to renegotiate some of the agreed performance criteria calling for privatisation of the Zambia National Commercial Bank and the national telephone and electricity utilities. Although agreements were reached on these issues, subsequent overspending on civil service wages delayed Zambia's final HIPC debt forgiveness from late 2003 to early 2005, at the earliest. In an effort to reach HIPC completion in 2004, the government drafted an austerity budget for 2004, freezing civil service salaries and increasing a number of taxes. The tax hike and public sector wage freeze prohibited salary increases and new hires. This sparked a nationwide strike in February 2004.
The Zambian government is pursuing an economic diversification program to reduce the economy's reliance on the copper industry. This initiative seeks to exploit other components of Zambia's rich resource base by promoting agriculture, tourism, gemstone mining, and hydro-power.
Agriculture.
Agriculture plays a very important part in Zambia's economy providing many more jobs than the mining industry. Private local company is the leading agri-business in Zambia with over 4.000 employees, producing row crops (5.000 ha irrigated, 1.500 ha non-irrigated), cattle (Zambeef), pork (Master Pork), chicken (ZamChick), eggs (ZamChick Egg), dairy products, leather, fish, feedstock (Novatek) and edible oil (Zamanita). Zambeef operates eight abattoirs, four farms and numerous retail stores (also in co-operation with Shoprite) and a fast-food chain (ZamChick Inn) throughout the country.
Mining.
The Zambian economy has historically been based on the copper mining industry. Output of copper had fallen to a low of 228,000 metric tons in 1998 after a 30-year decline in output due to lack of investment, low copper prices, and uncertainty over privatisation. In 2002, following privatisation of the industry, copper production rebounded to 337,000 metric tons. Improvements in the world copper market have magnified the effect of this volume increase on revenues and foreign exchange earnings.
In 2003, exports of nonmetals increased by 25% and accounted for 38% of all export earnings, previously 35%. The Zambian government has recently been granting licenses to international resource companies to prospect for minerals such as nickel, tin, copper and uranium.
It is hoped that nickel will take over from copper as the country's top metallic export. In 2009, Zambia has been badly hit by the world economic crisis.
Energy.
In 2009, Zambia generated 10.3 Twh and has been rated high in use of both Solar power and Hydroelectricity.
Social protection.
Zambia officially has extensive social protection targeted at low-capacity households, including social assistance (protection) and social insurance programmes (prevention), and programmes to improve economic productivity (promotion). However, these programmes face immense challenges and the actual coverage is very low and, in some cases, actually declining. Some analysts describe the programmes' coverage as patchy and transitory and not especially coherent or logical.
Public works, such as PUSH, and cash transfers are the main instruments used to protect consumption among low-capacity households by providing (1) seasonal safety nets to address cyclical poverty and vulnerability at times of need by offering employment and (2) community assets that are beneficial for productive activities. In practice, however, the programme prioritises food transfers to areas affected by natural disasters where vulnerability is acute and infrastructure development has remained a secondary objective. NGOs also have implemented short-term public works programmes implemented by NGOs, such as CARE's agricultural inputs-for-assets (AICA) programme.
Social insurance initiatives, such as micro-insurance, health insurance and other contributory schemes exist, but these are very limited in their membership. Formal sector workers are protected by well-resourced pension, sickness and disability benefits, but most low-capacity households, especially in rural areas, work outside the formal sector.
The emphasis on protection at the expense of prevention and promotion means that households move out of poverty only very slowly because they are unable to invest in activities that have greater returns. They remain highly at risk of sliding back into poverty and applying negative coping strategies. A balance between protection, prevention and promotion, however can only be achieved through more and consistent resources. Further improvements might also include
Social protection for LGBT is non-existent in Zambia and any expression thereof is illegal.
Education.
In Zambia, the education system consists of both government and private schools. Historically, the private school system began largely as a result of Christian mission efforts during the late 19th and early 20th centuries. In schools, a student may initially receive two levels of education; basic education (years 1 to 9), and upper secondary (years 10 to 12). Some schools provide a "basic" education covering years 1 to 9, as year 9 is considered to be a decent level of education for the majority of children. UNESCO estimated that 80% of children of primary school age in 2002 were enrolled.
In 2003, the adult literacy rate was estimated to be 80.6% (86.8% male and 74.8% female). Zambia has a first OLPC – One Laptop Per Child – deployment at the new Zambian library that is in traditional Zambian Style.
Higher education.
In Zambia, there are three universities and several technical schools that provide higher education. The in Zambia was also developed in 1992 to foster growth in technological fields. Educational opportunities beyond secondary school are limited in Zambia. After secondary school, most students study at the various colleges, around the country. Normally they all select students on the basis of ability; competition for places is intense.
The introduction of fees in the late 1990s has made university level education inaccessible for some, although the government does provide state bursaries. Copperbelt University opened in the late 1980s, taking over most of the former Zambia Institute of Technology site in Kitwe. There are also several teacher training colleges offering two-year training programmes, while missionary hospitals around the country offer internationally acceptable training for nurses. Several Christian schools offer seminary-level training.
There are three main universities and several others:
Additional Centres of Higher Education:
Health.
The Ministry of Health (MOH) provides information pertaining to Zambian health. In 2010, public expenditure on health was 3.4% of GDP, among the lowest in southern Africa. The 2014 CIA estimated average life expectancy in Zambia was 51.83 years.
HIV/AIDS epidemic.
Zambia faces a generalised HIV epidemic, with an estimated prevalence rate of 13.5% among adults (ages 15–49) in 2009. HIV incidence in Zambia has declined by more than 25% from 2001 to 2010, an indication that the epidemic appears to be declining.
Hospitals.
In Zambia, there are hospitals throughout the country which include: Levy Mwanawasa General Hospital, Chipata General Hospital, Kitwe Central Hospital, Konkola Mine Hospital, Lubwe Mission Hospital, Maacha Hospital, Mtendere Mission Hospital, Mukinge Mission Hospital, Mwandi Mission Hospital, Nchanga North Hospital, Chikankata Salvation Army Hospital, Kalene Mission Hospital, , and .
The University Teaching Hospital serves as both a hospital and a training site for future health workers. There are very few hospitals in rural or remote places in Zambia, where most communities rely on small government-run community health centres and rural health posts.
Maternal and child health care.
The 2010 maternal mortality rate per 100,000 births for Zambia is 470. This is compared with 602.9 in 2008 and 594.2 in 1990. The under-5 mortality rate, per 1,000 births is 145 and the neonatal mortality as a percentage of under 5's mortality is 25.
In Zambia the number of midwives per 1,000 live births is 5 and the lifetime risk of death for pregnant women is 1 in 38. Female genital mutilation (FGM), while not widespread, is practiced in parts of the country. According to the 2009 "Zambia Sexual Behaviour Survey", 0.7% of women have undergone FGM. According to UNICEF, 45% of children under five years are stunted.
Culture.
Prior to the establishment of modern Zambia, the natives lived in independent tribes, each with their own ways of life. One of the results of the colonial era was the growth of urbanisation. Different ethnic groups started living together in towns and cities, influencing each other as well as adopting a lot of the European culture. The original cultures have largely survived in the rural areas. In the urban setting there is a continuous integration and evolution of these cultures to produce what is now called "Zambian culture".
Traditional culture is very visible through colourful annual Zambian traditional ceremonies. Some of the more prominent are: Kuomboka and Kathanga (Western Province), Mutomboko (Luapula Province), Ncwala (Eastern Province), Lwiindi and Shimunenga (Southern Province), Lunda Lubanza (North Western), Likumbi Lyamize (North Western), Mbunda Lukwakwa (North Western Province), Chibwela Kumushi (Central Province), Vinkhakanimba (Muchinga Province), Ukusefya Pa Ng’wena (Northern Province).
Popular traditional arts are mainly in pottery, basketry (such as Tonga baskets), stools, fabrics, mats, wooden carvings, ivory carvings, wire craft and copper crafts. Most Zambian traditional music is based on drums (and other percussion instruments) with a lot of singing and dancing. In the urban areas foreign genres of music are popular, in particular Congolese rumba, African-American music and Jamaican reggae. Several psychedelic rock artists emerged in the 1970s to create a genre known as Zam-rock, including WITCH, Musi-O-Tunya, Rikki Ililonga, Amanaz, the Peace, Chrissy Zebby Tembo, Blackfoot, and the Ngozi Family.
Media.
The is responsible for the Zambian News Agency, while there are also numerous media outlets throughout the country which include; television stations, newspapers, FM radio stations, and Internet news websites.
Cuisine.
The Zambian staple diet is based on maize. It is normally eaten as a thick porridge, called "nshima" (a Nyanja word), prepared from maize flour, commonly known as mealie meal. In some regions of Zambia, "nshima" is made from dried, pounded cassava root that is sifted into a flour and whipped in hot water. This "nshima" has almost no nutritional value, but is very common in the poor villages of Northwestern, Northern, and Luapula regions of Zambia.
"Nshima" may be eaten with a variety of vegetables, beans, meat, insects, fish or sour milk depending on geographical location/origin.
Sports.
Zambia declared its independence on the day of the closing ceremony of the 1964 Summer Olympics, thereby becoming the first country ever to have entered an Olympic games as one country, and left it as another. Zambia took part in the 2008 Summer Olympics in Beijing.
Football is the most popular sport in Zambia, and the Zambia national football team has had its triumphant moments in football history. At the Seoul Olympics of 1988, the national team defeated the Italian national team by a score of 4–0. Kalusha Bwalya, Zambia's most celebrated football player and one of Africa's greatest football players in history had a hat trick in that match. However, to this day, many pundits say the greatest team Zambia has ever assembled was the one that perished on 28 April 1993 in a plane crash at Libreville, Gabon. Despite this, in 1996, Zambia was ranked 15th on the official FIFA World Football Team rankings, the highest attained by any southern African team. In 2012, Zambia won the African Cup of Nations for the first time after losing in the final twice. They beat Côte d'Ivoire 8–7 in a penalty shoot-out in the final, which was played in Libreville, just a few kilometres away from the plane crash 19 years previously.
Rugby Union, boxing and cricket are also popular sports in Zambia. Notably, at one point in the early 2000s, the Australia and South Africa national rugby teams were captained by players born in the same Lusaka hospital, George Gregan and Corné Krige. Zambia boasts having the highest rugby poles in the world, located at Luanshya Sports Complex in Luanshya.
Rugby union in Zambia is a minor but growing sport. They are currently ranked 73rd by the IRB and have 3,650 registered players and three "formally organised" clubs. Zambia used to play cricket as part of Rhodesia. Zambia has also strangely provided a shinty international, Zambian-born Eddie Tembo representing Scotland in the compromise rules Shinty/Hurling game against Ireland in 2008.
In 2011, Zambia was due to host the tenth All-Africa Games, for which three stadiums were to be built in Lusaka, Ndola, and Livingstone. The Lusaka stadium would have a capacity of 70,000 spectators while the other two stadiums would hold 50,000 people each. The government was encouraging the private sector to get involved in the construction of the sports facilities because of a shortage of public funds for the project. Zambia has since revoked its bid to host the 2011 All-Africa Games, citing a lack of funds. Hence, Mozambique took Zambia's place as host.
Zambia also produced the first black African (Madalitso Muthiya) to play in the United States Golf Open, one of the four major golf tournaments.

</doc>
<doc id="34427" url="http://en.wikipedia.org/wiki?curid=34427" title="Zulu people">
Zulu people

The Zulu (Zulu: amaZulu) are a Bantu ethnic group of Southern Africa and the largest ethnic group in South Africa, with an estimated 10–11 million people living mainly in the province of KwaZulu-Natal. Small numbers also live in Zimbabwe, Zambia, Tanzania and Mozambique. Their language, Zulu, is a Bantu language; more specifically, part of the Nguni subgroup.
Origins.
The Zulu were originally a major clan in what is today Northern KwaZulu-Natal, founded ca. 1709 by Zulu kaNtombela. In the Nguni languages, "iZulu/iliZulu/liTulu" means "heaven", or "sky." At that time, the area was occupied by many large Nguni communities and clans (also called "isizwe"=nation, people or "isibongo"=clan). Nguni communities had migrated down Africa's east coast over centuries, as part of the Bantu migrations probably arriving in what is now South Africa in about the 9th century.
Kingdom.
The Zulu formed a powerful state in 1818 under the leader Shaka. Shaka, as the Zulu King, gained a large amount of power over the tribe. As commander in the army of the powerful Mthethwa Empire, he became leader of his mentor Dingiswayo's paramouncy and united what was once a confederation of tribes into an imposing empire under Zulu hegemony.
Conflict with the British.
On 11 December 1878, agents of the British delivered an ultimatum to 11 chiefs representing Cetshwayo. The terms forced upon Cetshwayo required him to disband his army and accept British authority. Cetshwayo refused, and war followed January 12, 1879. During the war, the Zulus defeated the British at the Battle of Isandlwana on 22 January. The British managed to get the upper hand after the battle at Rorke's Drift, and subsequently win the war with the Zulu being defeated at the Battle of Ulundi on 4 July.
Absorption into Natal.
After Cetshwayo's capture a month following his defeat, the British divided the Zulu Empire into 13 "kinglets". The sub-kingdoms fought amongst each other until 1883 when Cetshwayo was reinstated as king over Zululand. This still did not stop the fighting and the Zulu monarch was forced to flee his realm by Zibhebhu, one of the 13 kinglets, supported by Boer mercenaries. Cetshwayo died in February 1884, killed by Zibhebhu's regime, leaving his son, the 15 year-old Dinuzulu, to inherit the throne. In-fighting between the Zulu continued for years, until Zululand was absorbed fully into the British colony of Natal.
Apartheid years.
KwaZulu homeland.
Under apartheid, the homeland of KwaZulu ("Kwa" meaning "place of") was created for Zulu people. In 1970, the Bantu Homeland Citizenship Act provided that all Zulus would become citizens of KwaZulu, losing their South African citizenship. KwaZulu consisted of a large number of disconnected pieces of land, in what is now KwaZulu-Natal. Hundreds of thousands of Zulu people living on privately owned "black spots" outside of KwaZulu were dispossessed and forcibly moved to bantustans – worse land previously reserved for whites contiguous to existing areas of KwaZulu – in the name of "consolidation." By 1993, approximately 5.2 million Zulu people lived in KwaZulu, and approximately 2 million lived in the rest of South Africa. The Chief Minister of KwaZulu, from its creation in 1970 (as Zululand) was Chief Mangosuthu Buthelezi. In 1994, KwaZulu was joined with the province of Natal, to form modern KwaZulu-Natal.
"Inkatha YeSizwe".
"Inkatha YeSizwe" means "the crown of the nation". In 1975, Buthelezi revived the Inkatha YaKwaZulu, predecessor of the Inkatha Freedom Party. This organization was nominally a protest movement against apartheid, but held more conservative views than the ANC. For example, Inkatha was opposed to the armed struggle, and to sanctions against South Africa. Inkatha was initially on good terms with the ANC, but the two organizations came into increasing conflict beginning in 1976 in the aftermath of the Soweto Uprising.
Modern Zulu population.
The modern Zulu population is fairly evenly distributed in both urban and rural areas. Although KwaZulu-Natal is still their heartland, large numbers have been attracted to the relative economic prosperity of Gauteng province. Indeed, Zulu is the most widely spoken home language in the province, followed by Sotho.
Language.
The language of the Zulu people is "isiZulu", a Bantu language; more specifically, part of the Nguni subgroup. Zulu is the most widely spoken language in South Africa, where it is an official language. More than half of the South African population are able to understand it, with over 9 million first-language and over 15 million second-language speakers. Many Zulu people also speak Afrikaans, English, Portuguese, Xitsonga, Sesotho and others from among South Africa's 11 official languages.
Clothing.
Zulus wear a variety of attire, both traditional for ceremonial or culturally celebratory occasions, and modern westernized clothing for everyday use.The women dress differently depending on whether they are single, engaged, or married.
Religion and beliefs.
Most Zulu people state their beliefs to be Christian. Some of the most common churches to which they belong are African Initiated Churches, especially the Zion Christian Church and United African Apostolic Church, although membership of major European Churches, such as the Dutch Reformed, Anglican and Catholic Churches are also common. Nevertheless, many Zulus retain their traditional pre-Christian belief system of ancestor worship in parallel with their Christianity.
Zulu religion includes belief in a creator God ("Unkulunkulu") who is above interacting in day-to-day human affairs, although this belief appears to have originated from efforts by early Christian missionaries to frame the idea of the Christian God in Zulu terms. Traditionally, the more strongly held Zulu belief was in ancestor spirits ("Amatongo" or "Amadhlozi"), who had the power to intervene in people's lives, for good or ill. This belief continues to be widespread among the modern Zulu population.
Traditionally, the Zulu recognize several elements to be present in a human being: the physical body ("inyamalumzimba" or "umzimba"); the breath or life force ("umoyalumphefumulo" or "umoya"); and the "shadow," prestige, or personality ("isithunzi"). Once the "umoya" leaves the body, the "isithunzi" may live on as an ancestral spirit ("idlozi") only if certain conditions were met in life. Behaving with ubuntu, or showing respect and generosity towards others, enhances one's moral standing or prestige in the community, one's "isithunzi". By contrast, acting in a negative way towards others can reduce the "isithunzi", and it is possible for the "isithunzi" to fade away completely.
In order to appeal to the spirit world, a diviner ("sangoma") must invoke the ancestors through divination processes to determine the problem. Then, a herbalist ("inyanga") prepares a mixture ("muthi") to be consumed in order to influence the ancestors. As such, diviners and herbalists play an important part in the daily lives of the Zulu people. However, a distinction is made between white "muthi" ("umuthi omhlope"), which has positive effects, such as healing or the prevention or reversal of misfortune, and black "muthi" ("umuthi omnyama"), which can bring illness or death to others, or ill-gotten wealth to the user. Users of black "muthi" are considered witches, and shunned by society.
Christianity had difficulty gaining a foothold among the Zulu people, and when it did it was in a syncretic fashion. Isaiah Shembe, considered the Zulu Messiah, presented a form of Christianity (the Nazareth Baptist Church) which incorporated traditional customs.

</doc>
<doc id="34436" url="http://en.wikipedia.org/wiki?curid=34436" title="Zambian Defence Force">
Zambian Defence Force

The Zambian Defence Force (ZDF) consists of the Zambia Army, the Air Force, and Zambian National Service (ZNS). The ZDF is designed primarily for internal defence in Zambia. Being a landlocked country, Zambia has no navy.
Air Force.
The Zambian Air Force is a small air force equipped with Mikoyan-Gurevich MiG-21MFs and Mikoyan-Gurevich MiG-19S in addition to a range of transport aircraft and helicopters. boo. ذرحكم

</doc>
<doc id="34443" url="http://en.wikipedia.org/wiki?curid=34443" title="Zocchihedron">
Zocchihedron

Zocchihedron is the trademark of a 100-sided die invented by Lou Zocchi, which debuted in 1985. Rather than being a polyhedron, it is more like a ball with 100 flattened planes. It is sometimes called "Zocchi's Golfball".
Zocchihedra are designed to handle percentage rolls in games, particularly in role-playing games.
History.
It took three years for Zocchi to design his die, and three more years to get it into production. Zocchi discovered that the die would perform best in water at a depth of 13.85 mm, so it floats. Since its introduction, Zocchi has improved the design of the Zocchihedron, filling it with teardrop-shaped free-falling weights to make it settle more swiftly when rolled.
The Zocchihedron 2 is a further improved model, and has another filler.
Probability distribution of rolls.
A test published in "White Dwarf" magazine concluded that the frequency distribution of the Zocchihedron was substantially uneven. Jason Mills noticed that some numbers were spaced closer together than others, and suspected that this non-uniform placement would cause some numbers to predictably come up more than others. Mills performed 5,164 rolls and the results confirmed these suspicions; some numbers came up significantly more than others. Most notably, rolls of more than 93 or less than 8 were significantly rarer than middling results. Not coincidentally, these numbers were all spaced closer together near the "poles" of the die, as opposed to numbers near the "equator" that are more widely spaced.
After the test was published, the numbering on the Zocchihedron was redesigned with higher and lower numbers placed more evenly across all parts of the die. While numbers spaced closely together near the "poles" still come up less often, the numbers that are placed in these areas are more evenly distributed between 1 and 100, instead of consisting mainly of very high and very low numbers.
Patents.
The aesthetic appearance of the Zocchihedron was protected by United States design patent D303,553, which expired on 19 September 2003. There was never a utility patent for the original Zocchihedron, although United States patent 6,926,276 may protect the braking mechanism of the Zocchihedron II. That patent will expire on 9 August 2025 and applies only to "spherical dice" containing "multi sized and irregularly shaped particles."

</doc>
<doc id="34446" url="http://en.wikipedia.org/wiki?curid=34446" title="Zachris Topelius">
Zachris Topelius

Zachris Topelius (]; 14 January 1818 – 12 March 1898) was a Swedish-speaking Finnish author, journalist, historian, and rector of the University of Helsinki who wrote novels related to Finnish history in Swedish.
Life and career.
Zacharias is his baptismal name, and this is used on the covers of his printed works. However, "he himself most often used the abbreviation Z. or the form Zachris, even in official contexts", as explained in the National Biography of Finland. Zachris is therefore the preferred form used in recent academic literature about him.
The original name of the Topelius family was the Finnish name Toppila which had been Latinized to Toppelius by the author's grandfather's grandfather and later changed to Topelius. Topelius was born at Kuddnäs, near Nykarleby in Ostrobothnia, the son of a physician of the same name (Zacharias Topelius the Elder), who was distinguished as the earliest collector of Finnish folk-songs. As a child he heard his mother, Katarina Sofia Calamnius, sing the songs of the Finnish-Swedish poet Franzén. At the age of eleven, he was sent to school in Oulu and boarded with relatives in the possession of a lending library, where he nurtured his fantasy with the reading of novels.
He came to Helsinki in 1831 and became a member of the circle of young nationalist men surrounding Johan Ludvig Runeberg, in whose home he stayed for some time. Topelius became a student at the Imperial Alexander University of Finland in 1833, received his master's degree ("cand. philol.") in 1840, the Licentiate degree in history in 1844 and his PhD in 1847, having defended a dissertation titled "De modo matrimonia jungendi apud fennos quondam vigente" ("About the custom of marriage among the ancient Finns"). Besides history, his academic studies had for periods been devoted both to Theology and Medicine. He was secretary of Societas pro fauna et flora fennica 1842–1846, was employed by the university library 1846–1861, and taught History, Statistics and Swedish at the school Helsingfors lyceum during the same period.
Through the intervention of a friend, Fredrik Cygnæus, Topelius was named professor extraordinary of the History of Finland at the University in 1854. He was made first ordinary professor of Finnish, Russian and Nordic history in 1863, and exchanged this chair for the one in general history in 1876. He was rector of the university from 1875 until 1878, when he retired as Emeritus Professor and received the title of "verkligt statsråd" (Russian: действительный статский советник; literally "state councillor", a Russian honorary title).
Quite early in his career he began to distinguish himself as a lyric poet, with the three successive volumes of his "Heather Blossoms" (1845–54). The earliest of his historical romances was "The Duchess of Finland", published in 1850. He was also editor-in-chief of the Helsingfors Gazette from 1841 to 1860. In 1878, Topelius was allowed to withdraw from his professional duties, but this did not sever his connection with the university; it gave him, however, more leisure for his abundant and various literary enterprises. Of all the multifarious writings of Topelius, in prose and verse, that which has enjoyed the greatest popularity is his "Tales of a Barber-Surgeon", episodes of historical fiction from the days of Gustavus II. Adolphus to those of Gustavus III., treated in the manner of Sir Walter Scott; the five volumes of this work appeared at intervals between 1853 and 1867. Topelius attempted the drama also, with most success in his tragedy of "Regina von Emmeritz" (1854). Topelius aimed at the cultivation of a strong Finnish patriotism.
Together with the composer Friedrich Pacius he wrote the libretto (in the style of Romantic nationalism) to the first Finnish opera: "Kung Karls jakt". Topelius initially thought of writing a trivial entertainment, but having heard extracts from the opera project at a concert in 1851, he realized that Pacius was writing a grand opera on the theme of salvation, following the early Romantic style of Carl Maria von Weber's "Der Freischütz" (1821) and "Oberon" (1826). Topelius wrote the libretto in Swedish (though it was translated later by others), but its subject is emphatically Finnish. He also wrote the libretto for "Prinsessan av Cypern", set by Fredrik Pacius and Lars-Erik Larsson.
Many of his works employed esoterical allegories harking to ancient mysteries and perhaps rosicrucian and alchemical themes, but on the other hand some of his short works examined the effects of the strong industrialisation of Finnish society.
Topelius died in his manor house of Koivuniemi, in Sipoo, Finland, where he wrote his greatest works.

</doc>
<doc id="34455" url="http://en.wikipedia.org/wiki?curid=34455" title="Zakat">
Zakat

Zakāt (Arabic: زكاة‎ ], "that which purifies",) is a form of obligatory alms-giving and religious tax in Islam. It is based on income and the value of all of one's possessions. It is customarily 2.5% of a Muslim's total income, savings and wealth above a minimum amount known as "nisab", but Islamic scholars differ on how much nisab is and other aspects of zakat. The collected amount is paid first to zakat collectors, and then to poor Muslims, to new converts to Islam, to Islamic clergy, to those fighting for Islamic causes, and others.
Zakat is is one of the Five Pillars of Islam, mandatory for all Muslims. Zakat is not a charitable contribution, and is considered as a tax. The payment and disputes on zakat have played a major role in the history of Islam, such as the Ridda wars that fueled the conflict between the Sunni and Shia sects of Islam.
Etymology.
Zakat, literally means "that which purifies". 
Zakat is considered a way to purify one's income and wealth from sometimes worldly, impure way of acquisition. 
According to Murata and Chittick, "Just as ablutions purify the body and salāt purifies the soul (in Islam), so zakat purifies possessions and makes them pleasing to God."
It is sometimes referred to as "Zakat al-mal".
Scripture.
Qur'an.
Qur'an discusses charity in many verses, some of which relate to zakat. The word zakat, with the meaning used in Islam now, is found in suras: 7:156, 19:31, 19:55, 21:72, 23:4, 27:3, 30:39, 31:3 and 41:7. 
Zakat is found in the early Medinan suras and described as obligatory for Muslims. It is given for the sake of salvation. Muslims believe those who give zakat can expect reward from God in the afterlife, neglecting to give zakat can result in damnation. Zakat is considered part of the covenant between God and a Muslim. 
Qur'an makes zakat as one of three prerequisites as to when a pagan becomes a Muslim, through verse 9.5: "but if they repent, establish prayers, and practice zakat they are your brethren in faith".
The Qur'an lists the beneficiaries of zakat (discussed below).
Hadith.
Each of the most trusted hadiths in Islam have a book dedicated to zakat. Sahih Bukhari's Book 24, Sahih Muslim's Book 5, and Sunan Abu-Dawud Book 9 discuss various aspects of zakat, including who must pay, how much, when and what. The 2.5% rate is also recited in the hadiths.
The hadiths admonish those who do not give the zakat. According to the hadith, refusal to pay or mockery of those who pay zakat is a sign of hypocrisy, and God will not accept the prayers of such people. The sunna also describe God's punishment for those who refuse or fail to pay zakat. On the day of Judgment, those who didn't give the zakat will be held accountable and punished.
The hadith contain advice on the state-authorized collection of the zakat. The collectors are required not to take more than what is due, and those who are paying the zakat are asked not to evade payment. The hadith also warn of punishment to those who take zakat when they are not eligible to receive it (see beneficiaries of zakat).
History.
"Zakat", an Islamic practice initiated by the Islamic prophet Muhammad, has played an important role throughout its history. Schact suggests that the idea of zakat may have entered Islam from Judaism, with roots in the Hebrew and Aramaic word "zakut". However, some Islamic scholars disagree that the Qur'anic verses on zakat (or zakah) have roots in Judaism.
The caliph Abū Bakr, believed by Sunni Muslims to be Muhammad's successor, was the first to institute a statutory "zakat" system. Abu Bakr established the principle that the zakat must be paid to the legitimate representative of the Prophet's authority, himself. Other Muslims disagreed and refused to pay zakat to Abu Bakr, leading to accusations of apostasy, the Ridda wars.
The second and third caliphs, Umar bin Al-Khattab and Usman ibn Affan, continued Abu Bakr's codification of the zakat. Uthman also modified the "zakat" collection protocol by decreeing that only "apparent" wealth was taxable, which had the effect of limiting "zakat" to mostly being paid on agricultural land and produce. During the reign of Ali ibn Abu Talib, the issue of zakat was tied to legitimacy of his government. After Ali, his supporters refused to pay the zakat to Muawiyah I, as they did not recognize his legitimacy.
The practice of Islamic state-administered "zakat" was short-lived in Medina. During the reign of Umar bin Abdul Aziz (717–720 A.D.), it is reported that no one in Medina needed the zakat. After him, zakat came to be considered more of an individual responsibility. This view changed over Islamic history. Sunni Muslims and rulers, for example, considered collection and disbursement of zakat as one of the functions of an Islamic state; this view has continued in modern Islamic countries.
Practice.
Zakat is one of the five pillars of Islam, and is expected to be paid by all practicing Muslims who have the financial means ("nisab"). In addition to their "zakat" obligations, Muslims are encouraged to make voluntary contributions ("sadaqat"). The "zakat" is not collected from non-Muslims, although they are required to pay the "jizyah" tax.
Contemporary Muslim world.
According to researcher Russell Powell, as of 2010 Zakat was mandatory by state law in Libya, Malaysia, Pakistan, Saudi Arabia, Sudan, and Yemen. There were government-run voluntary Zakat contribution programs in Bahrain, Bangladesh, Egypt, Indonesia, Iran, Jordan, Kuwait, Lebanon, and the United Arab Emirates,
Who pays.
Those required to pay Zakat must meet the following criteria: 
Amount.
The amount of Zakat to be paid by an individual depends on the amount of money and the type of assets the individual possesses. The Quran does not provide specific guidelines on which types of wealth are taxable under the "zakat", nor does it specify percentages to be given. But the customary practice is that the amount of zakat paid on capital assets (e.g. money) is 2.5% (1/40). Zakat is additionally payable on agricultural goods, precious metals, minerals, and livestock at a rate varying between 2.5 (1/40) and 20 percent, depending on the type of goods.
Zakat is usually payable on assets continuously owned over one lunar year that are in excess of the "nisab", a minimum monetary value. However, Islamic scholars have disagreed with each other on zakat and nisab. For example, Abū Ḥanīfa, did not regard nisab limit to be a pre-requisite for zakat, in the case of land crops, fruits and minerals. Other differences between Islamic scholars on zakat and nisab is acknowledged as follows by Yusuf al-Qaradawi,
Unlike prayers, we observe that even the ratio, the exemption, the kinds of wealth that are zakatable are subject to differences among scholars. Such differences have serious implications for Muslims at large when it comes to their application of the Islamic obligation of zakah. For example, some scholars consider the wealth of children and insane individals zakatable, others don't. Some scholars consider all agricultural products zakatable, others restrict zakah to specific kinds only. Some consider debts zakatable, others don't. Similar differences exist for business assets and women's jewelry. Some require certain minimum (nisab) for zakatability, some don't. etc. The same kind of differences also exist about the disbursement of zakat.<br> – Shiekh Mahmud Shaltut
Collection.
Today, in most Muslim countries, zakat is at the discretion of Muslims over how and whether to pay, typically enforced by peer pressure, fear of God, and an individual's personal feelings. Among the Sunni Muslims, zakat committees are established, linked to a religious cause or local mosque, which collect zakat. Among the Shia Muslims, deputies on behalf of Imams collect the zakat.
In a handful of Muslim countries including Saudi Arabia, Malaysia and Pakistan, the zakat is obligatory and is collected by the state. In Jordan, Bahrain, Kuwait, Lebanon, and Bangladesh, the "zakat" is regulated by the state, but contributions are voluntary.
In discretion-based systems of collection, studies suggest zakat is collected from and paid only by a fraction of Muslim population who can pay.
In obligatory systems of zakat tax collection, such as Malaysia and Pakistan, evasion is very common and the alms tax is regressive. The state collects from the poorest, a larger fraction of their income, as well as the largest share of zakat is collected for lowest income groups every year. In one study from Malaysia, for example, 93% of zakat collected came from poor rice farmers. In Pakistan, property is exempt from the zakat calculation basis, and the compulsory zakat is primarily collected from the agriculture sector.
Failure to pay.
Traditional Islamic scholars and some rulers such as Abu Bakr have held the view that any Muslim who consciously refuses to pay zakat, is an apostate. Failure to pay zakat is a form of apostasy in Islam and the defaulter is subject to punishment. Additionally, those who fail to pay the zakat must face God's punishment in the afterlife and the day of Judgment foretold in Islam.
The consequence of failure to pay zakat is a controversial subject, particularly when a Muslim is willing to pay zakat but refuses to pay it to a certain group or the state. Some Muslim scholars do not consider such individuals as apostates.
Recipients.
According to the Quran's Surah Al-Tawba, there are eight categories of people ("asnaf") who qualify to benefit from "zakat" funds.
 “Alms are for the poor and the needy, and those employed to administer the (funds); for those whose hearts have been (recently) reconciled (to Truth); for those in bondage and in debt; in the cause of Allah; and for the wayfarer: (thus is it) ordained by Allah, and Allah is full of knowledge and wisdom.”
 — Qur'an, Sura 9 (Al-Tawba), ayat 60 have interpreted this verse as identifying the following eight categories of Muslim causes to be the proper recipients of zakat:
Zakat should not be given to one's own parents, grandparents, children, grandchildren, spouses or the descendants of the Prophet Muhammad.
Distribution.
Neither the Quran nor the Hadiths specify the relative division of zakat into the above eight categories. Muslim scholar Nuh Ha Mim Keller in the Reliance of the Traveller says zakat is to be distributed entirely and equally among the eight categories of recipients, but notes the Hanafi school permits zakat to be distributed to all the categories, some of them, or just one of them.:h8.7 Classical schools of Islamic law including Shafi'i, are unanimous that collectors of zakat are to be paid first, with the balance to be distributed equally amongst the remaining seven categories of recipients, even in cases where one group’s need is more demanding.
Muslim scholars disagree whether zakat recipients can include Non-Muslims. Islamic scholarship, historically, has taught that only Muslims can be recipients of zakat. In recent times, some state that zakat may be paid to non-Muslims, but only after the needs of Muslims have been met.
Depending on the region, the dominant portion of zakat went typically to "Amil" (the zakat collectors) or Sabīlillāh (those fighting for religious cause, the caretaker of local mosque, or those working in the cause of God such as proselytizing non-Muslims to convert to Islam). These primary sources of sharia also do not specify to whom the zakat should be paid - to zakat collectors claiming to represent one class of zakat beneficiary (for example, poor), collectors who were representing religious bodies, or collectors representing the Islamic state. This has caused significant conflicts and allegations of zakat abuse within the Islamic community, both historically and in modern times.
"Fi Sabillillah" is the most prominent "asnaf" in Southeast Asian Muslim societies, where it broadly construed to include funding missionary work, Quranic schools and anything else that serves the Islamic community ("ummah") in general. Zakat can be used to finance a Jihad effort in the path of Allah. Zakat money should be used provided the effort is to raise the banner of Islam.
Additionally, the zakat funds may be spent on the administration of a centralized zakat collection system. There is an ongoing controversy as to whether those claiming to be fighting a war on behalf of an Islamic country or for Islam can be a rightful recipient to zakat funds from Muslim community. Also, it is forbidden to disburse zakat funds into investments instead of being given to one of the above eight categories of recipients.
Role in society.
In theory.
The "zakat" is considered by Muslims to be an act of piety through which one expresses concern for the well-being of fellow Muslims, as well as preserving social harmony between the wealthy and the poor. "Zakat" promotes a more equitable redistribution of wealth and fosters a sense of solidarity amongst members of the "Ummah".
In practice.
In 2012, Islamic financial analysts estimated annual zakat spending exceeded $200 billion per year, which they estimated at 15 times global humanitarian aid contributions. Islamic scholars and development workers state that much of this zakat practice is mismanaged, wasted or ineffective. About a quarter of the Muslim world continues to live on $1.25 a day or less, according to the 2012 report.
In a 2014 study, Nasim Shirazi states widespread poverty persists in Islamic world despite zakat collections every year. Over 70% of the Muslim population in most Muslim countries is impoverished and lives on less than $2 per day. In over 10 Muslim-majority countries, over 50% of the population lived on less than $1.25 per day income, states Shiraz. In Indonesia - the world's most populous and predominantly Muslim country – 50% of Muslims live on less than $2 per day. This suggest large scale waste and mismanagement by those who collect and spend zakat funds. Given the widespread poverty among Muslim-majority countries, the impact of zakat in practice, despite the theoretical intent and its use for centuries, has been questioned by scholars. Zakat has so far failed to relieve large scale absolute poverty among Muslims in most Muslim countries.
Comparative charity practice.
In the United Kingdom, according to a self-reported poll of 4000 people, three out of ten U.K. Muslims gave to charity. According to a nbc poll, British Muslims, on average, gave $567 to charity in 2013, compared to $412 for Jews, $308 for Protestants, $272 for Catholics and $177 for atheists in Britain.
Related terms.
Zakat is required of Muslims only. For non-Muslims living in an Islamic state, sharia traditionally mandates jizya (poll tax). Other forms of taxation on Muslims or non-Muslims, that have been used in Islamic history, include "kharaj" (land tax), "khums" (tax on booty and loot seized from non-Muslims, sudden wealth), "ushur" (tax at state border, sea port, and each city border on goods movement, customs), "kari" (house tax) and "chari" (sometimes called "maara", pasture tax).
There are differences in the interpretation and scope of zakat and other related taxes in various sects of Islam. For example, "khums" is interpreted differently by Sunnis and Shi'ites, with Shia expected to pay one fifth of their excess income after expenses as "khums", and Sunni don't. At least a tenth part of zakat and khums every year, among Shi'ites, after its collection by Imam and his religious deputies under its doctrine of "niyaba", goes as income for its hierarchical system of Shia clergy. Among Ismaili sub-sect of Shias, the mandatory taxes which includes zakat, is called "dasond", and 20% of thus collected amount from its Muslim faithfuls, is set aside as income for the Imams. Some branches of Shia Islam treat the right to lead as Imam and right to receive 20% of collected zakat and other alms as a hereditary right of its clergy.
Sadaqah is another related term for charity, but not mandatory for Muslims.
"Zakat al-Fitr".
"Zakat al-Fitr" is a mandatory one time alms-giving per person, with any means to pay, that is traditionally paid at the end of the fasting in the Islamic holy month of Ramadan. The collected amount is used to pay the zakat collectors and to the poor Muslims so that they can be provided with a means with which they can celebrate `Eid al-Fitr (the festival of breaking the fast) along with the rest of the Muslims.
Zakat al Fitr is based on person and is a fixed amount collected, while Zakat al mal is based on the total income and property.
See also.
Charity practices in other religions:

</doc>
<doc id="34468" url="http://en.wikipedia.org/wiki?curid=34468" title="Zechariah (Hebrew prophet)">
Zechariah (Hebrew prophet)

Zechariah (; Hebrew: זְכַרְיָה,  "Zekharya",  "Zəḵaryā", "YHWH has remembered"; Arabic: زكريّا‎ "Zakariya'" or "Zakkariya"; Greek: Ζαχαρίας "Zakharias"; Latin: "Zacharias") was a person in the Hebrew Bible and traditionally considered the author of the Book of Zechariah, the eleventh of the Twelve Minor Prophets. He was a prophet of the two-tribe Kingdom of Judah, and like Ezekiel was of priestly extraction.
Prophet.
According to and Iddo was the father of the prophet Zechariah. According to Berechiah was the father of Zechariah, and Iddo was his grandfather. According to "The Interpreter's Bible,
This discrepancy is best explained on the supposition that the words 'the son of Berechiah' did not form part of the original text of 1:1 - had they done so, it is very improbable that they would have been omitted in the Ezra passages - but that they are an insertion on the part of someone who identified the prophet Zechariah with Zechariah the son of Jeberechiah, who is mentioned in Isaiah 8:2, Berechiah in Zech. 1:1 being a corruption of Jeberechiah.
 Another explanation might be that 'father' is used in a broader sense, meaning that Iddo was Zecharia's father in the sense that he was antecedent to him. His prophetical career began in the second year of Darius, king of Persia (520 BC), about sixteen years after the return of the first company from their Babylonian exile. According to the Book of Ezra he was contemporary with Haggai.
Not much is known about Zechariah’s life other than what may be inferred from the book. It has been speculated that Iddo was the head of a priestly family who returned with Zerubbabel (), and that Zechariah may himself have been a priest as well as a prophet. This is supported by Zechariah's interest in the Temple and the priesthood, and from Iddo's preaching in the Books of Chronicles.
Possibility of martyrdom.
In the Gospel of Matthew Jesus is quoted as stating that Zechariah son of Barachiah was killed between the altar and the temple. A similar quotation is also found in the Gospel of Luke: "from the blood of Abel unto the blood of Zachariah, who perished between the altar and the sanctuary: yea, I say unto you, it shall be required of this generation." (Luke: 11:51, ASV)
Although there is an indication in Targum Lamentations that "Zechariah son of Iddo" was killed in the Temple, scholars generally understand this as a reference to the death of a much earlier figure, Zechariah ben Jehoiada. As Abel was the first prophetic figure killed in the Hebrew Scriptures, and Zechariah ben Jehoiada was the last figure killed in those scriptures, which conclude with 1 and 2 Chronicles, they represent the full historical scope of prophetic martyrdom. By using their names, Jesus brings to bear on the Jewish establishment of his day the cumulative guilt for killing those prophets, to which within a few days (in Matthew's chronology) they would add his own death. The logic of the accusation means that the reference is almost certainly to Zechariah ben Jehoiada.
Bahá'í Faith.
Bahá'í teachers have made comparisons between the prophecies of Zechariah and the "Súriy-i-Haykal" in the Summons of the Lord of Hosts, a collection of the Tablets of Bahá’u’lláh.[]
Islam.
The Qur'an mentions only 25 prophets by name, including a "different" Zechariah, the father of John the Baptist. Muslims believe that many prophets were sent to mankind to spread the message of God, including many not mentioned in the Qur'an. Therefore, although this particular Zechariah is not mentioned by name in the Qur'an, some scholars, including Abdullah Yusuf Ali have suggested that Qur'anic verses mentioning the martyrdom of prophets and righteous men are a reference to the slaying of, among others, Zechariah son of Berechiah.
Liturgical commemoration.
On the Eastern Orthodox liturgical calendar, his feast day is February 8. He is commemorated with the other Minor Prophets in the calendar of saints of the Armenian Apostolic Church on July 31.
References.
 incorporates text from a publication now in the public domain: 

</doc>
<doc id="34487" url="http://en.wikipedia.org/wiki?curid=34487" title="Zoop">
Zoop

Zoop is a puzzle game developed by Hookstone Productions and published by Viacom New Media. Some of its rules resemble those of the arcade game "Plotting" (known in some territories as "Flipull"), but unlike "Plotting", "Zoop" runs in real time. Official "Zoop" games have been released for Game Boy, Game Gear, Mega Drive/Genesis, SNES, Atari Jaguar, Sega Saturn (in Japan only), PlayStation, DOS, and Macintosh. To spark interest for the game, Blockbuster offered the game as a free rental for the Super Nintendo for a limited time.
Gameplay.
The player controls a triangle in the center of the screen. Every second (or more often in advanced levels), a piece comes in from the side and possibly pushes other pieces forward. Two consecutive pieces will never come in from the same quadrant, and runs of consecutive identical pieces on one row are longer, statistically, than one might think. If a piece falls into the center square, the game is over.
If the player shoots a piece of the same color as their triangle, it will be "zooped" (cleared) and points are earned. If the piece behind the target piece is also of the same color, it is also "zooped". The same goes for the next piece, and so on. In the example gameplay screenshot, shooting to the left of the position in the screenshot will "zoop" the green pieces and return the player to the center, facing right (the opposite direction).
If a piece of a color different from the player's current piece is shot, the player's piece will switch colors with it. This is also what happens when a piece of a different color is encountered after zooping one or more pieces of the same color. In the example, shooting down would bounce off the orange piece (leaving a green piece behind) and return with the orange piece.
When the quota of "zooped" pieces is met, the game speeds up, and (before level 10) the background changes.
Various special pieces do different things:
Points.
Generally speaking, every cleared piece is worth 100 points. In the case of zooping more than one piece at once, each piece is worth 100 points more than the piece before it. For example, zooping 3 pieces results in 100 + 200 + 300 = 600 points. In addition, if a row is full (one more piece being added will cause a loss of game) and all the pieces are of the same color, zooping the row earns a bonus of 5,000 points for the smaller rows on the top and bottom, and 10,000 for the rows on the left and right. All pieces cleared as a result of any of the four powerup items are worth 100 points.
Opti-Challenge.
To make matters even more difficult, the game also employed what was referred to as "opti-challenge" backgrounds. As the game progressed, the backgrounds would become increasingly distracting. Early on, this would involve the use of contrasting colors, and increasingly intricate color schemes. Background patterns would also become more intricate and would make subtle use of asymmetrical elements. Ultimately, the background on level 9 employed black and white tiles, roughly the size of the invading pieces, while the center square contained a picture of clouds, which expanded to fill the screen on levels 10 and later. Although the opti-challenge element of the game was used as a selling point, very little information exists about the technique itself, and no other game on the market has ever openly claimed to use opti-challenge graphics.
Sound and music.
The DOS version of the game supports various sound cards, and features wavetable-like MIDI music. The sound effects have a cartoonish tone to match the vivid colors used through the stages. The music is basically jazz, and "evolves" with the game. The title and options screens, and the first stages, feature "smooth jazz" tunes. As the levels get harder, the music gets more and more tense, adding to the fast-paced atmosphere of the game.
Reception.
"Sega Saturn Magazine" (previously "Sega Magazine") gave the Genesis/Mega Drive version a 62%, saying the game "has the curious compulsiveness of "Tetris" to a degree", but that it is overshadowed by more complex and graphically impressive games then on the market.

</doc>
<doc id="34513" url="http://en.wikipedia.org/wiki?curid=34513" title="0 (number)">
0 (number)

0 (zero; BrE: /ˈzɪərəʊ/ or AmE: /ˈziːroʊ/) is both a number
and the numerical digit used to represent that number in numerals.
It fulfills a central role in mathematics as the additive identity of the integers, real numbers, and many other algebraic structures. As a digit, 0 is used as a placeholder in place value systems. Names for the number 0 in English include zero, nought or (US) naught (), nil, or — in contexts where at least one adjacent digit distinguishes it from the letter "O" — oh or o (). Informal or slang terms for zero include zilch and zip. "Ought" and "aught" (), as well as "cipher", have also been used historically.
Etymology.
The word "zero" came into the English language via French "zéro" from Italian "zero", Italian contraction of Venetian "zevero" form of 'Italian "zefiro" via "ṣafira" or "ṣifr". In pre-Islamic time the word "ṣifr" (Arabic صفر) had the meaning 'empty'. "Sifr" evolved to mean zero when it was used to translate "śūnya" (Sanskrit: शून्य) from India. The first known English use of "zero" was in 1598.
The Italian mathematician Fibonacci (c.1170–1250), who grew up in North Africa and is credited with introducing the decimal system to Europe, used the term "zephyrum". This became "zefiro" in Italian, and was then contracted to "zero" in Venetian. The Italian word "" was already in existence (meaning "west wind" from Latin and Greek "zephyrus") and may have influenced the spelling when transcribing Arabic "ṣifr".
There are different words used for the number or concept of zero depending on the context. For the simple notion of lacking, the words "nothing" and "none" are often used. Sometimes the words "nought", "naught" and "aught" are used. Several sports have specific words for zero, such as "nil" in football, "love" in tennis and "a duck" in cricket. It is often called "oh" in the context of telephone numbers. Slang words for zero include "zip", "zilch", "nada", and "scratch." "Duck egg" or "goose egg "are also slang for zero.
History.
Egypt.
Ancient Egyptian numerals were base 10. They used hieroglyphs for the digits and were not positional. By 1740 BC, the Egyptians had a symbol for zero in accounting texts. The symbol nfr, meaning beautiful, was also used to indicate the base level in drawings of tombs and pyramids and distances were measured relative to the base line as being above or below this line.
Mesopotamia.
By the middle of the 2nd millennium BC, the Babylonian mathematics had a sophisticated sexagesimal positional numeral system. The lack of a positional value (or zero) was indicated by a "space" between sexagesimal numerals. By 300 BC, a punctuation symbol (two slanted wedges) was co-opted as a placeholder in the same Babylonian system. In a tablet unearthed at Kish (dating from about 700 BC), the scribe Bêl-bân-aplu wrote his zeros with three hooks, rather than two slanted wedges.
The Babylonian placeholder was not a true zero because it was not used alone. Nor was it used at the end of a number. Thus numbers like 2 and 120 (2×60), 3 and 180 (3×60), 4 and 240 (4×60), looked the same because the larger numbers lacked a final sexagesimal placeholder. Only context could differentiate them.
India.
The concept of zero as a number and not merely a symbol or an empty space for separation is attributed to India, where, by the 9th century AD, practical calculations were carried out using zero, which was treated like any other number, even in case of division.
The Indian scholar Pingala, of 2nd century BC or earlier, used binary numbers in the form of short and long syllables (the latter equal in length to two short syllables), a notation similar to Morse code. In his Chandah-sutras (prosody sutras), dated to 3rd or 2nd century BC, Pingala used the Sanskrit word "śūnya" explicitly to refer to zero. This is so far the oldest known use of śūnya to mean zero in India. The fourth Pingala sutra offers a way to accurately calculate large metric exponentiation, of the type (2)n, efficiently with less number of steps.
The earliest text to use a decimal place-value system, including a zero, is the Jain text from India entitled the "Lokavibhāga", dated 458 AD, where "śūnya" ("void" or "empty") was employed for this purpose. The first known use of special glyphs for the decimal digits that includes the indubitable appearance of a symbol for the digit zero, a small circle, appears on a stone inscription found at the Chaturbhuja Temple at Gwalior in India, dated 876 AD. There are many documents on copper plates, with the same small "o" in them, dated back as far as the sixth century AD, but their authenticity may be doubted.
In 498 AD, Indian mathematician and astronomer Aryabhata stated that "sthānāt sthānaṁ daśaguṇaṁ syāt;" i.e., "from place to place each is ten times the preceding," which is the origin of the modern decimal-based place value notation.
The rules governing the use of zero appeared for the first time in Brahmagupta's book "Brahmasputha Siddhanta (The Opening of the Universe)", written in 628 AD. Here Brahmagupta considers not only zero, but negative numbers, and the algebraic rules for the elementary operations of arithmetic with such numbers. In some instances, his rules differ from the modern standard. Here are the rules of Brahmagupta:
In saying zero divided by zero is zero, Brahmagupta differs from the modern position. Mathematicians normally do not assign a value to this, whereas computers and calculators sometimes assign NaN, which means "not a number." Moreover, non-zero positive or negative numbers when divided by zero are either assigned no value, or a value of unsigned infinity, positive infinity, or negative infinity.
Cambodia.
In the ruins of a 7th-century temple in the Mekong region, a stone tablet held the inscription “605” in Khmer numerals. The finding was dated to AD 683.
China.
The "Sunzi Suanjing", of unknown date but estimated to be dated from the 1st to 5th centuries, and Japanese records dated from the eighteenth century, describe how counting rods were used for calculations. According to "A History of Mathematics", the rods "gave the decimal representation of a number, with an empty space denoting zero." The counting rod system is considered a positional notation system.
Zero was not treated as a number at that time, but as a "vacant position", unlike the Indian mathematicians who developed the numerical zero. Ch'in Chiu-shao's 1247 "Mathematical Treatise in Nine Sections" is the oldest surviving Chinese mathematical text using a round symbol for zero. Chinese authors had been familiar with the idea of negative numbers by the Han Dynasty (2nd century AD), as seen in the "The Nine Chapters on the Mathematical Art", much earlier than the fifteenth century when they became well established in Europe.
Islamic world.
The Arabic-language inheritance of science was largely Greek, followed by Hindu influences. In 773, at Al-Mansur's behest, translations were made of many ancient treatises including Greek, Latin, Indian, and others.
Astronomical tables were prepared by Persian al-Khwarizmi prepared astronomical tables in AD 813 using Hindu numerals, and about 825, he published a book synthesizing Greek and Hindu knowledge and also contained his own contribution to mathematics including an explanation of the use of zero. This book was later translated into Latin in the 12th century under the title "Algoritmi de numero Indorum". This title means "al-Khwarizmi on the Numerals of the Indians". The word "Algoritmi" was the translator's Latinization of Al-Khwarizmi's name, and the word "Algorithm" or "Algorism" started meaning any arithmetic based on decimals.
Muhammad ibn Ahmad al-Khwarizmi, in 976, stated that if no number appears in the place of tens in a calculation, a little circle should be used "to keep the rows". This circle was called "ṣifr".
Greeks and Romans.
Records show that the ancient Greeks seemed unsure about the status of zero as a number. They asked themselves, "How can nothing "be" something?", leading to philosophical and, by the Medieval period, religious arguments about the nature and existence of zero and the vacuum. The paradoxes of Zeno of Elea depend in large part on the uncertain interpretation of zero.
By 130 AD, Ptolemy, influenced by Hipparchus and the Babylonians, was using a symbol for zero (a small circle with a long overbar) within a sexagesimal numeral system otherwise using alphabetic Greek numerals. Because it was used alone, not just as a placeholder, this Hellenistic zero was perhaps the first documented use of a "number" zero in the Old World. However, the positions were usually limited to the fractional part of a number (called minutes, seconds, thirds, fourths, etc.)—they were not used for the integral part of a number. In later Byzantine manuscripts of Ptolemy's "Syntaxis Mathematica" (also known as the "Almagest"), the Hellenistic zero had morphed into the Greek letter omicron (otherwise meaning 70).
Another zero was used in tables alongside Roman numerals by 525 (first known use by Dionysius Exiguus), but as a word, "nulla" meaning "nothing", not as a symbol. When division produced zero as a remainder, "nihil", also meaning "nothing", was used. These medieval zeros were used by all future medieval computists (calculators of Easter). The initial "N" was used as a zero symbol in a table of Roman numerals by Bede or his colleague around 725.
Medieval Europe.
Positional notation without the use of zero (using an empty space in tabular arrangements, or the word "kha" "emptiness") is known to have been in use in India from the 6th century. The earliest certain use of zero as a "decimal" positional digit dates to the 5th century mention in the text Lokavibhaga. The glyph for the zero digit was written in the shape of a dot, and consequently called "bindu" ("dot"). The dot had been used in Greece during earlier ciphered numeral periods.
The Hindu-Arabic numeral system (base 10) reached Europe in the 11th century, via the Iberian Peninsula through Spanish Muslims, the Moors, together with knowledge of astronomy and instruments like the astrolabe, first imported by Gerbert of Aurillac. For this reason, the numerals came to be known in Europe as "Arabic numerals". The Italian mathematician Fibonacci or Leonardo of Pisa was instrumental in bringing the system into European mathematics in 1202, stating:
After my father's appointment by his homeland as state official in the customs house of Bugia for the Pisan merchants who thronged to it, he took charge; and in view of its future usefulness and convenience, had me in my boyhood come to him and there wanted me to devote myself to and be instructed in the study of calculation for some days. There, following my introduction, as a consequence of marvelous instruction in the art, to the nine digits of the Hindus, the knowledge of the art very much appealed to me before all others, and for it I realized that all its aspects were studied in Egypt, Syria, Greece, Sicily, and Provence, with their varying methods; and at these places thereafter, while on business. I pursued my study in depth and learned the give-and-take of disputation. But all this even, and the algorism, as well as the art of Pythagoras, I considered as almost a mistake in respect to the method of the Hindus (Modus Indorum). Therefore, embracing more stringently that method of the Hindus, and taking stricter pains in its study, while adding certain things from my own understanding and inserting also certain things from the niceties of Euclid's geometric art. I have striven to compose this book in its entirety as understandably as I could, dividing it into fifteen chapters. Almost everything which I have introduced I have displayed with exact proof, in order that those further seeking this knowledge, with its pre-eminent method, might be instructed, and further, in order that the Latin people might not be discovered to be without it, as they have been up to now. If I have perchance omitted anything more or less proper or necessary, I beg indulgence, since there is no one who is blameless and utterly provident in all things. The nine Indian figures are: 9 8 7 6 5 4 3 2 1. With these nine figures, and with the sign 0 ... any number may be written.
Here Leonardo of Pisa uses the phrase "sign 0", indicating it is like a sign to do operations like addition or multiplication. From the 13th century, manuals on calculation (adding, multiplying, extracting roots, etc.) became common in Europe where they were called "algorismus" after the Persian mathematician al-Khwārizmī. The most popular was written by Johannes de Sacrobosco, about 1235 and was one of the earliest scientific books to be "printed" in 1488. Until the late 15th century, Hindu-Arabic numerals seem to have predominated among mathematicians, while merchants preferred to use the Roman numerals. In the 16th century, they became commonly used in Europe.
Americas.
The Mesoamerican Long Count calendar developed in south-central Mexico and Central America required the use of zero as a place-holder within its vigesimal (base-20) positional numeral system. Many different glyphs, including this partial quatrefoil——were used as a zero symbol for these Long Count dates, the earliest of which (on Stela 2 at Chiapa de Corzo, Chiapas) has a date of 36 BC.
Since the eight earliest Long Count dates appear outside the Maya homeland, it is assumed that the use of zero in the Americas predated the Maya and was possibly the invention of the Olmecs. Many of the earliest Long Count dates were found within the Olmec heartland, although the Olmec civilization ended by the 4th century BC, several centuries before the earliest known Long Count dates.
Although zero became an integral part of Maya numerals, with a different, empty tortoise-like "shell shape" used for many depictions of the "zero" numeral, it did not influence Old World numeral systems.
Quipu, a knotted cord device, used in the Inca Empire and its predecessor societies in the Andean region to record accounting and other digital data, is encoded in a base ten positional system. Zero is represented by the absence of a knot in the appropriate position.
Mathematics.
0 is the integer immediately preceding 1. Zero is an even number, because it is divisible by 2. 0 is neither positive nor negative. By most definitions 0 is a natural number, and then the only natural number not to be positive. Zero is a number which quantifies a count or an amount of null size. In most cultures, 0 was identified before the idea of negative things (quantities) that go lower than zero was accepted.
The value, or "number", zero is not the same as the "digit" zero, used in numeral systems using positional notation. Successive positions of digits have higher weights, so inside a numeral the digit zero is used to skip a position and give appropriate weights to the preceding and following digits. A zero digit is not always necessary in a positional number system, for example, in the number 02. In some instances, a leading zero may be used to distinguish a number.
Elementary algebra.
The number 0 is the smallest non-negative integer. The natural number following 0 is 1 and no natural number precedes 0. The number 0 may or may not be considered a natural number, but it is a whole number and hence a rational number and a real number (as well as an algebraic number and a complex number).
The number 0 is neither positive nor negative and appears in the middle of a number line. It is neither a prime number nor a composite number. It cannot be prime because it has an infinite number of factors and cannot be composite because it cannot be expressed by multiplying prime numbers (0 must always be one of the factors). Zero is, however, even.
The following are some basic (elementary) rules for dealing with the number 0. These rules apply for any real or complex number "x", unless otherwise stated.
The expression 0⁄0, which may be obtained in an attempt to determine the limit of an expression of the form "f"("x")⁄"g"("x") as a result of applying the lim operator independently to both operands of the fraction, is a so-called "indeterminate form". That does not simply mean that the limit sought is necessarily undefined; rather, it means that the limit of "f"("x")⁄"g"("x"), if it exists, must be found by another method, such as l'Hôpital's rule.
The sum of 0 numbers is 0, and the product of 0 numbers is 1. The factorial 0! evaluates to 1.
Physics.
The value zero plays a special role for many physical quantities. For some quantities, the zero level is naturally distinguished from all other levels, whereas for others it is more or less arbitrarily chosen. For example, for an absolute temperature (as measured in Kelvin) zero is the lowest possible value (negative temperatures are defined but negative temperature systems are not actually colder). This is in contrast to for example temperatures on the Celsius scale, where zero is arbitrarily defined to be at the freezing point of water. Measuring sound intensity in decibels or phons, the zero level is arbitrarily set at a reference value—for example, at a value for the threshold of hearing. In physics, the zero-point energy is the lowest possible energy that a quantum mechanical physical system may possess and is the energy of the ground state of the system.
Chemistry.
Zero has been proposed as the atomic number of the theoretical element tetraneutron. It has been shown that a cluster of four neutrons may be stable enough to be considered an atom in its own right. This would create an element with no protons and no charge on its nucleus.
As early as 1926, Professor Andreas von Antropoff coined the term neutronium for a conjectured form of matter made up of neutrons with no protons, which he placed as the chemical element of atomic number zero at the head of his new version of the periodic table. It was subsequently placed as a noble gas in the middle of several spiral representations of the periodic system for classifying the chemical elements.
Computer science.
The most common practice throughout human history has been to start counting at one, and this is the practice in early classic computer science programming languages such as Fortran and COBOL. However, in the late 1950s LISP introduced zero-based numbering for arrays while Algol 58 introduced completely flexible basing for array subscripts (allowing any positive, negative, or zero integer as base for array subscripts), and most subsequent programming languages adopted one or other of these positions. For example, the elements of an array are numbered starting from 0 in C, so that for an array of "n" items the sequence of array indices runs from 0 to "n"−1. This permits an array element's location to be calculated by adding the index directly to address of the array, whereas 1 based languages precalculate the array's base address to be the position one element before the first.
There can be confusion between 0 and 1 based indexing, for example Java's JDBC indexes parameters from 1 although Java itself uses 0-based indexing.
In databases, it is possible for a field not to have a value. It is then said to have a null value. For numeric fields it is not the value zero. For text fields this is not blank nor the empty string. The presence of null values leads to three-valued logic. No longer is a condition either "true" or "false", but it can be "undetermined". Any computation including a null value delivers a null result. Asking for all records with value 0 or value not equal 0 will not yield all records, since the records with value null are excluded.
A null pointer is a pointer in a computer program that does not point to any object or function. In C, the integer constant 0 is converted into the null pointer at compile time when it appears in a pointer context, and so 0 is a standard way to refer to the null pointer in code. However, the internal representation of the null pointer may be any bit pattern (possibly different values for different data types).
In mathematics −0 = +0 = 0, both −0 and +0 represent exactly the same number, i.e., there is no "negative zero" distinct from zero. In some signed number representations (but not the two's complement representation used to represent integers in most computers today) and most floating point number representations, zero has two distinct representations, one grouping it with the positive numbers and one with the negatives; this latter representation is known as negative zero.
Symbols and representations.
The modern numerical digit 0 is usually written as a circle or ellipse. Traditionally, many print typefaces made the capital letter O more rounded than the narrower, elliptical digit 0. Typewriters originally made no distinction in shape between O and 0; some models did not even have a separate key for the digit 0. The distinction came into prominence on modern character displays.
A slashed zero can be used to distinguish the number from the letter. The digit 0 with a dot in the center seems to have originated as an option on IBM 3270 displays and has continued with some modern computer typefaces such as Andalé Mono, and in some airline reservation systems. One variation uses a short vertical bar instead of the dot. Some fonts designed for use with computers made one of the capital-O–digit-0 pair more rounded and the other more angular (closer to a rectangle). A further distinction is made in falsification-hindering typeface as used on German car number plates by slitting open the digit 0 on the upper right side. Sometimes the digit 0 is used either exclusively, or not at all, to avoid confusion altogether.
Year label.
In the BC calendar era, the year 1 BC is the first year before AD 1; there is not a year zero. By contrast, in astronomical year numbering, the year 1 BC is numbered 0, the year 2 BC is numbered −1, and so on.
References.
</dl>

</doc>
<doc id="34553" url="http://en.wikipedia.org/wiki?curid=34553" title="1999">
1999

1999 ()
will be . Currently, it is the latest year, when officially written in Roman numerals, to have at least one C. The upcoming year is as late as 2090.
1999 was designated as:

</doc>
<doc id="34568" url="http://en.wikipedia.org/wiki?curid=34568" title="16th century">
16th century

The 16th century begins with the Julian year 1501 and ends with either the Julian or the Gregorian year 1600 (depending on the reckoning used; the Gregorian calendar introduced a lapse of 10 days in October 1582).
It is regarded by historians as the century in which the rise of the West occurred.
During the 16th century, Spain and Portugal explored the world's seas and opened world-wide oceanic trade routes. Large parts of the New World became Spanish and Portuguese colonies, and while the Portuguese became the masters of Asia's and Africa's Indian Ocean trade, the Spanish opened trade across the Pacific Ocean, linking the Americas with Asia.
In Europe, the Protestant Reformation gave a major blow to the authority of the papacy and the Roman Catholic Church. European politics became dominated by religious conflicts, with the groundwork for the epochal Thirty Years' War being laid towards the end of the century.
In the Middle East, the Ottoman Empire continued to expand, with the Sultan taking the title of Caliph, while dealing with a resurgent Persia. Iran and Iraq were caught by major popularity of the once-obscure Shiite sect of Islam under the rule of the Safavid dynasty of warrior-mystics, providing grounds for a Persia independent of the majority-Sunni Muslim world.
China evacuated the coastal areas, because of Japanese piracy. Japan was suffering a severe civil war at the time.
Mughal Emperor Akbar extended the power of the Mughal Empire to cover most of the Indian sub continent. His rule significantly influenced arts, and culture in the region.
Copernicus proposed the heliocentric universe, which was met with strong resistance, and Tycho Brahe refuted the theory of celestial spheres through observational measurement of the 1572 appearance of a Milky Way supernova. These events directly challenged the long-held notion of an immutable universe supported by Ptolemy and Aristotle, and led to major revolutions in astronomy and science.

</doc>
<doc id="34612" url="http://en.wikipedia.org/wiki?curid=34612" title="1948">
1948

1948 ()
will be .
Births.
February.
 

</doc>
<doc id="34622" url="http://en.wikipedia.org/wiki?curid=34622" title="1944">
1944

1944 ()
will be .
Events.
Below, events of World War II have the "WWII" prefix.

</doc>
<doc id="34637" url="http://en.wikipedia.org/wiki?curid=34637" title="1905">
1905

1905 ()
will be .
As the second year of the massive Russo-Japanese War began, more than 100,000 died in the largest world battles of that era, and the war chaos lead to a revolution against the Tsar. (Shostakovich's 11th Symphony is subtitled "The Year 1905" to commemorate this.) Canada and the U.S. expanded west, with the Alberta and Saskatchewan provinces and the founding of Las Vegas. 1905 is also the "annus mirabilis" of Albert Einstein, who published papers which lay the foundations for quantum physics, introduced the special theory of relativity, explained Brownian motion, and established mass–energy equivalence.

</doc>
<doc id="34645" url="http://en.wikipedia.org/wiki?curid=34645" title="11th century">
11th century

As a means of recording the passage of time, the 11th century is the period from 1001 to 1100 in accordance with the Julian calendar in the Common Era, and the 1st century of the 2nd millennium.
In the history of European culture, this period is considered the early part of the High Middle Ages. There was a sudden decline of Byzantine power and rise of Norman domination over much of Europe, along with the prominent role in Europe of notably influential popes. In Northern Italy, a growth of population in urban centers gave rise to early organized capitalism and more sophisticated, commercialized culture by the late 11th century.
In Song Dynasty China and the classical Islamic world, this century marked the high point for both classical Chinese civilization, science and technology, and classical Islamic science, philosophy, technology and literature.
Rival political factions at the Song Dynasty court created strife amongst the leading statesmen and ministers of the empire. Chola-era India and Fatimid-era Egypt, had reached their zenith in military might and international influence. The Western Chalukya Empire (the Chola's rival) also rose to power by the end of the century. In this century the Turkish Seljuk dynasty comes to power in the Middle East over the now fragmented Abbasid realm, while the first of the Crusades were waged towards the close of the century. In Japan, the Fujiwara clan continued to dominate the affairs of state. In the Americas, the Toltec and Mixtec civilizations flourished in central America, along with the Huari Culture of South America and the Mississippian culture of North America. In Ukraine, there was the golden age for the principality of Kievan Rus. In Korea, the Goryeo Kingdom flourished and faced external threats from the Liao Dynasty (Manchuria). In Vietnam, the Lý Dynasty began, while in Myanmar the Pagan Kingdom reached its height of political and military power.
Overview.
In European history, the 11th century is regarded as the beginning of the High Middle Ages, an age subsequent to the Early Middle Ages. The century began while the "translatio imperii" of 962 was still somewhat novel and ended in the midst of the Investiture Controversy. It saw the final Christianisation of Scandinavia and the emergence of the Peace and Truce of God movements, the Gregorian Reforms, and the Crusades which revitalised a church and a papacy that had survived tarnished by the tumultuous 10th century. In 1054, the Great Schism rent the church in two, however.
In Germany, the century was marked by the ascendancy of the Holy Roman Emperors, who hit their high-water mark under the Salians.
In Italy, it opened with the integration of the kingdom into the empire and the royal palace at Pavia was summoned in 1024. By the end of the century, Lombard and Byzantine rule in the Mezzogiorno had been usurped by the Normans and the power of the territorial magnates was being replaced by that of the citizens of the cities in the north.
In Britain, it saw the transformation of Scotland into a single, more unified and centralised kingdom and the Norman conquest of England in 1066. The social transformations wrought in these lands brought them into the fuller orbit of European feudal politics.
In France, it saw the nadir of the monarchy and the zenith of the great magnates, especially the dukes of Aquitaine and Normandy, who could thus foster such distinctive contributions of their lands as the pious warrior who conquered Britain, Italy, and the East and the impious peacelover, the troubadour, who crafted out of the European vernacular its first great literary themes. There were also the first figures of the intellectual movement known as Scholasticism, which emphasized dialectic arguments in disputes of Christian theology as well as classical philosophy.
In Spain, the century opened with the successes of the last caliphs of Córdoba and ended in the successes of the Almoravids. In between was a period of Christian unification under Navarrese hegemony and success in the Reconquista against the taifa kingdoms that replaced the fallen caliphate.
In China, there was a triangular affair of continued war and peace settlements between the Song Dynasty, the Tanguts-led Western Xia in the northwest, and the Khitans of the Liao Dynasty in the northeast. Meanwhile, opposing political factions evolved at the Song imperial court of Kaifeng. The political reformers at court, called the New Policies Group (新法, Xin Fa), were led by Emperor Shenzong of Song and the Chancellors Fan Zhongyan and Wang Anshi, while the political conservatives were led by Chancellor Sima Guang and Empress Dowager Gao, regent of the young Emperor Zhezong of Song. Heated political debate and sectarian intrigue followed, while political enemies were often dismissed from the capital to govern frontier regions in the deep south where malaria was known to be very fatal to northern Chinese people (see History of the Song Dynasty). This period also represents a high point in classical Chinese science and technology, with figures such as Su Song and Shen Kuo, as well as the age where the matured form of the Chinese pagoda was accomplished in Chinese architecture.
In India, the Chola Dynasty reached its height of naval power under leaders such as Rajaraja Chola I and Rajendra Chola I, dominating southern India (Tamil Nadu), Sri Lanka, and regions of South East Asia. They also sent raids into what is now Thailand.
In Japan, the Fujiwara clan dominated central politics by acting as imperial regents, controlling the actions of the Emperor of Japan, who acted merely as a 'puppet monarch' during the Heian period.
In the Middle East, the Fatimid Empire of Egypt reached its zenith only to face steep decline, much like the Byzantine Empire in the first half of the century. The Seljuks came to prominence while the Abbasid caliphs held traditional titles without real, tangible authority in state affairs.
In Nigeria, formation of city states, kingdoms and empires, including Hausa kingdoms and Borno dynasty in north, Oyo and Benin kingdoms in south.
In Korea, the rulers of the Goryeo Kingdom were able to concentrate more central authority into their own hands than in that of the nobles, and were able to fend off two Khitan invasions with their armies.

</doc>
<doc id="34687" url="http://en.wikipedia.org/wiki?curid=34687" title="1623">
1623

Year 1623 (MDCXXIII) was a common year starting on Sunday (link will display the full calendar) of the Gregorian calendar and a common year starting on Wednesday of the 10-day slower Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="34700" url="http://en.wikipedia.org/wiki?curid=34700" title="1840s">
1840s

The 1840s decade ran from January 1, 1840, to December 31, 1849.
Politics and wars.
Pacific Islands.
In 1842, Tahiti and Tahuata were declared a French protectorate, to allow Catholic missionaries to work undisturbed. The capital of Papeetē was founded in 1843. In 1845, George Tupou I united Tonga into a kingdom, and reigned as Tuʻi Kanokupolu.
East Asia.
China.
On August 29, 1842, the first of two Opium Wars ended between China and Britain with the Treaty of Nanking. One of the consequences was the cession of modern day Hong Kong Island to the British. Hong Kong would eventually be returned to China in 1997.
Other events:
Japan.
The 1840s comprised the end of the Tenpō era (1830-1844), the entirety of the Kōka era (1844-1848), and the beginning of the Kaei era (1848-1854). The decade saw the end of the reign of Emperor Ninko in 1846, who was succeeded by his son, Emperor Kōmei.
Southeastern Asia.
Vietnam.
Emperors Minh Mạng, Thiệu Trị and Tự Đức ruled Vietnam during the 1840s under the Nguyễn dynasty.
Southern Asia.
Afghanistan.
The First Anglo-Afghan War had started in 1838, started by the British as a means of defending India (under British control at the time) from the Russian Empire's expansion into Central Asia. The British attempted to impose a puppet regime on Afghanistan under Shuja Shah, but the regime was short lived and proved unsustainable without British military support. By 1842, mobs were attacking the British on the streets of Kabul and the British garrison was forced to abandon the city due to constant civilian attacks. During the retreat from Kabul, the British army of approximately 4,500 troops (of which only 690 were European) and 12,000 camp followers was subjected to a series of attacks by Afghan warriors. All Europeans but one were killed; a few Indian soldiers survived also and crossed into India later. The British curbed their ambitions in Afghanistan following this humiliating retreat.
Afghanistan would remain independent (under the Barakzai dynasty) through the rest of the decade, ruled by Akbar Khan (1842-1845), and Dost Mohammad Khan (1845-1863).
Sikh Empire.
The Sikh Empire was founded in 1799, ruled by Ranjit Singh. When Singh died in 1839, the Sikh Empire began to fall into disorder. There was a succession of short-lived rulers at the central Durbar (court), and increasing tension between the Khalsa (the Sikh Army) and the Durbar. In May 1841, the Dogra dynasty (a vassal of the Sikh Empire) invaded western Tibet, marking the beginning of the Sino-Sikh war. This war ended in a stalemate in September 1842, with the Treaty of Chushul.
The British East India Company began to build up its military strength on the borders of the Punjab. Eventually, the increasing tension goaded the Khalsa to invade British territory, under weak and possibly treacherous leaders. The hard-fought First Anglo-Sikh War (1845-1846) ended in defeat for the Khalsa. With the Treaty of Lahore, the Sikh Empire ceded Kashmir to the East India Company and surrendered the Koh-i-Noor diamond to Queen Victoria.
The Sikh empire was finally dissolved at the end of the Second Anglo-Sikh War in 1849 into separate princely states and the British province of Punjab. Eventually, a Lieutenant Governorship was formed in Lahore as a direct representative of the British Crown.
Western Asia.
Ottoman Empire.
The decade was near the beginning of the Tanzimât Era of the Ottoman Empire. Sultan Abdülmecid I ruled during this period.
Revolutions of 1848.
There was a wave of revolutions in Europe, collectively known as the Revolutions of 1848. It remains the most widespread revolutionary wave in European history, but within a year, reactionary forces had regained control, and the revolutions collapsed.
The revolutions were essentially bourgeois-democratic in nature with the aim of removing the old feudal structures and the creation of independent national states. The revolutionary wave began in France in February, and immediately spread to most of Europe and parts of Latin America. Over 50 countries were affected, but with no coordination or cooperation among the revolutionaries in different countries. Six factors were involved: widespread dissatisfaction with political leadership; demands for more participation in government and democracy; demands for freedom of press; the demands of the working classes; the upsurge of nationalism; and finally, the regrouping of the reactionary forces based on the royalty, the aristocracy, the army, and the peasants.
The uprisings were led by ad hoc coalitions of reformers, the middle classes and workers, which did not hold together for long. Tens of thousands of people were killed, and many more forced into exile. The only significant lasting reforms were the abolition of serfdom in Austria and Hungary, the end of absolute monarchy in Denmark, and the definitive end of the Capetian monarchy in France. The revolutions were most important in France, the Netherlands, Germany, Poland, Italy, and the Austrian Empire, but did not reach Russia, Sweden, Great Britain, and most of southern Europe (Spain, Serbia, Greece, Montenegro, Portugal, the Ottoman Empire).
Northern Europe.
United Kingdom.
Ireland.
The Great Famine of the 1840s caused the deaths of one million Irish people and over a million more emigrated to escape it. It is sometimes referred to, mostly outside Ireland, as the "Irish Potato Famine" because one-third of the population was then solely reliant on this cheap crop for a number of historical reasons. The proximate cause of famine was a potato disease commonly known as potato blight. A census taken in 1841 revealed a population of slightly over 8 million. A census immediately after the famine in 1851 counted 6,552,385, a drop of almost 1.5 million in 10 years.
The period of the potato blight in Ireland from 1845 to 1851 was full of political confrontation. A more radical Young Ireland group seceded from the Repeal movement and attempted an armed rebellion in the Young Irelander Rebellion of 1848, which was unsuccessful.
North America.
Canada.
In the prior decade, the desire for responsible government resulted in the abortive Rebellions of 1837. The Durham Report subsequently recommended responsible government and the assimilation of French Canadians into English culture. The Act of Union 1840 merged the Canadas into a united Province of Canada and responsible government was established for all British North American provinces by 1849. The signing of the Oregon Treaty by Britain and the United States in 1846 ended the Oregon boundary dispute, extending the border westward along the 49th parallel. This paved the way for British colonies on Vancouver Island (1849) and in British Columbia (1858).
United States.
Presidents.
The United States had five different Presidents during the decade. Only the 1880s would have as many. Martin Van Buren was President when the decade began, but was defeated by William Henry Harrison in the U.S. presidential election of 1840. Harrison's service was the shortest in history, starting with his inauguration on March 4, 1841, and ending when he died on April 4, 1841.
Harrison's vice president, John Tyler, replaced him as President (the first Presidential succession in U.S. history), and served out the rest of his term. Tyler spent much of his term in conflict with the Whig party. He ended his term having made an alliance with the Democrats, endorsing James K. Polk and signing the resolution to annex Texas into the United States.
In the Presidential election of 1844, James K. Polk defeated Henry Clay. During his presidency, Polk oversaw the U.S. victory in the Mexican–American War and subsequent annexation of what is now the southwest United States. He also negotiated a split of the Oregon Territory with Great Britain.
In the U.S. presidential election of 1848, Whig Zachary Taylor of Louisiana defeated Democrat Lewis Cass of Michigan. Taylor's term in office was cut short by his death in 1850.
Mexican–American War.
In 1845, the United States of America annexed Texas, which had won independence from Centralist Republic of Mexico in the Texas Revolution of 1836. Nevertheless, Mexico still considered Texas part of its territory, declaring war on the U.S., and starting the Mexican–American War (1846–1848).
Combat operations lasted a year and a half, from the spring of 1846 to the fall of 1847. American forces quickly occupied New Mexico and California, then invaded parts of Northeastern Mexico and Northwest Mexico; meanwhile, the Pacific Squadron conducted a blockade, and took control of several garrisons on the Pacific coast farther south in Baja California. Another American army captured Mexico City, and the war ended in a victory for the United States.
American territorial expansion to the Pacific coast was a major goal of U.S. President James K. Polk. Thus, in the war-ending Treaty of Guadalupe Hidalgo, the U.S. forced Mexican Cession of the territories of Alta California and New Mexico to the United States in exchange for $15 million, In addition, the United States assumed $3.25 million of debt owed by the Mexican government to U.S. citizens. Mexico accepted the loss of Texas and thereafter cited the Rio Grande as its national border.
Science and Technology.
Photography.
The 1840s saw the rise of the Daguerreotype. Introduced in 1839, the Daguerreotype was the first publicly announced photographic process and came into widespread use in the 1840s. Numerous events in the 1840s were captured by photography for the first time with the use of the Daguerreotype. A number of daguerrotypes were taken of the occupation of Saltillo during the Mexican–American War, in 1847 by an unknown photographer. These photographs stand as the first ever photos of warfare in history.
Transportation.
Rail.
Widespread interest to invest in rail technology led to a speculative frenzy in Britain, known there as Railway Mania. It reached its zenith in 1846, when no fewer than 272 Acts of Parliament were passed, setting up new railway companies, and the proposed routes totalled 9500 mi of new railway. Around a third of the railways authorised were never built – the company either collapsed due to poor financial planning, was bought out by a larger competitor before it could build its line, or turned out to be a fraudulent enterprise to channel investors' money into another business.
Popular culture.
Fashion.
Fashion in European and European-influenced clothing is characterized by a narrow, natural shoulder line following the exaggerated puffed sleeves of the later 1820s fashion and 1830s fashion. The narrower shoulder was accompanied by a lower waistline for both men and women.
Disasters, natural events, and notable mishaps.
Cholera.
The cholera epidemic struck throughout Europe.

</doc>
<doc id="34718" url="http://en.wikipedia.org/wiki?curid=34718" title="1597">
1597

Year 1597 (MDXCVII) was a common year starting on Wednesday (link will display the full calendar) of the Gregorian calendar and a common year starting on Saturday of the 10-day slower Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="34729" url="http://en.wikipedia.org/wiki?curid=34729" title="1740s">
1740s

This is a list of events occurring in the 0s, ordered by year.
 

</doc>
<doc id="34741" url="http://en.wikipedia.org/wiki?curid=34741" title="6th century">
6th century

The 6th century is the period from 501 to 600 in accordance with the Julian calendar in the Common Era. In the West this century marks the end of Classical Antiquity and the beginning of the Middle Ages. Following the collapse of the Western Roman Empire late in the previous century, Europe fractured into many small Germanic Kingdoms, which competed fiercely for land and wealth. From this upheaval the Franks rose to prominence, and carved out a sizeable domain encompassing much of modern France and Germany. Meanwhile, the surviving Eastern Roman Empire began to expand under the emperor Justinian, who eventually recaptured North Africa from the Vandals, and attempted to fully recover Italy as well in the hope of re-establishing Roman control over the lands once ruled by the Western Roman Empire. 
During its second Golden Age, the Sassanid Empire reached the peak of its power under Khosrau I in the 6th century. The classical Gupta Empire of Northern India, largely overrun by the Huna, ended in the mid-6th century. In Japan, the Kofun period gave way to the Asuka period. After being divided for more than 150 years into the Southern and Northern Dynasties, China was reunited under the Sui Dynasty toward the end of the 6th century. The Three Kingdoms of Korea persisted throughout the 6th century. The Göktürks became a major power in Central Asia after defeating the Rouran.
In the Americas, Teotihuacan began to decline in the 6th century after having reached its zenith between AD 150 and 450. Classic Period of the Maya civilization in Central America.

</doc>
<doc id="34764" url="http://en.wikipedia.org/wiki?curid=34764" title="1796">
1796

1796 (MDCCXCVI) was a leap year starting on Friday (link will display the full calendar) of the Gregorian calendar and a leap year starting on Tuesday of the 11-day slower Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="34775" url="http://en.wikipedia.org/wiki?curid=34775" title="1932">
1932

1932 ()
will be .

</doc>
