<doc id="25179" url="http://en.wikipedia.org/wiki?curid=25179" title="Quark">
Quark

A quark ( or ) is an elementary particle and a fundamental constituent of matter. Quarks combine to form composite particles called hadrons, the most stable of which are protons and neutrons, the components of atomic nuclei. Due to a phenomenon known as "color confinement", quarks are never directly observed or found in isolation; they can be found only within hadrons, such as baryons (of which protons and neutrons are examples), and mesons. For this reason, much of what is known about quarks has been drawn from observations of the hadrons themselves.
Quarks have various intrinsic properties, including electric charge, mass, color charge and spin. Quarks are the only elementary particles in the Standard Model of particle physics to experience all four fundamental interactions, also known as "fundamental forces" (electromagnetism, gravitation, strong interaction, and weak interaction), as well as the only known particles whose electric charges are not integer multiples of the elementary charge.
There are six types of quarks, known as "flavors": up, down, strange, charm, top, and bottom. Up and down quarks have the lowest masses of all quarks. The heavier quarks rapidly change into up and down quarks through a process of particle decay: the transformation from a higher mass state to a lower mass state. Because of this, up and down quarks are generally stable and the most common in the universe, whereas strange, charm, bottom, and top quarks can only be produced in high energy collisions (such as those involving cosmic rays and in particle accelerators). For every quark flavor there is a corresponding type of antiparticle, known as an "antiquark", that differs from the quark only in that some of its properties have equal magnitude but opposite sign. 
The quark model was independently proposed by physicists Murray Gell-Mann and George Zweig in 1964. Quarks were introduced as parts of an ordering scheme for hadrons, and there was little evidence for their physical existence until deep inelastic scattering experiments at the Stanford Linear Accelerator Center in 1968. Accelerator experiments have provided evidence for all six flavors. The top quark was the last to be discovered at Fermilab in 1995.
Classification.
The Standard Model is the theoretical framework describing all the currently known elementary particles. This model contains six flavors of quarks (#redirect ), named up (#redirect ), down (#redirect ), strange (#redirect ), charm (#redirect ), bottom (#redirect ), and top (#redirect ). Antiparticles of quarks are called "antiquarks", and are denoted by a bar over the symbol for the corresponding quark, such as #redirect for an up antiquark. As with antimatter in general, antiquarks have the same mass, mean lifetime, and spin as their respective quarks, but the electric charge and other charges have the opposite sign.
Quarks are spin-1⁄2 particles, implying that they are fermions according to the spin-statistics theorem. They are subject to the Pauli exclusion principle, which states that no two identical fermions can simultaneously occupy the same quantum state. This is in contrast to bosons (particles with integer spin), any number of which can be in the same state. Unlike leptons, quarks possess color charge, which causes them to engage in the strong interaction. The resulting attraction between different quarks causes the formation of composite particles known as "hadrons" (see "Strong interaction and color charge" below).
The quarks which determine the quantum numbers of hadrons are called "valence quarks"; apart from these, any hadron may contain an indefinite number of virtual (or "sea") quarks, antiquarks, and gluons which do not influence its quantum numbers. There are two families of hadrons: baryons, with three valence quarks, and mesons, with a valence quark and an antiquark. The most common baryons are the proton and the neutron, the building blocks of the atomic nucleus. A great number of hadrons are known (see list of baryons and list of mesons), most of them differentiated by their quark content and the properties these constituent quarks confer. The existence of "exotic" hadrons with more valence quarks, such as tetraquarks (#redirect #redirect #redirect #redirect ) and pentaquarks (#redirect #redirect #redirect #redirect #redirect ), has been conjectured but not proven.
Elementary fermions are grouped into three generations, each comprising two leptons and two quarks. The first generation includes up and down quarks, the second strange and charm quarks, and the third bottom and top quarks. All searches for a fourth generation of quarks and other elementary fermions have failed, and there is strong indirect evidence that no more than three generations exist. Particles in higher generations generally have greater mass and less stability, causing them to decay into lower-generation particles by means of weak interactions. Only first-generation (up and down) quarks occur commonly in nature. Heavier quarks can only be created in high-energy collisions (such as in those involving cosmic rays), and decay quickly; however, they are thought to have been present during the first fractions of a second after the Big Bang, when the universe was in an extremely hot and dense phase (the quark epoch). Studies of heavier quarks are conducted in artificially created conditions, such as in particle accelerators.
Having electric charge, mass, color charge, and flavor, quarks are the only known elementary particles that engage in all four fundamental interactions of contemporary physics: electromagnetism, gravitation, strong interaction, and weak interaction. Gravitation is too weak to be relevant to individual particle interactions except at extremes of energy (Planck energy) and distance scales (Planck distance). However, since no successful quantum theory of gravity exists, gravitation is not described by the Standard Model.
See the table of properties below for a more complete overview of the six quark flavors' properties.
History.
The quark model was independently proposed by physicists Murray Gell-Mann
(pictured) and George Zweig in 1964. The proposal came shortly after Gell-Mann's 1961 formulation of a particle classification system known as the "Eightfold Way"—or, in more technical terms, SU(3) flavor symmetry. Physicist Yuval Ne'eman had independently developed a scheme similar to the Eightfold Way in the same year.
At the time of the quark theory's inception, the "particle zoo" included, amongst other particles, a multitude of hadrons. Gell-Mann and Zweig posited that they were not elementary particles, but were instead composed of combinations of quarks and antiquarks. Their model involved three flavors of quarks, up, down, and strange, to which they ascribed properties such as spin and electric charge. The initial reaction of the physics community to the proposal was mixed. There was particular contention about whether the quark was a physical entity or a mere abstraction used to explain concepts that were not fully understood at the time.
In less than a year, extensions to the Gell-Mann–Zweig model were proposed. Sheldon Lee Glashow and James Bjorken predicted the existence of a fourth flavor of quark, which they called "charm". The addition was proposed because it allowed for a better description of the weak interaction (the mechanism that allows quarks to decay), equalized the number of known quarks with the number of known leptons, and implied a mass formula that correctly reproduced the masses of the known mesons.
In 1968, deep inelastic scattering experiments at the Stanford Linear Accelerator Center (SLAC) showed that the proton contained much smaller, point-like objects and was therefore not an elementary particle. Physicists were reluctant to firmly identify these objects with quarks at the time, instead calling them "partons"—a term coined by Richard Feynman. The objects that were observed at SLAC would later be identified as up and down quarks as the other flavors were discovered. Nevertheless, "parton" remains in use as a collective term for the constituents of hadrons (quarks, antiquarks, and gluons).
The strange quark's existence was indirectly validated by SLAC's scattering experiments: not only was it a necessary component of Gell-Mann and Zweig's three-quark model, but it provided an explanation for the kaon (#redirect ) and pion (#redirect ) hadrons discovered in cosmic rays in 1947.
In a 1970 paper, Glashow, John Iliopoulos and Luciano Maiani presented further reasoning for the existence of the as-yet undiscovered charm quark. The number of supposed quark flavors grew to the current six in 1973, when Makoto Kobayashi and Toshihide Maskawa noted that the experimental observation of CP violation could be explained if there were another pair of quarks.
Charm quarks were produced almost simultaneously by two teams in November 1974 (see November Revolution)—one at SLAC under Burton Richter, and one at Brookhaven National Laboratory under Samuel Ting. The charm quarks were observed bound with charm antiquarks in mesons. The two parties had assigned the discovered meson two different symbols, J and ψ; thus, it became formally known as the #redirect [[Template:Subatomic particle]] meson. The discovery finally convinced the physics community of the quark model's validity.
In the following years a number of suggestions appeared for extending the quark model to six quarks. Of these, the 1975 paper by Haim Harari was the first to coin the terms "top" and "bottom" for the additional quarks.
In 1977, the bottom quark was observed by a team at Fermilab led by Leon Lederman. This was a strong indicator of the top quark's existence: without the top quark, the bottom quark would have been without a partner. However, it was not until 1995 that the top quark was finally observed, also by the CDF and DØ teams at Fermilab. It had a mass much larger than had been previously expected, almost as large as that of a gold atom.
Etymology.
For some time, Gell-Mann was undecided on an actual spelling for the term he intended to coin, until he found the word "quark" in James Joyce's book "Finnegans Wake":
<poem>
Three quarks for Muster Mark!
Sure he has not got much of a bark
And sure any he has it's all beside the mark.
</poem>—James Joyce, "Finnegans Wake"
Gell-Mann went into further detail regarding the name of the quark in his book "The Quark and the Jaguar":
In 1963, when I assigned the name "quark" to the fundamental constituents of the nucleon, I had the sound first, without the spelling, which could have been "kwork". Then, in one of my occasional perusals of "Finnegans Wake", by James Joyce, I came across the word "quark" in the phrase "Three quarks for Muster Mark". Since "quark" (meaning, for one thing, the cry of the gull) was clearly intended to rhyme with "Mark", as well as "bark" and other such words, I had to find an excuse to pronounce it as "kwork". But the book represents the dream of a publican named Humphrey Chimpden Earwicker. Words in the text are typically drawn from several sources at once, like the "portmanteau" words in "Through the Looking-Glass". From time to time, phrases occur in the book that are partially determined by calls for drinks at the bar. I argued, therefore, that perhaps one of the multiple sources of the cry "Three quarks for Muster Mark" might be "Three quarts for Mister Mark", in which case the pronunciation "kwork" would not be totally unjustified. In any case, the number three fitted perfectly the way quarks occur in nature.
Zweig preferred the name "ace" for the particle he had theorized, but Gell-Mann's terminology came to prominence once the quark model had been commonly accepted.
The quark flavors were given their names for a number of reasons. The up and down quarks are named after the up and down components of isospin, which they carry. Strange quarks were given their name because they were discovered to be components of the strange particles discovered in cosmic rays years before the quark model was proposed; these particles were deemed "strange" because they had unusually long lifetimes. Glashow, who coproposed charm quark with Bjorken, is quoted as saying, "We called our construct the 'charmed quark', for we were fascinated and pleased by the symmetry it brought to the subnuclear world." The names "bottom" and "top", coined by Harari, were chosen because they are "logical partners for up and down quarks". In the past, bottom and top quarks were sometimes referred to as "beauty" and "truth" respectively, but these names have somewhat fallen out of use. While "truth" never did catch on, accelerator complexes devoted to massive production of bottom quarks are sometimes called "beauty factories".
Properties.
Electric charge.
Quarks have fractional electric charge values – either 1⁄3 or 2⁄3 times the elementary charge (e), depending on flavor. Up, charm, and top quarks (collectively referred to as "up-type quarks") have a charge of +2⁄3 e, while down, strange, and bottom quarks ("down-type quarks") have −1⁄3 e. Antiquarks have the opposite charge to their corresponding quarks; up-type antiquarks have charges of −2⁄3 e and down-type antiquarks have charges of +1⁄3 e. Since the electric charge of a hadron is the sum of the charges of the constituent quarks, all hadrons have integer charges: the combination of three quarks (baryons), three antiquarks (antibaryons), or a quark and an antiquark (mesons) always results in integer charges. For example, the hadron constituents of atomic nuclei, neutrons and protons, have charges of 0 e and +1 e respectively; the neutron is composed of two down quarks and one up quark, and the proton of two up quarks and one down quark.
Spin.
Spin is an intrinsic property of elementary particles, and its direction is an important degree of freedom. It is sometimes visualized as the rotation of an object around its own axis (hence the name ""), though this notion is somewhat misguided at subatomic scales because elementary particles are believed to be point-like.
Spin can be represented by a vector whose length is measured in units of the reduced Planck constant "ħ" (pronounced "h bar"). For quarks, a measurement of the spin vector component along any axis can only yield the values +"ħ"/2 or −"ħ"/2; for this reason quarks are classified as spin-1⁄2 particles. The component of spin along a given axis – by convention the "z" axis – is often denoted by an up arrow ↑ for the value +1⁄2 and down arrow ↓ for the value −1⁄2, placed after the symbol for flavor. For example, an up quark with a spin of +1⁄2 along the "z" axis is denoted by u↑.
Weak interaction.
A quark of one flavor can transform into a quark of another flavor only through the weak interaction, one of the four fundamental interactions in particle physics. By absorbing or emitting a W boson, any up-type quark (up, charm, and top quarks) can change into any down-type quark (down, strange, and bottom quarks) and vice versa. This flavor transformation mechanism causes the radioactive process of beta decay, in which a neutron (#redirect ) "splits" into a proton (#redirect ), an electron (#redirect ) and an electron antineutrino (#redirect ) (see picture). This occurs when one of the down quarks in the neutron (#redirect #redirect #redirect ) decays into an up quark by emitting a virtual #redirect boson, transforming the neutron into a proton (#redirect #redirect #redirect ). The #redirect boson then decays into an electron and an electron antineutrino.
Both beta decay and the inverse process of "inverse beta decay" are routinely used in medical applications such as positron emission tomography (PET) and in experiments involving neutrino detection.
While the process of flavor transformation is the same for all quarks, each quark has a preference to transform into the quark of its own generation. The relative tendencies of all flavor transformations are described by a mathematical table, called the Cabibbo–Kobayashi–Maskawa matrix (CKM matrix). Enforcing unitarity, the approximate magnitudes of the entries of the CKM matrix are:
where "V""ij" represents the tendency of a quark of flavor "i" to change into a quark of flavor "j" (or vice versa).
There exists an equivalent weak interaction matrix for leptons (right side of the W boson on the above beta decay diagram), called the Pontecorvo–Maki–Nakagawa–Sakata matrix (PMNS matrix). Together, the CKM and PMNS matrices describe all flavor transformations, but the links between the two are not yet clear.
Strong interaction and color charge.
According to quantum chromodynamics (QCD), quarks possess a property called "color charge". There are three types of color charge, arbitrarily labeled "blue", "green", and "red". Each of them is complemented by an anticolor – "antiblue", "antigreen", and "antired". Every quark carries a color, while every antiquark carries an anticolor.
The system of attraction and repulsion between quarks charged with different combinations of the three colors is called strong interaction, which is mediated by force carrying particles known as "gluons"; this is discussed at length below. The theory that describes strong interactions is called quantum chromodynamics (QCD). A quark, which will have a single color value, can form a bound system with an antiquark carrying the corresponding anticolor. The result of two attracting quarks will be color neutrality: a quark with color charge "ξ" plus an antiquark with color charge −"ξ" will result in a color charge of 0 (or "white" color) and the formation of a meson. This is analogous to the additive color model in basic optics. Similarly, the combination of three quarks, each with different color charges, or three antiquarks, each with anticolor charges, will result in the same "white" color charge and the formation of a baryon or antibaryon.
In modern particle physics, gauge symmetries – a kind of symmetry group – relate interactions between particles (see gauge theories). Color SU(3) (commonly abbreviated to SU(3)c) is the gauge symmetry that relates the color charge in quarks and is the defining symmetry for quantum chromodynamics. Just as the laws of physics are independent of which directions in space are designated "x", "y", and "z", and remain unchanged if the coordinate axes are rotated to a new orientation, the physics of quantum chromodynamics is independent of which directions in three-dimensional color space are identified as blue, red, and green. SU(3)c color transformations correspond to "rotations" in color space (which, mathematically speaking, is a complex space). Every quark flavor "f", each with subtypes "f"B, "f"G, "f"R corresponding to the quark colors, forms a triplet: a three-component quantum field which transforms under the fundamental representation of SU(3)c. The requirement that SU(3)c should be local – that is, that its transformations be allowed to vary with space and time – determines the properties of the strong interaction, in particular the existence of eight gluon types to act as its force carriers.
Mass.
Two terms are used in referring to a quark's mass: "current quark mass" refers to the mass of a quark by itself, while "constituent quark mass" refers to the current quark mass plus the mass of the gluon particle field surrounding the quark. These masses typically have very different values. Most of a hadron's mass comes from the gluons that bind the constituent quarks together, rather than from the quarks themselves. While gluons are inherently massless, they possess energy – more specifically, quantum chromodynamics binding energy (QCBE) – and it is this that contributes so greatly to the overall mass of the hadron (see mass in special relativity). For example, a proton has a mass of approximately 938 MeV/c2, of which the rest mass of its three valence quarks only contributes about 11 MeV/c2; much of the remainder can be attributed to the gluons' QCBE.
The Standard Model posits that elementary particles derive their masses from the Higgs mechanism, which is related to the Higgs boson. Physicists hope that further research into the reasons for the top quark's large mass of ~173 GeV/c2, almost the mass of a gold atom, might reveal more about the origin of the mass of quarks and other elementary particles.
Table of properties.
The following table summarizes the key properties of the six quarks. Flavor quantum numbers (isospin ("I"3), charm ("C"), strangeness ("S", not to be confused with spin), topness ("T"), and bottomness ("B"′)) are assigned to certain quark flavors, and denote qualities of quark-based systems and hadrons. The baryon number ("B") is +1⁄3 for all quarks, as baryons are made of three quarks. For antiquarks, the electric charge ("Q") and all flavor quantum numbers ("B", "I"3, "C", "S", "T", and "B"′) are of opposite sign. Mass and total angular momentum ("J"; equal to spin for point particles) do not change sign for the antiquarks.
Interacting quarks.
As described by quantum chromodynamics, the strong interaction between quarks is mediated by gluons, massless vector gauge bosons. Each gluon carries one color charge and one anticolor charge. In the standard framework of particle interactions (part of a more general formulation known as perturbation theory), gluons are constantly exchanged between quarks through a virtual emission and absorption process. When a gluon is transferred between quarks, a color change occurs in both; for example, if a red quark emits a red–antigreen gluon, it becomes green, and if a green quark absorbs a red–antigreen gluon, it becomes red. Therefore, while each quark's color constantly changes, their strong interaction is preserved.
Since gluons carry color charge, they themselves are able to emit and absorb other gluons. This causes "asymptotic freedom": as quarks come closer to each other, the chromodynamic binding force between them weakens. Conversely, as the distance between quarks increases, the binding force strengthens. The color field becomes stressed, much as an elastic band is stressed when stretched, and more gluons of appropriate color are spontaneously created to strengthen the field. Above a certain energy threshold, pairs of quarks and antiquarks are created. These pairs bind with the quarks being separated, causing new hadrons to form. This phenomenon is known as "color confinement": quarks never appear in isolation. This process of hadronization occurs before quarks, formed in a high energy collision, are able to interact in any other way. The only exception is the top quark, which may decay before it hadronizes.
Sea quarks.
Hadrons, along with the "valence quarks" (#redirect ) that contribute to their quantum numbers, contain virtual quark–antiquark (#redirect #redirect ) pairs known as "sea quarks" (#redirect ). Sea quarks form when a gluon of the hadron's color field splits; this process also works in reverse in that the annihilation of two sea quarks produces a gluon. The result is a constant flux of gluon splits and creations colloquially known as "the sea". Sea quarks are much less stable than their valence counterparts, and they typically annihilate each other within the interior of the hadron. Despite this, sea quarks can hadronize into baryonic or mesonic particles under certain circumstances.
Other phases of quark matter.
Under sufficiently extreme conditions, quarks may become deconfined and exist as free particles. In the course of asymptotic freedom, the strong interaction becomes weaker at higher temperatures. Eventually, color confinement would be lost and an extremely hot plasma of freely moving quarks and gluons would be formed. This theoretical phase of matter is called quark–gluon plasma. The exact conditions needed to give rise to this state are unknown and have been the subject of a great deal of speculation and experimentation. A recent estimate puts the needed temperature at kelvin. While a state of entirely free quarks and gluons has never been achieved (despite numerous attempts by CERN in the 1980s and 1990s), recent experiments at the Relativistic Heavy Ion Collider have yielded evidence for liquid-like quark matter exhibiting "nearly perfect" fluid motion.
The quark–gluon plasma would be characterized by a great increase in the number of heavier quark pairs in relation to the number of up and down quark pairs. It is believed that in the period prior to 10−6 seconds after the Big Bang (the quark epoch), the universe was filled with quark–gluon plasma, as the temperature was too high for hadrons to be stable.
Given sufficiently high baryon densities and relatively low temperatures – possibly comparable to those found in neutron stars – quark matter is expected to degenerate into a Fermi liquid of weakly interacting quarks. This liquid would be characterized by a condensation of colored quark Cooper pairs, thereby breaking the local SU(3)c symmetry. Because quark Cooper pairs harbor color charge, such a phase of quark matter would be color superconductive; that is, color charge would be able to pass through it with no resistance.

</doc>
<doc id="25198" url="http://en.wikipedia.org/wiki?curid=25198" title="Quaternary">
Quaternary

The Quaternary Period () is the current and most recent of the three periods of the Cenozoic Era in the geologic time scale of the International Commission on Stratigraphy (ICS). It follows the Neogene Period and spans from 2.588 ± 0.005 million years ago to the present. The Quaternary Period is divided into two epochs: the Pleistocene (2.588 million years ago to 11.7 thousand years ago) and the Holocene (11.7 thousand years ago to today). The informal term "Late Quaternary" refers to the past 0.5–1.0 million years.
The Quaternary period is typically defined by the cyclic growth and decay of continental ice sheets driven by Milankovitch cycles and the associated climate and environmental changes that occurred.
Research history.
The term Quaternary ("fourth") was proposed by Giovanni Arduino in 1759 for alluvial deposits in the Po River valley in northern Italy. It was introduced by Jules Desnoyers in 1829 for sediments of France's Seine Basin that seemed clearly to be younger than Tertiary Period rocks.
The Quaternary Period follows the Neogene Period and extends to the present. The Quaternary covers the time span of glaciations classified as the Pleistocene, and includes the present interglacial period, the Holocene.
This places the start of the Quaternary at the onset of Northern Hemisphere glaciation approximately 2.6 million years ago. Prior to 2009, the Pleistocene was defined to be from 1.805 million years ago to the present, so the current definition of the Pleistocene includes a portion of what was, prior to 2009, defined as the Pliocene.
Quaternary stratigraphers usually worked with regional subdivisions. From the 1970s, the International Commission on Stratigraphy (ICS) tried to make a single geologic time scale based on GSSP's, which could be used internationally. The Quaternary subdivisions were defined based on biostratigraphy instead of paleoclimate.
This led to the problem that the proposed base of the Pleistocene was at 1.805 Mya, long after the start of the major glaciations of the northern hemisphere. The ICS then proposed to abolish use of the name Quaternary altogether, which appeared unacceptable to the International Union for Quaternary Research (INQUA).
In 2009, it was decided to make the Quaternary the youngest period of the Cenozoic Era with its base at 2.588 Mya and including the Gelasian stage, which was formerly considered part of the Neogene Period and Pliocene Epoch.
The Anthropocene has been proposed as a third epoch as a mark of the anthropogenic impact on the global environment starting with the Industrial Revolution, or about 200 years ago. The Anthropocene is not officially designated by the ICS, however, but a working group is currently aiming to complete a proposal for the creation of an epoch or sub-period by 2016.
Geology.
The 2.6 million years of the Quaternary represents the time during which recognizable humans existed. Over this short time period, there has been relatively little change in the distribution of the continents due to plate tectonics.
The Quaternary geological record is preserved in greater detail than that for earlier periods.
The major geographical changes during this time period included the emergence of the Strait of Bosphorus and Skagerrak during glacial epochs, which respectively turned the Black Sea and Baltic Sea into fresh water, followed by their flooding (and return to salt water) by rising sea level; the periodic filling of the English Channel, forming a land bridge between Britain and the European mainland; the periodic closing of the Bering Strait, forming the land bridge between Asia and North America; and the periodic flash flooding of Scablands of the American Northwest by glacial water.
The current extent of Hudson Bay, the Great Lakes and other major lakes of North America are a consequence of the Canadian Shield's readjustment since the last ice age; different shorelines have existed over the course of Quaternary time.
Climate.
The climate was one of periodic glaciations with continental glaciers moving as far from the poles as 40 degrees latitude. There was a major extinction of large mammals in Northern areas at the end of the Pleistocene Epoch. Many forms such as saber-toothed cats, mammoths, mastodons, glyptodonts, etc., became extinct worldwide. Others, including horses, camels and American cheetahs became extinct in North America.
Quaternary glaciation.
Glaciation took place repeatedly during the Quaternary Ice Age – a term coined by Schimper in 1839 that began with the start of the Quaternary about 2.58 Mya and continues to the present-day.
Last glacial period.
In 1821, a Swiss engineer, Ignaz Venetz, presented an article in which he suggested the presence of traces of the passage of a glacier at a considerable distance from the Alps. This idea was initially disputed by another Swiss scientist, Louis Agassiz, but when he undertook to disprove it, he ended up affirming his colleague's hypothesis. A year later, Agassiz raised the hypothesis of a great glacial period that would have had long-reaching general effects. This idea gained him international fame and led to the establishment of the Glacial Theory.
In time, thanks to the refinement of geology, it has been demonstrated that there were several periods of glacial advance and retreat and that past temperatures on Earth were very different from today.
In particular, the Milankovitch cycles of Milutin Milankovitch are based on the premise that variations in incoming solar radiation are a fundamental factor controlling Earth's climate.
During this time, substantial glaciers advanced and retreated over much of North America and Europe, parts of South America and Asia, and all of Antarctica. The Great Lakes formed and giant mammals thrived in parts of North America and Eurasia not covered in ice. These mammals became extinct when the glacial period Age ended about 11,700 years ago. Modern humans evolved about 190,000 years ago (source: Leakey). During the Quaternary period, mammals, flowering plants, and insects dominated the land. 

</doc>
<doc id="25286" url="http://en.wikipedia.org/wiki?curid=25286" title="Quechuan languages">
Quechuan languages

Quechuan , also known as runa simi ("people's language"), is a Native American language family spoken primarily in the Andes region of South America, derived from a common ancestral language. It is the most widely spoken language family of the indigenous peoples of the Americas, with a total of probably some 8 million to 10 million speakers.
History: origins and divergence.
Quechua had already expanded across wide ranges of the central Andes long before the expansion of the Inca Empire. The Inca were just one among many peoples in present-day Peru who already spoke forms of Quechua. In the Cuzco region, Quechua was influenced by local languages such as Aymara. The Cuzco variety of Quechua developed as quite distinct. In similar way, a diverse group of dialects developed in different areas related to existing local languages during the period when the Inca Empire ruled and imposed Quechua as the official language.
After the Spanish conquest in the 16th century, Quechua continued to be used widely as the "general language" and main means of communication between the Spaniards and the indigenous population. The Roman Catholic Church adopted Quechua to use as the language of evangelisation. Given use by the missionaries, the range of Quechua continued to expand in some areas.
But the administrative and religious use of Quechua was terminated in the late 18th century when it was banned from public use in Peru in response to the Túpac Amaru II rebellion. The Crown banned even "loyal" pro-Catholic texts in Quechua, such as Garcilaso de la Vega's "Comentarios Reales." Despite a brief revival of the language immediately after independence in the 19th century, the prestige of Quechua had decreased sharply. Its use gradually was restricted to more isolated and conservative rural areas.
The oldest written records of the language are by missionary Fray Domingo de Santo Tomás, who arrived in Peru in 1538 and learned the language from 1540. He published his "Grammatica o arte de la lengua general de los indios de los reynos del Perú" in 1560.
Current status.
Today, Quechua has the status of an official language in Bolivia, Ecuador and Peru, along with Spanish.
Currently, the major obstacle to the diffusion of the usage and teaching of Quechua is the lack of written material in the Quechua language, namely books, newspapers, software, magazines, etc. Thus, Quechua, along with Aymara and the minor indigenous languages, remains essentially a spoken language.
In recent years, Quechua has been introduced in Intercultural bilingual education (IBE) in Bolivia, Ecuador and Peru, which is, however reaching only a part of the Quechua-speaking population. There is an ongoing process of Quechua-speaking populations shifting to Spanish for the purposes of social advancement.
Radio Nacional del Peru broadcasts spaces in quechua for news and agrarian programs in the mornings.
Quechua and Spanish are now heavily intermixed, with many hundreds of Spanish loanwords in Quechua. Conversely, Quechua phrases and words are commonly used by Spanish speakers. In southern rural Bolivia, for instance, many Quechua words such as "wawa" (infant), "misi" (cat), "waska" (strap, or thrashing) are as commonly used as their Spanish counterparts, even in entirely Spanish-speaking areas. Quechua has also had a profound impact on other native languages of the Americas, for example Mapudungun.
Number of speakers.
The number of speakers given varies widely according to the sources. The total in "Ethnologue" 16 is 10 million, mostly based on figures published 1987–2002, but with a few dating from the 1960s. The figure for Imbabura Quechua in "Ethnologue", for example, is 300,000, an estimate from 1977. The missionary organization FEDEPI, on the other hand, estimated one million Imbabura speakers (published 2006). Census figures are also problematic, due to under-reporting. The 2001 Ecuador census reports only 500,000 Quechua speakers, where most sources estimate over 2 million. The censuses of Peru (2007) and Bolivia (2001) are thought to be more reliable.
Additionally, there are an unknown number of speakers in emigrant communities, including Queens, New York and Paterson, New Jersey in the United States.
Classification.
There are significant differences between the varieties of Quechua spoken the central Peruvian highlands and the peripheral varieties of Ecuador on the one hand and southern Peru and Bolivia on the other. These can be labeled Quechua I (or Quechua B, central) and Quechua II (or Quechua A, peripheral). Within these two groups, there are few sharp boundaries, making them dialect continua. However, there is a secondary division in Quechua II between the grammatically simplified northern varieties of Ecuador, Quechua II-B, known there as "Kichwa", and the generally more conservative varieties of the southern highlands, Quechua II-C, which include the old Inca capital of Cuzco. The closeness is at least in part due to the influence of Cuzco Quechua on the Ecuadorean varieties during the Inca Empire, as northern nobles were required to educate their children in Cuzco, maintaining Cuzco as the prestige dialect in the north.
Speakers from different points within any one of these three regions can generally understand each other reasonably well. There are nonetheless significant local-level differences across each. (Wanka Quechua, in particular, has several very distinctive characteristics that make this variety distinctly difficult to understand, even for other Central Quechua speakers.) Speakers from "different" major regions, meanwhile, particularly Central vs Southern Quechua, are not able to communicate effectively.
The lack of mutual intelligibility is the basic criterion that defines Quechua not as a single language, but as a language family. The complex and progressive nature of how speech varies across the dialect continua makes it nearly impossible to differentiate discrete varieties; "Ethnologue" lists 44 that they judge require separate literature. As a reference point, the overall degree of diversity across the family is a little less than that of the Romance or Germanic families, and more of the order of Slavic or Arabic. The greatest diversity is within Central Quechua, AKA Quechua I, which is believed to lie close to the homeland of the ancestral Proto-Quechua language.
Family tree.
Alfredo Torero devised the traditional classification, the three divisions above plus a fourth, northern Peruvian, branch. The latter cause complications in the classification, however, as they (Cajamarca-Lambayeque, Pacaraos, and Yauyos) have features of both Quechua I and Quechua II, and so are difficult to assign to either. Torero's classification is,
Willem Adelaar adheres to the Quechua I / Quechua II (central/peripheral) bifurcation, but partially following later modifications by Torero, reassigns part of Quechua II-A to Quechua I:
Landerman (1991) does not believe a truly genetic classification is possible, and breaks up Quechua II, so that the family has four geographical–typological branches: Northern, North Peruvian, Central, and Southern. He includes Chachapoyas and Lamas in North Peruvian Quechua, so that Ecuadorian is synonymous with Northern Quechua.
Geographical distribution.
Quechua I (Central Quechua, "Waywash") is spoken in Peru's central highlands, from Ancash to Huancayo. It is the most diverse branch of Quechua, to the extent that its divisions are commonly considered different languages.
Quechua II (Peripheral Quechua, "Wamp'una" 'Traveler')
Cognates.
A sampling of words in several Quechuan languages:[]
Quechua and Aymara.
Quechua shares a large amount of vocabulary, and some striking structural parallels, with Aymara, and these two families have sometimes been grouped together as a 'Quechumaran' family. This hypothesis is generally rejected by specialists, however; the parallels are better explained by mutual influence and borrowing through intensive and long-term contact. Many Quechua–Aymara cognates are close, often closer than intra-Quechua cognates, and there is little relationship in the affixal system.
Vocabulary.
A number of Quechua loanwords have entered English via Spanish, including "coca", "condor", "guano", "jerky", "llama", "puma", "quinine", "quinoa", "vicuña" and possibly "gaucho". The word "lagniappe" comes from the Quechuan word "yapay" ("to increase; to add") with the Spanish article "la" in front of it, "la yapa" or "la ñapa" in Spanish.
The influence on Latin American Spanish includes such borrowings as "papa" for "potato", "chuchaqui" for "hangover" in Ecuador, and diverse borrowings for "altitude sickness", in Bolivia from Quechuan "suruqch'i" to Bolivian "sorojchi", in Colombia, Ecuador, and Peru "soroche". 
In Bolivia particularly, Quechua words are used extensively even by non-Quechua speakers. These include wawa (baby, infant), ch'aki (hangover), misi (cat), juk'ucho (mouse), q'omer uchu (green pepper), jacu ("lets go"), chhiri and chhurco (curly haired), among many others. Quechua grammar also enters Bolivian Spanish, such as the use of the suffix -ri. In Bolivian quechua, -ri is added to verbs to signify an action is performed with affection, or, in the imperative, as a rough equivalent to please. In Bolivia -ri is often included in the Spanish imperative to imply "please" or to soften commands. For example, the standard "pásame" (pass me), becomes pasarime.
Quechua has borrowed a large number of Spanish words, such as "piru" (from "pero", but), "bwenu" (from "bueno", good), iskwila (from "escuela," school), waka (from "vaca," cow) and "burru" (from "burro", donkey).
Etymology of Quechua.
At first, Spaniards referred to the language of the Inca empire as the "lengua general", the "general tongue". The name "quichua" is first used in 1560 by Domingo de Santo Tomás in his "Grammatica o arte de la lengua general de los indios de los reynos del Perú". It is not known what name the native speakers gave to their language before colonial times, and whether it was Spaniards who called it "quechua".
There are two possible etymologies of Quechua as the name of the language. There is a possibility that the name Quechua was derived from "*qiĉ.wa", the native word which originally meant the "temperate valley" altitude ecological zone in the Andes (suitable for maize cultivation) and to its inhabitants.
Alternatively, Pedro Cieza de León and Garcilaso de la Vega, the early Spanish chroniclers, inform about the existence of the people called Quichua in the present-day Apurímac Region, and it could be inferred that their name was given to the entire language.
The Hispanicised spellings "Quechua" and "Quichua" have been used in Peru and Bolivia since the 17th century, especially after the III Lima Council. Today the various local pronunciations of "Quechua Simi" include ], ], ], ].
Another name that native speakers give to their own language is "runa simi", "language of man/people"; it also seems to have emerged during the colonial period.
Phonology.
The description below applies to Cusco dialect; there are significant differences in other varieties of Quechua.
Vowels.
Quechua uses only three vowel phonemes: /a/ /i/ and /u/, as in Aymara (including Jaqaru). Monolingual speakers pronounce these as [æ] [ɪ] and [ʊ] respectively, though the Spanish vowels /a/ /i/ and /u/ may also be used. When the vowels appear adjacent to the uvular consonants /q/, /qʼ/, and /qʰ/, they are rendered more like [ɑ], [ɛ] and [ɔ] respectively.
Consonants.
None of the plosives or fricatives are voiced; voicing is not phonemic in the Quechua native vocabulary of the modern Cusco variety.
About 30% of the modern Quechua vocabulary is borrowed from Spanish, and some Spanish sounds (e.g. f, b, d, g) may have become phonemic, even among monolingual Quechua speakers.
Aspirated and ejective renderings of consonants are only phonemic in some varieties of Quechua. Others only use plain /p/, /t/, /t͡ʃ/, and /k/.
Stress.
Stress is penultimate in most dialects of Quechua. In some varieties the apocope of word-final vowels or other factors may cause exceptional final stress.
Writing system.
Quechua has been written using the Roman alphabet since the Spanish conquest of Peru. However, written Quechua is not used by the Quechua-speaking people at large due to the lack of printed referential material in Quechua.
Until the 20th century, Quechua was written with a Spanish-based orthography. Examples: "Inca, Huayna Cápac, Collasuyo, Mama Ocllo, Viracocha, quipu, tambo, condor". This orthography is the most familiar to Spanish speakers, and as a corollary, has been used for most borrowings into English.
In 1975, the Peruvian government of Juan Velasco adopted a new orthography for Quechua. This is the writing system preferred by the "Academia Mayor de la Lengua Quechua". Examples: "Inka, Wayna Qhapaq, Qollasuyu, Mama Oqllo, Wiraqocha, khipu, tampu, kuntur". This orthography:
In 1985, a variation of this system was adopted by the Peruvian government; it uses the Quechuan three-vowel system. Examples: "Inka, Wayna Qhapaq, Qullasuyu, Mama Uqllu, Wiraqucha, khipu, tampu, kuntur".
The different orthographies are still highly controversial in Peru. Advocates of the traditional system believe that the new orthographies look too foreign, and suggest that it makes Quechua harder to learn for people who have first been exposed to written Spanish. Those who prefer the new system maintain that it better matches the phonology of Quechua, and point to studies showing that teaching the five-vowel system to children causes reading difficulties in Spanish later on.
For more on this, see Quechuan and Aymaran spelling shift.
Writers differ in the treatment of Spanish loanwords. Sometimes these are adapted to the modern orthography, and sometimes they are left in Spanish. For instance, "I am Roberto" could be written "Robertom kani" or "Ruwirtum kani". (The "-m" is not part of the name; it is an evidential suffix.)
The Peruvian linguist Rodolfo Cerrón-Palomino has proposed an orthographic norm for all Southern Quechua. This norm, "el Quechua estándar" or "Hanan Runasimi", which is accepted by many institutions in Peru, has been made by combining conservative features of two widespread dialects, Ayacucho Quechua and Cusco Quechua. For instance:
Grammar.
Morphological type.
All varieties of Quechua are very regular agglutinative languages, as opposed to isolating or fusional ones. Their normal sentence order is SOV (subject–object–verb). Their large number of suffixes changes both the overall significance of words and their subtle shades of meaning. Notable grammatical features include bipersonal conjugation (verbs agree with both subject and object), evidentiality (indication of the source and veracity of knowledge), a set of topic particles, and suffixes indicating who benefits from an action and the speaker's attitude toward it, although some languages and varieties may lack some of these characteristics.
Pronouns.
In Quechua, there are seven pronouns. Quechua has two first person plural pronouns ("we", in English). One is called the inclusive, which is used when the speaker wishes to include in "we" the person to whom he or she is speaking ("us and you"). The other form is called the exclusive, which is used when the addressee is excluded. ("us without you"). Quechua also adds the suffix "-kuna" to the second and third person singular pronouns "qam" and "pay" to create the plural forms "qam-kuna" and "pay-kuna".
Adjectives.
Adjectives in Quechua are always placed before nouns. They lack gender and number, and are not declined to agree with substantives.
Nouns.
Noun roots accept suffixes which indicate person (defining of possession, not identity), number, and case. In general, the personal suffix precedes that of number – in the Santiago del Estero variety, however, the order is reversed. From variety to variety, suffixes may change.
Adverbs.
Adverbs can be formed by adding "-ta" or, in some cases, "-lla" to an adjective: "allin – allinta" ("good – well"), "utqay – utqaylla" ("quick – quickly"). They are also formed by adding suffixes to demonstratives: "chay" ("that") – "chaypi" ("there"), "kay" ("this") – "kayman" ("hither").
There are several original adverbs. For Europeans, it is striking that the adverb "qhipa" means both "behind" and "future", whereas "ñawpa" means "ahead, in front" and "past". This means that local and temporal concepts of adverbs in Quechua (as well as in Aymara) are associated to each other reversely compared to European languages. For the speakers of Quechua, we are moving backwards into the future (we cannot see it – i.e. it is unknown), facing the past (we can see it – i.e. we remember it).
Verbs.
The infinitive forms (unconjugated) have the suffix "-y" ("much'a"= "kiss"; "much'a-y" = "to kiss"). The endings for the indicative are:
The suffixes shown in the table above usually indicate the subject; the person of the object is also indicated by a suffix ("-a-" for first person and "-su-" for second person), which precedes the suffixes in the table. In such cases, the plural suffixes from the table ("-chik" and "-ku") can be used to express the number of the object rather than the subject.
Various suffixes are added to the stem to change the meaning. For example, "-chi" is a causative and "-ku" is a reflexive (example: "wañuy" = "to die"; "wañuchiy" = to kill "wañuchikuy" = "to commit suicide"); "-naku" is used for mutual action (example: "marq'ay"= "to hug"; "marq'anakuy"= "to hug each other"), and "-chka" is a progressive, used for an ongoing action (e.g., "mikhuy" = "to eat"; "mikhuchkay" = "to be eating").
Grammatical particles.
Particles are indeclinable, that is, they do not accept suffixes. They are relatively rare. The most common are "arí" ("yes") and "mana" ("no"), although "mana" can take some suffixes, such as "-n"/"-m" ("manan"/"manam"), "-raq" ("manaraq", not yet) and "-chu" ("manachu?", or not?), to intensify the meaning. Also used are "yaw" ("hey", "hi"), and certain loan words from Spanish, such as "piru" (from Spanish "pero" "but") and "sinuqa" (from "sino" "rather").
Evidentiality.
The Quechua languages have three different morphemes that mark evidentiality. Evidentiality refers to a morpheme whose primary purpose is to indicate the source of information. In the Quechua languages, evidentiality is a three-term system. This means that there are three evidential morphemes that mark varying levels of source information. These markers can apply to first, second, and third person. The chart below depicts an example of these morphemes from the Wanka Quechua language.
Wanka Quechua 
The parentheses around the vowels indicate that the vowel can be dropped in when following an open vowel. For the sake of cohesiveness, the above forms will be used to broadly discuss the evidential morphemes. However, it should be noted that there are dialectal variations to the forms. The variations will be presented in the following descriptions.
The following sentences provide examples of the three evidentials and further discuss the meaning behind each of them.
"-m(i)" : Direct Evidence and Commitment
Regional variations: In the Cuzco dialect, the direct evidential presents itself as "–mi" and "–n".
The evidential "–mi" indicates that the speaker has a “strong personal conviction the veracity of the circumstance expressed.” It has the basis of direct personal experience.
Wanka Quechua 
I saw them with my own eyes.
"-chr(a)" : Inference and Attenuation
Regional variations: In Quechua languages, not specified by the source, the inference morpheme appears as "–ch(i), -ch(a), -chr(a)".
The "–chr(a)" evidential indicates that the utterance is an inference or form of conjecture. This inference relays the speaker’s non-commitment to the truth-value of the statement. It also appears in cases such as acquiescence, irony, interrogative constructions, and first person inferences. These uses constitute non-prototypical use and will be later discussed in the Changes in Meaning and Other Uses section.
Wanka Quechua
I think they will probably come back.
"-sh(i)" : Hearsay
Regional variations: It can appear as "–sh(i)" or "–s(i)" depending on the dialect.
With the use of this morpheme, the speaker “serves as a conduit through which information from another source passes”. The information being related is hearsay or revelatory in nature. It also works to express the uncertainty of the speaker regarding the situation. However, it also appears in other constructions that are discussed in the Changes in Meaning section.
Wanka Quechua
(I was told) Shani borrowed it.
Hintz discusses an interesting case of evidential behavior found in Sihaus Quechua. The author postulates that instead of three single evidential markers, this Quechua language contains three pairs of evidential markers.
Affix or Clitic
It may have been noted the evidential morphemes have been referred to as ‘markers’ or ‘morphemes’. The literature seems to differ on whether or not the evidential morphemes are acting as affixes or clitics, in come cases, such as Wanka Quechua, enclitics. Lefebvre and Muysken (1998) discuss this issue in terms of case but remark as to how the line between affix and clitic is not a clear one. Both terms will be used interchangeably throughout these sections.
Position in the Sentence
The evidentials in the Quechua languages are “second position enclitics” that attach to the first constituent in the sentence as shown in the examples below.
Once there were an old man and an old woman.
They can also occur on a focused constituent.
It is now that Pedro is building the house.
Sometimes the affix is described as attaching to the focus, especially when in reference to Tarma Quechua, but this does not hold true for all varieties of Quechua. In Huanuco Quechua. The evidentials follow any number of topics, marked by the topic marker "–qa", and the element with the evidential must precede the main verb or be the main verb.
However, there are exceptions to this rule as well. The more topics there are in a sentence, the more likely to deviate from the usual form.
When she (the witch) reached the peak, God had already taken the child up into heaven.
Changes in Meaning and Other Uses
Evidentials can be used to relay different meanings depending on the context and perform other functions. The following examples are restricted to Wanka Quechua.
The direct evidential, -mi
The direct evidential appears in Wh-Questions and Yes/No Questions. Considering the direct evidential in terms of prototypical semantics, it seems somewhat counterintuitive to have a direct evidential, basically an evidential that confirms the speaker’s certainty about a topic, in a question. However, if one focuses less on the structure and more on the situation, some sense can be made. The speaker is asking the addressee for information. Therefore, the speaker assumes the speaker knows the answer, or else why would they bother asking. This assumption is where the direct evidential comes into play. The speaker holds a certain amount of certainty that the addressee will know the answer. The speaker interprets the addressee as being in “direct relation” to the proposed content; this situation is the same as when, in regular sentences, the speaker assumes direct relation to the proposed information.
When did he come back from Huancayo?
The direct evidential affix is also seen in Yes/No Questions. This is similar to the situation with the Wh-Questions. Floyd describes the Yes/No questions as being “characterized as instructions to the addressee to assert one of the propositions of a disjunction”. Once again, the burden of direct evidence is being placed on the addressee, not on the speaker. The question marker in Wanka Quechua, "-chun", is derived from the negative –chu marker and the direct evidential (realized as –n in some dialects).
Is he going to Tarma?
The inferential evidential, -chr(a)
While "–chr(a)" is usually used in an inferential context, it has some non-prototypical uses.
"Mild Exhortation"
In these constructions the evidential works to reaffirm and encourage the addressee’s actions or thoughts.
Yes, tell them, "I've gone farther."
This example comes from a conversation between husband and wife discussing the reactions of their family and friends after they have been gone for a while. The husband says he plans to stretch the truth and tell them about far places he has gone, and his wife (in the example above) echoes and encourages his thoughts.
"Acquiescence"
With these, the evidential is used to highlight the speaker’s assessment of inevitability of an event and acceptance of it. There is a sense of resistance, diminished enthusiasm, and disinclination in these constructions.
I suppose I'll pay you then.
This example comes from a discourse where a woman demands compensation from the man (the speaker in the example) whose pigs ruined her potatoes. He denies the pigs as being his, but finally realizes he may be responsible and produces the above example.
"Interrogative"
Somewhat similar to the "–mi" evidential, the inferential evidential can be found in content questions. However, the salient difference between the uses of the evidentials in questions is that in the "–m(i)" marked questions, an answer is expected. This is not the case with "–chr(a)" marked questions.
I wonder what we will give our families when we arrive.
"Irony"
Irony in language can be a somewhat complicated topic due to how it functions differently in languages and, by its semantic nature, is already somewhat vague. For these purposes, it is suffice to say that when irony takes place in Wanka Quechua, the "–chr(a)" marker is used.
(I suppose) That's how you learn [i.e., that is the way in which you will learn].
This example comes from discourse between a father and daughter about her refusal to attend school. It can be interpreted as a genuine statement, i.e., perhaps one can learn by resisting school, or as an ironic statement, i.e., that's an absurd idea.
The hearsay evidential, -sh(i)
Aside from being used to express hearsay and revelation, this affix also has other uses.
"Folktales, Myths, and Legends"
Because folktales, myths, and legends are, in essence, reported speech, it follows that the hearsay marker would be used with them. Many of these types of stories are passed down through generations, furthering this aspect of reported speech. A difference between simple hearsay and folktales can be seen in the frequency of the "–sh(i)" marker. In normal conversation using reported speech, the marker is used less to eliminate redundancy.
"Riddles"
Riddles are somewhat similar to myths and folktales due to their nature to be passed by word of mouth.
Omission and Overuse of Evidential Affixes
In certain grammatical structures, the evidential marker does not appear at all. In all the Quechuan languages the evidential will not appear in a dependent clause. Sadly, no example was given to depict this omission.
Omissions can and do occur in Quechua. The sentence is understood to have the same evidentiality as the other sentences in the context. It varies among Quechuan speakers as to how much they omit evidentials, though these occur only in connected speech.
An interesting contrast to omission of evidentials is overuse of evidentials. If a speaker uses evidentials too much with no reason, their competence is brought into question. For example, the overuse of –m(i) could lead others to believe that the speaker is not a native speaker of the language or, in some extreme cases, that one is mentally ill.
Cultural Aspect
By using evidentials, the Quechua culture has certain assumptions about the information being relayed. Those who do not abide by the cultural customs should not be trusted. A passage from Weber (1986) summarizes them nicely below:
Evidentials also show that being precise and stating the source of one’s information is extremely important in the language and the culture. Failure to use them correctly can lead to diminished standing in the community. Speakers are aware of the evidentials and even use proverbs to teach children the importance of being precise and truthful. Precision and information source are of the utmost importance. They are a powerful and resourceful method of human communication.
Literature.
Although the body of literature in Quechua is not as sizable as its historical and present-day prominence would suggest, it is nevertheless not negligible.
As in the case of the Mesoamerican civilizations, there are a number of surviving Andean documents in the local language that were written down in Latin characters after the European conquest, but which express to a great extent the culture of pre-conquest times. The Quechua literature of this type is somewhat scantier, but nevertheless significant. It includes the so-called Huarochiri manuscript (1598), describing the mythology and religion of the valley of Huarochirí, as well as Quechua poems quoted within the Spanish-language texts of some chronicles dealing with the pre-conquest period. There are a number of anonymous or signed Quechua dramas dating from the post-conquest period (starting from the 17th century), some of which deal with the Inca era, while most are on religious topics and of European inspiration. The most famous of these dramas are "Ollantay" and the plays describing the death of Atahualpa. For example, Juan de Espinosa Medrano wrote several dramas in the language. Poems in Quechua were also composed during the colonial period.
Dramas and poems continued to be written in the 19th and especially in 20th centuries as well; in addition, in the 20th century and more recently, more prose has been published. While some of that literature consists of original compositions (poems and dramas), the bulk of 20th century Quechua literature consists of traditional folk stories and oral narratives. Johnny Payne has translated two sets of Quechua oral short stories, one into Spanish and the other into English.
Many Andean musicians write and sing in their native languages, including Quechua and Aymara. Notable musical groups are Los Kjarkas, Kala Marka, J'acha Mallku, Savia Andina, Wayna Picchu, Wara and many others.
In popular culture.
In Da Vinci's Demons, season 2 they meet a group of Indians who speak this language
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="25422" url="http://en.wikipedia.org/wiki?curid=25422" title="Deaths in 2003">
Deaths in 2003

The following is a list of notable deaths in 2003. Names are listed under the date of death and not the date it was announced. Names under each date are listed in alphabetical order by family name.
A typical entry lists information in the following sequence:

</doc>
<doc id="25423" url="http://en.wikipedia.org/wiki?curid=25423" title="Rock music">
Rock music

Rock music is a genre of popular music that originated as "rock and roll" in the United States in the 1950s, and developed into a range of different styles in the 1960s and later, particularly in the United Kingdom and the United States. It has its roots in 1940s' and 1950s' rock and roll, itself heavily influenced by blues, rhythm and blues and country music. Rock music also drew strongly on a number of other genres such as electric blues and folk, and incorporated influences from jazz, classical and other musical sources.
Musically, rock has centered on the electric guitar, usually as part of a rock group with electric bass guitar and drums. Typically, rock is song-based music usually with a 4/4 time signature using a verse-chorus form, but the genre has become extremely diverse. Like pop music, lyrics often stress romantic love but also address a wide variety of other themes that are frequently social or political in emphasis. The dominance of rock by white, male musicians has been seen as one of the key factors shaping the themes explored in rock music. Rock places a higher degree of emphasis on musicianship, live performance, and an ideology of authenticity than pop music.
By the late 1960s, referred to as the "golden age" or "classic rock" period, a number of distinct rock music subgenres had emerged, including hybrids like blues rock, folk rock, country rock, raga rock, and jazz-rock fusion, many of which contributed to the development of psychedelic rock, which was influenced by the countercultural psychedelic scene. New genres that emerged from this scene included progressive rock, which extended the artistic elements; glam rock, which highlighted showmanship and visual style; and the diverse and enduring subgenre of heavy metal, which emphasized volume, power, and speed. In the second half of the 1970s, punk rock reacted against the perceived overblown, inauthentic and overly mainstream aspects of these genres to produce a stripped-down, energetic form of music valuing raw expression and often lyrically characterised by social and political critiques. Punk was an influence into the 1980s on the subsequent development of other subgenres, including new wave, post-punk and eventually the alternative rock movement. From the 1990s alternative rock began to dominate rock music and break through into the mainstream in the form of grunge, Britpop, and indie rock. Further fusion subgenres have since emerged, including pop punk, rap rock, and rap metal, as well as conscious attempts to revisit rock's history, including the garage rock/post-punk and synthpop revivals at the beginning of the new millennium.
Rock music has also embodied and served as the vehicle for cultural and social movements, leading to major sub-cultures including mods and rockers in the UK and the hippie counterculture that spread out from San Francisco in the US in the 1960s. Similarly, 1970s punk culture spawned the visually distinctive goth and emo subcultures. Inheriting the folk tradition of the protest song, rock music has been associated with political activism as well as changes in social attitudes to race, sex and drug use, and is often seen as an expression of youth revolt against adult consumerism and conformity.
Characteristics.
The sound of rock is traditionally centered on the electric guitar, which emerged in its modern form in the 1950s with the popularization of rock and roll, and was influenced by the sounds of electric blues guitarists. The sound of an electric guitar in rock music is typically supported by an electric bass guitar pioneered in jazz music in the same era, and percussion produced from a drum kit that combines drums and cymbals. This trio of instruments has often been complemented by the inclusion of others, particularly keyboards such as the piano, Hammond organ and synthesizers. The basic rock instrumentation was adapted from the basic blues band instrumentation (prominent lead guitar, second chord instrument, bass, and drums). A group of musicians performing rock music is termed a rock band or rock group and typically consists of between two and five members. Classically, a rock band takes the form of a quartet whose members cover one or more roles, including vocalist, lead guitarist, rhythm guitarist, bass guitarist, drummer and often that of keyboard player or other instrumentalist.
Rock music is traditionally built on a foundation of simple unsyncopated rhythms in a 4/4 meter, with a repetitive snare drum back beat on beats two and four. Melodies are often derived from older musical modes, including the Dorian and Mixolydian, as well as major and minor modes. Harmonies range from the common triad to parallel fourths and fifths and dissonant harmonic progressions. Rock songs, since the late 1950s and particularly from the mid-1960s onwards, often used the verse-chorus structure derived from blues and folk music, but there has been considerable variation from this model. Critics have stressed the eclecticism and stylistic diversity of rock. Because of its complex history and tendency to borrow from other musical and cultural forms, it has been argued that "it is impossible to bind rock music to a rigidly delineated musical definition."
Unlike many earlier styles of popular music, rock lyrics have dealt with a wide range of themes in addition to romantic love: including sex, rebellion against "The Establishment", social concerns and life styles. These themes were inherited from a variety of sources, including the Tin Pan Alley pop tradition, folk music and rhythm and blues. Music journalist Robert Christgau characterizes rock lyrics as a "cool medium" with simple diction and repeated refrains, and asserts that rock's primary "function" "pertains to music, or, more generally, noise." The predominance of white, male and often middle class musicians in rock music has often been noted and rock has been seen as an appropriation of black musical forms for a young, white and largely male audience. As a result it has been seen as articulating the concerns of this group in both style and lyrics.
Since the term rock began to be used in preference to rock and roll from the late-1960s, it has often been contrasted with pop music, with which it has shared many characteristics, but from which it is often distanced by an emphasis on musicianship, live performance and a focus on serious and progressive themes as part of an ideology of authenticity that is frequently combined with an awareness of the genre's history and development. According to Simon Frith "rock was something more than pop, something more than rock and roll. Rock musicians combined an emphasis on skill and technique with the romantic concept of art as artistic expression, original and sincere". In the new millennium the term "rock" has sometimes been used as a blanket term including forms such as pop music, reggae music, soul music, and even hip hop, with which it has been influenced but often contrasted through much of its history.
Origins.
Rock and roll.
The foundations of rock music are in rock and roll, which originated in the United States during the late 1940s and early 1950s, and quickly spread to much of the rest of the world. Its immediate origins lay in a melding of various black musical genres of the time, including rhythm and blues and gospel music, with country and western. In 1951, Cleveland, Ohio disc jockey Alan Freed began playing rhythm and blues music for a multi-racial audience, and is credited with first using the phrase "rock and roll" to describe the music.
Debate surrounds which record should be considered the first rock and roll record. Contenders include Goree Carter's "" (1949); Jimmy Preston's "Rock the Joint" (1949), which was later covered by Bill Haley & His Comets in 1952; and "Rocket 88" by Jackie Brenston and his Delta Cats (in fact, Ike Turner and his band the Kings of Rhythm), recorded by Sam Phillips for Sun Records in 1951. Four years later, Bill Haley's "Rock Around the Clock" (1955) became the first rock and roll song to top "Billboard" magazine's main sales and airplay charts, and opened the door worldwide for this new wave of popular culture.
It has also been argued that "That's All Right (Mama)" (1954), Elvis Presley's first single for Sun Records in Memphis, could be the first rock and roll record, but, at the same time, Big Joe Turner's "Shake, Rattle & Roll", later covered by Haley, was already at the top of the Billboard R&B charts. Other artists with early rock and roll hits included Chuck Berry, Bo Diddley, Fats Domino, Little Richard, Jerry Lee Lewis, and Gene Vincent. Soon rock and roll was the major force in American record sales and crooners, such as Eddie Fisher, Perry Como, and Patti Page, who had dominated the previous decade of popular music, found their access to the pop charts significantly curtailed.
Rock and roll has been seen as leading to a number of distinct subgenres, including rockabilly, combining rock and roll with "hillbilly" country music, which was usually played and recorded in the mid-1950s by white singers such as Carl Perkins, Jerry Lee Lewis, Buddy Holly and with the greatest commercial success, Elvis Presley. In contrast doo wop placed an emphasis on multi-part vocal harmonies and meaningless backing lyrics (from which the genre later gained its name), which were usually supported with light instrumentation and had its origins in 1930s and '40s African American vocal groups. Acts like the Crows, the Penguins, the El Dorados and the Turbans all scored major hits, and groups like the Platters, with songs including "The Great Pretender" (1955), and the Coasters with humorous songs like "Yakety Yak" (1958), ranked among the most successful rock and roll acts of the period.
The era also saw the growth in popularity of the electric guitar, and the development of a specifically rock and roll style of playing through such exponents as Chuck Berry, Link Wray, and Scotty Moore. The use of distortion, pioneered by electric blues guitarists such as Guitar Slim, Willie Johnson and Pat Hare in the early 1950s, was popularized by Chuck Berry in the mid-1950s. The use of power chords, pioneered by Willie Johnson and Pat Hare in the early 1950s, was popularized by Link Wray in the late 1950s.
In the United Kingdom, the trad jazz and folk movements brought visiting blues music artists to Britain. Lonnie Donegan's 1955 hit "Rock Island Line" was a major influence and helped to develop the trend of skiffle music groups throughout the country, many of which, including John Lennon's Quarrymen, moved on to play rock and roll.
Commentators have traditionally perceived a decline of rock and roll in the late 1950s and early 1960s. By 1959, the death of Buddy Holly, The Big Bopper and Richie Valens in a plane crash, the departure of Elvis for the army, the retirement of Little Richard to become a preacher, prosecutions of Jerry Lee Lewis and Chuck Berry and the breaking of the payola scandal (which implicated major figures, including Alan Freed, in bribery and corruption in promoting individual acts or songs), gave a sense that the rock and roll era established at that point had come to an end.
"In-between years".
The period of the later 1950s and early 1960s, between the end of the initial period of innovation and what became known in the US as the "British Invasion", has traditionally been seen as an era of hiatus for rock and roll. More recently some authors have emphasised important innovations and trends in this period without which future developments would not have been possible. While early rock and roll, particularly through the advent of rockabilly, saw the greatest commercial success for male and white performers, in this era the genre was dominated by black and female artists. Rock and roll had not disappeared at the end of the 1950s and some of its energy can be seen in the Twist dance craze of the early 60s, mainly benefiting the career of Chubby Checker. Having died down in the late 1950s, doo wop enjoyed a revival in the same period, with hits for acts like the Marcels, the Capris, Maurice Williams and the Zodiacs, and Shep and the Limelights. The rise of girl groups like the Chantels, the Shirelles and the Crystals placed an emphasis on harmonies and polished production that was in contrast to earlier rock and roll. Some of the most significant girl group hits were products of the Brill Building Sound, named after the block in New York where many songwriters were based, which included the number 1 hit for the Shirelles "Will You Love Me Tomorrow" in 1960, penned by the partnership of Gerry Goffin and Carole King.
Cliff Richard had the first British rock and roll hit with "Move It", effectively ushering in the sound of British rock. At the start of the 1960s, his backing group the Shadows was the most successful group recording instrumentals. While rock 'n' roll was fading into lightweight pop and ballads, British rock groups at clubs and local dances, heavily influenced by blues-rock pioneers like Alexis Korner, were starting to play with an intensity and drive seldom found in white American acts.
Also significant was the advent of soul music as a major commercial force. Developing out of rhythm and blues with a re-injection of gospel music and pop, led by pioneers like Ray Charles and Sam Cooke from the mid-1950s, by the early 60s figures like Marvin Gaye, James Brown, Aretha Franklin, Curtis Mayfield and Stevie Wonder were dominating the R&B charts and breaking through into the main pop charts, helping to accelerate their desegregation, while Motown and Stax/Volt Records were becoming major forces in the record industry. All of these elements, including the close harmonies of doo wop and girl groups, the carefully crafted song-writing of the Brill Building Sound and the polished production values of soul, have been seen as influencing the Merseybeat sound, particularly the early work of The Beatles, and through them the form of later rock music. Some historians of music have also pointed to important and innovative technical developments that built on rock and roll in this period, including the electronic treatment of sound by such innovators as Joe Meek, and the elaborate production methods of the Wall of Sound pursued by Phil Spector.
Surf music.
The instrumental rock and roll of performers such as Duane Eddy, Link Wray and the Ventures was developed by Dick Dale, who added distinctive "wet" reverb, rapid alternate picking, and Middle Eastern and Mexican influences. He produced the regional hit "Let's Go Trippin'" in 1961 and launched the surf music craze, following up with songs like "Misirlou" (1962). Like Dale and his Del-Tones, most early surf bands were formed in Southern California, including the Bel-Airs, the Challengers, and Eddie & the Showmen. The Chantays scored a top ten national hit with "Pipeline" in 1963 and probably the best known surf tune was 1963's "Wipe Out", by the Surfaris, which hit number 2 and number 10 on the Billboard charts in 1965.
Groups which crossed over to this genre included the Astronauts, from Boulder, Colorado; the Trashmen, from Minneapolis, Minnesota, who had a number 4 hit with "Surfin Bird" in 1964; and the Rivieras from South Bend, Indiana, who reached number 5 in 1964 with "California Sun". The Atlantics, from Sydney, made a significant contribution to the genre, with their hit "Bombora" (1963). European instrumental bands around this time generally focused more on the more rock and roll style played by The Shadows, but the Dakotas, who were the British backing band for Merseybeat singer Billy J. Kramer, gained some attention as surf musicians with "Cruel Sea" (1963), which was later covered by American instrumental surf bands, including the Ventures.
Surf music achieved its greatest commercial success as vocal music, particularly the work of the Beach Boys, formed in 1961 in Southern California. Their early albums included both instrumental surf rock (among them covers of music by Dick Dale) and vocal songs, drawing on rock and roll and doo wop and the close harmonies of vocal pop acts like the Four Freshmen. Their first chart hit, "Surfin'" in 1962 reached the Billboard top 100 and helped make the surf music craze a national phenomenon. From 1963 the group began to leave surfing behind as subject matter as Brian Wilson became their major composer and producer, moving on to the more general themes of male adolescence including cars and girls in songs like "Fun, Fun, Fun" (1964) and "California Girls" (1965). Other vocal surf acts followed, including one-hit wonders like Ronny & the Daytonas with "G. T. O." (1964) and Rip Chords with "Hey Little Cobra", which both reached the top ten, but the only other act to achieve sustained success with the formula were Jan & Dean, who had a number 1 hit with "Surf City" (co-written with Brian Wilson) in 1963. The surf music craze and the careers of almost all surf acts was effectively ended by the arrival of the British Invasion from 1964. Only the Beach Boys were able to sustain a creative career into the mid-1960s, producing a string of hit singles and albums, including the highly regarded "Pet Sounds" in 1966, which made them, arguably, the only American rock or pop act that could rival The Beatles.
Golden age.
British Invasion.
By the end of 1962, what would become the British rock scene had started with beat groups like the Beatles, Gerry & the Pacemakers and the Searchers from Liverpool and Freddie and the Dreamers, Herman's Hermits and the Hollies from Manchester. They drew on a wide range of American influences including soul, rhythm and blues and surf music, initially reinterpreting standard American tunes and playing for dancers. Bands like the Animals from Newcastle and Them from Belfast, and particularly those from London like the Rolling Stones and the Yardbirds, were much more directly influenced by rhythm and blues and later blues music. Soon these groups were composing their own material, combining US forms of music and infusing it with a high energy beat. Beat bands tended towards "bouncy, irresistible melodies", while early British rhythm and blues acts tended towards less sexually innocent, more aggressive songs, often adopting an anti-establishment stance. There was, however, particularly in the early stages, considerable musical crossover between the two tendencies. By 1963, led by the Beatles, beat groups had begun to achieve national success in Britain, soon to be followed into the charts by the more rhythm and blues focused acts.
"I Want to Hold Your Hand" was the Beatles' first number 1 hit on the "Billboard" Hot 100, spending 7 weeks at the top and a total of 15 weeks on the chart. Their first appearance on "The Ed Sullivan Show" on 9 February 1964, drawing an estimated 73 million viewers (at the time a record for an American television program) is often considered a milestone in American pop culture. The Beatles went on to become the biggest selling rock band of all time and they were followed into the US charts by numerous British bands. During the next two years British acts dominated their own and the US charts with Peter and Gordon, the Animals, Manfred Mann, Petula Clark, Freddie and the Dreamers, Wayne Fontana and the Mindbenders, Herman's Hermits, the Rolling Stones, the Troggs, and Donovan all having one or more number 1 singles. Other major acts that were part of the invasion included the Kinks and the Dave Clark Five.
The British Invasion helped internationalize the production of rock and roll, opening the door for subsequent British (and Irish) performers to achieve international success. In America it arguably spelled the end of instrumental surf music, vocal girl groups and (for a time) the teen idols, that had dominated the American charts in the late 1950s and 60s. It dented the careers of established R&B acts like Fats Domino and Chubby Checker and even temporarily derailed the chart success of surviving rock and roll acts, including Elvis. The British Invasion also played a major part in the rise of a distinct genre of rock music, and cemented the primacy of the rock group, based on guitars and drums and producing their own material as singer-songwriters.
Garage rock.
Garage rock was a raw form of rock music, particularly prevalent in North America in the mid-1960s and so called because of the perception that it was rehearsed in a suburban family garage. Garage rock songs revolved around the traumas of high school life, with songs about "lying girls" being particularly common. The lyrics and delivery were more aggressive than was common at the time, often with growled or shouted vocals that dissolved into incoherent screaming. They ranged from crude one-chord music (like the Seeds) to near-studio musician quality (including the Knickerbockers, the Remains, and the Fifth Estate). There were also regional variations in many parts of the country with flourishing scenes particularly in California and Texas. The Pacific Northwest states of Washington and Oregon had perhaps the most defined regional sound.
The style had been evolving from regional scenes as early as 1958. "Tall Cool One" (1959) by The Wailers and "Louie Louie" by the Kingsmen (1963) are mainstream examples of the genre in its formative stages. By 1963, garage band singles were creeping into the national charts in greater numbers, including Paul Revere and the Raiders (Boise), the Trashmen (Minneapolis) and the Rivieras (South Bend, Indiana). Other influential garage bands, such as the Sonics (Tacoma, Washington), never reached the "Billboard" Hot 100. In this early period many bands were heavily influenced by surf rock and there was a cross-pollination between garage rock and frat rock, sometimes viewed as merely a subgenre of garage rock.
The British Invasion of 1964–66 greatly influenced garage bands, providing them with a national audience, leading many (often surf or hot rod groups) to adopt a British influence, and encouraging many more groups to form. Thousands of garage bands were extant in the US and Canada during the era and hundreds produced regional hits. Examples include: "The Witch" by Tacoma's the Sonics (1965), "Where You Gonna Go" by Detroit's Unrelated Segments (1967), "Girl I Got News for You" by Miami's Birdwatchers (1966) and "1–2–5" by Montreal's the Haunted. Despite scores of bands being signed to major or large regional labels, most were commercial failures. It is generally agreed that garage rock peaked both commercially and artistically around 1966. By 1968 the style largely disappeared from the national charts and at the local level as amateur musicians faced college, work or the draft. New styles had evolved to replace garage rock (including blues rock, progressive rock and country rock). In Detroit, garage rock's legacy remained alive into the early 1970s, with bands such as the MC5 and the Stooges, who employed a much more aggressive approach to the form. These bands began to be labelled punk rock and are now often seen as proto-punk or proto-hard rock.
Pop rock.
The term "pop" has been used since the early 20th century to refer to popular music in general, but from the mid-1950s it began to be used for a distinct genre, aimed at a youth market, often characterized as a softer alternative to rock and roll. In the aftermath of the British Invasion, from about 1967, it was increasingly used in opposition to the term rock music, to describe a form that was more commercial, ephemeral and accessible. In contrast rock music was seen as focusing on extended works, particularly albums, was often associated with particular sub-cultures (like the counterculture of the 1960s), placed an emphasis on artistic values and "authenticity", stressed live performance and instrumental or vocal virtuosity and was often seen as encapsulating progressive developments rather than simply reflecting existing trends.
Nevertheless much pop and rock music has been very similar in sound, instrumentation and even lyrical content. The terms "pop-rock" and "power pop" have been used to describe more commercially successful music that uses elements from, or the form of, rock music. Pop-rock has been defined as an "upbeat variety of rock music represented by artists such as Elton John, Paul McCartney, the Everly Brothers, Rod Stewart, Chicago, and Peter Frampton." The term "power pop" was coined by Pete Townshend of the Who in 1966, but not much used until it was applied to bands like Badfinger in the 1970s, who proved some of the most commercially successful of the period.
Blues rock.
Although the first impact of the British Invasion on American popular music was through beat and R&B based acts, the impetus was soon taken up by a second wave of bands that drew their inspiration more directly from American blues, including the Rolling Stones and the Yardbirds. British blues musicians of the late 1950s and early 60s had been inspired by the acoustic playing of figures such as Lead Belly, who was a major influence on the Skiffle craze, and Robert Johnson. Increasingly they adopted a loud amplified sound, often centered on the electric guitar, based on the Chicago blues, particularly after the tour of Britain by Muddy Waters in 1958, which prompted Cyril Davies and guitarist Alexis Korner to form the band Blues Incorporated. The band involved and inspired many of the figures of the subsequent British blues boom, including members of the Rolling Stones and Cream, combining blues standards and forms with rock instrumentation and emphasis.
The other key focus for British blues was around John Mayall who formed the Bluesbreakers, whose members included Eric Clapton (after his departure from The Yardbirds) and later Peter Green. Particularly significant was the release of "Blues Breakers with Eric Clapton (Beano)" album (1966), considered one of the seminal British blues recordings and the sound of which was much emulated in both Britain and the United States. Eric Clapton went on to form supergroups Cream, Blind Faith and Derek and the Dominos, followed by an extensive solo career that helped bring blues rock into the mainstream. Green, along with the Bluesbreaker's rhythm section Mick Fleetwood and John McVie, formed Peter Green's Fleetwood Mac, who enjoyed some of the greatest commercial success in the genre. In the late 60s Jeff Beck, also an alumnus of the Yardbirds, moved blues rock in the direction of heavy rock with his band, the Jeff Beck Group. The last Yardbirds guitarist was Jimmy Page, who went on to form "The New Yardbirds" which rapidly became Led Zeppelin. Many of the songs on their first three albums, and occasionally later in their careers, were expansions on traditional blues songs.
In America, blues rock had been pioneered in the early 1960s by guitarist Lonnie Mack, but the genre began to take off in the mid-60s as acts developed a sound similar to British blues musicians. Key acts included Paul Butterfield (whose band acted like Mayall's Bluesbreakers in Britain as a starting point for many successful musicians), Canned Heat, the early Jefferson Airplane, Janis Joplin, Johnny Winter, the J. Geils Band and Jimi Hendrix with his power trios, the Jimi Hendrix Experience and Band of Gypsys, whose guitar virtuosity and showmanship would be among the most emulated of the decade. Blues rock bands from the southern states, like the Allman Brothers Band, Lynyrd Skynyrd, and ZZ Top, incorporated country elements into their style to produce distinctive Southern rock.
Early blues rock bands often emulated jazz, playing long, involved improvisations, which would later be a major element of progressive rock. From about 1967 bands like Cream and the Jimi Hendrix Experience had begun to move away from purely blues-based music into psychedelia. By the 1970s, blues rock had become heavier and more riff-based, exemplified by the work of Led Zeppelin and Deep Purple, and the lines between blues rock and hard rock "were barely visible", as bands began recording rock-style albums. The genre was continued in the 1970s by figures such as George Thorogood and Pat Travers, but, particularly on the British scene (except perhaps for the advent of groups such as Status Quo and Foghat who moved towards a form of high energy and repetitive boogie rock), bands became focused on heavy metal innovation, and blues rock began to slip out of the mainstream.
Folk rock.
By the 1960s, the scene that had developed out of the American folk music revival had grown to a major movement, utilising traditional music and new compositions in a traditional style, usually on acoustic instruments. In America the genre was pioneered by figures such as Woody Guthrie and Pete Seeger and often identified with progressive or labor politics. In the early sixties figures such as Joan Baez and Bob Dylan had come to the fore in this movement as singer-songwriters. Dylan had begun to reach a mainstream audience with hits including "Blowin' in the Wind" (1963) and "Masters of War" (1963), which brought "protest songs" to a wider public, but, although beginning to influence each other, rock and folk music had remained largely separate genres, often with mutually exclusive audiences.
Early attempts to combine elements of folk and rock included the Animals' "House of the Rising Sun" (1964), which was the first commercially successful folk song to be recorded with rock and roll instrumentation and the Beatles "I'm a Loser" (1964), arguably the first Beatles song to be influenced directly by Dylan. The folk rock movement is usually thought to have taken off with The Byrds' recording of Dylan's "Mr. Tambourine Man" which topped the charts in 1965. With members who had been part of the cafe-based folk scene in Los Angeles, the Byrds adopted rock instrumentation, including drums and 12-string Rickenbacker guitars, which became a major element in the sound of the genre. Later that year Dylan adopted electric instruments, much to the outrage of many folk purists, with his "Like a Rolling Stone" becoming a US hit single. Folk rock particularly took off in California, where it led acts like The Mamas & the Papas and Crosby, Stills and Nash to move to electric instrumentation, and in New York, where it spawned performers including The Lovin' Spoonful and Simon and Garfunkel, with the latter's acoustic "The Sounds of Silence" (1965) being remixed with rock instruments to be the first of many hits.
These acts directly influenced British performers like Donovan and Fairport Convention. In 1969 Fairport Convention abandoned their mixture of American covers and Dylan-influenced songs to play traditional English folk music on electric instruments. This electric folk was taken up by bands including Pentangle, Steeleye Span and The Albion Band, which in turn prompted Irish groups like Horslips and Scottish acts like the JSD Band, Spencer's Feat and later Five Hand Reel, to use their traditional music to create a brand of Celtic rock in the early 1970s.
Folk rock reached its peak of commercial popularity in the period 1967–68, before many acts moved off in a variety of directions, including Dylan and the Byrds, who began to develop country rock. However, the hybridization of folk and rock has been seen as having a major influence on the development of rock music, bringing in elements of psychedelia, and helping to develop the ideas of the singer-songwriter, the protest song and concepts of "authenticity".
Psychedelic rock.
Psychedelic music's LSD-inspired vibe began in the folk scene, with the New York-based Holy Modal Rounders using the term in their 1964 recording of "Hesitation Blues". The first group to advertise themselves as psychedelic rock were the 13th Floor Elevators from Texas, at the end of 1965; producing an album that made their direction clear, with "The Psychedelic Sounds of the 13th Floor Elevators" the following year. The Beatles introduced many of the major elements of the psychedelic sound to audiences in this period, with "I Feel Fine" using guitar feedback; in late 1965 the "Rubber Soul" album included the use of a sitar on "Norwegian Wood" and they employed backmasking on their 1966 single B-side "Rain" and other tracks that appeared on their "Revolver" album later that year.
Psychedelic rock particularly took off in California's emerging music scene as groups followed the Byrds from folk to folk rock from 1965. The psychedelic life style had already developed in San Francisco and particularly prominent products of the scene were the Grateful Dead, Country Joe and the Fish, the Great Society and Jefferson Airplane. The Byrds rapidly progressed from purely folk rock in 1966 with their single "Eight Miles High", widely taken to be a reference to drug use. In Britain, an influential band in the genre were The Yardbirds, who, with Jeff Beck as their guitarist, increasingly moved into psychedelic territory, adding up-tempo improvised "rave ups", Gregorian chant and world music influences to songs including "Still I'm Sad" (1965) and "Over Under Sideways Down" (1966). From 1966 the UK underground scene based in North London, supported new acts including Pink Floyd, Traffic and Soft Machine. The same year saw Donovan's folk-influenced hit album "Sunshine Superman", considered one of the first psychedelic pop records, as well as the débuts of blues rock bands Cream and the Jimi Hendrix Experience, whose extended guitar-heavy jams became a key feature of psychedelia.
Psychedelic rock reached its apogee in the last years of the decade. 1967 saw the Beatles release their definitive psychedelic statement in "Sgt. Pepper's Lonely Hearts Club Band", including the controversial track "Lucy in the Sky with Diamonds" and the Rolling Stones responded later that year with "Their Satanic Majesties Request". Pink Floyd produced what is usually seen as their best psychedelic work "The Piper at the Gates of Dawn". In America the Summer of Love was prefaced by the Human Be-In event and reached its peak at the Monterey Pop Festival, the latter helping to make major American stars of Jimi Hendrix and the Who, whose single "I Can See for Miles" delved into psychedelic territory. Key recordings included Jefferson Airplane's "Surrealistic Pillow" and the Doors' "Strange Days". These trends climaxed in the 1969 Woodstock festival, which saw performances by most of the major psychedelic acts, but by the end of the decade psychedelic rock was in retreat. Brian Wilson of the Beach Boys, Brian Jones of the Rolling Stones, Peter Green of Fleetwood Mac and Syd Barrett of Pink Floyd were early "acid casualties", the Jimi Hendrix Experience and Cream broke up before the end of the decade and many surviving acts moved away from psychedelia into more back-to-basics "roots rock", the wider experimentation of progressive rock, or riff-laden heavy rock.
Progression.
Roots rock.
Roots rock is the term now used to describe a move away from what some saw as the excesses of the psychedelic scene, to a more basic form of rock and roll that incorporated its original influences, particularly country and folk music, leading to the creation of country rock and Southern rock. In 1966 Bob Dylan went to Nashville to record the album "Blonde on Blonde". This, and subsequent more clearly country-influenced albums, have been seen as creating the genre of country folk, a route pursued by a number of, largely acoustic, folk musicians. Other acts that followed the back-to-basics trend were the Canadian group the Band and the California-based Creedence Clearwater Revival, both of which mixed basic rock and roll with folk, country and blues, to be among the most successful and influential bands of the late 1960s. The same movement saw the beginning of the recording careers of Californian solo artists like Ry Cooder, Bonnie Raitt and Lowell George, and influenced the work of established performers such as the Rolling Stones' "Beggar's Banquet" (1968) and the Beatles' "Let It Be" (1970).
In 1968 Gram Parsons recorded "Safe at Home" with the International Submarine Band, arguably the first true country-rock album. Later that year he joined the Byrds for "Sweetheart of the Rodeo" (1968), generally considered one of the most influential recordings in the genre. The Byrds continued in the same vein, but Parsons left to be joined by another ex-Byrds member Chris Hillman in forming the Flying Burrito Brothers who helped establish the respectability and parameters of the genre, before Parsons departed to pursue a solo career. Bands in California that adopted country rock included Hearts and Flowers, Poco and New Riders of the Purple Sage, the Beau Brummels and the Nitty Gritty Dirt Band. Some performers also enjoyed a renaissance by adopting country sounds, including: the Everly Brothers; one-time teen idol Rick Nelson who became the frontman for the Stone Canyon Band; former Monkee Mike Nesmith who formed the First National Band; and Neil Young. The Dillards were, unusually, a country act, who moved towards rock music. The greatest commercial success for country rock came in the 1970s, with artist including the Doobie Brothers, Emmylou Harris, Linda Ronstadt and the Eagles (made up of members of the Burritos, Poco and Stone Canyon Band), who emerged as one of the most successful rock acts of all time, producing albums that included "Hotel California" (1976).
The founders of Southern rock are usually thought to be the Allman Brothers Band, who developed a distinctive sound, largely derived from blues rock, but incorporating elements of boogie, soul, and country in the early 1970s. The most successful act to follow them were Lynyrd Skynyrd, who helped establish the "Good ol' boy" image of the subgenre and the general shape of 1970s' guitar rock. Their successors included the fusion/progressive instrumentalists Dixie Dregs, the more country-influenced Outlaws, jazz-leaning Wet Willie and (incorporating elements of R&B and gospel) the Ozark Mountain Daredevils. After the loss of original members of the Allmans and Lynyrd Skynyrd, the genre began to fade in popularity in the late 1970s, but was sustained the 1980s with acts like .38 Special, Molly Hatchet and the Marshall Tucker Band.
Progressive rock.
Progressive rock, a term sometimes used interchangeably with art rock, was an attempt to move beyond established musical formulas by experimenting with different instruments, song types, and forms. From the mid-1960s the Left Banke, the Beatles, the Rolling Stones and the Beach Boys, had pioneered the inclusion of harpsichords, wind and string sections on their recordings to produce a form of Baroque rock and can be heard in singles like Procol Harum's "A Whiter Shade of Pale" (1967), with its Bach-inspired introduction. The Moody Blues used a full orchestra on their album "Days of Future Passed" (1967) and subsequently created orchestral sounds with synthesisers. Classical orchestration, keyboards and synthesisers were a frequent edition to the established rock format of guitars, bass and drums in subsequent progressive rock.
Instrumentals were common, while songs with lyrics were sometimes conceptual, abstract, or based in fantasy and science fiction. The Pretty Things' "SF Sorrow" (1968), the Who's "Tommy" (1969) and the Kinks' "Arthur (Or the Decline and Fall of the British Empire)" (1969) introduced the format of rock operas and opened the door to concept albums, often telling an epic story or tackling a grand overarching theme. King Crimson's 1969 début album, "In the Court of the Crimson King", which mixed powerful guitar riffs and mellotron, with jazz and symphonic music, is often taken as the key recording in progressive rock, helping the widespread adoption of the genre in the early 1970s among existing blues-rock and psychedelic bands, as well as newly formed acts.
The vibrant Canterbury scene saw acts following Soft Machine from psychedelia, through jazz influences, toward more expansive hard rock, including Caravan, Hatfield and the North, Gong, and National Health. Greater commercial success was enjoyed by Pink Floyd, who also moved away from psychedelia after the departure of Syd Barrett in 1968, with "The Dark Side of the Moon" (1973), seen as a masterpiece of the genre, becoming one of the best-selling albums of all time. There was an emphasis on instrumental virtuosity, with Yes showcasing the skills of both guitarist Steve Howe and keyboard player Rick Wakeman, while Emerson, Lake & Palmer were a supergroup who produced some of the genre's most technically demanding work. Jethro Tull and Genesis both pursued very different, but distinctly English, brands of music. Renaissance, formed in 1969 by ex-Yardbirds Jim McCarty and Keith Relf, evolved into a high-concept band featuring the three-octave voice of Annie Haslam. Most British bands depended on a relatively small cult following, but a handful, including Pink Floyd, Genesis and Jethro Tull, managed to produce top ten singles at home and break the American market.
The American brand of prog rock varied from the eclectic and innovative Frank Zappa, Captain Beefheart and Blood, Sweat & Tears, to more pop rock orientated bands like Boston, Foreigner, Kansas, Journey and Styx. These, beside British bands Supertramp and ELO, all demonstrated a prog rock influence and while ranking among the most commercially successful acts of the 1970s, issuing in the era of "pomp" or "arena rock", which would last until the costs of complex shows (often with theatrical staging and special effects), would be replaced by more economical rock festivals as major live venues in the 1990s.
The instrumental strand of the genre resulted in albums like Mike Oldfield's "Tubular Bells" (1973), the first record, and worldwide hit, for the Virgin Records label, which became a mainstay of the genre. Instrumental rock was particularly significant in continental Europe, allowing bands like Kraftwerk, Tangerine Dream, Can and Faust to circumvent the language barrier. Their synthesiser-heavy "krautrock", along with the work of Brian Eno (for a time the keyboard player with Roxy Music), would be a major influence on subsequent synth rock. With the advent of punk rock and technological changes in the late 1970s, progressive rock was increasingly dismissed as pretentious and overblown. Many bands broke up, but some, including Genesis, ELP, Yes, and Pink Floyd, regularly scored top ten albums with successful accompanying worldwide tours. Some bands which emerged in the aftermath of punk, such as Siouxsie and the Banshees, Ultravox and Simple Minds, showed the influence of prog, as well as their more usually recognized punk influences.
Jazz rock.
In the late 1960s jazz rock emerged as a distinct subgenre out of the blues rock, psychedelic and progressive rock scenes, mixing the power of rock with the musical complexity and improvisational elements of jazz. Many early US rock and roll musicians had begun in jazz and carried some of these elements into the new music. In Britain the subgenre of blues rock, and many of its leading figures, like Ginger Baker and Jack Bruce of Cream, had emerged from the British jazz scene. Often highlighted as the first true jazz-rock recording is the only album by the relatively obscure New York-based the Free Spirits with "Out of Sight and Sound" (1966). The first group of bands to self-consciously use the label were R&B oriented white rock bands that made use of jazzy horn sections, like Electric Flag, Blood, Sweat & Tears and Chicago, to become some of the most commercially successful acts of the later 1960s and early 1970s.
British acts to emerge in the same period from the blues scene, to make use of the tonal and improvisational aspects of jazz, included Nucleus and the Graham Bond and John Mayall spin-off Colosseum. From the psychedelic rock and the Canterbury scenes came Soft Machine, who, it has been suggested, produced one of the artistically successfully fusions of the two genres. Perhaps the most critically acclaimed fusion came from the jazz side of the equation, with Miles Davis, particularly influenced by the work of Hendrix, incorporating rock instrumentation into his sound for the album "Bitches Brew" (1970). It was a major influence on subsequent rock-influenced jazz artists, including Herbie Hancock, Chick Corea and Weather Report. The genre began to fade in the late 1970s, as a mellower form of fusion began to take its audience, but acts like Steely Dan, Frank Zappa and Joni Mitchell recorded significant jazz-influenced albums in this period, and it has continued to be a major influence on rock music.
Glam rock.
Glam rock emerged from the English psychedelic and art rock scenes of the late 1960s and can be seen as both an extension of and reaction against those trends. Musically diverse, varying between the simple rock and roll revivalism of figures like Alvin Stardust to the complex art rock of Roxy Music, and can be seen as much as a fashion as a musical subgenre. Visually it was a mesh of various styles, ranging from 1930s Hollywood glamor, through 1950s pin-up sex appeal, pre-war Cabaret theatrics, Victorian literary and symbolist styles, science fiction, to ancient and occult mysticism and mythology; manifesting itself in outrageous clothes, makeup, hairstyles, and platform-soled boots. Glam is most noted for its sexual and gender ambiguity and representations of androgyny, beside extensive use of theatrics. It was prefigured by the showmanship and gender-identity manipulation of American acts such as the Cockettes and Alice Cooper.
The origins of glam rock are associated with Marc Bolan, who had renamed his folk duo to T. Rex and taken up electric instruments by the end of the 1960s. Often cited as the moment of inception is his appearance on the UK TV programme "Top of the Pops" in December 1970 wearing glitter, to perform what would be his first number 1 single "Ride a White Swan". From 1971, already a minor star, David Bowie developed his Ziggy Stardust persona, incorporating elements of professional make up, mime and performance into his act. These performers were soon followed in the style by acts including Roxy Music, Sweet, Slade, Mott the Hoople, Mud and Alvin Stardust. While highly successful in the single charts in the UK, very few of these musicians were able to make a serious impact in the United States; Bowie was the major exception becoming an international superstar and prompting the adoption of glam styles among acts like Lou Reed, Iggy Pop, New York Dolls and Jobriath, often known as "glitter rock" and with a darker lyrical content than their British counterparts. In the UK the term glitter rock was most often used to refer to the extreme version of glam pursued by Gary Glitter and his support musicians the Glitter Band, who between them achieved eighteen top ten singles in the UK between 1972 and 1976. A second wave of glam rock acts, including Suzi Quatro, Roy Wood's Wizzard and Sparks, dominated the British single charts from about 1974 to 1976. Existing acts, some not usually considered central to the genre, also adopted glam styles, including Rod Stewart, Elton John, Queen and, for a time, even the Rolling Stones. It was also a direct influence on acts that rose to prominence later, including Kiss and Adam Ant, and less directly on the formation of gothic rock and glam metal as well as on punk rock, which helped end the fashion for glam from about 1976. Glam has since enjoyed sporadic modest revivals through bands such as Chainsaw Kittens, the Darkness and in R n' B crossover act Prince.
Soft rock, hard rock and early heavy metal.
From the late 1960s it became common to divide mainstream rock music into soft and hard rock. Soft rock was often derived from folk rock, using acoustic instruments and putting more emphasis on melody and harmonies. Major artists included Carole King, Cat Stevens and James Taylor. It reached its commercial peak in the mid- to late 70s with acts like Billy Joel, America and the reformed Fleetwood Mac, whose "Rumours" (1977) was the best-selling album of the decade. In contrast, hard rock was more often derived from blues-rock and was played louder and with more intensity. It often emphasised the electric guitar, both as a rhythm instrument using simple repetitive riffs and as a solo lead instrument, and was more likely to be used with distortion and other effects. Key acts included British Invasion bands like the Who and the Kinks, as well as psychedelic era performers like Cream, Jimi Hendrix and the Jeff Beck Group. Hard rock-influenced bands that enjoyed international success in the later 1970s included Queen, Thin Lizzy, Aerosmith and AC/DC.
From the late 1960s the term heavy metal began to be used to describe some hard rock played with even more volume and intensity, first as an adjective and by the early 1970s as a noun. The term was first used in music in Steppenwolf's "Born to Be Wild" (1967) and began to be associated with pioneer bands like San Francisco's Blue Cheer and Michigan's Grand Funk Railroad. By 1970 three key British bands had developed the characteristic sounds and styles which would help shape the subgenre. Led Zeppelin added elements of fantasy to their riff laden blues-rock, Deep Purple brought in symphonic and medieval interests from their progressive rock phrase and Black Sabbath introduced facets of the gothic and modal harmony, helping to produce a "darker" sound. These elements were taken up by a "second generation" of heavy metal bands into the late 1970s, including: Judas Priest, UFO, Motörhead and Rainbow from Britain; Kiss, Ted Nugent, and Blue Öyster Cult from the US; Rush from Canada and Scorpions from Germany, all marking the expansion in popularity of the subgenre. Despite a lack of airplay and very little presence on the singles charts, late-1970s heavy metal built a considerable following, particularly among adolescent working-class males in North America and Europe.
Christian rock.
Rock has been criticized by some Christian religious leaders, who have condemned it as immoral, anti-Christian and even demonic. However, Christian rock began to develop in the late 1960s, particularly out of the Jesus movement beginning in Southern California, and emerged as a subgenre in the 1970s with artists like Larry Norman, usually seen as the first major "star" of Christian rock. The genre has been particularly popular in the United States. Many Christian rock performers have ties to the contemporary Christian music scene, while other bands and artists are closely linked to independent music. Since the 1980s Christian rock performers have gained mainstream success, including figures such as the American gospel-to-pop crossover artist Amy Grant and the British singer Cliff Richard. While these artists were largely acceptable in Christian communities the adoption of heavy rock and glam metal styles by bands like Petra and Stryper, who achieved considerable mainstream success in the 1980s, was more controversial. From the 1990s there were increasing numbers of acts who attempted to avoid the Christian band label, preferring to be seen as groups who were also Christians, including P.O.D and Collective Soul.
Punk era.
Punk rock.
Punk rock was developed between 1974 and 1976 in the United States and the United Kingdom. Rooted in garage rock and other forms of what is now known as protopunk music, punk rock bands eschewed the perceived excesses of mainstream 1970s rock. They created fast, hard-edged music, typically with short songs, stripped-down instrumentation, and often political, anti-establishment lyrics. Punk embraces a DIY (do it yourself) ethic, with many bands self-producing their recordings and distributing them through informal channels.
By late 1976, acts such as the Ramones and Patti Smith, in New York City, and the Sex Pistols and the Clash, in London, were recognized as the vanguard of a new musical movement. The following year saw punk rock spreading around the world. Punk quickly, though briefly, became a major cultural phenomenon in the United Kingdom. For the most part, punk took root in local scenes that tended to reject association with the mainstream. An associated punk subculture emerged, expressing youthful rebellion and characterized by distinctive clothing styles and a variety of anti-authoritarian ideologies.
By the beginning of the 1980s, faster, more aggressive styles such as hardcore and Oi! had become the predominant mode of punk rock. This has resulted in several evolved strains of hardcore punk, such as D-beat (a distortion-heavy subgenre influenced by the UK band Discharge), anarcho-punk (such as Crass), grindcore (such as Napalm Death), and crust punk. Musicians identifying with or inspired by punk also pursued a broad range of other variations, giving rise to New wave, post-punk and the alternative rock movement.
New wave.
Although punk rock was a significant social and musical phenomenon, it achieved less in the way of record sales (being distributed by small specialty labels such as Stiff Records), or American radio airplay (as the radio scene continued to be dominated by mainstream formats such as disco and album-oriented rock). Punk rock had attracted devotees from the art and collegiate world and soon bands sporting a more literate, arty approach, such as Talking Heads, and Devo began to infiltrate the punk scene; in some quarters the description "new wave" began to be used to differentiate these less overtly punk bands. Record executives, who had been mostly mystified by the punk movement, recognized the potential of the more accessible new wave acts and began aggressively signing and marketing any band that could claim a remote connection to punk or new wave. Many of these bands, such as the Cars and the Go-Go's can be seen as pop bands marketed as new wave; other existing acts, including the Police, the Pretenders and Elvis Costello, used the new wave movement as the springboard for relatively long and critically successful careers, while "skinny tie" bands exemplified by the Knack, or the photogenic Blondie, began as punk acts and moved into more commercial territory.
Between 1979 and 1985, influenced by Kraftwerk, Yellow Magic Orchestra, David Bowie and Gary Numan, British new wave went in the direction of such New Romantics as Spandau Ballet, Ultravox, Japan, Duran Duran, A Flock of Seagulls, Culture Club, Talk Talk and the Eurythmics, sometimes using the synthesizer to replace all other instruments. This period coincided with the rise of MTV and led to a great deal of exposure for this brand of synthpop, creating what has been characterised as a second British Invasion. Some more traditional rock bands adapted to the video age and profited from MTV's airplay, most obviously Dire Straits, whose "Money for Nothing" gently poked fun at the station, despite the fact that it had helped make them international stars, but in general, guitar-oriented rock was commercially eclipsed.
Post-punk.
If hardcore most directly pursued the stripped down aesthetic of punk, and new wave came to represent its commercial wing, post-punk emerged in the later 1970s and early '80s as its more artistic and challenging side. Major influences beside punk bands were the Velvet Underground, the Who, Frank Zappa and Captain Beefheart, and the New York-based no wave scene which placed an emphasis on performance, including bands such as James Chance and the Contortions, DNA and Sonic Youth. Early contributors to the genre included the US bands Pere Ubu, Devo, the Residents and Talking Heads.
The first wave of British post-punk included Gang of Four, Siouxsie and the Banshees and Joy Division, who placed less emphasis on art than their US counterparts and more on the dark emotional qualities of their music. Bands like Siouxsie and the Banshees, Bauhaus, the Cure, and the Sisters of Mercy, moved increasingly in this direction to found Gothic rock, which had become the basis of a major sub-culture by the early 1980s. Similar emotional territory was pursued by Australian acts like the Birthday Party and Nick Cave. Members of Bauhaus and Joy Division explored new stylistic territory as Love and Rockets and New Order respectively. Another early post-punk movement was the industrial music developed by British bands Throbbing Gristle and Cabaret Voltaire, and New York-based Suicide, using a variety of electronic and sampling techniques that emulated the sound of industrial production and which would develop into a variety of forms of post-industrial music in the 1980s.
The second generation of British post-punk bands that broke through in the early 1980s, including the Fall, the Pop Group, the Mekons, Echo and the Bunnymen and the Teardrop Explodes, tended to move away from dark sonic landscapes. Arguably the most successful band to emerge from post-punk was Ireland's U2, who incorporated elements of religious imagery together with political commentary into their often anthemic music, and by the late 1980s had become one of the biggest bands in the world. Although many post-punk bands continued to record and perform, it declined as a movement in the mid-1980s as acts disbanded or moved off to explore other musical areas, but it has continued to influence the development of rock music and has been seen as a major element in the creation of the alternative rock movement.
New waves and genres in heavy metal.
Although many established bands continued to perform and record, heavy metal suffered a hiatus in the face of the punk movement in the mid-1970s. Part of the reaction saw the popularity of bands like Motörhead, who had adopted a punk sensibility, and Judas Priest, who created a stripped down sound, largely removing the remaining elements of blues music, from their 1978 album "Stained Class". This change of direction was compared to punk and in the late 1970s became known as the New Wave of British Heavy Metal (NWOBHM). These bands were soon followed by acts including Iron Maiden, Vardis, Diamond Head, Saxon, Def Leppard and Venom, many of which began to enjoy considerable success in the US. In the same period Eddie Van Halen established himself as a metal guitar virtuoso after his band's self-titled 1978 album. Randy Rhoads and Yngwie Malmsteen also became established virtuosos, associated with what would be known as the neoclassical metal style.
Inspired by NWOBHM and Van Halen's success, a metal scene began to develop in Southern California from the late 1970s, based on the clubs of L.A.'s Sunset Strip and including such bands as Quiet Riot, Ratt, Mötley Crüe, and W.A.S.P., who, along with similarly styled acts such as New York's Twisted Sister, incorporated the theatrics (and sometimes makeup) of glam rock acts like Alice Cooper and Kiss. The lyrics of these glam metal bands characteristically emphasized hedonism and wild behavior and musically were distinguished by rapid-fire shred guitar solos, anthemic choruses, and a relatively melodic, pop-oriented approach. The most commercially significant release of the era being "Slippery When Wet" (1986) by Bon Jovi from New Jersey, selling over 12 million copies in the US alone. The album has been credited with widening the audience for the subgenre, particularly by appealing to women as well as the traditional male dominated audience, and opening the door to MTV and commercial success for other bands at the end of the decade. By the mid-1980s bands were beginning to emerge from the L.A. scene that pursued a less glam image and a rawer sound, particularly Guns N' Roses, breaking through with the chart-topping "Appetite for Destruction" (1987), and Jane's Addiction, who emerged with their major label debut "Nothing's Shocking", the following year.
In the late 1980s metal fragmented into several subgenres, including thrash metal, which developed in the US from the style known as speed metal, under the influence of hardcore punk, with low-register guitar riffs typically overlaid by shredding leads. Lyrics often expressed nihilistic views or deal with social issues using visceral, gory language. It was popularised by the "Big Four of Thrash": Metallica, Anthrax, Megadeth, and Slayer. Death metal developed out of thrash, particularly influenced by the bands Venom and Slayer. Florida's Death and the Bay Area's Possessed emphasized lyrical elements of blasphemy, diabolism and millenarianism, with vocals usually delivered as guttural "death growls," high-pitched screaming, complemented by downtuned, highly distorted guitars and extremely fast double bass percussion. Black metal, again influenced by Venom and pioneered by Denmark's Mercyful Fate, Switzerland's Hellhammer and Celtic Frost, and Sweden's Bathory, had many similarities in sound to death metal, but was often intentionally lo-fi in production and placed greater emphasis on satanic and pagan themes. Bathory were particularly important in inspiring the further subgenres of Viking metal and folk metal. Power metal emerged in Europe in the late 1980s as a reaction to the harshness of death and black metal and was established by Germany's Helloween, who combined a melodic approach with thrash's speed and energy. England's DragonForce and Florida's Iced Earth have a sound indebted to NWOBHM, while acts such as Florida's Kamelot, Finland's Nightwish, Italy's Rhapsody of Fire, and Russia's Catharsis feature a keyboard-based "symphonic" sound, sometimes employing orchestras and opera singers. In contrast to other subgenres doom metal, influenced by Gothic rock, slowed down the music, with bands like England's Pagan Altar and Witchfinder General and the United States' Pentagram, Saint Vitus and Trouble, emphasizing melody, down-tuned guitars, a 'thicker' or 'heavier' sound and a sepulchral mood. American bands such as Queensrÿche and Dream Theater pioneered an often instrumentally challenging fusion of NWOBHM and progressive rock called progressive metal, with bands such as Symphony X combining aspects of power metal and classical music with the style, while Sweden's Opeth developed a unique style indebted to both death metal and atmospheric 70s prog rock.
Heartland rock.
American working-class oriented heartland rock, characterized by a straightforward musical style, and a concern with the lives of ordinary, blue-collar American people, developed in the second half of the 1970s. The term heartland rock was first used to describe Midwestern arena rock groups like Kansas, REO Speedwagon and Styx, but which came to be associated with a more socially concerned form of roots rock more directly influenced by folk, country and rock and roll. It has been seen as an American Midwest and Rust Belt counterpart to West Coast country rock and the Southern rock of the American South. Led by figures who had initially been identified with punk and New Wave, it was most strongly influenced by acts such as Bob Dylan, the Byrds, Creedence Clearwater Revival and Van Morrison, and the basic rock of 60s garage and the Rolling Stones.
Exemplified by the commercial success of singer songwriters Bruce Springsteen, Bob Seger, and Tom Petty, along with less widely known acts such as Southside Johnny and the Asbury Jukes and Joe Grushecky and the Houserockers, it was partly a reaction to post-industrial urban decline in the East and Mid-West, often dwelling on issues of social disintegration and isolation, beside a form of good-time rock and roll revivalism. The genre reached its commercial, artistic and influential peak in the mid-1980s, with Springsteen's "Born in the USA" (1984), topping the charts worldwide and spawning a series of top ten singles, together with the arrival of artists including John Mellencamp, Steve Earle and more gentle singer/songwriters such as Bruce Hornsby. It can also be heard as an influence on artists as diverse as Billy Joel, Kid Rock and the Killers.
Heartland rock faded away as a recognized genre by the early 1990s, as rock music in general, and blue collar and white working class themes in particular, lost influence with younger audiences, and as heartland's artists turned to more personal works. Many heartland rock artists continue to record today with critical and commercial success, most notably Bruce Springsteen, Tom Petty and John Mellencamp, although their works have become more personal and experimental and no longer fit easily into a single genre. Newer artists whose music would perhaps have been labelled heartland rock had it been released in the 1970s or 1980s, such as Missouri's Bottle Rockets and Illinois' Uncle Tupelo, often find themselves labeled alt-country.
Emergence of alternative rock.
The term alternative rock was coined in the early 1980s to describe rock artists who did not fit into the mainstream genres of the time. Bands dubbed "alternative" had no unified style, but were all seen as distinct from mainstream music. Alternative bands were linked by their collective debt to punk rock, through hardcore, New Wave or the post-punk movements. Important alternative rock bands of the 1980s in the US included R.E.M., Hüsker Dü, Jane's Addiction, Sonic Youth, and the Pixies, and in the UK the Cure, New Order, the Jesus and Mary Chain, and the Smiths. Artists were largely confined to independent record labels, building an extensive underground music scene based on college radio, fanzines, touring, and word-of-mouth. They rejected the dominant synthpop of the early 1980s, marking a return to group-based guitar rock.
Few of these early bands achieved mainstream success, although exceptions to this rule include R.E.M., the Smiths, and the Cure. Despite a general lack of spectacular album sales, the original alternative rock bands exerted a considerable influence on the generation of musicians who came of age in the 1980s and ended up breaking through to mainstream success in the 1990s. Styles of alternative rock in the U.S. during the 1980s included jangle pop, associated with the early recordings of R.E.M., which incorporated the ringing guitars of mid-1960s pop and rock, and college rock, used to describe alternative bands that began in the college circuit and college radio, including acts such as 10,000 Maniacs and the Feelies. In the UK Gothic rock was dominant in the early 1980s, but by the end of the decade indie or dream pop like Primal Scream, Bogshed, Half Man Half Biscuit and the Wedding Present, and what were dubbed shoegaze bands like My Bloody Valentine, Ride, Lush, Chapterhouse, and the Boo Radleys. Particularly vibrant was the Madchester scene, produced such bands as Happy Mondays, the Inspiral Carpets, and Stone Roses. The next decade would see the success of grunge in the United States and Britpop in the United Kingdom, bringing alternative rock into the mainstream.
Alternative.
Grunge.
Disaffected by commercialized and highly produced pop and rock in the mid-1980s, bands in Washington state (particularly in the Seattle area) formed a new style of rock which sharply contrasted with the mainstream music of the time. The developing genre came to be known as "grunge", a term descriptive of the dirty sound of the music and the unkempt appearance of most musicians, who actively rebelled against the over-groomed images of other artists. Grunge fused elements of hardcore punk and heavy metal into a single sound, and made heavy use of guitar distortion, fuzz and feedback. The lyrics were typically apathetic and angst-filled, and often concerned themes such as social alienation and entrapment, although it was also known for its dark humor and parodies of commercial rock.
Bands such as Green River, Soundgarden, the Melvins and Skin Yard pioneered the genre, with Mudhoney becoming the most successful by the end of the decade. However, grunge remained largely a local phenomenon until 1991, when Nirvana's "Nevermind" became a huge success thanks to the lead single "Smells Like Teen Spirit". "Nevermind" was more melodic than its predecessors, but the band refused to employ traditional corporate promotion and marketing mechanisms. During 1991 and 1992, other grunge albums such as Pearl Jam's "Ten", Soundgarden's "Badmotorfinger" and Alice in Chains' "Dirt", along with the "Temple of the Dog" album featuring members of Pearl Jam and Soundgarden, became among the 100 top-selling albums. Major record labels signed most of the remaining grunge bands in Seattle, while a second influx of acts moved to the city in the hope of success. However, with the death of Kurt Cobain and the subsequent break-up of Nirvana in 1994, touring problems for Pearl Jam and the departure of Alice in Chains' lead singer Layne Staley in 1996, the genre began to decline, partly to be overshadowed by Britpop and more commercial sounding post-grunge.
Britpop.
Britpop emerged from the British alternative rock scene of the early 1990s and was characterised by bands particularly influenced by British guitar music of the 1960s and 1970s. The Smiths were a major influence, as were bands of the Madchester scene, which had dissolved in the early 1990s. The movement has been seen partly as a reaction against various U.S. based, musical and cultural trends in the late 1980s and early 1990s, particularly the grunge phenomenon and as a reassertion of a British rock identity. Britpop was varied in style, but often used catchy tunes and hooks, beside lyrics with particularly British concerns and the adoption of the iconography of the 1960s British Invasion, including the symbols of British identity previously utilised by the mods. It was launched around 1992 with releases by groups such as Suede and Blur, who were soon joined by others including Oasis, Pulp, Supergrass and Elastica, who produced a series of top ten albums and singles. For a while the contest between Blur and Oasis was built by the popular press into "The Battle of Britpop", initially won by Blur, but with Oasis achieving greater long-term and international success, directly influencing a third generation of Britpop bands, including The Boo Radleys, Ocean Colour Scene and Cast. Britpop groups brought British alternative rock into the mainstream and formed the backbone of a larger British cultural movement known as Cool Britannia. Although its more popular bands, particularly Blur and Oasis, were able to spread their commercial success overseas, especially to the United States, the movement had largely fallen apart by the end of the decade.
Post-grunge.
The term post-grunge was coined for the generation of bands that followed the emergence into the mainstream and subsequent hiatus of the Seattle grunge bands. Post-grunge bands emulated their attitudes and music, but with a more radio-friendly commercially oriented sound. Often they worked through the major labels and came to incorporate diverse influences from jangle pop, pop-punk, alternative metal or hard rock. The term post-grunge was meant to be pejorative, suggesting that they were simply musically derivative, or a cynical response to an "authentic" rock movement. From 1994, former Nirvana drummer Dave Grohl's new band, the Foo Fighters, helped popularize the genre and define its parameters.
Some post-grunge bands, like Candlebox, were from Seattle, but the subgenre was marked by a broadening of the geographical base of grunge, with bands like Los Angeles' Audioslave, and Georgia's Collective Soul and beyond the US to Australia's Silverchair and Britain's Bush, who all cemented post-grunge as one of the most commercially viable subgenres of the late 1990s. Although male bands predominated, female solo artist Alanis Morissette's 1995 album "Jagged Little Pill", labelled as post-grunge, also became a multi-platinum hit. Bands like Creed and Nickelback took post-grunge into the 21st century with considerable commercial success, abandoning most of the angst and anger of the original movement for more conventional anthems, narratives and romantic songs, and were followed in this vein by new acts including Shinedown, Seether, 3 Doors Down and Puddle of Mudd.
Pop punk.
The origins of 1990s pop punk can be seen in the more song-oriented bands of the 1970s punk movement like the Buzzcocks and the Clash, commercially successful New Wave acts such as the Jam and the Undertones, and the more hardcore-influenced elements of alternative rock in the 1980s. Pop-punk tends to use power-pop melodies and chord changes with speedy punk tempos and loud guitars. Punk music provided the inspiration for some California-based bands on independent labels in the early 1990s, including Rancid, Pennywise, Weezer and Green Day. In 1994 Green Day moved to a major label and produced the album "Dookie", which found a new, largely teenage, audience and proved a surprise diamond-selling success, leading to a series of hit singles, including two number ones in the US. They were soon followed by the eponymous début from Weezer, which spawned three top ten singles in the US. This success opened the door for the multi-platinum sales of metallic punk band the Offspring with "Smash" (1994). This first wave of pop punk reached its commercial peak with Green Day's "Nimrod" (1997) and The Offspring's "Americana" (1998).
A second wave of pop punk was spearheaded by Blink-182, with their breakthrough album "Enema of the State" (1999), followed by bands such as Good Charlotte, Bowling for Soup and Sum 41, who made use of humour in their videos and had a more radio-friendly tone to their music, while retaining the speed, some of the attitude and even the look of 1970s punk. Later pop-punk bands, including Simple Plan, the All-American Rejects and Fall Out Boy, had a sound that has been described as closer to 1980s hardcore, while still achieving commercial success.
Indie rock.
In the 1980s the terms indie rock and alternative rock were used interchangeably. By the mid-1990s, as elements of the movement began to attract mainstream interest, particularly grunge and then Britpop, post-grunge and pop-punk, the term alternative began to lose its meaning. Those bands following the less commercial contours of the scene were increasingly referred to by the label indie. They characteristically attempted to retain control of their careers by releasing albums on their own or small independent labels, while relying on touring, word-of-mouth, and airplay on independent or college radio stations for promotion. Linked by an ethos more than a musical approach, the indie rock movement encompassed a wide range of styles, from hard-edged, grunge-influenced bands like the Cranberries and Superchunk, through do-it-yourself experimental bands like Pavement, to punk-folk singers such as Ani DiFranco. It has been noted that indie rock has a relatively high proportion of female artists compared with preceding rock genres, a tendency exemplified by the development of feminist-informed Riot Grrrl music. Many countries have developed an extensive local indie scene, flourishing with bands with enough popularity to survive inside the respective country, but virtually unknown outside them.
By the end of the 1990s many recognisable subgenres, most with their origins in the late '80s alternative movement, were included under the umbrella of indie. Lo-fi eschewed polished recording techniques for a D.I.Y. ethos and was spearheaded by Beck, Sebadoh and Pavement. The work of Talk Talk and Slint helped inspire both post rock, an experimental style influenced by jazz and electronic music, pioneered by Bark Psychosis and taken up by acts such as Tortoise, Stereolab, and Laika, as well as leading to more dense and complex, guitar-based math rock, developed by acts like Polvo and Chavez. Space rock looked back to progressive roots, with drone heavy and minimalist acts like Spacemen 3, the two bands created out of its split, Spectrum and Spiritualized, and later groups including Flying Saucer Attack, Godspeed You Black Emperor! and Quickspace. In contrast, Sadcore emphasised pain and suffering through melodic use of acoustic and electronic instrumentation in the music of bands like American Music Club and Red House Painters, while the revival of Baroque pop reacted against lo-fi and experimental music by placing an emphasis on melody and classical instrumentation, with artists like Arcade Fire, Belle and Sebastian and Rufus Wainright.
Alternative metal, rap rock and nu metal.
Alternative metal emerged from the hardcore scene of alternative rock in the US in the later 1980s, but gained a wider audience after grunge broke into the mainstream in the early 1990s. Early alternative metal bands mixed a wide variety of genres with hardcore and heavy metal sensibilities, with acts like Jane's Addiction and Primus utilizing prog-rock, Soundgarden and Corrosion of Conformity using garage punk, the Jesus Lizard and Helmet mixing noise-rock, Ministry and Nine Inch Nails influenced by industrial music, Monster Magnet moving into psychedelia, Pantera, Sepultura and White Zombie creating groove metal, while Biohazard and Faith No More turned to hip hop and rap.
Hip hop had gained attention from rock acts in the early 1980s, including The Clash with "The Magnificent Seven" (1981) and Blondie with "Rapture" (1981). Early crossover acts included Run DMC and the Beastie Boys. Detroit rapper Esham became known for his "acid rap" style, which fused rapping with a sound that was often based in rock and heavy metal. Rappers who sampled rock songs included Ice-T, The Fat Boys, LL Cool J, Public Enemy and Whodini. The mixing of thrash metal and rap was pioneered by Anthrax on their 1987 comedy-influenced single "I'm the Man".
In 1990, Faith No More broke into the mainstream with their single "Epic", often seen as the first truly successful combination of heavy metal with rap. This paved the way for the success of existing bands like 24-7 Spyz and Living Colour, and new acts including Rage Against the Machine and Red Hot Chili Peppers, who all fused rock and hip hop among other influences. Among the first wave of performers to gain mainstream success as rap rock were 311, Bloodhound Gang, and Kid Rock. A more metallic sound - "nu metal" - was pursued by bands including Limp Bizkit, Korn and Slipknot. Later in the decade this style, which contained a mix of grunge, punk, metal, rap and turntable scratching, spawned a wave of successful bands like Linkin Park, P.O.D. and Staind, who were often classified as rap metal or nu metal, the first of which are the best-selling band of the genre.
In 2001, nu metal reached its peak with albums like Staind's "Break the Cycle", P.O.D's "Satellite", Slipknot's "Iowa" and Linkin Park's "Hybrid Theory". New bands also emerged like Disturbed, Godsmack and Papa Roach, whose major label début "Infest" became a platinum hit. Korn's long awaited fifth album "Untouchables", and Papa Roach's second album "Lovehatetragedy", did not sell as well as their previous releases, while nu metal bands were played more infrequently on rock radio stations and MTV began focusing on pop punk and emo. Since then, many bands have changed to a more conventional hard rock, heavy metal, or electronic music sound.
Post-Britpop.
From about 1997, as dissatisfaction grew with the concept of Cool Britannia, and Britpop as a movement began to dissolve, emerging bands began to avoid the Britpop label while still producing music derived from it. Many of these bands tended to mix elements of British traditional rock (or British trad rock), particularly the Beatles, Rolling Stones and Small Faces, with American influences, including post-grunge. Drawn from across the United Kingdom (with several important bands emerging from the north of England, Scotland, Wales and Northern Ireland), the themes of their music tended to be less parochially centered on British, English and London life and more introspective than had been the case with Britpop at its height. This, beside a greater willingness to engage with the American press and fans, may have helped some of them in achieving international success.
Post Britpop bands have been seen as presenting the image of the rock star as an ordinary person and their increasingly melodic music was criticised for being bland or derivative. Post Britpop bands like the Verve with "Urban Hymns" (1997), Radiohead from "OK Computer" (1997), Travis from "The Man Who" (1999), Stereophonics from "Performance and Cocktails" (1999), Feeder from "Echo Park" (2001) and particularly Coldplay from their debut album "Parachutes" (2000), achieved much wider international success than most of the Britpop groups that had preceded them, and were some of the most commercially successful acts of the late 1990s and early 2000s, arguably providing a launchpad for the subsequent garage rock or post-punk revival, which has also been seen as a reaction to their introspective brand of rock.
2000s–present.
Post-hardcore and emo.
Post-hardcore developed in the US, particularly in the Chicago and Washington, D.C areas, in the early to mid-1980s, with bands that were inspired by the do-it-yourself ethics and guitar-heavy music of hardcore punk, but influenced by post-punk, adopting longer song formats, more complex musical structures and sometimes more melodic vocal styles. Existing bands that moved on from hardcore included Fugazi. From the late 1980s they were followed by bands including Quicksand, Girls Against Boys and The Jesus Lizard. Bands that formed in the 1990s included Thursday, Thrice, Finch, and Poison the Well.
Emo also emerged from the hardcore scene in 1980s Washington, D.C., initially as "emocore", used as a term to describe bands who favored expressive vocals over the more common abrasive, barking style. The style was pioneered by bands Rites of Spring and Embrace, the last formed by Ian MacKaye, whose Dischord Records became a major centre for the emerging D.C. emo scene, releasing work by Rites of Spring, Dag Nasty, Nation of Ulysses and Fugazi. Fugazi emerged as the definitive early emo band, gaining a fanbase among alternative rock followers, not least for their overtly anti-commercial stance. The early emo scene operated as an underground, with short-lived bands releasing small-run vinyl records on tiny independent labels. The mid-'90s sound of emo was defined by bands like Jawbreaker and Sunny Day Real Estate who incorporated elements of grunge and more melodic rock. Only after the breakthrough of grunge and pop punk into the mainstream did emo come to wider attention with the success of Weezer's "Pinkerton" (1996) album, which utilised pop punk. Late 1990s bands drew on the work of Fugazi, SDRE, Jawbreaker and Weezer, including The Promise Ring, The Get Up Kids, Braid, Texas Is the Reason, Joan of Arc, Jets to Brazil and most successfully Jimmy Eat World, and by the end of the millennium it was one of the more popular indie styles in the US.
Emo broke into mainstream culture in the early 2000s with the platinum-selling success of Jimmy Eat World's "Bleed American" (2001) and Dashboard Confessional's "The Places You Have Come to Fear the Most" (2003). The new emo had a much more mainstream sound than in the 90s and a far greater appeal amongst adolescents than its earlier incarnations. At the same time, use of the term emo expanded beyond the musical genre, becoming associated with fashion, a hairstyle and any music that expressed emotion. The term emo has been applied by critics and journalists to a variety of artists, including multi-platinum acts such as Fall Out Boy and My Chemical Romance and disparate groups such as Paramore and Panic at the Disco, even when they protest the label. By 2003 post-hardcore bands had also caught the attention of major labels and began to enjoy mainstream success in the album charts. A number of these bands were seen as a more aggressive offshoot of emo and given the often vague label of screamo. Around this time, a new wave of post-hardcore bands began to emerge onto the scene that incorporated more pop punk and alternative rock styles into their music, including The Used, Hawthorne Heights, Senses Fail, From First to Last and Emery and Canadian bands Silverstein and Alexisonfire. British bands like Funeral For A Friend, The Blackout and Enter Shikari also made headway.
Garage rock/post-punk revival.
In the early 2000s, a new group of bands that played a stripped down and back-to-basics version of guitar rock, emerged into the mainstream. They were variously characterised as part of a garage rock, post-punk or new wave revival. Because the bands came from across the globe, cited diverse influences (from traditional blues, through New Wave to grunge), and adopted differing styles of dress, their unity as a genre has been disputed. There had been attempts to revive garage rock and elements of punk in the 1980s and 1990s and by 2000 scenes had grown up in several countries. The Detroit rock scene included the Von Bondies, Electric Six, the Dirtbombs and the Detroit Cobras and that of New York Radio 4, Yeah Yeah Yeahs and the Rapture. Elsewhere, other lesser-known acts such as Billy Childish and the Buff Medways from Britain, the (International) Noise Conspiracy from Sweden, the 5.6.7.8's from Japan, and the Oblivians from Memphis enjoyed underground, regional or national success.
The commercial breakthrough from these scenes was led by four bands: the Strokes, who emerged from the New York club scene with their début album "Is This It" (2001); the White Stripes, from Detroit, with their third album "White Blood Cells" (2001); the Hives from Sweden after their compilation album "Your New Favourite Band" (2001); and the Vines from Australia with "Highly Evolved" (2002). They were christened by the media as the "The" bands, and dubbed "The saviours of rock 'n' roll", leading to accusations of hype. A second wave of bands that gained international recognition due to the movement included Black Rebel Motorcycle Club, the Killers, Interpol and Kings of Leon from the US, the Libertines, Arctic Monkeys, Bloc Party, Editors, Franz Ferdinand and Placebo from the UK, Jet from Australia and the Datsuns and the D4 from New Zealand.
Contemporary heavy metal, metalcore and retro-metal.
By the new millennium Scandinavia had emerged as one of the areas producing innovative and successful bands, while Belgium, Holland and especially Germany were the most significant markets. Established continental metal bands that placed multiple albums in the top 20 of the German charts between 2003 and 2008, including Finnish band Children of Bodom, Norwegian act Dimmu Borgir, Germany's Blind Guardian and Sweden's HammerFall.
Metalcore, originally an American hybrid of thrash metal and hardcore punk, emerged as a commercial force in the mid-2000s. It was rooted in the crossover thrash style developed two decades earlier by bands such as Suicidal Tendencies, Dirty Rotten Imbeciles, and Stormtroopers of Death and remained an underground phenomenon through the 1990s; early bands include Earth Crisis, Converge, Hatebreed and Shai Hulud. Killswitch Engage's "The End of Heartache" and Shadows Fall's "The War Within" to debut at number 21 and number 20, respectively, on the "Billboard" album chart. Bullet for My Valentine, from Wales, broke into the top 5 in both the U.S. and British charts with "Scream Aim Fire" (2008). Metalcore bands have received prominent slots at Ozzfest and the Download Festival. Lamb of God, with a related blend of metal styles, reached number 2 on the "Billboard" charts in 2009 with "Wrath".
The success of these bands and others such as Trivium, who have released both metalcore and straight-ahead thrash albums, and Mastodon, who played in a progressive/sludge style, inspired claims of a metal revival in the United States, dubbed by some critics the "New Wave of American Heavy Metal". Its roots have been traced to the music of acts like Pantera, Biohazard and Machine Head, drawing on New York hardcore, thrash metal and punk, helping to inspire a move away from the nu metal of the early 2000s and a return to riffs and guitar solos.
The term "retro-metal" has been applied to such bands as Texas-based the Sword, California's High on Fire, Sweden's Witchcraft, and Australia's Wolfmother. The Sword's "Age of Winters" (2006) drew heavily on the work of Black Sabbath and Pentagram, while Witchcraft added elements of folk rock and psychedelic rock, and Wolfmother's self-titled 2005 debut album combined elements of the sounds of Deep Purple and Led Zeppelin.
Digital electronic rock.
In the 2000s, as computer technology became more accessible and music software advanced, it became possible to create high quality music using little more than a single laptop computer. This resulted in a massive increase in the amount of home-produced electronic music available to the general public via the expanding internet, and new forms of performance such as laptronica and live coding. These techniques also began to be used by existing bands, as with industrial rock act Nine Inch Nails' album "Year Zero" (2007), and by developing genres that mixed rock with digital techniques and sounds, including indie electronic, electroclash, dance-punk and new rave.
Indie electronic, which had begun in the early 1990s with bands like Stereolab and Disco Inferno, took off in the new millennium as the new digital technology developed, with acts including Broadcast from the UK, Justice from France, Lali Puna from Germany and The Postal Service, and Ratatat from the US, mixing a variety of indie sounds with electronic music, largely produced on small independent labels. The electroclash subgenre began in New York at the end of the 1990s, combining synth pop, techno, punk and performance art. It was pioneered by I-F with their track "Space Invaders Are Smoking Grass" (1998), and pursued by artists including Felix da Housecat, Peaches, Chicks on Speed, and Ladytron. It gained international attention at the beginning of the new millennium and spread to scenes in London and Berlin, but rapidly faded as a recognisable genre. Dance-punk, mixing post-punk sounds with disco and funk, had developed in the 1980s, but it was revived among some bands of the garage rock/post-punk revival in the early years of the new millennium, particularly among New York acts such as Liars, The Rapture and Radio 4, joined by dance-oriented acts who adopted rock sounds such as Out Hud. In Britain the combination of indie with dance-punk was dubbed new rave in publicity for Klaxons and the term was picked up and applied by the NME to bands including Trash Fashion, New Young Pony Club, Hadouken!, Late of the Pier, Test Icicles and Shitdisco, forming a scene with a similar visual aesthetic to earlier rave music.
Renewed interest in electronic music and nostalgia for the 1980s led to the beginnings of a synthpop revival, with acts including Adult and Fischerspooner. In 2003-4 it began to move into the mainstream with Ladytron, the Postal Service, Cut Copy, the Bravery and, with most commercial success, The Killers all producing records that incorporated vintage synthesizer sounds and styles which contrasted with the dominant sounds of post-grunge and nu-metal. The style was picked up by a large number of performers, particularly female solo artists, leading the British and other media to proclaim a new era of the female electropop star. Artists named included British acts Little Boots, La Roux and Ladyhawke. Male acts that emerged in the same period included Calvin Harris, Frankmusik, Hurts, Kaskade, LMFAO, and Owl City, whose single "Fireflies" (2009) reached the top of the Billboard chart.
Social impact.
Different subgenres of rock were adopted by, and became central to, the identity of a large number of sub-cultures. In the 1950s and 1960s, respectively, British youths adopted the Teddy Boy and Rockers subcultures, which revolved around US rock and roll. The counterculture of the 1960s was closely associated with psychedelic rock. The mid-1970s punk subculture began in the US, but it was given a distinctive look by British designer Vivienne Westwood, a look which spread worldwide. Out of the punk scene, the Goth and Emo subcultures grew, both of which presented distinctive visual styles.
When an international rock culture developed, it supplanted cinema as the major sources of fashion influence. Paradoxically, followers of rock music have often mistrusted the world of fashion, which has been seen as elevating image above substance. Rock fashions have been seen as combining elements of different cultures and periods, as well as expressing divergent views on sexuality and gender, and rock music in general has been noted and criticised for facilitating greater sexual freedom. Rock has also been associated with various forms of drug use, including the stimulants taken by some mods in the early to mid-1960s, through the LSD linked with psychedelic rock in the late 1960s and early 1970s; and sometimes to cannabis, cocaine and heroin, all of which have been eulogised in song.
Rock has been credited with changing attitudes to race by opening up African-American culture to white audiences; but at the same time, rock has been accused of appropriating and exploiting that culture. While rock music has absorbed many influences and introduced Western audiences to different musical traditions, the global spread of rock music has been interpreted as a form of cultural imperialism. Rock music inherited the folk tradition of protest song, making political statements on subjects such as war, religion, poverty, civil rights, justice and the environment. Political activism reached a mainstream peak with the "Do They Know It's Christmas?" single (1984) and Live Aid concert for Ethiopia in 1985, which, while successfully raising awareness of world poverty and funds for aid, have also been criticised (along with similar events), for providing a stage for self-aggrandisement and increased profits for the rock stars involved.
Since its early development rock music has been associated with rebellion against social and political norms, most obviously in early rock and roll's rejection of an adult-dominated culture, the counterculture's rejection of consumerism and conformity and punk's rejection of all forms of social convention, however, it can also be seen as providing a means of commercial exploitation of such ideas and of diverting youth away from political action.

</doc>
<doc id="25521" url="http://en.wikipedia.org/wiki?curid=25521" title="Relay league">
Relay league

A relay league is a chain of message forwarding stations in a system of optical telegraphs, radio telegraph stations, or riding couriers.
An interesting description of these early 19th century methods and its evolution into the electrical telegraph networks of the mid-to-late 19th century is found in "The Victorian Internet", a book by Tom Standage ISBN 0-425-17169-8.
Radio relay leagues.
Radio amateurs have been early in arranging relay leagues, as is reflected in the name of the organization of American Radio Relay League (ARRL), http://www.arrl.org/.
Radio amateur message relay operations were originally conducted using Morse code in the first two decades of the 20th century using spark-gap transmitters. As vacuum tubes became affordable operations shifted to more efficient manual telegraphy transmitters, referred to as CW (Continuous wave). Messages were relayed station to station typically covering four or more re-transmission cycles to cover the continental United States, in an organized system of amateur radio networks. After World War II, voice and radioteletype implementations of the message relay system were employed.

</doc>
<doc id="25522" url="http://en.wikipedia.org/wiki?curid=25522" title="History of radio">
History of radio

The early history of radio is the history of technology that produced radio instruments that use radio waves. Within the timeline of radio, many people contributed theory and inventions in what became radio. Radio development began as "wireless telegraphy". Later radio history increasingly involves matters of programming and content.
Summary.
Invention.
The idea of wireless communication predates the discovery of "radio" with experiments in "wireless telegraphy" via inductive and capacitive induction and transmission through the ground, water, and even train tracks from the 1830s on. In 1873 James Clerk Maxwell showed mathematically that electromagnetic waves could propagate through free space. It is likely that the first intentional transmission of a signal by means of electromagnetic waves was performed in an experiment by David Edward Hughes around 1880, although this was considered to be induction at the time. In 1888 Heinrich Rudolf Hertz was able to conclusively prove transmitted airborne electromagnetic waves in an experiment confirming Maxwell's theory of electromagnetism.
After the discovery of these "Hertzian waves" (it would take almost 20 years for the term "radio" to be universally adopted for this type of electromagnetic radiation) many scientists and inventors experimented with wireless transmission, some trying to develop a system of communication, some not, some intentionally using these new Hertzian waves, some not. Maxwell's theory showing that light and Hertzian electromagnetic waves were the same phenomenon at different wavelengths led "Maxwellian" scientist such as John Perry, Frederick Thomas Trouton and Alexander Trotter to assume they would be analogous to optical signaling and the Serbian American engineer Nikola Tesla to consider them relatively useless for communication since "light" could not transmit further than line of sight. In 1892 the physicist William Crookes wrote on the possibilities of wireless telegraphy based on Hertzian waves and in 1893 Tesla proposed a system for transmitting intelligence and wireless power using the earth as the medium. Others, such as Amos Dolbear, Sir Oliver Lodge, Reginald Fessenden, and Alexander Popov were involved in the development of components and theory involved with the transmission and reception of airborne electromagnetic waves for their own theoretical work or as a potential means of communication.
Over several years starting in 1894 the Italian inventor Guglielmo Marconi built the first complete, commercially successful wireless telegraphy system based on airborne Hertzian waves (radio transmission). Marconi demonstrated application of radio in military and marine communications and started a company for the development and propagation of radio communication services and equipment.
19th century.
The meaning and usage of the word "radio" has developed in parallel with developments within the field of communications and can be seen to have three distinct phases: electromagnetic waves and experimentation; wireless communication and technical development; and radio broadcasting and commercialization. James Clerk Maxwell predicted the propagation of electromagnetic waves (radio waves) (1873) and Heinrich Rudolf Hertz made the first demonstration of transmission of radio waves through free space (1887) but many individuals—inventors, engineers, developers and businessmen constructed systems based on their own understanding of these and other phenomenon, some predating Maxwell and Hertz' discoveries. Thus "wireless telegraphy" and radio wave based systems can be attributed to multiple "inventors". Development from a laboratory demonstration to a commercial entity spanned several decades and required the efforts of many practitioners.
In 1878, David E. Hughes noticed that sparks could be heard in a telephone receiver when experimenting with his carbon microphone. He developed this carbon-based detector further and eventually could detect signals over a few hundred yards. He demonstrated his discovery to the Royal Society in 1880, but was told it was merely induction, and therefore abandoned further research.
Experiments were undertaken by Thomas Edison and his employees at Menlo Park. Edison applied in 1885 to the U.S. Patent Office for a patent on an electrostatic coupling system between elevated terminals. The patent was granted as U.S. Patent on December 29, 1891. The Marconi Company would later purchase rights to the Edison patent to protect them legally from lawsuits.
In 1884 Temistocle Calzecchi-Onesti at Fermo in Italy experiments with tubes containing powder and nickel silver with traces of mercury metal filings and their reactions when conducting electricity. This would lead to the development of the iron filings filled coherer, a radio detecting device usually credited to Edouard Branly in 1890.
Hertzian waves.
Between 1886 and 1888 Heinrich Rudolf Hertz published the results of his experiments where he was able to transmit electromagnetic waves (radio waves) through the air, proving Maxwell's electromagnetic theory. Early on after their discovery, radio waves were referred to as "Hertzian waves". Between 1890 and 1892 physicists such as John Perry, Frederick Thomas Trouton and William Crookes proposed electromagnetic or Hertzian waves as a navigation aid or means of communication, with Crookes writing on the possibilities of wireless telegraphy based on Hertzian waves in 1892.
After learning of Hertz demonstrations of wireless transmission, inventor Nikola Tesla began developing his own system based on Hertz and Maxwell's ideas, primarily as a means of wireless lighting and power distribution. Tesla, concluding that Hertz had not demonstrated airborne electromagnetic waves (radio transmission), went on to develop a system based on what he thought was the primary conductor, the earth. In 1893 demonstrations of his ideas, in St. Louis, Missouri and at the "Franklin Institute" in Philadelphia, Tesla proposed this wireless power technology could also incorporate a system for the telecommunication of information.
In a lecture on the work of Hertz, shortly after his death, Professor Oliver Lodge and Alexander Muirhead demonstrated wireless signaling using Hertzian (radio) waves in the lecture theater of the Oxford University Museum of Natural History on August 14, 1894. During the demonstration a radio signal was sent from the neighboring Clarendon laboratory building, and received by apparatus in the lecture theater.
Building on the work of Lodge, the Indian Bengali physicist Jagadish Chandra Bose ignited gunpowder and rang a bell at a distance using millimeter range wavelength microwaves in a November 1894 public demonstration at the Town Hall of Kolkata. Bose wrote in a Bengali essay, Adrisya Alok (Invisible Light), "The invisible light can easily pass through brick walls, buildings etc. Therefore, messages can be transmitted by means of it without the mediation of wires." Bose’s first scientific paper, "On polarisation of electric rays by double-refracting crystals" was communicated to the Asiatic Society of Bengal in May 1895. His second paper was communicated to the Royal Society of London by Lord Rayleigh in October 1895. In December 1895, the London journal The Electrician (Vol. 36) published Bose’s paper, "On a new electro-polariscope". At that time, the word 'coherer', coined by Lodge, was used in the English-speaking world for Hertzian wave receivers or detectors. The Electrician readily commented on Bose’s coherer. (December 1895). The Englishman (18 January 1896) quoted from the Electrician and commented as follows: "Should Professor Bose succeed in perfecting and patenting his ‘Coherer’, we may in time see the whole system of coast lighting throughout the navigable world revolutionised by an Indian Bengali scientist working single handed in our Presidency College Laboratory." Bose planned to "perfect his coherer", but never thought of patenting it.
In 1895, conducting experiments along the lines of Hertz's research, Alexander Stepanovich Popov built his first radio receiver, which contained a coherer. Further refined as a lightning detector, it was presented to the Russian Physical and Chemical Society on May 7, 1895. A depiction of Popov's lightning detector was printed in the Journal of the Russian Physical and Chemical Society the same year (publication of the minutes 15/201 of this session — December issue of the journal RPCS). An earlier description of the device was given by Dmitry Aleksandrovich Lachinov in July 1895 in the 2nd edition of his course "Fundamentals of Meteorology and climatology" — the first in Russia. Popov's receiver was created on the improved basis of Lodge's receiver, and originally intended for reproduction of its experiments.
Marconi.
In 1894 the young Italian inventor Guglielmo Marconi began working on the idea of building a commercial wireless telegraphy system based on the use of Hertzian waves (radio waves), a line of inquiry that he noted other inventors did not seem to be pursuing. Marconi read through the literature and used the ideas of others who were experimenting with radio waves but did a great deal to develop devices such as portable transmitters and receiver systems that could work over long distances, turning what was essentially a laboratory experiment into useful communication system. By August 1895 Marconi was field testing his system but even with improvements he was only able to transmit signals up to one-half mile, a distance Oliver Lodge had predicted in 1894 as the maximum transmission distance for radio waves. Marconi raised the height of his antenna and hit upon the idea of grounding his transmitter and receiver. With these improvements the system was capable of transmitting signals up to 2 mi and over hills. Marconi's experimental apparatus proved to be the first engineering-complete, commercially successful radio transmission system. Marconi’s apparatus is also credited for saving the 700 people that survived the tragic Titanic disaster.
In 1896, Marconi was awarded British patent 12039, "Improvements in transmitting electrical impulses and signals and in apparatus there-for", the first patent ever issued for a Hertzian wave (radio wave) base wireless telegraphic system. In 1897, he established a radio station on the Isle of Wight, England. Marconi opened his "wireless" factory in the former silk-works at Hall Street, Chelmsford, England in 1898, employing around 60 people. Shortly after the 1900s, Marconi held the patent rights for radio. Marconi would go on to win the Nobel Prize in Physics in 1909 and be more successful than any other inventor in his ability to "commercialize" radio and its associated equipment into a global business. In the US some of his subsequent patented refinements (but not his original radio patent) would be overturned in a 1935 court case (upheld by the US Supreme Court in 1943).
20th century.
In 1900, Brazilian priest Roberto Landell de Moura transmitted the human voice wirelessly. According to the newspaper "Jornal do Comercio" (June 10, 1900), he conducted his first public experiment on June 3, 1900, in front of journalists and the General Consul of Great Britain, C.P. Lupton, in São Paulo, Brazil, for a distance of approximately 8 km. The points of transmission and reception were Alto de Santana and Paulista Avenue.
One year after that experiment, he received his first patent from the Brazilian government. It was described as "equipment for the purpose of phonetic transmissions through space, land and water elements at a distance with or without the use of wires." Four months later, knowing that his invention had real value, he left Brazil for the United States with the intent of patenting the machine at the US Patent Office in Washington, DC.
Having few resources, he had to rely on friends to push his project. In spite of great difficulty, three patents were awarded: "The Wave Transmitter" (October 11, 1904), which is the precursor of today's radio transceiver; "The Wireless Telephone" and the "Wireless Telegraph", both dated November 22, 1904.
The next advancement was the vacuum tube detector, invented by Westinghouse engineers. On Christmas Eve 1906, Reginald Fessenden used a synchronous rotary-spark transmitter for the first radio program broadcast, from Ocean Bluff-Brant Rock, Massachusetts. Ships at sea heard a broadcast that included Fessenden playing "O Holy Night" on the violin and reading a passage from the Bible.
In June 1912 Marconi opened the world's first purpose-built radio factory at New Street Works in Chelmsford, England.
This was, for all intents and purposes, the first transmission of what is now known as amplitude modulation or AM radio. The first radio news program was broadcast August 31, 1920 by station 8MK in Detroit, Michigan, which survives today as all-news format station WWJ under ownership of the CBS network. The first college radio station began broadcasting on October 14, 1920 from Union College, Schenectady, New York under the personal call letters of Wendell King, an African-American student at the school.
That month 2ADD (renamed WRUC in 1947), aired what is believed to be the first public entertainment broadcast in the United States, a series of Thursday night concerts initially heard within a 100 mi radius and later for a 1000 mi radius. In November 1920, it aired the first broadcast of a sporting event. At 9 pm on August 27, 1920, Sociedad Radio Argentina aired a live performance of Richard Wagner's opera "Parsifal" from the Coliseo Theater in downtown Buenos Aires. Only about twenty homes in the city had receivers to tune in this radio program. Meanwhile, regular entertainment broadcasts commenced in 1922 from the Marconi Research Centre at Writtle, England.
Sports broadcasting began at this time as well, including the college football on radio broadcast of a 1921 West Virginia vs. Pittsburgh football game.
One of the first developments in the early 20th century was that aircraft used commercial AM radio stations for navigation. This continued until the early 1960s when VOR systems became widespread. In the early 1930s, single sideband and frequency modulation were invented by amateur radio operators. By the end of the decade, they were established commercial modes. Radio was used to transmit pictures visible as television as early as the 1920s. Commercial television transmissions started in North America and Europe in the 1940s.
In 1947 AT&T commercialized the Mobile Telephone Service. From its start in St. Louis in 1946, AT&T then introduced Mobile Telephone Service to one hundred towns and highway corridors by 1948. Mobile Telephone Service was a rarity with only 5,000 customers placing about 30,000 calls each week. Because only three radio channels were available, only three customers in any given city could make mobile telephone calls at one time. Mobile Telephone Service was expensive, costing 15 USD per month, plus 0.30 to 0.40 USD per local call, equivalent to about 176 USD per month and 3.50 to 4.75 per call in 2012 USD. The Advanced Mobile Phone System analog mobile cell phone system, developed by Bell Labs, was introduced in the Americas in 1978, gave much more capacity. It was the primary analog mobile phone system in North America (and other locales) through the 1980s and into the 2000s.
In 1954, the Regency company introduced a pocket transistor radio, the TR-1, powered by a "standard 22.5 V Battery." In 1955, the newly formed Sony company introduced its first transistorized radio. It was small enough to fit in a vest pocket, powered by a small battery. It was durable, because it had no vacuum tubes to burn out. Over the next 20 years, transistors replaced tubes almost completely except for high-power transmitters.
By 1963, color television was being broadcast commercially (though not all broadcasts or programs were in color), and the first (radio) communication satellite, "Telstar", was launched. In the late 1960s, the U.S. long-distance telephone network began to convert to a digital network, employing digital radios for many of its links. In the 1970s, LORAN became the premier radio navigation system.
Soon, the U.S. Navy experimented with satellite navigation, culminating in the launch of the Global Positioning System (GPS) constellation in 1987. In the early 1990s, amateur radio experimenters began to use personal computers with audio cards to process radio signals. In 1994, the U.S. Army and DARPA launched an aggressive, successful project to construct a software-defined radio that can be programmed to be virtually any radio by changing its software program. Digital transmissions began to be applied to broadcasting in the late 1990s.
Start of the 20th century.
Around the start of the 20th century, the Slaby-Arco wireless system was developed by Adolf Slaby and Georg von Arco. In 1900, Reginald Fessenden made a weak transmission of voice over the airwaves. In 1901, Marconi conducted the first successful transatlantic experimental radio communications. In 1904, The U.S. Patent Office reversed its decision, awarding Marconi a patent for the invention of radio, possibly influenced by Marconi's financial backers in the States, who included Thomas Edison and Andrew Carnegie. This also allowed the U.S. government (among others) to avoid having to pay the royalties that were being claimed by Tesla for use of his patents. For more information see Marconi's radio work. In 1907, Marconi established the first commercial transatlantic radio communications service, between Clifden, Ireland and Glace Bay, Newfoundland.
Julio Cervera Baviera.
Julio Cervera Baviera developed radio in Spain around 1902. Cervera Baviera obtained patents in England, Germany, Belgium, and Spain. In May–June 1899, Cervera had, with the blessing of the Spanish Army, visited Marconi's radiotelegraphic installations on the English Channel, and worked to develop his own system. He began collaborating with Marconi on resolving the problem of a wireless communication system, obtaining some patents by the end of 1899. Cervera, who had worked with Marconi and his assistant George Kemp in 1899, resolved the difficulties of wireless telegraph and obtained his first patents prior to the end of that year. On March 22, 1902, Cervera founded the Spanish Wireless Telegraph and Telephone Corporation and brought to his corporation the patents he had obtained in Spain, Belgium, Germany and England. He established the second and third regular radiotelegraph service in the history of the world in 1901 and 1902 by maintaining regular transmissions between Tarifa and Ceuta for three consecutive months, and between Javea (Cabo de la Nao) and Ibiza (Cabo Pelado). This is after Marconi established the radiotelegraphic service between the Isle of Wight and Bournemouth in 1898. In 1906, Domenico Mazzotto wrote: "In Spain the Minister of War has applied the system perfected by the commander of military engineering, Julio Cervera Baviera (English patent No. 20084 (1899))." Cervera thus achieved some success in this field, but his radiotelegraphic activities ceased suddenly, the reasons for which are unclear to this day.
British Marconi.
Using various patents, the British Marconi company was established in 1897 and began communication between coast radio stations and ships at sea. This company, along with its subsidiaries Canadian Marconi and American Marconi, had a stranglehold on ship to shore communication. It operated much the way American Telephone and Telegraph operated until 1983, owning all of its equipment and refusing to communicate with non-Marconi equipped ships. In June 1912, after the RMS "Titanic" disaster, due to increased production Marconi opened the world's first purpose-built radio factory at New Street Works in Chelmsford, and in 1932 the Marconi Research Laboratory. Many inventions improved the quality of radio, and amateurs experimented with uses of radio, thus planting the first seeds of broadcasting.
Telefunken.
The company Telefunken was founded on May 27, 1903, as "Telefunken society for wireless telefon" of Siemens & Halske (S & H) and the Allgemeine Elektrizitäts-Gesellschaft ("General Electricity Company") as joint undertakings for radio engineering in Berlin. It continued as a joint venture of AEG and Siemens AG, until Siemens left in 1941. In 1911, Kaiser Wilhelm II sent Telefunken engineers to West Sayville, New York to erect three 600-foot (180-m) radio towers there. Nikola Tesla assisted in the construction. A similar station was erected in Nauen, creating the only wireless communication between North America and Europe.
Reginald Fessenden.
The invention of amplitude-modulated (AM) radio, so that more than one station can send signals (as opposed to spark-gap radio, where one transmitter covers the entire bandwidth of the spectrum) is attributed to Reginald Fessenden and Lee de Forest. On Christmas Eve 1906, Reginald Fessenden used an Alexanderson alternator and rotary spark-gap transmitter to make the first radio audio broadcast, from Brant Rock, Massachusetts. Ships at sea heard a broadcast that included Fessenden playing "O Holy Night" on the violin and reading a passage from the Bible.
Ferdinand Braun.
In 1909, Marconi and Karl Ferdinand Braun were awarded the Nobel Prize in Physics for "contributions to the development of wireless telegraphy".
Charles David Herrold.
In April 1909 Charles David Herrold, an electronics instructor in San Jose, California constructed a broadcasting station. It used spark gap technology, but modulated the carrier frequency with the human voice, and later music. The station "San Jose Calling" (there were no call letters), continued to eventually become today's KCBS in San Francisco. Herrold, the son of a Santa Clara Valley farmer, coined the terms "narrowcasting" and "broadcasting", respectively to identify transmissions destined for a single receiver such as that on board a ship, and those transmissions destined for a general audience. (The term "broadcasting" had been used in farming to define the tossing of seed in all directions.) Charles Herrold did not claim to be the first to transmit the human voice, but he claimed to be the first to conduct "broadcasting". To help the radio signal to spread in all directions, he designed some omnidirectional antennas, which he mounted on the rooftops of various buildings in San Jose. Herrold also claims to be the first broadcaster to accept advertising (he exchanged publicity for a local record store for records to play on his station), though this dubious honour usually is foisted on WEAF (1922).
In 1912, the RMS "Titanic" sank in the northern Atlantic Ocean. After this, wireless telegraphy using spark-gap transmitters quickly became universal on large ships. In 1913, the International Convention for the Safety of Life at Sea was convened and produced a treaty requiring shipboard radio stations to be manned 24 hours a day. A typical high-power spark gap was a rotating commutator with six to twelve contacts per wheel, nine inches (229 mm) to a foot wide, driven by about 2,000 volts DC. As the gaps made and broke contact, the radio wave was audible as a tone in a magnetic detector at a remote location. The telegraph key often directly made and broke the 2,000 volt supply. One side of the spark gap was directly connected to the antenna. Receivers with thermionic valves became commonplace before spark-gap transmitters were replaced by continuous wave transmitters.
Harold J. Power.
On March 8, 1916, Harold Power with his radio company American Radio and Research Company (AMRAD), broadcast the first continuous broadcast in the world from Tufts University under the call sign 1XE (it lasted 3 hours). The company later became the first to broadcast on a daily schedule, and the first to broadcast radio dance programs, university professor lectures, the weather, and bedtime stories.
Edwin Armstrong.
Inventor Edwin Howard Armstrong is credited with developing many of the features of radio as it is known today. Armstrong patented three important inventions that made today's radio possible. Regeneration, the superheterodyne circuit and wide-band frequency modulation or FM. Regeneration or the use of positive feedback greatly increased the amplitude of received radio signals to the point where they could be heard without headphones. The superhet simplified radio receivers by doing away with the need for several tuning controls. It made radios more sensitive and selective as well. FM gave listeners a static-free experience with better sound quality and fidelity than AM.
Audio broadcasting (1919 to 1950s).
Crystal sets.
The most common type of receiver before vacuum tubes was the crystal set, although some early radios used some type of amplification through electric current or battery. Inventions of the triode amplifier, motor-generator, and detector enabled audio radio. The use of amplitude modulation (AM), with which more than one station can simultaneously send signals (as opposed to spark-gap radio, where one transmitter covers the entire bandwidth of spectra) was pioneered by Fessenden and Lee de Forest.
To this day there is a small but avid base of fans of this technology who study and practice the art and science of designing and making crystal sets as a hobby; the Boy Scouts of America have often undertaken such craft projects to introduce boys to electronics and radio, and quite a number of them having grown up remain staunch fans of a radio that 'runs on nothing, forever'. As the only energy available is that gathered by the antenna system, there are inherent limitations on how much sound even an ideal set could produce, but with only moderately decent antenna systems remarkable performance is possible with a superior set.
The first vacuum tubes.
During the mid-1920s, amplifying vacuum tubes (or "thermionic valves" in the UK) revolutionized radio receivers and transmitters. John Ambrose Fleming developed an earlier tube known as an "oscillation valve" (it was a diode). Lee de Forest placed a screen, the "grid" electrode, between the filament and plate electrode, creating the triode. The Dutch company "Nederlandsche Radio-Industrie" and its owner engineer, Hanso Idzerda, made the first regular wireless broadcast for entertainment from its workshop in The Hague on 6 November 1919. The company manufactured both transmitters and receivers. Its popular program was broadcast four nights per week on AM 670 metres, until 1924 when the company ran into financial troubles.
On 27 August 1920, regular wireless broadcasts for entertainment began in Argentina, pioneered by the group around Enrique Telémaco Susini, and spark gap telegraphy stopped. On 31 August 1920 the first known radio news program was broadcast by station 8MK, the unlicensed predecessor of WWJ (AM) in Detroit, Michigan. In 1922 regular wireless broadcasts for entertainment began in the UK from the Marconi Research Centre 2MT at Writtle near Chelmsford, England. Early radios ran the entire power of the transmitter through a carbon microphone. In the 1920s, the Westinghouse company bought Lee de Forest's and Edwin Armstrong's patent. During the mid-1920s, Amplifying vacuum tubes (US)/thermionic valves (UK) revolutionized radio receivers and transmitters. Westinghouse engineers developed a more modern vacuum tube.
Political interest in the United Kingdom.
The British government and the state-owned postal services found themselves under massive pressure from the wireless industry (including telegraphy) and early radio adopters to open up to the new medium. In an internal confidential report from February 25, 1924, the "Imperial Wireless Telegraphy Committee" stated:
Licensed commercial public radio stations.
The question of the 'first' publicly targeted licensed radio station in the U.S. has more than one answer and depends on semantics. Settlement of this 'first' question may hang largely upon what constitutes 'regular' programming.
"There is the history noted above of Charles David Herrold's radio services as early as 1909 with call signs FN, SJN, 6XF, and 6XE until 1921 when it became WKQW and then finally KCBS in 1949.
Outside the United States there are also claims for the first radio stations:
Broadcasting was not yet supported by advertising or listener sponsorship. The stations owned by manufacturers and department stores were established to sell radios and those owned by newspapers to sell newspapers and express the opinions of the owners. In the 1920s, radio was first used to transmit pictures visible as television. During the early 1930s, single sideband (SSB) and frequency modulation (FM) were invented by amateur radio operators. By 1940, they were established commercial modes.
Westinghouse was brought into the patent allies group, General Electric, American Telephone and Telegraph, and Radio Corporation of America, and became a part owner of RCA. All radios made by GE and Westinghouse were sold under the RCA label 60% GE and 40% Westinghouse. ATT's Western Electric would build radio transmitters. The patent allies attempted to set up a monopoly, but they failed due to successful competition. Much to the dismay of the patent allies, several of the contracts for inventor's patents held clauses protecting "amateurs" and allowing them to use the patents. Whether the competing manufacturers were really amateurs was ignored by these competitors.
These features arose:
FM and television start.
In 1933, FM radio was patented by inventor Edwin H. Armstrong. FM uses frequency modulation of the radio wave to reduce static and interference from electrical equipment and the atmosphere. In 1937, W1XOJ, the first experimental FM radio station, was granted a construction permit by the US Federal Communications Commission (FCC). In the 1930s, regular analog television broadcasting began in some parts of Europe and North America. By the end of the decade there were roughly 25,000 all-electronic television receivers in existence worldwide, the majority of them in the UK. In the US, Armstrong's FM system was designated by the FCC to transmit and receive television sound.
FM in Europe.
After World War II, the FM radio broadcast was introduced in Germany. In 1948, a new wavelength plan was set up for Europe at a meeting in Copenhagen. Because of the recent war, Germany (which did not exist as a state and so was not invited) was only given a small number of medium-wave frequencies, which are not very good for broadcasting. For this reason Germany began broadcasting on UKW ("Ultrakurzwelle", i.e. ultra short wave, nowadays called VHF) which was not covered by the Copenhagen plan. After some amplitude modulation experience with VHF, it was realized that FM radio was a much better alternative for VHF radio than AM. Because of this history FM Radio is still referred to as "UKW Radio" in Germany. Other European nations followed a bit later, when the superior sound quality of FM and the ability to run many more local stations because of the more limited range of VHF broadcasts were realized.
Later 20th century developments.
In 1954 Regency introduced a pocket transistor radio, the TR-1, powered by a "standard 22.5V Battery". In the early 1960s, VOR systems finally became widespread for aircraft navigation; before that, aircraft used commercial AM radio stations for navigation. (AM stations are still marked on U.S. aviation charts). In 1960 Sony introduced their first transistorized radio, small enough to fit in a vest pocket, and able to be powered by a small battery. It was durable, because there were no tubes to burn out. Over the next twenty years, transistors displaced tubes almost completely except for picture tubes and very high power or very high frequency uses.
Telex on radio.
Telegraphy did not go away on radio. Instead, the degree of automation increased. On land-lines in the 1930s, teletypewriters automated encoding, and were adapted to pulse-code dialing to automate routing, a service called telex. For thirty years, telex was the absolute cheapest form of long-distance communication, because up to 25 telex channels could occupy the same bandwidth as one voice channel. For business and government, it was an advantage that telex directly produced written documents.
Telex systems were adapted to short-wave radio by sending tones over single sideband. CCITT R.44 (the most advanced pure-telex standard) incorporated character-level error detection and retransmission as well as automated encoding and routing. For many years, telex-on-radio (TOR) was the only reliable way to reach some third-world countries. TOR remains reliable, though less-expensive forms of e-mail are displacing it. Many national telecom companies historically ran nearly pure telex networks for their governments, and they ran many of these links over short wave radio.
Mobile phones.
In 1947 AT&T commercialized the Mobile Telephone Service. From its start in St. Louis in 1946, AT&T then introduced Mobile Telephone Service to one hundred towns and highway corridors by 1948. Mobile Telephone Service was a rarity with only 5,000 customers placing about 30 000 calls each week. Because only three radio channels were available, only three customers in any given city could make mobile telephone calls at one time. Mobile Telephone Service was expensive, costing 15 USD per month, plus 0.30 to 0.40 USD per local call, equivalent to about 176 USD per month and 3.50 to 4.75 per call in 2012 USD. The Advanced Mobile Phone System analog mobile cell phone system, developed by Bell Labs, was introduced in the Americas in 1978, gave much more capacity. It was the primary analog mobile phone system in North America (and other locales) through the 1980s and into the 2000s.
Legal issues with radio.
When radio was introduced in the 1920s many predicted the end of records. Radio was a free medium for the public to hear music for which they would normally pay. While some companies saw radio as a new avenue for promotion, others feared it would cut into profits from record sales and live performances. Many companies had their major stars sign agreements that they would not appear on radio.
Indeed, the music recording industry had a severe drop in profits after the introduction of the radio. For a while, it appeared as though radio was a definite threat to the record industry. Radio ownership grew from two out of five homes in 1931 to four out of five homes in 1938. Meanwhile record sales fell from $75 million in 1929 to $26 million in 1938 (with a low point of $5 million in 1933), though the economics of the situation were also affected by the Great Depression.
The copyright owners of these songs were concerned that they would see no gain from the popularity of radio and the ‘free’ music it provided. Luckily, everything they needed to make this new medium work for them already existed in previous copyright law. The copyright holder for a song had control over all public performances ‘for profit.’ The problem now was proving that the radio industry, which was just figuring out for itself how to make money from advertising and currently offered free music to anyone with a receiver, was making a profit from the songs.
The test case was against Bamberger Department Store in Newark, New Jersey in 1922. The store was broadcasting music throughout its store on the radio station WOR. No advertisements were heard, except for at the beginning of the broadcast which announced "L. Bamberger and Co., One of America's Great Stores, Newark, New Jersey." It was determined through this and previous cases (such as the lawsuit against Shanley's Restaurant) that Bamberger was using the songs for commercial gain, thus making it a public performance for profit, which meant the copyright owners were due payment.
With this ruling the American Society of Composers, Authors and Publishers (ASCAP) began collecting licensing fees from radio stations in 1923. The beginning sum was $250 for all music protected under ASCAP, but for larger stations the price soon ballooned up to $5,000. Edward Samuels reports in his book "The Illustrated Story of Copyright" that "radio and TV licensing represents the single greatest source of revenue for ASCAP and its composers […] and [a]n average member of ASCAP gets about $150–$200 per work per year, or about $5,000-$6,000 for all of a member's compositions." Not long after the Bamberger ruling, ASCAP had to once again defend their right to charge fees, in 1924. The Dill Radio Bill would have allowed radio stations to play music without paying and licensing fees to ASCAP or any other music-licensing corporations. The bill did not pass.
See also.
Histories
General
Many contributed to wireless. Individuals that helped to further the science include, among others:
Categories
References.
Primary sources.
</dl>
Secondary sources.
</dl>
External links.
</dl>

</doc>
<doc id="25632" url="http://en.wikipedia.org/wiki?curid=25632" title="Telecommunications in Réunion">
Telecommunications in Réunion

Réunion has a number of systems for communication, including telephony, Internet and radio. The island is connected to the SAFE undersea cable system.
As of 2001, there were 300,000 main-line telephones and, in 2005, there were 610,000 mobile phones. The telephone system has its centre in Saint-Denis, and the domestic telephone system uses a modern open wire and microwave relay network. The international system employs a radiotelephone system, with connections to Comoros, France and Madagascar. A new microwave relay station to Mauritius is also in use, along with one Intelsat satellite-earth station.
As of 1998, there are two AM broadcast stations and 55 FM stations, serving (as of 1997) 173,000 radios. In 1997, there were 22 television broadcast stations (and 18 low power repeaters), serving 127,000 televisions.
The Internet service providers of Réunion are: Orange, Guetali, Runnet, Moebius, and Outremer Telecom. The island is linked to the SAFE cable system for its Internet connection. The top-level domain for Réunion is "RE".

</doc>
<doc id="25665" url="http://en.wikipedia.org/wiki?curid=25665" title="Rosaceae">
Rosaceae

Rosaceae (the rose family) is a medium-sized family of flowering plants, including about 2830 species in 95 genera.
The name is derived from the type genus "Rosa". Among the most species-rich genera are "Alchemilla" (270), "Sorbus" (260), "Crataegus" (260), "Cotoneaster" (260), "Rubus" (250), and "Prunus" (plums, cherries, peaches, apricots, and almonds) with about 200 species. However, all of these numbers should be seen as estimates - much taxonomic work remains.
Rosaceae includes herbs, shrubs and trees. Most species are deciduous, but some are evergreen. They have a worldwide range, but are most diverse in the Northern Hemisphere.
Several economically important products come from the Rosaceae, including many edible fruits (such as apples, pears, quinces, apricots, plums, cherries, peaches, raspberries, loquats, and strawberries), almonds, and ornamental trees and shrubs (such as roses, meadowsweets, photinias, firethorns, rowans, and hawthorns).
Distribution.
The Rosaceae have a cosmopolitan distribution (found nearly everywhere except for Antarctica), but are primarily concentrated in the Northern Hemisphere in regions that are not desert or tropical rainforest.
Taxonomy.
The family was traditionally divided into six subfamilies: Rosoideae, Spiraeoideae, Maloideae (Pomoideae), Amygdaloideae (Prunoideae), Neuradoideae, and Chrysobalanoideae, and most of these were treated as families by various authors. More recently, Chrysobalanoideae has also been treated as a family, but also in Rosales, but is placed in Malpighiales in molecular analyses. Neuradoideae has been assigned to Malvales. Schulze-Menz, in Engler's Syllabus edited by Melchior (1964) recognized Rosoideae, Dryadoideae, Lyonothamnoideae, Spireoideae, Amygdaloideae, and Maloideae. They were primarily diagnosed by the structure of the fruits. More recent work has identified that not all of these groups were monophyletic. Hutchinson (1964) and Kalkmann (2004) recognized only tribes (17 and 21, respectively). Takhtajan (1997) delimited 10 subfamilies: Filipenduloideae, Rosoideae, Ruboideae, Potentilloideae, Coleogynoideae, Kerroideae, Amygdaloideae (Prunoideae), Spireoideae, Maloideae (Pyroideae), Dichotomanthoideae, and 21 tribes. A more modern model comprises three subfamilies, one of which (Rosoideae) has largely remained the same. A cladogram of the family is:
Three cladistic analyses were done in 1999 by Rodger Evans, one based on the phenotype, one on molecules, and the 3rd combined. The only major difference in the results with the above cladogram is the position of "Kerria", which is basal in Evans and embedded in Spireoideae in Potter et al.
While the boundaries of Rosaceae are not disputed, there is not general agreement as to how many genera into which it should be divided. Areas of divergent opinion include the treatment of "Potentilla s.l." and "Sorbus s.l.". Compounding the problem is the fact that apomixis is common in several genera. This results in an uncertainty in the number of species contained in each of these genera, due to the difficulty of dividing apomictic complexes into species. For example, "Cotoneaster" contains between 70 and 300 species, "Rosa" around 100 (including the taxonomically complex dog roses), "Sorbus" 100 to 200 species, "Crataegus" between 200 and 1,000, "Alchemilla" contains around 300 species, "Potentilla" roughly 500, and "Rubus" hundreds, or possibly even thousands of species.
Characteristics.
Rosaceae can be trees, shrubs, or herbaceous plants. The herbs are mostly perennials, but some annuals also exist.
Leaves.
The leaves are generally arranged spirally, but have an opposite arrangement in some species. They can be simple or pinnately compound (either odd- or even-pinnate). Compound leaves appear in around 30 genera. The leaf margin is most often serrate. Paired stipules are generally present, and are a primitive feature within the family, independently lost in many groups of Amygdaloideae (previously called Spiraeoideae). The stipules are sometimes adnate (attached surface to surface) to the petiole. Glands or extrafloral nectaries may be present on leaf margins or petioles. Spines may be present on the midrib of leaflets and the rachis of compound leaves.
Flowers.
Flowers of plants in the rose family are generally described as "showy". They are actinomorphic (i.e. radially symmetrical) and almost always hermaphroditic. Rosaceae generally have five sepals, five petals, and many spirally arranged stamens. The bases of the sepals, petals, and stamens are fused together to form a characteristic cup-like structure called a hypanthium. They can be arranged in racemes, spikes, or heads; solitary flowers are rare.
Fruits and seeds.
The fruits come in many varieties and were once considered the main characters for the definition of subfamilies amongst Rosaceae, giving rise to a fundamentally artificial subdivision. They can be follicles, capsules, nuts, achenes, drupes ("Prunus"), and accessory fruits, like the pome of an apple, or the hip of a rose. Many fruits of the family are edible, but their seeds often contain amygdalin, which can be converted to cyanide during digestion.
Genera.
Identified clades include:
Economic importance.
The rose family is arguably one of the six most economically important crop plant families,
and includes apples, pears, quinces, medlars, loquats, almonds, peaches, apricots, plums, cherries, strawberries, raspberries, sloes, and roses among the crop plants belonging to the family.
Many genera are also highly valued ornamental shrubs; these include "Cotoneaster", "Crataegus", "Kerria", "Photinia", "Potentilla", "Prunus", "Pyracantha", "Rhodotypos", "Rosa", "Sorbus", "Spiraea", and others.
However, several genera are also introduced noxious weeds in some parts of the world, costing money to be controlled. These invasive plants can have negative impacts on the diversity of local ecosystems once established. Such naturalised pests include "Acaena", "Cotoneaster", "Crataegus", "Pyracantha", and "Rosa".

</doc>
<doc id="25706" url="http://en.wikipedia.org/wiki?curid=25706" title="Telecommunications in Russia">
Telecommunications in Russia

Russia was among the first countries to introduce radio and television. Due to the enormous size of the country Russia leads in the number of TV broadcast stations and repeaters. There were few channels in the Soviet time, but in the past two decades many new state-run and private-owned radio stations and TV channels appeared.
The telecommunications system in Russia has undergone significant changes since the 1980s, resulting in more than 1,000 companies licensed to offer communication services today. The foundation for liberalization of broadcasting was laid by the decree signed by the President of the USSR in 1990. Telecommunication is mainly regulated through the Federal Law ""On Communications" and the Federal Law "On Mass Media""
The Soviet-time "Ministry of communications of the RSFSR" was through 1990s transformed to "Ministry for communications and informatization" and in 2004 it was renamed to "Ministry of information technologies and communications (Mininformsvyazi)", and since 2008 Ministry of Communications and Mass Media.
Russia is served by an extensive system of automatic telephone exchanges connected by modern networks of fiber-optic cable, coaxial cable, microwave radio relay, and a domestic satellite system; cellular telephone service is widely available, expanding rapidly, and includes roaming service to foreign countries. Fiber to the x infrastructure has been expanded rapidly in recent years, principally by regional players including Southern Telecom Company, SibirTelecom, ER Telecom and Golden Telecom. Collectively, these players are having a significant impact of fiber broadband in regional areas, and are enabling operators to take advantage of consumer demand for faster access and bundled services.
Early history.
Retrospectively, "networking" of "data" in the Russian language can be traced to the spread of mail and journalism in Russia, and information transfer by technical means came to Russia with the telegraph and radio (besides, an 1837 sci-fi novel "Year 4338", by the 19th-century Russian philosopher Vladimir Odoevsky, contains predictions such as "friends' houses are connected by means of magnetic telegraphs that allow people who live far from each other to talk to each other" and "household journals" "having replaced regular correspondence" with "information about the hosts’ good or bad health, family news, various thoughts and comments, small inventions, as well as invitations").
Computing systems became known in the USSR by the 1950s. Starting from 1952, works were held in the Moscow-based Institute of Precision Mechanics and Computer Engineering (headed by Sergei Lebedev) on automated missile defense system which used a "computer network" which calculated radar data on test missiles through central machine called M-40 and was interchanging information with smaller remote terminals about 100—200 kilometers distant. The scientists used several locations in the USSR for their works, the largest was a massive test range to the West from Lake Balkhash. In the meantime amateur radio users all over USSR were conducting "P2P" connections with their comrades worldwide using data codes. Later, a massive "automated data network" called "Express" was launched in 1972 to serve needs of Russian Railways.
From early 1980s the All Union Scientific Research Institute for Applied Computerized Systems ("VNIIPAS") was working to implement data connections over the X.25 telephone protocol. A test Soviet connection to Austria in 1982 existed, in 1982 and 1983 there were series of "world computer conferences" at VNIIPAS initiated by the U. N. where USSR was represented by a team of scientists from many Soviet Republics headed by biochemist Anatole Klyosov; the other participating countries were UK, USA, Canada, Sweden, FRG, GDR, Italy, Finland, Philippines, Guatemala, Japan, Thailand, Luxembourg, Denmark, Brazil and New Zealand.
Also, in 1983 the "San Francisco Moscow Teleport (SFMT)" project was started by VNIIPAS and an American team which included George Soros. It resulted in the creation in the latter 80s of the data transfer operator "SovAm" (Soviet-American) "Teleport". Meanwhile, on April 1, 1984 a Fool's Day hoax about "Kremlin computer" Kremvax was made in English-speaking Usenet. There are reports of spontaneous Internet (UUCP and telnet) connections "from home" through X.25 in the USSR in as early as 1988. In 1990 a "GlasNet" non-profit initiative by the US-based Association for Progressive Communications sponsored Internet usage in several educational projects in the USSR (through Sovam).
1998 financial crisis.
When the Russian economy’s collapse came about in August 1998, the market shrank drastically and the ruble fell several cellular operators were squeezed between low traffic and huge foreign currency denominated credits and telecommunications equipment bills. In 1998, prepaid subscriptions were made at a loss and infrastructure investments fell. NMT450 operator Moscow Cellular communications was hardest hit due to its about 50% corporate users. The 1998 crisis also caused many regional
operators tariff and payment problems with accumulated debt to vendors; large debts were restructured and foreign investors lost out.
2000s.
On November 2013 President Putin instructed Dmitry Medvedev's Cabinet in 2014-2016 to provide "modern communication services" to rural settlements throughout Russia with a population of 250 to 500 people, by Rostelecom at the expense of the provision of universal service. The document does not specify what means "modern communication services", but sources close to the Ministry of Communications and the state operator, saying that it means to connect villages to the wired Internet. The budget comes among others, from the Universal Service Fund.
Regulation.
The Ministry of Communications and Mass Media is responsible for establishing and enforcing state policy in the sphere of electronic and postal communications, for promulgating the development and introduction of new information and communication technologies, and for coordinating the work of other state agencies in this area. Legislative oversight is exercised mainly through the State Duma Committee for mass media. The Committee develops mass media-related draft laws, and provides expert analysis of laws submitted by other Duma committees regarding their compliance with current media law.
Universal Service Fund.
Universal Service Fund is a fund to finance socially important projects, for example, providing payphones in remote settlements. It consists of the contributions of all Russian operators of 1.2% of revenue. These funds are the Federal Communications Agency (Rossvyaz) distributes between 21 universal operator. These operators money comes to the budget, and Rossvâz receives from the budget for compensation and still these amounts roughly coincided, employee profile departments. But universal operators recently complained that they themselves lack the money to compensate for losses in the implementation of social projects.
On February 2014, Russian President Vladimir Putin signed amendments to the federal law "On Communications", which set Rostelecom a single operator of universal communication services. The company must commit itself to support the existing infrastructure of Universal Service, including payphones and access points (VRM) on the Internet. In addition to these duties, a single operator will also fight the digital divide by providing broadband at speeds of at least 10 Mbit / s settlements up to 250 people.
Landline telephony.
Telephones – main lines in use: 44.152 million (2011)
Telephones – mobile cellular: 236.7 million (2011)
The telephone system employs an extensive system of modern network elements such as digital telephone exchanges, mobile switching centres, media gateways and signalling gateways at the core, interconnected by a wide variety of transmission systems using fibre-optics or Microwave radio relay networks. The access network, which connects the subscriber to the core, is highly diversified with different copper-pair, optic-fibre and wireless technologies.; cellular services, both analog and digital, are available in many areas. In the rural areas, the telephone services are still outdated, inadequate, and low density.
The Tsarist government of Russia issued its first decree on the development of urban telephone networks in 1881 and, as already discussed, the first exchanges in the Empire opened the following year. Initially, telephone exchanges were granted to private developers as concessions in the major cities, but in 1884 the government began to construct the first of its own exchanges and subsequently suspended the award of new concessions. Intercity telephone communications grew very slowly, with only a dozen lines in place by the start of the 20th century, most serving Moscow-Saint Petersburg traffic. After 1900, when the initial concessions had expired, the government eased control over private concessionaires and a burst of new construction took place. Included in the expansion during this period was the slow growth of exchanges built and operated by rural "Zemstva", which were treated essentially as private concessionaires by the Imperial government.
Telephones played a significant role during the upheavals of 1917. In February, according to the last tsarist Chief of Police, 'neither the military authorities nor the mutineers thought of occupying the Telephone Exchange'; consequently it continued to function, serving both sides, until the operators finally left their
positions amidst the growing confusion. In early July, however, the Provisional Government, fearing a Bolshevik coup, reportedly ordered the central telephone exchange to boycott calls requested by Bolsheviks (automatic switching systems had not yet been introduced).
In 1918, when the Soviet government moved to Moscow and war conditions were producing extreme shortages, Sovnarkom ordered a reduction of 50% in the volume of telephone communications in the new capital, to ensure that official needs of the new government would be served. The primary consequence of this decree for individuals was the 'communalisation' of telephones in private houses and flats. According to the decree, restrictions were focused on the 'parasitic stratum' of society, in the interest of the 'working population'. With the exception of personal phones belonging to high government officials, doctors and midwives, telephones in private flats were placed at the disposal of 'house committees', to be made available for 'general use' free of charge. Houses without telephones were entitled to free use of the communal phone of a neighbouring house; the decree further ordered the immediate installation of at least 150 telephones in public squares, particularly in outlying regions.
One year later Sovnarkom nationalized all telephone systems in the Russian Republic-including all intercity, urban, concessionary and zemstvo exchangesand assigned their administration and operation to the People's Commissariat of Posts and Telegraphs. Beginning with the nationalization of telephones in 1919, Soviet policy exhibited two main characteristics: telephones increasingly became instruments for the bureaucracy and bureaucrats, and telephones in general were accorded a low investment priority. In March 1920, for instance, government institutions were exempted from the telephone tariff, receiving the right to use the telephone without payment, albeit for sharply restricted periods.
Until the end of 1991 (the end of the USSR), the sole fixed-line telephone operator in the country was the Ministry of Communications of the USSR. The state possessed all telecommunications structure and access networks. In 1994, the investment communication company (OJSC “Sviazinvest”) was established by the Presidential Decree №1989 dated 10 October 1994 “"On the specific features of the state management of the electric communication network for public use in Russian Federation"”. The authorised capital of OJSC “Sviazinvest” was formed by the consolidation of federal shares of joint stock companies acting in the area of electric communications and established during the privatisation of the state enterprises for electric communications. The seven regional incumbents which make up Svyazinvest, majority-owned by the government, in early 2011 merged with the key subsidiary Rostelecom. The move created an integrated company based on Rostelecom which will be better placed to exploit economies of scale in coming years.
Cross-country digital trunk lines run from Saint Petersburg to Vladivostok, and from Moscow to Novorossiysk.
Liberalization of the long distance communication market is another market driver. In January 2006, Russia passed a new law in relation to long distance telecommunications, which partially broke up the monopolization that Rostelecom had been enjoying in the toll market. The law now allows other carriers to operate toll services. Currently, there are about 32 active companies in this space, including Interregional TransitTelekom (MTT), Golden Telecom, TransTelekom and Synterra Media. share of fixed-line business of Rostelecom’s main competitors varied in 2012 from 6% (Megafon) to 19% (MTS). Still, At the beginning of the 2010s, Rostelecom is de facto a monopoly local telephony provider to households in Russia, except for few regions, where incumbents were not part of Svyazinvest holding after the privatization in early 1990s (the cities of Moscow, Pskov, Kostroma, the republics of Tatarstan, Bashkortostan, as well as Tuva, Chukotka, Chechnya, and Ingushetia).
The substitution of long-distance fixed-line voice services by mobile and IP traffic sped up after 2008, when mobile operators shifted to the fixed-line segment (Vimpelcom was the first company out of the Big 3 to acquire Golden Telecom in early 2008) and simultaneously increased investments into own trunk network infrastructure to support rapid 3G traffic growth. On February 2014 Megafon, through its subsidiary NetByNet purchused Tele-MIG Besides a company founded in 2003 which provides fixed telephony, IP-telephony and data transmission in Yamalo-Nenets Autonomous Okrug.
Russian regulation stipulates that new players must build their own networks. The growth of traffic between Europe and Asia is an additional opportunity; more than 6,000 km of international communication cables were built during the first nine months of 2007, representing a 48.5% increase on 2006, according to the Russian Ministry of Communication and Mass Media.
Tariffs.
Tariffs in the fixed-line segment are determined by the Federal Tariff Service on an annual basis, taking into consideration inflation and the operators' expenses. The price competition in the long-distance segment increased as mobile operators began implementing promotional tariffs to stimulate voice traffic growth after the crisis (long-distance traffic is predominantly built by corporate clients). At the same time, traditional operators had limited room for maneuver as intra-zonal and domestic LD tariffs, which are subject to regulation by the government, remained flat over the last three years. As a result, mobile operators managed to bite off a heavy share of intraregional and long-distance market from traditional fixed-line operators, first of all regional operators of Svyazinvest, which are now united under Rostelecom.
Public switched telephone network.
Russian public switched telephone network (PSTN) has specific features. The lowest part of this model is example of the local network in the middle and large cities. The central office (CO) is connected to the tandem exchange (TE). In some cases, COs are connected by the directly. Such possibility is shown by the dotted lines for three COs connected to the TEIII. COs may be directly connected with the toll exchange. This option is shown by the dotted line for the COII1. Automatic Branch Exchange (PABX) is served by the nearest CO. All TEs are forming the meshed network. Up to the 1990s, TE was independent element of the local network. Operators did not use the equipment combined functions Tandem and Toll Exchanges. So, TE provided connections between COs of the local network, and access to the toll exchange. A function of the toll exchange is to establish connections for the long-distance and international calls. Last type of calls is served by the Gateway (GW). Processing of the local calls is performed by the COs and TEs. If a subscriber dials digit "8" (prefix of the long-distance connection in the national PSTN) all further processing of the call is a function of a toll exchange. The numbering plan for the cellular networks based on the Area Code (three digits) and number of mobile terminal (seven digits). In this case, the Area Code defines the concrete cellular network.
Mobile phone.
There are three mobile phone service brands that cover all Russia: Beeline, MegaFon and Mobile TeleSystems. At the end of 2013 there were about 239 million SIM cards in use in the country, which is equal to 168% of the population. The access points (AP) are built in long-distance telephone exchanges (LDTEs), Russian fixed-line communication infrastructure which is present in every province. As a result, interconnecting mobile operator only needs to create "last kilometer" circuits to the regional LDTE, the requirement already imposed by its mobile license. Rostelecom, the leading fixed-line operator in the country has regional subsidiaries who provide cellular services.
In May 2008, 3G network was deployed in St. Petersburg, in Kazan in June of that year, and in Sochi in July of that year. By 2010, 3G networks covered largely most of Russia.
In April 2011, MegaFon deployed high-definition voice services on its Moscow and Sochi GSM and UMTS networks. As the key supplier of core and access networks to MegaFon, Nokia Siemens Networks was responsible for the HD voice implementation, which is also a world first for a commercial GSM network.
In early 2011, Rostelecom signed a memorandum of understanding with the three main MNOs to develop a joint LTE network using the infrastructure to be built by Yota. The network will expand LTE availability to 70 million Russians in 180 cities by 2014, vastly improving regional broadband availability in coming years.
In December 2011, Rostelecom signed an agreement with Yota, a Russian mobile broadband provider, to jointly develop and use 4G wireless networks. The agreement facilitated the development and expansion of advanced communications technologies in the country, including the latest 4G-LTE system. Both companies will make full use of each other's telecommunications infrastructures and advanced telecommunications services will be made more accessible to Russian residents. As part of the agreement, Rostelecom have the right to use Yota's wireless networks and to provide customers with telecommunications services as a MVNO. The agreement will also provide Rostelecom with access to Yota's existing telecommunications equipment sites and its wire communications channels at these sites. In return, Yota will use Rostelecom's wire communications channels at their telecommunication equipment sites; it will gain access to Rostelecom's Internet connection and inter-city backbone links and the company's existing telecommunication equipment sites and data centres.
On September 2012 MTS launched the country’s first TD-LTE network, using the TD-LTE spectrum in the 2595-2620 MHz band it secured in February. On May 2013 there were over one million LTE subscribers in Russia.
Radio.
Radio Rossii is the primary public radio station in Russia. Digital radio broadcasting is developing fast with the Voice of Russia announced on 1 July 2004, the successful implementation, and planned expansion, of its DRM broadcasts on short-wave and medium-wave. In September 2009 the Russian State Commission for Radio Frequencies, the national regulator of broadcasting, has decided on the DRM has the standard for mediumwave and shortwave services.
Radios: 61.5 million (1998)
Radio broadcasting stations: AM 420, FM 447, shortwave 56 (1998).
Television.
Privately owned stations are often owned by industrial groups either controlled by the State or with close connections to the government so that they can be called semi-state. Both state and private stations can have a national status (broadcasters that reach over 70% of the national territory), or a regional, district or local status. Local partners are often united in bigger networks.
In the 1970s and 1980s, television become the preeminent mass medium. In 1988 approximately 75 million households owned television sets, and an estimated 93 percent of the population watched television. Moscow, the base from which most of the television stations broadcast, transmitted some 90 percent of the country's programs, with the help of more than 350 stations and nearly 1,400 relay facilities.
There are about 15,000 TV transmitters. Development of domestic digital TV transmitters, led within "Multichannel" research program, had already been finished. New domestic digital transmitters have been developed and installed in Nizhniy Novgorod and Saint Petersburg in 2001–2002.
The state television broadcaster is Pervy kanal (Channel One)., VGTRK (channels: Rossiya 1, Rossiya 2, Rossiya K, Rossiya 24, Carousel (together with Channel One)), TV Tsentr (it is owned by the administration of the city of Moscow), Telekanal Zvezda (owner Ministry of Defence) and TV-Novosti (RT channel in English, Rusiya Al-Yaum channel in Arabic, RT America channel based in Washington, D.C. , United States in English, RT Actualidad channel in Spanish, RT Documentary channel in Russian).
Internet.
Broadband internet access is becoming more readily available in Russia, and as a result the internet is growing as an avenue for Russian commerce, with 42% of internet users in Russia shopping online, and 38% using online banking services.
IPTV.
The IPTV developing fast as a cheap alternative to regular television. On July 2011 Rostelecom started a plan to unify IPTV services in Russia's regions offering standard features such as linear and on-demand TV along with new interactive and OTT services provided by the operator to various mobile devices. For this Russian company SmartLabs was chosen.
Country code top-level domain: RU (Also SU – left from Soviet Union)
International connection.
Russia is connected internationally by three undersea fiber-optic cables; digital switches in several cities provide more than 50,000 lines for international calls; satellite earth stations provide access to Intelsat, Intersputnik, Eutelsat, Inmarsat, and Orbita. Rostelecom set up international fiber-optic communication lines providing access to Finland, Turkey, Italy, Bulgaria, Japan, China, Estonia, Latvia, Kazakhstan, Ukraine, Azerbaijan, Georgia, and Belarus. The company’s international points of presence are in Stockholm, Frankfurt, Amsterdam, and London. Russia due to its connections to Europe and Asia offers high-speed transit services from Europe to Asia via the Russian territory. international digital transit telephone network of Rostelecom is based on ten international transit and communication centers and six combined communication centers. The total installed capacity of the zonal network by the end of 2011 constituted 1,100,600 channels. The level of international communication centers digitalization constituted 100%.
In May 2006, Rostelecom launched a new fiber-optic data transmission line linking Russia's Far Eastern cities of Belogorsk and Blagoveshchensk with the Chinese city of Heihe on the Chinese-Russian border. On May 2006 TransTeleCom Company and North Korea’s Ministry of Communications have signed an agreement for the construction and joint operation of a fiber-optic transmission line (FOTL) in the section of the Khasan–Tumangang railway checkpoint. This is the first direct land link between Russia and North Korea. TTC’s partner in the design, construction, and connection of the communication line from the Korean side to the junction was Korea Communication Company of North Korea’s Ministry of Communications. The technology transfer was built around STM-1 level digital equipment with the possibility of further increasing bandwidth. The construction was completed in 2007.
In 2011 Rostelecom came to an agreement with Mongolian operator Mobicom aimed at establishing a Russia-Mongolia border-crossing transmission line and at providing telecommunications services. It also opened a new international Kaliningrad-Poland transmission line through the Poland–Russia border to optimize costs when providing services to end users and operators in Kaliningrad.
On February 2012 the national operator Rostelecom has selected TeliaSonera International Carrier to operate and manage its new backbone network between Kingisepp, Russia and Stockholm. The next-generation managed optical network provides connectivity between the cable landing points of the Baltic Cable System, Kingisepp and Kotka, implemented over TeliaSonera International Carrier's wholly owned fibre-optic infrastructure to Stockholm.
On September 2013 EPEG International Cable System, of which Russia is a member, became in commercial use. Main line connects Western Europe and the Middle East through Russia. The line, connecting Frankfurt across Eastern Europe, Russia, Azerbaijan, Iran and the Persian Gulf to the capital of the Oman, Muscat, has an initial capacity of 540 gigabits per second. The total length of the new cable system amounted to about 10,000 kilometers, and design capacity is up to 3.2 terabits per second. Vodafone organized a main line connecting Europe with Ukraine to the border with Russia. From the Russian-Ukrainian border to the border with Azerbaijan and through Azerbaijan to the borders with Iran the line was built by Rostelecom together with the Azerbaijani partner Delta Telecom.
In 2015 Transarctic Russian optical cable system (ROTAX) will be completed. The fiber optical cable a pass route from Bude (UK) through Murmansk, Anadyr and Vladivostok in Russia and finish at Tokyo. The total length of the cable system will be about 16,000 km with capacity of the system is 60 Tbit/s. The project was initiated ROTAX is JSC "Polarnet Project", and is being built by Tyco Electronic Subcom.
Fiber optical infrastructure.
In late 2012 Russia's leading telecom companies Rostelecom, MTS, Vimpelcom and Megafon signed memorandum to jointly build and operate submarine-laid fiber optic cable to connect between town of Okha on Sakhalin Island with the mainland towns of Magadan and Petropavlovsk-Kamchatsky. Capacity of the underwater cable will amount to 8 Tbit/s (80*100 Gbit/s) with the total length of lines around 2,000 km.
At the end of 2013 Rostelecom completed to deploy the Tynda - Yakutsk fiber line which according to the company provides network redundancy, optimizing traffic and increase trunk in areas Tynda - Skovorodino - Khabarovsk. The 1,056-km, 80 Gbit/s link is based on DWDM technology. Its capacity can be expanded to 3.2 Tbit/s in future. The new backbone increased the capacity of telecommunications links in Yakutsk, Aldan and Neryungri, as well as Nizhny-Bestyakh, Kachikatsy, Nizhny-Kuranakh, Bolshoy-Khatymi and Yengra.
Emergency calls.
In December 2010, then President Dmitry Medvedev signed a presidential decree enabling the implementation of a single number, 112, for emergency services in all the regions of Russia. Transition to the new emergency number will be gradual; it is envisaged that 112 will replace the previous emergency numbers 01, 02, 03 and 04 by 2017. In December 2012, Russian President Vladimir Putin signed a law establishing the single emergency service number 112 throughout the country. In a press conference on December 2013, Minister of Emergency Situations Vladimir Puchkov said that the unified system will be running in a full pilot mode from 2014 and will fully enter to operational mode in 2016.
Statistics.
Percentage (%) of enterprises using selected hardware and ICT services in Russia, 2004-2010
Key data on the telecommunications and ICT market in Russia, 2004-2011
"e - estimate"

</doc>
<doc id="25982" url="http://en.wikipedia.org/wiki?curid=25982" title="Tug of war">
Tug of war

Tug of war (also known as tug o' war, tug war, rope war, rope pulling, or tugging war) is a sport that directly puts two or more teams against each other in a test of strength: teams pull on opposite ends of a rope, with the goal being to bring the rope a certain distance in one direction against the force of the opposing team's pull.
Terminology.
The "Oxford English Dictionary" says that the phrase "tug of war" originally meant "the decisive contest; the real struggle or tussle; a severe contest for supremacy". Only in the 19th century was it used as a term for an athletic contest between two teams who haul at the opposite ends of a rope.
Origins.
The origins of tug of war are uncertain, but this sport was practised in ancient Egypt, Greece and China, where it was held in legend that the Sun and Moon played Tug of War over the light and darkness.
According to a Tang dynasty book, "The Notes of Feng", tug of war, under the name "hook pulling" (牽鉤), was used by the military commander of the State of Chu during the Spring and Autumn Period (8th century BC to 5th century BC) to train warriors. During the Tang dynasty, Emperor Xuanzong of Tang promoted large-scale tug of war games, using ropes of up to 167 m with shorter ropes attached, and more than 500 people on each end of the rope. Each side also had its own team of drummers to encourage the participants.
In ancient Greece the sport was called "helkustinda" (Greek: ἑλκυστίνδα), "efelkustinda" (ἐφελκυστίνδα) and "dielkustinda" (διελκυστίνδα), which derives from "dielkō" (διέλκω), meaning amongst others "I pull through", all deriving from the verb "helkō" (ἕλκω), "I draw, I pull". "Helkustinda" and "efelkustinda" seem to have been ordinary versions of tug of war, while "dielkustinda" had no rope, according to Julius Pollux. It is possible that the teams held hands when pulling, which would have increased difficulty, since handgrips are more difficult to sustain than a grip of a rope. Tug of war games in ancient Greece were among the most popular games used for strength and would help build strength needed for battle in full armor.
Archeological evidence shows that tug of war was also popular in India in the 12th century:
There is no specific time and place in history to define the origin of the game of Tug of War. The contest of pulling on the rope originates from ancient ceremonies and rituals. Evidence is found in countries like Egypt, India, Myanmar, New Guinea... The origin of the game in India has strong archaeological roots going back at least to the 12th century AD in the area what is today the State of Orissa on the east coast. The famous Sun Temple of Konark has a stone relief on the west wing of the structure clearly showing the game of Tug of War in progress.
Tug of war stories about heroic champions from Scandinavia and Germany circulate Western Europe where Viking warriors pull on animal skins over open pits of fire in tests of strength and endurance, in preparation for battle and plunder.
1500 and 1600 – tug of war is popularised during tournaments in French châteaux gardens and later in Great Britain
1800 – tug of war begins a new tradition among seafaring men who were required to tug on lines to adjust sails while ships were under way and even in battle.
The Mohave Indians occasionally used tug-of-war matches as means of settling disputes.
As a sport.
There are tug of war clubs in many countries, and both men and women participate.
The sport was part of the Olympic Games from 1900 until 1920, but has not been included since. The sport is part of the World Games. The Tug of War International Federation (TWIF), organises World Championships for nation teams biannually, for both indoor and outdoor contests, and a similar competition for club teams.
In England the sport is catered for by the Tug of War Association (formed in 1958), and the Tug of War Federation of Great Britain (formed in 1984). In Scotland, the Scottish Tug of War Association was formed in 1980. The sport also features in Highland Games there.
Between 1976 and 1988 Tug of War was a regular event during the television series "Battle of the Network Stars". Teams of celebrities representing each major network competed in different sporting events culminating into the final event, the Tug of War. Lou Ferrigno's in May 1979 is considered the greatest feat in 'Battle' history.
National organizations.
The sport is played almost in every country in the world. However, a small selection of countries have set up a national body to govern the sport. Most of these national bodies are associated then with the International governing body call TWIF which stands for The Tug of War International Federation. As of 2008 there are 53 countries associated with TWIF, among which are Scotland, Ireland, England, India, Switzerland, Belgium, and the United States.
Formal rules.
Two teams of eight, whose total mass must not exceed a maximum weight as determined for the class, align themselves at the end of a rope approximately 11 cm in circumference. The rope is marked with a "centre line" and two markings 4 m either side of the centre line. The teams start with the rope's centre line directly above a line marked on the ground, and once the contest (the "pull") has commenced, attempt to pull the other team such that the marking on the rope closest to their opponent crosses the centre line, or the opponents commit a foul (such as a team member sitting or falling down).
Lowering ones elbow below the knee during a 'pull' - known as 'Locking' - is a foul, as is touching the ground for extended periods of time. The rope must go under the arms; actions such as pulling the rope over the shoulders may be considered a foul. These rules apply in highly organized competitions such as the World Championships. However, in small or informal entertainment competitions, the rules are often arbitrarily interpreted and followed.
A contest may feature a moat in a neutral zone, usually of mud or softened ground, which eliminates players who cross the zone or fall into it.
Tactics.
Aside from the raw muscle power needed for tug of war, it is also a technical sport. The cooperation or "rhythm" of team members play an equally important role in victory, if not more, than their physical strength. To achieve this, a person called a "driver" is used to harmonize the team's joint traction power. He moves up and down next to his team pulling on the rope, giving orders to them when to pull and when to rest (called "hanging"). If he spots the opponents tries to pull his team away, he gives a "hang" command, each member will dig into the grass with his/her boots and movement of the rope is limited. When the opponents are played out, he shouts "pull" and rhythmically waves his hat or handkerchief for his team to pull together. Slowly but surely, the other team is forced into surrender by a runaway pull.
Injury risks.
In addition to injuries from falling and from back strains (some of which may be serious), catastrophic injuries may occur, such as finger, hand, or even arm amputations. Amputations or avulsions may result from two causes: looping or wrapping the rope around a hand or wrist, and impact from elastic recoil if the rope breaks. Amateur organizers of tugs of war may underestimate the forces generated, or overestimate the breaking strength of common ropes, and may thus be unaware of the possible consequences if a rope snaps under extreme tension. The broken ends of a rope made with a somewhat elastic polymer such as common nylon can reach high speeds, and can easily sever fingers. For this reason, specially engineered tug of war ropes exist that can safely withstand the forces generated.
Some notable tug of war accidents include:
1997 arm severing incident.
On October 25, 1997, Yang Chiung-ming and Chen Ming-kuo each had their left arms severed below the shoulder during a tug-of-war event in Taipei, Taiwan. The event, held at a park along the Keelung River in Taipei in celebration of Retrocession Day (the 52nd anniversary of the end of the Japanese colonial rule in Taiwan), involved over 1,600 participants whose combined strength exerted over 80000 kg of force on a 5 cm nylon rope that could only withstand a maximum of 26000 kg. The rope snapped, and the sheer rebounding force of the broken rope tore off the men's arms. Both men were taken to a nearby hospital where their arms were successfully reattached following several hours of microsurgery.
Bibliography.
Henning Eichberg, "Pull and tug: Towards a philosophy of the playing 'You'", in: "Bodily Democracy: Towards a Philosophy of Sport for All", London: Routledge 2010, 180-199.

</doc>
<doc id="25987" url="http://en.wikipedia.org/wiki?curid=25987" title="Rickets">
Rickets

Rickets is defective mineralization or calcification of bones before epiphyseal closure in immature mammals due to deficiency or impaired metabolism of vitamin D, phosphorus or calcium, potentially leading to fractures and deformity. Rickets is among the most frequent childhood diseases in many developing countries. The predominant cause is a vitamin D deficiency, but lack of adequate calcium in the diet may also lead to rickets (cases of severe diarrhea and vomiting may be the cause of the deficiency). Although it can occur in adults, the majority of cases occur in children suffering from severe malnutrition, usually resulting from famine or starvation during the early stages of childhood.
Osteomalacia is a similar condition occurring in adults, generally due to a deficiency of vitamin D but occurs after epiphyseal closure.
Signs and symptoms.
Signs and symptoms of rickets include:
An X-ray or radiograph of an advanced sufferer from rickets tends to present in a classic way: bow legs 
(outward curve of long bone of the legs) and a deformed chest. Changes in the skull also occur causing a distinctive "square headed" appearance (Caput Quadratum). These deformities persist into adult life if not treated. Long-term consequences include permanent bends or disfiguration of the long bones, and a curved back.
Cause.
The primary cause of rickets is a vitamin D deficiency. Vitamin D is required for proper calcium absorption from the gut. Sunlight, especially ultraviolet light, lets human skin cells convert vitamin D from an inactive to active state. In the absence of vitamin D, dietary calcium is not properly absorbed, resulting in hypocalcaemia, leading to skeletal and dental deformities and neuromuscular symptoms, e.g. hyperexcitability. Foods that contain vitamin D include butter, eggs, fish liver oils, margarine, fortified milk and juice, portabella and shiitake mushrooms, and oily fishes such as tuna, herring, and salmon. A rare X-linked dominant form exists called vitamin D-resistant rickets or X-linked hypophosphatemia.
Cases have been reported in Britain in recent years of rickets in children of many social backgrounds caused by insufficient production in the body of vitamin D because the sun's ultraviolet light was not reaching the skin due to use of strong sunblock, too much "covering up" in sunlight, or not getting out into the sun. Other cases have been reported among the children of some ethnic groups in which mothers avoid exposure to the sun for religious or cultural reasons, leading to a maternal shortage of vitamin D; and people with darker skins need more sunlight to maintain vitamin D levels. The "British Medical Journal" reported in 2010 that doctors in Newcastle on Tyne saw 20 cases of rickets per year. Rickets had been a significant malaise in London, especially during the Industrial Revolution. Persistent thick fog and heavy industrial smog permeating the city blocked out significant amounts of sunlight so much so that up to 80 percent of children at one time had varying degrees of rickets in one form or the other. Diseases causing soft bones in infants, like hypophosphatasia or hypophosphatemia can also lead to rickets.
Evolutionary considerations.
Vitamin D natural selection hypotheses:
Rickets is often a result of Vitamin D3 deficiency. The Vitamin D natural selection hypothesis suggests that Vitamin D production from sunlight is a selective force for human skin color variation. The correlation between human skin color and latitude is thought to be the result of positive selection to varying levels of solar ultraviolet radiation. Northern latitudes have selection for lighter skin that allows UV rays to produce Vitamin D from 7-dehydrocholesterol. Conversely, latitudes near the equator have selection for darker skin that can block the majority of UV radiation to protect from toxic levels of Vitamin D, as well as skin cancer.
An anecdote often cited to support this hypothesis is that Arctic populations whose skin is relatively darker for their latitude, such as the Inuit, have a diet that is historically rich in vitamin D. Since these people acquire Vitamin D through their diet, there is not a positive selective force to synthesize Vitamin D from sunlight.
Environment mismatch:
Ultimately, Vitamin D deficiency arises from a mismatch between a populations previous evolutionary environment and the individual’s current environment. This risk of mismatch increases with advances in transportation methods and increases in urban population size at high latitudes.
Similar to the environmental mismatch when dark-skinned people live at high latitudes, Rickets can also occur in religious communities that require long garments with hoods and veils. These hoods and veils act as sunlight barriers that prevent individuals from synthesizing Vitamin D naturally from the sun.
In a study by Mithal et al., Vitamin D insufficiency of various countries was measured by lower 25-hydroxyvitamin D. 25(OH)D is an indicator of vitamin D insufficiency that can be easily measured. These percentages should be regarded as relative Vitamin D levels, and not as predicting evidence for development of rickets.
Asian immigrants living in Europe have an increased risk for Vitamin D deficiency. Vitamin D insufficiency was found in 40% of non-Western immigrants in the Netherlands, and in more than 80% of Turkish and Moroccan immigrants.
Interestingly, the Middle East, despite high rates of sun-exposure, has the highest rates of rickets worldwide . This can be explained by limited sun exposure due to cultural practices and lack of vitamin D supplementation for breast-feeding women. Up to 70% and 80% of adolescent girls in Iran and Saudi Arabia, respectively, have Vitamin D insufficiency. Socioeconomic factors that limit a Vitamin D rich diet also plays a role. 
In the United States, Vitamin D insufficiency varies dramatically by ethnicity. Among males aged 70 years and older, the prevalence of low serum 25(OH) D levels was 23% for non-Hispanic whites, 45% for Mexican Americans, and 58% for non-Hispanic blacks. Among women, the prevalence was 28.5%, 55%, and 68%, respectively.
With this evolutionary perspective in mind, parents can supplement their nutritional intake with vitamin D enhanced beverages if they feel their child is at risk for Vitamin D deficiency, or, with more assurance of benefit and no cost, enable the child to spend more time with some of their skin receiving the summer sun's rays.
Diagnosis.
Rickets may be diagnosed with the help of:
Treatment and prevention.
The treatment and prevention of rickets is known as antirachitic. The most common treatment of rickets is the use of Vitamin D. However, surgery may be required to remove severe bone abnormalities.
Diet and sunlight.
Treatment involves increasing dietary intake of calcium, phosphates and vitamin D. Exposure to ultraviolet B light (most easily obtained when the sun is highest in the sky), cod liver oil, halibut-liver oil, and viosterol are all sources of vitamin D.
A sufficient amount of ultraviolet B light in sunlight each day and adequate supplies of calcium and phosphorus in the diet can prevent rickets. Darker-skinned people need to be exposed longer to the ultraviolet rays. The replacement of vitamin D has been proven to correct rickets using these methods of ultraviolet light therapy and medicine.
Recommendations are for 400 international units (IU) of vitamin D a day for infants and children. Children who do not get adequate amounts of vitamin D are at increased risk of rickets. Vitamin D is essential for allowing the body to uptake calcium for use in proper bone calcification and maintenance.
Supplementation.
Sufficient vitamin D levels can also be achieved through dietary supplementation and/or exposure to sunlight. Vitamin D3 (cholecalciferol) is the preferred form since it is more readily absorbed than vitamin D2. Most dermatologists recommend vitamin D supplementation as an alternative to unprotected ultraviolet exposure due to the increased risk of skin cancer associated with sun exposure. Endogenous production with full body exposure to sunlight is approximately 250 µg (10,000 IU) per day.
According to the American Academy of Pediatrics (AAP), all infants, including those who are exclusively breast-fed, may need Vitamin D supplementation until they start drinking at least 17 USfloz of vitamin D-fortified milk or formula a day.
Epidemiology.
In developed countries, rickets is a rare disease (incidence of less than 1 in 200,000).
Those at higher risk for developing rickets include:
Mistaken for child abuse.
Infants with rickets often suffer bone fractures. This sometimes leads to child abuse allegations. This issue appears to be more common for solely nursing infants of black mothers, in winter in temperate climates, suffering poor nutrition and no vitamin D supplementation. People with darker skin produce less vitamin D than those with lighter skin, for the same amount of sunlight.
History.
Greek physician Soranus of Ephesus, one of the chief representatives of the Methodic school of medicine who practiced in Alexandria and subsequently in Rome, reported deformation of the bones in infants as early as the first and second centuries AD. Rickets was not defined as a specific medical condition until 1645, when an English physician Daniel Whistler gave the earliest known description of the disease. In 1650 a treatise on rickets was published by Francis Glisson, a physician at Caius College, Cambridge. The origin of the word "rickets" is probably from the Old English word "wrickken" ('to twist'), although because this is conjectured, several major dictionaries simply say "origin unknown". The name "rickets" is plural in form but usually singular in construction. The Greek word "rachitis" (ῥαχίτης, meaning "in or of the spine") was later adopted as the scientific term for rickets, due chiefly to the words' similarity in sound.

</doc>
<doc id="25998" url="http://en.wikipedia.org/wiki?curid=25998" title="Radical feminism">
Radical feminism

Radical feminism is a perspective within feminism that calls for a radical reordering of society in which male supremacy is eliminated in all social and economic contexts. Radical feminists seek to abolish patriarchy by challenging existing social norms and institutions, rather than through a purely political process. This includes challenging traditional gender roles, opposing the objectification of women in media, and raising public awareness about rape and violence against women.
Early radical feminism, arising within second-wave feminism in the 1960s, typically viewed patriarchy as a "transhistorical phenomenon" prior to or deeper than other sources of oppression, "not only the oldest and most universal form of domination but the primary form" and the model for all others. Later politics derived from radical feminism ranged from cultural feminism to more syncretic politics that placed issues of class, economics, etc. on a par with patriarchy as sources of oppression. Radical feminists locate the root cause of women's oppression in patriarchal gender relations, as opposed to legal systems (as in liberal feminism) or class conflict (as in socialist feminism and Marxist feminism).
Theory and ideology.
Radical feminists assert that society is a patriarchy in which the class of men are the oppressors of the class of women. They posit that because of patriarchy, women have come to be viewed as the "other" to the male norm and as such have been systematically oppressed and marginalized; they furthermore assert that men as a class benefit from the oppression of women. Radical feminists seek to abolish patriarchy, and believe that the way to do this and to deal with oppression of any kind is to address the underlying causes of it through revolution. 
While some radical feminists propose that the oppression of women is the most fundamental form of oppression, one that cuts across boundaries of all other forms of oppression, others acknowledge the simultaneous and intersecting effect of other independent categories of oppression. These other categories of oppression may include, but are not limited to, oppression based on race, social class, perceived attractiveness, sexual orientation, and ability.
Patriarchal theory is not generally defined as a belief that all men always benefit from the oppression of all women. Rather, patriarchal theory maintains that the primary element of patriarchy is a relationship of dominance, where one party is dominant and exploits the other party for the benefit of the former. Radical feminists believe that men (as a class) use social systems and other methods of control to keep women (and non-dominant men) suppressed. Radical feminists also believe that eliminating patriarchy, and other systems which perpetuate the domination of one group over another, will liberate everyone from an unjust society.
Some radical feminists called for women to govern women and men, among them Andrea Dworkin, Phyllis Chesler, Monique Wittig (in fiction), Mary Daly, Jill Johnston, and Robin Morgan.
Redstockings co-founder Ellen Willis wrote in 1984 that radical feminists "got sexual politics recognized as a public issue," "created the vocabulary... with which the second wave of feminism entered popular culture," "sparked the drive to legalize abortion", "were the first to demand total equality in the so-called private sphere" ("housework and child care ... emotional and sexual needs"), and "created the atmosphere of urgency" that almost led to the passage of the Equal Rights Amendment. The influence of radical feminism can be seen in the adoption of these issues by the National Organization for Women (NOW), a feminist group that had previously been focused almost entirely on economic issues.
Movement.
Roots.
The ideology of radical feminism in the United States developed as a component of the women's liberation movement. It grew largely due to the influence of the civil rights movement that had gained momentum in the 1960s and many of the women who took up the cause of radical feminism had previous experience with radical protest in the struggle against racism. Chronologically, it can be seen within the context of second wave feminism that started in the early 1960s. The primary players and the pioneers of this second wave of feminism included Shulamith Firestone, Kathie Sarachild, Ti-Grace Atkinson, Carol Hanisch, and Judith Brown. Many local women's groups in the late sixties, such as the UCLA Women's Liberation Front (WLF), offered diplomatic statements of radical feminism's ideologies. UCLA's WLF co-founder Devra Weber recalls, "'... the radical feminists were opposed to patriarchy, but not necessarily capitalism. In our group at least, they opposed so-called male dominated national liberation struggles'".
These women helped secure the bridge that translated radical protest for racial equality over to the struggle for women's rights; by witnessing the discrimination and oppression to which the black population was subjected, they were able to gain strength and motivation to do the same for their fellow women. They took up the cause and advocated for a variety of women's issues, including abortion, the Equal Rights Amendment, access to credit, and equal pay. They failed to stir up enough interest among most of the women's fringe groups of society. Most women of color (who were predominantly working-class) did not participate in the formation of the radical feminist movement because it did not address many issues that were relevant to those from a working-class background. But for those who felt compelled enough to stand up for the cause, radical action was needed, and so they took to the streets and formed consciousness raising groups to rally support for the cause and recruit people who would be willing to fight for it. Later on, Second Wave radical feminism saw greater numbers of black feminists and other women of color participating.
In the 1960s, radical feminism emerged simultaneously within liberal feminist and working class feminist discussions, first in the United States, then in the United Kingdom and Australia. Those involved had gradually come to believe that it was not only the middle-class nuclear family oppressed women, but that it was also social movements and organizations that claimed to stand for human liberation, notably the counterculture, the New Left, and Marxist political parties, all of which they considered to be male-dominated and male-oriented. Women in countercultural groups related that the gender relations present in such groups were very much those of mainstream culture.
In the United States, radical feminism developed as a response to some of the perceived failings of both New Left organizations such as the Students for a Democratic Society (SDS) and feminist organizations such as NOW. Initially concentrated in big cities like New York, Chicago, Boston, Washington, DC, and on the West Coast, radical feminist groups spread across the country rapidly from 1968 to 1972.
In the United Kingdom, feminism developed out of discussions within community based radical women's organizations and discussions by women within the Trotskyist left. Radical feminism was imported into the UK by American radical feminists and seized on by British radical women as offering an exciting new theory to replace Trotskyism. As the 1970s progressed, British feminists split into two major schools of thought: socialist and radical. In 1977, another split occurred, with a third grouping calling itself "revolutionary feminism" breaking away from the other two.
Australian radical feminism developed slightly later, during an extended period of social radicalization, largely as an expression of that radicalization.
Radical feminists introduced the use of consciousness raising (CR) groups. These groups brought together intellectuals, workers, and middle class women in developed Western countries to discuss their experiences. During these discussions, women noted a shared and repressive system regardless of their political affiliation or social class. Based on these discussions, the women drew the conclusion that ending of patriarchy was the most necessary step towards a truly free society. These consciousness-raising sessions allowed early radical feminists to develop a political ideology based on common experiences women faced with male supremacy. Consciousness raising was extensively used in chapter sub-units of the National Organization for Women (NOW) during the 1970s. The feminism that emerged from these discussions stood first and foremost for the liberation of women, as women, from the oppression of men in their own lives, as well as men in power. Radical feminism claimed that a totalizing ideology and social formation—"patriarchy" (government or rule by fathers)—dominated women in the interests of men.
Within groups such as New York Radical Women (1967–1969; no relation to the present-day socialist feminist organization Radical Women), which Ellen Willis characterized as "the first women's liberation group in New York City", a radical feminist ideology began to emerge that declared that "the personal is political" and "sisterhood is powerful", formulations that arose from these consciousness-raising sessions. New York Radical Women fell apart in early 1969 in what came to be known as the "politico-feminist split" with the "politicos" seeing capitalism as the source of women's oppression, while the "feminists" saw male supremacy as "a set of material, institutionalized relations, not just bad attitudes." The feminist side of the split, which soon began referring to itself as "radical feminists", soon constituted the basis of a new organization, Redstockings. At the same time, Ti-Grace Atkinson led "a radical split-off from NOW", which became known as The Feminists. A third major stance would be articulated by the New York Radical Feminists, founded later in 1969 by Shulamith Firestone (who broke from the Redstockings) and Anne Koedt.
During this period, the movement produced "a prodigious output of leaflets, pamphlets, journals, magazine articles, newspaper and radio and TV interviews." Many important feminist works, such as Koedt's essay "The Myth of the Vaginal Orgasm" (1970) and Kate Millet's book "Sexual Politics" (1970), emerged during this time and in this milieu.
Ideology emerges and diverges.
At the beginning of this period, "heterosexuality was more or less an unchallenged assumption." Among radical feminists, the view became widely held that, thus far, the sexual freedoms gained in the sexual revolution of the 1960s, in particular, the decreasing emphasis on monogamy, had been largely gained by men at women's expense. This assumption of heterosexuality would soon be challenged by the rise of political lesbianism, closely associated with Atkinson and The Feminists. The belief that the sexual revolution was a victory of men over women would eventually lead to the women's anti-pornography movement of the late 1970s.
Redstockings and The Feminists were both radical feminist organizations, but held rather distinct views. Most members of Redstockings held to a materialist and anti-psychologistic view. They viewed men's oppression of women as ongoing and deliberate, holding individual men responsible for this oppression, viewing institutions and systems (including the family) as mere vehicles of conscious male intent, and rejecting psychologistic explanations of female submissiveness as blaming women for collaboration in their own oppression. They held to a view—which Willis would later describe as "neo-Maoist"—that it would be possible to unite all or virtually all women, as a class, to confront this oppression by personally confronting men.
The Feminists held a more idealistic, psychologistic, and utopian philosophy, with a greater emphasis on "sex roles", seeing sexism as rooted in "complementary patterns of male and female behavior". They placed more emphasis on institutions, seeing marriage, family, prostitution, and heterosexuality as all existing to perpetuate the "sex-role system". They saw all of these as institutions to be destroyed. Within the group, there were further disagreements, such as Koedt's viewing the institution of "normal" sexual intercourse as being focused mainly on male sexual or erotic pleasure, while Atkinson viewed it mainly in terms of reproduction. In contrast to the Redstockings, The Feminists generally considered genitally focused sexuality to be inherently male. Ellen Willis would later write that insofar as the Redstockings considered abandoning heterosexual activity, they saw it as a "bitter price" they "might have to pay for [their] militance", whereas The Feminists embraced separatist feminism as a strategy.
The New York Radical Feminists (NYRF) took a more psychologistic (and even biologically determinist) line. They argued that men dominated women not so much for material benefits as for the ego satisfaction intrinsic in domination. Similarly, they rejected the Redstockings view that women submitted only out of necessity or The Feminists' implicit view that they submitted out of cowardice, but instead argued that social conditioning simply led most women to accept a submissive role as "right and natural".
Action.
Radical feminism was not and is not only a movement of ideology and theory. Radical feminists also take direct action. In 1968, they protested against the Miss America pageant by throwing high heels and other feminine accoutrements into a garbage bin, to represent freedom. In 1970, they also staged a sit-in at the "Ladies' Home Journal". In addition, they held speakouts about topics such as rape.
Radical egalitarianism.
Because of their commitment to radical egalitarianism, most early radical feminist groups operated initially without any formal internal structure. When informal leadership developed, it was often resented. Many groups ended up expending more effort debating their own internal operations than dealing with external matters, seeking to "perfect a perfect society in microcosm" rather than focus on the larger world. Resentment of leadership was compounded by the view that all "class striving" was "male-identified". In the extreme, exemplified by The Feminists, the upshot, according to Ellen Willis, was "unworkable, mechanistic demands for an absolutely random division of labor, taking no account of differences in skill, experience, or even inclination". "The result," writes Willis, "was not democracy but paralysis." When The Feminists began to select randomly who could talk to the press, Ti-Grace Atkinson quit the organization she had founded.
Social organization and aims in the U.S. and Australia.
Radical feminists have generally formed small activist or community associations around either consciousness raising or concrete aims. Many radical feminists in Australia participated in a series of squats to establish various women's centers, and this form of action was common in the late 1970s and early 1980s. By the mid-1980s many of the original consciousness raising groups had dissolved, and radical feminism was more and more associated with loosely organized university collectives. Radical feminism can still be seen, particularly within student activism and among working class women.
In Australia, many feminist social organizations accepted government funding during the 1980s, and the election of a conservative government in 1996 crippled these organizations.
While radical feminists aim to dismantle patriarchal society in a historical sense, their immediate aims are generally concrete. Some common demands include:
Other nations.
The movement also arose in Israel among Jews.
Views on the sex industry.
Radical feminists have written about a wide range of issues regarding the sex industry – which they tend to oppose – including but not limited to: harm to women during the production of pornography, the social harm from consumption of pornography, the coercion and poverty that leads women to become prostitutes, the long-term effects of prostitution, the raced and classed nature of prostitution, and male dominance over women in prostitution and pornography.
Views on prostitution.
Radical feminists argue that, in most cases, prostitution is not a conscious and calculated choice. They say that most women who become prostitutes do so because they were forced or coerced by a pimp or by human trafficking, or, when it is an independent decision, it is generally the result of extreme poverty and lack of opportunity, or of serious underlying problems, such as drug addiction, past trauma (such as child sexual abuse) and other unfortunate circumstances.
Radical feminists point out that women from the lowest socioeconomic classes—impoverished women, women with a low level of education, women from the most disadvantaged racial and ethnic minorities—are overrepresented in prostitution all over the world. "If prostitution is a free choice, why are the women with the fewest choices the ones most often found doing it?" (MacKinnon, 1993). A large percentage of prostitutes polled in one study of 475 people involved in prostitution reported that they were in a difficult period of their lives and most wanted to leave the occupation.
Catharine MacKinnon argues that "In prostitution, women have sex with men they would never otherwise have sex with. The money thus acts as a form of force, not as a measure of consent. It acts like physical force does in rape."
They believe no person can be said to truly consent to their own oppression and no people should have the right to consent to the oppression of others. In the words of Kathleen Barry, consent is not a “good divining rod as to the existence of oppression, and consent to violation is a fact of oppression. Oppression cannot effectively be gauged according to the degree of “consent,” since even in slavery there was some consent, if consent is defined as inability to see, or feel any alternative.”
Andrea Dworkin stated her opinions as: "Prostitution in and of itself is an abuse of a woman's body. Those of us who say this are accused of being simple-minded. But prostitution is very simple. (...) In prostitution, no woman stays whole. It is impossible to use a human body in the way women's bodies are used in prostitution and to have a whole human being at the end of it, or in the middle of it, or close to the beginning of it. It's impossible. And no woman gets whole again later, after.”
Radical feminist thinking has analyzed prostitution as a cornerstone of patriarchal domination and sexual subjugation of women that impacts negatively not only on the women and girls in prostitution but on all women as a group because prostitution continually affirms and reinforces patriarchal definitions of women as having a primary function to serve men sexually. They claim it is crucial that society does not replace one patriarchal view on female sexuality - e.g., that women should not have sex outside marriage/a relationship and that casual sex is shameful for a woman, etc. - with another similarly oppressive and patriarchal view - acceptance of prostitution, a sexual practice which is based on a highly patriarchal construct of sexuality: that the sexual pleasure of a woman is irrelevant, that her only role during sex is to submit to the man’s sexual demands and to do what he tells her, that sex should be controlled by the man and that the woman’s response and satisfaction are irrelevant. These feminists argue that sexual liberation for women cannot be achieved as long as we normalize unequal sexual practices where a man dominates a woman.
They see prostitution as a form of male dominance, as it puts the woman in a subordinate position, reducing her to a mere instrument of sexual pleasure for the client. These feminists believe that many clients use the services of prostitutes because they enjoy the "power trip" they derive from the act and the control they have over the woman during the sexual activity. Catharine MacKinnon argues that prostitution "isn't sex only, it’s you do what I say, sex."
Radical feminists strongly object to the patriarchal ideology which has been one of the justifications for the existence of prostitution throughout history (and which they say continues to justify it in many cultures), that is, that prostitution is a "necessary evil", as men cannot control themselves, and thus it is "necessary" that a small number of women be "sacrificed" to be used and abused by men, in order to protect "chaste" women from rape and harassment. These feminists see prostitution as a form of slavery, and say that, far from decreasing rape rates, prostitution leads to a sharp "increase" in sexual violence against women, by sending the message that it is acceptable for a man to treat a woman as a sexual instrument over which he has total control. Melissa Farley argues that Nevada's high rape rate is connected to legal prostitution because Nevada is the only US state which allows legal brothels and is ranked 4th out of the 50 U.S. states for sexual assault crimes, saying, "Nevada's rape rate is higher than the U.S. average and way higher than the rape rate in California, New York and New Jersey. Why is this? Legal prostitution creates an atmosphere in this state in which women are not humans equal to them, are disrespected by men, and which then sets the stage of increased violence against women."
Indigenous women the world over are particularly targeted for prostitution. In Canada, New Zealand, Mexico, and Taiwan, studies have shown that indigenous women are at the bottom of the race and class hierarchy of prostitution, often subjected to the worst conditions, most violent demands and sold at the lowest price. It is common for indigenous women to be over-represented in prostitution when compared with their total population. This is as a result of the combined forces of colonialism, physical displacement from ancestral lands, destruction of indigenous social and cultural order, misogyny, globalization/neoliberalism, race discrimination and extremely high levels of violence perpetrated against them.
Views on pornography.
Radical feminists, notably Catherine MacKinnon, charge that the production of pornography entails physical, psychological, and/or economic coercion of the women who perform and model in it. This is said to be true even when the women are being presented as enjoying themselves. It is also argued that much of what is shown in pornography is abusive by its very nature. Gail Dines holds that pornography, exemplified by gonzo pornography, is becoming increasingly violent and that women who perform in pornography are brutalized in the process of its production.
Radical feminists point to the testimony of well known participants in pornography, such as Traci Lords and Linda Boreman, and argue that most female performers are coerced into pornography, either by somebody else, or by an unfortunate set of circumstances. The feminist anti-pornography movement was galvanized by the publication of "Ordeal", in which Linda Boreman (who under the name of "Linda Lovelace" had starred in "Deep Throat") stated that she had been beaten, raped, and pimped by her husband Chuck Traynor, and that Traynor had forced her at gunpoint to make scenes in "Deep Throat", as well as forcing her, by use of both physical violence against Boreman as well as emotional abuse and outright threats of violence, to make other pornographic films. Dworkin, MacKinnon, and Women Against Pornography issued public statements of support for Boreman, and worked with her in public appearances and speeches.
Radical feminists hold the view that pornography contributes to sexism, arguing that in pornographic performances the actresses are reduced to mere receptacles—objects—for sexual use and abuse by men. They argue that the narrative is usually formed around men's pleasure as the only goal of sexual activity, and that the women are shown in a subordinate role. Some opponents believe pornographic films tend to show women as being extremely passive, or that the acts which are performed on the women are typically abusive and solely for the pleasure of their sex partner. On-face ejaculation and anal sex are increasingly popular among men, following trends in porn. MacKinnon and Dworkin defined pornography as "the graphic sexually explicit subordination of women through pictures or words".
Radical feminists say that consumption of pornography is a cause of rape and other forms of violence against women. Robin Morgan summarizes this idea with her often-quoted statement, "Pornography is the theory, and rape is the practice."
Radical feminists charge that pornography eroticizes the domination, humiliation, and coercion of women, and reinforces sexual and cultural attitudes that are complicit in rape and sexual harassment. MacKinnon argued that pornography leads to an increase in sexual violence against women through fostering rape myths. Such rape myths include the belief that women really want to be raped and that they mean yes when they say no. Additionally, according to MacKinnon, pornography desensitizes viewers to violence against women, and this leads to a progressive need to see more violence in order to become sexually aroused, an effect she claims is well documented.
German radical feminist Alice Schwarzer is one proponent of the point of view according to which pornography gives a distorted view of men and women's bodies, as well as the actual sexual act, often showing the performers with synthetic implants or exaggerated expressions of pleasure, as well as fetishes that are not the norm, such as watersports, being presented as popular and normal.
Radical lesbian feminism.
Radical lesbians are distinguished from other radical feminists through their ideological roots in political lesbianism. Radical lesbians see lesbianism as an act of resistance against the political institution of heterosexuality, which they view as violent and oppressive towards women.
Views on transgenderism.
A dispute began in 1973, when the West Coast Lesbian Conference split over a scheduled performance by the transgender folk-singer Beth Elliott.
In 1979 Janice Raymond released the book "The Transsexual Empire", which she framed as a critique of a patriarchal medical and psychiatric establishment. Sheila Jeffreys argues that transgenderism is not immutable and thus does not warrant radical medical intervention, considers detransitioners to be evidence of this, and describes genital reassignment surgery as "mutilation". Jeffreys also argues that "the vast majority of transsexuals still subscribe to the traditional stereotype of women" and that by transitioning medically and socially, trans women are "constructing a conservative fantasy of what women should be. They are inventing an essence of womanhood which is deeply insulting and restrictive". Throughout "Gender Hurts: A Feminist Analysis of the Politics of Transgenderism" co-written with Lorene Gottschalk, Jeffreys insists on using male pronouns to refer to trans women arguing that "use by men of feminine pronouns conceals the masculine privilege bestowed upon them by virtue of having been placed in and brought up in the male sex caste". Julie Bindel said "I don't have a problem with men disposing of their genitals, but it does not make them women, in the same way that shoving a bit of vacuum hose down your 501s [jeans] does not make you a man." As of 2009 Bindel maintained that "people should question the basis of the diagnosis of male psychiatrists, at a time when gender polarisation and homophobia work hand-in-hand." She argues that "Iran carries out the highest number of sex change surgeries in the world" because "surgery is an attempt to keep gender stereotypes intact" and that "the idea that certain distinct behaviours are appropriate for males and females underlies feminist criticism of the phenomenon of 'transgenderism'."
According to The Guardian editor, her article 'Gender benders, beware' was criticized by international LGBT lobbies among local readers, receiving many letters condemning it and the Guardian for publishing it from transsexual people as well as doctors, therapists and academics. He explained that Bindel understood that there were problems with the way the article was written and agreed with one therapist claiming that the column abused an already abused minority.
Radical feminists have sometimes advocated for the exclusion of trans women from feminist events, a source of much controversy. Lisa Vogel, the Michfest event organizer claimed that protesters from Camp Trans responded to this controversy with vandalism. They argue that trans women cannot be counted as women because they were not born biologically female. Such radical feminists hold that trans women have enjoyed male privilege by virtue of being assigned male at birth and their insistence on acceptance is a type of male entitlement. Radical feminists reject the notion of a female brain. They believe that the differences in behavior between men and women are a result of different socialization and believe that - in the words of Lierre Keith - femininity is "ritualized submission". In this view, gender is less an identity than a caste position and transgenderism is an obstacle to gender abolition. These views are not widely held by feminists, are rejected by many trans women and are often labeled transphobic.
The term TERF (trans-exclusionary radical feminist) has been used by transgender people and allies to refer to radical feminists who hold such views. The term is considered a slur by those at whom it is directed, such as Elizabeth Hungerford.
Criticism.
During the early years, some radical feminists were criticized for emphasizing sex-based discrimination at the expense of race- and class-based discrimination, for being unwilling to work with men to affect change through political channels, and for reinforcing sexual essentialism (the idea that men and women are inherently different).
According to Ellen Willis' 1984 essay "Radical Feminism and Feminist Radicalism", within the New Left, radical feminists were accused of being "bourgeois", "antileft", or even "apolitical", whereas they saw themselves as further "radicalizing the left by expanding the definition of radical". Early radical feminists tended to be white and middle class. Willis hypothesized that this was, at least in part, because "most black and working-class women could not accept the abstraction of feminist issues from race and class issues"; the resulting narrow demographic base, in turn, limited the validity of generalizations based on radical feminists' personal experiences of gender relations. Many early radical feminists broke political ties with "male-dominated left groups", or would work with them only in "ad hoc" coalitions.
Also, Willis, although very much a part of early radical feminism and continuing to hold that it played a necessary role in placing feminism on the political agenda, later criticized its inability "to integrate a feminist perspective with an overall radical politics," while viewing this limitation as inevitable in the historical context of the times. In part this limitation arose from the fact that consciousness raising, as "the primary method of understanding women's condition" in the movement at this time and its "most successful organizing tool", led to an emphasis on personal experience that concealed "prior political and philosophical assumptions".

</doc>
<doc id="26023" url="http://en.wikipedia.org/wiki?curid=26023" title="RS-232">
RS-232

In telecommunications, RS-232 is a standard for serial communication transmission of data. It formally defines the signals connecting between a "DTE" ("data terminal equipment") such as a computer terminal, and a "DCE" ("data circuit-terminating equipment", originally defined as "data communication equipment"), such as a modem. The RS-232 standard is commonly used in computer serial ports. The standard defines the electrical characteristics and timing of signals, the meaning of signals, and the physical size and pinout of connectors. The current version of the standard is "TIA-232-F Interface Between Data Terminal Equipment and Data Circuit-Terminating Equipment Employing Serial Binary Data Interchange", issued in 1997.
An RS-232 serial port was once a standard feature of a personal computer, used for connections to modems, printers, mice, data storage, uninterruptible power supplies, and other peripheral devices. However, RS-232 is hampered by low transmission speed, large voltage swing, and large standard connectors. In modern personal computers, USB has displaced RS-232 from most of its peripheral interface roles. Many computers do not come equipped with RS-232 ports and must use either an external USB-to-RS-232 converter or an internal expansion card with one or more serial ports to connect to RS-232 peripherals. Nevertheless, RS-232 devices are still used, especially in industrial machines, networking equipment and scientific instruments.
Scope of the standard.
The Electronic Industries Association (EIA) standard RS-232-C as of 1969 defines:
The standard does not define such elements as the character encoding or the framing of characters, or error detection protocols. The character format and transmission bit rate are set by the serial port hardware which may also contain circuits to convert the internal logic levels to RS-232 compatible signal levels. The standard does not define bit rates for transmission, except that it says it is intended for bit rates lower than 20,000 bits per second.
History.
RS-232 was first introduced in 1962 by the "Radio Sector" of the EIA. The original DTEs were electromechanical teletypewriters, and the original DCEs were (usually) modems. When electronic terminals (smart and dumb) began to be used, they were often designed to be interchangeable with teletypewriters, and so supported RS-232. The C revision of the standard was issued in 1969 in part to accommodate the electrical characteristics of these devices.
Since the requirements of devices such as computers, printers, test instruments, POS terminals and so on were not foreseen by the standard, designers implementing an RS-232 compatible interface on their equipment often interpreted the standard idiosyncratically. The resulting common problems were non-standard pin assignment of circuits on connectors, and incorrect or missing control signals. The lack of adherence to the standards produced a thriving industry of breakout boxes, patch boxes, test equipment, books, and other aids for the connection of disparate equipment. A common deviation from the standard was to drive the signals at a reduced voltage. Some manufacturers therefore built transmitters that supplied +5 V and -5 V and labeled them as "RS-232 compatible".
Later personal computers (and other devices) started to make use of the standard so that they could connect to existing equipment. For many years, an RS-232-compatible port was a standard feature for serial communications, such as modem connections, on many computers. It remained in widespread use into the late 1990s. In personal computer peripherals, it has largely been supplanted by other interface standards, such as USB. RS-232 is still used to connect older designs of peripherals, industrial equipment (such as PLCs), console ports and special purpose equipment.
The standard has been renamed several times during its history as the sponsoring organization changed its name, and has been variously known as EIA RS-232, EIA 232, and most recently as TIA 232. The standard continued to be revised and updated by the Electronic Industries Alliance and since 1988 by the Telecommunications Industry Association (TIA). Revision C was issued in a document dated August 1969. Revision D was issued in 1986. The current revision is "TIA-232-F Interface Between Data Terminal Equipment and Data Circuit-Terminating Equipment Employing Serial Binary Data Interchange", issued in 1997. Changes since Revision C have been in timing and details intended to improve harmonization with the CCITT standard V.24, but equipment built to the current standard will interoperate with older versions.
Related ITU-T standards include V.24 (circuit identification) and V.28 (signal voltage and timing characteristics).
In revision D of EIA-232, the D-subminiature connector was formally included as part of the standard (it was only referenced in the appendix of RS 232 C). The voltage range was extended to +/- 25 volts, and the circuit capacitance limit was expressly stated as 2500 pF. Revision E of EIA 232 introduced a new, smaller, standard D-shell 26-pin "Alt A" connector, and made other changes to improve compatibility with CCITT standards V.24, V.28 and ISO 2110.
Limitations of the standard.
Because RS-232 is used beyond the original purpose of interconnecting a terminal with a modem, successor standards have been developed to address the limitations. Issues with the RS-232 standard include:
Role in modern personal computers.
In the book "PC 97 Hardware Design Guide", Microsoft deprecated support for the RS-232 compatible serial port of the original IBM PC design. Today, RS-232 has mostly been replaced in personal computers by USB for local communications. Compared with RS-232, USB is faster, uses lower voltages, and has connectors that are simpler to connect and use. However, USB is limited by standard to no more than 5 meters of cable, thus favoring RS-232 when longer distances are needed. Both standards have software support in popular operating systems.
USB is designed to make it easy for device drivers to communicate with hardware. USB is more complex than the RS-232 standard because it includes a protocol for transferring data to devices. This requires more software to support the protocol used. There is no direct analog to the terminal programs used to let users communicate directly with serial ports.
Serial ports of personal computers are also sometimes used to directly control various hardware devices, such as relays or lamps. Personal computers may use a serial port to interface to devices such as uninterruptible power supplies. In some cases, serial data is not exchanged, but the control lines are used to signal conditions such as loss of power or low battery alarms. An application program can detect or change the state of RS 232 control lines in the registers of the serial hardware using only a few input/output instructions; by contrast, a USB interface requires software to decode the serial data.
Devices that convert between USB and RS-232 do not work with all software or on all personal computers.
In fields such as laboratory automation or surveying, RS 232 devices may continue to be used. PLCs, VFDs, servo drives, and CNC equipment are programmable via RS-232. Some manufacturers have responded to this demand: Toshiba re-introduced the DE-9M connector on the Tecra laptop.
Serial ports with RS-232 are also commonly used to communicate to headless systems such as servers, where no monitor or keyboard is installed, during boot when operating system is not running yet and therefore no network connection is possible. An RS-232 serial port can communicate to some embedded systems such as routers as an alternative to network mode of monitoring.
Physical interface.
In RS-232, user data is sent as a time-series of bits. Both synchronous and asynchronous transmissions are supported by the standard. In addition to the data circuits, the standard defines a number of control circuits used to manage the connection between the DTE and DCE. Each data or control circuit only operates in one direction, that is, signaling from a DTE to the attached DCE or the reverse. Since transmit data and receive data are separate circuits, the interface can operate in a full duplex manner, supporting concurrent data flow in both directions. The standard does not define character framing within the data stream, or character encoding.
Voltage levels.
The RS-232 standard defines the voltage levels that correspond to logical one and logical zero levels for the data transmission and the control signal lines. Valid signals are either in the range of +3 to +15 volts or the range -3 to -15 volts with respect to the "Common Ground" (GND) pin; consequently, the range between -3 to +3 volts is not a valid RS-232 level. For data transmission lines (TxD, RxD and their secondary channel equivalents) logic one is defined as a negative voltage, the signal condition is called "mark". Logic zero is positive and the signal condition is termed "space". Control signals have the opposite polarity: the asserted or active state is positive voltage and the deasserted or inactive state is negative voltage. Examples of control lines include request to send (RTS), clear to send (CTS), data terminal ready (DTR), and data set ready (DSR).
The standard specifies a maximum open-circuit voltage of 25 volts: signal levels of ±5 V, ±10 V, ±12 V, and ±15 V are all commonly seen depending on the voltages available to the line driver circuit. Some RS-232 driver chips have inbuilt circuitry to produce the required voltages from a 3 or 5 volt supply. RS-232 drivers and receivers must be able to withstand indefinite short circuit to ground or to any voltage level up to ±25 volts. The slew rate, or how fast the signal changes between levels, is also controlled.
Because the voltage levels are higher than logic levels typically used by integrated circuits, special intervening driver circuits are required to translate logic levels. These also protect the device's internal circuitry from short circuits or transients that may appear on the RS-232 interface, and provide sufficient current to comply with the slew rate requirements for data transmission.
Because both ends of the RS-232 circuit depend on the ground pin being zero volts, problems will occur when connecting machinery and computers where the voltage between the ground pin on one end, and the ground pin on the other is not zero. This may also cause a hazardous ground loop. Use of a common ground limits RS-232 to applications with relatively short cables. If the two devices are far enough apart or on separate power systems, the local ground connections at either end of the cable will have differing voltages; this difference will reduce the noise margin of the signals. Balanced, differential, serial connections such as USB, RS-422 and RS-485 can tolerate larger ground voltage differences because of the differential signaling.
Unused interface signals terminated to ground will have an undefined logic state. Where it is necessary to permanently set a control signal to a defined state, it must be connected to a voltage source that asserts the logic 1 or logic 0 level, for example with a pullup resistor. Some devices provide test voltages on their interface connectors for this purpose.
Connectors.
RS-232 devices may be classified as Data Terminal Equipment (DTE) or Data Communication Equipment (DCE); this defines at each device which wires will be sending and receiving each signal. The standard recommended but did not make mandatory the D-subminiature 25-pin connector. According to the standard, male connectors have DTE pin functions, and female connectors have DCE pin functions. Other devices may have any combination of connector gender and pin definitions. Many terminals were manufactured with female connectors but were sold with a cable with male connectors at each end; the terminal with its cable satisfied the recommendations in the standard. The standard specifies 20 different signal connections. Since most devices use only a few signals, smaller connectors can often be used.
Personal computer manufacturers replaced the DB-25M connector by the smaller DE-9M connector. Different pin numbers were used for the signals (for this see serial port). This connector, with varying pinouts, became common for personal computers and related devices.
Presence of a 25-pin D-sub connector does not necessarily indicate an RS-232-C compliant interface. For example, on the original IBM PC, a male D-sub was an RS-232-C DTE port (with a non-standard current loop interface on reserved pins), but the female D-sub connector on the same PC model was used for the parallel Centronics printer port. Some personal computers put non-standard voltages or signals on some pins of their serial ports.
Cables.
The standard does not define a maximum cable length but instead defines the maximum capacitance that a compliant drive circuit must tolerate. A widely used rule of thumb indicates that cables more than 50 ft long will have too much capacitance, unless special cables are used. By using low-capacitance cables, full speed communication can be maintained over larger distances up to about 1,000 ft. For longer distances, other signal standards are better suited to maintain high speed.
Since the standard definitions are not always correctly applied, it is often necessary to consult documentation, test connections with a breakout box, or use trial and error to find a cable that works when interconnecting two devices. Connecting a fully standard-compliant DCE device and DTE device would use a cable that connects identical pin numbers in each connector (a so-called "straight cable"). "Gender changers" are available to solve gender mismatches between cables and connectors. Connecting devices with different types of connectors requires a cable that connects the corresponding pins according to the table above. Cables with 9 pins on one end and 25 on the other are common. Manufacturers of equipment with 8P8C connectors usually provide a cable with either a DB-25 or DE-9 connector (or sometimes interchangeable connectors so they can work with multiple devices). Poor-quality cables can cause false signals by crosstalk between data and control lines (such as Ring Indicator).
If a given cable will not allow a data connection, especially if a gender changer is in use, a null modem cable may be necessary. Gender changers and null modem cables are not mentioned in the standard, so there is no officially sanctioned design for them.
3-wire and 5-wire RS-232.
A minimal "3-wire" RS-232 connection consisting only of transmit data, receive data, and ground, is commonly used when the full facilities of RS-232 are not required. Even a two-wire connection (data and ground) can be used if the data flow is one way (for example, a digital postal scale that periodically sends a weight reading, or a GPS receiver that periodically sends position, if no configuration via RS-232 is necessary). When only hardware flow control is required in addition to two-way data, the RTS and CTS lines are added in a 5-wire version.
Data and control signals.
The following table lists commonly used RS-232 signals (called "circuits" in the specifications) and pin assignments. See serial port (pinouts) for non-standard variations including the popular DE-9 connector.
The signals are named from the standpoint of the DTE. The ground pin is a common return for the other connections, and establishes the "zero" voltage to which voltages on the other pins are referenced. The DB-25 connector includes a second "protective ground" on pin 1; this is connected to equipment frame ground. 
Data can be sent over a secondary channel (when implemented by the DTE and DCE devices), which is equivalent to the primary channel. Pin assignments are described in following table:
Ring indicator.
"Ring Indicator" (RI), is a signal sent from the DCE to the DTE device. It indicates to the terminal device that the phone line is ringing. In many computer serial ports, a hardware interrupt is generated when the RI signal changes state. Having support for this hardware interrupt means that a program or operating system can be informed of a change in state of the RI pin, without requiring the software to constantly "poll" the state of the pin. RI does not correspond to another signal that carries similar information the opposite way.
On an external modem the status of the Ring Indicator pin is often coupled to the "AA" (auto answer) light, which flashes if the RI signal has detected a ring. The asserted RI signal follows the ringing pattern closely, which can permit software to detect distinctive ring patterns. 
The Ring Indicator signal is used by some older uninterruptible power supplies (UPS's) to signal a power failure state to the computer.
Certain personal computers can be configured for wake-on-ring, allowing a computer that is suspended to answer a phone call.
RTS, CTS, and RTR.
The RTS and CTS signals were originally defined for use with half-duplex (one direction at a time) modems that disable their transmitters when not required, and must transmit a synchronization preamble to the receiver when they are re-enabled. 
The DTE asserts RTS to indicate a desire to transmit to the DCE, and in response the DCE asserts CTS to grant permission, once synchronization with the DCE at the far end is achieved. Such modems are no longer in common use. 
There is no corresponding signal that the DTE could use to temporarily halt incoming data from the DCE. 
Thus RS-232's use of the RTS and CTS signals, per the older versions of the standard, is asymmetric. 
This scheme is also employed in present-day RS-232 to RS-485 converters. 
RS-485 is a multiple-access bus on which only one device can transmit at a time, a concept that is not provided for in RS-232. 
The RS-232 device asserts RTS to tell the converter to take control of the RS-485 bus so that the converter, and thus the RS-232 device, can send data onto the bus.
Modern communications environments use full-duplex (both directions simultaneously) modems. In that environment, DTEs have no reason to deassert RTS. However, due to the possibility of changing line quality, delays in processing of data, etc., there is a need for symmetric, bidirectional flow control. 
A symmetric alternative providing flow control in both directions was developed and marketed in the late 1980s by various equipment manufacturers. 
It redefined the RTS signal to mean that the DTE is ready to receive data from the DCE. 
This scheme was eventually codified in version RS-232-E (actually TIA-232-E by that time) by defining a new signal, "RTR (Ready to Receive)," which is CCITT V.24 circuit 133. TIA-232-E and the corresponding international standards were updated to show that circuit 133, when implemented, shares the same pin as RTS (Request to Send), and that when 133 is in use, RTS is assumed by the DCE to be asserted at all times.
In this scheme, commonly called "RTS/CTS flow control" or "RTS/CTS handshaking" (though the technically correct name would be "RTR/CTS"),
the DTE asserts RTR to whenever it is ready to receive data from the DCE, and the DCE asserts CTS whenever it is ready to receive data from the DTE. 
Unlike the original use of RTS and CTS with half-duplex modems, these two signals operate independently from one another. 
This is an example of hardware flow control. However, "hardware flow control" in the description of the options available on an RS-232-equipped device does not always mean RTS/CTS handshaking.
Note that equipment using this protocol must be prepared to buffer some extra data, since a transmission may have begun just before the control line state change.
Seldom used features.
The EIA-232 standard specifies connections for several features that are not used in most implementations. Their use requires 25-pin connectors and cables.
Signal rate selection.
The DTE or DCE can specify use of a "high" or "low" signaling rate. The rates as well as which device will select the rate must be configured in both the DTE and DCE. The prearranged device selects the high rate by setting pin 23 to ON.
Loopback testing.
Many DCE devices have a loopback capability used for testing. When enabled, signals are echoed back to the sender rather than being sent on to the receiver. If supported, the DTE can signal the local DCE (the one it is connected to) to enter loopback mode by setting pin 18 to ON, or the remote DCE (the one the local DCE is connected to) to enter loopback mode by setting pin 21 to ON. The latter tests the communications link as well as both DCE's. When the DCE is in test mode it signals the DTE by setting pin 25 to ON.
A commonly used version of loopback testing does not involve any special capability of either end. A hardware loopback is simply a wire connecting complementary pins together in the same connector (see "loopback").
Loopback testing is often performed with a specialized DTE called a bit error rate tester (or BERT).
Timing signals.
Some synchronous devices provide a clock signal to synchronize data transmission, especially at higher data rates. Two timing signals are provided by the DCE on pins 15 and 17. Pin 15 is the transmitter clock, or send timing (ST); the DTE puts the next bit on the data line (pin 2) when this clock transitions from OFF to ON (so it is stable during the ON to OFF transition when the DCE registers the bit). Pin 17 is the receiver clock, or receive timing (RT); the DTE reads the next bit from the data line (pin 3) when this clock transitions from ON to OFF.
Alternatively, the DTE can provide a clock signal, called transmitter timing (TT), on pin 24 for transmitted data. Data is changed when the clock transitions from OFF to ON and read during the ON to OFF transition. TT can be used to overcome the issue where ST must traverse a cable of unknown length and delay, clock a bit out of the DTE after another unknown delay, and return it to the DCE over the same unknown cable delay. Since the relation between the transmitted bit and TT can be fixed in the DTE design, and since both signals traverse the same cable length, using TT eliminates the issue. TT may be generated by looping ST back with an appropriate phase change to align it with the transmitted data. ST loop back to TT lets the DTE use the DCE as the frequency reference, and correct the clock to data timing.
Synchronous clocking is required for such protocols as SDLC, HDLC, and X.25.
Secondary channel.
There is a secondary data channel, identical in capability to the first. Five signals (plus the common ground of the primary channel) comprise the secondary channel: Secondary Transmitted Data (STD), Secondary Received Data (SRD), Secondary Request To Send (SRTS), Secondary Clear To Send (SCTS), and Secondary Carrier Detect (SDCD).
Related standards.
Other serial signaling standards may not interoperate with standard-compliant RS-232 ports. For example, using the TTL levels of near +5 and 0 V puts the mark level in the undefined area of the standard. Such levels are sometimes used with NMEA 0183-compliant GPS receivers and depth finders.
A 20 mA current loop uses the absence of 20 mA current for high, and the presence of current in the loop for low; this signaling method is often used for long-distance and optically isolated links. Connection of a current-loop device to a compliant RS-232 port requires a level translator. Current-loop devices can supply voltages in excess of the withstand voltage limits of a compliant device. The original IBM PC serial port card implemented a 20 mA current-loop interface, which was never emulated by other suppliers of plug-compatible equipment.
Other serial interfaces similar to RS-232:
Development tools.
When developing or troubleshooting systems using RS-232, close examination of hardware signals can be important to find problems. A serial line analyzer is a device similar to a logic analyzer but specialized for RS-232's voltage levels, connectors, and, where used, clock signals. The serial line analyzer can collect, store, and display the data and control signals, allowing developers to view them in detail. Some simply display the signals as waveforms; more elaborate versions include the ability to decode characters in ASCII or other common codes and to interpret common protocols used over RS-232 such as SDLC, HDLC, DDCMP, and X.25. Serial line analyzers are available as standalone units, as software and interface cables for general-purpose logic analyzers and oscilloscopes, and as programs that run on common personal computers and devices.

</doc>
<doc id="26036" url="http://en.wikipedia.org/wiki?curid=26036" title="Reform Judaism">
Reform Judaism

The term Reform Judaism is today used for a confessional division within Judaism especially in North America and in the United Kingdom. The reform movement in Judaism has historically started in the 19th century in Germany by Abraham Geiger.
"Reform Judaism" used as a proper name specifically refers to two denominations, American Reform Judaism and British Reform Judaism. For historical reasons, there is a disparity between British and American terminology. British Reform Judaism is more conservative than American Reform Judaism; American Reform Judaism approximately corresponds to what in Britain is known as Liberal Judaism, and British Reform Judaism approximately corresponds to what in America is known as Conservative Judaism ("conservative" is here used relative to positions within the reform movement, and still less conservative than Orthodox Judaism).
Reconstructionist Judaism is an offshoot of Conservative Judaism.
A World Union for Progressive Judaism was formed in 1926 with the aim of providing an umbrella organization for 
the Reform, Liberal, Progressive, and Reconstructionist movements, now summarized under the term Progressive Judaism.
Reform movement in Judaism.
In general, the Reform movement in Judaism maintains that Judaism and Jewish traditions should be modernized and compatible with participation in Western culture. This means many branches of Reform Judaism hold that Jewish law should undergo a process of critical evaluation and renewal. Traditional Jewish law is therefore often interpreted as a set of general guidelines rather than as a list of restrictions whose literal observance is required of all Jews.
Along with other forms of non-orthodox Judaism, the North American Reform, 
UK Reform, UK Liberal Judaism and Israeli Progressive Movement can all trace their intellectual roots to the Reform movement in Judaism which emerged in nineteenth-century Germany. Elements of Orthodoxy developed their cohesive identity in reaction to the Reform movement in Judaism.
Although North American Reform, UK Reform, UK Liberal Judaism and Israeli Progressive Judaism all share an intellectual heritage, they occupy different positions on the non-orthodox spectrum. The North American Reform movement and UK Liberal Judaism are situated at the more radical end. The North American Conservative movement and 
Masorti Judaism occupy the more conservative end of the non-orthodox Judaisms, and are not regarded as forms of Reform Judaism at all. The UK Reform and Israeli Progressive movements come somewhere in between.
Reform Judaism in North America.
Reform Judaism and Orthodox Judaism are the two largest denominations of American Jews today. In a 2013 opinion poll, 35% of American Jews described themselves as Reform Jews (compared with 18% Conservative Judaism and 10% who identify themselves as Orthodox), but in terms of actual membership Reform Judaism with an estimated 670,000 members was roughly the same size as Orthodox Judaism in 2013. However, Reform Judaism accounts for the largest number of Jews affiliated with Progressive Judaism worldwide. It was founded by Rabbi Isaac M. Wise in Cincinnati, Ohio in the mid-1800s.
Official bodies of the Reform Movement in North America include the Union for Reform Judaism, the Central Conference of American Rabbis, and Hebrew Union College-Jewish Institute of Religion.
Reform Judaism in the UK.
UK Reform and Liberal Judaism are the two Progressive movements in the UK. For details on the relationship between the two progressive movements, see Progressive Judaism (United Kingdom).
Progressive Judaism in Israel.
After a failed attempt in the 1930s to start an Israeli movement, the World Union for Progressive Judaism tried again in the 1970s and created the movement. While calling itself "Israeli Progressive Movement" at first to stress its independence from the American counterpart, it officially changed its name to "The Reform Movement - Progressive Judaism in Israel" in 2009.

</doc>
<doc id="26041" url="http://en.wikipedia.org/wiki?curid=26041" title="RPGnet">
RPGnet

RPGnet is a role-playing game website. It includes sections on wargames, tabletop games and video games, as well as columns on gaming topics.
RPGnet was founded in 1996 by Emma and Sandy Antunes, Shawn Althouse (etrigan) and Brian David Phillips, as a way to unify a number of transient game sites. In 2001 it was purchased by Skotos Tech, but maintains creative and editorial autonomy. Currently it is being run by Shannon Appelcline of Skotos, and Allan Sugarbaker, who maintain oversight over editorial content, with Skotos managing the site's columns and programming, and Sugarbaker managing the reviews and operations. A number of forum moderators and administrators also help maintain the site.
RPGnet services.
Forums.
The forums have grown over time. "Tabletop Roleplaying Open", the general game discussion forum, has the most posts per day. There are also computer and board game forums, an "other media" forum that covers television, comic books, movies and books. The site also hosts small forums for photography, parenting, and other specific interests.
Like most large forums, RPGnet has developed numerous in-jokes, taglines, and recurring flame wars. Moderation was at one time very loose, but now follows fairly strict guidelines (see link below). Many game writers, artists, and designers post.
A wide range of tastes are present on the forums. Smaller "fringe" or indie role-playing game are particularly well represented and the latest releases often generate a great deal of discussion. Threads on Dungeons & Dragons, World of Darkness, GURPS and other popular systems are fairly common. Exalted is known for generating a particularly large number of discussion threads. 
Other websites will excerpt or reference forum posts that (much as with the Fark PhotoShop contests) have lasting value, such as
 excerpting WoW-erizing movie quotes and
 borrowing from the Demotivators thread.
Reviews.
Reviews have been an important part of RPGnet since its inception . Today, RPGnet has an active archive of approximately 7,500 reviews. Most reviews are of roleplaying games or supplements. In the last few years, users have contributed numerous reviews of board and card games. RPGnet also publishes reviews of movies, books, music albums and comics, though less frequently.
The review system was overhauled in early 2003 and since then reviews have appeared with numerous cross-references in an effort to improve navigation of the large review archive.
Currently, reviews appear on Mondays, Wednesdays, and Fridays. RPG reviews are published on Mondays and Fridays, while reviews of other products are published on Wednesdays.
Columns and articles.
RPGnet currently has approximately 20 regular columns. Columns are posted on a four-week, Monday-Friday schedule (with 3 to 4 columns posted during a typical week, as per columnist cooperation), with any "extra weeks" in the schedule filled in with additional columns, as they become available. Most columns cover gamemasters offering advice on running roleplaying games to other gamemasters, but there is some variety. This site has become noted as a source for player theory on role-playing games, and these are often written by authors with an academic background.
Notable columns have included: which promised to offer a new game every week for a year (it got to about a dozen before the initial author gave up, then another dozen before the second one did); which continues to detail the runnings of a gaming retail store; and which describes how to freelance in the gaming industry. Noteworthy columnists have included game industry veterans such as Ross Winn, Chad Underkoffler and Matt Drake. Sandy Antunes' monthly column has run without interruption since inception.
The forums include threads describing actual play of role-playing games in concrete terms. These threads include descriptions of how players have overcome specific challenges, and they allow observers to view how a role-playing game is performed without having to actually participate.
The columns software was upgraded in 2006, and it now includes full RSS feeds as well as a variety of database-oriented lookups and full integration into the RPGnet forums.
Prior to 2008, Columns Editing was handled by C.W. Richeson (2008), Shannon Appelcline (2006–2007), Michael Fiegel (2001–2005) and Sandy Antunes (inception - 2001). As of January 2008, it is handled by Shannon Appelcline.
RPGnet columns have been referenced on Slashdot (including
and ), 
as well as on many blogs and gaming sites.
Wiki.
The RPGnet wiki was added in early 2005. Its original purpose was to offer a place for people to jointly design roleplaying supplements and game systems; there has been some work on this, but others have begun to use it as a place to assemble an encyclopedia of roleplaying terms.
The RPGnet Wiki is built on MediaWiki, the same software used by Wikipedia.
News.
2005 also saw a facelift of the News & Press section of RPGnet. RPGnet now aggregates RSS feeds, with over two dozen feeds in six different gaming categories available.
Gaming Index.
In 2006, RPGnet added a Gaming Index, a catalog of RPGs added by users. This new system is intended to hold every English RPG product, and is searchable through a variety of means, notable hyperlinks to other products by the publisher, authors or game line and links to RPGnet's reviews of the product. Users can also rank the products and comment on them. As of March 14, 2011 the site has 15786 games, 2473 additional editions, and 2014 magazines, accounting for 1049 unique game systems.
The Gaming Index is not searchable on RPG.net itself, as RPG.net does not maintain a working "search" function.
Other features.
RPGnet features a , which is a RPGnet-branded version of . The site also offers a membership program which gives subscribers early access to reviews, a few forum privileges, and online access to some Days of Wonder games.
The Game Manufacturers Association (GAMA) publishes a semi-annual journal called "Games and Education". As of 1998, past issues of this journal are archived on the RPGnet site.

</doc>
<doc id="26055" url="http://en.wikipedia.org/wiki?curid=26055" title="Radical Dreamers">
Radical Dreamers

Radical Dreamers: Nusumenai Hōseki (Japanese: ラジカル・ドリーマーズ -盗めない宝石-, lit. "Radical Dreamers: The Unstealable Jewel") is a Japanese video game produced by Square (now Square Enix) in 1996 for the Satellaview add-on for the Nintendo Super Famicom. It is a text-based visual novel in which the player takes the role of Serge, a young adventurer accompanied by Kid, a teen-aged thief, and Gil, a mysterious masked magician.
The game belongs to the "Chrono" series and is a "gaiden", or side story, to the 1995 game "Chrono Trigger". It was released to complement its predecessor's plot, and later served as inspiration for "Chrono Cross". It features text-based gameplay with minimal graphics and sound effects, and was scored by composer Yasunori Mitsuda. Unlike many Satellaview titles, "Radical Dreamers" was not designed to lock after a certain number of play-throughs, so players owning an 8M Memory Pack onto which the game was downloaded can still play today.
"Radical Dreamers" and other Satellaview titles were planned to be released at the Akihabara electronics district of Tokyo. Square also tried to integrate it into the Japanese PlayStation port of "Chrono Trigger" as an Easter egg. Writer and director Masato Kato halted both releases, unhappy with the quality of his work. Though the game was never officially released abroad, ROM hackers completed an English fan translation in 2003.
Gameplay.
Gameplay consists of text-based scenarios presented to the player through the narration of the main character, Serge. As the narrative progresses, the game presents a list of possible actions and the player must choose his or her course. Depending on the choices made, the player may enter a new area, be presented with a new situation or character, or have to choose again if the previous selection was incorrect. In combat with enemies, the player must select from options such as "Fight", "Magic", "Run", and often more complex situational commands like "Run my knife into the goblin's chest!" or "Quickly slash at its hand!". Some decisions must be made before an invisible timer runs out; in combat, hesitation results in injury or death. Serge's health is tracked by an invisible point count, restored by various events (such as finding a potion). The game also tracks Kid's affection for Serge, influenced by battles and scripted events. Her feelings determine whether Serge survives the story's climactic fight.
"Radical Dreamers" features minimal graphics and animation; most areas are rendered with dim, static backgrounds. The game also uses atmospheric music and sounds. Like other "Chrono" games, "Radical Dreamers" contains a variant of New Game + mode. Only one scenario is available on the first play-through; after finishing it and obtaining one of three possible endings, players can explore six others. These later stories often feature comical situations or allusions to "Chrono Trigger".
Characters and story.
"Radical Dreamers" features three protagonists—Serge, Kid, and Magil—who seek out treasure as venturesome, reputable thieves. The young adult narrator, Serge, is a drifting musician who met Kid by chance three years ago in a remote town. Serge enjoys adventure with a carefree attitude. Kid, only sixteen years old, is a renowned professional thief with a reputation for boisterous behavior. Possessing a turbulent history, Kid dubiously fancies herself as a kind of Robin Hood. Magil is an enigmatic, handsome masked man skilled in magic who rarely speaks and can fade into shadow at will. Crowned by flowing, blue hair, Magil accompanied Kid well before Serge joined the group. They seek the Frozen Flame, a mythic artifact capable of granting any wish. It is hidden in Viper Manor—the home of a terrible and powerful aristocrat named Lynx, who gained control of the estate after usurping power from and killing the Acacia Dragoons, a familial unit of warriors.
Following Kid, the group infiltrates Viper Manor on the night of a full moon. While sneaking through the corridors, they battle goblins and other creatures of legend while unraveling the history of the manor and its occupants. Magil explains that the Frozen Flame is a fragment of the massive, extraterrestrial creature known as Lavos, splintered off when Lavos impacted the planet in prehistory and burrowed to its core. The thieves locate Lynx and the Frozen Flame deep within an underground ruin of the Kingdom of Zeal—an ancient, airborne civilization destroyed after it awakened Lavos in search of immortality. Serge discovers that Kid is an orphan, hoping to exact revenge upon Lynx for killing her caretaker, Lucca. Kid attempted to find Lynx in her childhood after Lucca's death, but was stopped and saved from certain defeat by Magil, who accompanied her thereafter.
The trio battle Lynx for the Frozen Flame, and Lynx gains the upper hand after trapping Magil with a powerful spell. He plans to acquire Kid's special gift from Lucca—a Time Egg, or Chrono Trigger. With a Time Egg and the Frozen Flame, Lynx boasts that he shall achieve control over time. Kid lunges at him, but Lynx easily parries her attack and wounds her. She desperately removes the Chrono Trigger from her back pocket. The Trigger shatters and causes a localized temporal distortion, leading Serge to see various scenes in history. Kid learns of her heritage as princess Schala of Zeal, a meek girl who was coerced to help awaken Lavos with her magical power. As Zeal collapsed, Schala was wracked with anguish and guilt for her role in the incident. Nearby in the Ocean Palace, the Frozen Flame felt her grief and changed her to a baby, sending her to the modern era where Lucca found her. It is also circumstantially revealed that Magil is in fact Magus, Schala's wayward brother who searched for her after battling Lavos in "Chrono Trigger". Once the distortion subsides, an army from Porre—a large nation in search of the Frozen Flame—storms the mansion. Lynx withdraws as Kid, Serge, and Magil flee. Kid tells Serge that she is aware of her true origin, and knowing that is a treasure which cannot be stolen. She bids him goodbye before disappearing into the darkness with Magil.
Other scenarios are available after players complete the first. These include both humorous and serious variations of the main plot.
Music.
The music of "Radical Dreamers" was written by composer Yasunori Mitsuda, the artist who scored "Chrono Trigger" and "Chrono Cross". The soundtrack includes several ambient pieces, including the sound of water running in a fountain and wind accompanied by strings. Players can listen to the game's songs by accessing a hidden menu in the "Gil: Caught Between Love and Adventure" scenario. The tracks were never officially named by Square, though Demiforce provided names for the "Radical Dreamers" fan translation. They include:
Several themes and musical patterns were later adapted for "Chrono Cross" on the suggestion of director Masato Kato; many appear unchanged except for new instrumentation. Appearing in "Chrono Cross" are "Gale", "Frozen Flame", "Viper Manor", "Far Promise ~ Dream Shore" (as part of "On the Beach of Dreams ~ Another World", "The Dream that Time Dreams"), "The Girl who Stole the Stars", and "Epilogue ~ Dream Shore" (as part of "Jellyfish Sea"). Mitsuda also titled the game's ending song "Radical Dreamers -Le Trésor Interdit-".
Development.
Masato Kato wrote "Radical Dreamers" after "Chrono Trigger"'s release, feeling that "Trigger" concluded with "unfinished business". He composed the main story and drafted the concepts for the sub-scenarios, leaving them to be completed by his peers. He allowed Makoto Shimamoto to write the entire "Kid and the Sunflower" segment, later joking that he "avoided having any part in that episode," while Miwa Shoda was in charge of the "Shadow Realm and the Goddess of Death" segment. According to scenario writer Daisuke Fukugawa (responsible for the game's "The Enigmatic Gigaweapon: Paradise X" subplot), the game's graphical content pushed the Satellaview's technical limits, requiring developers to redraw prerendered models until functional gameplay could be ensured. Kato remarked that his "savage feelings" from "Chrono Trigger"'s hectic development manifested in Kid's "unusually nihilistic attitude". He explained his approach towards the project in 1999:
Kato's team completed "Radical Dreamers" in only three months under a rushed production schedule, prompting him to label the game "unfinished" in an interview for the "Ultimania" "Chrono Cross" guide. Kato regretted that the schedule hampered the quality of his work, and explained that the connections to "Chrono Trigger" were evoked towards the end of the project:
Fan translation and notability.
In April 2003, the ROM hacking group Demiforce released a fan translation rendering "Radical Dreamers" in English. The patch works by modifying the ROM image of "Dreamers" used for playing console-based video games on personal computers through emulation. The ability to save games was not enabled with the first patch, and some minor typos were left in, later remedied by successive releases. On Christmas Day 2005, Demiforce and Radical R released the final version (1.4) of the translation, which fixed remaining minor bugs. The French team Terminus Traduction made a French translation patch soon after. Masato Kato did not perceive significant demand to include "Radical Dreamers" as a bonus with the release of "Chrono Trigger DS", and omitted it to preserve continuity between "Trigger" and "Cross". He expressed concern in 2009 over re-releasing "Radical Dreamers" "as-is", citing a need to revise the work.
A reviewer for Home of the Underdogs lauded the game's excellent writing and the "superb" English translation patch, noting that the "interesting plot" would appeal to fantasy fans if they could stomach the limited interactivity. Having never played a "Chrono" game prior, the reviewer stated, "I was still able to follow the story and be drawn into the world of colorful characters." While praising the replay value afforded by the extra scenarios, the critic derided the random battles of "Radical Dreamers", writing that "RPG-style random combat doesn't translate well to [a] text-only medium." The website awarded "Dreamers" "Top Dog" status, and the game maintains a voter score of 8.95 out of 10.
"Radical Dreamers" preceded "Chrono Cross", a full role-playing video game sequel to "Chrono Trigger". Masato Kato cited the desire to "redo "Radical Dreamers" properly" as the genesis of "Cross", attributing the latter's serious atmosphere to the influence of "Dreamers". Kato's desire to finish the story of the characters Kid and Serge principally shaped the plot of "Cross". "Chrono Cross" borrowed certain thematic elements, story points, characters, music, and objects introduced in "Radical Dreamers"—including the infiltration of Viper Manor, the Frozen Flame, the name "Radical Dreamers" for Kid's thievery, and the characters of Kid, Lynx, and Serge (who became a non-speaking protagonist). Though these characters and items were not presented in the same context, their general traits survived the transition. Gil, confirmed by Kato to be Magus, was also going to be featured in "Chrono Cross". This idea was scrapped due to difficulties in representing the story of Magus among the game's numerous other characters; the unrelated, enigmatic magician Guile was created instead. Since the release of "Chrono Cross", "Radical Dreamers" is considered an alternate continuity of the "Chrono" series. "Chrono Cross" addressed this through an easter egg hinting that "Radical Dreamers" took place in a different dimension. In the English version of "Chrono Cross", this easter egg refers to Gil as "Magil". Kato intended "Dreamers" and "Cross" to prompt players to pursue their personal dreams in life. A new "Chrono" series game has not been made due to the difficulty of reuniting the splintered "Cross" development team, some of which continue to work on "Final Fantasy XI" and "Final Fantasy XIV", while others left Square Enix (then Square) after the completion of "Chrono Cross" to form Monolith Soft. Composer Yasunori Mitsuda stressed that "there are a lot of politics involved" in the creation of a new game, and that Masato Kato should participate in development.

</doc>
<doc id="26079" url="http://en.wikipedia.org/wiki?curid=26079" title="Revolutionary Association of the Women of Afghanistan">
Revolutionary Association of the Women of Afghanistan

The Revolutionary Association of the Women of Afghanistan (RAWA) (Persian:جمعیت انقلابی زنان افغانستان, "Jamiyat-e Enqelābi-ye Zanān-e Afghānestān", Pashto:د افغانستان د ښڅو انقلابی جمعیت) is a women's organization based in Quetta, Pakistan, that promotes women's rights and secular democracy. It was founded in 1977 by Meena Keshwar Kamal, an Afghan student activist who was assassinated in February 1987 for her political activities. The group, which supports non-violent strategies, had its initial office in Kabul, Afghanistan, but then moved to Pakistan in the early 1980s.
The organization aims to involve women of Afghanistan in both political and social activities aimed at acquiring human rights for women and continuing the struggle against the government of Afghanistan based on democratic and secular, not fundamentalist principles, in which women can participate fully. RAWA also strives for multilateral disarmament.
The group opposed the Soviet-supported government, the following Mujahideen and Islamist governments, and the present United States-supported Islamic Republican form of government.
Background.
The RAWA was first initiated in Kabul in 1977 as an independent social and political organization of Afghan women fighting for human rights and social justice. The organization then moved parts of its work out of Afghanistan into Pakistan and established their main base there to work for Afghan women.
Founder.
RAWA was founded by a group of Afghan women led by Meena Keshwar Kamal. At age 21, she laid the foundations of RAWA through her work educating women. In 1979, Kamal began a campaign against Soviet forces and the Soviet-supported government of Afghanistan. In 1981, she launched a bilingual magazine called Payam-e-Zan (Women’s Message). In the same year, she visited France for the French Socialist Party Congress. She also established schools for Afghan refugee children, hospitals and handicraft centers for refugee women in Pakistan. Her activities and views, as well as her work against the government and religious fundamentalists led to her assassination on February 4, 1987.
Early activities.
Much of RAWA's efforts in the 1990s involved holding seminars and press conferences and other fund-raising activities in Pakistan. RAWA also created secret schools, orphanages, nursing courses, and handicraft centers for women and girls in Pakistan and Afghanistan. They secretly filmed women being beaten in the street in Afghanistan by the religious police, and being executed. RAWA activities were forbidden by both the Taliban and the Northern Alliance, but they persisted, and even publicised their work in publications like Payam-e-Zan.
RAWA after the 2001 invasion.
RAWA is highly critical of the NATO war that began in 2001, because of the high rate of casualties among the civilian population. The organization went so far as to threaten to sue United States government for unauthorized use of four photos from their website that were used in propaganda handbills dropped on various cities in Afghanistan during the 2001 invasion.
After the defeat of the Taliban government by US and Afghan Northern Alliance forces, RAWA warned that the Northern Alliance were just as fundamentalist and dangerous as the Taliban. They continue to charge that the current government led by President Hamid Karzai lacks support in most areas of Afghanistan, and that fundamentalists are enforcing laws unfairly treating women as they were under the Taliban. These claims are supported by media reports about the Herat government of Ismail Khan, who has created a religious police that forces women to obey strict dress and behavior codes, as well as many reports by Human Rights Watch.
One recent report released by Human Rights Watch in 2012 describes a situation where women are punished by the judicial system for attempting to escape from domestic abuse and also occasionally for being victims of rape. It says that Karzai is "[u]nwilling or unable to take a consistent line against conservative forces within the country," and that the lack of improvement in the plight of women in Afghanistan after ten years is "shocking."
Recent activities.
RAWA collects funds to support hospitals, schools and orphanages and still run many projects in Pakistan and Afghanistan, including a project in conjunction with CharityHelp.org for orphan sponsorships.
Recently RAWA restarted its mission inside Afghanistan and organized some of its events in Kabul. They've held events annually on International Women's Day since 2006.
On September 27, 2006, a RAWA member for the first time (perhaps in the whole history of RAWA) appeared on a round table debate on a local Afghan TV channel, TOLO TV. She had a debate with a representative of a hard line Islamic fundamentalist group. She named the top leaders of the Islamist groups and termed them "war criminal and responsible for the ongoing tragedy in Afghanistan". Tolo TV censored the audio of any sections where names were called.
On October 7, the Afghan Women's Mission (AWM) organized a fund raising event for RAWA in Los Angeles, California. Eve Ensler was the chief guest and Sonali Kolhatkar and Zoya, a member of RAWA, were among the speakers. "Zoya" is a pseudonym for an active member of RAWA's Foreign Committee who has traveled to many countries, including the U.S. and Spain as well as Germany. In 2003, she received international acclaim for her biography "Zoya’s Story - An Afghan Woman’s Battle for Freedom."
In June 2008 Zoya testified to the Human Rights Commission of the German Parliament (Bundestag) to persuade the German government to withdraw its troops from Afghanistan.
In 2009, RAWA and other women's rights groups strongly condemned a "Shia Family Code" which is claimed to legalise spousal rape within Northern Afghan Shia Muslim communities, as well as endorsing child marriage, purdah (seclusion) for married women, which was passed by President Hamid Karzai to garner support for his coalition government from hardline elements within the aforesaid communities, as well as the neighbouring Shia-dominated Islamic Republic of Iran. In addition to the above, the new "Family Code" also enshrines discriminatory legal status in the context of inheritance and divorce against women.
In February 2012, the group commemorated the 25th anniversary of the death of RAWA founder Meena Keshwar Kamal with a gathering of women in Kabul.
Recognition.
RAWA has so far won 16 awards and certificates from around the world for its work for human rights and democracy. They include the sixth Asian Human Rights Award - 2001, the French Republic's Liberty, Equality, Fraternity Human Rights Prize, 2000, Emma Humphreys Memorial Prize 2001,
Glamour Women of the Year 2001, 2001 SAIS-Novartis International Journalism Award from Johns Hopkins University, Certificate of Special Congressional Recognition from the U.S. Congress, 2004, Honorary Doctorate from University of Antwerp (Belgium) for outstanding non-academic achievements, and many other awards.
What others say about RAWA.
In the book "With All Our Strength: The Revolutionary Association of the Women of Afghanistan"
by Anne Brodsky, a number of world-known writers and human rights activists share their views of RAWA. They include Arundhati Roy who says "Each of us needs a little RAWA"; Eve Ensler, author of "The Vagina Monologues", who suggests that RAWA must stand as a model for every group working to end violence; Katha Pollitt, author of "Subject to Debate: Sense and Dissents on Women, Politics, and Culture"; Ahmed Rashid, author of "Taliban and Jihad"; and Asma Jahangir, Special Rapporteur of the United Nations and prominent women's rights activist of Pakistan are two Pakistanis who write about RAWA and express their support.

</doc>
<doc id="26082" url="http://en.wikipedia.org/wiki?curid=26082" title="Rock Bridge High School">
Rock Bridge High School

Rock Bridge High School is a public high school located in southern Columbia, Missouri. The school serves grades 9 through 12, and is a part of the Columbia Public Schools.
History.
Due to the increasing population of Columbia in the 70's, and the crowding of David H. Hickman High School towards the end of the 1960s, the Columbia Board of Education decided to form a new high school. The board bought 42 acre of land in Southern Columbia and started the construction of the new high school. The name "Rock Bridge" was chosen because of the schools proximity to the natural rock bridge of Rock Bridge State Park. In September 1973, Rock Bridge first opened and had a class of 583 students. The original portion of RBHS had 20 classrooms in the present-day east wing, the main gymnasium, the present-day cafeteria (built a library), the planetarium, and main offices, which was soon expanded in 1978 with the addition of the west wing for a then-total of about 40 classrooms. The design of the school at this point won a national award in school design. Many of the original classrooms were connected with motorized folding walls. The school is centrally air conditioned. Three new science classrooms, as well as a performing arts center, were added in 1992. Eight years later, in 2000, a large addition opened between the east and west wings, featuring seven science classrooms, eight English and Social Studies classrooms, seven foreign language classrooms, a new Media Center, and three new computer labs, (two of which were converted to science classrooms in 2013). The new facilities allowed the school to repurpose several areas, including the cafeteria and library. As of 2010-2011, the school had Wi-Fi throughout the media center, commons, and main hallways, with the entire building wired in the summer of 2012. In 2011, the school's library received a set of 30 laptops for teachers to reserve for their class. When the students need the laptops, the teacher sends the students to the library, where each student will check out a laptop under their name as an hourly checkout. By 2013, the library had received nine more carts (plus a set of 30 iPads). Each science classroom also has 15 laptops. In January 2013 Rock Bridge opened a new auxiliary gym due to the expected ninth graders to begin attending high school following the secondary redistricting in August. The area under the auxiliary gym featured a new wrestling room, making room for three new math classrooms in the former location. A new weight room was also added. The library received yet another cart of 30 laptops for the 2014-15 school year, plus a set of 30 iPads in every math classroom.
Structure.
Rock Bridge runs on a block scheduling format during the hours of 8:55 AM to 4:05 PM. This format is structured so that students have four 95-minute-long classes each day (86 minutes on Wednesday and Thursday). However, most of these classes meet every other day for a total of eight classes for the year. Block scheduling was established in the 1994-95 school year.
AUT.
AUT, short for Alternating Unassigned Time, is an unassigned time that is offered to juniors and seniors. It is like a study hall in the fact that students have no class that block and can use it for academic purpose. However, unlike a study hall, students do not have to report to nor work in a specific location. Rather, students on AUT are allowed to roam the building to access tutoring, use the computers in the Media Center, visit with friends, or anything else they wish to do. Furthermore, students are allowed to leave campus so long as they return to their next class on time and have parental permission to leave in the first place. Freshmen and Sophomores are given AUT privileges as a part of the Advisory Class, provided they check in with their Advisory Teacher every 30 minutes and maintain a 70% or better in all classes.
Academics.
The school offers 18 Advanced Placement courses and a multitude of honors classes available to students. However, RBHS does not weight grade point averages.
Clubs and organizations.
Rock Bridge houses around fifty-five clubs and organizations, including, but not limited to, Intramural Table Tennis League, Chess Club, Global Issues Club, Dumbledore's Army, the Zombie Defense League, Guitar Club, Bible Club, National Honors Society, Student Environmental Coalition, MSU-Muslim Students Union, Future Doctors of America, Future Lawyers of America, Astronomy Club, National Art Honors Society, Weight Lifting Club, and the language Honors Societies. Rock Bridge High School has a planetarium (Room 303) where the school conducts star shows for all ages as well as students at RBHS and other schools as well. The school also has a Student Volunteer Corps, which offers students the opportunity to be teachers' assistants for a teacher of the student's choosing. Positions for Media Center or Copy Center assistants are also available. While the SVC is technically open only to Juniors and Seniors, Freshmen and Sophomores have rarely been known to join early.
Athletics.
Rock Bridge High offers a variety of sports. Fall sports include Cross Country, Football, Girls Golf, Boys Soccer, Softball, Boys Swimming/Diving, Girls Tennis, and Volleyball. Winter sports include Basketball and Wrestling. Spring sports include Baseball, Boys Golf, Boys Tennis, Girls Swimming/Diving, Girls Soccer, and Track & Field. Year round sports include Cheerleading and Poms. Rock Bridge is a perennial powerhouse in both boys and girls tennis, having won the Girls State Title in 1999, 2002, 2003, 2011 and 2014, as well as the Boys State Title in 2008, 2010, 2011, and 2012. At the club level, Rock Bridge fields a Boys Varsity Lacrosse team in the spring.

</doc>
<doc id="26095" url="http://en.wikipedia.org/wiki?curid=26095" title="Room (disambiguation)">
Room (disambiguation)

A room is any distinguishable space within a structure.
Room may also refer to:

</doc>
<doc id="26100" url="http://en.wikipedia.org/wiki?curid=26100" title="FARC">
FARC

The Revolutionary Armed Forces of Colombia—People's Army (Spanish: Fuerzas Armadas Revolucionarias de Colombia—Ejército del Pueblo, FARC–EP and FARC) is a guerilla movement involved in the continuing Colombian armed conflict since 1964. It has been known to employ a variety of military tactics in addition to more unconventional methods including terrorism. The FARC-EP claim to be an army of peasant Marxist–Leninists with a political platform of agrarianism and anti-imperialism. The operations of the FARC–EP are funded by kidnap to ransom, illegal mining, extortion and/or taxation of various forms of economic activity and the production and distribution of illegal drugs.
The strength of the FARC–EP forces is indeterminate; in 2007, the FARC said they were an armed force of 18,000 men and women; in 2010, the Colombian military calculated that FARC forces consisted of approximately 13,800 members, 50 per cent of which were armed guerrilla combatants; and, in 2011, the President of Colombia, Juan Manuel Santos, said that FARC–EP forces comprised fewer than 10,000 members. In 2013 it was reported that 26,648 FARC and ELN members had decided to demobilize since 2002. According to a report from Human Rights Watch, approximately 20–30% of the recruits are minors, most of whom are forced to join the FARC. The greatest concentrations of FARC forces are in the south-eastern regions of Colombia's 500000 km2 of jungle, in the plains at the base of the Andean mountain chain and in northwestern Colombia. However, the FARC and the "ELN" (National Liberation Army of Colombia) lost control of the territory, forcing them to hide primarily in remote areas in the jungle.
In 1964, the FARC–EP were established as the military wing of the Colombian Communist Party (Partido Comunista Colombiano, PCC), after the Colombian military attacked rural Communist enclaves in the aftermath of "The Violence" (La Violencia, ca. 1948–58). The FARC are a violent non-state actor (VNSA) whose formal recognition as legitimate belligerent forces is disputed by some organizations. As such, the FARC has been classified as a terrorist organization by the governments of Colombia, the United States, Canada, Chile, New Zealand, and the European Union; whereas the governments of Venezuela, Brazil, Argentina, Ecuador, and Nicaragua do not. In 2008, Venezuelan President Hugo Chávez recognized the FARC-EP as a proper army. President Chávez also asked the Colombian government and their allies to recognize the FARC as a belligerent force, arguing that such political recognition would oblige the FARC to forgo kidnapping and terrorism as methods of civil war and to abide by the Geneva Convention. Juan Manuel Santos, the current President of Colombia, has followed a middle path by recognizing in 2011 that there is an "armed conflict" in Colombia although his predecessor, Alvaro Uribe, strongly disagreed. In 2012, FARC announced they would no longer participate in kidnappings for ransom and released the last 10 soldiers and police officers they kept as prisoners, but it has kept silent about the status of hundreds of civilians still reported as hostages, and continued kidnapping soldiers and civilians. In February 2008, millions of Colombians demonstrated against the FARC.
In 2012, the FARC made 239 attacks on the energy infrastructure. However, the FARC have shown signs of fatigue. Also the FARC are, as of 2014, not seeking to engage in outright combat with the army, instead concentrating on small-scale ambushes against isolated army units. Meanwhile, since 2008, the FARC have opted to attack police patrols with home-made mortars, sniper rifles and explosives, as they are not considered strong enough to engage police units directly. This follows the trend of the 1990s during the strengthening of the Government forces. Currently the FARC and the Colombian Government are in peace talks in the city of Havana in Cuba, despite FARC failing to follow their own peace offers and ceasefires and repeatedly stating they will not stop their arms race.
History.
"La Violencia" and the National Front.
"There is more repression of individual freedom here, than in any country we've been to; the police patrol the streets, carrying rifles, and demand your papers every few minutes... the atmosphere, here, is tense, and it seems a revolution may be brewing. The countryside is in open revolt, and the army is powerless to suppress it."
 Diary of Ernesto "Che" Guevara, July 6, 1952
In 1948, in the aftermath of the assassination of the populist politician Jorge Eliécer Gaitán, there occurred a decade of large-scale political violence throughout Colombia, which was a Conservative – Liberal civil war that killed more than 200,000 people. In Colombian history and culture, the killings are known as "La Violencia" (The Violence, 1948–58); most of the people killed were peasants and laborers in rural Colombia. In 1957–1958, the political leadership of the Liberal Party and the Conservative Party agreed to establish a bipartisan political system known as the National Front (Frente Nacional, 1958–74). The Liberal and the Conservative parties agreed to alternate in the exercise of government power by presenting a joint National Front candidate to each election and restricting the participation of other political movements. The pact was ratified as a constitutional amendment by a national plebiscite on 1 December 1957 and was supported by the Roman Catholic Church as well as Colombia's business leaders. The initial power-sharing agreement was effective until 1974; nonetheless, with modifications, the Liberal–Conservative bipartisan system lasted until 1990. The sixteen-year extension of the bipartisan power-sharing agreement permitted the Liberal and Conservative élites to consolidate their socioeconomic control of Colombian society, and to strengthen the military to suppress political reform and radical politics proposing alternative forms of government for Colombia.
During the 1960s, the Colombian government effected a policy of Accelerated Economic Development (AED), the agribusiness plan of Lauchlin Currie, a Canadian-born U.S.economist who owned ranching land in Colombia. The plan promoted industrial farming that would produce great yields of agricultural and animal products for world-wide exportation, while the Colombian government would provide subsidies to large-scale private farms. The AED policy came at the expense of the small-scale family farms that only yielded food supplies for local consumption. Based on a legalistic interpretation of what constituted "efficient use" of the land, thousands of peasants were forcefully evicted from their farms and migrated to the cities, where they became part of the industrial labor pool. In 1961, the dispossession of farmland had produced 40,000 landless families and by 1969 their numbers amounted to 400,000 throughout Colombia. By 1970, the "latifundio" type of industrial farm (more than 50 hectares in area) occupied more than 77 per cent of arable land in the country. The AED policy increased the concentration of land ownership among cattle ranchers and urban industrialists, whose businesses expanded their profits as a result of reductions in the cost of labor wages after the influx of thousands of displaced peasants into the cities. During this period, most rural workers lacked basic medical care and malnutrition was almost universal, which increased the rates of preventable disease and infant mortality.
PCC and self-defense communities.
Communists were active throughout rural and urban Colombia in the period immediately following World War I. The Colombian Communist Party ("Partido Comunista Colombiano", PCC) was formally accredited by the Comintern in 1930. The PCC began establishing "peasant leagues" in rural areas and "popular fronts" in urban areas, calling for improved living and working conditions, education, and rights for the working class. These groups began networking together to present a defensive front against the state-supported violence of large landholders. Members organized strikes, protests, seizures of land, and organized communist-controlled "self-defense communities" in southern Colombia that were able to resist state military forces, while providing for the subsistence needs of the populace. Many of the PCC's attempts at organizing peasants were met with violent repression by the Colombian government and the landowning class. U.S. military intelligence estimated that in 1962, the size of the PCC had grown to 8,000 to 10,000 active members, and an additional 28,000 supporters.
In 1961, a guerrilla leader and long-time PCC organizer named Manuel Marulanda Vélez declared an independent "Republic of Marquetalia". The Lleras government attempted unsuccessfully to attack the communities to drive out the guerrillas, due to fears that "a Cuban-style revolutionary situation might develop". After the failed attacks, several army outposts were set up in the area.
Plan Lazo.
In October 1959, the United States sent a "Special Survey Team" composed of counterinsurgency experts to investigate Colombia's internal security situation. Among other policy recommendations the US team advised that "in order to shield the interests of both Colombian and US authorities against 'interventionist' charges any special aid given for internal security was to be sterile and covert in nature." In February 1962, three years after the 1959 "US Special Survey Team", a Fort Bragg top-level U.S. Special Warfare team headed by Special Warfare Center commander General William P. Yarborough, visited Colombia for a second survey.
In a secret supplement to his report to the Joint Chiefs of Staff, Yarborough encouraged the creation and deployment of a US-backed force to commit "paramilitary, sabotage and/or terrorist activities against known communist proponents".
The new counter-insurgency policy was instituted as Plan Lazo in 1962 and called for both military operations and civic action programs in violent areas. Following Yarborough's recommendations, the Colombian military recruited civilians into "civil defense" groups which worked alongside the military in its counter-insurgency campaign, as well as in civilian intelligence networks to gather information on guerrilla activity. Doug Stokes argues that it was not until the early part of the 1980s that the Colombian government attempted to move away from the counterinsurgency strategy represented by Plan Lazo and Yarborough's 1962 recommendations.
The Colombian government began attacking many of the communist groups in the early 1960s, attempting to re-assimilate the territories under the control of the national government. FARC was formed in 1964 by Manuel Marulanda Vélez and other PCC members, after a military attack on the community of Marquetalia. 16,000 Colombian troops attacked the community, which only had 48 armed fighters. Marulanda and 47 others fought against government forces at Marquetalia, and then escaped into the mountains along with the other fighters. These 48 men formed the core of FARC, which later grew in size to hundreds of fighters.
Seventh Guerrilla Conference of the FARC-EP.
In 1982, FARC-EP held its Seventh Guerrilla Conference, which called for a major shift in FARC's strategy. FARC had historically been doing most of its fighting in rural areas, and was limited to small-scale confrontations with Colombian military forces. By 1982, increased income from the "coca boom" allowed them to expand into an irregular army, which would then stage large-scale attacks on Colombian troops. They also began sending fighters to Vietnam and the Soviet Union for advanced military training. They also planned to move closer to middle-sized cities, as opposed to only remote rural areas, and closer to areas rich in natural resources, in order to create a strong economic infrastructure. It was also at this conference that FARC added the initials "EP", for "Ejército del Pueblo" or "People's Army", to the organization's name.
"La Uribe" Agreement and "Union Patriótica".
In the early 1980s, President Belisario Betancur began discussing the possibility of peace talks with the guerrillas. Ultimately this resulted in the 1984 "La Uribe" Agreement, which called for a cease-fire, which ended up lasting from 1984 to 1987.
In 1985, members of the FARC-EP, along with a large number of other leftist and communist groups, formed a political party known as the "Union Patriótica" ("Patriotic Union", UP). The UP sought political reforms (known as "Apertura Democratica") such as constitutional reform, more democratic local elections, political decentralization, and ending the domination of Colombian politics by the Liberal and Conservative parties. They also pursued socioeconomic reforms such land redistribution, greater health and education spending, the nationalization of foreign businesses, Colombian banks, and transportation, and greater public access to mass media. While many members of the UP were involved with the FARC-EP, the large majority of them were not and came from a wide variety of backgrounds such as labor unions and socialist parties such as the PCC. In the cities, the FARC-EP began integrating itself with the UP and forming "Juntas Patrióticas" (or "solidarity cells") – small groups of people associated with labor unions, student activist groups, and peasant leagues, who traveled into the "barrios" discussing social problems, building support for the UP, and determining the sociopolitical stance of the urban peasantry.
The UP performed better in elections than any other leftist party in Colombia's history. In 1986, UP candidates won 350 local council seats, 23 deputy positions in departmental assemblies, 9 seats in the House, and 6 seats in the Senate. The 1986 Presidential candidate, Jaime Pardo Leal, won 4.6% of the national vote.
Since 1986, thousands of members of the UP and other leftist parties were murdered (estimates range from 4,000 to 6,000). In 1987, the President of the UP, Jaime Pardo, was murdered. In 1989 a single large landholder had over 400 UP members murdered. Over 70% of all Colombian presidential candidates in 1990—and 100% of those from center-left parties—were assassinated.
1990–1998.
During this period, the Colombian government continued its negotiations with the FARC-EP and other armed groups, some of which were successful. Some of the groups which demobilized at this time include the EPL, the ERP, the Quintín Lame Armed Movement, and the M-19.
Towards the end of 1990, the army, with no advance warning and while negotiations were still ongoing with the group, attacked a compound known as Casa Verde, which housed the National Secretariat of the FARC-EP. The Colombian government argued that the attack was caused by the FARC-EP's lack of commitment to the process, since the organization was continuing its criminal activities.
During this year on 10 August senior leader Jacobo Arenas, an ideological leader and founder of FARC-EP, died.
On 3 June 1991, dialogue resumed between the Simón Bolívar Guerrilla Coordinating Board and the government on neutral territory in Caracas, Venezuela and Tlaxcala, Mexico. However, the war did not stop, and armed attacks by both sides continued. The negotiation process was broken off in 1993 after no agreement was reached. The Coordinating Board disappeared not long after that time, and guerrilla groups continued their activities independently.
Before the break off of dialogue, a letter written by a group of Colombian intellectuals (among whom were Nobel laureate Gabriel García Márquez) to the Simón Bolívar Guerrilla Coordinating Board was released denouncing the approach taken by the FARC-EP and the dire consequences that it was having for the country.
In the early 1990s, the FARC-EP had between 7,000 and 10,000 fighters, organized into 70 fronts spread throughout the country. From 1996 to 1998 they inflicted a series of strikes on the Colombian Army, including a three-day offensive in Mitú (Vaupés department), taking a large number of soldiers prisoner.
On 23 September 1994, the FARC kidnapped American agricultural scientist Thomas Hargrove and held him captive for 11 months. After his release, Hargrove wrote a book about his ordeal which inspired the 2000 film Proof of Life starring Meg Ryan and Russell Crowe.
Over this period in Colombia, the cultivation of different drugs expanded and there were widespread coca farmers' marches. These marches brought to a halt several major arteries in southern Colombia. Government officials said that FARC-EP had forced the protesters to participate. According to social anthropologist María Clemencia Ramírez, the relationship between the guerrillas and the marches was ambivalent: FARC-EP promoted the 1996 protests as part of their participatory democracy policies yet also exercised authoritarianism, which led to tensions and negotiations with peasant leaders, but the "cocalero" movement brought proposals on behalf of the coca growers and defended its own interests.
Andrés Pastrana's presidency (1998–2002).
In March 1999 members of a local FARC contingent killed 3 USA-based indigenous rights activists, who were working with the U'Wa people to build a school for U'Wa children, and were fighting against encroachment of U'Wa territory by multinational oil corporations. The killings were questioned by many and condemned by many others, and led the United States to increase pressure on the Pastrana administration to crack down on FARC guerrillas.
1998–2002 peace process.
With the hope of negotiating a peace settlement, on 7 November 1998, President Andrés Pastrana granted FARC-EP a 42000 km2 safe haven meant to serve as a confidence building measure, centered around the San Vicente del Caguán settlement.
After a series of high-profile guerrilla actions, including the hijacking of an aircraft, the attack on several small towns and cities, the arrest of the Irish Colombia Three (see below) and the alleged training of FARC-EP militants in bomb making by them, and the kidnapping of several political figures, Pastrana ended the peace talks on 21 February 2002 and ordered the armed forces to start retaking the FARC-EP controlled zone, beginning at midnight. A 48-hour respite that had been previously agreed to with the rebel group was not respected as the government argued that it had already been granted during an earlier crisis in January, when most of the more prominent FARC-EP commanders had apparently left the demilitarized zone. Shortly after the end of talks, the FARC-EP kidnapped Oxygen Green Party presidential candidate Ingrid Betancourt, who was traveling in Colombian territory. Betancourt was rescued by the Colombian government on 2 July 2008 (see Operation Jaque below).
The Colombia Three case.
On 24 April 2002, the U.S. House of Representatives Committee on International Relations published the findings of its investigation into IRA activities in Colombia. Their report alleged a longstanding connection between the IRA and FARC-EP, mentioned at least 15 IRA members who had been traveling in and out of Colombia since 1998, and estimated that the IRA had received at least $2 million in drug proceeds for training FARC-EP members. The IRA/FARC-EP connection was first made public on 11 August 2001, following the arrest in Bogotá of two IRA explosives and urban warfare experts and of a representative of Sinn Féin who was known to be stationed in Cuba. Jim Monaghan, Martin McCauley and Niall Connolly (known as the Colombia Three), were arrested in Colombia in August 2001 and were accused of teaching bomb-making methods to FARC-EP.
On 15 February 2002, the Colombia Three were charged with training FARC-EP members in bomb-making in Colombia. The Colombian authorities had received satellite footage, probably supplied by the CIA, of the men with FARC-EP in an isolated jungle area, where they are thought to have spent the last five weeks. They could have spent up to 20 years in jail if the allegations were proved.
During October 2001, a key witness in the case against the three Irish republicans disappeared. This came as Sinn Féin President Gerry Adams admitted one of the men was the party's representative in Cuba. The missing witness, a former police inspector, said he had seen Mr McCauley with FARC-EP members in 1998. Without his testimony, legal sources said the chances of convicting the three men were reduced.
They were eventually found guilty of traveling on false passports in June 2004 but were acquitted of training FARC-EP members. That decision was reversed after an appeal by the Attorney General of Colombia and they were sentenced to 17-year terms. However, they vanished in December 2004 while on bail and returned to Ireland. Tánaiste Mary Harney said no deal had been done with Sinn Féin or the IRA over the three's return to Ireland adding that the Irish government would consider any request from the Colombian authorities for their extradition. Colombian vice-president Francisco Santos Calderón did not rule out allowing them to serve their sentences in Ireland.
Álvaro Uribe's Presidency (2002–2010).
2002–2005 period.
For most of the period between 2002 and 2005, the FARC-EP was believed to be in a strategic withdrawal due to the increasing military and police actions of new president Álvaro Uribe, which led to the capture or desertion of many fighters and medium-level commanders. Uribe ran for office on an anti-FARC-EP platform and was determined to defeat FARC-EP in a bid to create "confidence" in the country. Uribe's own father had been killed by FARC-EP in an attempted kidnapping in 1983.
In 2002 and 2003, FARC broke up ten large ranches in Meta, an eastern Colombian province, and distributed the land to local subsistence farmers.
During the first two years of the Uribe administration, several FARC-EP fronts, most notably in Cundinamarca and Antioquia, were broken by the government's military operations.
On 5 May 2003, the FARC assassinated the governor of Antioquia, Guillermo Gaviria Correa, his advisor for peace, former defense minister Gilberto Echeverri Mejía, and 8 soldiers. The FARC had kidnapped Mr. Gaviria and Mr. Echeverri a year earlier, when the 2 men were leading a march for peace from Medellín to Caicedo in Antioquia.
On 13 July 2004, the office of the United Nations' High Commissioner for Human Rights publicly condemned the group, proving that FARC-EP violated article 17 of the additional Protocol II of the Geneva Convention and international humanitarian law, as a result of the 10 July massacre of seven peasants and the subsequent displacement of eighty individuals in San Carlos, Antioquia.
In early February 2005, a series of small-scale actions by the FARC-EP around the southwestern departments of Colombia, resulted in an estimated 40 casualties. The FARC-EP, in response to government military operations in the south and in the southeast, would now be displacing its military center of gravity towards the Nariño, Putumayo and Cauca departments.
Possibility of prisoner exchange with the government.
The FARC-EP originally said that they would only release the police and military members they held captive (whom they considered to be prisoners of war) through exchanges with the government for imprisoned FARC-EP members. During the duration of the DMZ negotiations, a small humanitarian exchange took place.
The group demanded a demilitarized zone including two towns (Florida and Pradera) in the strategic region of Valle del Cauca, where much of the current military action against them has taken place, plus this region is also an important way of transporting drugs to the Pacific coast. This demand was rejected by the Colombian government based on previous experience during the 2002 peace talks.
On 2 December 2004, the government announced the pardon of 23 FARC-EP prisoners, to encourage a reciprocal move. The prisoners to be released were all of low rank and had promised not to rejoin the armed struggle. In November 2004, the FARC-EP had rejected a proposal to hand over 59 of its captives in exchange for 50 guerrillas imprisoned by the government.
In a communique dated 28 November but released publicly on 3 December, the FARC-EP declared that they were no longer insisting on the demilitarization of San Vicente del Caguán and Cartagena del Chairá as a precondition for the negotiation of the prisoner exchange, but instead that of Florida and Pradera in the Valle department. They state that this area would lie outside the "area of influence" of both their Southern and Eastern Blocks (the FARC-EP's strongest) and that of the military operations being carried out by the Uribe administration.
They requested security guarantees both for the displacement of their negotiators and that of the guerrillas that would be freed, which are specifically stated to number as many as 500 or more, and ask the Catholic Church to coordinate the participation of the United Nations and other countries in the process.
The FARC-EP also mention in the communique that Simón Trinidad's extradition, would be a serious obstacle to reaching a prisoner exchange agreement with the government. On 17 December 2004, the Colombian government authorized Trinidad's extradition to the United States, but stated that the measure could be revoked if the FARC-EP released all political hostages and military captives in its possession before 30 December. The FARC-EP rejected the demand.
Partial captive releases and escapes during 2006 and 2007.
On 25 March 2006, after a public announcement made weeks earlier, the FARC-EP released two captured policemen at La Dorada, Putumayo. The release took place some 335 mi southwest of Bogotá, near the Ecuadorean border. The Red Cross said the two were released in good health. Military operations in the area and bad weather had prevented the release from occurring one week earlier.
In a separate series of events, civilian hostage and German citizen Lothar Hintze was released by FARC-EP on 4 April 2006, after five years in captivity. Hintze had been kidnapped for extortion purposes, and his wife had paid three ransom payments without any result. 
One prisoner, Julian Ernesto Guevara Castro, a police officer, died of tuberculosis on 28 January 2006. He was a captain and was captured on 1 November 1998. On 29 March 2009, the FARC-EP announced that they would give Guevara's remains to his mother. The FARC handed over Guevara's remains on 1 April 2010.
Another civilian hostage, Fernando Araújo, later named Minister of Foreign Relations and formerly Development Minister, escaped his captors on 31 December 2006. Araújo had to walk through the jungle for five days before being found by troops in the hamlet of San Agustin, 350 mi north of Bogotá. He was kidnapped on 5 December 2000 while jogging in the Caribbean coastal city of Cartagena. He was reunited with his family on 5 January 2007.
Another prisoner, Jhon Frank Pinchao, a police officer, escaped his captors on 28 April 2007 after nine years in captivity. He was reunited with his family on 15 May 2007.
2007 death of 11 hostage deputies.
On 28 June 2007, the FARC-EP reported the death of 11 out of 12 provincial deputies from the Valle del Cauca Department whom the guerrillas had kidnapped in 2002. The guerrillas claimed that the deputies had been killed by crossfire during an attack by an "unidentified military group." The Colombian government stated that government forces had not made any rescue attempts and that the FARC-EP executed the hostages. FARC did not report any other casualties on either side and delayed months before permitting the Red Cross to recover the remains. According to the government, the guerrillas delayed turning over the corpses to let decomposition hide evidence of how they died. The Red Cross reported that the corpses had been washed and their clothing changed before burial, hiding evidence of how they were killed. The Red Cross also reported that the deputies had been killed by multiple close-range shots, many of them in the back of the victims, and even two by shots to the head.
In February 2009, Sigifredo López, the only deputy who survived and was later released by FARC, accused the group of killing the 11 captives and denied that any military rescue attempt had taken place. According to López, the unexpected arrival of another guerrilla unit resulted in confusion and paranoia, leading the rebels to kill the rest of the Valle deputies. He survived after previously being punished for insubordination and was held in chains nearby but separated from the rest of the group.
Major developments during 2008.
Clara Rojas and Consuelo González liberation.
On 10 January 2008, former vice presidential candidate Clara Rojas and former congresswoman Consuelo González were freed after nearly six years in captivity. In a Venezuela-brokered deal, a helicopter flew deep into Colombia to pick up both hostages. The women were escorted out of the jungle by armed guerrillas to a clearing where they were picked up by Venezuelan helicopters that bore International Red Cross insignias. In a statement published on a pro-rebel Web site, the FARC-EP said the unilateral release demonstrated the group's willingness to engage the Colombian government in talks over the release of as many as 800 people who are still being held. In a televised speech, Colombia's U.S.-allied president, Álvaro Uribe, thanked Chavez for his efforts.
During the period she was held kidnapped in the jungle in 2004, Clara Rojas gave birth to her son by Caesarean. At 8 months old, the baby was removed from the area and Rojas didn't hear of the boy again until 31 December, when she heard Colombian President Álvaro Uribe say on the radio that the child was no longer with her captors. DNA tests later confirmed the boy, who had been living in a Bogotá foster home for more than two years under a different name, was hers. She reclaimed her son. Asked about her opinion of the FARC-EP as group, Rojas called it "a criminal organization", condemning its kidnappings as "a total violation of human dignity" and saying some captive police and soldiers are constantly chained.
February 2008 liberations.
On 31 January 2008, the FARC-EP announced that they would release civilian hostages Luis Eladio Perez Bonilla, Gloria Polanco, and Orlando Beltran Cuellar to Venezuelan President Hugo Chávez as a humanitarian gesture. On 27 February 2008, the three hostages and Jorge Eduardo Gechem Turbay (who was added to the list due to his poor health) were released by FARC-EP. With the authorization of the Colombian government and the participation of the International Red Cross, a Venezuelan helicopter transported them to Caracas from San Jose del Guaviare. The FARC-EP had called its planned release of the hostages a gesture of recognition for the mediation efforts of Chávez, who had called on the international community to recognize the rebels as belligerents a month prior. Colombian President Álvaro Uribe, who had tense relations with Chavez, thanked the socialist leader and called for the release of all hostages. He said Colombia was still in a fight "against terrorist actions" but was open to reconciliation.
Anti-FARC rallies.
On 4 February 2008, several rallies were held in Colombia and in other locations around the world, criticizing FARC-EP and demanding the liberation of hundreds of hostages. The protests were originally organized through the popular social networking site Facebook and were also supported by local Colombian media outlets as well as the Colombian government. Participation estimates vary from the hundreds of thousands to several millions of people in Colombia and thousands worldwide.
Kiraz Janicke of leftist "Venezuela News, Views and Analysis" website criticized the rallies, claiming that "right-wing paramilitary leaders featured prominently" in their organization and arguing that workers were also pressured to attend the gatherings. According to her, the purpose of the protests was to promote "Uribe's policy of perpetuating Colombia's decades-long civil war." Shortly before the rallies took place thirteen demobilized AUC paramilitary leaders, including Salvatore Mancuso, had expressed their support of the protest through a communique. However, this move was rejected by organizer Carlos Andrés Santiago, who stated that such an endorsement was harmful and criticized the AUC's actions.
On 20 July 2008, a subsequent set of rallies against FARC included thousands of Colombians in Bogotá and hundreds of thousands throughout the rest of the country.
Death of Raúl Reyes.
On 1 March 2008, the Colombian military attacked a FARC-EP camp inside Ecuador's territory as part of a targeted killing directed at Raúl Reyes. The attack killed over 20 people, about 17 of whom were members of the FARC-EP. Reyes, found among the dead along with at least 16 of his fellow guerrillas, was known as FARC-EP's international spokesman and hostage release negotiator. He was considered to be FARC-EP's second-in-command.
This incident led to a breakdown in diplomatic relations between Ecuador and Colombia, and between Venezuela and Colombia. Ecuador condemned the attack.
It has been considered the biggest blow against FARC-EP in its more than four decades of existence. This event was quickly followed by the death of Ivan Rios, another member of FARC-EP's seven-man Secretariat, less than a week later, by the hand of his own bodyguard. It came as a result of heavy Colombian military pressure and a reward offer of up to $5 million from the Colombian government.
Death of Manuel Marulanda Vélez.
Manuel Marulanda Vélez died on 26 March 2008 after a heart attack. His death would be kept a secret, until Colombian magazine, Revista Semana, published an interview with Colombian defense minister Juan Manuel Santos on 24 May 2008 in which Santos mentions the death of Manuel Marulanda Vélez. The news was confirmed by FARC-EP commander 'Timochenko' on pan-Latin American television station teleSUR on 25 May 2008. 'Timochenko' announced the new commander in chief is 'Alfonso Cano' After speculations in several national and international media about the 'softening up' of the FARC and the announcement of Colombian President Álvaro Uribe that several FARC leaders were ready to surrender and free their captives, the secretariat of the FARC sent out a communiqué emphasizing the death of their founder would not change their approach towards the captives or the humanitarian agreement.
Hugo Chávez's call to disarm.
On 11 January 2008 during the annual State of the Nation in the Venezuelan National Assembly, Venezuelan President Hugo Chavez referred to the FARC as "a real army that occupies territory in Colombia, they're not terrorists [...] They have a political goal and we have to recognize that".
However, on 13 January 2008, Chavez retracted his previous statement and stated his disapproval of the FARC-EP strategy of armed struggle and kidnapping, saying "I don't agree with kidnapping and I don't agree with armed struggle". President Hugo Chávez has repeatedly expressed his disapproval of the practice of kidnapping stating on 14 April that, "If I were a guerrilla, I wouldn't have the need to hold a woman, a man who aren't soldiers...Free the civilians who don't have anything to do with the war. I don't agree with that.". On 7 March at the Cumbre de Rio, Chavez stated again that the FARC-EP should lay down their arms "Look at what has happened and is happening in Latin America, reflect on this (FARC-EP), we are done with war... enough with all this death". On 8 June Chavez repeated his call for a political solution and an end to the war, "The guerrilla war is history...At this moment in Latin America, an armed guerrilla movement is out of place".
Operation Jaque.
On 2 July 2008, under a Colombian military operation called Operation Jaque, the FARC-EP was tricked by the Colombian Government into releasing 15 captives to Colombian Intelligence agents disguised as journalists and international aid workers in a helicopter rescue. Military intelligence agents infiltrated the guerrilla ranks and led the local commander in charge of the captives, Gerardo Aguilar Ramírez, alias Cesar, to believe they were going to take them by helicopter to Alfonso Cano, the guerrillas' supreme leader. The rescued included Íngrid Betancourt (former presidential Candidate), U.S. military contractors Marc Gonsalves, Thomas Howes, and Keith Stansell, as well as eleven Colombian police officers and soldiers. The commander, Cesar and one other rebel were taken into custody by agents without incident after boarding the helicopter. On 4 July, some observers questioned whether or not this was an intercepted captive release made to look like a rescue.
In a 5 July communique, FARC itself blamed rebels Cesar and Enrique for the escape of the captives and acknowledged the event as a setback but reiterated their willingness to reach future humanitarian agreements.
Immediately after the captive rescue, Colombian military forces cornered the rest of FARC-EP's 1st Front, the unit which had held the captives. Colombian forces did not wish to attack the 1st Front but instead offered them amnesty if they surrender.
Colombia's Program for Humanitarian Attention for the Demobilized announced in August 2008 that 339 members of Colombia's rebel groups surrendered and handed in their weapons in July, including 282 guerrillas from the Revolutionary Armed Forces of Colombia.
Óscar Tulio Lizcano Freed.
Lizcano, a Colombian Conservative Party congressman, was kidnapped 5 August 2000. On Sunday, 26 October 2008, the ex-congressman, Óscar Tulio Lizcano escaped from FARC-EP rebels. Tulio Lizcano was a hostage for over 8 years, and escaped with a FARC-EP rebel he convinced to travel with him. They evaded pursuit for three days as they trekked through mountains and jungles, encountering the military in the western coastal region of Colombia. Tulio Lizcano is the first hostage to escape since the successful military rescue of Ingrid Betancourt, and the longest held political hostage by the organization. He became the 22nd Colombian political hostage to gain freedom during 2008.
During his final days in captivity, Lizcano told Santos, they had nothing to eat but wild palm hearts and sugar cane. With the military tightening the noose, a FARC-EP rebel turned himself in and provided Colombian authorities with Lizcano's exact location in the northwest state of Choco. As police and army troops prepared to launch a rescue operation, Lizcano escaped alongside one of his guerrilla guards who had decided to desert. The two men hiked through the rain forest for three days and nights until they encountered an army patrol. Speaking from a clinic in the western city of Cali, Mr Lizcano said that when soldiers saw him screaming from across a jungle river, they thought he was drunk and ignored him. Only when he lifted the FARC-EP rebel's Galil assault rifle did the soldiers begin to understand that he was escaping from the FARC-EP rebels. "They jumped into the river, and then I started to shout, 'I'm Lizcano'", he said.
Other late 2008 developments.
Soon after the liberation of this prominent political hostage, the Vice President of Colombia Francisco Santos Calderón called Latin America's biggest guerrilla group a "paper tiger" with little control of the nation's territory, adding that "they have really been diminished to the point where we can say they are a minimal threat to Colombian security", and that "After six years of going after them, reducing their income and promoting reinsertion of most of their members, they look like a paper tiger." However, he warned against any kind of premature triumphalism, because "crushing the rebels will take time." The 500000 km2 of jungle in Colombia makes it hard to track them down to fight.
February 2009 liberations.
On 21 December 2008, The FARC-EP announced that they would release civilian hostages Alan Jara, Sigifredo López, three low-ranking police officers and a low-ranking soldier to Senator Piedad Córdoba as a humanitarian gesture. On 1 February 2009, the FARC-EP proceeded with the release of the four security force members, Juan Fernando Galicio Uribe, José Walter Lozano Guarnizo, Alexis Torres Zapata and William Giovanni Domínguez Castro. All of them were captured in 2007. Jara (kidnapped in 2001) was released on 3 February and López (kidnapped in 2002) was released on 5 February.
Release of Swedish hostage.
On 17 March 2009, The FARC-EP released Swedish hostage Erik Roland Larsson. Larsson, paralyzed in half his body, was handed over to detectives in a rugged region of the northern state of Córdoba. Larsson was kidnapped from his ranch in Tierralta, not far from where he was freed, on 16 May 2007, along with his Colombian girlfriend, Diana Patricia Pena while paying workers. She escaped that same month following a gun battle between her captors and police. Larsson suffered a stroke while in captivity. The FARC-EP had sought a $5 million ransom. One of Larsson's sons said that the ransom was not paid.
December 2009 hostage killing.
On 22 December 2009, the body of Luis Francisco Cuéllar, the Governor of Caquetá, was discovered, a day after he had been kidnapped from his house in Florencia, Caquetá. Officials said the abduction and execution had been carried by the FARC. According to officials, he had been killed soon after the abduction. The kidnappers cut the governor's throat as they evaded security forces. In a statement broadcast on radio, the acting governor, Patricia Vega, said, "I no longer have any doubts that FARC has done it again." The FARC claimed responsibility for Cuéllar's kidnapping and murder in January 2010. The group said that they kidnapped him in order to "put him on trial for corruption" and blamed his death on an attempt to rescue him by force.
March 2010 liberations.
On 16 April 2009, The FARC-EP announced that they would release Army Corporal Pablo Emilio Moncayo Cabrera to Piedad Córdoba as a humanitarian gesture. Moncayo was captured on 21 December 1997. On 28 June 2009, the FARC announced that they would release Professional Soldier Josue Daniel Calvo Sanchez. Calvo was captured on 20 April 2009. Calvo was released on 28 March 2010. Moncayo was released on 30 March 2010. 
Operation Chameleon.
On 13 June 2010, Colombian troops rescued Police Colonel Luis Herlindo Mendieta Ovalle, Police Captain Enrique Murillo Sanchez and Army Sergeant Arbey Delgado Argote, after twelve years as prisoners. Argote was captured on 3 August 1998. Ovalle and Sanchez were captured on 1 November 1998. On 14 June, Police Lieutenant William Donato Gomez was also rescued. He was also captured on 3 August 1998.
Juan Manuel Santos's presidency.
President Juan Manuel Santos began his term with a suspected FARC bomb-blast in Bogotá. This followed the resolution of the 2010 Colombia–Venezuela diplomatic crisis which erupted over outgoing President Álvaro Uribe's allegations of active Venezuelan support for FARC.
In early September 2010, FARC-EP attacks in the Nariño Department and Putumayo Department in southern Colombia killed some fifty policemen and soldiers in hit-and-run assaults.
According to a December report by the Corporación Nuevo Arco Iris NGO, 473 FARC-EP guerrillas and 357 members of the Colombian security forces died in combat between January and September 2010. An additional 1,382 government soldiers or policemen were wounded during the same period, with the report estimating that the total number of casualties could reach 2,500 by the end of the year. Nuevo Arco Iris head León Valencia considered that FARC guerrillas have reacted to a series of successful military blows against them by splitting up their forces into smaller groups and intensifying the offensive use of anti-personnel land mines, leading to what he called a further "degradation" of the conflict. Valencia also added that both coca crops and the drug trade have "doubled" in areas with FARC-EP presence. Researcher Claudia López considered that the Colombian government is winning the strategic and aerial side of the war but not the infantry front, where both the FARC-EP and ELN continue to maintain an offensive capacity.
Military offensives carried out under former President Alvaro Uribe and President Juan Manuel Santos have been able to reduce the number of FARC combattants to 7,000, which is much lower than the 20,000 combattants FARC employed in the early 2000s. This military offensive has also been able to reduce FARC territorial control and push guerillas to more remote and sparsely populated regions, often close to territorial or internal borders.
2010 death of Mono Jojoy.
Colombian authorities announced the death of Víctor Julio Suárez Rojas, also known as "Mono Jojoy", on 23 September 2010. President Juan Manuel Santos stated that the FARC commander was killed in an operation that began in the early hours of 21 September in the department of Meta, 200 mi south of the capital Bogotá. According to Santos, he was "the impersonation of terror and a symbol of violence". After this event, the FARC-EP released a statement saying that defeating the group would not bring peace to Colombia and called for a negotiated solution, not surrender, to the social and political conflict.
January through October 2011.
In January 2011 Juan Manuel Santos admitted that FARC-EP had killed 460 government soldiers and wounded over 2,000 in 2010. In April 2011 the Colombian congress issued a statement saying that FARC has a 'strong presence' in roughly one third of the municipalities in Colombia, while their attacks have increased. Overall FARC operations, including attacks against security forces as well as kidnappings and the use of land mines, have increased every year since 2005. In the first 6 months of 2011 the FARC carried out an estimated 1,115 actions, which constitutes a 10% increase over the same period in 2010.
By early 2011 Colombian authorities and news media reported that the FARC and the clandestine sister groups have partly shifted strategy from guerrilla warfare to 'a war of militias', meaning that they are increasingly operating in civilian clothes while hiding amongst sympathizers in the civilian population. In early January 2011 the Colombian army said that the FARC has some 18,000 members, with 9,000 of those forming part of the militias. The army says it has 'identified' at least 1,400 such militia members in the FARC strongholds of Valle del Cauca and Cauca in 2011. In June 2011 Colombian chief of staff Edgar Cely claimed that the FARC wants to 'urbanize their actions', which could partly explain the increased guerrilla activity in Medellín and particularly Cali. Jeremy McDermott, co-director of Insight Crime, estimates that FARC may have some 30,000 'part-time fighters' in 2011, consisting of both armed and unarmed civilian supporters making up the rebel militia network, instead of full-time fighters wearing uniforms.
According to Corporación Nuevo Arco Iris, FARC-EP killed 429 members of the Colombian government's security forces between January and October 2011. During this same period, the rebel group lost 316 of its own members. The year 2011 saw over 2,000 incidents of FARC activity, which was the highest figure recorded since 1998. The NGO has stated that while most of these incidents remain defensive in nature and are not like the large offensives from years past, FARC actions have been growing since 2005, and the rebel group is currently carrying out intense operations against small and medium-sized Colombian military units in vulnerable areas.
Death of Alfonso Cano.
Colombian troops killed FARC leader Alfonso Cano in a firefight on 4 November 2011. The 6th Front of the FARC, which was in charge of Cano's security at the time of his death, retaliated by killing two policemen in Suarez and Jambaló some 24 hours after the death of Cano.
Death of captives in Operation Jupiter.
On 26 November 2011, the FARC killed Police Captain Edgar Yesid Duarte Valero, Police Lieutenant Elkin Hernandez Rivas, Army Corporal Libio Jose Martinez Estrada and Police Intendant Alvaro Moreno after government troops approached the guerrilla camp where they were held. Police Sergeant Luis Alberto Erazo Maya managed to escape his captors and was later rescued.
The Colombian military had information indicating that there could be captives in the area and initiated Operation Jupiter in October 2011, using a 56 men Special Forces unit to carry out surveillance for preparing a future rescue mission that would involve additional troops and air support. According to the Colombian military, this same unit remained in the area for 43 days and did not find the captives until they accidentally ran into the FARC camp on the way back, which led to a shootout. Relatives of the captives, former victims and civil society groups blamed both the government and FARC for the outcome, questioning the operation as well as criticizing military rescues.
2012 release of last political hostages.
On 26 February 2012, the FARC announced that they would release their remaining ten political hostages. The hostages were released on 2 April 2012. The president of Colombia, Juan Manuel Santos, said that this incident was "not enough", and asked the FARC to release the civilian hostages they possess.
Release of Chinese hostages.
On 22 November 2012, the FARC released four Chinese oil workers. The hostages were working for the Emerald Energy oil company, a British-based subsidiary of China's Sinochem Group, when they were kidnapped on 8 June 2011. Their Colombian driver was also kidnapped, but released several hours later. Authorities identified the freed men as Tang Guofu, Zhao Hongwei, Jian Mingfu, and Jiang Shan.
Peace talks.
Santos announced on 27 August 2012 that the Colombian government has engaged in exploratory talks with FARC in order to seek an end to the conflict:
Exploratory conversations have been held with the FARC to find an end to the conflict. I want to make very clear to Colombians that the approaches that have been carried out and the ones that will happen in the future will be carried out within the framework based on these principles: We are going to learn from the mistakes made in the past so that they are not repeated. Second, any process must lead to the end of the conflict, not making it longer. Third, operations and military presence will be maintained across the entire national territory
He also said that he would learn from the mistakes of previous leaders, who failed to secure a lasting ceasefire with FARC, though the military would still continue operations throughout Colombia while talks continued. An unnamed Colombian intelligence source said Santos has assured FARC that no one would be extradited to stand trial in another country. "Al Jazeera" reported that the initiative began after Santos met with Venezuelan President Hugo Chavez and asked him to mediate. Former President Uribe has criticized Santos for seeking peace "at any cost" and rejected the idea of holding talks. "Telesur" reported that FARC and the Colombian government had signed a preliminary agreement in Havana the same day. The first round of the talks will take place in Oslo on 5 October and then return to Havana for approximately six months of talks before culminating in Colombia. However, Santos later ruled out a ceasefire pending the talks in Oslo and reiterated that offensive operations against FARC would continue.
ELN leader Nicolas Rodriguez Bautista, otherwise known as Gabino, added that his group was interested in joining the talks too: "Well we are open, it's exactly our proposal, to seek room for open dialogue without conditions and start to discuss the nation's biggest problems. But the government has said no! Santos says he has the keys to peace in his pocket, but I think he has lost them because there seems to be no possibility of a serious dialogue, we remain holding out for that."
Colombia's RCN Radio reported on 29 September that a preliminary draft of the proposals indicated that a resolution would involve answering FARC's historic grievances including rural development and agrarian reform; democracy development via an enhancement of the number of registered political parties; security and compensation for the victims of the conflict. In this regards, the Colombian government has alread passed a series of laws that entail compensation for the victims and a return of land to the displaced. FARC also indicated a willingness to give up their arms. Former M19 member Antonio Navarro Wolff said: "If the government wants a serious peace plan they will have to take control of the coca leaf plantations that are currently owned by the FARC because if not another criminal group will take over it." Santos later told "Al Jazeera" that peace was possible if there was "goodwill" on both sides. Santos told the General debate of the sixty-seventh session of the United Nations General Assembly on 26 September, that Venezuela and Chile were also helping in the discussion along with Cuba and Norway.
Peace talks were formally started on 18 October in a hotel 30 miles north of the Norwegian capital Oslo with a joint-press conference by both delegations. The representatives of the government, led by Humberto de la Calle and the FARC, led by Iván Márquez, said the so-called second phase of the peace process will be inaugurated in Oslo on 15 November, after which the delegations will go to Cuba to work on the negotiation of the peace accord, which will ultimately lead to a permanent agreement and ceasefire. The Colombian government has also stated that they expect that a post-Chavez government will continue to support the peace process. In late 2012, FARC declared a two-month unilateral cease-fire and said that they would be open to extending it as a bilateral truce afterwards during the rest of the negotiations. The Colombian government refused to agree to a bilateral cease-fire, alleging violations of the truce by FARC.
Shortly after lifting the ceasefire, FARC conducted attacks on a coal transport railway, which derailed 17 wagons and forced a suspension of operations and assaulted Milan, a town in the southern Caquetá, killing at least seven government soldiers and injuring five others.
Santos has been far more responsive to threats against social leaders than his predecessors. He has also been decisive in combatting the New Illegal Armed Groups that emerged as a result of the paramilitary process, especially in fighting threats and violence against human rights defenders and social leaders. During Santos' presidency, private security and proclaimed self-defense movements have also lost their legitimacy.
On 27 May 2013, it was announced that one of the most contentious issues had been resolved. Land reform and compensation was tackled with promises to compensate those who had lost land. This is the first time the government and FARC have reached an agreement on a substantive issue in four different negotiating attempts over 30 years. The peace process then moved on to the issue of "political participation", during which FARC insisted on its demand for an elected Constituent Assembly to rewrite Colombia's constitution. This demand has been forcefully rejected by Colombia's lead government negotiator, Humberto de la Calle.
On 1 July 2013, FARC and the second-largest guerrilla group in Colombia, ELN, announced that they would be working together to find a "political solution to the social and armed conflict." The details of this partnership, however, are far from clear; Washington Office on Latin America's Adam Isacson explains that two issues central to peace accords with ELN—resource policy and kidnapping—are currently off the table in the talks in Havana with FARC, and the addition of these topics may complicate and slow down an already sluggish process.
On 6 November 2013 the Colombian government and FARC announced that they had come to an agreement regarding the participation of political opposition and would begin discussing their next issue, the illicit drug trade.
On 23 January 2014 Juan Fernando Cristo, the President of the Senate of Colombia, proposed a second Plan Colombia during a conference on the Colombian peace process in Washington, D.C. Cristo stated that this new plan should be "for the victims" and should redirect the resources from the original Plan Colombia towards supporting a post-conflict Colombia.
On May 16, 2014, the Colombian government and the FARC rebels agreed to work together against drug trafficking, added to the development of these peace talks.
On May 22, 2015, FARC announced it would be ending the unilateral ceasefire it had announced the previous December.
Role of FARC in the areas it controls.
Relations between the FARC-EP and local populations vary greatly depending on the history and specific characteristics of each region. In rural areas where the guerrillas have maintained a continuous presence for several decades, there are often organic links between the FARC and peasant communities. Such ties include shared generational membership and historical struggles dating back to the period of "La Violencia". These areas have traditionally been located in the departments of Caquetá, Meta, Guaviare and Putumayo, and - to a lesser extent - portions of Huila, Tolima and Nariño. Within remote locations under FARC control and where the national government is generally absent, the group can function as a revolutionary vanguard and institutes its "de facto" rule of law by carrying out activities that aim to combat corruption and reduce small-scale crime. 
The FARC has also been able to provide limited social services in these regions, such as health care and education, including building minor infrastructure works in the form of rural roads. Peasants who have grown up in areas under historical FARC control may become accustomed to accepting them as the local authority. The guerrillas also attempt to keep the peace between peasants and drug traffickers in addition to regulating other aspects of daily life and economics. 
In other rural regions of the country, where a FARC presence has only been established within the past twenty years and primarily remains military in nature, there is often a level of distrust between FARC rebels and the local peasant communities, which lack historical ties to the group. Civilians in these locations also tend to get caught in the middle of the conflict between FARC and its government or paramilitary opponents. In the populated urban areas where the Colombian state has maintained a solid historical presence, some FARC sympathies may exist in the poorest neighborhoods and among certain progressive sectors of the middle class, but most city inhabitants tend to view the guerrillas as one of Colombia's main problems.
By the end of 2010, FARC-EP influence was significantly reduced in the regions where it had only carried out a recent military-focused expansion during the 1980s and 1990s, in part due to the failure to establish close social ties with local populations. Government offensives eradicated much of the visible guerrilla presence in northern and central Colombia as well as in Guainía, Vaupés and Amazonas, limiting FARC to clandestine operations. Similar military setbacks and retreats occurred even within its traditional strongholds, forcing the FARC to move towards the most remote areas, but there the guerrillas did appear to maintain popular support among the peasants that had developed organic links to the insurgency.
Financing.
FARC receives most of its funding—which has been estimated to average some $300 million per year—from taxation of the illegal drug trade and other activities, ransom kidnappings, bank robberies, and extortion of large landholders, multinational corporations, and agribusiness. From taxation of illegal drugs and other economic activity, FARC has been estimated to receive approximately 60 to 100 million dollars per year.
Means of financing.
The guerillas main means of financing is through the drug trade which includes both direct and indirect participation; taxation, administration or control of areas of production and trafficking. A large but often difficult to estimate portion of funding comes from the taxation of businesses and even local farmers, often lumped in with or defined by its opponents as extortion. 
Drug trade.
FARC-EP was not initially involved in direct drug cultivation, trafficking, or trans-shipment prior to or during the 1980s. Instead, it maintained a system of taxation on the production that took place in the territories that they controlled, in exchange for protecting the growers and establishing law and order in these regions by implementing its own rules and regulations. During the 1990s, FARC expanded its operations, in some areas, to include trafficking and production, which has provided a significant portion of its funding. Right-wing paramilitary groups also receive a large portion of their income from drug trafficking and production operations.
A 1992 Central Intelligence Agency report "acknowledged that the FARC had become increasingly involved in drugs through their 'taxing' of the trade in areas under their geographical control and that in some cases the insurgents protected trafficking infrastructure to further fund their insurgency", but also described the relationship between the FARC and the drug traffickers as one "characterized by both cooperation and friction" and concluded that "we do not believe that the drug industry [in Colombia] would be substantially disrupted in the short term by attacks against guerrillas. Indeed, many traffickers would probably welcome, and even assist, increased operations against insurgents."
In 1994, the DEA came to three similar conclusions. First, that any connections between drug trafficking organizations and Colombian insurgents were "ad hoc 'alliances of convenience'". Second, that "the independent involvement of insurgents in Colombia's domestic drug productions, transportation, and distribution is limited ... there is no evidence that the national leadership of either the FARC or the ELN has directed, as a matter of policy, that their respective organizations directly engage in independent illicit drug production, transportation, or distribution." Third, the report determined that the DEA "has no evidence that the FARC or ELN have been involved in the transportation, distribution, or marketing of illegal drugs in the United States. Furthermore it is doubtful that either insurgent group could develop the international transportation and logistics infrastructure necessary to establish independent drug distribution in the United States or Europe… DEA believes that the insurgents never will be major players in Colombia's drug trade."
FARC has called for crop substitution programs that would allow coca farmers to find alternative means of income and subsistence. In 1999, FARC worked with a United Nations alternative development project to enable the transition from coca production to sustainable food production. On its own, the group has also implemented agrarian reform programs in Putumayo.
In those FARC-EP controlled territories that do produce coca, it is generally grown by peasants on small plots; in paramilitary or government controlled areas, coca is generally grown on large plantations. The FARC-EP generally makes sure that peasant coca growers receive a much larger share of profits than the paramilitaries would give them, and demands that traffickers pay a decent wage to their workers. When growers in a FARC-controlled area are caught selling coca to non-FARC brokers, they are generally forced to leave the region, but when growers are caught selling to FARC in paramilitary-controlled areas, they are generally killed. Lower prices paid for raw coca in paramilitary-controlled areas lead to significantly larger profits for the drug processing and trafficking organizations, which means that they generally prefer that paramilitaries control an area rather than FARC.
In 2000, FARC Spokesman Simon Trinidad said that taxes on drug laboratories represented an important part of the organization's income, although he didn't say how much it was. He defended this funding source, arguing that drug trade was endemic in Colombia because it had pervaded many sectors of its economy.
After the 21 April 2001 capture of Brazilian drug lord Luiz Fernando da Costa (aka Fernandinho Beira-Mar) in Colombia, Colombian and Brazilian authorities accused him of cooperating with FARC-EP through the exchange of weapons for cocaine. They also claimed that he received armed protection from the guerrilla group.
In Monday, 18 March 2002 the Attorney General of the United States John Ashcroft indicted leaders of the FARC after an 18-month investigation into their narcotics trafficking. Tomas Molina Caracas, the commander of the FARC's 16th Front, led the 16th Front's drug-trafficking activities together with Carlos Bolas and a rebel known as Oscar El Negro. Between 1994 and 2001, Molina and other 16th Front members controlled Barranco Minas, where they collected cocaine from other FARC fronts to sell it to international drug traffickers for payment in currency, weapons and equipment.
On 22 March 2006 the Attorney General Alberto Gonzales announced the indictment of fifty leaders of FARC for exporting more than $25 billion worth of cocaine to the United States and other countries. Several of the FARC leaders appeared on the Justice Department's Consolidated Priority Organization target list, which identifies the most dangerous international drug trafficking organizations. Recognizing the increased profits, the FARC moved to become directly involved in the manufacture and distribution of cocaine by setting the price paid for cocaine paste and transporting it to jungle laboratories under FARC control. The charged FARC leaders ordered that Colombian farmers who sold paste to non-FARC buyers would be murdered and that U.S. fumigation planes should be shot down.
On 11 October 2012 Jamal Yousef, aka "Talal Hassan Ghantou", a native of Lebanon, was sentenced to 12 years in prison for conspiring to provide military-grade weapons to the Fuerzas Armadas Revolucionarias de Colombia (the FARC), in exchange for over a ton of cocaine. Yousef pled guilty in May 2012 to one count of providing material support to the FARC.
Kidnappings.
The FARC-EP has carried out both ransom and politically motivated kidnappings in Colombia and has been responsible for the majority of such kidnappings carried out in the country.
The guerrillas initially targeted the families of drug traffickers, the wealthy upper-class and foreigners but the group later expanded its kidnapping and extortion operations to include the middle-class.
During the 1984 peace negotiations, FARC pledged to stop kidnapping and condemned the practice. However, hostage-taking by FARC increased in the years following this declaration. In a 1997 interview, FARC-EP Commander Alfonso Cano argued that some guerrilla units continued to do so for "political and economic reasons" in spite of the prohibition issued by the leadership.
In 2000, the FARC-EP issued a directive called "Law 002" which demanded a "tax" from all individuals and corporations with assets worth at least $1 million USD, warning that those who failed to pay would be detained by the group. In 2001, FARC Commander Simón Trinidad claimed that the FARC-EP does not engage in kidnapping but instead "retains [individuals] in order to obtain resources needed for our struggle". Commander Trinidad said he did not know how many people had been taken by FARC or how much money was collected by the organization in exchange for their freedom. In addition, FARC spokesperson Joaquín Gómez stated that the payment demanded was a tax which many people paid "voluntarily", with kidnapping undertaken because "those who have the resources must pay their share".
In 2002, Amnesty International sent a letter to FARC-EP Commander Manuel Marulanda condemning kidnapping and hostage-taking as well as rejecting the threats directed at municipal or judicial officials and their families, arguing that they are civilians who are protected by international humanitarian law as long as they do not participate in hostilities.
According to Amnesty International, the number of kidnappings has decreased in recent years but the human rights organization estimates that FARC and ELN guerrillas continue to be behind hundreds of cases. In 2008, press reports estimated that about 700 hostages continued to be held captive by FARC. According to the "Fundación País Libre" anti-kidnapping NGO, an estimated total of 6,778 people were kidnapped by FARC between 1997 and 2007. In 2009, the state's anti-kidnapping agency "Fondelibertad" reviewed 3,307 officially unsettled cases and removed those that had already been resolved or for which there was insufficient information. The agency concluded that 125 hostages remained in captivity nationwide of whom 66 were being held by the FARC-EP. The government's revised figures were considered "absurdly low" by "Fundación País Libre", which has argued that its own archives suggest an estimated 1,617 people taken hostage between 2000 and 2008 remain in the hands of their captors, including hundreds seized by FARC. FARC claimed at the time that it was holding nine people for ransom in addition to hostages kept for a prisoner exchange.
In 2008, Venezuelan President Hugo Chávez expressed his disagreement with FARC-EP's resorting to kidnappings. Former President Fidel Castro of Cuba also criticized the use of hostage-taking by the guerrillas as "objectively cruel" and suggested that the group free all of its prisoners and hostages.
In February 2012, FARC announced that it would release ten members of the security forces, who it described as political prisoners, representing the last such captives in its custody. It further announced the repeal of Law 002, bringing to an end its support for the practice of kidnapping for ransom. However, it was not clear from the FARC statement what would happen to the civilians it still held in captivity. Colombian president Juan Manuel Santos used Twitter to welcome the move as a "necessary, if insufficient, step in the right direction".
Human rights concerns.
FARC has been accused of committing violations of human rights by numerous groups, including Human Rights Watch, Amnesty International, the United Nations as well as by the Colombian, U.S. and European Union governments.
A February 2005 report from the United Nations' High Commissioner for Human Rights mentioned that, during 2004, "FARC-EP continued to commit grave breaches [of human rights] such as murders of protected persons, torture and hostage-taking, which affected many civilians, including men, women, returnees, boys and girls, and ethnic groups."
Child soldiers.
FARC-EP, the ELN and right-wing paramilitaries all train teens as soldiers and informants. Human Rights Watch estimates that the FARC-EP has the majority of child combatants in Colombia, and that approximately one quarter of its guerrillas are under 18. Forcible recruitment of children, by either side, is rare in Colombia. They join for a variety of reasons including poverty, lack of educational opportunities, avoiding dangerous work in coca processing, escaping from domestic violence, offers of money (mostly from paramilitaries, who pay their soldiers). Human Rights Watch has noted that "once integrated into the FARC-EP, children are typically barred from leaving".
FARC-EP Commander Simón Trinidad has stated that FARC does not allow the enlistment of people under 15 years of age, arguing that this is in accordance with Article 38 of the United Nations' Convention on the Rights of the Child. He has argued that the alternatives for many children in Colombia are worse, including prostitution and exploitative work in mines and coca production. Amnesty International has rejected the validity of such a position in international law.
In June 2000, FARC-EP Commander Carlos Antonio Lozada told Human Rights Watch that the minimum recruitment age of fifteen years was set in 1996 but admitted that "this norm was not enforced" until recently. Lozada said, however, that it had become an obligatory standard after Commander Jorge Briceño's statements on the matter in April 2000. A 2001 Human Rights Watch report considered FARC-EP's refusal to admit children under fifteen years old into their forces to be "encouraging" but added that there is "little evidence that this rule is being strictly applied" and called on the group to demobilize all existing child soldiers and cease this practice in the future.
In 2003, Human Rights Watch reported that FARC-EP shows no leniency to children because of their age, assigning minors the same duties as adults and sometimes requiring them to participate in executions or witness torture.
Extrajudicial executions.
FARC has consistently carried out attacks against civilians specifically targeting suspected supporters of paramilitary groups, political adversaries, journalists, local leaders, and members of certain indigenous groups since at least as early as 1994. From 1994 to 1997 the region of Urabá in Antioquia department was the site of FARC attacks against civilians. FARC has also executed civilians for failing to pay "war taxes" to their group.
In 2001, Human Rights Watch (HRW) denounced that the FARC-EP had abducted and executed civilians accused of supporting paramilitary groups in the demilitarized zone and elsewhere, without providing any legal defense mechanisms to the suspects and generally refusing to give any information to relatives of the victims. The human rights NGO directly investigated three such cases and received additional information about over twenty possible executions during a visit to the zone.
According to HRW, those extrajudicial executions would qualify as forced disappearances if they had been carried out by agents of the government or on its behalf, but nevertheless remained "blatant violations of the FARC-EP's obligations under international humanitarian law and in particular key provisions of article 4 of Protocol II, which protects against violence to the life, physical, and mental well-being of persons, torture, and ill-treatment."
The Colombian human rights organization CINEP reported that FARC-EP killed an estimated total of 496 civilians during 2000.
Use of gas cylinder mortars and landmines.
The FARC-EP has employed a type of improvised mortars made from gas canisters (or cylinders), when launching attacks.
According to Human Rights Watch, the FARC-EP has killed civilians not involved in the conflict through the use of gas cylinder mortars and its use of landmines.
Human Rights Watch considers that "the FARC-EP's continued use of gas cylinder mortars shows this armed group's flagrant disregard for lives of civilians...gas cylinder bombs are impossible to aim with accuracy and, as a result, frequently strike civilian objects and cause avoidable civilian casualties."
According to the ICBL Landmine and Cluster Munitions Monitor, "FARC is probably the most prolific current user of antipersonnel mines among rebel groups anywhere in the world." Furthermore, FARC use child soldiers to carry and deploy antipersonnel mines.
Violence against indigenous people.
FARC has sometimes threatened or assassinated indigenous Colombian leaders for attempting to prevent FARC incursions into their territory and resisting the forcible recruitment by FARC of indigenous youth. Between 1986 and 2001, FARC was responsible for 27 assassinations, 15 threats, and 14 other abuses of indigenous people in Antioquia Department.
In March 1999 members of a local FARC contingent killed 3 indigenous rights activists, who were working with the U'Wa people to build a school for U'Wa children, and were fighting against encroachment of U'Wa territory by multinational oil corporations. The killings were almost universally condemned, and seriously harmed public perceptions of FARC.
Members of indigenous groups have demanded the removal of military bases set up by the Colombian government and guerrilla encampments established by FARC in their territories, claiming that both the Colombian National Army and the FARC should respect indigenous autonomy and international humanitarian law. According to the National Indigenous Organization of Colombia (ONIC), 80.000 members of indigenous communities have been displaced from their native lands since 2004 because of FARC-related violence.
Luis Evelis, an indigenous leader and ONIC representative, has stated that "the armed conflict is still in force, causing damages to the indigenous. Our territories are self-governed and we demand our autonomy. During the year 2011, fifty-six indigenous people have been killed." The United Nations Declaration on the Rights of Indigenous Peoples has indicated that no military activities may be carry out within indigenous territories without first undertaking an "effective consultation" with indigenous representatives and authorities from the communities involved.
The Regional Indigenous Council of Cauca (CRIC) issued a statement concerning the release of two hostages taken by FARC in 2011: "Compared to past statements made by the national government, it is important to reiterate that the presence of armed groups in our territories is a fact that has been imposed by force of arms, against which our communities and their leaders have remained in peaceful resistance." The CRIC also indicated that neither the Colombian government nor the mediators and armed groups involved consulted with the indigenous people and their authorities about the hostage release, raising concerns about the application of national and international law guaranteeing their autonomy, self-determination and self-government. The indigenous organization also demanded the immediate end of all violence and conflict within indigenous territories and called for a negotiated solution to the war.
Official Colombian government statistics show that murders of indigenous people between January and May 2011 have increased 38% compared to the same timeframe in 2010. Colombia is home to nearly 1 million indigenous people, divided into around 100 different ethnicities. The Colombian Constitutional Court has warned that 35 of those groups are in danger of dying out. The Permanent Assembly for the Defense of Life and Territorial Control has stated that the armed conflict "is not only part of one or two areas, it is a problem of all the indigenous people."
Sexual abuse and forced abortions.
According to Amnesty International, both civilian women and female combatants have been sexually exploited or victimized by all of the different parties involved in the Colombian armed conflict. In the case of FARC, it has been reported that young female recruits have been sexually abused by veteran guerrilla soldiers and in several cases pregnancies were aborted against their will by FARC doctors.
Organization and structure.
FARC-EP remains the largest and oldest insurgent group in the Americas. According to the Colombian government, FARC-EP had an estimated 6,000–8,000 members in 2008, down from 16,000 in 2001, having lost much of their fighting force since President Álvaro Uribe took office in 2002. Political analyst and former guerrilla León Valencia has estimated that FARC's numbers have been reduced to around 11,000 from their 18,000 peak but cautions against considering the group a defeated force. In 2007, FARC-EP Commander Raúl Reyes claimed that their force consisted of 18,000 guerrillas.
The largest concentrations of FARC-EP guerrillas are located throughout the southeastern parts of Colombia's 500000 km2 of jungle and in the plains at the base of the Andean mountains.
FARC's organized hierarchically into military units as follows:
The FARC-EP secretariat was led by Alfonso Cano and six others after the death of Manuel Marulanda (Pedro Antonio Marín), also known as "Tirofijo", or Sureshot in 2008. The "international spokesman" of the organization was represented by "Raul Reyes", who was killed in a Colombian army raid against a guerrilla camp in Ecuador on 1 March 2008. Cano was killed in a military operation on 4 November 2011.
FARC-EP remains open to a negotiated solution to the nation's conflict through dialogue with a flexible government that agrees to certain conditions, such as the demilitarization of certain areas, cessation of paramilitary and government violence against rural peasants, social reforms to reduce poverty and inequality, and the release of all jailed (and extradited) FARC-EP rebels. It claims that until these conditions surface, the armed revolutionary struggle will remain necessary to fight against Colombia's elites. The FARC-EP says it will continue its armed struggle because it perceives the current Colombian government as an enemy because of historical politically motivated violence against its members and supporters including members of the Patriotic Union, a FARC-EP-created political party.

</doc>
<doc id="26147" url="http://en.wikipedia.org/wiki?curid=26147" title="Outline of religion">
Outline of religion

The following outline is provided as an overview of and topical guide to religion:
Religion – organized collection of beliefs, cultural systems, and world views that relate humanity to an order of existence. Many religions have narratives, symbols, and sacred histories that are intended to explain the meaning of life and/or to explain the origin of life or the Universe. From their beliefs about the cosmos and human nature, people derive morality, ethics, religious laws or a preferred lifestyle. According to some estimates, there are roughly 4,200 religions in the world.
Religious studies.
Religious studies
Religion-specific topics.
Ayyavazhi topics.
Ayyavazhi
Cao Dai topics.
Cao Dai
Judaism topics.
Judaism
Modern Paganism topics.
Modern Paganism
New Age topics.
List of New Age topics
New religious movement topics.
New religious movement
Open source religion topics.
Open source religion
Rasta topics.
Rastafari Movement
Satanism topics.
Satanism
Shintō topics.
Shinto
Spiritism topics.
Spiritism
Tenrikyo topics.
Tenrikyo
Unitarian Universalism topics.
Unitarian Universalism
Zoroastrianism topics.
Zoroastrianism
Irreligion topics.
Irreligion

</doc>
<doc id="26167" url="http://en.wikipedia.org/wiki?curid=26167" title="Robert Calvert">
Robert Calvert

Robert Newton Calvert (9 March 1945 – 14 August 1988) was a South African writer, poet, and musician.
Biography.
Calvert was born in Pretoria, South Africa and moved with his parents to England when he was two. He attended school in London and Margate. Having finished school he joined The Air Training Corps, where he became a corporal and played the trumpet for the 438 squadron band. He then went on to college in Canterbury. After leaving college, and having been denied his childhood dream of becoming a fighter pilot he slowly acquainted himself with the UK's bohemian scene. Calvert began his career in earnest by writing poetry. In 1967 he formed the Street Theatre group, "Street Dada Nihilismus".
At the end of the 1960s he moved to London and joined the flourishing 'psychedelic' subculture. He soon became one of its most active members; joining, amongst other activities, Frendz, one of the leading underground magazines of the time. During that time he acquainted himself with the "New Wave" of Science Fiction writers. Acclaimed author Michael Moorcock, winner of several Science Fiction literary awards and publisher of the influential New Worlds magazine, became a lifelong friend. Calvert's poems were published in "New World" and other magazines. Although he was influenced by the New Wave, Calvert developed a distinct style of his own. His ability to change fluently between poetry, music and theatre allowed him to develop into a multimedia artist. Calvert then became acquainted with Dave Brock, and became the resident poet, lyricist and frontman of Hawkwind, intermittently from 1972–1979. Calvert co-wrote Hawkwind's hit single "Silver Machine", which reached No. 3 in the UK singles chart. Although Lemmy sings on the single version, this is an overdub of a live recording taken at the Roundhouse in London, with Calvert on vocals. "They tried everyone else singing it except me", Lemmy later said. Calvert also directed Hawkwind's live opus, the Space Ritual Tour.
Calvert suffered from bipolar disorder, which often caused a fractious relationship with his fellow musicians. At one point he was sectioned under the Mental Health Act. Despite his sometimes debilitating mental health, Calvert remained a fiercely creative, driven and multi-talented artist. During periods away from Hawkwind duties, he worked on his solo career; his creative output including albums, stage plays, poetry, and a novel. His first solo album, "Captain Lockheed and the Starfighters", was released in 1974. The record is a concept album; an amalgam of music and theatre focused around the Lockheed bribery scandals. In 1975 Calvert won the Capital Radio poetry competition with his poem "Circle Line". Later in 1975, musician and producer Brian Eno produced and played on Calvert's second solo album, Lucky Leif and the Longships, also a concept album, this time focusing on the history of the US and that of the Vikings who discovered America long before Columbus.
As well as Michael Moorcock and Brian Eno, Calvert's collaborators included Arthur Brown, Steve Peregrin Took, Jim Capaldi, Steve Pond, Inner City Unit, Vivian Stanshall, Nektar, John Greaves, Adrian Wagner, Amon Düül II and, posthumously, Spirits Burning.
Aged 43, Calvert died of a heart attack in 1988 in Ramsgate, England and was buried in Minster Cemetery near Margate. His gravestone is engraved with the line "Love's not Time's fool", from William Shakespeare's Sonnet 116.

</doc>
<doc id="26174" url="http://en.wikipedia.org/wiki?curid=26174" title="Julio-Claudian family tree">
Julio-Claudian family tree

Around the start of the Common Era, the family trees of the gens Julia and the gens Claudia became intertwined into the Julio-Claudian family tree as a result of marriages and adoptions.
Descendancy of the emperors of the Julio-Claudian dynasty.
The Julio-Claudian dynasty was the first dynasty of Roman emperors. All emperors of that dynasty descended from Julii Caesares and/or from Claudii. Marriages between descendants of Sextus Julius Caesar I and Claudii had occurred from the late stages of the Roman Republic, but the intertwined Julio-Claudian family tree resulted mostly from adoptions and marriages in Imperial Rome's first decades. Note that descendancy of the Julii Caesares before the generation of Julius Caesar's grandfather is in part conjectural, but as presented by scholars.
By generation.
In the Julio-Claudian dynasty of Roman emperors the lineage of the Julii Caesares was separated from those of the Claudii up to Augustus' generation. The next generation had both Claudii with a Julia Caesaris as ancestor, as Claudii adopted into the Julii Caesares family. After Tiberius, the remaining three emperors of the dynasty had, outside adoptions, ancestors both in the Julian as the Claudian families.
Generation of Julius Caesar's grandfather.
Gaius Julius Caesar II and Lucius Julius Caesar II may have had Sextus Julius Caesar, the military tribune of 181 BC, as a common ancestor.
Generation of Julius Caesar's father.
This generation of Julii Caesares has two consuls: Sextus Julius Caesar III in 91 BC, and Lucius Julius Caesar III the next year. This generation has also two female descendants very close to the centers of power by their marriages: Julia Caesaris, the daughter Gaius Julius Caesar II was married to seven-times consul Gaius Marius, while Julia, the daughter of Lucius Julius Caesar II was married to the two-times consul and Roman dictator Lucius Cornelius Sulla, who had successfully challenged Marius' power. For ensuing generations, Gaius Julius Caesar the Elder, married to a consul's daughter, and Lucius Julius Caesar III proved to be quintessential ancestors of those who held Imperial power in the Julio-Claudian dynasty.
Julius Caesar's generation.
Following Sulla's example Julius Caesar's and Pompey's first marriages were with women of their own generation, later marrying women of a younger generation. After being betrothed to Cossutia, Julius Caesar's first wife was Cornelia Cinna Minor, the mother of Julia. The youngest of Julius Caesar's elder sisters married Marcus Atius: they were ancestors of all the Julio-Claudian emperors, apart from Tiberius.
This is also the generation of Mark Antony's parents. Mark Antony's mother Julia Antonia was the daughter of Lucius Julius Caesar III: she was a Julia Caesaris ancestor to the last three Claudian emperors of the Julio-Claudian dynasty.
Generation of Julius Caesar's daughter.
By this time marriages with a political agenda among the powerful families were in full swing, however not yet between Julii Caesares and Claudii. Pompey married Julius Caesar's daughter Julia. Julius Caesar's second wife Pompeia, possibly a great-granddaughter of Lucius Julius Caesar II, was a granddaughter of Sulla. His third wife Calpurnia is said to be younger than his daughter. His son Caesarion resulted from his relation with Cleopatra.
Atia Balba Caesonia, the daughter of Julius Caesar's sister, married Gaius Octavius: they became the parents of the first emperor of the Julio-Claudian dynasty, then still called Octavianus. Their daughter Octavia the Younger became an ancestor to the last three emperors of that dynasty. In this generation Mark Antony had children by, among others, Antonia Hybrida Minor, and Fulvia.
Generation of the Octavias.
The Claudii were a powerful gens with consuls and other high ranking politicians in several of its families across several generations. In this generation the first marriages between Claudii and descendants of the Julii Caesares took place. This however didn't mean yet that the dynastic family trees of both gentes got merged into a single one: that didn't happen until the adoption of Claudii by (adopted) Julii Caesares in the generations to come.
Octavia the Younger's first husband was a Claudius from the Marcelli family. Clodia Pulchra, descending from Claudii, became the first wife of Octavian, who by then was adopted in the Julii Caesares family by the testament of his uncle Julius Caesar. After her first husband's death, Octavia married Mark Antony, who besides the offspring of his first three marriages had had children by Cleopatra.
 
Antonia Maior's generation.
Octavianus, becoming Augustus the first Roman emperor, married Scribonia who gave him a daughter (Julia the Elder). His last marriage was with Livia, a Claudia who had been married to a Claudius. Their son Tiberius, by birth a Claudius, was later adopted by Augustus, thus, like his stepfather Augustus, becoming one of the Julii Caesares by adoption.
Antonia Minor's generation.
Antonia Minor's husband Nero Claudius Drusus, a.k.a. Drusus the Elder, was a Claudian like his brother emperor Tiberius: they were the sons of Tiberius Claudius Nero, the praetor of 42 BC.
Agrippina the Elder's generation.
Without son, Augustus had adopted his grandsons (by his only daughter Julia) Gaius, Lucius and Postumus, and his stepson Tiberius, in order to ensure an heir and successor. Around the time of his death only Tiberius remained and he became the next emperor. Tiberius, a Claudius by birth had become one of the Julii Caesares by adoption: from this moment this first dynasty of Roman emperors was both Julian and Claudian. The further emperors of this dynasty had both Julian and Claudian ancestors.
Agrippina the Younger's generation.
Caligula was the last emperor adopted into the family of the Julii Caesares. He was a Claudius by descendance, although he had Julii Caesares among his ancestors, both from his mother's as his father's side.
Most marriages remained childless and many potential successors in the dynasty were eliminated after rampant accusations.
Claudius, the fourth emperor of the Julio-Claudian dynasty, was a brother to Caligula's father Germanicus. He belonged to the gens Claudia with, from his mother's side, Julian ancestors.
Poppaea Sabina's generation.
Nero, the last emperor of the dynasty, was by birth a Domitius with as well Julian ancestors (from both his mother's as his father's side), as Claudian (from his mother's side). He became a Claudian himself, by adoption by his stepfather emperor Claudius, a brother to his grandfather from his mother's side, or, from his father's side, a son of his grandmother's sister.

</doc>
<doc id="26199" url="http://en.wikipedia.org/wiki?curid=26199" title="Roald Amundsen">
Roald Amundsen

Roald Engelbregt Gravning Amundsen (]; 16 July 1872 – c. 18 June 1928) was a Norwegian explorer of polar regions. He led the Antarctic expedition (1910–12) that was the first to reach the South Pole, on 14 December 1911. In 1926 he was the first expedition leader to be recognized without dispute as having reached the North Pole. He is also known as having the first expedition to traverse the Northwest Passage (1903–06) in the Arctic. 
He disappeared in June 1928 in the Arctic while taking part in a rescue mission by plane. Amundsen was among key expedition leaders, including Douglas Mawson, Robert Falcon Scott, and Ernest Shackleton, during the Heroic Age of Antarctic Exploration.
Early life.
Amundsen was born to a family of Norwegian shipowners and captains in Borge, between the towns Fredrikstad and Sarpsborg. His parents were Jens Amundsen and Hanna Sahlqvist. Roald was the fourth son in the family. His mother wanted him to avoid the family maritime trade and encouraged him to become a doctor, a promise that Amundsen kept until his mother died when he was aged 21. He promptly quit university for a life at sea.
Amundsen had hidden a lifelong desire inspired by Fridtjof Nansen's crossing of Greenland in 1888 and Franklin's lost expedition. He decided on a life of intense exploration of wilderness places.
Polar treks.
Belgian Antarctic Expedition (1897–99).
Amundsen joined the Belgian Antarctic Expedition (1897–99) as first mate. This expedition, led by Adrien de Gerlache using the ship the RV "Belgica", became the first expedition to winter in Antarctica. The "Belgica", whether by mistake or design, became locked in the sea ice at 70°30′S off Alexander Island, west of the Antarctic Peninsula. The crew endured a winter for which they were poorly prepared. By Amundsen's own estimation, the doctor for the expedition, the American Frederick Cook, probably saved the crew from scurvy by hunting for animals and feeding the crew fresh meat. In cases where citrus fruits are lacking, fresh meat from animals that make their own vitamin C (which most do) contains enough of the vitamin to prevent scurvy, and even partly treat it. This was an important lesson for Amundsen's future expeditions.
Northwest Passage (1903–1906).
In 1903, Amundsen led the first expedition to successfully traverse Canada's Northwest Passage between the Atlantic and Pacific oceans. He planned a small expedition of six men in a 45-ton fishing vessel, "Gjøa," in order to have flexibility. His ship had relatively shallow draft. His technique was to use a small ship and hug the coast. Amundsen had the ship outfitted with a small gasoline engine. They traveled via Baffin Bay, the Parry Channel and then south through Peel Sound, James Ross Strait, Simpson Strait and Rae Strait. They spent two winters (1903-1904 and 1904-1905) at King William Island in the harbor of what is today Gjoa Haven, Nunavut, Canada. During this time, Amundsen and the crew learned from the local Netsilik Inuit people about Arctic survival skills, which he found invaluable in his later expedition to the South Pole. For example, he learned to use sled dogs for transportation of goods and to wear animal skins in lieu of heavy, woolen parkas, which could not deter cold when wet.
Leaving Gjoa Haven, he sailed west and passed Cambridge Bay, which had been reached from the west by Richard Collinson in 1852. Continuing to the south of Victoria Island, the ship cleared the Canadian Arctic Archipelago on 17 August 1905. It had to stop for the winter before going on to Nome on the Alaska District's Pacific coast. Five hundred miles (800 km) away, Eagle City, Alaska, had a telegraph station; Amundsen traveled there (and back) overland to wire a success message (collect) on 5 December 1905. His team reached Nome in 1906. Because the water along the route was sometimes as shallow as 3 ft, a larger ship could not have made the voyage.
At this time that Amundsen learned that Norway had formally become independent of Sweden and had a new king. The explorer sent the new King Haakon VII news that his traversing the Northwest Passage "was a great achievement for Norway". He said he hoped to do more and signed it "Your loyal subject, Roald Amundsen." The crew returned to Oslo in November 1906, after almost 3.5 years abroad. It took until 1972 to have the "Gjøa" returned to Norway. After a 45-day trip from San Francisco on a bulk carrier, the "Gjøa" was placed in her current location on land, outside the Fram Museum in Oslo.
South Pole Expedition (1910–12).
Amundsen planned next to take an expedition to the North Pole and explore the Arctic Basin. Finding it difficult to raise funds, when he heard in 1909 that the Americans Frederick Cook and Robert Peary had claimed to reach the North Pole as a result of two different expeditions, he decided to reroute to Antarctica. He was not clear about his intentions, and the Englishman Robert F. Scott and the Norwegian supporters felt misled. Scott was planning his own expedition to the South Pole that year. Using the ship "Fram" ("Forward"), earlier used by Fridtjof Nansen, Amundsen left Oslo for the south on 3 June 1910. At Madeira, Amundsen alerted his men that they would be heading to Antarctica, and sent a telegram to Scott, notifying him simply: "BEG TO INFORM YOU FRAM PROCEEDING ANTARCTIC--AMUNDSEN."
Nearly six months later, the expedition arrived at the eastern edge of the Ross Ice Shelf (then known as "the Great Ice Barrier"), at a large inlet called the Bay of Whales, on 14 January 1911. Amundsen established his base camp there, calling it "Framheim." Amundsen eschewed the heavy wool clothing worn on earlier Antarctic attempts in favour of adopting Inuit-style furred skins.
Using skis and dog sleds for transportation, Amundsen and his men created supply depots at 80°, 81° and 82° South on the Barrier, along a line directly south to the Pole. Amundsen also planned to kill some of his dogs on the way and use them as a source for fresh meat. A small group, including Hjalmar Johansen, Kristian Prestrud and Jørgen Stubberud, set out on 8 September 1911, but had to abandon their trek due to extreme temperatures. The painful retreat caused a quarrel within the group, and Amundsen sent Johansen and the other two men to explore King Edward VII Land.
A second attempt, with a team made up of Olav Bjaaland, Helmer Hanssen, Sverre Hassel, Oscar Wisting, and Amundsen, departed base camp on 19 October 1911. They took four sledges and 52 dogs. Using a route along the previously unknown Axel Heiberg Glacier, they arrived at the edge of the Polar Plateau on 21 November after a four-day climb. On 14 December 1911, the team of five, with 16 dogs, arrived at the Pole (90° 0′ S). They arrived 33–34 days before Scott’s group. Amundsen named their South Pole camp "Polheim," meaning "Home on the Pole." Amundsen renamed the Antarctic Plateau as King Haakon VII’s Plateau. They left a small tent and letter stating their accomplishment, in case they did not return safely to Framheim.
The team returned to Framheim on 25 January 1912, with 11 surviving dogs. They made their way off the continent and to Hobart, Australia, where Amundsen publicly announced his success on 7 March 1912. He telegraphed news to backers.
Amundsen's expedition benefited from his careful preparation, good equipment, appropriate clothing, a simple primary task, an understanding of dogs and their handling, and the effective use of skis. In contrast to the misfortunes of Scott’s team, Amundsen’s trek proved rather smooth and uneventful.
In Amundsen's own words:
I may say that this is the greatest factor—the way in which the expedition is equipped—the way in which every difficulty is foreseen, and precautions taken for meeting or avoiding it. Victory awaits him who has everything in order—luck, people call it. Defeat is certain for him who has neglected to take the necessary precautions in time; this is called bad luck.—from "The South Pole," by Roald Amundsen
Amundsen wrote about the expedition in "The South Pole: An Account of the Norwegian Antarctic Expedition in the 'Fram,' 1910–12" (1912).
Northeast Passage (1918–20).
In 1918, Amundsen began an expedition with a new ship "Maud", lasted until 1925. "Maud" sailed west to east through the Northeast Passage, now called the "Northern Route" (1918–20).
With him on this expedition were Oscar Wisting and Helmer Hanssen, both of whom had been part of the team to reach the South Pole. In addition, Henrik Lindstrøm was included as a cook. He suffered a stroke and was so physically reduced that he could not participate.
The goal of the expedition was to explore the unknown areas of the Arctic Ocean, strongly inspired by Fridtjof Nansen's earlier expedition with "Fram". The plan was to sail along the coast of Siberia and go into the ice farther to the north and east than Nansen had. In contrast to Amundsen's earlier expeditions, this was expected to yield more material for academic research, and he carried the geophysicist Harald Sverdrup on board.
The voyage was to the northeasterly direction over the Kara Sea. Amundsen planned to freeze the "Maud" into the polar ice cap and drift towards the North Pole (as Nansen had done with the "Fram"), and he did so off Cape Chelyuskin. But, the ice became so thick that the ship was unable to break free, although it was designed for such a journey in heavy ice. In September 1919, the crew got the ship loose from the ice, but it froze again after eleven days somewhere between the New Siberian Islands and Wrangel Island.
During this time, Amundsen participated little in the work outdoors, such as sleigh rides and hunting, because he had suffered numerous injuries. He had a broken arm and had been attacked by polar bears. Hanssen and Wisting, along with two other men, embarked on an expedition by dog sled to Nome, Alaska, more than 1,000 kilometres away. But they found that the ice was not frozen solid in the Bering Strait, and it could not be crossed. They sent a telegram from Anadyr to signal their location.
After two winters frozen in the ice, without having achieved the goal of drifting over the North Pole, Amundsen decided to go to Nome to repair the ship and buy provisions. Several of the crew ashore there, including Hanssen, did not return on time to the ship. Amundsen considered Hanssen to be in breach of contract, and dismissed him from the crew.
During the third winter, "Maud" was frozen in the western Bering Strait. She finally became free and the expedition sailed south, reaching Seattle, Washington in the US Pacific Northwest in 1921 for repairs. Amundsen returned to Norway, needing to put his finances in order. He took with him two young indigenous girls, the adopted four-year-old Kakonita and her companion Camilla. When Amundsen went bankrupt two years later, however, he sent the girls to be cared for by Camilla's father, who lived in eastern Russia.
In June 1922 Amundsen returned to "Maud", which had been sailed to Nome. He decided to shift from the planned naval expedition to aerial ones, and arranged to charter a plane. He divided the expedition team in two: one part was to survive the winter and prepare for an attempt to fly over the pole. This part was led by Amundsen. The second team on "Maud", under the command of Wisting, was to resume the original plan to drift over the North Pole in the ice. The ship drifted in the ice for three years east of the New Siberian Islands, never reaching the North Pole. It was finally seized by Amundsen's creditors as collateral for his mounting debt.
The attempt to fly over the Pole failed, too. Amundsen and Oskar Omdal, of the Royal Norwegian Navy, tried to fly from Wainwright, Alaska, to Spitsbergen across the North Pole. When their aircraft was damaged, they abandoned the journey. To raise additional funds, Amundsen traveled around the United States in 1924 on a lecture tour. 
Although he was unable to reach the North Pole, the scientific results of the expedition, mainly the work of Sverdrup, have proven to be of considerable value. Many of these carefully collected scientific data were lost during the ill-fated journey of Peter Tessem and Paul Knutsen, two crew members sent on a mission by Amundsen. The scientific materials were later retrieved by Russian scientist Nikolay Urvantsev from where they had been abandoned on the shores of the Kara Sea.
Reaching the North Pole.
In 1925 accompanied by Lincoln Ellsworth, pilot Hjalmar Riiser-Larsen, and three other team members, Amundsen took two Dornier Do J flying boats, the N-24 and N-25, to 87° 44′ north. It was the northernmost latitude reached by plane up to that time. The aircraft landed a few miles apart without radio contact, yet the crews managed to reunite. The N-24 was damaged. Amundsen and his crew worked for more than three weeks to clean up an airstrip to take off from ice. They shovelled 600 tons of ice while consuming only one pound (400 g) of daily food rations. In the end, six crew members were packed into the N-25. In a remarkable feat, Riiser-Larsen took off, and they barely became airborne over the cracking ice. They returned triumphant when everyone thought they had been lost forever.
In 1926 Amundsen and 15 other men (including Ellsworth, Riiser-Larsen, Oscar Wisting, and the Italian air crew led by aeronautical engineer Umberto Nobile) made the first crossing of the Arctic in the airship "Norge," designed by Nobile. They left Spitzbergen on 11 May 1926, and they landed in Alaska two days later. The three previous claims to have arrived at the North Pole: Frederick Cook in 1908; Robert Peary in 1909; and Richard E. Byrd in 1926 (just a few days before the "Norge") are all disputed, as being either of dubious accuracy or outright fraud. If their claims are false, the crew of the "Norge" would be the first verified explorers to have reached the North Pole. If the "Norge" expedition was the first to the North Pole, Amundsen and Oscar Wisting were the first men to reach each geographical pole, by ground or by air.
Disappearance and death.
Amundsen disappeared with five crew on 18 June 1928 while flying on a rescue mission in the Arctic. His team included Norwegian pilot Leif Dietrichson, French pilot René Guilbaud, and three more Frenchmen. They were seeking missing members of Nobile's crew, whose new airship "Italia" had crashed while returning from the North Pole. Afterward, a wing-float and bottom gasoline tank from Amundsen's French Latham 47 flying boat, adapted as a replacement wing-float, were found near the Tromsø coast. It is believed that the plane crashed in fog in the Barents Sea, and that Amundsen and his crew were killed in the crash, or died shortly afterward. The search for Amundsen and team was called off in September 1928 by the Norwegian Government and the bodies were never found.
In 2004 and in late August 2009, the Royal Norwegian Navy used the unmanned submarine "Hugin 1000" to search for the wreckage of Amundsen's plane. The searches focused on a 40 sqmi area of the sea floor, and were documented by the German production company ContextTV. They found nothing from the Amundsen flight.
Legacy.
A number of places have been named after Amundsen:
Several ships are named after him:
Other tributes include:
European-Inuit descendant claims.
Some Inuit people in Gjøa Haven with European ancestry have claimed to be descendants of Amundsen (or one of his six crew, whose names have not remained as well known), from the period of their extended winter stay on King Williams Island from 1903 to 1905. Accounts by members of the expedition told of their relations with Inuit women, and historians have speculated that Amundsen might also have taken a partner, although he wrote a warning against this. Specifically, half brothers Bob Konona and Paul Ikuallaq say that their father Luke Ikuallaq (b. 1904) told them on his deathbed that he was the son of Amundsen. Konona said that their father Ikuallaq was left out on the ice to die after his birth, as his European ancestry made him illegitimate to the Inuit, threatening their community. His Inuit grandparents saved him. In 2012, Y-DNA analysis, with the families' permission, showed that Ikuallaq (and his sons) was not a match to the direct male line of Amundsen. Not all descendants claiming European ancestry have been tested for a match to Amundsen, nor has there been a comparison of Ikuallaq's DNA to that of other European members of Amundsen's crew.
Notes.
Notes
Citations
Sources
External links.
Works by Amundsen

</doc>
<doc id="26229" url="http://en.wikipedia.org/wiki?curid=26229" title="Riboflavin">
Riboflavin

Riboflavin (vitamin B2) is part of the vitamin B group. It is the central component of the cofactors FAD and FMN and as such required for a variety of flavoprotein enzyme reactions including activation of other vitamins. It was formerly known as vitamin G.
Riboflavin is a yellow-orange solid substance with poor solubility in water. It is best known visually as it imparts the color to vitamin supplements and the yellow color to the urine of persons taking it.
The name "riboflavin" comes from "ribose" (the sugar whose reduced form, ribitol, forms part of its structure) and "flavin", the ring-moiety which imparts the yellow color to the oxidized molecule (from Latin "flavus", "yellow"). The reduced form, which occurs in metabolism along with the oxidized form, is colorless.
Function.
The active forms Flavin mononucleotide (FMN) and flavin adenine dinucleotide (FAD) function as cofactors for a variety of flavoproteine enzyme reactions:
For the molecular mechanism of action see main articles Flavin mononucleotide (FMN) and flavin adenine dinucleotide (FAD)
Nutrition.
Food sources.
Sources of riboflavin are milk, cheese, leaf vegetables, liver, kidneys, legumes, yeast, mushrooms, and almonds.
Yeast extract is considered to be exceptionally rich in vitamin B2. Cereals contain relatively low concentrations of flavins, but are important sources in those parts of the world where cereals constitute the staple diet.
The milling of cereals results in considerable loss (up to 60%) of vitamin B2, so white flour is enriched in some countries such as USA by addition of the vitamin. The enrichment of bread and ready-to-eat breakfast cereals contributes significantly to the dietary supply of vitamin B2. Polished rice is not usually enriched, because the vitamin’s yellow color would make the rice visually unacceptable to the major rice-consumption populations. However, most of the flavin content of whole brown rice is retained if the rice is steamed (parboiled) prior to milling. This process drives the flavins in the germ and aleurone layers into the endosperm. Free riboflavin is naturally present in foods along with protein-bound FMN and FAD. Bovine milk contains mainly free riboflavin, with a minor contribution from FMN and FAD. In whole milk, 14% of the flavins are bound noncovalently to specific proteins. Egg white and egg yolk contain specialized riboflavin-binding proteins, which are required for storage of free riboflavin in the egg for use by the developing embryo.
It is used in baby foods, breakfast cereals, pastas, sauces, processed cheese, fruit drinks, vitamin-enriched milk products, and some energy drinks. It is difficult to incorporate riboflavin into many liquid products because it has poor solubility in water, hence the requirement for riboflavin-5'-phosphate (E101a), a more soluble form of riboflavin. Riboflavin is also used as a food coloring and as such is designated in Europe as the E number E101.
Riboflavin is generally stable during the heat processing and normal cooking of foods if light is excluded. The alkaline conditions in which riboflavin is unstable are rarely encountered in foodstuffs. Riboflavin degradation in milk can occur slowly in dark during storage in the refrigerator.
Dietary reference intakes.
The latest (1998) RDA recommendations for vitamin B2 are similar to the 1989 RDA, which for adults, suggested a minimum intake of 1.2 mg for persons whose caloric intake may be > 2,000 Kcal. The current RDAs for riboflavin for adult men and women are 1.3 mg/day and 1.1 mg/day, respectively; the estimated average requirement for adult men and women are 1.1 mg and 0.9 mg, respectively. Recommendations for daily riboflavin intake increase with pregnancy and lactation to 1.4 mg and 1.6 mg, respectively (1in advanced). For infants, the RDA is 0.3-0.4 mg/day and for children it is 0.6-0.9 mg/day.
Deficiency.
Signs and symptoms.
In humans.
Riboflavin deficiency (also called ariboflavinosis) results in stomatitis including painful red tongue with sore throat, chapped and fissured lips (cheilosis), and inflammation of the corners of the mouth (angular stomatitis). There can be oily scaly skin rashes on the scrotum, vulva, philtrum of the lip, or the nasolabial folds. The eyes can become itchy, watery, bloodshot and sensitive to light. Due to interference with iron absorption, riboflavin deficiency results in an anemia with normal cell size and normal hemoglobin content (i.e. normochromic normocytic anemia). This is distinct from anemia caused by deficiency of folic acid (B9) or cyanocobalamin (B12), which causes anemia with large blood cells (megaloblastic anemia). Deficiency of riboflavin during pregnancy can result in birth defects including congenital heart defects and limb deformities.
The stomatitis symptoms are similar to those seen in pellagra, which is caused by niacin (B3) deficiency. Therefore, riboflavin deficiency is sometimes called "pellagra sine pellagra" (pellagra without pellagra), because it causes stomatitis but not widespread peripheral skin lesions characteristic of niacin deficiency.
Riboflavin deficiency has been implicated in cancer, and has been noted to prolong recovery from malaria, despite preventing growth of plasmodium.
In other animals.
In other animals, riboflavin deficiency results in lack of growth, failure to thrive, and eventual death. Experimental riboflavin deficiency in dogs results in growth failure, weakness, ataxia, and inability to stand. The animals collapse, become comatose, and die. During the deficiency state, dermatitis develops together with hair loss. Other signs include corneal opacity, lenticular cataracts, hemorrhagic adrenals, fatty degeneration of the kidney and liver, and inflammation of the mucous membrane of the gastrointestinal tract. Post-mortem studies in rhesus monkeys fed a riboflavin-deficient diet revealed about one-third the normal amount of riboflavin was present in the liver, which is the main storage organ for riboflavin in mammals. Riboflavin deficiency in birds results in low egg hatch rates.
Diagnosis.
Overt clinical signs are rarely seen among inhabitants of the developed countries. The assessment of Riboflavin status is essential for confirming cases with unspecific symptoms where deficiency is suspected.
Causes.
Riboflavin is continuously excreted in the urine of healthy individuals, making deficiency relatively common when dietary intake is insufficient. Riboflavin deficiency is usually found together with other nutrient deficiencies, particularly of other water-soluble vitamins.
A deficiency of riboflavin can be primary - poor vitamin sources in one's daily diet - or secondary, which may be a result of conditions that affect absorption in the intestine, the body not being able to use the vitamin, or an increase in the excretion of the vitamin from the body.
Subclinical deficiency has also been observed in women taking oral contraceptives, in the elderly, in people with eating disorders, chronic alcoholism and in diseases such as HIV, inflammatory bowel disease, diabetes and chronic heart disease.
Phototherapy to treat jaundice in infants can cause increased degradation of riboflavin, leading to deficiency if not monitored closely.
Treatment.
Treatment involves a diet which includes an adequate amount of riboflavin usually in form of commercially available supplements.
Medical uses.
Riboflavin has been used in several clinical and therapeutic situations. For over 30 years, riboflavin supplements have been used as part of the phototherapy treatment of neonatal jaundice. The light used to irradiate the infants breaks down not only bilirubin, the toxin causing the jaundice, but also the naturally occurring riboflavin within the infant's blood, so extra supplementation is necessary.
One clinical trial found that high dose riboflavin appears to be useful alone or along with beta-blockers in the prevention of migraine. A dose of 400 mg daily has been used effectively in the prophylaxis of migraines, especially in combination with a daily supplement of magnesium citrate 500 mg and, in some cases, a supplement of coenzyme Q10. However, two other clinical studies have failed to find any significant results for the effectiveness of B2 as a treatment for migraine.
Riboflavin in combination with UV light has been shown to be effective in reducing the ability of harmful pathogens found in blood products to cause disease. When UV light is applied to blood products containing riboflavin, the nucleic acids in pathogens are damaged, rendering them unable to replicate and cause disease. Riboflavin and UV light treatment has been shown to be effective for inactivating pathogens in platelets and plasma, and is under development for application to whole blood. Because platelets and red blood cells do not contain a nucleus (i.e. they have no DNA to be damaged) the technique is well-suited for destroying nucleic acid containing pathogens (including viruses, bacteria, parasites, and white blood cells) in blood products.
Recently, riboflavin has been used in a new treatment to slow or stop the progression of the corneal disorder keratoconus. This is called corneal collagen cross-linking. In corneal crosslinking, riboflavin drops are applied to the patient’s corneal surface. Once the riboflavin has penetrated through the cornea, ultraviolet A light therapy is applied. This induces collagen crosslinking, which increases the tensile strength of the cornea. The treatment has been shown in several studies to stabilize keratoconus.
Treatment for Brown vialetto van laere, fazio londe, and the myopathic form of adult onset coenzyme q10 deficiency.
Industrial uses.
Because riboflavin is fluorescent under UV light, dilute solutions (0.015-0.025% w/w) are often used to detect leaks or to demonstrate coverage in an industrial system such a chemical blend tank or bioreactor. (See the ASME BPE section on Testing and Inspection for additional details.)
Toxicity.
In humans, there is no evidence for riboflavin toxicity produced by excessive intakes, as its low solubility keeps it from being absorbed in dangerous amounts within the digestive tract. Even when 400 mg of riboflavin per day was given orally to subjects in one study for three months to investigate the efficacy of riboflavin in the prevention of migraine headache, no short-term side effects were reported. Although toxic doses can be administered by injection, any excess at nutritionally relevant doses is excreted in the urine, imparting a bright yellow color when in large quantities.
Industrial synthesis.
Various biotechnological processes have been developed for industrial scale riboflavin biosynthesis using different microorganisms, including filamentous fungi such as "Ashbya gossypii", "Candida famata" and "Candida flaveri", as well as the bacteria "Corynebacterium ammoniagenes" and "Bacillus subtilis". The latter organism has been genetically modified to both increase the bacteria's production of riboflavin and to introduce an antibiotic (ampicillin) resistance marker, and is now successfully employed at a commercial scale to produce riboflavin for feed and food fortification purposes. The chemical company BASF has installed a plant in South Korea, which is specialized on riboflavin production using "Ashbya gossypii". The concentrations of riboflavin in their modified strain are so high, that the mycelium has a reddish/brownish color and accumulates riboflavin crystals in the vacuoles, which will eventually burst the mycelium. Riboflavin is sometimes overproduced, possibly as a protective mechanism, by certain bacteria in the presence of high concentrations of hydrocarbons or aromatic compounds. One such organism is "Micrococcus luteus" (American Type Culture Collection strain number ATCC 49442), which develops a yellow color due to production of riboflavin while growing on pyridine, but not when grown on other substrates, such as succinic acid.
Research.
An animal model of riboflavin kinase deficiency has been identified. Since riboflavin cannot be converted into the catalytically active cofactors without this enzyme, a vitamin deficiency syndrome is generated in the model.
History.
Vitamin B was originally considered to have two components, a heat-labile vitamin B1 and a heat-stable vitamin B2. In the 1920s, vitamin B2 was thought to be the factor necessary for preventing pellagra. In 1923, Paul Gyorgy in Heidelberg was investigating egg-white injury in rats; the curative factor for this condition was called vitamin H (which is now called biotin or vitamin B7). Since both pellagra and vitamin H deficiency were associated with dermatitis, Gyorgy decided to test the effect of vitamin B2 on vitamin H deficiency in rats. He enlisted the service of Wagner-Jauregg in Kuhn’s laboratory. In 1933, Kuhn, Gyorgy, and Wagner found that thiamin-free extracts of yeast, liver, or rice bran prevented the growth failure of rats fed a thiamin-supplemented diet.
Further, the researchers noted that a yellow-green fluorescence in each extract promoted rat growth, and that the intensity of fluorescence was proportional to the effect on growth. This observation enabled them to develop a rapid chemical and bioassay to isolate the factor from egg white in 1933, they called it Ovoflavin. The same group then isolated the same preparation (a growth-promoting compound with yellow-green fluorescence) from whey using the same procedure (lactoflavin). In 1934 Kuhn’s group identified the structure of so-called flavin and synthesized vitamin B2.

</doc>
<doc id="26277" url="http://en.wikipedia.org/wiki?curid=26277" title="Real property">
Real property

In English common law, real property, real estate, realty, or immovable property is any subset of land that has been legally defined and the improvements to it have been made by human efforts: buildings, machinery, wells, dams, ponds, mines, canals, roads, etc. Real property and personal property are the two main subunits of property in English Common Law.
In countries with personal ownership of real property, civil law protects the status of real property in real-estate markets, where estate agents work in the market of buying and selling real estate. Scottish civil law calls real property "heritable property", and in French-based law, it is called "immobilier".
Historical background.
The word "real" ultimately derives from Latin "res" ("thing") and was used in Middle English to mean "relating to things, especially real property".
In common law, real property was property that could be protected by some form of real action, in contrast to personal property, where a plaintiff would have to resort to another form of action. As a result of this formalist approach, some things the common law deems to be land would not be classified as such by most modern legal systems, for example an advowson (the right to nominate a priest) was real property. By contrast the rights of a leaseholder originate in personal actions and so the common law originally treated a leasehold as part of personal property.
The law now broadly distinguishes between real property (land and anything affixed to it) and personal property (everything else, e.g., clothing, furniture, money). The conceptual difference was between immovable property, which would transfer title along with the land, and movable property, which a person would retain title to.
In modern legal systems derived from English common law, classification of property as real or personal may vary somewhat according to jurisdiction or, even within jurisdictions, according to purpose, as in defining whether and how the property may be taxed.
Bethell (1998) contains much historical information on the historical evolution of real property and property rights.
Identification of real property.
To be of any value a claim to any property must be accompanied by a verifiable and legal property description. Such a description usually makes use of natural or manmade boundaries such as seacoasts, rivers, streams, the crests of ridges, lakeshores, highways, roads, and railroad tracks, and/or purpose-built markers such as cairns, surveyor's posts, fences, official government surveying marks (such as ones affixed by the U.S. Geodetic Survey (USGS)), and so forth. In many cases, a description refers to one or more lots on a plat, a map of property boundaries kept in public records.
Estates and ownership interests defined.
The law recognizes different sorts of interests, called estates, in real property. The type of estate is generally determined by the language of the 5 deed, lease, bill of sale, will, land grant, etc., through which the estate was acquired. Estates are distinguished by the varying property rights that vest in each, and that determine the duration and transferability of the various estates. A party enjoying an estate is called a "tenant."
Some important types of estates in land include:
A tenant enjoying an undivided estate in some property after the termination of some estate of limited term, is said to have a "future interest." Two important types of future interests are:
Estates may be held jointly as joint tenants with rights of survivorship or as tenants in common. The difference in these two types of joint ownership of an estate in land is basically the inheritability of the estate and the shares of interest that each tenant owns.
In a joint tenancy with rights of survivorship deed, or JTWROS, the death of one tenant means that the surviving tenant(s) become the sole owner(s) of the estate. Nothing passes to the heirs of the deceased tenant. In some jurisdictions, the specific words "with right of survivorship" must be used, or the tenancy will assumed to be tenants in common without rights of survivorship. The co-owners always take a JTWROS deed in equal shares, so each tenant must own an equal share of the property regardless of his/her contribution to purchase price. If the property is someday sold or subdivided, the proceeds must be distributed equally with no credits given for any excess than any one co-owner may have contributed to purchase the property.
The death of a co-owner of a tenants in common (TIC) deed will have a heritable portion of the estate in proportion to his ownership interest which is presumed to be equal among all tenants unless otherwise stated in the transfer deed. However, if TIC property is sold or subdivided, in some States, Provinces, etc., a credit can be automatically made for unequal contributions to the purchase price (unlike a partition of a JTWROS deed).
Real property may be owned jointly with several tenants, through devices such as the condominium, housing cooperative, and building cooperative.
Bundle of Rights:
Real property is unique due to the fact that there are multiple "rights" associated with each piece of property. For example, most U.S. jurisdictions recognized the following rights: right to sell, right to lease, right to acquire minerals/gas/oil/etc. within the land, right to use, right to possess, right to develop, etc. These multiple rights are important because the owner of the real property can generally do what he/she chooses with each right. For example, the owner could choose to keep all the rights but lease the right to dig for oil to an oil company. Or the owner could choose to keep all the rights but lease the property to a tenant. In other words, the owner can elect to keep and/or lease and/or sell the rights to his/her land.
Other Ownership types:
Jurisdictional peculiarities.
In the law of almost every country, the state is the ultimate owner of all land under its jurisdiction, because it is the sovereign, or supreme lawmaking authority. Physical and corporate persons do not have allodial title; they do not own land but only enjoy estates in the land, also known as "equitable interests."
Australia and New Zealand.
In many countries the Torrens title system of real estate ownership is managed and guaranteed by the government and replaces cumbersome tracing of ownership. The Torrens title system operates on the principle of "title by registration" (i.e. the indefeasibility of a registered interest) rather than "registration of title." The system does away with the need for a chain of title (i.e. tracing title through a series of documents) and does away with the conveyancing costs of such searches. The State guarantees title and is usually supported by a compensation scheme for those who lose their title due to the State's operation. It has been in practice in all Australian states and in New Zealand since between 1858 and 1875, has more recently been extended to strata title, and has been adopted by many states, provinces and countries, and in modified form in 9 states of the USA.
United Kingdom.
In the United Kingdom, The Crown is held to be the ultimate owner of all real property in the realm. This fact is material when, for example, property has been disclaimed by its erstwhile owner, in which case the law of escheat applies. In some other jurisdictions (not including the United States), real property is held absolutely.
England and Wales.
English law has retained the common law distinction between real property and personal property, whereas the civil law distinguishes between "movable" and "immovable" property. In English law, real property is not confined to the ownership of property and the buildings sited thereon – often referred to as "land." Real property also includes many legal relationships between individuals or owners of land that are purely conceptual. One such relationship is the easement, where the owner of one property has the right to pass over a neighboring property. Another is the various "incorporeal hereditaments," such as "profits-à-prendre", where an individual may have the right to take crops from land that is part of another's estate.
English law retains a number of forms of property which are largely unknown in other common law jurisdictions such as the advowson, chancel repair liability and lordships of the manor. These are all classified as real property, as they would have been protected by real actions in the early common law.
USA.
Each U.S. State except Louisiana has its own laws governing real property and the estates therein, grounded in the common law. In Arizona, real property is generally defined as land and the things permanently attached to the land. Things that are permanently attached to the land, which also can be referred to as "improvements", include homes, garages, and buildings. Manufactured homes can obtain an affidavit of affixture.
Economic aspects of real property.
Land use, land valuation, and the determination of the incomes of landowners, are among the oldest questions in economic theory. Land is an essential input (factor of production) for agriculture, and agriculture is by far the most important economic activity in pre-industrial societies. With the advent of industrialization, important new uses for land emerge, as sites for factories, warehouses, offices, and urban agglomerations. Also, the value of real property taking the form of man-made structures and machinery increases relative to the value of land alone. The concept of real property eventually comes to encompass effectively all forms of tangible fixed capital. with the rise of extractive industries, real property comes to encompass natural capital. With the rise of tourism and leisure, real property comes to include scenic and other amenity values.
Starting in the 1960s, as part of the emerging field of law and economics, economists and legal scholars began to study the property rights enjoyed by tenants under the various estates, and the economic benefits and costs of the various estates. This resulted in a much improved understanding of the:
For an introduction to the economic analysis of property law, see Shavell (2004), and Cooter and Ulen (2003). For a collection of related scholarly articles, see Epstein (2007). Ellickson (1993) broadens the economic analysis of real property with a variety of facts drawn from history and ethnography.
References and further reading.
</dl>

</doc>
<doc id="26287" url="http://en.wikipedia.org/wiki?curid=26287" title="Roy Jenkins">
Roy Jenkins

Roy Harris Jenkins, Baron Jenkins of Hillhead, OM, PC (11 November 1920 – 5 January 2003) was a British politician and writer.
The son of a Welsh coal miner, Roy Jenkins later became a union official and Labour MP. He also served with distinction in World War II. Elected to Parliament as a Labour member in 1948, he served in several major posts in Harold Wilson's First Government. As Home Secretary from 1965–1967, he sought to build what he described as "a civilised society", with measures such as the effective abolition in Britain of capital punishment and theatre censorship, the decriminalisation of homosexuality, relaxing of divorce law, suspension of birching and the legalisation of abortion. As Chancellor of the Exchequer from 1967–1970, he pursued a tight fiscal policy. On 8 July 1970, he was elected Deputy Leader of the Labour Party, but resigned in 1972 because he supported entry to the Common Market, while the party opposed it.
When Wilson re-entered government in 1974 Jenkins returned to the Home Office, but increasingly disenchanted by the leftward swing of the Labour Party, he chose to leave British politics in 1976 and was appointed President of the European Commission in 1977, serving until 1981: he was the first and to date only British holder of this office. In 1981, dismayed with the Labour Party's continuing leftward drift, he was one of the "Gang of Four" – Labour moderates who formed the Social Democratic Party (SDP). In 1982 he won a famous by-election in a Conservative seat and returned to parliament; but after disappointment with the performance of the SDP in the 1983 election he resigned as SDP leader.
In 1987, Jenkins was elected to succeed Harold Macmillan as Chancellor of the University of Oxford following the latter's death; he held this position until his death. A few months after becoming Chancellor, Jenkins was defeated in his Hillhead constituency by then-Labour politician George Galloway. He accepted a life peerage and sat as a Liberal Democrat. In the late 1990s, he was an adviser to Tony Blair and chaired the Jenkins Commission on electoral reform. Roy Jenkins died in 2003, aged 82.
In addition to his political career, he was also a noted historian, biographer and writer. His "A Life at the Centre" (1991) is regarded as one of the best autobiographies of the later twentieth century, which 'will be read with pleasure long after most examples of the genre have been forgotten'.
Early life.
Born in Abersychan, Monmouthshire, in south-eastern Wales, as an only child, Roy Jenkins was the son of a National Union of Mineworkers official, Arthur Jenkins. His father was imprisoned during the 1926 General Strike for his alleged involvement in a disturbances. Jenkins later became President of the South Wales Miners' Federation and Member of Parliament for Pontypool, Parliamentary Private Secretary to Clement Attlee, and briefly a minister in the 1945 Labour government. Jenkins's mother, Hattie Harris, was the daughter of a steelworks manager.
Jenkins was educated at Abersychan County Grammar School, University College, Cardiff, and at Balliol College, Oxford, where he was twice defeated for the Presidency of the Oxford Union but took First-Class Honours in Politics, Philosophy and Economics (PPE). His university colleagues included Tony Crosland, Denis Healey, and Edward Heath, and he became friends with all three, although he was never particularly close to Healey.
During the Second World War, Jenkins served with the Royal Artillery and then as a Bletchley Park codebreaker, reaching the rank of captain.
Member of Parliament.
Having failed to win Solihull in 1945, he was elected to the House of Commons in a 1948 by-election as the Member of Parliament for Southwark Central, becoming the "Baby of the House." His constituency was abolished in boundary changes for the 1950 general election, when he stood instead in the new Birmingham Stechford constituency. He won the seat and represented the constituency until 1977.
Jenkins was principal sponsor, in 1959, of the bill which became the liberalising Obscene Publications Act, responsible for establishing the "liable to deprave and corrupt" criterion as a basis for a prosecution of suspect material and for specifying literary merit as a possible defence. Like Healey and Crosland, he had been a close friend of Hugh Gaitskell and for them Gaitskell's death and the elevation of Harold Wilson as Labour Party leader was a setback.
In Government.
After the 1964 general election Jenkins was appointed Minister of Aviation. While at Aviation he oversaw the high profile cancellations of the BAC TSR-2 and Concorde projects (although the latter was later reversed after strong opposition from the French Government). In January 1965 Patrick Gordon Walker resigned as Foreign Secretary and in the ensuing reshuffle Wilson offered Jenkins the Department for Education and Science; however. he declined it, preferring to stay at Aviation.
In the summer of 1965 Jenkins eagerly accepted an offer to replace Frank Soskice as Home Secretary. However Wilson, dismayed by a sudden bout of press speculation about the potential move, delayed Jenkins' appointment until December. Once Jenkins took office – the youngest Home Secretary since Churchill – he immediately set about reforming the operation and organisation of the Home Office. The Principal Private Secretary, Head of the Press and Publicity Department and Permanent Under-Secretary were all replaced. He also redesigned his office, famously replacing the board on which condemned prisoners were listed with a drinks cabinet. After the 1966 general election, in which Labour won a comfortable majority, Jenkins pushed through a series of police reforms which reduced the number of separate forces from 117 to 49.
Immigration was a divisive and provocative issue during the late 1960s and on 23 May 1966 Jenkins delivered a speech on race relations, which is widely considered to be one of his best. Addressing a London meeting of the National Committee for Commonwealth Immigrants he notably defined Integration:
... not as a flattening process of assimilation but as equal opportunity, accompanied by cultural diversity, in an atmosphere of mutual tolerance.
Before going onto ask:
Where in the world is there a university which could preserve its fame, or a cultural centre which could keep its eminence, or a metropolis which could hold its drawing power, if it were to turn inwards and serve only its own hinterland and its own racial group?
And concluding that:
To live apart, for a person, a city, a country, is to lead a life of declining intellectual stimulation.
Roy Jenkins is often seen as responsible for the most wide-ranging social reforms of the late 1960s, with popular historian Andrew Marr claiming 'the greatest changes of the Labour years' were thanks to Jenkins. He refused to authorise the birching of prisoners and was responsible for the relaxation of the laws relating to divorce, abolition of theatre censorship and gave government support to David Steel's Private Member's Bill for the legalisation of abortion and Leo Abse's bill for the decriminalisation of homosexuality. Wilson, with his puritan background, was not especially sympathetic to these developments, however. Jenkins replied to public criticism by asserting that the so-called permissive society was in reality the civilised society. For some conservatives, such as Peter Hitchens, Jenkins' reforms remain objectionable. In his book "The Abolition of Britain" Hitchens accuses him of being a "cultural revolutionary" who takes a large part of the responsibility for the decline of "traditional values" in Britain.
Chancellor of the Exchequer.
From 1967 to 1970 Jenkins served as Chancellor of the Exchequer, replacing James Callaghan following the devaluation crisis of November 1967. He quickly gained a reputation as a particularly tough Chancellor with his 1968 budget increasing taxes by £923 million, more than twice the increase of any previous budget to date. Despite Edward Heath claiming it was a 'hard, cold budget, without any glimmer of warmth' Jenkins' first budget broadly received a warm reception, with Harold Wilson remarking that 'it was widely acclaimed as a speech of surpassing quality and elegance' and Barbara Castle that it 'took everyone's breath away'. However, following a further sterling crisis in November 1968 Jenkins was forced to raise taxes by a further £250 million. After this the currency markets slowly began to settle and his 1969 budget represented more of the same with a £340 million increase in taxation to further limit consumption.
By May 1969 Britain's current account position was in surplus, thanks to a growth in exports, a drop in overall consumption and, in part, the Inland Revenue correcting a previous underestimation in export figures. In July Jenkins was also able to announce that the size of Britain's foreign currency reserves had been increased by almost $1 billion since the beginning of the year. It was at this time that he presided over Britain's only excess of government revenue over expenditure in the period 1936-7 to 1987–8. Thanks in part to these successes there was a high expectation that the 1970 budget would be a more generous one. Jenkins, however, was cautious about the stability of Britain's recovery and decided to present a more muted and fiscally neutral budget. It is often argued that this, combined with a series of bad trade figures, contributed to the Conservative victory at the 1970 general election. Historians and economists have often praised Jenkins for presiding over the transformation in Britain's fiscal and current account positions towards the end of the 1960s. Andrew Marr, for example, described him as one of the 20th century's 'most successful chancellors'.
Shadow Cabinet 1970–1974.
After Labour unexpectedly lost power in 1970 Jenkins was appointed Shadow Chancellor of the Exchequer by Harold Wilson. Jenkins was also subsequently elected to the deputy leadership of the Labour Party in July 1970, defeating future Labour Leader Michael Foot and former Leader of the Commons Fred Peart at the first ballot. At this time he appeared the natural successor to Harold Wilson, and it appeared to many only a matter of time before he inherited the leadership of the party, and the opportunity to become Prime Minister. 
This changed completely, however, as Jenkins refused to accept the tide of anti-European feeling that became prevalent in the Labour Party in the early 1970s. In 1972, he led sixty-nine Labour MPs through the division lobby in support of the Heath's government's motion to take Britain into the EEC. In so-doing they were defying a three-line whip and a five-to-one vote at the Labour Party annual conference. Jenkins's action gave the European cause a legitimacy that would have otherwise been absent had the issue been considered solely as a party political matter. At this stage, however, Jenkins would not fully abandon his position as a political insider, and chose to stand again for deputy leader, an act his colleague David Marquand claimed he later came to regret. Jenkins narrowly defeated Michael Foot on a second ballot.
Six months later, however, he resigned both the deputy leadership and his shadow cabinet position in April 1972, over the party's policy on favouring a referendum on British membership of the European Economic Community (EEC). This led to some former admirers, including Roy Hattersley, choosing to distance themselves from Jenkins. His lavish lifestyle — Wilson once described him as "more a socialite than a socialist" — had already alienated much of the Labour Party from him. Jenkins returned to the shadow cabinet in November 1973 as Shadow Home Secretary.
Return to Government.
When Labour returned to power in early 1974, Jenkins was appointed Home Secretary for the second time. Earlier, he had been promised the treasury; however, Wilson later decided to appoint Denis Healey as Chancellor instead. Upon hearing from Bernard Donoughue that Wilson had reneged on his promise, Jenkins reacted angrily. Despite being on a public staircase, he is reported to have shouted 'You tell Harold Wilson he must bloody well come to see me ... and if he doesn't watch out, I won't join his bloody government ... This is typical of the bloody awful way Harold Wilson does things!'
Jenkins served from 1974 to 1976. In this period he undermined his previous liberal credentials to some extent by pushing through the controversial Prevention of Terrorism Act, which, among other things, extended the length of time suspects could be held in custody and instituted exclusion orders. Although becoming increasingly disillusioned during this time by what he considered the party's drift to the left, he was the leading Labour figure in the referendum in September 1975 which saw the 'yes' campaign win a two-to-one victory in the referendum on continued membership of the European Community.
President of the European Commission.
When Harold Wilson suddenly resigned as Prime Minister, Jenkins was one of six candidates for the leadership of the Labour Party in March 1976, but came third out of the six candidates in the first ballot, behind Callaghan and Michael Foot. Realising that his vote was lower than expected, and sensing that the parliamentary party was in no mode to overlook his actions five years before, he immediately withdrew from the contest. Jenkins had wanted to become Foreign Secretary, but accepted an appointment as President of the European Commission (succeeding François-Xavier Ortoli) after Callaghan appointed Anthony Crosland to the Foreign Office.
The main development overseen by the Jenkins Commission was the development of the Economic and Monetary Union of the European Union from 1977, which began in 1979 as the European Monetary System, a forerunner of the Single Currency or Euro. President Jenkins was the first President to attend a G8 summit on behalf of the Community. Jenkins remained in Brussels until 1981, contemplating the political changes in the UK from there.
He received an Honorary Degree (Doctor of Laws) from the University of Bath in 1978.
The Social Democratic Party.
As one of the so-called "Gang of Four", Roy Jenkins was a founder of the Social Democratic Party (SDP) in January 1981 with David Owen, Bill Rodgers and Shirley Williams.
He attempted to re-enter Parliament at the Warrington by-election in 1981 but Labour retained the seat with a small majority. He was more successful in 1982, being elected in the Glasgow Hillhead by-election as the MP for a previously Conservative-held seat.
During the 1983 election campaign his position as the prime minister designate for the SDP-Liberal Alliance was questioned by his close colleagues, as his campaign style was now regarded as ineffective; the Liberal leader David Steel was considered to have a greater rapport with the electorate.
He led the new party from March 1982 until after the 1983 general election, when Owen succeeded him unopposed. Jenkins was disappointed with Owen's move to the right, and his acceptance and backing of some of Thatcher's policies. At heart, Jenkins remained a Keynesian.
He continued to serve as SDP Member of Parliament for Glasgow Hillhead until his defeat at the 1987 general election by the Labour candidate George Galloway.
Peerage, achievements, books and death.
From 1987, Jenkins remained in politics as a member of the House of Lords as a life peer with the title Baron Jenkins of Hillhead, of Pontypool in the County of Gwent. Also in 1987, Jenkins was elected Chancellor of the University of Oxford.
In 1988 he fought and won an amendment to the Education Reform Act of that year, guaranteeing academic freedom of speech in further and higher education establishments. This affords and protects the right of students and academics to "question and test received wisdom" and has been incorporated into the statutes or articles and instruments of governance of all universities and colleges in Britain.
In 1993, he was appointed to the Order of Merit. He was leader of the Liberal Democrats in the Lords until 1997. In December 1997, he was appointed chair of a Government-appointed Independent Commission on the Voting System, which became known as the "Jenkins Commission", to consider alternative voting systems for the UK. The Jenkins Commission reported in favour of a new uniquely British mixed-member proportional system called "Alternative vote top-up" or "limited AMS" in October 1998, although no action was taken on this recommendation.
Jenkins wrote 19 books, including a biography of Gladstone (1995), which won the 1995 Whitbread Award for Biography, and a much-acclaimed biography of Winston Churchill (2001). His official biographer, Andrew Adonis, Baron Adonis, was to have finished the Churchill biography had Jenkins not survived the heart surgery he underwent towards the end of its writing. Churchill's daughter Lady Mary Soames believed the Jenkins biography of her father to be the best available.
Jenkins underwent heart surgery in November 2000, and postponed his 80th birthday celebrations, by having a celebratory party on 7 March 2001. He died on 5 January 2003, aged 82, after suffering a heart attack at his home at East Hendred, in Oxfordshire. His last words, to his wife, were, "Two eggs, please, lightly poached". At the time of his death Jenkins was apparently starting work on a biography of US President Franklin D. Roosevelt.
Jenkins is seen by many as a key influence on "New Labour", as the Labour Party marketed itself after the election of Tony Blair (who served as prime minister from winning the first of three successive general elections in 1997) in 1994, when the party abandoned many of its long-established policies including nationalisation, nuclear disarmament and unconditional support for the trade unions. He was well regarded by other Labour statesmen including Tony Benn, but came under heavy criticism from others including Denis Healey, who condemned the SDP split as a "disaster" for the Labour Party which prolonged their time in opposition and allowed the Tories to have an unbroken run of 18 years in government.
Cardiff University honours the memory of Roy Jenkins by naming one of its halls of residence 'Roy Jenkins Hall'.
Marriage and personal life.
On 20 January 1945, in the final year of the War, he married Jennifer Morris. They were married for 58 years until his death. 
She was made a DBE for services to ancient and historical buildings. They had two sons, Charles and Edward, and a daughter, Cynthia.
Dame Jennifer Jenkins is still alive, now in her nineties, more than a decade after her husband's death.
Selected bibliography.
Books by Roy Jenkins:
Books about Roy Jenkins:

</doc>
<doc id="26478" url="http://en.wikipedia.org/wiki?curid=26478" title="Real analysis">
Real analysis

Real analysis (traditionally, the theory of functions of a real variable) is a branch of mathematical analysis dealing with the real numbers and real-valued functions of a real variable. In particular, it deals with the analytic properties of real functions and sequences, including convergence and limits of sequences of real numbers, the calculus of the real numbers, and continuity, smoothness and related properties of real-valued functions.
Scope.
Construction of the real numbers.
There are several ways of defining the real number system as an ordered field. The "synthetic" approach gives a list of axioms for the real numbers as a "complete ordered field". Under the usual axioms of set theory, one can show that these axioms are categorical, in the sense that there is a model for the axioms, and any two such models are isomorphic. Any one of these models must be explicitly constructed, and most of these models are built using the basic properties of the rational number system as an ordered field. These constructions are described in more detail in the main article.
Order properties of the real numbers.
The real numbers have several important lattice-theoretic properties that are absent in the complex numbers. Most importantly, the real numbers form an ordered field, in which addition and multiplication preserve positivity. Moreover, the ordering of the real numbers is total, and the real numbers have the least upper bound property. These order-theoretic properties lead to a number of important results in real analysis, such as the monotone convergence theorem, the intermediate value theorem and the mean value theorem.
However, while the results in real analysis are stated for real numbers, many of these results can be generalized to other mathematical objects. In particular, many ideas in functional analysis and operator theory generalize properties of the real numbers – such generalizations include the theories of Riesz spaces and positive operators. Also, mathematicians consider real and imaginary parts of complex sequences, or by pointwise evaluation of operator sequences.
Sequences.
A sequence is usually defined as a function whose domain is a countable totally ordered set, although in many disciplines the domain is restricted, such as to the natural numbers. In real analysis a sequence is a function from a subset of the natural numbers to the real numbers. In other words, a sequence is a map "f"("n") : N → R. We might identify "an" = "f"("n")   for all "n" or just write "an" : N → R.
Limits.
A limit is the value that a function or sequence "approaches" as the input or index approaches some value. Limits are essential to calculus (and mathematical analysis in general) and are used to define continuity, derivatives, and integrals.
Continuity.
A function from the set of real numbers to the real numbers can be represented by a graph in the Cartesian plane; such a function is continuous if, roughly speaking, the graph is a single unbroken curve with no "holes" or "jumps".
There are several ways to make this intuition mathematically rigorous. These definitions are equivalent to one another, so the most convenient definition can be used to determine whether a given function is continuous or not. In the definitions below, 
is a function defined on a subset "I" of the set R of real numbers. This subset "I" is referred to as the domain of "f". Some possible choices include "I"=R, the whole set of real numbers, an open interval
or a closed interval
Here, "a" and "b" are real numbers.
Uniform continuity.
If "X" and "Y" are subsets of the real numbers, a function "f" : "X" → "Y" is called uniformly continuous if for all "ε" > 0 there exists a "δ" > 0 such that for all "x", "y" ∈ "X", |"x" − "y"| < "δ" implies |"f"("x") − "f"("y")| < "ε.
The difference between being uniformly continuous, and being simply continuous at every point, is that in uniform continuity the value of "δ" depends only on "ε" and not on the point in the domain.
Absolute continuity.
Let formula_4 be an interval in the real line R. A function formula_5 is absolutely continuous on formula_4 if for every positive number formula_7, there is a positive number formula_8 such that whenever a finite sequence of pairwise disjoint sub-intervals formula_9 of formula_4 satisfies
then
The collection of all absolutely continuous functions on "I" is denoted AC("I").
The following conditions on a real-valued function "f" on a compact interval ["a","b"] are equivalent:
If these equivalent conditions are satisfied then necessarily "g" = "f" ′ almost everywhere.
Equivalence between (1) and (3) is known as the fundamental theorem of Lebesgue integral calculus, due to Lebesgue.
Series.
Given an infinite sequence of numbers { "a""n" }, a series is informally the result of adding all those terms together: "a"1 + "a"2 + "a"3 + · · ·. These can be written more compactly using the summation symbol ∑. An example is the famous series from Zeno's dichotomy and its mathematical representation:
The terms of the series are often produced according to a certain rule, such as by a formula, or by an algorithm.
Taylor series.
The Taylor series of a real or complex-valued function "ƒ"("x") that is infinitely differentiable at a real or complex number "a" is the power series
which can be written in the more compact sigma notation as
where "n"! denotes the factorial of "n" and "ƒ" ("n")("a") denotes the "n"th derivative of "ƒ" evaluated at the point "a". The derivative of order zero "ƒ" is defined to be "ƒ" itself and ("x" − "a")0 and 0! are both defined to be 1. In the case that , the series is also called a Maclaurin series.
Fourier Series.
A Fourier series decomposes periodic functions or periodic signals into the sum of a (possibly infinite) set of simple oscillating functions, namely sines and cosines (or complex exponentials). The study of Fourier series is a branch of Fourier analysis.
Differentiation.
Formally, the derivative of the function "f" at "a" is the limit
If the derivative exists everywhere, the function is differentiable. One can take higher derivatives as well, by iterating this process.
One can classify functions by their differentiability class. The class "C"0 consists of all continuous functions. The class "C"1 consists of all differentiable functions whose derivative is continuous; such functions are called continuously differentiable. Thus, a "C"1 function is exactly a function whose derivative exists and is of class "C"0. In general, the classes "Ck" can be defined recursively by declaring "C"0 to be the set of all continuous functions and declaring "Ck" for any positive integer "k" to be the set of all differentiable functions whose derivative is in "C""k"−1. In particular, "Ck" is contained in "C""k"−1 for every "k", and there are examples to show that this containment is strict. "C"∞ is the intersection of the sets "Ck" as "k" varies over the non-negative integers. "C"ω is strictly contained in "C"∞.
Integration.
Riemann integration.
The Riemann integral is defined in terms of Riemann sums of functions with respect to "tagged partitions" of an interval. Let ["a","b"] be a closed interval of the real line; then a "tagged partition" of ["a","b"] is a finite sequence
This partitions the interval ["a","b"] into "n" sub-intervals ["x""i"−1, "x""i"] indexed by "i", each of which is "tagged" with a distinguished point "t""i" ∈ ["x""i"−1, "x""i"]. A "Riemann sum" of a function "f" with respect to such a tagged partition is defined as
thus each term of the sum is the area of a rectangle with height equal to the function value at the distinguished point of the given sub-interval, and width the same as the sub-interval width. Let be the width of sub-interval "i"; then the "mesh" of such a tagged partition is the width of the largest sub-interval formed by the partition, . The "Riemann integral" of a function "f" over the interval ["a","b"] is equal to "S" if:
When the chosen tags give the maximum (respectively, minimum) value of each interval, the Riemann sum becomes an upper (respectively, lower) Darboux sum, suggesting the close connection between the Riemann integral and the Darboux integral.
Lebesgue integration.
Lebesgue integration is a mathematical construction that extends the integral to a larger class of functions; it also extends the domains on which these functions can be defined.
Distributions.
Distributions (or generalized functions) are objects that generalize functions. Distributions make it possible to differentiate functions whose derivatives do not exist in the classical sense. In particular, any locally integrable function has a distributional derivative. 
Relation to complex analysis.
Real analysis is an area of analysis that studies concepts such as sequences and their limits, continuity, differentiation, integration and sequences of functions. By definition, real analysis focuses on the real numbers, often including positive and negative infinity to form the extended real line. Real analysis is closely related to complex analysis, which studies broadly the same properties of complex numbers. In complex analysis, it is natural to define differentiation via holomorphic functions, which have a number of useful properties, such as repeated differentiability, expressability as power series, and satisfying the Cauchy integral formula.
In real analysis, it is usually more natural to consider differentiable, smooth, or harmonic functions, which are more widely applicable, but may lack some more powerful properties of holomorphic functions. However, results such as the fundamental theorem of algebra are simpler when expressed in terms of complex numbers.
Techniques from the theory of analytic functions of a complex variable are often used in real analysis – such as evaluation of real integrals by residue calculus.
Important results.
Important results include the Bolzano–Weierstrass and Heine–Borel theorems, the intermediate value theorem and mean value theorem, the fundamental theorem of calculus, and the monotone convergence theorem.
Various ideas from real analysis can be generalized from real space to general metric spaces, as well as to measure spaces, Banach spaces, and Hilbert spaces.

</doc>
<doc id="26485" url="http://en.wikipedia.org/wiki?curid=26485" title="Calendar-based contraceptive methods">
Calendar-based contraceptive methods

Calendar-based methods are various methods of estimating a woman's likelihood of fertility, based on a record of the length of previous menstrual cycles. Various methods are known as the Knaus–Ogino Method and the Rhythm Method. The Standard Days Method is also considered a calendar-based method, because when using it, a woman tracks the days of her menstrual cycle without observing her physical fertility signs. The Standard Days Method is based on a fixed formula taking into consideration the timing of ovulation, the functional life of the sperm and the ovum, and the resulting likelihood of pregnancy on particular days of the menstrual cycle. These methods may be used to achieve pregnancy by timing unprotected intercourse for days identified as fertile, or to avoid pregnancy by avoiding unprotected intercourse during fertile days.
The first formalized calendar-based method was developed in 1930 by John Smulders, a Roman Catholic physician from the Netherlands. It was based on knowledge of the menstrual cycle. This method was independently discovered by Hermann Knaus (Austria), and Kyusaku Ogino (Japan). This system was a main form of birth control available to Catholic couples for several decades, until the popularization of symptoms-based fertility awareness methods. A new development in calendar-based methods occurred in 2002, when Georgetown University introduced the Standard Days Method. The Standard Days Method is promoted in conjunction with a product called CycleBeads, a ring of colored beads which are meant to help the user keep track of her fertile and non-fertile days.
Terminology.
Some sources may treat the terms "rhythm method" and "fertility awareness" as synonymous. However, fertility awareness is usually used as a broad term that includes tracking basal body temperature and cervical mucus as well as cycle length. The World Health Organization considers the rhythm method to be a specific type of calendar-based method, and calendar-based methods to be only one form of fertility awareness.
More effective than calendar-based methods, systems of fertility awareness that track basal body temperature, cervical mucus, or both, are known as symptoms-based methods. Teachers of symptoms-based methods take care to distance their systems from the poor reputation of the rhythm method. Many consider the rhythm method to have been obsolete for at least 20 years, and some even exclude calendar-based methods from their definition of fertility awareness.
Some sources may treat the terms "rhythm method" and "natural family planning" as synonymous. In the early 20th century, the calendar-based method known as the "rhythm method" was promoted by members of the Roman Catholic Church as the only morally acceptable form of family planning. Methods accepted by this church are referred to as natural family planning (NFP): so at one time, the term "the rhythm method" was synonymous with NFP. Today, NFP is an umbrella term that includes symptoms-based fertility awareness methods and the lactational amenorrhea method as well as calendar-based methods such as rhythm. This overlap between uses of the terms "the rhythm method" and "natural family planning" may contribute to confusion. 
The term "the rhythm method" is sometimes used, in error, to describe the behavior of any people who have unprotected vaginal intercourse, yet wish to avoid pregnancy.
History.
Early methods.
It is not known if historical cultures were aware of what part of the menstrual cycle is most fertile. In the year 388, Augustine of Hippo wrote of periodic abstinence. Addressing followers of Manichaeism, his former religion, he said, "Is it not you who used to counsel us to observe as much as possible the time when a woman, after her purification, is most likely to conceive, and to abstain from cohabitation at that time...?" If the Manichaieans practiced something like the Jewish observances of menstruation, then the "time... after her purification" would have indeed been when "a woman... is most likely to conceive." Over a century previously, however, the influential Greek physician Soranus had written that "the time directly before and after menstruation" was the most fertile part of a woman's cycle; this inaccuracy was repeated in the 6th century by the Byzantine physician Aëtius. Similarly, a Chinese sex manual written close to the year 600 stated that only the first five days following menstruation were fertile. Some historians believe that Augustine, too, incorrectly identified the days immediately after menstruation as the time of highest fertility.
Written references to a "safe period" do not appear again for over a thousand years. Scientific advances prompted a number of secular thinkers to advocate periodic abstinence to avoid pregnancy: in the 1840s it was discovered that many animals ovulate during estrus. Because some animals (such as dogs) have a bloody discharge during estrus, it was assumed that menstruation was the corresponding most fertile time for women. This inaccurate theory was popularized by physicians Bischoff, Félix Archimède Pouchet, and Adam Raciborski. In 1854, an English doctor named George Drysdale correctly taught his patients that the days near menstruation are the "least" fertile, but this remained the minority view for the remainder of the 19th century.
Knaus–Ogino or rhythm method.
In 1905 Theodoor Hendrik van de Velde, a Dutch gynecologist, showed that women only ovulate once per menstrual cycle. In the 1920s, Kyusaku Ogino, a Japanese gynecologist, and Hermann Knaus, from Austria, working independently, each made the discovery that ovulation occurs about fourteen days before the next menstrual period. Ogino used his discovery to develop a formula for use in aiding infertile women to time intercourse to achieve pregnancy.
In 1930, Johannes Smulders, a Roman Catholic physician from the Netherlands, used Knaus and Ogino's discoveries to create a method for "avoiding" pregnancy. Smulders published his work with the Dutch Roman Catholic medical association, and this was the official rhythm method promoted over the next several decades. In 1932 a Catholic physician, Dr. Leo J Latz, published a book titled "The Rhythm of Sterility and Fertility in Women" describing the method, and the 1930s also saw the first U.S. Rhythm Clinic (founded by John Rock) to teach the method to Catholic couples.
Later 20th century to present.
In the first half of the 20th century, most users of the rhythm method were Catholic; they were following their church's teaching that all other methods of birth control were sinful. In 1968 the encyclical "Humanae vitae" included the statement, "It is supremely desirable... that medical science should by the study of natural rhythms succeed in determining a sufficiently secure basis for the chaste limitation of offspring." This is interpreted as favoring the then-new, more reliable symptoms-based fertility awareness methods over the rhythm method. Currently, many fertility awareness teachers consider the rhythm method to have been obsolete for at least 20 years.
New attention was drawn to calendar-based methods in 2002, when the Institute for Reproductive Health at Georgetown University introduced the Standard Days Method. Designed to be simpler to teach and use than the older rhythm method, the Standard Days Method is being successfully integrated into family planning programs worldwide.
Types and effectiveness.
Most menstrual cycles have several days at the beginning that are infertile (pre-ovulatory infertility), a period of fertility, and then several days just before the next menstruation that are infertile (post-ovulatory infertility). The first day of red bleeding is considered day one of the menstrual cycle. To use these methods, a woman is required to know the length of her menstrual cycles.
Imperfect use of calendar-based methods would consist of not correctly tracking the length of the woman's cycles, thus using the wrong numbers in the formula, or of having unprotected intercourse on an identified fertile day. The discipline required to keep accurate records of menstrual cycles, and to abstain from unprotected intercourse, makes imperfect use fairly common. The typical-use failure rate of calendar-based methods is 25% per year.
Rhythm method (Knaus–Ogino method).
To find the estimated length of the pre-ovulatory infertile phase, nineteen (19) is subtracted from the length of the woman's shortest cycle. To find the estimated start of the post-ovulatory infertile phase, ten (10) is subtracted from the length of the woman's longest cycle. A woman whose menstrual cycles ranged in length from 30 to 36 days would be estimated to be infertile for the first 11 days of her cycle (30-19=11), to be fertile on days 12-25, and to resume infertility on day 26 (36-10=26). When used to avoid pregnancy, the rhythm method has a perfect-use failure rate of up to 9% per year.
Standard Days Method.
Developed by Georgetown University's Institute for Reproductive Health, the Standard Days Method has a simpler rule set and is more effective than the rhythm method. A product called CycleBeads was developed alongside the method to help the user keep track of estimated high and low fertility points during her menstrual cycle. The Standard Days Method may only be used by women whose cycles are usually between 26 and 32 days in length. In this system:
When used to avoid pregnancy, the Standard Days Method has perfect-use efficacy of 95+% and typical-use efficacy of 88%.
Software-based systems.
Several Web-based implementations of the cycle method are known.
Advantages.
The Standard Days method (SDM) is increasingly being introduced as part of family planning programs in developing countries. The method is satisfactory for many women and men; offering it through family planning centers results in a significant increase in contraceptive use among couples who do not want to become pregnant. The low cost of the method may also enable it to have a significant positive impact in countries that lack funding to provide other methods of birth control.
Potential concerns.
Failure rate.
One concern related to the use of calendar-based methods is their relatively high failure rate, compared to other methods of birth control. Even when used perfectly, calendar-based methods, especially the rhythm method, result in a high pregnancy rate among couples intending to avoid pregnancy. Of commonly known methods of birth control, only the cervical cap and contraceptive sponge have comparably high failure rates. This lower level of reliability of calendar-based methods is because their formulas make several assumptions that are not always true.
The postovulatory (luteal) phase has a normal length of 12 to 16 days, and the rhythm method formula assumes all women have luteal phase lengths within this range. However, many women have shorter luteal phases, and a few have longer luteal phases. For these women, the rhythm method formula incorrectly identifies a few fertile days as being in the infertile period.
Calendar-based methods use records of past menstrual cycles to predict the length of future cycles. However, the length of the pre-ovulatory phase can vary significantly, depending on the woman's typical cycle length, stress factors, medication, illness, menopause, breastfeeding, and whether she is just coming off hormonal contraception. If a woman with previously regular cycles has a delayed ovulation due to one of these factors, she will still be fertile when the method tells her she is in the post-ovulatory infertile phase. If she has an unusually early ovulation, calendar-based methods will indicate she is still in the pre-ovulatory infertile phase when she has actually become fertile. 
Finally, calendar-based methods assume that all bleeding is true menstruation. However, mid-cycle or anovulatory bleeding can be caused by a number of factors. Incorrectly identifying bleeding as menstruation will cause the method's calculations to be incorrect.
Embryonic health.
It has been suggested that unprotected intercourse in the infertile periods of the menstrual cycle may still result in conceptions, but create embryos incapable of implanting.
It has also been suggested that pregnancies resulting from method failures of periodic abstinence methods are at increased risk of miscarriage and birth defects due to aged gametes at the time of conception. Other research suggests that timing of conception has no effect on miscarriage rates, low birth weight, or preterm delivery.

</doc>
<doc id="26490" url="http://en.wikipedia.org/wiki?curid=26490" title="Reference counting">
Reference counting

In computer science, reference counting is a technique of storing the number of references, pointers, or handles to a resource such as an object, block of memory, disk space or other resource. It may also refer, more specifically, to a garbage collection algorithm that uses these reference counts to deallocate objects which are no longer referenced.
Use in garbage collection.
As a collection algorithm, reference counting tracks, for each object, a count of the number of references to it held by other objects. If an object's reference count reaches zero, the object has become inaccessible, and can be destroyed.
When an object is destroyed, any objects referenced by that object also have their reference counts decreased. Because of this, removing a single reference can potentially lead to a large number of objects being freed. A common modification allows reference counting to be made incremental: instead of destroying an object as soon as its reference count becomes zero, it is added to a list of unreferenced objects, and periodically (or as needed) one or more items from this list are destroyed.
Simple reference counts require frequent updates. Whenever a reference is destroyed or overwritten, the reference count of the object it references is decremented, and whenever one is created or copied, the reference count of the object it references is incremented.
Reference counting is also used in disk operating systems and distributed systems, where full non-incremental tracing garbage collection is too time consuming because of the size of the object graph and slow access speed.
Advantages and disadvantages.
The main advantage of the reference counting over tracing garbage collection is that objects are reclaimed "as soon as" they can no longer be referenced, and in an incremental fashion, without long pauses for collection cycles and with clearly defined lifetime of every object. In real-time applications or systems with limited memory, this is important to maintain responsiveness. Reference counting is also among the simplest forms of memory management to implement. It also allows for effective management of non-memory resources such as operating system objects, which are often much scarcer than memory (tracing GC systems use finalizers for this, but the delayed reclamation may cause problems). Weighted reference counts are a good solution for garbage collecting a distributed system.
Tracing garbage collection cycles are triggered too often if the set of live objects fills most of the available memory; it requires extra space to be efficient. Reference counting performance does not deteriorate as the total amount of free space decreases.
Reference counts are also useful information to use as input to other runtime optimizations. For example, systems that depend heavily on immutable objects such as many functional programming languages can suffer an efficiency penalty due to frequent copies. However, if we know an object has only one reference (as most do in many systems), and that reference is lost at the same time that a similar new object is created (as in the string append statement codice_1), we can replace the operation with a mutation on the original object.
Reference counting in naive form has two main disadvantages over the tracing garbage collection, both of which require additional mechanisms to ameliorate:
In addition to these, if the memory is allocated from a free list, reference counting suffers from poor locality. Reference counting alone cannot move objects to improve cache performance, so high performance collectors implement a tracing garbage collector as well. Most implementations (such as the ones in PHP and Objective-C) suffer from poor cache performance since they do not implement copying objects.
Graph interpretation.
When dealing with garbage collection schemes, it is often helpful to think of the reference graph, which is a directed graph where the vertices are objects and there is an edge from an object A to an object B if A holds a reference to B. We also have a special vertex or vertices representing the local variables and references held by the runtime system, and no edges ever go to these nodes, although edges can go from them to other nodes.
In this context, the simple reference count of an object is the in-degree of its vertex. Deleting a vertex is like collecting an object. It can only be done when the vertex has no incoming edges, so it does not affect the out-degree of any other vertices, but it can affect the in-degree of other vertices, causing their corresponding objects to be collected as well if their in-degree also becomes 0 as a result.
The connected component containing the special vertex contains the objects that can't be collected, while other connected components of the graph only contain garbage. If a reference-counting garbage collection algorithm is implemented, then each of these garbage components must contain at least one cycle; otherwise, they would have been collected as soon as their reference count (i.e., the number of incoming edges) dropped to zero.
Dealing with inefficiency of updates.
Incrementing and decrementing reference counts every time a reference is created or destroyed can significantly impede performance. Not only do the operations take time, but they damage cache performance and can lead to pipeline bubbles. Even read-only operations like calculating the length of a list require a large number of reads and writes for reference updates with naive reference counting.
One simple technique is for the compiler to combine a number of nearby reference updates into one. This is especially effective for references which are created and quickly destroyed. Care must be taken, however, to put the combined update at the right position so that a premature free be avoided.
The Deutsch-Bobrow method of reference counting capitalizes on the fact that most reference count updates are in fact generated by references stored in local variables. It ignores these references, only counting references in data structures, but before an object with reference count zero can be deleted, the system must verify with a scan of the stack and registers that no other reference to it still exists.
Another technique devised by Henry Baker involves deferred increments, in which references which are stored in local variables do not immediately increment the corresponding reference count, but instead defer this until it is necessary. If such a reference is destroyed quickly, then there is no need to update the counter. This eliminates a large number of updates associated with short-lived references. However, if such a reference is copied into a data structure, then the deferred increment must be performed at that time. It is also critical to perform the deferred increment before the object's count drops to zero, resulting in a premature free.
A dramatic decrease in the overhead on counter updates was obtained by Levanoni and Petrank. They introduce the update coalescing method which coalesces many of the redundant reference count updates. Consider a pointer that in a given interval of the execution is updated several times. It first points to an object "O1", then to an object "O2", and so forth until at the end of the interval it points to some object "On". A reference counting algorithm would typically execute "rc(O1)--", "rc(O2)++", "rc(O2)--", "rc(O3)++", "rc(O3)--", ..., "rc(On)++". But most of these updates are redundant. In order to have the reference count properly evaluated at the end of the interval it is enough to perform "rc(O1)--" and "rc(On)++". The rest of the updates are redundant.
Levanoni and Petrank show how to use such update coalescing in a reference counting collector. It turns out that when using update coalescing with an appropriate treatment of new objects, more than 99% of the counter updates are eliminated for typical Java benchmarks. In addition, the need for atomic operations during pointer updates on parallel processors is eliminated. Finally, they present an enhanced algorithm that may run concurrently with multithreaded applications employing only fine synchronization. The details appear in the paper.
Blackburn and McKinley's ulterior reference counting combines deferred reference counting with a copying nursery, observing that the majority of pointer mutations occur in young objects. This algorithm achieves throughput comparable with the fastest generational copying collectors with the low bounded pause times of reference counting.
More work on improving performance of reference counting collectors can be found in Paz's Ph.D thesis. In particular, he advocates the use of age oriented collectors and prefetching.
Dealing with reference cycles.
Perhaps the most obvious way to handle reference cycles is to design the system to avoid creating them. A system may explicitly forbid reference cycles; file systems with hard links often do this. Judicious use of "weak" (non-counted) references may also help avoid retain cycles; the Cocoa framework, for instance, recommends using "strong" references for parent-to-child relationships and "weak" references for child-to-parent relationships.
Systems may also be designed to tolerate or correct the cycles they create in some way. Developers may design code to explicitly "tear down" the references in a data structure when it is no longer needed, though this has the cost of requiring them to manually track that data structure's lifetime. This technique can be automated by creating an "owner" object that does the tearing-down when it is destroyed; for instance, a Graph object's destructor could delete the edges of its GraphNodes, breaking the reference cycles in the graph. Cycles may even be ignored in systems with short lives and a small amount of cyclic garbage, particularly when the system was developed using a methodology of avoiding cyclic data structures wherever possible, typically at the expense of efficiency.
Computer scientists have also discovered ways to detect and collect reference cycles automatically, without requiring changes in the data structure design. One simple solution is to periodically use a tracing garbage collector to reclaim cycles; since cycles typically constitute a relatively small amount of reclaimed space, the collector can be run much less often than with an ordinary tracing garbage collector.
Bacon describes a cycle-collection algorithm for reference counting systems with some similarities to tracing systems, including the same theoretical time bounds, but that takes advantage of reference count information to run much more quickly and with less cache damage. It is based on the observation that an object cannot appear in a cycle until its reference count is decremented to a nonzero value. All objects which this occurs to are put on a "roots" list, and then periodically the program searches through the objects reachable from the roots for cycles. It knows it has found a cycle that can be collected when decrementing all the reference counts on a cycle of references brings them all down to zero. An enhanced version of this algorithm by Paz et al.
is able to run concurrently with other operations and improve its efficiency by using the update coalescing method of Levanoni and Petrank.
Variants of reference counting.
Although it is possible to augment simple reference counts in a variety of ways, often a better solution can be found by performing reference counting in a fundamentally different way. Here we describe some of the variants on reference counting and their benefits and drawbacks.
Weighted reference counting.
In weighted reference counting, we assign each reference a "weight", and each object tracks not the number of references referring to it, but the total weight of the references referring to it. The initial reference to a newly created object has a large weight, such as 216. Whenever this reference is copied, half of the weight goes to the new reference, and half of the weight stays with the old reference. Because the total weight does not change, the object's reference count does not need to be updated.
Destroying a reference decrements the total weight by the weight of that reference. When the total weight becomes zero, all references have been destroyed. If an attempt is made to copy a reference with a weight of 1, we have to "get more weight" by adding to the total weight and then adding this new weight to our reference, and then splitting it. An alternative in this situation is to create an "indirection" reference object, the initial reference to which is created with a large weight which can then be split.
The property of not needing to access a reference count when a reference is copied is particularly helpful when the object's reference count is expensive to access, for example because it is in another process, on disk, or even across a network. It can also help increase concurrency by avoiding many threads locking a reference count to increase it. Thus, weighted reference counting is most useful in parallel, multiprocess, database, or distributed applications.
The primary problem with simple weighted reference counting is that destroying a reference still requires accessing the reference count, and if many references are destroyed this can cause the same bottlenecks we seek to avoid. Some adaptations of weighted reference counting seek to avoid this by attempting to give weight back from a dying reference to one which is still active.
Weighted reference counting was independently devised by Bevan, in the paper "Distributed garbage collection using reference counting", and Watson & Watson, in the paper "An efficient garbage collection scheme for parallel computer architectures", both in 1987.
Indirect reference counting.
In indirect reference counting, it is necessary to keep track of whom the reference was obtained from. This means that two references are kept to the object: a direct one which is used for invocations; and an indirect one which forms part of a diffusion tree, such as in the Dijkstra-Scholten algorithm, which allows a garbage collector to identify dead objects. This approach prevents an object from being discarded prematurely.
Examples of use.
COM.
Microsoft's Component Object Model (COM) makes pervasive use of reference counting. In fact, two of the three methods that all COM objects must provide (in the IUnknown interface) increment or decrement the reference count. Much of the Windows Shell and many Windows applications (including MS Internet Explorer, MS Office, and countless third-party products) are built on COM, demonstrating the viability of reference counting in large-scale systems.
One primary motivation for reference counting in COM is to enable interoperability across different programming languages and runtime systems. A client need only know how to invoke object methods in order to manage object life cycle; thus, the client is completely abstracted from whatever memory allocator the implementation of the COM object uses. As a typical example, a Visual Basic program using a COM object is agnostic towards whether that object was allocated (and must later be deallocated) by a C++ allocator or another Visual Basic component.
However, this support for heterogeneity has a major cost: it requires correct reference count management by all parties involved. While high-level languages like Visual Basic manage reference counts automatically, C/C++ programmers are entrusted to increment and decrement reference counts at the appropriate time. C++ programs can and should avoid the task of managing reference counts manually by using smart pointers. Bugs caused by incorrect reference counting in COM systems are notoriously hard to resolve, especially because the error may occur in an opaque, third-party component.
Microsoft abandoned reference counting in favor of tracing garbage collection for the .NET Framework. However, it has been reintroduced in the COM-based WinRT and the new C++/CX (Component Extensions) language.
C++.
C++11 provides reference counted smart pointers, via the codice_2 class. Programmers can use weak pointers (via codice_2) to break cycles. C++ does not "require" all objects to be reference counted; in fact, programmers can choose to apply reference counting to only those objects that are truly shared; objects not intended to be shared can be referenced using a codice_2, and objects that are shared but not owned can be accessed via an iterator.
In addition, C++11's move semantics further reduce the extent to which reference counts need to be modified.
Cocoa.
Apple's Cocoa and Cocoa Touch frameworks (and related frameworks, such as Core Foundation) use manual reference counting, much like COM. Traditionally this was accomplished by the programmer manually sending codice_5 and codice_6 messages to objects, but Automatic Reference Counting, a Clang compiler feature that automatically inserts these messages as needed, was added in iOS 5 and Mac OS X 10.7. Mac OS X 10.5 introduced a tracing garbage collector as an alternative to reference counting, but it was deprecated in OS X 10.8 and is expected to be removed in a future version. iOS has never supported a tracing garbage collector.
Delphi.
One language that uses reference counting for garbage collection is Delphi. Delphi is not a completely garbage collected language, in that user-defined types must still be manually allocated and deallocated. It does provide automatic collection, however, for a few built-in types, such as strings, dynamic arrays, and interfaces, for ease of use and to simplify the generic database functionality. It is up to the programmer to decide whether to use the built-in types or not; Delphi programmers have complete access to low-level memory management like in C/C++. So all potential cost of Delphi's reference counting can, if desired, be easily circumvented.
Some of the reasons reference counting may have been preferred to other forms of garbage collection in Delphi include:
GObject.
The GObject object-oriented programming framework implements reference counting on its base types, including weak references. Reference incrementing and decrementing uses atomic operations for thread safety. A significant amount of the work in writing bindings to GObject from high-level languages lies in adapting GObject reference counting to work with the language's own memory management system.
The Vala programming language uses GObject reference counting as its primary garbage collection system, along with copy-heavy string handling.
Perl.
Perl also uses reference counting, without any special handling of circular references, although (as in Cocoa and C++ above), Perl does support weak references, which allows programmers to avoid creating a cycle.
PHP.
PHP uses a reference counting mechanism for its internal variable management. Since PHP 5.3, it implements the algorithm from Bacon's above mentioned paper. PHP allows you to turn on and off the cycle collection with user-level functions. It also allows you to manually force the purging mechanism to be run.
Python.
Python also uses reference counting and offers cycle detection as well.
Squirrel.
Squirrel also uses reference counting and offers cycle detection as well.
This tiny language is relatively unknown outside the video game industry; however, it is a concrete example of how reference counting can be practical and efficient (especially in realtime environments).
Tcl.
Tcl 8 uses reference counting for memory management of values (Tcl Obj structs). Since Tcl's values are immutable, reference cycles are impossible to form and no cycle detection scheme is needed. Operations that would replace a value with a modified copy are generally optimized to instead modify the original when its reference count indicates it to be unshared. The references are counted at a data structure level, so the problems with very frequent updates discussed above do not arise.
File systems.
Many file systems maintain a count of the number of references to any particular block or file, for example the inode "link count" on Unix-style file systems. When the count falls to zero, the file can be safely deallocated. In addition, while references can still be made from directories, some Unixes allow that the referencing can be solely made by live processes, and there can be files that do not exist in the file system hierarchy.
References.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.

</doc>
<doc id="26494" url="http://en.wikipedia.org/wiki?curid=26494" title="Race and intelligence">
Race and intelligence

The connection between race and intelligence has been a subject of debate in both popular science and academic research since the inception of IQ testing in the early 20th century. The debate concerns the interpretation of research findings that American test takers identifying as "White" tend on average to score higher than test takers of African ancestry on IQ tests, and subsequent findings that test takers of East Asian background tend to score higher than whites. It is still not resolved what relation, if any, there is between group differences in IQ and race.
The first test showing differences in IQ test results between different population groups in the US was the tests of United States Army recruits in World War I. In the 1920s groups of eugenics lobbyists argued that this demonstrated that these groups were of inferior intellect to Anglo-Saxon whites due to innate biological differences, using this as an argument for policies of racial segregation. Soon, other studies appeared, contesting these conclusions and arguing instead that the Army tests had not adequately controlled for the environmental factors such as socio-economic and educational inequality between African-Americans and Whites. The debate reemerged again in 1969, when Arthur Jensen championed the view that for genetic reasons Africans were less intelligent than whites and that compensatory education for African-American children was therefore doomed to be ineffective. In 1994, the book "The Bell Curve," argued that social inequality in America could largely be explained as a result of IQ differences between races and individuals rather than being their cause, and rekindled the public and scholarly debate with renewed force. During the debates following the book's publication the American Anthropological Association and the American Psychological Association (APA) published official statements regarding the issue, both highly skeptical of some of the book's claims, although the APA report called for more empirical research on the issue.
In subsequent decades much research has been published about the relationships between hereditary influences on IQ, group differences in intelligence, race, environmental influences on IQ. Particularly contentious in the ongoing debate has been the definition of both the concept "race" and the concept "intelligence", and especially whether they can in fact be objectively defined and operationalized. While several environmental factors have been shown to affect group differences in intelligence, it has not been demonstrated that they can explain the entire disparity. On the other hand, no genetic factor has been conclusively shown to have a causal relation with group difference in intelligence test scores. Recent summaries of the debate call for more research into the topic to determine the relative contributions of environmental and genetic factors in explaining the apparent IQ disparity among racial groups.
History of the debate.
Claims of races having different intelligence were used to justify colonialism, slavery, racism, social Darwinism, and racial eugenics. Racial thinkers such as Arthur de Gobineau relied crucially on the assumption that black people were innately inferior to Whites in developing their ideologies of White supremacy. Even enlightenment thinkers such as Thomas Jefferson, a slave owner, believed Blacks to be innately inferior to Whites in physique and intellect.
Early IQ testing.
The first practical intelligence test was developed between 1905 and 1908 by Alfred Binet in France for school placement of children. Binet warned that results from his test should not be assumed to measure innate intelligence or used to label individuals permanently. Binet's test was translated into English and revised in 1916 by Lewis Terman (who introduced IQ scoring for the test results) and published under the name the Stanford–Binet Intelligence Scales. As Terman's test was published, there was great concern in the United States about the abilities and skills of recent immigrants. Different immigrant nationalities were sometimes thought to belong to different races, such as Slavs. A different set of tests developed by Robert Yerkes were used to evaluate draftees for World War I, and researchers found that people from southern and eastern Europe scored lower than native-born Americans, That Americans from northern states had higher scores than Americans from southern states, and that Black Americans scored lower than white Americans. The results were widely publicized by a lobby of anti-immigration activists, including the New York patrician and conservationist Madison Grant, who considered the nordic race to be superior, but under threat of immigration by inferior breeds. In his influential work "A Study of American Intelligence" psychologist Carl Brigham used the results of the Army tests to argue for a stricter immigration policy, limiting immigration to countries considered to belong to the "nordic race".
In the 1920s, states like Virginia enacted eugenic laws, such as its 1924 Racial Integrity Act, which established the one-drop rule as law. On the other hand, many scientists reacted to eugenicist claims linking abilities and moral character to racial or genetic ancestry. They pointed to the contribution of environment to test results (such as speaking English as a second language). By the mid-1930s, many United States psychologists adopted the view that environmental and cultural factors played a dominant role in IQ test results, among them Carl Brigham who repudiated his own previous arguments, on the grounds that he realized that the tests were not a measure of innate intelligence. Discussion of the issue in the United States also influenced German Nazi claims of the "nordics" being a "master race", influenced by Grant's writings. As the American public sentiment shifted against the Germans, claims of racial differences in intelligence increasingly came to be regarded as problematic. Anthropologists such as Franz Boas, and Ruth Benedict and Gene Weltfish, did much to demonstrate the unscientific status of many of the claims about racial hierarchies of intelligence. Nonetheless a powerful eugenics and segregation lobby funded largely by textile-magnate Wickliffe Draper, continued to publicize studies using intelligence studies as an argument for eugenics, segregation, anti-immigration legislation.
The Jensenism debates.
As the de-segregation of the American South was begun in the 1950s the debate about Black intelligence resurfaced. Audrey Shuey, funded by Draper's Pioneer fund, published a new analysis of Yerkes' tests, concluding that blacks really were of inferior intellect to whites. This study was used by segregationists as an argument that it was to the advantage of Black children to be educated separately from the superior White children. In the 1960s, the debate was further revived as Nobel laureate William Shockley, publicly defended the argument that Black children were innately unable to learn as well as White children. Arthur Jensen stimulated scholarly discussion of the issue with his "Harvard Education Review" article, "How Much Can We Boost IQ and Scholastic Achievement?" Jensen's article questioned remedial education for African-American children; he suggested their poor educational performance reflected an underlying genetic cause rather than lack of stimulation at home. Jensen continued to publish on the issue until his death in 2012.
The Bell Curve debate.
Another revival of public debate followed the appearance of "The Bell Curve" (1994), a book by Richard Herrnstein and Charles Murray, who strongly emphasized the societal effects of low IQ (focusing in most chapters strictly on the white population of the United States). In 1994 a group of 52 researchers (mostly psychologists) signed an editorial statement "Mainstream Science on Intelligence" in response to the book. "The Bell Curve" also led to a 1995 report from the American Psychological Association, "", acknowledging a difference between mean IQ scores of whites and blacks as well as the absence of any adequate explanation of it, either environmental or genetic. "The Bell Curve" prompted the publication of several multiple-author books responding from a variety of points of view. They include "The Bell Curve Debate" (1995), ' (1996) and a second edition of "The Mismeasure of Man" (1996) by Steven J. Gould. Jensen's last book-length publication, ' was published a few years later in 1998.
The review article "Thirty Years of Research on Race Differences in Cognitive Ability" by Rushton and Jensen was published in 2005. The article was followed by a series of responses, some in support, some critical. Richard Nisbett, another psychologist who had also commented at the time, later included an amplified version of his critique as part of the book "Intelligence and How to Get It: Why Schools and Cultures Count" (2009). Rushton and Jensen in 2010 made a point-by-point reply to this thereafter. A comprehensive review article on the issue was published in the journal "American Psychologist" in 2012.
Some of the authors proposing genetic explanations for group differences have received funding from the Pioneer Fund which was headed by Rushton until his death in 2012. The Southern Poverty Law Center lists the Pioneer Fund as a hate group, citing the fund's history, its funding of race and intelligence research, and its connections with racist individuals. On the other hand, Ulrich Neisser writes that "Pioneer has sometimes sponsored useful research—research that otherwise might not have been done at all." Other researchers have criticized the Pioneer Fund for promoting scientific racism, eugenics and white supremacy.
Validity of race and IQ.
Intelligence, IQ, "g" and IQ tests.
The concept of intelligence and the degree to which intelligence is measurable is a matter of debate. While there is some consensus about how to define intelligence, it is not universally accepted that it is something that can be unequivocally measured by a single figure. A recurring criticism is that different societies value and promote different kinds of skills and that the concept of intelligence is therefore culturally variable and cannot be measured by the same criteria in different societies. Consequently, some critics argue that proposed relationships to other variables are necessarily tentative.
In relation to the study of racial differences in IQ test scores it becomes a crucial question what exactly it is that IQ tests measure. Arthur Jensen has been a proponent of the view, now mainstream in psychometry, that there is a correlation between scores on all the known types of IQ tests and that this correlation points to an underlying factor of general intelligence, or "g". In most conceptions of "g" it is considered to be fairly fixed in a given individual and unresponsive to training or other environmental influences. In this view test score differences, especially in those tasks considered to be particularly "g-loaded" reflect the test takers innate capability.
Other psychometricians argue that, while there may or may not be a general intelligence factor, performance on tests rely crucially on knowledge acquired through prior exposure to the types of tasks that such tests contain. This view would mean that tests cannot be expected to reflect only the innate abilities of a given individual, because the expression of potential will always be mediated by experience and cognitive habits. It also means that comparison of test scores from persons with widely different life experiences and cognitive habits is not an expression of their relative innate potentials.
Race.
The biological validity of race is disputed. The current mainstream view in the social sciences and biology is that race is a social construction based on folk ideologies that construct groups based on social disparities and superficial physical characteristics. state, "Race is a socially constructed concept, not a biological one. It derives from people's desire to classify." The concept of human "races" as natural and separate divisions within the human species has also been rejected by the American Anthropological Association. The official position of the AAA, adopted in 1998, finds that advances in scientific knowledge have made it "clear that human populations are not unambiguous, clearly demarcated, biologically distinct groups" and that "any attempt to establish lines of division among biological populations [is] both arbitrary and subjective." However within population genetics there is ongoing debate about whether the social category of "race" can and should be used as a proxy for individual genetic ancestry. With current methods of genetic analysis it is possible to determine the composition of genetic ancestry of an individual with significant precision. This is because different genes occur with different frequencies in different geographically defined populations, and by correlating a large amount of genes through cluster analysis it is probable to determine with high likelihood the geographic origins of an individual through DNA. This suggests to some that the classical socially defined genetic categories really have a biological basis, in the sense that racial categorization is visual estimate of an a persons continental ancestry based on their phenotype - which correlates with genotypical ancestry as determined by DNA tests.
Race in studies of human intelligence is almost always determined using self-reports, rather than based on analyses of the genetic characteristics of the tested individuals. According to psychologist David Rowe, self-report is the preferred method for racial classification in studies of racial differences because classification based on genetic markers alone ignore the "cultural, behavioral, sociological, psychological, and epidemiological variables" that distinguish racial groups. Hunt and Carlson write that "Nevertheless, self-identification is a surprisingly reliable guide to genetic composition. applied mathematical clustering techniques to sort genomic markers for over 3,600 people in the United States and Taiwan into four groups. There was almost perfect agreement between cluster assignment and individuals' self-reports of racial/ethnic identification as white, black, East Asian, or Latino." Sternberg and Grigorenko disagree with Hunt and Carlson's interpretation of Tang, "Tang et al.'s point was that ancient geographic ancestry rather than current residence is associated with self-identification and not that such self-identification provides evidence for the existence of biological race."
Anthropologist C. Loring Brace and geneticist Joseph Graves contradict the notion that cluster analysis and the correlation between self-reported race and genetic ancestry support biological race. They argue that while it is possible to find biological and genetic variation corresponding roughly to the groupings normally defined as races, this is true for almost all geographically distinct populations. The cluster structure of the genetic data is dependent on the initial hypotheses of the researcher and the populations sampled. When one samples continental groups, the clusters become continental; if one had chosen other sampling patterns, the clusters would be different. therefore concludes that, while differences in particular allele frequencies can be used to identify populations that loosely correspond to the racial categories common in Western social discourse, the differences are of no more biological significance than the differences found between any human populations (e.g., the Spanish and Portuguese).
Earl B. Hunt agrees that racial categories are defined by social conventions, though he points out that they also correlate with clusters of both genetic traits and cultural traits. Hunt explains that, due to this, racial IQ differences are caused by these variables that correlate with race, and race itself is rarely a causal variable. Researchers who study racial disparities in test scores are studying the relationship between the scores and the many race-related factors which could potentially affect performance. These factors include health, wealth, biological differences, and education.
Group differences.
The study of human intelligence is one of the most controversial topics in psychology. It remains unclear whether group differences in intelligence test scores are caused by heritable factors or by "other correlated demographic variables such as socioeconomic status, education level, and motivation."
Hunt and Carlson outlined four contemporary positions on differences in IQ based on race or ethnicity. The first is that these reflect real differences in average group intelligence, which is caused by a combination of environmental factors and heritable differences in brain function. A second position is that differences in average cognitive ability between races are caused entirely by social and/or environmental factors. A third position holds that differences in average cognitive ability between races do not exist, and that the differences in average test scores are the result of inappropriate use of the tests themselves. Finally, a fourth position is that either or both of the concepts of race and general intelligence are poorly constructed and therefore any comparisons between races are meaningless.
United States test scores.
 write that, in the United States, self-identified blacks and whites have been the subjects of the greatest number of studies. They state that the black-white IQ difference is about 15 to 18 points or 1 to 1.1 standard deviations (SDs), which implies that between 11 and 16 percent of the black population have an IQ above 100 (the general population median). According to Arthur Jensen and J. Philippe Rushton the black-white IQ difference is largest on those components of IQ tests that are claimed best to represent the general intelligence factor "g". The 1996 APA report "" and the 1994 editorial statement "Mainstream Science on Intelligence" gave more or less similar estimates. , in a review of the results of a total of 6,246,729 participants on other tests of cognitive ability or aptitude, found a difference in mean IQ scores between blacks and whites of 1.1 SD. Consistent results were found for college and university application tests such as the Scholastic Aptitude Test (N = 2.4 million) and Graduate Record Examination (N = 2.3 million), as well as for tests of job applicants in corporate sections (N = 0.5 million) and in the military (N = 0.4 million).
North East Asians have tended to score relatively higher on visuospatial subtests with lower scores in verbal subtests while Ashkenazi Jews score higher in verbal and reasoning subtests with lower scores in visuospatial subtests. The few Amerindian populations who have been systematically tested, including Arctic Natives, tend to score worse on average than white populations but better on average than black populations.
The racial groups studied in the United States and Europe are not necessarily representative samples for populations in other parts of the world. Cultural differences may also factor in IQ test performance and outcomes. Therefore, results in the United States and Europe do not necessarily correlate to results in other populations.
Global variation of IQ scores.
A number of studies have compared average IQ scores between the world's nations, finding patterns of difference between continental populations similar to those associated with race. Particularly Psychologist Richard Lynn with Tatu Vanhanen has published several books about this topic. Lynn and Vanhanen argue that due to genetic limitations in intelligence particularly in African populations, education cannot be effective in creating social and economic development in third world countries. Lynn and Vanhanen's studies have been severely criticized for relying on low quality data, which is often not of comparable quality between nations. Additionally they have been criticized for having chosen the sources of IQ estimates selectively in ways that seem to be biased severely towards underestimating the average IQ potential of developing nations, particularly in Africa. Nonetheless there is a general consensus that the average IQ in developing countries is lower than in developed countries, but subsequent research has favored environmental explanations for this fact, such as lack of basic infrastructure related to health and education.
In the 2002 book "IQ and the Wealth of Nations", and "IQ and Global Inequality" in 2006, Richard Lynn and Tatu Vanhanen created estimates of average IQs for 113 nations. They estimated IQs of 79 other nations based on neighboring nations or other via other manners. They saw a consistent correlation between national development and national IQ averages. They found the highest national IQs among Western and Asian developed nations and the lowest national IQs in the world's least developed nations in Sub-Saharan Africa and Latin America. 
In a metaanalysis of studies of IQ estimates in Africa, concluded that Lynn and Vanhanen had relied on unsystematic methodology by failing to publish their criteria for including or excluding studies. They found that Lynn and Vanhanen's exclusion of studies had depressed their IQ estimate for sub-Saharan Africa, and that including studies excluded in "IQ and Global Inequality" resulted in average IQ of 82 for sub-Saharan Africa, lower than the average in Western countries, but higher than Lynn and Vanhanen's estimate of 67. Wicherts at al. conclude that this difference is likely due to sub-Saharan Africa having limited access to modern advances in education, nutrition and health care.
A 2007 meta-analysis by Rindermann found many of the same groupings and correlations found by Lynn and Vanhanen, with the lowest scores in sub-Saharan Africa, and a correlation of .60 between cognitive skill and GDP per capita. considers Rindermann's analysis to be much more reliable than Lynn and Vanhanen's. By measuring the relationship between educational data and social well-being over time, this study also performed a causal analysis, finding that when nations invest in education this leads to increased well-being later on.
Flynn effect and the closing gap.
For the past century raw scores on IQ tests have been rising; this score increase is known as the "Flynn effect," named after Jim Flynn. In the United States, the increase was continuous and approximately linear from the earliest years of testing to about 1998 when the gains stopped and some tests even showed decreasing test scores. For example, in the United States the average scores of blacks on some IQ tests in 1995 were the same as the scores of whites in 1945. Flynn has argued that given that these changes take place between one generation and the next it is highly unlikely that genetic factors could account for the increasing scores, which must then be caused by environmental factors. The Flynn Effect has often been used as an argument that the racial gap in IQ test scores must be environmental too, but this is not generally agreed – others have asserted that the two may have entirely different causes. A meta-analysis by Te Nijenhuis and van der Flier (2013) concluded that the Flynn effect and group differences in intelligence were likely to have different causes. They stated that the Flynn effect is caused primarily by environmental factors and that it's unlikely these same environmental factors play an important role in explaining group differences in IQ. The importance of the Flynn effect in the debate over the causes for the IQ gap lies in demonstrating that environmental factors may cause changes in test scores on the scale of 1 SD. This had previously been doubted.
A separate phenomenon from the Flynn effect has been the discovery that the IQ gap has been gradually closing over the last decades of the 20th century, as black test-takers increased their average scores relative to white test-takers. A 2006 study by Dickens and Flynn estimated that the difference between mean scores of blacks and whites closed by about 5 or 6 IQ points between 1972 and 2002, which would be a reduction of about one-third. In the same period the educational achievement disparity also diminished. However this was challenged by Rushton & Jensen who claim the difference remains stable. In a 2006 study, Murray agreed with Dickens and Flynn that there has been a narrowing of the difference; "Dickens' and Flynn's estimate of 3–6 IQ points from a base of about 16–18 points is a useful, though provisional, starting point". But he argued that this has stalled and that there has been no further narrowing for people born after the late 1970s. Recent reviews by Flynn and Dickens (2006), Hunt (2011), Mackintosh (2011), Nisbett et al. 2012 accept the gradual closing of the gap as a fact. cite who consider arguments to the contrary by Rushton, Jensen and Murray to be erroneous.
Some studies reviewed by found that rise in the average achievement of African Americans was caused by a reduction in the number of African American students in the lowest range of scores without a corresponding increase in the number of students in the highest ranges. A 2012 review of the literature found that the IQ gap had diminished by 0.33 standard deviations since first reported.
Environmental influences on group differences in IQ.
The following environmental factors are some of those suggested as explaining a portion of the differences in average IQ between races. These factors are not mutually exclusive with one another, and some may in fact contribute directly to others. Furthermore, the relationship between genetics and environmental factors may be complicated. For example, the differences in socioeconomic environment for a child may be due to differences in genetic IQ for the parents, and the differences in average brain size between races could be the result of nutritional factors. All recent reviews agree that some environmental factors that are unequally distributed between racial groups have been shown to affect intelligence in ways that could contribute to the test score gap. However currently the question is whether these factors can account for the entire gap between white and black test scores, or only part of it. One group of scholars, including Richard Nisbett, James R. Flynn, Joshua Aronson, Diane Halpern, William Dickens, Eric Turkheimer (2012) have argued that the environmental factors so far demonstrated are sufficient to account for the entire gap, Nicholas Mackintosh(2011) considers this a reasonable argument, but argues that probably it is impossible to ever know for sure; Another group including Earl B. Hunt (2010), Arthur Jensen, J. Philippe Rushton and Richard Lynn have argued that this is impossible. Jensen and Rushton consider that it may account for as little as 20% of the gap, while Hunt consider this a vast overstatement. Hunt nonetheless considers it likely that some portion of the gap will eventually be shown to be caused by genetic factors.
Test bias.
A number of studies have reached the conclusion that IQ tests may be biased against certain groups. The validity and reliability of IQ scores obtained from outside the United States and Europe have been questioned, in part because of the inherent difficulty of comparing IQ scores between cultures. Several researchers have argued that cultural differences limit the appropriateness of standard IQ tests in non-industrialized communities.
However, a 1996 report by the American Psychological Association states that controlled studies show that differences in mean IQ scores were not substantially due to bias in the content or administration of the IQ tests. Furthermore, the tests are equally valid predictors of future achievement for black and white Americans. This view is reinforced by Nicholas Mackintosh in his 1998 book "IQ and Human Intelligence", and by a 1999 literature review by . Today test bias in the sense that some test items systematically give White test takers an unfair advantage because of the way the test has been elaborated is no longer considered a likely cause of the test score gap. However reviews by Hunt (2011) and Mackintosh (2011) do admit the possibility that IQ tests measure a cognitive skill that Blacks have less chance to develop, and that there is in this sense a bias in society that causes one group to perform under their true potential on the tests. But both scholars maintain that there is no evidence that current tests are systemically biased against black test takers.
Stereotype threat and minority status.
Stereotype threat is the fear that one's behavior will confirm an existing stereotype of a group with which one identifies or by which one is defined; this fear may in turn lead to an impairment of performance. Testing situations that highlight the fact that intelligence is being measured tend to lower the scores of individuals from racial-ethnic groups who already score lower on average or are expected to score lower. Stereotype threat conditions cause larger than expected IQ differences among groups. Psychometrician Nicholas Mackintosh considers that there is little doubt that the effects of stereotype threat contribute to the IQ gap between blacks and whites.
A large number of studies have shown that systemically disadvantaged minorities, such as the African American minority of the United States generally perform worse in the educational system and in intelligence tests than the majority groups or less disadvantaged minorities such as immigrant or "voluntary" minorities. The explanation of these findings may be that children of caste-like minorities, due to the systemic limitations of their prospects of social advancement, do not have "effort optimism", i.e. they do not have the confidence that acquiring the skills valued by majority society, such as those skills measured by IQ tests, is worthwhile. They may even deliberately reject certain behaviors that are seen as "acting white."
Socioeconomic environment.
Different aspects of the Socioeconomic environment in which children are raised have been shown to correlate with part of the IQ gap, but they do not account for the entire gap. Generally the difference between mean test scores of blacks and whites is not eliminated when individuals and groups are matched on SES, suggesting that the relationship between IQ and SES is not simply one in which SES determines IQ. Rather it may be the case that differences in intelligence, particularly parental intelligence, may also cause differences in SES, making separating the two factors difficult. points out that when controlling for both SES and parental IQ in populations of young children, the gap becomes so small as to be statistically unreliable, and the best predictors of IQ then becomes parental occupation status, mother's verbal comprehension score and nature of parental interaction with the child. Hunt also finds that the correlation between home environment and IQ becomes weaker with age.
However, another set of observations have shown that there is a difference in the causes of variation within low SES and high SES populations. In low SES populations, environmental differences account for a larger degree of the variance than in high SES populations where genetic factors explain a larger portion of the variance. This is taken by to mean that high SES individuals are more likely to be able to develop their full biological potential, whereas low SES individuals are likely to be hindered in their development by adverse environmental conditions. The same review also points out that adoption studies generally are biased towards including only high and high middle SES families, meaning that they will tend to overestimate genetic effects. They also state that studies of adoption from lower-class homes to middle-class homes have shown that such children experience a 12 - 18 pt gain in IQ relative to children who remain in low SES homes.
Health and nutrition.
Environmental factors including lead exposure, breast feeding, and nutrition can significantly affect cognitive development and functioning. For example, iodine deficiency causes a fall, on average, of 12 IQ points. Such impairments may sometimes be permanent, sometimes be partially or wholly compensated for by later growth. The first two years of life is the critical time for malnutrition, the consequences of which are often irreversible and include poor cognitive development, educability, and future economic productivity. The African American population of the United States is statistically more likely to be exposed to many detrimental environmental factors such as poorer neighborhoods, schools, nutrition, and prenatal and postnatal health care. Mackintosh points out that for American Blacks infant mortality is about twice as high as for whites, and low birthweight is twice as prevalent. At the same time white mothers are twice as likely to breastfeed their infants, and breastfeeding is highly correlated with IQ for low birthweight infants. In this way a wide number of health related factors that influence IQ are unequally distributed between the two groups.
The Copenhagen consensus in 2004 stated that lack of both iodine and iron has been implicated in impaired brain development, and this can affect enormous numbers of people: it is estimated that one-third of the total global population are affected by iodine deficiency. In developing countries, it is estimated that 40% of children aged four and under suffer from anaemia because of insufficient iron in their diets.
Other scholars have found that simply the standard of nutrition has a significant effect on population intelligence, and that the Flynn effect may be caused by increasing nutrition standards across the world. James Flynn has himself argued against this view.
Some recent research has argued that the retardation caused in brain development by infectious diseases, many of which are more prevalent in non-White populations, may be an important factor in explaining the differences in IQ between different regions of the world. The findings of this research, showing the correlation between IQ, race and infectious diseases was also shown to apply to the IQ gap in the US, suggesting that this may be an important environmental factor.
Education.
Several studies have proposed that a large part of the gap can be attributed to differences in quality of education. Racial discrimination in education has been proposed as one possible cause of differences in educational quality between races. According to a paper by Hala Elhoweris, Kagendo Mutua, Negmeldin Alsheikh and Pauline Holloway, teachers' referral decisions for students to participate in gifted and talented educational programs were influenced in part by the students' ethnicity.
The Abecedarian Early Intervention Project, an intensive early childhood education project, was also able to bring about an average IQ gain of 4.4 points at age 21 in the black children who participated in it compared to controls. Arthur Jensen agreed that the Abecedarian project demonstrates that education can have a significant effect on IQ, but also said that no educational program thus far has been able to reduce the black-white IQ gap by more than a third, and that differences in education are thus unlikely to be its only cause.
Rushton and Jensen argue that long-term follow-up of the Head Start Program found large immediate gains for blacks and whites but that these were quickly lost for the blacks although some remained for whites. They argue that also other more intensive and prolonged educational interventions have not produced lasting effects on IQ or scholastic performance. Nisbett argues that they ignore studies such as which found that at the age 12, 87% black of infants exposed to an intervention had IQs in the normal range (above 85) compared to 56% of controls, and none of the intervention-exposed children were mildly retarded compared to 7% of controls. Other early intervention programs have shown IQ effects in the range of 4–5 points, which are sustained until at least age 8–15. Effects on academic achievement can also be substantial. Nisbett also argues that not only early age intervention can be effective, citing other successful intervention studies from infancy to college.
A series of studies by Fagan and Holland, measured the effect of prior exposure to the kind of cognitive tasks posed in IQ tests on test performance. Assuming that the IQ gap was the result of lower exposure to tasks using the cognitive functions usually found in IQ tests among African American test takes, they prepared a group of African Americans in this type of tasks before taking an IQ test. The researchers found that there was no subsequent difference in performance between the African-Americans and White test takers. Daley and Onwugbuezie conclude that Fagan and Holland demonstrate that "differences in knowledge between Blacks and Whites for intelligence test items can be erased when equal opportunity is provided for exposure to the information to be tested". A similar argument is made by David Marks who argues that IQ differences correlate well with differences in literacy suggesting that developing literacy skills through education causes an increase in IQ test performance.
Research into the possible genetic influences on test score differences.
It is well-established that intelligence is highly heritable for individuals, and many different kinds of genetically caused intelligence impairments are known. But the possible relations between genetic differences in intelligence within the normal range are not established. Ongoing research aims to understand the contribution of genes to individual differences in intelligence. Currently there is no non-circumstantial evidence that the test score gap has a genetic component, although some researchers believe that the existing circumstantial evidence makes it plausible to believe that hard evidence for a genetic component will eventually appear. Several lines of investigation have been followed in the attempt to ascertain whether there is a genetic component to the test score gap as well as its relative contribution to the magnitude of the gap.
Genetics of race and intelligence.
The decoding of the human genome has enabled scientists to search for sections of the genome that may contribute to cognitive abilities.
Geneticist, Alan R. Templeton argues that the question about the possible genetic effects on the test score gap is muddled by the general focus on "race" rather than on populations defined by gene frequency or by geographical proximity, and by the general insistence on phrasing the question in terms of heritability. Templeton points out that racial groups neither represent sub-species nor distinct evolutionary lineages, and that therefore there is no basis for making claims about the general intelligence of races. From this point of view the search for possible genetic influences on the black-white test score gap is a priori flawed, because there is no genetic material shared by all Africans or by all Europeans. points out that by using genetic cluster analysis to correlate gene frequencies with continental populations it could possibly be the case that African populations had a higher frequency of certain genetic variants that contribute to an average lower intelligence. Such a hypothetical situation could hold without all Africans carrying the same genes or belonging to a single Evolutionary lineage. According to Mackintosh, a biological basis for the gap thus cannot be ruled out on a priori grounds.
Intelligence is a polygenic trait. This means that intelligence is under the influence of several genes, possibly several thousand. The effect of most individual genetic variants on intelligence is thought to be very small, well below 1% of the variance in "g". Current studies using quantitative trait loci have yielded little success in the search for genes influencing intelligence. Robert Plomin is confident that QTLs responsible for the variation in IQ scores exist, but due to their small effect sizes, more powerful tools of analysis will be required to detect them. Others assert that no useful answers can be reasonably expected from such research before an understanding of the relation between DNA and human phenotypes emerges. Several candidate genes have been proposed to have a relationship with intelligence. However, a review of candidate genes for intelligence published in failed to find evidence of an association between these genes and general intelligence, stating "there is still almost no replicated evidence concerning the individual genes, which have variants that contribute to intelligence differences".
A 2005 literature review article by Sternberg, Grigorenko and Kidd stated that no gene has been shown to be linked to intelligence, "so attempts to provide a compelling genetic link of race to intelligence are not feasible at this time". and concurred, both scholars noting that while several environmental factors have been shown to influence the IQ gap, the evidence for a genetic influence has been circumstantial, and according to Mackintosh negligible. Mackintosh however suggests that it may never become possible to account satisfyingly for the relative contributions of genetic and environmental factors. The 2012 review by the concluded that "Almost no genetic polymorphisms have been discovered that are consistently associated with variation in IQ in the normal range". Hunt and several other researchers however maintain that genetic causes cannot be ruled out and that new evidence may yet show a genetic contribution to the gap. Hunt concurs with Rushton and Jensen who considered the 100% environmental hypothesis to be impossible. Nonetheless, Nisbett and colleagues (2012) consider the entire IQ gap to be explained by the environmental factors that have thus far been demonstrated to influence it, and Mackintosh does not find this view to be unreasonable.
Heritability within and between groups.
Intelligence as tested by IQ tests is generally considered to be highly heritable. Psychometricians have found that intelligence is substantially heritable within populations, with 30–50% of variance in IQ scores in early childhood being attributable to genetic factors in analyzed US populations, increasing to 75–80% by late adolescence. In biology heritability is defined as the ratio of variation attributable to genetic differences in an observable trait to the trait's total observable variation. The heritability of a trait describes the proportion of variation in the trait that is attributable to genetic factors within a particular population. A heritability of 1 indicates that variation correlates fully with genetic variation and a heritability of 0 indicates that there is no correlation between the trait and genes at all. In psychological testing heritability tends to be understood as the degree of correlation between the results of a test taker and those of their biological parents. However, since high heritability is simply a correlation between traits and genes, it does not describe the causes of heritability which in humans can be either genetic or environmental.
Therefore, a high heritability measure does not imply that a trait is genetic or unchangeable, however, as environmental factors that affect all group members equally will not be measured by heritability and the heritability of a trait may also change over time in response to changes in the distribution of genes and environmental factors. High heritability also doesn't imply that all of the heritability is genetically determined, but can also be due to environmental differences that affect only a certain genetically defined group (indirect heritability). The figure to the left demonstrates how heritability works. In both gardens the difference between tall and short cornstalks is 100% heritable as cornstalks that are genetically disposed for growing tall will become taller than those without this disposition, but the difference in height between the cornstalks to the left and those on the right is 100% environmental as it is due to different nutrients being supplied to the two gardens. Hence the causes of differences within a group and between groups may not be the same, even when looking at traits that are highly heritable.
In regards to the IQ gap the question becomes whether racial groups can be shown to be influenced by different environmental factors that may account for the observed differences between them. Jensen originally argued that given the high heritability of IQ the only way that the IQ gap could be explained as caused by the environment would be if it could be shown that all blacks were subject to a single "x-factor" which affected no white populations while affecting all black populations equally. Jensen considered the existence of such an x-factor to be extremely improbable, but Flynn's discovery of the Flynn effect showed that in spite of high heritability environmental factors could cause considerable disparities in IQ between generations of the same population, showing that the existence of such an x-factor was not only possible but real.
Jensen has also argued that heritability of traits rises with age as the genetic potential of individuals becomes expressed. He sees this as related to the fact that the IQ gap between white and black test takers has been shown to appear gradually, with the gap widening as cohorts reach adulthood. This he sees as a further argument in favor of Spearman's hypothesis (see section below).
In contrast, Dickens and Flynn argued that the conventional interpretation ignores the role of feedback between factors, such as those with a small initial IQ advantage, genetic or environmental, seeking out more stimulating environments which will gradually greatly increase their advantage, which, as one consequence in their alternative model, would mean that the "heritability" figure is only in part due to direct effects of genotype on IQ.
Today researchers such as , and consider that rather than a single factor accounting for the entire gap, probably many different environmental factors differ systematically between the environments of White and Black people converge to create part of the gap and perhaps all of it. They argue that it does not make sense to talk about a single universal heritability figure for IQ, rather, they state, heritability of IQ varies between and within groups. They point specifically to studies showing a higher heritability of test scores in White and medium-high SES families, but considerably lower heritability for Black and low-SES families. This they interpret to mean that children who grow up with limited resources do not get to develop their full genetic potential.
Spearman's hypothesis.
Spearman's hypothesis states that the magnitude of the black-white difference in tests of cognitive ability is entirely or mainly a function of the extent to which a test measures general mental ability, or "g". The hypothesis was first formalized by Arthur Jensen who devised the statistical Method of Correlated Vectors to test it. Jensen holds that if Spearman's hypothesis holds true then some cognitive tasks have a higher g-load than others, and that these tasks are exactly the tasks in which the gap between Black and White test takers are greatest. This he (and other psychometricians such as Rushton and Lynn) takes to show that the cause of "g" and the cause of the gap are the same - in their view genetic differences.
 acknowledges that Jensen and Rushton have shown a modest correlation between g-loading, heritability, and the test score gap, but he does not accept that this demonstrates a genetic origin of the gap. He points out that it is exactly in those the tests that Rushton and Jensen consider to have the highest g-loading and heritability such as the Wechsler that has seen the highest increases due to the Flynn effect. This suggests that they are also the most sensitive to environmental changes. And in turn if the highly g-loaded tests are both more liable to environmental influences and as Jensen argues the ones where the black-white gap is most pronounced, it suggests in fact contrary to Jensen's argument that the gap is most likely caused by environmental factors. Mackintosh also argues that Spearman's hypothesis, which he considers to be likely to be correct, simply shows that the test score gap is based on whatever cognitive faculty is central to intelligence - but not what this factor is. make the same point, noting also that the increase in the IQ scores of Black test takers is necessarily also an increase in "g".
James argues that there is an inherent flaw in Jensen's argument that the correlation between g-loadings, test scores and heritability support a genetic cause of the gap. He points out that as the difficulty of a task increases a low performing group will naturally fall further behind, and heritability will therefore also naturally increase. The same holds for increases in performance which will first affect the least difficult tasks, but only gradually affect the most difficult ones. Flynn thus sees the correlation between in g-loading and the test score gap to offer no clue to the cause of the gap.
 states that many of conclusions of Jensen, and his colleagues rest on the validity of Spearman's hypothesis, and the method of correlated vectors used to test it. Hunt points out that other researchers have found this method of calculation to produce false positive results, and that other statistical methods should be used instead. According to Hunt, Jensen and Rushton's frequent claim that Spearman's hypothesis should be regarded as empirical fact does not hold, and that new studies based on better statistical methods would be required to confirm or reject the hypothesis that the correlation between g-loading, heritability and the IQ gap is due to IQ gaps consisting mostly of g.
Adoption studies.
A number of studies have been done on the effect of similar rearing conditions on children from different races. The hypothesis is that by investigating whether black children adopted into white families demonstrated gains in IQ test scores relative to black children reared in black families. Depending on whether their test scores are more similar to their biological or adoptive families, that could be interpreted as either supporting a genetic or an environmental hypothesis. The main point of critique in studies like these however whether the environment of black children even when raised in White families are truly comparable to the environment of White children. Several reviews of the adoption study literature has pointed out that it is perhaps impossible to avoid confounding of biological and environmental factors in this type of studies. Given the differing heritability estimates in medium-high SES and low-SES families, argue that adoption studies on the whole tend to overstate the role of genetics because they represent a restricted set of environments, mostly in the medium-high SES range.
The Minnesota Transracial Adoption Study (1976) examined the IQ test scores of 122 adopted children and 143 nonadopted children reared by advantaged white families. The children were restudied ten years later. The study found higher IQ for whites compared to blacks, both at age 7 and age 17. cite the Minnesota study as providing support to a genetic explanation. Nonetheless, acknowledging the existence of confounding factors, Scarr and Weinberg the authors of the original study, did not themselves consider that it provided support for either the hereditarian or environmentalist view.
Three other adoption studies found contrary evidence to the Minnesota study, lending support to a mostly environmental hypothesis: 
Rushton and Jensen have argued that unlike the Minnesota Transracial Adoption Study, these studies did not retest the children post-adolescence when heritability of IQ would presumably be higher. however point out that the difference in heritability between ages 7 and 17 are quite small, and that consequently this is no reason to disregard Moore's findings.
Frydman and Lynn (1989) showed a mean IQ of 119 for Korean infants adopted by Belgian families. After correcting for the Flynn effect, the IQ of the adopted Korean children was still 10 points higher than the indigenous Belgian children.
Reviewing the evidence from adoption studies Mackintosh considers the studies by Tizard and Eyferth to be inconclusive, and the Minnesota study to be consistent only with a partial genetic hypothesis. On the whole he finds that environmental and genetic variables remain confounded and considers evidence from adoption studies inconclusive on the whole, and fully compatible with a 100% environmental explanation.
Racial admixture studies.
Most people have an ancestry from different geographic regions, particularly African Americans typically have ancestors from both Africa and Europe, with, on average, 20% of their genome inherited from European ancestors. If racial IQ gaps have a partially genetic basis, one might expect blacks with a higher degree of European ancestry to score higher on IQ tests than blacks with less European ancestry, because the genes inherited from European ancestors would likely include some genes with a positive effect on IQ. Geneticist Alan Templeton has argued that an experiment based on the Mendelian "common garden" design where specimens with different hybrid compositions are subjected to the same environmental influences, would be the only way to definitively show a causal relation between genes and IQ. Summarizing the findings of admixture studies, he concludes that it has shown no significant correlation between any cognitive and the degree of African or European ancestry.
Studies have employed different ways of measuring or approximating relative degrees of ancestry from Africa and Europe. One set of studies have used skin color as a measure, and other studies have used blood groups. surveys the literature and argues that the blood groups studies may be seen as providing some support to the genetic hypothesis, even though the correlation between ancestry and IQ was quite low. He finds that studies by , Willerman, Naylor & Myrianthopoulos (1970) did not find a correlation between degree of African&/European ancestry and IQ. The latter study did find a difference based on the race of the mother, with children of white mothers with black fathers scoring higher than children of black mothers and white fathers. Loehlin considers that such a finding is compatible with either a genetic or an environmental cause. All in all Loehlin finds admixture studies inconclusive and recommends more research.
Another study cited by , and by , was study which found that adopted mixed-race children's has test scores identical to children with two black parents - receiving no apparent "benefit" from their white ancestry. Rushton and Jensen find admixture studies to have provided overall support for a genetic explanation though this view is not shared by , , , nor by .
Reviewing the evidence from admixture studies considers it to be inconclusive because of too many uncontrolled variables. quotes a statement by to the effect that admixture studies have not provided a shred of evidence in favor of a genetic basis for the gap.
Mental chronometry.
Mental chronometry measures the elapsed time between the presentation of a sensory stimulus and the subsequent behavioral response by the participant. This reaction time (RT) is considered a measure of the speed and efficiency with which the brain processes information. Scores on most types of RT tasks tend to correlate with scores on standard IQ tests as well as with "g", and no relationship has been found between RT and any other psychometric factors independent of "g". The strength of the correlation with IQ varies from one RT test to another, but Hans Eysenck gives 0.40 as a typical correlation under favorable conditions. According to Jensen individual differences in RT have a substantial genetic component, and heritability is higher for performance on tests that correlate more strongly with IQ. Nisbett argues that some studies have found correlations closer to 0.2, and that the correlation is not always found.
Several studies have found differences between races in average reaction times. These studies have generally found that reaction times among black, Asian and white children follow the same pattern as IQ scores. have argued that reaction time is independent of culture and that the existence of race differences in average reaction time is evidence that the cause of racial IQ gaps is partially genetic instead of entirely cultural. Responding to this argument in "Intelligence and How to Get It", Nisbett has pointed to the study in which a group of Chinese Americans had longer reaction times than a group of European Americans, despite having higher IQs. Nisbett also mentions findings in and suggesting that movement time (the measure of how long it takes a person to move a finger after making the decision to do so) correlates with IQ just as strongly as reaction time, and that average movement time is faster for blacks than for whites. considers reaction time evidence unconvincing and points out that other cognitive tests that also correlate well with IQ show no disparity at all, for example the habituation/dishabituation test. And he points out that studies show that rhesus monkeys have shorter reaction times than American college students, suggesting that different reaction times may not tell us anything useful about intelligence.
Brain size.
A number of studies have reported a moderate statistical correlation between differences in IQ and brain size between individuals in the same group. And some scholars have reported differences in average brain sizes between Africans, Europeans and Asians. J. P. Rushton has argued that Africans on average have smaller brain cases and brains than Europeans, and that this is evidence that the gap is biological in nature. Critics of Rushton have argued that Rushton's arguments rest on outdated data collected by unsound methods and should be considered invalid. Recent reviews by and consider that current data does show an average difference in brain size and head-circumference between American Blacks and Whites, but question whether this has any relevance for the IQ gap. Nesbitt et al. argue that crude brain size is unlikely to be a good measure of IQ, for example brain size also differs between men and women, but without well documented differences in IQ. At the same time newborn Black children have the same average brain size as Whites, suggesting that the difference in average size could be accounted for by differences in postnatal environment. Several factors that reduce brain size have been demonstrated to disproportionately affect Black children.
Earl Hunt states that brain size is found to have a correlation of about .35 with intelligence among whites and cites studies showing that genes may account for as much as 90% of individual variation in brain size. According to Hunt, race differences in average brain size could potentially be an important argument for a possible genetic contribution to racial IQ gaps. Nonetheless, Hunt notes that Rushton's head size data would account for a difference of .09 standard deviations between Black and White average test scores, less than a tenth of the 1.0 standard deviation gap in average scores that is observed.
Policy relevance and ethics.
The 1996 report of the APA commented on the ethics of research on race and intelligence. as well as have also discussed different possible ethical guidelines. "Nature" in 2009 featured two editorials on the ethics of research in race and intelligence by Steven Rose (against) and Stephen J. Ceci and Wendy M. Williams (for).
According to critics, research on group differences in IQ will run the risk of reproducing the negative effects of social ideologies (such as Nazism or social Darwinism) that were justified in part on claimed hereditary racial differences. Steven Rose maintains that the history of eugenics makes this field of research difficult to reconcile with current ethical standards for science.
Linda Gottfredson argues that suggestion of higher ethical standards for research into group differences in intelligence is a double standard applied in order to undermine disliked results. James R. Flynn has argued that had there been a ban on research on possibly poorly conceived ideas, much valuable research on intelligence testing (including his own discovery of the Flynn effect) would not have occurred.
Jensen and Rushton argued that the existence of biological group differences does not rule out, but raises questions about the worthiness of policies such as affirmative action or placing a premium on diversity. They also argued for the importance of teaching people not to overgeneralize or stereotype individuals based on average group differences, because of the significant overlap of people with varying intelligence between different races.
The environmentalist viewpoint argues for increased interventions in order to close the gaps. Nisbett argues that schools can be greatly improved and that many interventions at every age level are possible. Flynn, arguing for the importance of the black subculture, writes that "America will have to address all the aspects of black experience that are disadvantageous, beginning with the regeneration of inner city neighbourhoods and their schools. A resident police office and teacher in every apartment block would be a good start." Researchers from both sides agree that interventions should be better researched.
Especially in developing nations, society has been urged to take on the prevention of cognitive impairment in children as of the highest priority. Possible preventable causes include malnutrition, infectious diseases such as meningitis, parasites, cerebral malaria, in utero drug and alcohol exposure, newborn asphyxia, low birth weight, head injuries, lead poisoning and endocrine disorders.
Bibliography.
</dl>

</doc>
<doc id="26555" url="http://en.wikipedia.org/wiki?curid=26555" title="Rashi">
Rashi

Shlomo Yitzchaki (Hebrew: רבי שלמה יצחקי‎; 22 February 1040 – 13 July 1105), in Latin Salomon Isaacides, and today generally known by the acronym Rashi (Hebrew: רש"י‎, RAbbi SHlomo Itzhaki), was a medieval French rabbi and author of a comprehensive commentary on the Talmud and commentary on the "Tanakh". Acclaimed for his ability to present the basic meaning of the text in a concise and lucid fashion, Rashi appeals to both learned scholars and beginning students, and his works remain a centerpiece of contemporary Jewish study. His commentary on the Talmud, which covers nearly all of the Babylonian Talmud (a total of 30 tractates), has been included in every edition of the Talmud since its first printing by Daniel Bomberg in the 1520s. His commentary on Tanach—especially on the Chumash ("Five Books of Moses")—is an indispensable aid to students of all levels. The latter commentary alone serves as the basis for more than 300 "supercommentaries" which analyze Rashi's choice of language and citations, penned by some of the greatest names in rabbinic literature.
Name.
Rashi's surname, Yitzhaki, derives from his father's name, Yitzhak. The acronym is sometimes also fancifully expanded as Rabban Shel YIsrael which means the rabbi of Israel, or as Rabbenu SheYichyeh (Our Rabbi, may he live). He may be cited in Hebrew and Aramaic texts as (1) "Shlomo son of Rabbi Yitzhak," (2) "Shlomo son of Yitzhak," (3) "Shlomo Yitzhaki," etc.
In older literature, Rashi is sometimes referred to as "Jarchi" or "Yarhi" (ירחי), his abbreviated name being interpreted as Rabbi Shlomo Yarhi. This was understood to refer to the Hebrew name of Lunel in Provence, popularly derived from the French "lune" "moon", in Hebrew ירח, in which Rashi was assumed to have lived at some time or to have been born, or where his ancestors were supposed to have originated. Simon and Wolf claimed that only Christian scholars referred to Rashi as Jarchi, and that this epithet was unknown to the Jews. Bernardo de Rossi, however, demonstrated that Hebrew scholars also referred to Rashi as Yarhi. In 1839, Leopold Zunz showed that the Hebrew usage of Jarchi was an erroneous propagation of the error by Christian writers, instead interpreting the abbreviation as it is understood today: Rabbi Shlomo Yitchaki. The evolution of this term has been thoroughly traced.
Biography.
Birth and early life.
Rashi was an only child born at Troyes, Champagne, in northern France. His mother's brother was Simon the Elder, Rabbi of Mainz. Simon was a disciple of Rabbeinu Gershom Meor HaGolah, who died that same year. On his father's side, Rashi has been claimed to be a 33rd generation descendant of Yochanan Hasandlar, who was a fourth-generation descendant of Gamaliel the Elder, who was reputedly descended from the royal line of King David. In his voluminous writings, Rashi himself made no such claim at all. The main early rabbinical source about his ancestry, Responsum No. 29 by Solomon Luria, makes no such claim either.
Legends.
His fame later made him the subject of many legends. One tradition contends that his parents were childless for many years. Rashi's father, Yitzhak, a poor winemaker, once found a precious jewel and was approached by non-Jews who wished to buy it to adorn their idol. Yitzhak agreed to travel with them to their land, but en route, he cast the gem into the sea. Afterwards he was visited by either the Voice of God or the prophet Elijah, who told him that he would be rewarded with the birth of a noble son "who would illuminate the world with his Torah knowledge."
Legend also states that the couple moved to Worms, Germany while Rashi's mother was expecting. As she walked down one of the narrow streets in the Jewish quarter, she was imperiled by two oncoming carriages. She turned and pressed herself against a wall, which opened to receive her. This miraculous niche is still visible in the wall of the Worms Synagogue.
Yeshiva studies.
According to tradition, Rashi was first brought to learn Torah by his father on Shavuot day at the age of five. His father was his main Torah teacher until his death when Rashi was still a youth. At the age of 17 he married and soon after went to learn in the yeshiva of Rabbi Yaakov ben Yakar in Worms, returning to his wife three times yearly, for the Days of Awe, Passover and Shavuot. When Rabbi Yaakov died in 1064, Rashi continued learning in Worms for another year in the yeshiva of his relative, Rabbi Isaac ben Eliezer Halevi, who was also chief rabbi of Worms. Then he moved to Mainz, where he studied under another of his relatives, Rabbi Isaac ben Judah, the rabbinic head of Mainz and one of the leading sages of the Lorraine region straddling France and Germany.
Rashi's teachers were students of Rabbeinu Gershom and Rabbi Eliezer Hagadol, leading Talmudists of the previous generation. From his teachers, Rashi imbibed the oral traditions pertaining to the Talmud as they had been passed down for centuries, as well as an understanding of the Talmud's unique logic and form of argument. Rashi took concise, copious notes from what he learned in yeshiva, incorporating this material in his commentaries.
Rosh yeshiva.
He returned to Troyes at the age of 25, after which time his mother died, and he was asked to join the Troyes "Beth din" (rabbinical court). He also began answering halakhic questions. Upon the death of the head of the "Bet din", Rabbi Zerach ben Abraham, Rashi assumed the court's leadership and answered hundreds of halakhic queries.
In around 1070 he founded a yeshiva which attracted many disciples. It is thought by some that Rashi earned his living as a vintner since Rashi shows an extensive knowledge of its utensils and process, but there is no evidence for this. Most scholars and a Jewish oral tradition contend that he was a vintner. The only reason given for the centuries-old tradition that he was a vintner being not true is that the soil in all of Troyes is not optimal for wine growing grapes, claimed by the research of Rabbi Haym Soloveitchik. Earlier references such as a reference to an actual seal from his vineyard are said not to prove he sold wine just fermented his grapes for his own use.
Although there are many legends about his travels, Rashi likely never went further than from the Seine to the Rhine; the utmost limit of his travels were the yeshivas of Lorraine.
In 1096, the People's Crusade swept through the Lorraine, murdering 12,000 Jews and uprooting whole communities. Among those murdered in Worms were the three sons of Rabbi Isaac ben Eliezer Halevi, Rashi's teacher. Rashi wrote several "Selichot" (penitential poems) mourning the slaughter and the destruction of the region's great yeshivot. Seven of Rashi's "Selichot" still exist, including "Adonai Elohei Hatz'vaot"", which is recited on the eve of Rosh Hashanah, and "Az Terem Nimtehu", which is recited on the Fast of Gedalia.
Death and burial site.
Rashi died on July 13, 1105 (Tammuz 29, 4865) aged 65. He was buried in Troyes. The approximate location of the cemetery in which he was buried was recorded in "Seder Hadoros", but over time the location of the cemetery was forgotten. A number of years ago, a Sorbonne professor discovered an ancient map depicting the site of the cemetery, which now lay under an open square in the city of Troyes. After this discovery, French Jews erected a large monument in the center of the square—a large, black and white globe featuring the three Hebrew letters of רשי artfully arranged counterclockwise in negative space, evoking the style of Hebrew microcalligraphy. The granite base of the monument is engraved: "Rabbi Shlomo Yitzchaki — Commentator and Guide".
In 2005, Yisroel Meir Gabbai erected an additional plaque at this site marking the square as a burial ground. The plaque reads: "The place you are standing on is the cemetery of the town of Troyes. Many Rishonim are buried here, among them Rabbi Shlomo, known as Rashi the holy, may his merit protect us".
Descendants.
Rashi had no sons, but his three daughters, Miriam, Yocheved, and Rachel, all married Talmudic scholars. Legends exist that Rashi's daughters put on tefillin. While some women in medieval Ashkenaz did wear tefillin, there is no evidence that Rashi's daughters did so.
Works.
Commentary on the Tanach.
Rashi's commentary on the Tanakh—and especially his commentary on the Chumash—is the essential companion for any study of the Bible at any level. Drawing on the breadth of Midrashic, Talmudic and Aggadic literature (including literature that is no longer extant), as well as his knowledge of grammar, halakhah, and how things work, Rashi clarifies the "simple" meaning of the text so that a bright child of five could understand it. At the same time, his commentary forms the foundation for some of the most profound legal analysis and mystical discourses that came after it. Scholars debate why Rashi chose a particular Midrash to illustrate a point, or why he used certain words and phrases and not others. Rabbi Shneur Zalman of Liadi wrote that "Rashi's commentary on Torah is the 'wine of Torah'. It opens the heart and uncovers one's essential love and fear of G-d."
Scholars believe that Rashi's commentary on the Torah grew out of the lectures he gave to his students in his yeshiva, and evolved with the questions and answers they raised on it. Rashi only completed this commentary in the last years of his life. It was immediately accepted as authoritative by all Jewish communities, Ashkenazi and Sephardi alike.
The first dated Hebrew printed book was Rashi's commentary on the Chumash, printed by Abraham ben Garton in Reggio di Calabria, Italy, 18 February 1475. (This version did not include the text of the Chumash itself.)
Rashi wrote commentaries on all the books of Tanakh except Chronicles I & II. Scholars believe that the commentary which appears under Rashi's name in those books was compiled by the students of Rabbi Saadiah of the Rhine, who incorporated material from Rashi's yeshiva. Rashi's students, Rabbi Shemaya and Rabbi Yosef, edited the final commentary on the Torah; some of their own notes and additions also made their way into the version we have today.
Today, tens of thousands of men, women and children study "Chumash with Rashi" as they review the Torah portion to be read in synagogue on the upcoming Shabbat. According to halakha, a man may even study the Rashi on each Torah verse in fulfillment of the requirement to review the Parsha twice with Targum (which normally refers to Targum Onkelos) This practice is called in Hebrew: "Shnayim mikra ve-echad targum". Since its publication, Rashi's commentary on the Torah is standard in almost all Chumashim produced within the Orthodox Jewish community.
Rabbi Mordechai Leifer of Nadvorna said that anyone who learns the weekly Parsha together with the commentary by Rashi every week, is guaranteed to sit in the Yeshiva (school) of Rashi in the Afterlife.
Commentary on the Talmud.
Rashi wrote the first comprehensive commentary on the Talmud. Rashi's commentary, drawing on his knowledge of the entire contents of the Talmud, attempts to provide a full explanation of the words and of the logical structure of each Talmudic passage. Unlike other commentators, Rashi does not paraphrase or exclude any part of the text, but elucidates phrase by phrase. Often he provides punctuation in the unpunctuated text, explaining, for example, "This is a question"; "He says this in surprise," "He repeats this in agreement," etc.
As in his commentary on the Tanakh, Rashi frequently illustrates the meaning of the text using analogies to the professions, crafts, and sports of his day. He also translates difficult Hebrew or Aramaic words into the spoken French language of his day, giving latter-day scholars a window into the vocabulary and pronunciation of Old French.
Rashi exerted a decisive influence on establishing the correct text of the Talmud. Up to and including his age, texts of each Talmudic tractate were copied by hand and circulated in yeshivas. Errors often crept in: sometimes a copyist would switch words around, and other times incorporate a student's marginal notes into the main text. Because of the large number of merchant-scholars who came from throughout the Jewish world to attend the great fairs in Troyes, Rashi was able to compare different manuscripts and readings in Tosefta, Jerusalem Talmud, Midrash, Targum, and the writings of the Geonim, and determine which readings should be preferred. However, in his humility, he deferred to scholars who disagreed with him. For example, in Chulin 4a, he comments about a phrase, "We do not read this. But as for those who do, this is the explanation..."
Rashi's commentary, which covers nearly all of the Babylonian Talmud (a total of 30 tractates), has been included in every version of the Talmud since its first printing in the fifteenth century. It is always situated towards the middle of the opened book display; i.e., on the side of the page closest to the binding.
Some of the other printed commentaries which are attributed to Rashi were composed by others, primarily his students. In some commentaries, the text indicates that Rashi died before completing the tractate, and that it was completed by a student. This is true of the tractate Makkot, the concluding portions of which were composed by his son-in-law, Rabbi Judah ben Nathan, and of the tractate Bava Batra, finished (in a more detailed style) by his grandson, the Rashbam. There is a legend that the commentary on Nedarim, which is clearly not his, was actually composed by his daughters. Another legend states that Rashi died while writing a commentary on Talmud, and that the very last word he wrote was 'tahor,' which means pure in Hebrew - indicating that his soul was pure as it left his body.
Responsa.
About 300 of Rashi's responsa and halakhic decisions are extant. These responsa were copied and preserved by his students. "Siddur Rashi", compiled by an unknown student, also contains Rashi's responsa on prayer. Other compilations include "Sefer Hapardes", edited by Rabbi Shemayah, Rashi's student, and "Sefer Haorah", prepared by Rabbi Nathan Hamachiri.
Influence in non-Jewish circles.
Rashi also influenced non-Jewish circles. His commentaries on the Bible circulated in many different communities especially his commentaries on the Pentateuch. In the 12th–17th centuries, Rashi's influence spread from French and German provinces to Spain and the east. He had a tremendous influence on Christian scholars. The French monk Nicolas de Lyre of Manjacoria, who was known as the "ape of Rashi", was dependent on Rashi when writing the 'Postillae Perpetuate' on the Bible. He believed that Rashi's commentaries were the "official repository of Rabbinical tradition". and significant to understanding the Bible. De Lyre also had great influence on Martin Luther. Rashi's commentaries became significant to humanists at this time who studied grammar and exegesis. Christian Hebraists studied Rashi's commentaries as important interpretations "authorized by the Synagogue".
Rashi's influence grew the most in the 15th century; from the 17th century onwards, his commentaries were translated into many other languages. Rashi's commentary on the Pentateuch was known as the first printed Hebrew work. Many of his works were translated into English by M. Rosenbaum and A.M. Silbermann in London from 1929–1934. Although Rashi had an influence on communities outside of Judaism, his lack of connection to science prevented him from entering the general domain and he remained more popular among the Jewish community.
Criticism of Rashi.
Although Rashi's interpretations were widely respected, there are many who criticize his work. After the 12th century, criticism on Rashi's commentaries became common on Jewish works such as the Talmud. The criticisms mainly dealt with difficult passages. Generally Rashi provides the "pshat" or literal meaning of Jewish texts, while his disciples known as the Tosafot, criticized his work and gave more interpretative descriptions of the texts. The Tosafot's commentaries can be found in Jewish texts opposite Rashi's commentary. The Tosafot added comments and criticism in places where Rashi had not added comments. The Tosafot went beyond the passage itself in search of arguments, parallels, and distinctions that could be drawn out. This addition to Jewish texts was seen as causing a "major cultural product" which became an important part of Torah study.
Additionally Rashi was according to different scholars a corporealist, this could indeed be argued when one reads Rashi´s commentaries on a sentence in the old testament: "And Pharaoh shall not listen to you, and I shall give My hand upon Egypt, and I shall take out my host, my people, the Children of Israel,from the land of Egypt, with great judgments". (Exodus 7:4)
Rashi comments (on the Pentateuch): "My hand"—An actual hand (yad mamash), with which to smite them. This is brought as one of several evidential cases to which one can conclude that Rashi was a corporealist. Alternatively, it could be understood to mean that an incorporeal Deity utilized a created physical hand, a la Green Lantern. The fact that the text does not say "Yado Mamash - His actual hand" is indicative that this reading is more likely. The issue has been debated for the past two decades (see Spring 2015 Dialogue).
Legacy.
Rashi's commentary on the Talmud continues to be a key basis for contemporary rabbinic scholarship and interpretation. Without Rashi's commentary, the Talmud would have remained a closed book. With it, any student who has been introduced to its study by a teacher can continue learning on his own, deciphering its language and meaning with the aid of Rashi.
The Schottenstein Edition interlinear translation of the Talmud bases its English-language commentary primarily on Rashi, and describes his continuing importance as follows: It has been our policy throughout the Schottenstein Edition of the Talmud to give Rashi's interpretation as the primary explanation of the Gemara. Since it is not possible in a work of this nature to do justice to all of the Rishonim, we have chosen to follow the commentary most learned by people, and the one studied first by virtually all Torah scholars. In this we have followed the ways of our teachers and the Torah masters of the last nine hundred years, who have assigned a pride of place to Rashi's commentary and made it a point of departure for all other commentaries.
In 2006, the Jewish National and University Library at Hebrew University put on an exhibit commemorating the 900th anniversary of Rashi's death (2005), showcasing rare items from the library collection written by Rashi, as well as various works by others concerning Rashi.
Supercommentaries.
Voluminous supercommentaries have been published on Rashi's commentaries on the Bible and Talmud, including "Gur Aryeh" by Rabbi Judah Loew (the Maharal), "Sefer ha-Mizrachi" by Rabbi Elijah Mizrachi (the Re'em), and "Yeri'ot Shlomo" by Rabbi Solomon Luria (the Maharshal). Almost all rabbinic literature published since the Middle Ages discusses Rashi, either using his view as supporting evidence or debating against it.
Rashi's explanations of the Chumash were also cited extensively in "Postillae Perpetuae" by Nicholas de Lyra (1292–1340), a French Franciscan. De Lyra's book was one of the primary sources that was used in Luther's translation of the Bible.
"Rashi script".
The semi-cursive typeface in which Rashi's commentaries are printed both in the Talmud and Tanakh is often referred to as "Rashi script." This does not mean that Rashi himself used such a script: the typeface is based on a 15th-century Sephardic semi-cursive hand. What would be called "Rashi script" was employed by early Hebrew typographers such as the and Daniel Bomberg, a Christian printer in Venice, in their editions of commented texts (such as the Mikraot Gedolot and the Talmud, in which Rashi's commentaries prominently figure) to distinguish the rabbinic commentary from the primary text proper, for which a square typeface was used.

</doc>
<doc id="26589" url="http://en.wikipedia.org/wiki?curid=26589" title="Red Hot Chili Peppers">
Red Hot Chili Peppers

The Red Hot Chili Peppers (also sometimes shortened to "The Chili Peppers" or abbreviated as "RHCP") are an American rock band formed in Los Angeles in 1983. The group's musical style primarily consists of rock with an emphasis on funk, as well as elements from other genres such as punk rock and psychedelic rock. When played live, their music incorporates elements of jam band due to the improvised nature of much of their performances. Currently, the band consists of founding members Anthony Kiedis (vocals) and Flea (bass), longtime drummer Chad Smith, and guitarist Josh Klinghoffer, who joined in late 2009, replacing John Frusciante. Red Hot Chili Peppers have won seven Grammy Awards, and have become one of the best-selling bands of all time, selling over 80 million records worldwide. In 2012, they were inducted into the Rock and Roll Hall of Fame.
The band's original line-up featured guitarist Hillel Slovak and drummer Jack Irons, alongside Kiedis and Flea. Because of commitments to other bands, Slovak and Irons did not play on the band's debut album, "The Red Hot Chili Peppers" (1984). Cliff Martinez was the drummer for the first two records (Irons played on the third), and guitarist Jack Sherman played on the first. Slovak performed on the second and third albums by the band, "Freaky Styley" (1985) and "The Uplift Mofo Party Plan" (1987); he died of a heroin overdose in 1988. As a result of the death of his friend, drummer Irons chose to depart from the group. Parliament-Funkadelic guitarist DeWayne McKnight was brought in to replace Slovak though his tenure was short and he was replaced by Frusciante in 1988. Former Dead Kennedys drummer D.H. Peligro was brought in to replace Irons though after a short tenure with the band he was out and replaced by Chad Smith that same year. The line-up of Flea, Kiedis, Frusciante and Smith was the longest-lasting, and recorded five studio albums starting with 1989's "Mother's Milk". In 1990, the group signed with Warner Bros. Records and recorded under producer Rick Rubin the album "Blood Sugar Sex Magik" (1991), which became the band's first commercial success. Frusciante grew uncomfortable with the success of the band and left abruptly in 1992, in the middle of the world tour.
After recruiting guitarist Arik Marshall to complete the tour, Kiedis, Flea, and Smith employed Jesse Tobias though after a few weeks he was replaced by Dave Navarro of Jane's Addiction for their subsequent album, "One Hot Minute" (1995). Although commercially successful, the album failed to match the critical or popular acclaim of "Blood Sugar Sex Magik", selling less than half as much as its predecessor. Navarro was fired from the band in 1998. Frusciante, fresh out of drug rehabilitation, rejoined the band that same year at Flea's request. The reunited quartet returned to the studio to record "Californication" (1999), which became the band's biggest commercial success with 16 million copies worldwide. That album was followed three years later by "By the Way" (2002), and then four years later by the double album "Stadium Arcadium" (2006), their first number one album in America. After a world tour, the group went on an extended hiatus. Frusciante announced he was amicably leaving the band in 2009 to focus on his solo career. Josh Klinghoffer, who had worked both as a sideman for the band on their Stadium Arcadium tour and on Frusciante's solo projects, joined as lead guitarist shortly after Frusciante's departure and the band spent the next year and a half recording their tenth studio album, "I'm with You", which was released in 2011 and topped the charts in 18 different countries and included a world tour which lasted until April 2013. The band quickly followed up that tour a month later with another tour lasting into mid-2014 and that included an appearance with Bruno Mars as a part of the halftime performance at Super Bowl XLVIII, a performance that was viewed by a record 115.3 million viewers. The band started recording their eleventh studio album in December 2014, which is scheduled to be released in 2015.
History.
Early history (1983–1984).
Red Hot Chili Peppers were formed in Los Angeles by Fairfax High School classmates singer Anthony Kiedis, guitarist Hillel Slovak, bassist Flea and drummer Jack Irons. Originally going under the band name of "Tony Flow and the Majestic Masters of Mayhem", their first performance was at the Rhythm Lounge to a crowd of approximately 30 people, opening for Gary and Neighbor's Voices. They "wrote" for the occasion, which involved the band improvising music while Kiedis rapped a poem he had written called "Out in L.A.". At the time, Slovak and Irons were already committed to another group, What Is This? however, the performance was so lively, that the band was asked to return the following week. Due to this unexpected success, the band changed its name to The Red Hot Chili Peppers, playing several more shows at various LA clubs and musical venues. Six songs from these initial shows were on the band's first demo tape.
In November 1983, manager Lindy Goetz struck a seven album deal with EMI America and Enigma Records. Two weeks earlier however, What Is This? had also obtained a record deal with MCA. Slovak and Irons still considered The Red Hot Chili Peppers as only a side project and so in December 1983 they quit to focus on What Is This?. Instead of dissolving the band, Kiedis and Flea recruited new members. Cliff Martinez, a friend of Flea's and from the punk band, The Weirdos, was the new replacement for Irons. The band held auditions for a new guitarist but decided after a few practices that Weirdos guitarist Dix Denney didn't fit. Kiedis described the two final candidates, Mark Nine and Jack Sherman, respectively as a "hip avant-garde art school refugee" and a nerd looking guy with a combed-back Jewfro with an unknown background. Musically Sherman clicked right away with Flea and Martinez and was hired as Slovak's replacement.
The band released their eponymous debut album, "The Red Hot Chili Peppers" on August 10, 1984. Though the album did not set sales records, airplay on college radio and MTV helped to build a fan base, and the album ultimately sold 300,000 copies. Gang of Four guitarist Andy Gill, who produced the album "didn't embrace [the band's] musical aesthetic or ideology", argued constantly with the band over the record's sound. Kiedis recalled, that "Andy's thing was having a hit at all costs, but it was such a mistake to have an agenda." Despite the misgivings of Kiedis and Flea, Gill pushed the band to play with a cleaner, crisper, more radio-friendly sound. The band was disappointed in the record's overall sound, feeling it was overly polished and as if it had "gone through a sterilizing Goody Two-shoes machine". The album included backing vocals by Gwen Dickey, the singer for the successful 70's group, Rose Royce. The band embarked on a grueling tour during which they performed sixty shows in sixty four days. During the tour, continuing musical and lifestyle tension between Kiedis and Sherman complicated the transition between concert and daily band life. When the tour ended in October 1984, Sherman was fired. Hillel Slovak, who had just quit What Is This?, would re-join the band in early 1985.
Building a following and Slovak's death (1985–1988).
George Clinton produced the next album, "Freaky Styley" (1985). Clinton combined various elements of punk and funk into the band's repertoire, allowing their music to incorporate a variety of distinct styles. The band often indulged in heavy heroin use while recording the album, which influenced the lyrics and musical direction of the album. The band had a much better relationship with Clinton than with Gill, but "Freaky Styley", released on August 16, 1985, also achieved little success, failing to make an impression on any chart. The subsequent tour was also considered unproductive by the band. Despite the lack of success, the band was satisfied with "Freaky Styley"; Kiedis reflected, that "it so surpassed anything we thought we could have done that we were thinking we were on the road to enormity." The band appeared in the 1986 movie "Thrashin"' (directed by David Winters and starring Josh Brolin) playing the song "Blackeyed Blonde" from "Freaky Styley". During this time the band also appeared in the movie "Tough Guys" starring Burt Lancaster and Kirk Douglas performing the song "Set It Straight" at a Los Angeles nightclub.
In the spring of 1986, the band decided to begin work on their upcoming album. EMI gave the band a budget of $5,000 to record a demo tape, and the band chose to work with producer Keith Levene, because he shared the band's interest in drugs. Levene and Slovak decided to put aside $2,000 of the budget to spend on heroin and cocaine, which created tension between the band members. Martinez' "heart was no longer in the band", but he did not quit, so Kiedis and Flea fired him. After the firing of Martinez in April 1986, original drummer Jack Irons rejoined the band to Kiedis, Flea, and Slovak's great surprise, which marked the first time all four founding members were together since 1983. During the recording and subsequent tour of "Freaky Styley", Kiedis and Slovak were dealing with debilitating heroin addictions. Due to his addiction, Kiedis "didn't have the same drive or desire to come up with ideas or lyrics" and appeared at rehearsal "literally asleep". He was briefly kicked out of the band after the tour, and given a month to rehabilitate.
The band won the "LA Weekly" "Band of the Year Award," which prompted Kiedis to get clean in order to continue making music. He called his mother in Michigan for guidance, who sent him to drug rehabilitation. After Kiedis completed his stint in rehab, he felt a "whole new wave of enthusiasm" due to his sobriety and wrote the lyrics to "Fight Like a Brave" on the plane ride home. He rejoined Red Hot Chili Peppers in Los Angeles to record the group's next album, "The Uplift Mofo Party Plan" (1987). The Chili Peppers attempted to hire Rick Rubin to produce their third album, but he declined. The band eventually hired Michael Beinhorn, the band's last choice. Kiedis sat down with producer Michael Beinhorn to discuss the recording of the album; Kiedis planned to record the album in ten days and write the songs during the recording sessions. Songs began to form quickly, and the album took shape, blending the same funk feel and rhythms as "Freaky Styley", with a harder, more immediate approach to punk rock.
The album was recorded in the basement of the Capitol Records Building. The recording process for the album was difficult; Kiedis would frequently disappear to seek drugs. After fifty days of sobriety, Kiedis decided to take drugs again to celebrate his new music. His drug use "made a mess of the early recording process", but the band still had an enjoyable time recording the album. The band was musically inspired by the return of their original drummer Jack Irons, who added "such an important and different element to our chemistry." Slovak helped Kiedis record his vocals on the album. In between takes, Slovak would run around the studio out of excitement and say "This is the most beautiful thing we've ever done."
On September 29, 1987, "The Uplift Mofo Party Plan" was released, becoming their first album to appear on any chart. Although it peaked at only No. 148 on the "Billboard" 200, this was a significant success compared to the first two. During this period however, Kiedis and Slovak had both developed serious drug addictions, often abandoning the band, each other, and their significant others for days on end. Slovak's addiction led to his death on June 25, 1988, not long after the conclusion of the "Uplift" tour. Kiedis fled the city and did not attend Slovak's funeral (referenced in the song "This Is the Place"), considering the situation to be surreal and dreamlike. After returning to L.A. following his departure after Slovak's death, Kiedis, Flea, Irons and manager Lindy Goetz had a meeting to figure out what to do next. Irons decided he had to leave the group, saying that he did not want to be part of a group where his friends were dying. Irons, who would battle through years of depression, went on to become a member of Seattle grunge band Pearl Jam many years later. With Slovak dead, Irons quitting, Kiedis and Flea debated whether they should continue making music, but ultimately decided to move ahead, hoping to continue what Slovak "helped build".
Successful new line-up (1988–1989).
After losing two of the original band members, Flea and Kiedis started looking for musicians to fill those spots. Shortly after Irons departure they chose as Slovak's replacement DeWayne "Blackbyrd" McKnight, former member of Parliament-Funkadelic and who at one point briefly filled in for Slovak, when he was temporarily fired. D. H. Peligro of the punk rock outfit Dead Kennedys replaced Irons. Kiedis and Flea had been friends of Peligro for many years and even had a joke band together called Three Little Butt Hairs. With a new lineup set, Kiedis decided to enter rehab to fix his drug problem. Kiedis entered a rehab facility in Van Nuys called ASAP. After two weeks into Kiedis' rehab he was taken by his counselor, Bob Timmons, to finally visit Slovak's grave. Kiedis had no desire to be there, however Timmons urged him to talk to Slovak. Within minutes, Kiedis had opened up and could not stop crying. Thirty days later, Kiedis left rehab and was ready to resume his career with the band. Three dates into the tour, McKnight was fired, because the chemistry wasn't there with the other three McKnight was with the band however long enough to record one song, "Blues For Meister", a song sung by Flea. McKnight was so unhappy about being fired he threatened to burn Kiedis' house down.
Shortly after McKnight's firing, Peligro introduced Kiedis and Flea to a young teenage guitarist named John Frusciante. Kiedis actually had met Frusciante a year earlier outside of one of the band's shows. Frusciante was originally directed to audition for the band Thelonious Monster, however Kiedis said right away he knew, that Frusciante was going to be in his band. An avid Red Hot Chili Peppers fan, Frusciante was, according to Flea, "a really talented and knowledgeable musician. He [Frusciante] knows all the shit I don't know. I basically know nothing about music theory and he's studied it to death, inside and out. He's a very disciplined musician—all he cares about are his guitar and his cigarettes." Frusciante performed his first show with the band on September 17, 1988. The new lineup started right away writing music for the next album and went on a short tour dubbed the "Turd Town Tour" although in November, Kiedis and Flea felt the need to fire drummer Peligro due to his own various drug and alcohol problems. Much like McKnight, Peligro didn't take the news well. It was Flea's turn to do the firing and it was worse than the band could have imagined. Flea stayed in bed for days after making the tough decision. Years later Kiedis said firing Peligro was one of the toughest things the band ever had to do, however Kiedis became a major part of Peligro's road to sobriety, which began right after he was fired.
The Chili Peppers were again without a drummer and were forced to hold open auditions. Denise Zoom, a friend of the band suggested Chad Smith, claiming he was the best drummer she had ever seen, that he ate drums for breakfast. The band agreed to audition Smith, however he was late and the last to audition. Kiedis recalled the first time he saw Smith by saying "I spied this big lummox walking down the street with a really bad Guns N' Roses hairdo and clothes that were not screaming I've got style". Smith was a six-foot three-inch tall drummer who, according to Flea, "lit a fire under our asses". From the moment they started jamming, Smith and Flea instantly clicked. The band knew they had their guy. Smith was a hard-hitting musician the Chili Peppers believed they would create a strong relationship with. Kiedis later said the audition with Smith "left the band in a state of frenzied laughter, that we couldn't shake out of for a half an hour". Smith was so much different from the other three. Kiedis, Flea and Frusciante were heavily influenced by the punk rock, where Smith's taste in heavy metal music and biker appearance went against their punk rock views. Kiedis informed Smith he would be hired on one condition. As an initiation to the band, Smith had to cut his long heavy metal looking haircut. Smith refused though Kiedis wasn't about to argue with the much larger Smith. Smith was hired as the band's fourth drummer on December 3, 1988.
Unlike the stop-start sessions for "The Uplift Mofo Party Plan" (1987), where Kiedis would frequently disappear to seek drugs, pre-production for "Mother's Milk" (1989) went smoothly. The band recorded basic tracks during March and early April 1989 at Hully Gully studios in Silver Lake; songs like "Knock Me Down" were formed from jam sessions without any input from returning producer Michael Beinhorn. Although there had been stress and conflict during the recording of other Chili Peppers albums, the "Mother's Milk" sessions were especially uncomfortable due to Beinhorn's incessant desire to create a hit. Frusciante and Kiedis were frustrated with the producer's attitude. In April 1989, the Chili Peppers embarked on a short tour to break in the new lineup.
Released on August 16, 1989, "Mother's Milk" peaked at number 52 on the U.S. Billboard 200. The record failed to chart in the United Kingdom and Europe, but climbed to number 33 in Australia. "Knock Me Down" reached number six on the U.S. Modern Rock Tracks whereas "Higher Ground" charted at number eleven; the latter of the two ultimately proved to be more successful, however, by influencing foreign charts at number fifty-four in the UK and forty-five in Australia and France. "Mother's Milk" was certified gold by the Recording Industry Association of America in late March 1990—it is now certified platinum—and was the first Chili Peppers album to ship in excess of 500,000 units.
Breakthrough, international fame and Frusciante's first departure (1990–1992).
In 1990, after the success of "Mother's Milk", the group decided they had enough with EMI and entered a major label bidding war ultimately signing with Warner Bros. Records and hired Rick Rubin to produce their then-untitled fifth album. Rubin, who would go on to produce 5 of the band's subsequent studio albums, originally turned the band down in 1987, because of Anthony and Hillel's drug problems, this time felt the band was in a better place and much more focused. The writing process for this album was far more productive than it had been for "Mother's Milk", with Kiedis stating, "[every day], there was new music for me to lyricize".
The band spent six-months recording a new album, with long periods of rehearsal, songwriting, and incubating ideas. However, Rubin was dissatisfied with a regular recording studio, thinking the band would work better in a less orthodox setting, believing it would enhance their creative output. Rubin suggested the mansion magician Harry Houdini once lived in, to which they agreed. A crew was hired to set up a recording studio and other equipment required for production in the house. The band decided that they would remain inside the mansion for the duration of recording, though Smith, convinced the location was haunted, refused to stay. He would, instead, come each day by motorcycle. Frusciante agreed with Smith, and said "There are definitely ghosts in the house", but unlike Smith, Frusciante felt they were "very friendly. We [the band] have nothing but warm vibes and happiness everywhere we go in this house." Rubin is the current owner of the studio known as The Mansion. During production, the band agreed to let Flea's brother-in-law document the creative process on film. When the album's recording was complete, the Chili Peppers released the film, titled "Funky Monks". The band was unable to decide on the title of the album, but to Rubin, one particular song title stuck out: "Blood Sugar Sex Magik". Although it was not a featured song, Rubin believed it to be "clearly the best title".
On September 24, 1991, "Blood Sugar Sex Magik" was released. "Give It Away" was released as the first single; it eventually became one of the band's biggest and most well known songs, winning a Grammy Award in 1992 for "Best Hard Rock Performance With Vocal" and became the band's first number one single on the Modern Rock chart
The ballad "Under the Bridge" was released as a second single, and went on to reach No. 2 on the Billboard Hot 100 chart, the highest the band has reached on that chart as of 2011, and became one of the band's most recognizable songs. Other singles such as "Breaking the Girl" and "Suck My Kiss" also charted well.
The album itself was an international sensation, selling over 15 million copies and greatly broadening the Chili Peppers' audience and becoming one of the most iconic albums of its era. "Blood Sugar Sex Magik" was listed at number 310 on the Rolling Stone magazine list of The 500 Greatest Albums of All Time, and in 1992 it rose to No. 3 on the U.S. album charts, almost a year after its release.
The unexpected success instantly turned Red Hot Chili Peppers into rock stars. Frusciante was blindsided by his newfound fame, and struggled to cope with it. Soon after the album's release, he began to develop a dislike for the band's popularity and personal problems between Kiedis and Frusciante began to unfold. Kiedis recalled that he and Frusciante used to get into heated discussions backstage after concerts: "John would say, 'We're too popular. I don't need to be at this level of success. I would just be proud to be playing this music in clubs like you guys were doing two years ago.'" The final dates with Frusciante were a mess and Frusciante was so disconnected from the group, often changing the way he played on certain songs, which further got under Kiedis' skin. Unknown to others, Frusciante was also starting his own drug habit at the time and was shutting himself off from everyone except for his girlfriend. Frusciante abruptly quit the band hours before a show during the Blood Sugar Japanese tour in May 1992. The band reached out to Dave Navarro, who had just split from Jane's Addiction, but who was involved in his own personal drug battles. The group held rehearsals with Zander Schloss, though after a few days they decided he wasn't the right fit either. Guitarist Arik Marshall, of Los Angeles band Marshall Law, was hired to replace Frusciante and the band headlined the Lollapalooza festival in 1992. Marshall would also appear in the music videos for "Breaking the Girl", "If You Have to Ask" and on "The Simpsons" fourth season finale, "Krusty Gets Kancelled".
In September 1992, the Peppers, with Marshall, performed "Give It Away" at the MTV Video Music Awards. The band was nominated for seven awards including Video of the Year (which they did not obtain), however they did manage to win three other awards, including Viewer's Choice. On February 24, 1993, the band, along with George Clinton & the P.Funk All-Stars and Weapon of Choice, performed "Give It Away" at the Grammy Awards, a song which won the band their first Grammy later that evening. The performance marked the end of the "Blood Sugar Sex Magik" tour and Marshall's final performance with the band. The band had planned to begin a follow-up to "Blood Sugar Sex Magik" with Marshall. However, when it came time to play music, Marshall was always busy, so the band decided that Marshall failed to fit with their future plans and he was dismissed.
With Marshall gone, the band decided to hold open auditions (which they considered a huge mess), but which however did lead to an encounter with Buckethead. The band enjoyed his rehearsal even though he claimed to have never heard of them; Flea felt he didn't fit the feel of the band. Still without a guitarist, Kiedis was out one night at a local club and spotted Jesse Tobias of the Los Angeles -based band Mother Tongue. Kiedis felt like he had the right vibe for the band and he was recruited to be the new guitarist after a few auditions. However, his tenure with the band did not last long, with the rest of the band stating that "the chemistry wasn't right". It was at this same time that Smith informed the band that Navarro was now ready to join the band. When offered the spot this time he accepted.
Transitional period (1993–1997).
Navarro first appeared with the band at Woodstock '94. The band opened their set wearing enormous light bulb costumes attached precariously to chrome metallic suits, making it near-impossible for them to play their instruments. Navarro hated the idea but went with it. The performance saw the debut of new songs such as "Warped", "Aeroplane", and "Pea" although the songs were in the beginning stages and the lyrics were very different from the final versions. The band followed up their performance at Woodstock with a brief tour, which included headlining appearances at the Pukkelpop and Reading Festivals as well two performances as the opening act for The Rolling Stones. According to Kiedis, however, opening for the Stones was a horrible experience. While externally, the band appeared to be settled, the relationship between the three established members and Navarro had begun to deteriorate. His differing musical background made performing difficult as they began playing together, and continued to be an issue over the next year. Navarro admitted he didn't care for funk music or jamming. Kiedis was also struggling with his heroin addiction; he had been through a dental procedure in Beverly Hills, in which an addictive sedative, Valium, was used; this caused him to relapse, and he once again became dependent on drugs, although the band wouldn't find out this fact for a while.
Navarro's joining and Kiedis's continued drug addiction had a profound effect on the band and the subsequent sound of their next album, "One Hot Minute" (1995). With Frusciante no longer present for collaboration, songs were written at a far slower rate. Working with Frusciante had been something Kiedis took for granted: "John had been a true anomaly when it came to song writing. He made it even easier than Hillel to create music, even though I'd known Hillel for years. I just figured that was how all guitar players were, that you showed them your lyrics and sang a little bit and the next thing you knew you had a song. That didn't happen right off the bat with Dave." To compensate, Kiedis and bassist Flea took several vacations together, during which entire songs were conceived and with Kiedis often absent from recording due to his drug problems or struggling to come up with lyrics, Flea took a much bigger role in the writing process; coming up with ideas for many songs including full lyrics and even singing lead on his own song, "Pea".
Navarro's only album with the band was "One Hot Minute", released on September 12, 1995 after many delays and setbacks. Navarro's guitar work had created a stylistic departure in the band's sound, which was now characterized by prominent use of heavy metal guitar riffs and hints of psychedelic rock. The band described the album as a darker, sadder record compared to their previous material, which was not as universally well-received as "Blood Sugar Sex Magik". Many of the lyrics written by Kiedis were drug-related, including the lead single, "Warped," which left Kiedis stunned that nobody else in the band picked up on his lyrics that he was using again. Broken relationships and deaths of friends and family also played a major role in the album's darker tone and lyrics. The ballad, "Tearjerker," was written about Kurt Cobain, while "Transcending", which was written by Flea, was about longtime friend, River Phoenix; and the single "Shallow Be Thy Game" took shots at religion. Despite mixed reviews, the album was a commercial success. Selling eight million copies worldwide, it spawned the band's third No.1 single, the ballad "My Friends", and enjoyed chart success with the songs "Warped" and "Aeroplane". This iteration of the band appeared on several soundtracks. "I Found Out", a John Lennon cover, was featured on "". The Ohio Players cover, "Love Rollercoaster", was featured on the "Beavis and Butthead Do America soundtrack", and was released as a single.
The band began its tour for "One Hot Minute" in Europe on September 27, 1995 and played 16 shows. A US tour was to follow but was postponed after Chad Smith broke his wrist. The band considered carrying on with the tour even at one point considering Jack Irons as a replacement for Smith but there just wasn't enough time to rehearse with anyone new and the dates were rescheduled for early 1996. The band spent most of 1996 playing shows in the United States and Europe. By 1997, for the first time, the band cancelled many shows. Most of this was again due to problems within the band. Flea at that point was exhausted, tired of playing the same songs each night and was seriously talking about quitting the band while Kiedis had recently been involved in a motorcycle accident which left one arm in a sling and created yet another drug relapse due to his use of painkillers. Even Navarro was back to using drugs. 1997 saw the band playing just one show. This was at the very first Fuji Rock Festival on July 26, 1997. A massive typhoon hit that day but the band played anyway. They played through an 8 song set before having to cut the show short due to the storm. This would be the final show with Navarro and due to Flea's previous comments left many speculating if it was the end of the band.
After making attempts to carry on with Navarro and record a follow-up to "One Hot Minute" things were not working out and due to Navarro's drug problems and lack of effort in wanting to create new music and chemistry on the road the band felt it was time to part ways. In April 1998 it was announced that Navarro had left the band due to creative differences; Kiedis stated that the decision was "mutual". Reports at the time, however, indicated Navarro's departure came after he attended a band practice under the influence of drugs, which at one point involved him falling backwards over his own amp.
Return of Frusciante and new-found popularity (1998–2001).
In the years following his departure from the band, it became public that John Frusciante had developed a heroin addiction, which left him in poverty and near death. Frusciante had lost contact with most of his friends. However, Flea always remained in contact, and he helped talk Frusciante into admitting himself to Las Encinas Drug Rehabilitation Center in January 1998. He concluded the process in February of that year and began renting a small apartment in Silver Lake. He acquired many injuries and problems in the years of his addiction, some requiring surgery, including permanent scarring on his arms, a restructured nose, and new teeth to prevent fatal infection.
After Navarro's departure in early 1998, Red Hot Chili Peppers were on the verge of breaking up. Flea told Kiedis, "the only way I could imagine carrying on [with Red Hot Chili Peppers] is if we got John back in the band." Kiedis was surprised and thought there was no way Frusciante would ever want to work with him as the two still had unresolved personal problems from when Frusciante quit in 1992. With Frusciante free of his addictions and ailments, Kiedis and Flea thought it was an appropriate time to invite him back. In April 1998, when Flea visited him at his home and asked him to rejoin the band, Frusciante began sobbing and said "nothing would make me happier in the world." Flea decided to contact Anthony and have him meet with John to try and resolve any personal problems that the two might have had. Flea was relieved to find out that both had no bad blood towards each other and were once again excited to make music together. Within the week and, for the first time in six years, the reunited foursome jump-started the newly reunited Red Hot Chili Peppers. Anthony Kiedis said of the situation:
Despite the band's elation, Frusciante was both mentally and physically torn. Frusciante had not played with the band since his departure and other than his solo albums had not picked up a guitar in years. He had lost his guitars in a house fire from which he barely escaped, and experienced a difficult time resuming his prior life. The group began jamming in Flea's garage and it didn't take long for Frusciante to regain his talent, however, and new songs began to roll out. Frusciante's return restored a key component of the Chili Peppers' sound, as well as a healthy morale. He brought with him his deep devotion to music, which had an impact on the band's recording style during the sessions which produced their next album. Frusciante has frequently stated that his work on "Californication" was his favorite. On June 8, 1999, after more than a year of production and meticulous practice, "Californication" was released as the band's seventh studio album. The album ultimately sold over 16 million copies and became the band's most successful recording to date. "Californication" contained fewer rap-driven songs than its predecessors, instead integrating textured, consistent, and melodic guitar riffs, vocals and bass-lines. The record produced three more number one modern rock hits, "Scar Tissue", "Otherside" and "Californication". "Californication" gained positive critical acceptance in contrast to its less popular predecessor, "One Hot Minute", and was a greater success worldwide. While many critics credited the success of the album to Frusciante's return, they also noted that Kiedis' vocals had also greatly improved. It was later listed at number 399 on the Rolling Stone magazine list of The 500 Greatest Albums of All Time.
In July 1999, as part of the band's two-year-long international world tour in support of their new album, Red Hot Chili Peppers played at Woodstock 1999, which became infamous for the violence that resulted. Some 10 minutes before the show, they were asked by Jimi Hendrix's stepsister to play a cover of her brother's songs. After some hesitation due to not having performed the song in years, the band decided to play his classic "Fire", which they had covered on "Mother's Milk". Coincidentally, about two thirds of the way into the band's set, the closing set of the three-day concert, a small fire escalated into full-fledged vandalism and resulted in the intervention of riot control squads. The disruption escalated into violence when nearby property including ATMs and several semi-tractor trailers were looted and destroyed. Kiedis felt that "It was clear that this situation had nothing to do with Woodstock anymore. It wasn't symbolic of peace and love, but of greed and cashing in ... We woke up to papers and radio stations vilifying us for playing 'Fire'." The tour also originated the band's first concert DVD, 2001's "Off the Map".
Continued success (2001–2007).
The writing and formation of the band's next album, "By the Way" began immediately following the culmination of "Californication's" world tour, in the Spring of 2001. As with "Californication", much of the creation took place in the band members' homes, and other locations of practice, such as a recording studio stage. Kiedis recalled of the situation: "We started finding some magic and some music and some riffs and some rhythms and some jams and some grooves, and we added to it and subtracted from it and pushed it around and put melodies to it." Frusciante and Kiedis would collaborate for days straight, discussing and sharing guitar progressions and lyrics. For Kiedis, "writing "By the Way" ... was a whole different experience from "Californication". John was back to himself and brimming with confidence."
Before recording "By the Way" (2002), the Chili Peppers decided that they would again have Rick Rubin produce the album. In the past, Rubin had given the band creative freedom on their recording material; this was something they thought essential for the album to be unique, and could only occur with his return. Originally the album was headed in a much different direction than the final production. The album started out as a group of fast, hardcore punk songs, which Rubin rejected. Frusciante also wanted a darker, 1980s UK pop/new wave sound mixed in with 1980s hardcore. The recording process was tough for Flea, who felt like an outsider in the band and that his role was being diminished due to a musical power struggle with Frusciante. Flea wanted to create more funk-inspired songs, while Frusciante felt that the band had overused their funk side. Flea considered quitting the band after the album, but the two eventually worked out all their problems.
"By the Way" was released on July 9, 2002 and produced four singles; "By the Way", "The Zephyr Song", "Can't Stop" and "Universally Speaking". The album was their most subdued album to date, focusing primarily on melodic ballads as opposed to their classic rap-driven funk. Frusciante also concentrated on a more layered texture on many of the songs, often adding keyboard parts, that featured low in the mix, and also writing string arrangements for songs such as "Midnight" and "Minor Thing". The album was followed by an eighteen month-long world tour. The European leg of the "By the Way" tour produced the band's second full-length concert DVD, "Live at Slane Castle", recorded at Slane Castle in Ireland on August 23, 2003. The band released their first full-length live album, "Live in Hyde Park"; recorded during their performances in Hyde Park, London. More than 258,000 fans paid over $17,100,000 for tickets over three nights, a 2004 record; the event ranked No.1 on "Billboard"'s Top Concert Boxscores of 2004.
In November 2003, the Chili Peppers released their "Greatest Hits" album, which featured two new songs, "Fortune Faded" and "Save the Population". The two songs were selected out of sessions that generated fifteen tracks and Smith later said the band had hopes to use along with new compositions to create a full album after finishing the tour, but the idea was vetoed by Frusciante because his musical influences and styles had evolved and he wanted to do something new.
In 2006 the band released the Grammy Award–winning "Stadium Arcadium" produced by Rick Rubin. Although 38 songs were created with the intention of being released as three separate albums spaced six months apart, the band instead chose to release a 28-track double album, and released nine of the ten as B–sides. It was their first album to debut at No. 1 on the US charts, where it stayed for two weeks, and debuted at number one in the UK and 25 other countries. "Stadium Arcadium" sold over seven million units.
The record's first single, "Dani California", was the band's fastest-selling single, debuting on top of the Modern Rock chart in the U.S., peaking at No. 6 on the "Billboard" Hot 100, and reaching No. 2 in the UK. "Tell Me Baby", released next, also topped the charts in 2006. "Snow ((Hey Oh))" was released in late 2006, breaking multiple records by 2007. The song became their eleventh number one single, giving the band a cumulative total of 81 weeks at number one. It was also the first time three consecutive singles by the band made it to number one. "Desecration Smile" was released internationally in February 2007 and reached number 27 on the UK charts. "Hump de Bump" was planned to be the next single for the US, Canada, and Australia only, but due to positive feedback from the music video, it was released as a worldwide single in May 2007.
The band began another world tour in support of "Stadium Arcadium" in 2006, beginning with promotional concerts in Europe and culminating in a two-month-long European tour from late May to mid-July. During this tour Frusciante's friend and frequent musical collaborator Josh Klinghoffer joined the touring band, contributing guitar parts, back up vocals, and keyboards. Klinghoffer's presence allowed the live performances of songs to sound more like the recorded versions, in which Frusciante laid down multiple tracks himself. The group toured North America from early August to early November, returning to Europe later in November for a second leg, that ran until mid–December. The Chili Peppers began 2007 with a second North American leg, this time including Mexico, from mid–January to mid–March. This was followed by April shows in various cities in Australia and New Zealand and concerts in Japan in early June. The Peppers concluded their tour with a third European leg from late June to late August. They appeared at the Live Earth concert at London's Wembley Stadium on July 7, 2007. The band appeared at several festivals, including Denmarks Roskilde festival, Ireland's Oxegen in July 2006, Lollapalooza in August 2006 in Grant Park, Chicago, a subsequent set at the Coachella Valley Music and Arts Festival in Indio, California in late April 2007 and in August 2007 they appeared as one of three headliners at the Reading and Leeds festivals along with Razorlight and Smashing Pumpkins.
In February 2007, "Stadium Arcadium" won five Grammys: Best Rock Album, Best Rock Song ("Dani California"), Best Rock Performance by a Duo Or Group With Vocal ("Dani California"), Best Boxed Or Special Limited Edition Package, and Best Producer (Rick Rubin). Rolling Stone's '100 Best Albums of the Decade (2000–2009)' included "Stadium Arcadium" at # 74.
Hiatus and Frusciante's second departure (2008–2009).
Following the last leg of the Stadium Arcadium tour, the band members took an extended break. Kiedis attributed this to the band being worn out from their years of nonstop work since "Californication" (1999). The band's only recording during this time was in 2008 with George Clinton on his latest album "George Clinton and His Gangsters of Love". Accompanied by Kim Manning, the band recorded a new version of Shirley and Lee's classic "Let the Good Times Roll". The song would become the last song the band would record with Frusciante.
Kiedis, who had recently become a father, was looking forward to the time off and taking care of his son Everly Bear and possibly creating a short television series called "Spider and Son", which was set to recap his autobiography. Flea began taking music theory classes at the University of Southern California, and revealed plans to release a mainly instrumental solo record, that was being recorded in his home; guest musicians include Patti Smith and a choir from the Silverlake Conservatory. Flea also joined Thom Yorke of Radiohead in the supergroup Atoms for Peace. Frusciante continued his solo career and released his solo album, "The Empyrean". Chad Smith worked with Sammy Hagar, Joe Satriani, and Michael Anthony in the supergroup Chickenfoot, as well as on his solo project, Chad Smith's Bombastic Meatbats. The band planned to remain on hiatus for "a minimum of one year".
In October 2009 the band officially ended their hiatus and minus Frusciante, entered the studio to begin writing for their tenth studio album. The band was joined by Josh Klinghoffer, who to the public was still the band's backup touring guitarist, although it was later confirmed, that he was already an official member and Frusciante's replacement with Frusciante having quit the band on July 29, 2009. An official announcement on Frusciante's departure wasn't made until December 2009. Frusciante explained on his MySpace page, that there was no drama or anger about him leaving the band this time, and that the other members were very supportive and understanding. Frusciante said he felt his musical interests had led him in a different direction, and that he needed to fully focus his efforts on his solo career.
Klinghoffer replaces Frusciante and "I'm with You" (2010–2013).
The band, with Josh Klinghoffer on guitar, made their live comeback on January 29, 2010, paying tribute to Neil Young with a cover of "A Man Needs a Maid" at MusiCares. After months of speculation, in February 2010 Klinghoffer was officially confirmed by Chad Smith as Frusciante's full-time replacement.
The band officially began recording their tenth studio album with producer Rick Rubin in September 2010. According to Rubin, the band recorded enough material to release a second double album, following "Stadium Arcadium" but ultimately decided not to. Rubin notes, "it was painful not to share all of the material that we had, but we felt it would be too much. We really wanted it to be twelve songs but it ended up being fourteen just because nobody could agree on which twelve." The recording process lasted until March 2011. Many of the songs were written between October 2009 and August 2010 and according to Flea around 60–70 songs were written in the ten months prior to entering the studio to record the album.
In July 2011, the band kicked off a trio of invitation-only warm-up dates in California. These were the first shows the band played since August 2007 and their first official shows with Josh as their lead guitarist.
"I'm with You", the band's tenth studio album was released in the United States in August 2011. The album topped the charts in 18 different countries although failed to provide the band with their second straight number one debut in the U.S. The album was met with mostly positive reviews from the critics. The album's first single, "The Adventures of Rain Dance Maggie" was released a month earlier and went on to become the band's twelfth number one hit single, topping their own record. Kreayshawn was tapped to direct the music video for the single, however due to unknown reasons, the video shot by Kreayshawn went unreleased and a second video directed by Marc Klasfeld was released in its place. "Monarchy of Roses", "Look Around" and "Did I Let You Know", released only in Brazil, followed as singles/music videos. "Brendan's Death Song" would be the next single and released during the summer of 2012.
The band kicked off a month long promotional tour in August 2011 starting in Asia. On August 30, 2011, the band appeared on movie screens throughout the world via satellite from Cologne, Germany performing the entire new album in sequence, minus "Even You Brutus" and adding "Give It Away" and "Me and My Friends". The band officially kicked off the I'm with You Tour on September 11, 2011, the next day, on September 12, they played in Costa Rica. The tour is expected to last into 2013 and be one of the band's biggest to date. All shows from the upcoming world tour will be made available to purchase as downloads through LiveChiliPeppers.com. The North American leg of the tour, which was expected to begin on January 19, 2012, had to be postponed due to a surgery Kiedis went to resolve multiple foot injuries he had suffered through since the Stadium Arcadium tour. The first U.S. leg of the tour, including dates in Canada, kicked off in March 2012 and lasted into June, followed by summer shows in Europe, while the rest of the already scheduled U.S. dates took place in August and then from September through November 2012. Jack Irons and Cliff Martinez again joined the band during their August 12, 2012 performance of "Give it Away" in Los Angeles. Following the "I'm with You World Tour", the band set out on another small tour consisting mostly of festivals in the United States however the tour expanded to dates in South America as well for November. Flea along with Chili Peppers touring percussionist, Mauro Refosco spent the band's break keeping busy with their side-project Atoms For Peace, who had many dates throughout the world scheduled from July to November 2013. On May 11, 2013, the band performed a special concert in Portland, Oregon for the Dalai Lama as part of the Dalai Lama Environmental Summit. According to the press release "The musical element of this event is intended to be a display of joyful celebration and an inspiration to future generations to care for our planet. The Red Hot Chili Peppers have been great supporters of the Tibetan cause, of His Holiness the Dalai Lama, and of the need to work to protect and preserve our environment."
The band was nominated for two MTV Europe Music Awards for "Best Rock Band" and "Best Live Artist" and nominated for "Best Group" at the 2012 People's Choice Awards "I'm with You" was also nominated for a 2012 Grammy Award for Best Rock Album.
The band released "2011 Live EP" on March 29, 2012. The EP a free five live song MP3 download through their website. The five songs were selected by Chad Smith from the band's 2011 European live albums, which were released for purchase through their website as well. On April 14, 2012, the band was inducted into the Rock and Roll Hall of Fame. On the date following their Hall of Fame induction, the band performed a free concert in downtown Cleveland, Ohio in support of President Barack Obama's re-election campaign. The requirement for getting into the concert was agreeing to volunteer for the Obama 2012 phone bank. The event quickly met its capacity limit after being announced. May 1, 2012 saw the release of digital download only "Rock & Roll Hall of Fame Covers EP" which consisted of previously released studio and live covers of artists, that influenced the band. In addition to their newly released live performances, starting in August 2012, the band started to put out a collection of singles from the "I'm With You Sessions". The singles, which contained two songs each and total 17 songs were made available on 7" vinyl, digital download and CD. All of the singles were eventually released together as "I'm Beside You" LP on November 29, 2013 as a Record Store Day exclusive.
The band wrapped up the "I'm with You Tour" in April 2013. The tour ranked 15th on Billboard 's "Top 25 Tours" list of 2012. Following the end of that tour, the band headed right back out on the road the next month for another lengthy tour which included their first ever shows in Alaska, Paraguay, the Philippines and Puerto Rico.
Eleventh studio album (2014–present).
It was announced on January 10, 2014 that the Chili Peppers would be joining Bruno Mars as performers at the Super Bowl XLVIII halftime show on February 2, 2014. The Super Bowl halftime show was the most watched in the history of the Super Bowl. A record 115.3 million viewers tuned in which passed the record 114 million who watched Madonna perform two years earlier. The band's performance was met with mixed reviews and heavy criticism from fans, the media, and even musicians towards Flea and Klinghoffer for not plugging in their instruments and instead playing to pre-recorded music. Flea gave a lengthy response through the band's website stating it was a NFL rule for bands to pre-record music due to time and technical issues and that the band heavily weighed their options before committing to the performance ultimately deciding to perform because it would be fun and a once in a lifetime opportunity. He said Kiedis' vocals were completely live and the band pre-recorded "Give it Away" during rehearsals. Bruno Mars' backing band also performed to pre-recorded music but largely escaped the negative reaction the Chili Peppers faced. Smith also confirmed that over the past ten years, all musicians who have performed the halftime show have used pre-recorded music.
The band wrapped up their latest tour, which began in May 2013. In total, the band performed a total of 158 shows from September 2011 to June 2014 of the two separate tours making it the longest span of touring in their history without a real break. "2012-13 Live EP" was released on July 1, 2014 through their website as a free download. Like the "2011 Live EP", five songs were selected by Chad Smith from the band's tour as a way to announce the official conclusion of the tour.
On November 17, 2014, Kiedis gave an interview with KROQ where he announced that the band would be returning to the studio in December to record their newly written album. Kiedis said that he hoped for the album to contain 13 songs, however it is likely they will "put 10 more songs on top of that". Kiedis felt the album will show Josh Klinghoffer's coming of age as their guitarist and that the new material will have him as a guitarist and songwriter stand out a lot.
On January 10, 2015, the band performed their first show of the new year and first since their tour ended the previous June when they were invited by actor Sean Penn to perform a surprise 30 minute, six song set to close out his 4th annual fundraiser, "Sean Penn & Friends Help Haiti Home" which is in support of the J/P Haitian Relief Organization which was founded by Penn in 2010. The nearly 4 hour long event held in Beverly Hills, CA featured many celebrities including former United States president Bill Clinton and raised over $6 million.
On January 31, 2015, Flea posted on his Twitter page saying "Bout to start recording. Danger Mouse producing" which confirmed he would be replacing Rubin as the next producer. The tweet was removed for unknown reasons shortly after being posted. On February 15, 2015, Flea again posted an album update on his Twitter page saying "We are still writing and demoing. Going in to cut for real in a few weeks!" A day later, Flea suffered a broken arm during a skiing trip.
On March 17, 2015, the band released "", a full show for free MP3 download through their website. The show is most notable for featuring the first ever official release of "Mini-Epic (Kill For Your Country)". The studio version of the song has never been released.
In an April 2015, Kiedis' mother, Peggy Noble Idema, mentioned during an interview that the band was going to start recording the next album in July 2015.
Kiedis received the George and Ira Gershwin Award for Lifetime Musical Achievement, presented at the UCLA Spring Sing on May 16, 2015. Following his acceptance speech, he was joined on stage Josh Klinghoffer for an acoustic performance of "Otherside", "If You Want Me to Stay" (a song the Chili Peppers haven't performed in almost 30 years), and "By the Way".
Recognition and legacy.
In May 2009, Kiedis was honored with the Stevie Ray Vaughan Award at the fifth annual MusiCares event for his dedication and support of the MusiCares MAP Fund and for his commitment to helping other addicts with addiction and recovery process. Kiedis' fellow band members, minus Frusciante were on hand to pay tribute and under the name, 'The Insects', Kiedis, Smith, Flea along with Ron Wood, Josh Klinghoffer and Ivan Neville performed a brief set of cover songs.
The band was inducted into the Rock and Roll Hall of Fame on April 14, 2012 by actor/comedian Chris Rock, who is a longtime friend and fan of the band. The induction line-up was Kiedis, Flea, Smith, Klinghoffer, Frusciante, Slovak (who was represented by his brother James), Irons and Martinez. Frusciante did not attend the ceremony. Smith said "We asked him, he said, 'I'm just not really comfortable with that, but good luck and thanks for inviting me.' He's the kind of guy, I think, that once he's finished with something he's just on to the next phase of his life. The Chili Peppers are not really on his radar right now." Despite recording one album with the band, Dave Navarro and Jack Sherman were the only two former recording band members not inducted. Kiedis explained that Navarro, who appeared to be fine with not being included, was better suited for induction as a member of Jane's Addiction, the band he was best known for. Sherman on the other hand was angry about not being included and spoke out about the band's induction blaming the band for not including him or Navarro by saying he was told only original members, current members and those who played on multiple albums were eligible for induction (although this has proven to not be the case with other inductees). "It appeared to be a politically correct way of omitting Dave Navarro and I for whatever reasons they have that are probably the band's and not the Hall's," Sherman also said "It's really painful to see all this celebrating going on and be excluded. I'm not claiming that I've brought anything other to the band ... but to have soldiered on under arduous conditions to try to make the thing work, and I think that's what you do in a job, looking back. And that's been dishonored. I'm being dishonored, and it sucks." During his induction speech, Smith made sure to thank all of the band members, past and present including Sherman, Navarro along D.H. Peligro (whom he accidentally called H.R. Peligro) and Arik Marshall for the contributions they brought to the band. At 32, Klinghoffer was the youngest artist ever to be inducted into the hall of fame passing Stevie Wonder, who was 38 at the time of his induction. The band performed three songs, including "By the Way", "Give it Away" and "Higher Ground", which included Irons and Martinez on drums. It was the first time Kiedis and Flea had performed with Irons in 24 years and Martinez in 26 years. To end the ceremony, the band was joined by George Clinton, Michael Hampton, Slash, Billie Joe Armstrong, Tre Cool, Ronnie Wood and Kenney Jones for a performance of "Higher Ground".
In 2012, three of the band's albums "Blood Sugar Sex Magik", "Californication", and "By the Way" are ranked among Rolling Stone's 500 Greatest Albums of All Time ranked at 310, 399, and 304 respectively.
The band released their book, "Fandemonium" on November 18, 2014. The book is dedicated to the band's fans throughout the world. To promote the book, Kiedis, Flea and Smith did some in store book signings at Barnes & Noble in Los Angeles and New York. On November 20, Kiedis appeared on "The Tonight Show with Jimmy Fallon" sitting in the entire show with The Roots and performing some Chili Peppers songs while also promoting the book.
Musical style.
Techniques.
Red Hot Chili Peppers' musical style is characterized as funk rock, alternative rock and funk metal, with influences from hard rock, psychedelic rock and punk rock. The band's influences include Defunkt, Parliament-Funkadelic, Jimi Hendrix, The Misfits, James Brown, Gang of Four, Bob Marley, Big Boys, Bad Brains, Sly and the Family Stone, Ohio Players, Queen, Stevie Wonder, Elvis Presley, Deep Purple, The Beach Boys, Black Flag, Ornette Coleman, Led Zeppelin, Yes, Fugazi, Fishbone, Marvin Gaye, Billie Holiday, Santana, Elvis Costello, The Stooges, The Clash, Siouxsie and the Banshees, Devo, and Miles Davis.
Vocalist Anthony Kiedis provided multiple vocal styles. His primary approach up to "Blood Sugar Sex Magik" was spoken verse and "rapping". Complemented with traditional vocals, he helped the band maintain a consistent style. Nevertheless, as the group matured, starting with "Californication" (1999) the group reduced the number of rapped verses. "By the Way" contained only two rap-driven-verse/melodic chorus form. Kiedis' more recent style was developed through ongoing coaching.
Original guitarist Hillel Slovak's style was strongly based on blues and funk. Slovak was primarily influenced by hard rock artists such as Jimi Hendrix, Kiss and Led Zeppelin. His playing method was highly based on improvisation, a style commonly used in funk music. He also was noted for his aggressive playing style; he would often play with such force, that his fingers would "come apart". Kiedis observed that his playing evolved during his time away from the group in What Is This?, with Slovak adopting a more fluid style featuring "sultry" elements as opposed to his original hard rock techniques. On "The Uplift Mofo Party Plan" (1987), Slovak experimented with genres outside of traditional funk music including reggae and speed metal. His guitar riffs would often serve as the basis of the group's songs, with the other members writing their parts to complement his guitar work. His melodic riff featured in the song "Behind the Sun" inspired the group to create "pretty" songs with an emphasis on melody. Kiedis describes the song as "pure Hillel inspiration". Slovak also used a talk box on songs such as "Green Heaven" and "Funky Crime", in which he would sing into a tube while playing to create psychedelic effects.
Guitarist John Frusciante's musical style has evolved over the course of his career. His guitar playing employs melody and emotion rather than virtuosity. Although virtuoso influences can be heard throughout his career, he has said that he often minimizes this. Frusciante brought a more melodic and textured sound to albums such as "Californication" (1999), "By the Way" (2002) and "Stadium Arcadium" (2006). This contrasts with his previous abrasive approach in "Mother's Milk", as well as his dry, funky and more docile arrangements on "Blood Sugar Sex Magik". On "Californication" (1999) and "By the Way" (2002), Frusciante derived the technique of creating tonal texture through chord patterns from post-punk guitarist Vini Reilly of The Durutti Column, and bands such as Fugazi and The Cure. He originally intended "By the Way" to be made up of "these punky, rough songs", drawing inspiration from early punk artists such as The Germs and The Damned. However, this was discouraged by producer Rick Rubin, and he instead built upon "Californication"'s (1999) melodically driven style. During the recording of "Stadium Arcadium" (2006), he moved away from his New Wave influences and concentrated on emulating flashier guitar players such as Hendrix and Van Halen.
Guitarist Dave Navarro brought an entirely different sound to the band during his tenure, with his style based on heavy metal, progressive rock and psychedelia.
Current guitarist Josh Klinghoffer's style employs a wide range of unconventional guitar effects and vocal treatments. In his debut Chili Peppers album, "I'm With You" (2011), he focused heavily on producing a textured, emotional sound to complement the vocals and atmosphere of each song. He has stated that he is a huge fan of jazz and funk, which shows itself in many of the album's tracks.
Flea's electric bass style is an amalgamation of funk, psychedelic, punk, and hard rock. The groove-heavy, low-tuned melodies, played through either finger–picking, or slapping, contributed to their signature style. While Flea's slap bass style was prominent in earlier albums, albums after "Blood Sugar Sex Magik" have more melodic and funk–driven bass lines. He has also used double stops on some newer songs. Flea's bass playing has changed considerably throughout the years. When he joined Fear, his technique centered largely around traditional punk rock bass lines, however he was to change this style, when Red Hot Chili Peppers formed. He began to incorporate a "slap" bass style, that drew influence largely from Bootsy Collins. "Blood Sugar Sex Magik" saw a notable shift in style as it featured none of his signature technique but rather styles, that focused more on traditional and melodic roots. His intellectual beliefs, on how to play the instrument, were also altered: "I was trying to play simply on "Blood Sugar Sex Magik" because I had been playing too much prior to that, so I thought, 'I've really got to chill out and play half as many notes'. When you play less, it's more exciting—there's more room for everything. If I do play something busy, it stands out, instead of the bass being a constant onslaught of notes. Space is good."
Drummer Chad Smith blends rock with funk. He mixes funk, rock, metal and jazz to his beats. Influences include Buddy Rich to John Bonham. He brought a different sound to "Mother's Milk", playing tight and fast. In "Blood Sugar Sex Magik", he displays greater power. He is recognized for his ghost notes, his beats and his fast right foot. MusicRadar put him in sixth place on their list of the "50 Greatest Drummers Of All Time", behind Mike Portnoy, Neil Peart, Keith Moon, Rich and Bonham.
Lyrics and songwriting.
Through the years, Kiedis' lyrics covered a variety of topics, which shifted as time progressed. Early in the group's career, Kiedis wrote mostly comical songs filled with sexual innuendos as well as songs inspired by friendship and the band members' personal experiences. However, after the death of his close friend and band mate Hillel Slovak, Kiedis' lyrics became much more introspective and personal, as exemplified by the "Mother's Milk" (1989) song "Knock Me Down", which was dedicated to Slovak along with the "Blood Sugar Sex Magik" (1991) song, "My Lovely Man".
When the band recorded "One Hot Minute" (1995) Kiedis had turned to drugs once again, which resulted in darker lyrics. He began to write about anguish, and the self mutilating thoughts he would experience as a result of his heroin and cocaine addiction. The album also featured tributes to close friends the band lost during the recording process including Kurt Cobain on the song "Tearjerker" and River Phoenix, on the song "Transcending".
After witnessing Frusciante's recovery from his heroin addiction, Kiedis wrote many songs inspired by rebirth and the meaning of life on "Californication" (1999). He was also intrigued by the life lessons, that the band had learned, including Kiedis' experience with meeting a young mother at the YMCA, who was attempting to battle her crack addiction while living with her infant daughter.
On "By the Way" (2002), Kiedis was lyrically influenced by love, his girlfriend, and the emotions expressed, when one fell in love. Drugs also played an integral part in Kiedis' writings, as he had only been sober since December 2000. Tracks like "This Is the Place" and "Don't Forget Me" expressed his intense dislike for narcotics and the harmful physical and emotional effects they caused him. "Stadium Arcadium" (2006) continued the themes of love and romance; Kiedis stated, that "love and women, pregnancies and marriages, relationship struggles – those are real and profound influences on this record. And it's great, because it wasn't just me writing about the fact that I'm in love. It was everybody in the band. We were brimming with energy based on falling in love." "I'm With You" (2011) again featured Kiedis writing about the loss of a close friend this time in the song "Brendan's Death Song", a tribute to club owner Brendan Mullen who gave the band some of their earliest shows and showed support to them throughout their career.
Themes within Kiedis' repertoire include love and friendship, teenage angst, good-time aggression, various sexual topics and the link between sex and music, political and social commentary (Native American issues in particular), romance, loneliness, globalization and the cons of fame and Hollywood, poverty, drugs, alcohol, dealing with death, and California.
References.
</dl>

</doc>
<doc id="26691" url="http://en.wikipedia.org/wiki?curid=26691" title="Set (mathematics)">
Set (mathematics)

In mathematics, a set is a collection of distinct or well defined objects, considered as an object in its own right. For example, the numbers 2, 4, and 6 are distinct objects when considered separately, but when they are considered collectively they form a single set of size three, written {2,4,6}. Sets are one of the most fundamental concepts in mathematics. Developed at the end of the 19th century, set theory is now a ubiquitous part of mathematics, and can be used as a foundation from which nearly all of mathematics can be derived. In mathematics education, elementary topics such as Venn diagrams are taught at a young age, while more advanced concepts are taught as part of a university degree. The German word "Menge", rendered as "set" in English, was coined by Bernard Bolzano in his work "The Paradoxes of the Infinite".
Definition.
A set is a well defined collection of distinct objects. The objects that make up a set (also known as the elements or members of a set) can be anything: numbers, people, letters of the alphabet, other sets, and so on. Georg Cantor, the founder of set theory, gave the following definition of a set at the beginning of his "":
A set is a gathering together into a whole of definite, distinct objects of our perception [Anschauung] or of our thought—which are called elements of the set.
Sets are conventionally denoted with capital letters. Sets "A" and "B" are equal if and only if they have precisely the same elements.
There is the image popular, that sets are like boxes containing their elements. But there is a huge difference between boxes and sets. While boxes don't change their identity when objects are removed from or added to them, sets change their identity when their elements change. So its better to have the image of a set as the content of an imaginary box:
Cantor's definition turned out to be inadequate for formal mathematics; instead, the notion of a "set" is taken as an undefined primitive in axiomatic set theory, and its properties are defined by the Zermelo–Fraenkel axioms. The most basic properties are that a set has elements, and that two sets are equal (one and the same) if and only if every element of each set is an element of the other.
Describing sets.
There are two ways of describing, or specifying the members of, a set. One way is by intensional definition, using a rule or semantic description:
The second way is by extension – that is, listing each member of the set. An extensional definition is denoted by enclosing the list of members in curly brackets:
One often has the choice of specifying a set either intensionally or extensionally. In the examples above, for instance, "A" = "C" and "B" = "D".
There are two important points to note about sets. First, a set can have two or more members which are identical, for example, {11, 6, 6}. However, we say that two sets which differ only in that one has duplicate members are in fact exactly identical (see Axiom of extensionality). Hence, the set {11, 6, 6} is exactly identical to the set {11, 6}. The second important point is that the order in which the elements of a set are listed is irrelevant (unlike for a sequence or tuple). We can illustrate these two important points with an example:
For sets with many elements, the enumeration of members can be abbreviated. For instance, the set of the first thousand positive integers may be specified extensionally as
where the ellipsis ("...") indicates that the list continues in the obvious way. Ellipses may also be used where sets have infinitely many members. Thus the set of positive even numbers can be written as {2, 4, 6, 8, ... }.
The notation with braces may also be used in an intensional specification of a set. In this usage, the braces have the meaning "the set of all ...". So, "E" = {playing card suits} is the set whose four members are ♠, ♦, ♥, and ♣. A more general form of this is set-builder notation, through which, for instance, the set "F" of the twenty smallest integers that are four less than perfect squares can be denoted
In this notation, the colon (":") means "such that", and the description can be interpreted as ""F" is the set of all numbers of the form "n"2 − 4, such that "n" is a whole number in the range from 0 to 19 inclusive." Sometimes the vertical bar ("|") is used instead of the colon.
Membership.
If "B" is a set and "x" is one of the objects of "B", this is denoted "x" ∈ "B", and is read as "x belongs to B", or "x is an element of B". If "y" is not a member of "B" then this is written as "y" ∉ "B", and is read as "y does not belong to B".
For example, with respect to the sets "A" = {1,2,3,4}, "B" = {blue, white, red}, and "F" = {"n"2 − 4 : "n" is an integer; and 0 ≤ "n" ≤ 19} defined above,
Subsets.
If every member of set "A" is also a member of set "B", then "A" is said to be a "subset" of "B", written "A" ⊆ "B" (also pronounced "A is contained in B"). Equivalently, we can write "B" ⊇ "A", read as "B is a superset of A", "B includes A", or "B contains A". The relationship between sets established by ⊆ is called "inclusion" or "containment".
If "A" is a subset of, but not equal to, "B", then "A" is called a "proper subset" of "B", written "A" ⊊ "B" ("A is a proper subset of B") or "B" ⊋ "A" ("B is a proper superset of A").
Note that the expressions "A" ⊂ "B" and "B" ⊃ "A" are used differently by different authors; some authors use them to mean the same as "A" ⊆ "B" (respectively "B" ⊇ "A"), whereas other use them to mean the same as "A" ⊊ "B" (respectively "B" ⊋ "A").
Example:
The empty set is a subset of every set and every set is a subset of itself:
An obvious but useful identity, which can often be used to show that two seemingly different sets are equal:
A partition of a set "S" is a set of nonempty subsets of "S" such that every element "x" in "S" is in exactly one of these subsets.
Power sets.
The power set of a set "S" is the set of all subsets of "S". Note that the power set contains "S" itself and the empty set because these are both subsets of "S". For example, the power set of the set {1, 2, 3} is {{1, 2, 3}, {1, 2}, {1, 3}, {2, 3}, {1}, {2}, {3}, ∅}. The power set of a set "S" is usually written as "P"("S").
The power set of a finite set with "n" elements has 2"n" elements. This relationship is one of the reasons for the terminology "power set"{{Citation needed|date=February 2014}}. For example, the set {1, 2, 3} contains three elements, and the power set shown above contains 23 = 8 elements.
The power set of an infinite (either countable or uncountable) set is always uncountable. Moreover, the power set of a set is always strictly "bigger" than the original set in the sense that there is no way to pair every element of "S" with exactly one element of "P"("S"). (There is never an onto map or surjection from "S" onto "P"("S").)
Every partition of a set "S" is a subset of the powerset of "S".
Cardinality.
There is a unique set with no members and zero cardinality, which is called the "empty set" (or the "null set") and is denoted by the symbol ∅ (other notations are used; see empty set). For example, the set of all three-sided squares has zero members and thus is the empty set. Though it may seem trivial, the empty set, like the number zero, is important in mathematics; indeed, the existence of this set is one of the fundamental concepts of axiomatic set theory.
Some sets have infinite cardinality. The set N of natural numbers, for instance, is infinite. Some infinite cardinalities are greater than others. For instance, the set of real numbers has greater cardinality than the set of natural numbers. However, it can be shown that the cardinality of (which is to say, the number of points on) a straight line is the same as the cardinality of any segment of that line, of the entire plane, and indeed of any finite-dimensional Euclidean space.
Special sets.
There are some sets that hold great mathematical importance and are referred to with such regularity that they have acquired special names and notational conventions to identify them. One of these is the empty set, denoted {} or ∅. Another is the unit set {x}, which contains exactly one element, namely x. Many of these sets are represented using blackboard bold or bold typeface. Special sets of numbers include
Positive and negative sets are denoted by a superscript - or +. For example ℚ+ represents the set of positive rational numbers.
Each of the above sets of numbers has an infinite number of elements, and each can be considered to be a proper subset of the sets listed below it. The primes are used less frequently than the others outside of number theory and related fields.
Basic operations.
There are several fundamental operations for constructing new sets from given sets.
Unions.
Two sets can be "added" together. The "union" of "A" and "B", denoted by "A" ∪ "B", is the set of all things that are members of either "A" or "B".
Examples:
Some basic properties of unions:
Intersections.
A new set can also be constructed by determining which members two sets have "in common". The "intersection" of "A" and "B", denoted by {{nowrap|"A" ∩ "B",}} is the set of all things that are members of both "A" and "B". If {{nowrap|1="A" ∩ "B" = ∅,}} then "A" and "B" are said to be "disjoint".
Examples:
Some basic properties of intersections:
Complements.
Two sets can also be "subtracted". The "relative complement" of "B" in "A" (also called the "set-theoretic difference" of "A" and "B"), denoted by {{nowrap|"A" \ "B"}} (or {{nowrap|"A" − "B"}}), is the set of all elements that are members of "A" but not members of "B". Note that it is valid to "subtract" members of a set that are not in the set, such as removing the element "green" from the set {{nowrap|{1, 2, 3};}} doing so has no effect.
In certain settings all sets under discussion are considered to be subsets of a given universal set "U". In such cases, {{nowrap|"U" \ "A"}} is called the "absolute complement" or simply "complement" of "A", and is denoted by "A"′.
Examples:
Some basic properties of complements:
An extension of the complement is the symmetric difference, defined for sets "A", "B" as
For example, the symmetric difference of {7,8,9,10} and {9,10,11,12} is the set {7,8,11,12}.
Cartesian product.
A new set can be constructed by associating every element of one set with every element of another set. The "Cartesian product" of two sets "A" and "B", denoted by "A" × "B" is the set of all ordered pairs ("a", "b") such that "a" is a member of "A" and "b" is a member of "B".
Examples:
Some basic properties of cartesian products:
Let "A" and "B" be finite sets. Then
Applications.
Set theory is seen as the foundation from which virtually all of mathematics can be derived. For example, structures in abstract algebra, such as groups, fields and rings, are sets closed under one or more operations.
One of the main applications of naive set theory is constructing relations. A relation from a domain "A" to a codomain "B" is a subset of the Cartesian product "A" × "B". Given this concept, we are quick to see that the set "F" of all ordered pairs ("x", "x"2), where "x" is real, is quite familiar. It has a domain set R and a codomain set that is also R, because the set of all squares is subset of the set of all reals. If placed in functional notation, this relation becomes "f"("x") = "x"2. The reason these two are equivalent is for any given value, "y" that the function is defined for, its corresponding ordered pair, ("y", "y"2) is a member of the set "F".
Axiomatic set theory.
Although initially naive set theory, which defines a set merely as "any well-defined" collection, was well accepted, it soon ran into several obstacles. It was found that this definition spawned , most notably:
The reason is that the phrase "well-defined" is not very well defined. It was important to free set theory of these paradoxes because nearly all of mathematics was being redefined in terms of set theory. In an attempt to avoid these paradoxes, set theory was axiomatized based on first-order logic, and thus axiomatic set theory was born.
For most purposes however, naive set theory is still useful.
Principle of inclusion and exclusion.
This principle gives us the cardinality of the union of sets.
formula_2
De Morgan's Law.
De Morgan stated two laws about Sets.
If A and B are any two Sets then,
The complement of A union B equals the complement of A intersected with the complement of B.
The complement of A intersected with B is equal to the complement of A union to the complement of B.

</doc>
<doc id="26703" url="http://en.wikipedia.org/wiki?curid=26703" title="Statistic">
Statistic

A statistic (singular) is a single measure of some attribute of a sample (e.g., its arithmetic mean value). It is calculated by applying a function (statistical algorithm) to the values of the items of the sample, which are known together as a set of data.
More formally, statistical theory defines a statistic as a function of a sample where the function itself is independent of the sample's distribution; that is, the function can be stated before realization of the data. The term statistic is used both for the function and for the value of the function on a given sample.
A statistic is distinct from a statistical parameter, which is not computable because often the population is much too large to examine and measure all its items. However, a statistic, when used to estimate a population parameter, is called an estimator. For instance, the "sample mean" is a statistic that estimates the "population mean", which is a parameter.
When a statistic (a function) is being used for a specific purpose, it may be referred to by a name indicating its purpose: in descriptive statistics, a descriptive statistic is used to describe the data; in estimation theory, an estimator is used to estimate a parameter of the distribution (population); in statistical hypothesis testing, a test statistic is used to test a hypothesis. However, a single statistic can be used for multiple purposes – for example the sample mean can be used to describe a data set, to estimate the population mean, or to test a hypothesis.
Examples.
In calculating the arithmetic mean of a sample, for example, the algorithm works by summing all the data values observed in the sample and then dividing this sum by the number of data items. This single measure, the mean of the sample, is called a statistic; its value is frequently used as an estimate of the mean value of all items comprising the population from which the sample is drawn. The population mean is also a single measure; however, it is not called a statistic, because it is not obtained from a sample; instead it is called a population parameter, because it is obtained from the whole population.
Other examples of statistics include
Properties.
Observability.
A statistic is an "observable" random variable, which differentiates it both from a "parameter" that is a generally unobservable quantity describing a property of a statistical population, and from an unobservable random variable, such as the difference between an observed measurement and a population average. A parameter can only be computed exactly if the entire population can be observed without error; for instance, in a perfect census or for a population of standardized test takers.
Statisticians often contemplate a parameterized family of probability distributions, any member of which could be the distribution of some measurable aspect of each member of a population, from which a sample is drawn randomly. For example, the parameter may be the average height of 25-year-old men in North America. The height of the members of a sample of 100 such men are measured; the average of those 100 numbers is a statistic. The average of the heights of all members of the population is not a statistic unless that has somehow also been ascertained (such as by measuring every member of the population). The average height of "all" (in the sense of "genetically possible") 25-year-old North American men is a "parameter", and not a statistic.
Statistical properties.
Important potential properties of statistics include completeness, consistency, sufficiency, unbiasedness, minimum mean square error, low variance, robustness, and computational convenience.
Information of a statistic.
Information of a statistic on model parameters can be defined in several ways. The most common is the Fisher information, which is defined on the statistic model induced by the statistic. Kullback information measure can also be used.

</doc>
<doc id="26707" url="http://en.wikipedia.org/wiki?curid=26707" title="Sverige (disambiguation)">
Sverige (disambiguation)

Sverige is the Swedish language name for Sweden and appears on postage stamps of Sweden, but may also refer to:

</doc>
<doc id="26715" url="http://en.wikipedia.org/wiki?curid=26715" title="Slashdot">
Slashdot

Slashdot (sometimes abbreviated as /.) is a news website that originally billed itself as "News for Nerds. Stuff that Matters". It features news stories on science and technology that are submitted and evaluated by its users. Each story has a comments section attached to it. 
"Slashdot" was founded in 1997 by Hope College student Rob Malda, also known as "Commander Taco", and classmate Jeff Bates, also known as "Hemos". It was acquired by holding company Dice in 2012.
Summaries of stories and links to news articles are submitted by Slashdot's own readers, and each story becomes the topic of a threaded discussion among users. Discussion is moderated by a user-based moderation system. Randomly selected moderators are assigned points (typically 5) which they can use to rate a comment. Moderation applies either "-1" or "+1" to the current rating, based on whether the comment is perceived as either "normal", "offtopic", "insightful", "redundant", "interesting", or "troll" (among others). The site's comment and moderation system is administered by its own open source content management system, Slash, which is available under the GNU General Public License.
In 2012, "Slashdot" had around 3.7 million unique visitors per month and received over 5300 comments per day. The site has won more than 20 awards, including People's Voice Awards in 2000 for "Best Community Site" and "Best News Site". Occasionally, a story will link to a server causing a large surge of traffic, which can overwhelm some smaller or independent sites. This phenomenon is known as the "Slashdot effect".
The site is well known for its bias towards the open source software movement.
History.
Slashdot was preceded by Rob Malda's personal website "Chips & Dips", which, launched in July 1997, featured a single "rant" each day about something that interested its author – typically something to do with Linux or open source software. At the time, Malda was a student at Hope College in Holland, Michigan, majoring in computer science. The site became "Slashdot" in September 1997 under the slogan "News for Nerds. Stuff that Matters," and quickly became a hotspot on the Internet for news and information of interest to computer geeks. 
The name "Slashdot" came from a somewhat "obnoxious parody of a URL" – when Malda registered the domain, he desired to make a name that was "silly and unpronounceable" – try pronouncing out, "h-t-t-p-colon-slash-slash-slashdot-dot-org".
By June 1998, the site was seeing as many as 100,000 page views per day and advertisers began to take notice. By December 1998, Slashdot had net revenues of $18,000, yet its Internet profile was higher, and revenues were expected to increase. On June 29, 1999, the site was sold to Linux megasite Andover.net for $1.5 million in cash and $7 million in Andover stock at the IPO price. Part of the deal was contingent upon the continued employment of Malda and Bates and on the achievement of certain "milestones". With the acquisition of Slashdot, Andover.net could now advertise itself as "the leading Linux/Open Source destination on the Internet". 
Andover.net merged with VA Linux on February 3, 2000, which changed its name to SourceForge, Inc. on May 24, 2007, and became Geeknet, Inc. on November 4, 2009.
Slashdot's 10,000th article was posted after two and a half years on February 24, 2000, and the 100,000th article was posted on December 11, 2009 after 12 years online. 
During the first 12 years, the most active story with the most responses posted was the post-2004 US Presidential Election article "Kerry Concedes Election To Bush" with 5,687 posts. This followed the creation of a new article section, "politics.slashdot.org", created at the start of the 2004 election on September 7, 2004. Many of the most popular stories are political, with "Strike on Iraq" (March 19, 2003) the second-most-active article and "Barack Obama Wins US Presidency" (November 5, 2008) the third-most-active. The rest of the 10 most active articles are an article announcing the 2005 London bombings, and several articles about Evolution vs. Intelligent Design, Saddam Hussein's capture, and "Fahrenheit 9/11". Articles about Microsoft and its Windows Operating System are popular. A thread posted in 2002 titled "What's Keeping You On Windows?" was the 10th-most-active story, and an article about Windows 2000/NT4 source-code leaks the most visited article with more than 680,000 hits.
Some controversy erupted on March 9, 2001 after an anonymous user posted the full text of Scientology's "Operating Thetan Level Three" (OT III) document in a comment attached to a Slashdot article. The Church of Scientology demanded that Slashdot remove the document under the Digital Millennium Copyright Act. A week later, in a long article, Slashdot editors explained their decision to remove the page while providing links and information on how to get the document from other sources.
Slashdot Japan was launched on May 28, 2001 (although the first article was published April 5, 2001) and is an official offshoot of the US-based Web site. The site is currently owned by OSDN-Japan, Inc., and carries some of the US-based Slashdot articles as well as localized stories. 
An external site, "New Media Services", has reported the importance of Online Moderation last December 1 2011.
On Valentine's Day 2002, founder Rob Malda proposed to longtime girlfriend Kathleen Fent using the front page of Slashdot. They were married on December 8, 2002, in Las Vegas, Nevada.
Slashdot implemented a subscription service on March 1, 2002. Slashdot's subscription model works by allowing users to pay a small fee to be able to view pages without banner ads, starting at a rate of $5 per 1,000 page views – non-subscribers may still view articles and respond to comments, with banner ads in place. On March 6, 2003, subscribers were given the ability to see articles 10 to 20 minutes before they are released to the public.
Slashdot altered its threaded discussion forum display software to explicitly show domains for links in articles, as "users made a sport out of tricking unsuspecting readers into visiting [Goatse.cx]."
In observance of April Fools' Day in 2006, Slashdot temporarily changed its signature teal color theme to a warm palette of bubblegum pink and changed its masthead from the usual, "News for Nerds" motto to, "OMG!!! Ponies!!!" Editors joked that this was done to increase female readership. In another supposed April Fools' Day joke, User Achievement tags were introduced on April 1, 2009. This system allowed users to be tagged with various achievements, such as "The Tagger" for tagging a story or "Member of the {1,2,3,4,5} Digit UID Club" for having a Slashdot UID consisting of a certain number of digits. While it was posted on April Fools' Day to allow for certain joke achievements, the system is real.
Slashdot unveiled its newly redesigned site on June 4, 2006, following a CSS Redesign Competition. The winner of the competition was Alex Bendiken, who built on the initial CSS framework of the site. The new site looks similar to the old one but is more polished with more rounded curves, collapsible menus, and updated fonts. On November 9 that same year, Malda wrote that Slashdot attained 16,777,215 (or 224 − 1) comments, which broke the database for three hours until the administrators fixed the issue.
On January 25, 2011, the site launched its third major redesign in its 13.5-year history, which gutted the HTML and CSS, and updated the graphics.
On August 25, 2011, Malda resigned as Editor-in-Chief with immediate effect. He did not mention any plans for the future, other than spending more time with his family, catching up on some reading, and possibly writing a book. His final farewell message received over 1,400 posts within 24 hours on the site.
On December 7, 2011, Slashdot announced that it would start to push what the company described as "sponsored" Ask Slashdot questions.
On March 28, 2012, Slashdot launched slashdot TV.
In September 2012, Slashdot was acquired by DHI Group, Inc., alongside other Geeknet websites including SourceForge and Freecode, for $20 million in cash. DHI, owner of job listing and career websites, stated that there were no plans for major changes to Slashdot. However, beginning October 1, 2013, the site is currently in "beta" for a new, controversial redesign that looks more like typical blog websites, with the once elaborate comment system now replaced with a simpler one. Many longtime users were upset with the mandatory migration to the Beta version which adds visual complexity and removes many of the features, such as comment viewing, that distinguished Slashdot from other news sites. An organized "Slashcott" (boycott) of the site was held on February 10–17, 2014.
Administration.
Team.
The site is currently owned by DHI Group, Inc. It was run by its founder, Rob "CmdrTaco" Malda, from 1998 until 2011. He shared editorial responsibilities with several other editors including Timothy Lord, Patrick "Scuttlemonkey" McGarry, Jeff "Soulskill" Boehm, Rob "Samzenpus" Rozeboom, and Keith Dawson. Jonathan "cowboyneal" Pater is another popular editor of Slashdot, who came to work for Slashdot as a programmer and systems administrator. His online nickname (handle), CowboyNeal, is inspired by a Grateful Dead tribute to Neal Cassady in their song, "That's It for the Other One". He is best known as the target of the usual comic poll option, a tradition started by Chris DiBona.
Slash.
Slashdot runs on Slash, a content management system available under the GNU General Public License. 
Peer Moderation.
Slashdot's editors are primarily responsible for selecting and editing the primary stories daily from submitters; they provide a one-paragraph summary for each and a link to an external site where the story originated. Each story becomes the topic for a threaded discussion among the site's users. A user-based moderation system is employed to filter out abusive comments. Every comment is initially given a score of "-1" to "+2", with a default score of "+1" for registered users, "0" for anonymous users (Anonymous Coward), "+2" for users with high "karma", or "−1" for users with low "karma". As moderators read comments attached to articles, they click to moderate the comment, either up ("+1") or down ("−1"). Moderators may choose to attach a particular descriptor to the comments as well, such as "normal", "offtopic", "flamebait", "troll", "redundant", "insightful", "interesting", "informative", "funny", "overrated", or "underrated", with each corresponding to a "-1" or "+1" rating. So a comment may be seen to have a rating of "+1 insightful" or "-1 troll".
Moderation points add to a user's karma, and users with high "karma" are eligible to become moderators themselves. The system does not promote regular users as "moderators" and instead assigns five moderation points at a time to users based on the number of comments they have entered in the system – once a user's moderation points are used up, they can no longer moderate articles (though they can be assigned more moderation points at a later date). Paid staff editors have an unlimited number of moderation points.
A given comment can have any integer score from "-1" to "+5", and registered users of Slashdot can set a personal threshold so that no comments with a lesser score are displayed. For instance, a user reading Slashdot at level "+5" will only see the highest rated comments, while a user reading at level "-1" will see a more "unfiltered, anarchic version".
A meta-moderation system was implemented on September 7, 1999, to moderate the moderators and help contain abuses in the moderation system. Meta-moderators are presented with a set of moderations that they may rate as either "fair" or "unfair". For each moderation, the meta-moderator sees the original comment and the reason assigned by the moderator (e.g. "troll", "funny"), and the meta-moderator can click to see the context of comments surrounding the one that was moderated.
Features.
Slashdot features discussion forums on a variety of technology- and science-related topics, or "News for Nerds", as its motto states. Articles are divided into the following sections:
Tags.
Slashdot uses a system of "tags" where users can categorize a story to group them together and sorting them. Tags are written in all lowercase, with no spaces, and limited to 64 characters. For example, articles could be tagged as being about "security" or "mozilla". Some articles are tagged with longer tags, such as "whatcouldpossiblygowrong" (expressing the perception of catastrophic risk), "suddenoutbreakofcommonsense" (used when the community feels that the subject has finally figured out something obvious), "correlationnotcausation" (used when scientific articles lack direct evidence; see correlation does not imply causation), or "getyourasstomars" (commonly seen in articles about Mars or space exploration).
Culture.
As an online community with primarily user-generated content, many in-jokes and internet memes have developed over the course of the site's history. A popular meme (based on an unscientific Slashdot user poll) is, "In Soviet Russia, "noun" "verb" you!" This type of joke has its roots in the 1960s or earlier, and is known as a "Russian reversal". Other popular memes usually pertain to computing or technology, such as "Imagine a Beowulf cluster of these", "But does it run Linux?", or "Netcraft now confirms: BSD (or some other software package or item) is dying." Some users will also refer to seemingly innocent remarks by correcting them and adding "you insensitive clod!" to the statement – a reference to a February 14, 1986, "Calvin & Hobbes" cartoon. Also, on the television show "Cheers", Season 8 Episode 23 (4/12/1990), Frasier, in protest to a seance, rattles glasses to startle the participants. His wife Lillith remarks, "Frasier, don't be such an insensitive clod!" Or the 11th season of "The Simpsons" episode, "Last Tap Dance in Springfield" (5/7/2000), wherein Frink exclaims to Homer, "I was merely trying to spare the girl's feelings, you insensitive clod!" Users will also typically refer to articles referring to data storage and data capacity by inquiring how much it is in units of Libraries of Congress. Sometimes bandwidth speeds are referred to in units of Libraries of Congress per second. When numbers are quoted, people will comment that the number happens to be the "combination to their luggage" and express false anger at the person who revealed it.
Slashdotters often use the abbreviation TFA which stands for "The fucking article" or RTFA ("Read the fucking article"), which itself is derived from the abbreviation RTFM. Usage of this abbreviation often exposes comments from posters who have not read the article linked to in the main story.
Slashdotters typically like to mock United States Senator Ted Stevens' 2006 description of the Internet as a "series of tubes" or Microsoft CEO Steve Ballmer's chair-throwing incident from 2005. Microsoft founder Bill Gates is a popular target of jokes by Slashdotters, and all stories about Microsoft were once identified with a graphic of Gates looking like a Borg from "". Many Slashdotters have long talked about the supposed release of "Duke Nukem Forever", which was promised in 1997 but was delayed indefinitely (the game was eventually released in 2011). References to the game are commonly brought up in other articles about software packages that are not yet in production even though the announced delivery date has long passed (see vaporware).
Having a low Slashdot user identifier (user ID) is highly valued since they are assigned sequentially; having one is a sign that someone has an older account and has contributed to the site longer. For Slashdot's 10-year anniversary in 2007, one of the items auctioned off in the charity auction for the Electronic Frontier Foundation was a 3-digit Slashdot user ID.
Traffic and publicity.
As of 2006, Slashdot had approximately 5.5 million users per month. As of January 2013, the site's Alexa rank is 2,000, with the average user spending 3 minutes and 18 seconds per day on the site and 82,665 sites linking in. The primary stories on the site consist of a short synopsis paragraph, a link to the original story, and a lengthy discussion section, all contributed by users. Discussion on stories can get up to 10,000 posts per day. Slashdot has been considered a pioneer in user-driven content, influencing other sites such as Google News and Wikipedia. 
There has been a dip in readership as of 2011, primarily due to the increase of technology-related blogs and Twitter feeds. In 2002, approximately 50% of Slashdot's traffic consisted of people who simply check out the headlines and click through, while others participate in discussion boards and take part in the community. Many links in Slashdot stories caused the linked site to get swamped by heavy traffic and its server to collapse. This is known as the "Slashdot effect", a term which was first coined on February 15, 1999 that refers to an article about a "new generation of niche Web portals driving unprecedented amounts of traffic to sites of interest". Today, most major websites can handle the surge of traffic, but the effect continues to occur on smaller or independent sites. These sites are then said to have been "Slashdotted".
Slashdot has received over twenty awards, including People's Voice Awards in 2000 in both of the categories for which it was nominated ("Best Community Site" and "Best News Site"). 
It was also voted as one of "Newsweek"‍ '​s favorite technology Web sites and rated in Yahoo!'s Top 100 Web sites as the "Best Geek Hangout" (2001). The main antagonists in the 2004 novel "Century Rain", by Alastair Reynolds – The Slashers – are named after Slashdot users. The site was mentioned briefly in the 2000 novel "Cosmonaut Keep", written by Ken MacLeod. 
Several celebrities have stated that they either checked the website regularly or participated in its discussion forums using an account. Some of these celebrities include: Apple co-founder Steve Wozniak, writer and actor Wil Wheaton, and id Software technical director John Carmack.

</doc>
<doc id="26740" url="http://en.wikipedia.org/wiki?curid=26740" title="Scandinavia">
Scandinavia

Scandinavia is a historical and cultural-linguistic region in Northern Europe characterized by a common ethno-cultural Germanic heritage and related languages. It comprises the three kingdoms of Denmark, Norway, and Sweden. Modern Norway and Sweden proper are situated on the Scandinavian Peninsula, whereas modern Denmark consists of Jutland and the Danish islands. 
The term Scandinavia is usually used as a cultural term, but in English usage, it is occasionally confused with the purely geographical term "Scandinavian Peninsula", which took its name from the cultural-linguistic concept. The name Scandinavia originally referred vaguely to the formerly Danish, now Swedish, region Scania. The terms "Scandinavia" and "Scandinavian" entered usage in the late 18th century as terms for the three Scandinavian countries, their Germanic majority peoples and associated language and culture, being introduced by the early linguistic and cultural Scandinavist movement. In foreign usage, the term Scandinavia is sometimes incorrectly taken to also include Iceland, the Faroe Islands, and Finland, on account of their historical association with the Scandinavian countries and the Scandinavian peoples and languages.
The southern and by far most populous regions of Scandinavia have a temperate climate. Scandinavia extends north of the Arctic Circle, but has relatively mild weather for its latitude due to the Gulf Stream. Much of the Scandinavian mountains have an alpine tundra climate. There are many lakes and moraines, legacies of the last glacial period, which ended about ten millennia ago.
The Danish, Norwegian, and Swedish languages form a dialect continuum and are known as the Scandinavian languages—all of which are considered mutually intelligible with one another, although Danish is considered much closer to Norwegian. Faroese and Icelandic, sometimes referred to as insular Scandinavian languages, are intelligible with continental Scandinavian languages only to a very limited extent. Finnish, Estonian, Sami languages and several minority languages spoken in Western Russia are related to each other, but are entirely unrelated to the Scandinavian languages. They do, however, include several words that have been adopted during the history from the neighboring languages, just as Swedish, spoken in Sweden today, has borrowed from Finnish.
The vast majority of the human population of Scandinavia are Scandinavians, descended from several (North) Germanic tribes who originally inhabited the southern part of Scandinavia and what is now northern Germany, who spoke a Germanic language that evolved into Old Norse and who were known as Norsemen in the Early Middle Ages. The Vikings are popularly associated with Norse culture. The Icelanders and the Faroese are to a significant extent, but not exclusively, descended from peoples retrospectively known as Scandinavians. A small minority of Sami people live in the extreme north of Scandinavia.
Terminology and use.
In English, Scandinavia usually refers to Denmark, Norway, and Sweden. Some tourist-oriented sources argue for the inclusion of Finland and Iceland, though that broader region is usually known by the countries concerned as "Norden" (Finnish "Pohjoismaat", Icelandic "Norðurlöndin", Faroese "Norðurlond"), or the Nordic countries.
The use of the name Scandinavia as a convenient general term for the three kingdoms of Denmark, Norway and Sweden is fairly recent; according to some historians, it was adopted and introduced in the eighteenth century, at a time when the ideas about a common heritage started to appear and develop into early literary and linguistic Scandinavism. Before this time, the term "Scandinavia" was familiar mainly to classical scholars through Pliny the Elder's writings, and was used vaguely for Scania and the southern region of the peninsula.
As a political term, "Scandinavia" was first used by students agitating for Pan-Scandinavianism in the 1830s. The popular usage of the term in Sweden, Denmark and Norway as a unifying concept became established in the nineteenth century through poems such as Hans Christian Andersen's "I am a Scandinavian" of 1839. After a visit to Sweden, Andersen became a supporter of early political Scandinavism. In a letter describing the poem to a friend, he wrote: "All at once I understood how related the Swedes, the Danes and the Norwegians are, and with this feeling I wrote the poem immediately after my return: 'We are one people, we are called Scandinavians!'"
Finland.
The clearest example of the use of the term "Scandinavia" as a political and societal construct is the unique position of Finland, based largely on parts of modern day Finland being one of the four historical lands in the Swedish kingdom for hundreds of years, thus to much of the world associating Finland with all of Scandinavia. But the creation of a Finnish identity is unique in the region in that it was formed in relation to two different imperial models, the Swedish and the Russian, as described by the University of Jyväskylä based editorial board of the Finnish journal "Yearbook of Political Thought and Conceptual History";
The term is often defined according to the conventions of the cultures that lay claim to the term in their own use. When a speaker wants to explicitly include Finland alongside Scandinavia-proper, the geographic terms Fenno-Scandinavia or Fennoscandia are sometimes used in English, although these terms are hardly if at all used within Scandinavia. More precisely, and subject to no dispute, is that Finland is included in the broader term 'Nordic countries'.
Societal and tourism promotional organizations.
Various promotional agencies of the Nordic countries in the United States (such as The American-Scandinavian Foundation, established in 1910 by the Danish American industrialist Niels Poulsen) serve to promote market and tourism interests in the region. Today, the five Nordic heads of state act as the organization's patrons and according to the official statement by the organization, its mission is "to promote the Nordic region as a whole while increasing the visibility of Denmark, Finland, Iceland, Norway and Sweden in New York City and the United States." The official tourist boards of Scandinavia sometimes cooperate under one umbrella, such as the Scandinavian Tourist Board. The cooperation was introduced for the Asian market in 1986, when the Swedish national tourist board joined the Danish national tourist board to coordinate intergovernmental promotion of the two countries. Norway's government entered one year later. All five Nordic governments participate in the joint promotional efforts in the United States through the Scandinavian Tourist Board of North America.
Use of "Nordic countries" vs. "Scandinavia".
While the term "Scandinavia" is commonly used for Denmark, Norway and Sweden, the term "the Nordic countries" is used unambiguously for Denmark, Norway, Sweden, Finland, and Iceland, including their associated territories (Greenland, the Faroe Islands, and the Åland Islands). Scandinavia can thus be considered a subset of the Nordic countries. Furthermore, the term Fennoscandia refers to Scandinavia, Finland and Karelia, excluding Denmark and overseas territories; however, the usage of this term is restricted to geology, when speaking of the Fennoscandian Shield (Baltic Shield).
In addition to the mainland Scandinavian countries of:
the Nordic countries also consist of:
Estonia has applied for membership in the Nordic Council, referring to its cultural heritage and close linguistic links to Finland, and historical connections to both Denmark and Sweden, although normally Estonia is regarded as one of the Baltic countries. It is similar to the situation of Finland around the 1920s as Finland was considered to be one of the Baltic States as well, as it too had emerged from Russian domination along with the other three countries under similar circumstances. While Finnish and Estonian are Finnic languages, Latvian and Lithuanian are Baltic languages.
It should be noted that whereas the term "Scandinavia" is relatively straightforward as traditionally relating to the three kingdoms of Denmark, Norway and Sweden there exists some ambiguity as regards the ethnic aspect of the concept in the modern era. Traditionally, the terms refers specifically to the majority peoples of Denmark, Norway and Sweden, their states, their Germanic languages and their culture. In the modern era, the term will often include minority peoples such as the Sami and Meänkieli speakers in a political and to some extent cultural sense, as they are citizens of Scandinavian countries and speak Scandinavian languages either as their first or second language. However, Scandinavian is still also seen as an ethnic term for the Germanic majority peoples of Scandinavia, and as such, the inclusion of Sami and Finnish speakers can be seen as controversial within these groups. This is especially the case in Finland, where the Finnish identity has been formed in opposition to a Scandinavian language and identity.
Etymology.
Scandinavia and Scania ("Skåne", the southernmost province of Sweden) are considered to have the same etymology. Both terms are thought to be derived from the Germanic root *"Skaðin-awjō", which appears later in Old English as "Scedenig" and in Old Norse as "Skáney". The earliest identified source for the name Scandinavia is Pliny the Elder's "Natural History", dated to the first century A.D.
Various references to the region can also be found in Pytheas, Pomponius Mela, Tacitus, Ptolemy, Procopius and Jordanes, usually in the form of Scandza. It is believed that the name used by Pliny may be of West Germanic origin, originally denoting Scania. According to some scholars, the Germanic stem can be reconstructed as *"Skaðan-" meaning "danger" or "damage" (English "scathing", German "Schaden", Dutch "schade"). The second segment of the name has been reconstructed as *"awjō", meaning "land on the water" or "island". The name Scandinavia would then mean "dangerous island", which is considered to refer to the treacherous sandbanks surrounding Scania. Skanör in Scania, with its long Falsterbo reef, has the same stem ("skan") combined with -"ör", which means "sandbanks".
In the reconstructed Germanic root *"Skaðin-awjō" (the edh represented in Latin by t or d), the first segment is sometimes considered more uncertain than the second segment. The American Heritage Dictionary derives the second segment from Proto-Indo-European "*akwa-", "water", in the sense of "watery land".
The Old Norse goddess name "Skaði", along with "Sca(n)dinavia" and "Skáney", may be related to Gothic "skadus", Old English "sceadu", Old Saxon "scado", and Old High German "scato" (meaning "shadow"). Scholar John McKinnell comments that this etymology suggests that the goddess Skaði may have once been a personification of the geographical region of Scandinavia or associated with the underworld.
Pliny the Elder's descriptions.
Pliny's descriptions of "Scatinavia" and surrounding areas are not always easy to decipher. Writing in the capacity of a Roman admiral, he introduces the northern region by declaring to his Roman readers that there are 23 islands "Romanis armis cognitae" ("known to Roman arms") in this area. According to Pliny, the "clarissima" ("most famous") of the region's islands is "Scatinavia", of unknown size. There live the Hilleviones. The belief that Scandinavia was an island became widespread among classical authors during the first century and dominated descriptions of Scandinavia in classical texts during the centuries that followed.
Pliny begins his description of the route to "Scatinavia" by referring to the mountain of Saevo ("mons Saevo ibi"), the Codanus Bay ("Codanus sinus") and the Cimbrian promontory. The geographical features have been identified in various ways; by some scholars "Saevo" is thought to be the mountainous Norwegian coast at the entrance to Skagerrak and the Cimbrian peninsula is thought to be Skagen, the north tip of Jutland, Denmark. As described, Saevo and Scatinavia can also be the same place.
Pliny mentions Scandinavia one more time: in Book VIII he says that the animal called "achlis" (given in the accusative, "achlin", which is not Latin), was born on the island of Scandinavia. The animal grazes, has a big upper lip and some mythical attributes.
The name "Scandia", later used as a synonym for Scandinavia, also appears in Pliny's Naturalis Historia, but is used for a group of Northern European islands which he locates north of Britannia. "Scandia" thus does not appear to be denoting the island Scadinavia in Pliny's text. The idea that ""Scadinavia" may have been one of the "Scandiae" islands was instead introduced by Ptolemy (c. 90 – c. 168 AD), a mathematician, geographer and astrologer of Roman Egypt. He used the name "Skandia" for the biggest, most easterly of the three "Scandiai"" islands, which according to him were all located east of Jutland.
Neither Pliny's nor Ptolemy's lists of Scandinavian tribes include the Suiones mentioned by Tacitus. Some early Swedish scholars of the Swedish Hyperborean school and of the 19th-century romantic nationalism period proceeded to synthesize the different versions by inserting references to the Suiones, arguing that they must have been referred to in the original texts and obscured over time by spelling mistakes or various alterations.
Germanic reconstruction.
The Latin names in Pliny's text gave rise to different forms in medieval Germanic texts. In Jordanes' history of the Goths (AD 551) the form "Scandza" is the name used for their original home, separated by sea from the land of Europe (chapter 1, 4). Where Jordanes meant to locate this quasi-legendary island is still a hotly debated issue, both in scholarly discussions and in the nationalistic discourse of various European countries.
The form "Scadinavia" as the original home of the Langobards appears in Paulus Diaconus' "Historia Langobardorum"; in other versions of "Historia Langobardorum" appear the forms "Scadan", "Scandanan", "Scadanan" and "Scatenauge". Frankish sources used "Sconaowe" and Aethelweard, an Anglo-Saxon historian, used "Scani". In Beowulf, the forms "Scedenige" and "Scedeland" are used, while the Alfredian translation of Orosius and Wulfstan's travel accounts used the Old English "Sconeg".
Sami etymology.
The earliest Sami yoik texts written down refer to the world as "Skadesi-suolo" (north-Sami) and "Skađsuâl" (east-Sami), meaning "Skaði's island" (Svennung 1963). Svennung considers the Sami name to have been introduced as a loan word from the North Germanic languages; "Skaði" is the giant stepmother of Freyr and Freyja in Norse mythology. It has been suggested that Skaði to some extent is modeled on a Sami woman. The name for Skade's father Thjazi is known in Sami as "Čáhci", "the waterman", and her son with Odin, Saeming, can be interpreted as a descendent of "Saam" the Sami population (Mundel 2000), (Steinsland 1991). Older joik texts give evidence of the old Sami belief about living on an island and state that the wolf is known as "suolu gievra", meaning "the strong one on the island." The Sami place name "Sulliidčielbma" means "the island's threshold" and "Suoločielgi" means "the island's back."
In recent substrate studies, Sami linguists have examined the initial cluster sk- in words used in Sami and concluded that sk- is a phonotactic structure of alien origin.
Other etymologies.
Scadin- can be segmented various ways to obtain various IndoEuropean uses: scand- or scad-in-, scan- or sca-din, scandin or scadin-. This segmentation has resulted in a number of possible etymologies, such as "climbing island" (*scand-), "island of the Scythian people", "island of the woodland of *sca-".
Another possibility is that all or part of the segments of the name came from the Mesolithic people inhabiting the region. In modernity, Scandinavia is a peninsula, but between approximately 10,300 and 9,500 years ago, the southern part of Scandinavia was an island separated from the northern peninsula, with water exiting the Baltic Sea through the area where Stockholm is now located.
Some Basque scholars have presented the idea that the segment "sk" that appears in *Ska∂inaujàin is connected to the name for the Euzko peoples, akin to Basques, that populated Paleolithic Europe. According to some of these intellects, Scandinavian people share particular genetic markers with the Basque people.
The name of the Scandinavian mountain range, "Skanderna" in Swedish, was artificially derived from "Skandinavien" in the nineteenth century, in analogy with "Alperna" for the Alps. The commonly used names are "bergen" or "fjällen"; both names meaning "the mountains".
According to another source, the Scandinavian name may have originated from the Baltic word "scandia", which means "flooding shores".
Geography.
The geography of Scandinavia is extremely varied. Notable are the Norwegian fjords, the Scandinavian Mountains, the flat, low areas in Denmark, and the archipelagos of Sweden and Norway. Sweden has many lakes and moraines, legacies of the ice age.
The climate varies from north to south and from west to east; a marine west coast climate () typical of western Europe dominates in Denmark, southernmost part of Sweden and along the west coast of Norway reaching north to 65°N, with orographic lift giving more mm/year precipitation (<5000 mm) in some areas in western Norway. The central part – from Oslo to Stockholm – has a humid continental climate (Dfb), which gradually gives way to subarctic climate (Dfc) further north and cool marine west coast climate (Cfc) along the northwestern coast. A small area along the northern coast east of the North Cape has tundra climate (Et) as a result of a lack of summer warmth. The Scandinavian Mountains block the mild and moist air coming from the southwest, thus northern Sweden and the Finnmarksvidda plateau in Norway receive little precipitation and have cold winters. Large areas in the Scandinavian mountains have alpine tundra climate.
The warmest temperature ever recorded in Scandinavia is 38.0 °C in Målilla (Sweden). The coldest temperature ever recorded is −52.6 °C in Vuoggatjålme (Sweden). The coldest month was February 1985 in Vittangi (Sweden) with a mean of −27.2 °C.
Southwesterly winds further warmed by foehn wind can give warm temperatures in narrow Norwegian fjords in winter; Tafjord has recorded 17.9 °C in January and Sunndal 18.9 °C in February.
Languages in Scandinavia.
Two language groups have coexisted on the Scandinavian peninsula since prehistory—the North Germanic languages (Scandinavian languages) and the Sami languages. The majority languages on the peninsula, Swedish and Norwegian, are today, along with Danish, classified as Continental Scandinavian.
The North Germanic languages of Scandinavia are traditionally divided into an East Scandinavian branch (Danish and Swedish) and a West Scandinavian branch (Norwegian, Icelandic, and Faroese), but because of changes appearing in the languages since 1600, the East Scandinavian and West Scandinavian branches are now usually reconfigured into Insular Scandinavian ("ö-nordisk"/"øy-nordisk") featuring Icelandic and Faroese and Continental Scandinavian ("Skandinavisk"), comprising Danish, Norwegian, and Swedish. The modern division is based on the degree of mutual comprehensibility between the languages in the two branches. Note that "skandinavisk(a)" may also refer to a way of speaking one Scandinavian language in a way intended to be more easily understood by speakers of the other Scandinavian languages, like the Danish saying the beginning of a number in Swedish to Swedish people.
Apart from Sami and the languages of minority groups speaking a variant of the majority language of a neighboring state, the following minority languages in Scandinavia are protected under the European Charter for Regional or Minority Languages: Yiddish, Romani Chib, Romanes and Romani.
Continental Scandinavian languages.
The dialects of Denmark, Norway and Sweden form a dialect continuum and are mutually intelligible. The populations of the Scandinavian countries, with a Scandinavian mother tongue, can—at least with some training—understand each other's standard languages as they appear in print and are heard on radio and television. The reason Danish, Swedish and the two official written versions of Norwegian ("Nynorsk" and "Bokmål") are traditionally viewed as different languages, rather than dialects of one common language, is that each is a well established standard language in its respective country. They are related to, but not mutually intelligible with, the other North Germanic languages, Icelandic and Faroese, which are descended from Old West Norse. Danish, Swedish and Norwegian have, since medieval times, been influenced to varying degrees by Middle Low German and standard German. A substantial amount of that influence was a by-product of the economic activity generated by the Hanseatic League.
Norwegians are accustomed to variation, and may perceive Danish and Swedish only as slightly more distant dialects. This is because they have two official written standards, in addition to the habit of strongly holding on to local dialects. The people of Stockholm, Sweden and Copenhagen, Denmark, have the greatest difficulty in understanding other Scandinavian languages. In the Faroe Islands, learning Danish is mandatory. This causes Faroese people to become bilingual in two very distinct North Germanic languages, making it relatively easy for them to understand the other two Mainland Scandinavian languages.
The Scandinavian languages are (as a language family) entirely unrelated to Finnish, Estonian, and Sami languages, which as Uralic languages are distantly related to Hungarian. Owing to the close proximity, there is still a great deal of borrowing from the Swedish and Norwegian languages in the Finnish, Estonian, and Sami languages. The long history of linguistic influence of Swedish on Finnish is also due to the fact that Finnish, the language of the majority in Finland, was treated as a minority language while Finland was part of Sweden. Finnish-speakers had to learn Swedish in order to advance to higher positions. Swedish spoken in today's Finland includes a lot of words that are borrowed from Finnish, whereas the written language remains closer to that of Sweden.
Although Iceland was under the political control of Denmark until a much later date (1918), very little influence and borrowing from Danish has occurred in the Icelandic language. Icelandic remained the preferred language among the ruling classes in Iceland; Danish was not used for official communications, most of the royal officials were of Icelandic descent and the language of the church and law courts remained Icelandic.
Sami languages.
The Sami languages are indigenous minority languages in Scandinavia. They belong to their own branch of the Uralic language family and are unrelated to the North Germanic languages other than by limited grammatical (particularly lexical) characteristics resulting from prolonged contact. Sami is divided into several languages or dialects Consonant gradation is a feature in both Finnish and northern Sami dialects, but it is not present in south Sami, which is considered to have a different language history. According to the Sami Information Centre of the Sami Parliament in Sweden, southern Sami may have originated in an earlier migration from the south into the Scandinavian peninsula.
Finland and Scandinavia.
Finland is officially bilingual, with Finnish and Swedish having mostly the same status at national level. Finland's majority population are Finns, whose mother tongue is either Finnish (approximately 95%), Swedish or both; the Swedish speaking minority lives mainly on the coast from the city of Porvoo, in the Gulf of Finland, to the city of Kokkola, up in the Bothnian Bay. The Åland Islands, an autonomous province of Finland, situated in the Baltic Sea between Finland and Sweden, is entirely Swedish speaking. Children are taught the other official language at school; for Swedish-speakers, this is Finnish (usually from the 3rd grade), and for Finnish-speakers, Swedish (usually from the 3rd, 5th or 7th grade).
Finnish speakers constitute a language minority in Sweden, Norway and Russian Federation. There are also languages derived from Finnish, having evolved separately, known as Meänkieli in Sweden and Kven in Norway.
History.
During a period of Christianization and state formation in the 10th–13th centuries, numerous Germanic petty kingdoms and chiefdoms were unified into three kingdoms:
In the 1645 Treaty of Brömsebro, Denmark–Norway ceded the Norwegian provinces of Jämtland, Härjedalen and Idre & Särna, as well as the Baltic Sea islands of Gotland and Ösel (in Estonia) to Sweden. The Treaty of Roskilde, signed in 1658, forced Denmark–Norway to cede the Danish provinces Scania, Blekinge, Halland, Bornholm and the Norwegian provinces of Båhuslen and Trøndelag to Sweden. The 1660 Treaty of Copenhagen forced Sweden to return Bornholm and Trøndelag to Denmark–Norway, and to give up its recent claims to the island Funen.
Expanding North and East, today's Finland, as well as parts of Estonia and Russia, were under Scandinavian rule for significant periods of time. Scandinavia has, despite many wars over the years since the formation of the three kingdoms, been politically and culturally close.
Scandinavian unions.
The three Scandinavian kingdoms joined in 1387 in the Kalmar Union under Queen Margaret I of Denmark. Sweden left the union in 1523 under King Gustav Vasa. In the aftermath of Sweden's secession from the Kalmar Union, civil war broke out in Denmark and Norway. The Protestant Reformation followed. When things had settled, the Norwegian Privy Council was abolished—it assembled for the last time in 1537. A personal union, entered into by the kingdoms of Denmark and Norway in 1536, lasted until 1814. Three sovereign successor states have subsequently emerged from this unequal union: Denmark, Norway and Iceland.
Denmark–Norway as a historiographical name refers to the former political union consisting of the kingdoms of Denmark and Norway, including the Norwegian dependencies of Iceland, Greenland and the Faroe Islands. The corresponding adjective and demonym is Dano-Norwegian. During Danish rule, Norway kept its separate laws, coinage and army, as well as some institutions such as a royal chancellor. Norway's old royal line had died out with the death of Olav IV in 1387, but Norway's remaining a hereditary kingdom became an important factor for the Oldenburg dynasty of Denmark–Norway in its struggles to win elections as kings of Denmark.
The Treaty of Kiel (14 January 1814) formally dissolved the Dano-Norwegian union and ceded the territory of Norway proper to the King of Sweden, but Denmark retained Norway's overseas possessions. However, widespread Norwegian resistance to the prospect of a union with Sweden induced the governor of Norway, crown prince Christian Frederick (later Christian VIII of Denmark), to call a constituent assembly at Eidsvoll in April 1814. The assembly drew up a liberal constitution and elected Christian Frederick to the throne of Norway. Following a Swedish invasion during the summer, the peace conditions of the Convention of Moss (14 August 1814) specified that king Christian Frederik had to resign, but Norway would keep its independence and its constitution within a personal union with Sweden. Christian Frederik formally abdicated on 10 August 1814 and returned to Denmark. The Norwegian parliament Storting elected king Charles XIII of Sweden as king of Norway on 4 November.
The Storting dissolved the union between Sweden and Norway in 1905, after which the Norwegians elected Prince Charles of Denmark as king of Norway: he reigned as Haakon VII.
Political Scandinavism.
The modern use of the term "Scandinavia" has been influenced by Scandinavism (the Scandinavist political movement), which was active in the middle of the nineteenth century, mainly between the First Schleswig War (1848–1850), in which Sweden and Norway contributed with considerable military force, and the Second Schleswig War (1864). In 1864, the Swedish parliament denounced the promises of military support made to Denmark by Charles XV of Sweden. The members of the Swedish parliament were wary of joining an alliance against Prussia, the rising German power.
The Swedish king also proposed a unification of Denmark, Norway and Sweden into a single united kingdom. The background for the proposal was the tumultuous events during the Napoleonic wars in the beginning of the century. This war resulted in Finland (formerly the eastern third of Sweden) becoming the Russian Grand Duchy of Finland in 1809 and Norway ("de jure" in union with Denmark since 1387, although "de facto" treated as a province) becoming independent in 1814, but thereafter swiftly forced to accept a personal union with Sweden. The dependent territories Iceland, the Faroe Islands and Greenland, historically part of Norway, remained with Denmark in accordance with the Treaty of Kiel. Sweden and Norway were thus united under the Swedish monarch, but Finland's inclusion in the Russian Empire excluded any possibility for a political union between Finland and any of the other Nordic countries.
The end of the Scandinavian political movement came when Denmark was denied the military support promised from Sweden and Norway to annex the (Danish) Duchy of Schleswig, which together with the (German) Duchy of Holstein had been in personal union with Denmark. The Second war of Schleswig followed in 1864, a brief but disastrous war between Denmark and Prussia (supported by Austria). Schleswig-Holstein was conquered by Prussia, and after Prussia's success in the Franco-Prussian War a Prussian-led German Empire was created, and a new power-balance of the Baltic sea countries was established.
Even if a Scandinavian political union never came about at this point, there was a Scandinavian Monetary Union established in 1873, lasting until World War I.

</doc>
<doc id="26751" url="http://en.wikipedia.org/wiki?curid=26751" title="Sun">
Sun

The Sun is the star at the center of the Solar System and is by far the most important source of energy for life on Earth. It is a nearly perfect spherical ball of hot plasma, with internal convective motion that generates a magnetic field via a dynamo process.<ref name="doi10.1146/annurev-astro-081913-040012">Error: Bad DOI specified: 10.1146/annurev-astro-081913-040012</ref> Its diameter is about 109 times that of Earth, and it has a mass about 330,000 times that of Earth, accounting for about 99.86% of the total mass of the Solar System.
Chemically, about three quarters of the Sun's mass consists of hydrogen, whereas the rest is mostly helium, and much smaller quantities of heavier elements, including oxygen, carbon, neon and iron.
The Sun is a G-type main-sequence star (G2V) based on spectral class and it is informally referred to as a yellow dwarf. It formed approximately 4.567 billion years ago from the gravitational collapse of matter within a region of a large molecular cloud. Most of this matter gathered in the center, whereas the rest flattened into an orbiting disk that became the Solar System. The central mass became increasingly hot and dense, eventually initiating thermonuclear fusion in its core. It is thought that almost all stars form by this process. The Sun is roughly middle age and has not changed dramatically for four billion years, and will remain fairly stable for four billion more. However, after hydrogen fusion in its core has stopped, the Sun will undergo severe changes and become a red giant. It is calculated that the Sun will become sufficiently large to engulf the current orbits of Mercury, Venus, and possibly Earth.
The enormous effect of the Sun on the Earth has been recognized since prehistoric times, and the Sun has been regarded by some cultures as a deity. Earth's movement around the Sun is the basis of the solar calendar, which is the predominant calendar in use today.
Name and etymology.
The English proper noun "Sun" developed from Old English "sunne" and may be related to "south". Cognates to English "sun" appear in other Germanic languages, including Old Frisian "sunne", "sonne", Old Saxon "sunna", Middle Dutch "sonne", modern Dutch "zon", Old High German "sunna", modern German "Sonne", Old Norse "sunna", and Gothic "sunnō". All Germanic terms for the Sun stem from Proto-Germanic *"sunnōn".
The Sun is viewed as a goddess in Germanic paganism, Sól/Sunna. Scholars theorize that the Sun, as a Germanic goddess, may represent an extension of an earlier Proto-Indo-European Sun deity due to Indo-European linguistic connections between Old Norse "Sól", Sanskrit "Surya", Gaulish "Sulis", Lithuanian "Saulė", and Slavic "Solntse".
The English weekday name "Sunday" stems from Old English ("Sunnandæg"; "Sun's day", from before 700) and is ultimately a result of a Germanic interpretation of Latin "dies solis", itself a translation of the Greek ἡμέρα ἡλίου ("hēméra hēlíou"). The Latin name for the Sun, "Sol", is widely known but is not common in general English language use; the adjectival form is the related word "solar". The term "sol" is also used by planetary astronomers to refer to the duration of a solar day on another planet, such as Mars. A mean Earth solar day is approximately 24 hours, whereas a mean Martian 'sol' is 24 hours, 39 minutes, and 35.244 seconds.
Characteristics.
The Sun is a G-type main-sequence star that comprises about 99.86% of the mass of the Solar System. Once regarded by astronomers as a small and relatively insignificant star, the Sun has an absolute magnitude of +4.83. This is now estimated to be brighter than about 85% of the stars in the Milky Way, most of which are red dwarfs.
The Sun is a Population I, or heavy-element-rich, star. The formation of the Sun may have been triggered by shockwaves from one or more nearby supernovae. This is suggested by a high abundance of heavy elements in the Solar System, such as gold and uranium, relative to the abundances of these elements in so-called Population II, heavy-element-poor, stars. These elements could most plausibly have been produced by endothermic nuclear reactions during a supernova, or by transmutation through neutron absorption within a massive second-generation star.
The Sun is by far the brightest object in the sky, with an apparent magnitude of −26.74. This is about 13 billion times brighter than the next brightest star, Sirius, which has an apparent magnitude of −1.46. The mean distance of the Sun to Earth is approximately 1 AU, though the distance varies as Earth moves from perihelion in January to aphelion in July. At this average distance, light travels from the Sun to Earth in about 8 minutes and 19 seconds. The energy of this sunlight supports almost all life on Earth by photosynthesis, and drives Earth's climate and weather.
The Sun's radius can be measured from its center to the edge of the photosphere, the apparent visible surface of the Sun. The Sun is a near-perfect sphere with an oblateness estimated at about 9 millionths, which means that its polar diameter differs from its equatorial diameter by only 10 km.
The tidal effect of the planets is weak and does not significantly affect the shape of the Sun. The Sun rotates faster at its equator than at its poles. This differential rotation is caused by convective motion due to heat transport and the Coriolis force due to the Sun's rotation. In a frame of reference defined by the stars, the rotational period is approximately 25.6 days at the equator and 33.5 days at the poles. Viewed from Earth as it orbits the Sun, the "apparent rotational period" of the Sun at its equator is about 28 days.
The Sun does not have a definite boundary, and in its outer parts its density decreases exponentially with increasing distance from its center. The solar interior is not directly observable, and the Sun itself is opaque to electromagnetic radiation. However, just as seismology uses waves generated by earthquakes to reveal the interior structure of Earth, the discipline of helioseismology makes use of pressure waves (infrasound) traversing the Sun's interior to measure and visualize its inner structure. Computer modeling of the Sun is also used as a theoretical tool to investigate its deeper layers.
During a total solar eclipse, when the disk of the Sun is covered by that of the Moon, the Sun's surrounding atmosphere, the corona, can be seen. As the corona expands outward into space, creating the solar wind, a stream of charged particles. The spatial extent of the influence of the solar wind defines the heliosphere, a "bubble" in the interstellar medium that is roughly 100 astronomical units in radius, the largest continuous structure in the Solar System.
The outer boundary of the heliosphere is the heliopause.
Sunlight.
The Sun's color is white, with a CIE color-space index near (0.3, 0.3), when viewed from space or when high in the sky; when low in the sky, atmospheric scattering renders the Sun yellow, red, orange, or magenta. Despite its typical whiteness, most people mentally picture the Sun as yellow; the reasons for this are the subject of debate.
The Sun is a G2V star, with "G2" indicating its surface temperature of approximately 5,778 K (5,505 °C, 9,941 °F), and "V" that it, like most stars, is a main-sequence star. The luminance of the Sun is about 1.88 gigacandela per square metre, but as viewed through Earth's atmosphere, this is lowered to about 1.44 Gcd/m2.
Sunlight is Earth's primary source of energy. The other significant source of Earth's energy is the store of fissionable materials generated by the cataclysmic death of other stars. These fissionable materials trapped in Earth's crust give rise to geothermal energy, which drives the volcanism on Earth and also makes it possible for humans to fuel nuclear reactors. The solar constant is the amount of power that the Sun deposits per unit area that is directly exposed to sunlight. The solar constant is equal to approximately (watts per square meter) at a distance of one astronomical unit (AU) from the Sun (that is, on or near Earth). Sunlight on the surface of Earth is attenuated by Earth's atmosphere so that less power arrives at the surface—closer to in clear conditions when the Sun is near the zenith. Sunlight at the top of Earth's atmosphere is composed (by total energy) of about 50% infrared light, 40% visible light, and 10% ultraviolet light. The atmosphere in particular filters out over 70% of solar ultraviolet, especially at the shorter wavelengths. Solar ultraviolet radiation ionizes the Earth's dayside upper atmosphere, creating the electrically conducting ionosphere.
Solar energy can be harnessed by a variety of natural and synthetic processes—photosynthesis by plants captures the energy of sunlight and converts it to chemical form (oxygen and reduced carbon compounds), whereas direct heating or electrical conversion by solar cells are used by solar power equipment to generate electricity or to do other useful work, sometimes employing concentrating solar power (that it is measured in suns). The energy stored in petroleum and other fossil fuels was originally converted from sunlight by photosynthesis in the distant past.
Composition.
The Sun is composed primarily of the chemical elements hydrogen and helium; they account for 74.9% and 23.8% of the mass of the Sun in the photosphere, respectively. All heavier elements, called "metals" in astronomy, account for less than 2% of the mass, with oxygen (roughly 1% of the Sun's mass), carbon (0.3%), neon (0.2%), and iron (0.2%) being the most abundant.
The Sun inherited its chemical composition from the interstellar medium out of which it formed. The hydrogen and helium in the Sun were produced by Big Bang nucleosynthesis, and the heavier elements were produced by stellar nucleosynthesis in generations of stars that completed their stellar evolution and returned their material to the interstellar medium before the formation of the Sun. The chemical composition of the photosphere is normally considered representative of the composition of the primordial Solar System. However, since the Sun formed, some of the helium and heavy elements have gravitationally settled from the photosphere. Therefore, in today's photosphere the helium fraction is reduced and the metallicity is only 84% of that in the protostellar phase (before nuclear fusion in the core started). The protostellar Sun's composition was reconstructed as 71.1% hydrogen, 27.4% helium, and 1.5% heavier elements.
In the inner portions of the Sun, nuclear fusion has modified the composition by converting hydrogen into helium, so the innermost portion of the Sun is now roughly 60% helium, with the abundance of heavier elements unchanged. Because the interior of the Sun is radiative, not convective (see Radiative zone below), none of the fusion products from the core have risen to the photosphere.
The reactive core zone of "hydrogen burning", where hydrogen is converted into helium, is starting to surround the core of "helium ash". This development will continue and will eventually cause the Sun to leave the main sequence, to become a red giant.
The solar heavy-element abundances described above are typically measured both using spectroscopy of the Sun's photosphere and by measuring abundances in meteorites that have never been heated to melting temperatures. These meteorites are thought to retain the composition of the protostellar Sun and are thus not affected by settling of heavy elements. The two methods generally agree well.
Singly ionized iron-group elements.
In the 1970s, much research focused on the abundances of iron-group elements in the Sun. Although significant research was done, up until 1978 it was difficult to determine the abundance of some iron-group elements (e.g. cobalt and manganese) via spectrography because of their hyperfine structures.
The first largely complete set of oscillator strengths of singly ionized iron-group elements were made available in the 1960s, and these were subsequently improved. In 1978 the abundances of 'singly Ionized' elements of the iron group were derived.
Solar and planetary mass fractionation relationship.
Fractionation is a separation process in which the composition of a mixture varies according to a gradient. Various authors have considered the existence of a mass fractionation relationship between the isotopic compositions of solar and planetary noble gases. For example, correlations between isotopic compositions of neon and xenon in the Sun and on the planets.
Prior to 1983, the belief that the whole Sun has the same composition as the solar atmosphere was widespread. In 1983, it was claimed that it was the fractionation in the Sun itself that caused the fractionation relationship between the isotopic compositions of planetary and solar-wind-implanted noble gases.
Structure.
Core.
The core of the Sun extends from the center to about 20–25% of the solar radius. It has a density of up to (about 150 times the density of water) and a temperature of close to 15.7 million kelvin (K). By contrast, the Sun's surface temperature is approximately 5,800 K. Recent analysis of SOHO mission data favors a faster rotation rate in the core than in the rest of the radiative zone. Through most of the Sun's life, energy is produced by nuclear fusion in the core region through a series of steps called the p–p (proton–proton) chain; this process converts hydrogen into helium. Only 0.8% of the energy generated in the Sun comes from the CNO cycle, though this proportion is expected to increase as the Sun gets older.
The core is the only region in the Sun that produces an appreciable amount of thermal energy through fusion; 99% of the power is generated within 24% of the Sun's radius, and by 30% of the radius, fusion has stopped nearly entirely. The rest of the Sun is heated by this energy that is transferred outwards through many successive layers to the solar photosphere before it escapes into space as sunlight or the kinetic energy of particles.
The proton–proton chain occurs around times each second in the core, converting about 3.7×1038 protons into alpha particles (helium nuclei) every second (out of a total of ~8.9×1056 free protons in the Sun), or about 6.2×1011 kg/s. Fusing four free protons (hydrogen nuclei) into a single alpha particle (helium nuclei) releases around 0.7% of the fused mass as energy, so the Sun releases energy at the mass–energy conversion rate of 4.26 million metric tons per second, for 384.6 yotta watts (), or 9.192×1010 megatons of TNT per second. Theoretical models of the Sun's interior indicate a power density of approximately 276.5 W/m3, a value that more nearly approximates reptile metabolism than a thermonuclear bomb. Peak power production in the Sun has been compared to the volumetric heat generated in an active compost heap. The tremendous power output of the Sun is not due to its high power per volume, but instead due to its large size.
The fusion rate in the core is in a self-correcting equilibrium: a slightly higher rate of fusion would cause the core to heat up more and expand slightly against the weight of the outer layers, reducing the fusion rate and correcting the perturbation; and a slightly lower rate would cause the core to cool and shrink slightly, increasing the fusion rate and again reverting it to its present level.
Radiative zone.
From the core out to about 0.7 solar radii, thermal radiation is the primary means of energy transfer. This zone is not regulated by thermal convection; however the temperature drops from approximately 7 to 2 million kelvin with increasing distance from the core. This temperature gradient is less than the value of the adiabatic lapse rate and hence cannot drive convection. Energy is transferred by radiation—ions of hydrogen and helium emit photons, which travel only a brief distance before being reabsorbed by other ions. The density drops a hundredfold (from 20 g/cm3 to only 0.2 g/cm3) from 0.25 solar radii to the top of the radiative zone.
Tachocline.
The radiative zone and the convective zone are separated by a transition layer, the tachocline. This is a region where the sharp regime change between the uniform rotation of the radiative zone and the differential rotation of the convection zone results in a large shear—a condition where successive horizontal layers slide past one another. The fluid motions found in the convection zone above, slowly disappear from the top of this layer to its bottom, matching the calm characteristics of the radiative zone on the bottom. Presently, it is hypothesized (see Solar dynamo) that a magnetic dynamo within this layer generates the Sun's magnetic field.
Convective zone.
In the Sun's outer layer, from its surface to approximately 200,000 km below (70% of the solar radius from the center), the temperature is lower than in the radiative zone and heavier atoms are not fully ionized. As a result, radiative heat transport is less effective. The density of the plasma is low enough to allow convective currents to develop. Material heated at the tachocline picks up heat and expands, thereby reducing its density and allowing it to rise. As a result, thermal convection develops as thermal cells carry the majority of the heat outward to the Sun's photosphere. Once the material diffusively and radiatively cools just beneath the photospheric surface, its density increases, and it sinks to the base of the convection zone, where it picks up more heat from the top of the radiative zone and the convective cycle continues. At the photosphere, the temperature has dropped to 5,700 K and the density to only 0.2 g/m3 (about 1/6,000th the density of air at sea level).
The thermal columns in the convection zone form an imprint on the surface of the Sun as the solar granulation and supergranulation. The turbulent convection of this outer part of the solar interior sustains "small-scale" dynamo action over the near-surface volume of the Sun. The Sun's thermal columns are Bénard cells and take the shape of hexagonal prisms.
Photosphere.
The visible surface of the Sun, the photosphere, is the layer below which the Sun becomes opaque to visible light. Above the photosphere visible sunlight is free to propagate into space, and its energy escapes the Sun entirely. The change in opacity is due to the decreasing amount of H− ions, which absorb visible light easily. Conversely, the visible light we see is produced as electrons react with hydrogen atoms to produce H− ions.
The photosphere is tens to hundreds of kilometers thick, being slightly less opaque than air on Earth. Because the upper part of the photosphere is cooler than the lower part, an image of the Sun appears brighter in the center than on the edge or "limb" of the solar disk, in a phenomenon known as limb darkening. The spectrum of sunlight has approximately the spectrum of a black-body radiating at about 6,000 K, interspersed with atomic absorption lines from the tenuous layers above the photosphere. The photosphere has a particle density of ~1023 m−3 (about 0.37% of the particle number per volume of Earth's atmosphere at sea level). The photosphere is not fully ionized—the extent of ionization is about 3%, leaving almost all of the hydrogen in atomic form.
During early studies of the optical spectrum of the photosphere, some absorption lines were found that did not correspond to any chemical elements then known on Earth. In 1868, Norman Lockyer hypothesized that these absorption lines were caused by a new element that he dubbed "helium", after the Greek Sun god Helios. Twenty-five years later, helium was isolated on Earth.
Atmosphere.
The parts of the Sun above the photosphere are referred to collectively as the "solar atmosphere". They can be viewed with telescopes operating across the electromagnetic spectrum, from radio through visible light to gamma rays, and comprise five principal zones: the "temperature minimum", the chromosphere, the transition region, the corona, and the heliosphere.
The coolest layer of the Sun is a temperature minimum region about above the photosphere, with a temperature of about . This part of the Sun is cool enough to allow the existence of simple molecules such as carbon monoxide and water, which can be detected via their absorption spectra.
The chromosphere, transition region, and corona are much hotter than the surface of the Sun. The reason is not well understood, but evidence suggests that Alfvén waves may have enough energy to heat the corona.
Above the temperature minimum layer is a layer about thick, dominated by a spectrum of emission and absorption lines. It is called the "chromosphere" from the Greek root "chroma", meaning color, because the chromosphere is visible as a colored flash at the beginning and end of total solar eclipses. The temperature in the chromosphere increases gradually with altitude, ranging up to around near the top. In the upper part of the chromosphere helium becomes partially ionized.
Above the chromosphere, in a thin (about 200 km) transition region, the temperature rises rapidly from around 20,000 K in the upper chromosphere to coronal temperatures closer to 1,000,000 K. The temperature increase is facilitated by the full ionization of helium in the transition region, which significantly reduces radiative cooling of the plasma. The transition region does not occur at a well-defined altitude. Rather, it forms a kind of nimbus around chromospheric features such as spicules and filaments, and is in constant, chaotic motion. The transition region is not easily visible from Earth's surface, but is readily observable from space by instruments sensitive to the extreme ultraviolet portion of the spectrum.
The corona is the next layer of the Sun. The low corona, near the surface of the Sun, has a particle density around 1015–1016 m−3. The average temperature of the corona and solar wind is about 1,000,000–2,000,000 K; however, in the hottest regions it is 8,000,000–20,000,000 K. Although no complete theory yet exists to account for the temperature of the corona, at least some of its heat is known to be from magnetic reconnection.
The corona is the extended atmosphere of the Sun, which has a volume much larger than the volume enclosed by the Sun's photosphere. Waves at the outer surface of the corona that randomly blow even further from the Sun is called the solar wind, and is one of the ways the Sun influences the whole Solar System.
The heliosphere, the tenuous outermost atmosphere of the Sun, is filled with the solar wind plasma. This outermost layer of the Sun is defined to begin at the distance where the flow of the solar wind becomes "superalfvénic"—that is, where the flow becomes faster than the speed of Alfvén waves, at approximately 20 solar radii (0.1 AU).
Turbulence and dynamic forces in the heliosphere cannot affect the shape of the solar corona within, because the information can only travel at the speed of Alfvén waves. The solar wind travels outward continuously through the heliosphere, forming the solar magnetic field into a spiral shape, until it impacts the heliopause more than 50 AU from the Sun. In December 2004, the Voyager 1 probe passed through a shock front that is thought to be part of the heliopause. Both of the Voyager probes have recorded higher levels of energetic particles as they approach the boundary.
The heliosphere extends to the outer fringe of the Solar System, farther than the orbit of Pluto, is defined to end at the heliopause, which is the end of influence from the Sun, and is the boundary with the interstellar medium.
Photons and neutrinos.
High-energy gamma-ray photons initially released with fusion reactions in the core are almost immediately absorbed by the solar plasma of the radiative zone, usually after traveling only a few millimeters. Re-emission happens in a random direction and usually at a slightly lower energy. With this sequence of emissions and absorptions, it takes a long time for radiation to reach the Sun's surface. Estimates of the photon travel time range between 10,000 and 170,000 years. In contrast, it takes only 2.3 seconds for the neutrinos, which account for about 2% of the total energy production of the Sun, to reach the surface. Because energy transport in the Sun is a process that involves photons in thermodynamic equilibrium with matter, the time scale of energy transport in the Sun is longer, on the order of 30,000,000 years. This is the time it would take the Sun to return to a stable state, if the rate of energy generation in its core were suddenly changed.
Neutrinos are also released by the fusion reactions in the core, but, unlike photons, they rarely interact with matter, so almost all are able to escape the Sun immediately. For many years measurements of the number of neutrinos produced in the Sun were lower than theories predicted by a factor of 3. This discrepancy was resolved in 2001 through the discovery of the effects of neutrino oscillation: the Sun emits the number of neutrinos predicted by the theory, but neutrino detectors were missing 2⁄3 of them because the neutrinos had changed flavor by the time they were detected.
Magnetism and activity.
Magnetic field.
The Sun has a magnetic field that varies across a wide range of timescales. The most prominent of such variation is related to the quasi-periodic 11-year solar cycle waxing and waning in the number and size of sunspots.
Sunspots are visible as dark patches on the Sun's photosphere and correspond to concentrations of magnetic field where the convective transport of heat is inhibited from the solar interior to the surface. As a result, sunspots are slightly cooler than the surrounding photosphere, and, so, they appear dark. At a typical solar minimum, few sunspots are visible, and occasionally none can be seen at all. Those that do appear are at high solar latitudes. As the solar cycle progresses towards its maximum, sunspots tend form closer to the solar equator, a phenomenon known as Spörer's law. The largest sunspots can be tens of thousands of kilometers across.
An 11-year sunspot cycle is half of a 22-year Babcock–Leighton dynamo cycle, which corresponds to an oscillatory exchange of energy between toroidal and poloidal solar magnetic fields. At solar-cycle maximum, the external poloidal dipolar magnetic field is near its dynamo-cycle minimum strength, but an internal toroidal quadrupolar field, generated through differential rotation within the tachocline, is near its maximum strength. At this point in the dynamo cycle, buoyant upwelling within the convective zone forces emergence of toroidal magnetic field through the photosphere, giving rise to pairs of sunspots, roughly aligned east–west and having footprints with opposite magnetic polarities. The magnetic polarity of sunspot pairs alternates every solar cycle, a phenomenon known as the Hale cycle.
During the solar cycle’s declining phase, energy shifts from the internal toroidal magnetic field to the external poloidal field, and sunspots diminish in number. At solar-cycle minimum, the toroidal field is, correspondingly, at minimum strength, sunspots are relatively rare, and the poloidal field is at its maximum strength. With the rise of the next 11-year sunspot cycle, differential rotation shifts magnetic energy back from the poloidal to the toroidal field, but with a polarity that is opposite to the previous cycle. The process carries on continuously, and in an idealized, simplified scenario, each 11-year sunspot cycle corresponds to a change, then, in the overall polarity of the Sun's large-scale magnetic field.
The solar magnetic field extends well beyond the Sun itself. The electrically conducting solar wind plasma carries the Sun's magnetic field into space, forming what is called the interplanetary magnetic field. In an approximation known as ideal magnetohydrodynamics, plasma particles only move along the magnetic field lines. As a result, the outward-flowing solar wind stretches the interplanetary magnetic field outward, forcing it into a roughly radial structure. For a simple dipolar solar magnetic field, with opposite hemispherical polarities on either side of the solar magnetic equator, a thin current sheet is formed in the solar wind. At great distances, the rotation of the Sun twists the dipolar magnetic field and corresponding current sheet into an Archimedean spiral structure called the Parker spiral. The interplanetary magnetic field is much stronger than the dipole component of the solar magnetic field. The Sun's dipole magnetic field of 50–400 μT (at the photosphere) reduces with the inverse-cube of the distance to about 0.1 nT at the distance of Earth. However, according to spacecraft observations the interplanetary field at Earth's location is around 5 nT, about a hundred times greater. The difference is due to magnetic fields generated by electrical currents in the plasma surrounding the Sun.
Variation in activity.
The Sun's magnetic field leads to many effects that are collectively called solar activity. Solar flares and coronal-mass ejections tend to occur at sunspot groups. Slowly changing high-speed streams of solar wind are emitted from coronal holes at the photospheric surface. Both coronal-mass ejections and high-speed streams of solar wind carry plasma and interplanetary magnetic field outward into the Solar System. The effects of solar activity on Earth include auroras at moderate to high latitudes and the disruption of radio communications and electric power. Solar activity is thought to have played a large role in the formation and evolution of the Solar System.
With solar-cycle modulation of sunspot number comes a corresponding modulation of space weather conditions, including those surrounding Earth where technological systems can be affected.
Long-term change.
Long-term secular change in sunspot number is thought, by some scientists, to be correlated with long-term change in solar irradiance, which, in turn, might influence Earth's long-term climate.
For example, in the 17th century, the solar cycle appeared to have stopped entirely for several decades; few sunspots were observed during a period known as the Maunder minimum. This coincided in time with the era of the Little Ice Age, when Europe experienced unusually cold temperatures. Earlier extended minima have been discovered through analysis of tree rings and appear to have coincided with lower-than-average global temperatures.
A recent theory claims that there are magnetic instabilities in the core of the Sun that cause fluctuations with periods of either 41,000 or 100,000 years. These could provide a better explanation of the ice ages than the Milankovitch cycles.
Life phases.
The Sun today is roughly halfway through the most stable part of its life. It has not changed dramatically for four billion years, and will remain fairly stable for four billion more. However after hydrogen fusion in its core has stopped, the Sun will undergo severe changes, both internally and externally.
Formation.
The Sun was formed about 4.57 billion years ago from the collapse of part of a giant molecular cloud that consisted mostly of hydrogen and helium and that probably gave birth to many other stars. This age is estimated using computer models of stellar evolution and through nucleocosmochronology. The result is consistent with the radiometric date of the oldest Solar System material, at 4.567 billion years ago. Studies of ancient meteorites reveal traces of stable daughter nuclei of short-lived isotopes, such as iron-60, that form only in exploding, short-lived stars. This indicates that one or more supernovae must have occurred near the location where the Sun formed. A shock wave from a nearby supernova would have triggered the formation of the Sun by compressing the matter within the molecular cloud and causing certain regions to collapse under their own gravity. As one fragment of the cloud collapsed it also began to rotate due to conservation of angular momentum and heat up with the increasing pressure. Much of the mass became concentrated in the center, whereas the rest flattened out into a disk that would become the planets and other Solar System bodies. Gravity and pressure within the core of the cloud generated a lot of heat as it accreted more matter from the surrounding disk, eventually triggering nuclear fusion. Thus, the Sun was born.
Main sequence.
The Sun is about halfway through its main-sequence stage, during which nuclear fusion reactions in its core fuse hydrogen into helium. Each second, more than four million tonnes of matter are converted into energy within the Sun's core, producing neutrinos and solar radiation. At this rate, the Sun has so far converted around 100 times the mass of Earth into energy, about 0.03% of the total mass of the Sun. The Sun will spend a total of approximately 10 billion years as a main-sequence star. The Sun is gradually becoming hotter during its time on the main sequence, because the helium atoms in the core occupy less volume than the hydrogen atoms that were fused. The core is therefore shrinking, allowing the outer layers of the Sun to move closer to the centre and experience a stronger gravitational force, according to the inverse-square law. This stronger force increases the pressure on the core, which is resisted by a gradual increase in the rate at which fusion occurs. This process speeds up as the core gradually becomes denser. It is estimated that the Sun has become 30% brighter in the last 4.5 billion years. At present, it is increasing in brightness by about 1% every 100 million years.
After core hydrogen exhaustion.
The Sun does not have enough mass to explode as a supernova. Instead it will exit the main sequence in approximately 5.4 billion years and start to turn into a red giant. It is calculated that the Sun will become sufficiently large to engulf the current orbits of the Solar System's inner planets, possibly including Earth.
Even before it becomes a red giant, the luminosity of the Sun will have nearly doubled, and Earth will be hotter than Venus is today. Once the core hydrogen is exhausted in 5.4 billion years, the Sun will expand into a subgiant phase and slowly double in size over about half a billion years. It will then expand more rapidly over about half a billion years until it is over two hundred times larger than today and a couple of thousand times more luminous. This then starts the red-giant-branch (RGB) phase where the Sun will spend around a billion years and lose around a third of its mass.
After RGB the Sun has approximately 120 million years of active life left, but much happens. First, the core, full of degenerate helium ignites violently in the helium flash, where it is estimated that 6% of the core, itself 40% of the Sun's mass, will be converted into carbon within a matter of minutes through the triple-alpha process. The Sun then shrinks to around 10 times its current size and 50 times the luminosity, with a temperature a little lower than today. It will then have reached the red clump or horizontal branch (HB), but a star of the Sun's mass does not evolve blueward along the HB. Instead, it just becomes moderately larger and more luminous over about 100 million years as it continues to burn helium in the core.
When the helium is exhausted, the Sun will repeat the expansion it followed when the hydrogen in the core was exhausted, except that this time it all happens faster, and the Sun becomes larger and more luminous. This is the asymptotic-giant-branch (AGB) phase, and the Sun is alternately burning hydrogen in a shell or helium in a deeper shell. After about 20 million years on the early AGB, the Sun becomes increasingly unstable, with rapid mass loss and thermal pulses that increase the size and luminosity for a few hundred years every 100,000 years or so. The thermal pulses become larger each time, with the later pulses pushing the luminosity to as much as 5,000 times the current level and the radius to over 1 AU. Models vary depending on the rate and timing of mass loss. Models that have higher mass loss on the RGB produce smaller, less luminous stars at the tip of the AGB, perhaps only 2,000 times the luminosity and less than 200 times the radius. For the Sun, four thermal pulses are predicted before it completely loses its outer envelope and starts to make a planetary nebula. By the end of that phase – lasting approximately 500,000 years – the Sun will only have about half of its current mass.
The post AGB evolution is even faster. The luminosity stays approximately constant as the temperature increases, with the ejected half of the Sun's mass becoming ionised into a planetary nebula as the exposed core reaches 30,000 K. The final naked core temperature will be over 100,000 K, after which the remnant will cool towards a white dwarf. The planetary nebula will disperse in about 10,000 years, but the white dwarf will survive for trillions of years before fading to black.
Earth's fate.
During the Sun's life in the main sequence, the Sun is becoming more luminous (about 10% every 1 billion years, at the present time). The surface temperature of the Sun is almost constant. The increase of luminosity is essentially due to a slow increase in the solar radius. The increase in solar luminosity is such that in about another billion years Earth's water will evaporate and escape into space, rendering it inhospitable to all known terrestrial life.
Earth is not expected to survive the Sun's transition into a red giant. At its largest, the Sun will have a maximum radius beyond Earth's current orbit, 1 AU (1.5×1011 m), 250 times the present radius of the Sun. By the time the Sun has entered the asymptotic red giant branch, the orbits of the planets will have drifted outwards due to a loss of roughly 30% of the Sun's present mass. Most of this mass will be lost as the solar wind increases. Also, tidal acceleration will help boost Earth to a higher orbit (similar to what Earth does to the Moon). If it were only for this, Earth would probably remain outside the Sun. However, current research suggests that after the Sun becomes a red giant, Earth will be pulled in owing to tidal deceleration.
Motion and location.
Orbit in Milky Way.
The Sun lies close to the inner rim of the Milky Way's Orion Arm, in the Local Interstellar Cloud or the Gould Belt, at a distance of 7.5–8.5 kpc (25,000–28,000 light-years) from the Galactic Center.
The Sun is contained within the Local Bubble, a space of rarefied hot gas, possibly produced by the supernova remnant Geminga. The distance between the local arm and the next arm out, the Perseus Arm, is about 6,500 light-years. The Sun, and thus the Solar System, is found in what scientists call the galactic habitable zone.
The "Apex of the Sun's Way", or the solar apex, is the direction that the Sun travels through space in the Milky Way, relative to other nearby stars. The general direction of the Sun's galactic motion is towards the star Vega in the constellation of Lyra at an angle of roughly 60 sky degrees to the direction of the Galactic Center. Of the 50 nearest stellar systems within 17 light-years from Earth (the closest being the red dwarf Proxima Centauri at approximately 4.2 light-years), the Sun ranks fourth in mass.
The Sun's orbit around the Milky Way is expected to be roughly elliptical with the addition of perturbations due to the galactic spiral arms and non-uniform mass distributions. In addition the Sun oscillates up and down relative to the galactic plane approximately 2.7 times per orbit. It has been argued that the Sun's passage through the higher density spiral arms often coincides with mass extinctions on Earth, perhaps due to increased impact events. It takes the Solar System about 225–250 million years to complete one orbit through the Milky Way (a "galactic year"), so it is thought to have completed 20–25 orbits during the lifetime of the Sun. The orbital speed of the Solar System about the center of the Milky Way is approximately 251 km/s (156 mi/s). At this speed, it takes around 1,190 years for the Solar System to travel a distance of 1 light-year, or 7 days to travel 1 AU.
The Sun's motion about the center of mass of the Solar System is complicated by perturbations from the planets. The barycenter is just outside the volume of the Sun when Jupiter and Saturn (the two planets with the greatest masses) are roughly in the same direction, as seen from the Sun. When they are in opposite directions, and the other planets are aligned appropriately, the barycenter can be very close to the center of the Sun. Every few hundred years this motion switches between prograde and retrograde.
Theoretical problems.
Coronal heating problem.
The temperature of the photosphere is approximately 6,000 K, whereas the temperature of the corona reaches 1,000,000–2,000,000 K. The high temperature of the corona shows that it is heated by something other than direct heat conduction from the photosphere.
It is thought that the energy necessary to heat the corona is provided by turbulent motion in the convection zone below the photosphere, and two main mechanisms have been proposed to explain coronal heating. The first is wave heating, in which sound, gravitational or magnetohydrodynamic waves are produced by turbulence in the convection zone. These waves travel upward and dissipate in the corona, depositing their energy in the ambient matter in the form of heat. The other is magnetic heating, in which magnetic energy is continuously built up by photospheric motion and released through magnetic reconnection in the form of large solar flares and myriad similar but smaller events—nanoflares.
Currently, it is unclear whether waves are an efficient heating mechanism. All waves except Alfvén waves have been found to dissipate or refract before reaching the corona. In addition, Alfvén waves do not easily dissipate in the corona. Current research focus has therefore shifted towards flare heating mechanisms.
Faint young Sun problem.
Theoretical models of the Sun's development suggest that 3.8 to 2.5 billion years ago, during the Archean period, the Sun was only about 75% as bright as it is today. Such a weak star would not have been able to sustain liquid water on Earth's surface, and thus life should not have been able to develop. However, the geological record demonstrates that Earth has remained at a fairly constant temperature throughout its history, and that the young Earth was somewhat warmer than it is today. The consensus among scientists is that the atmosphere of the young Earth contained much larger quantities of greenhouse gases (such as carbon dioxide, methane and/or ammonia) than are present today, which trapped enough heat to compensate for the smaller amount of solar energy reaching it.
History of observation.
The enormous effect of the Sun on the Earth has been recognized since prehistoric times, and the Sun has been regarded by some cultures as a deity.
Early understanding.
The Sun has been an object of veneration in many cultures throughout human history. Humanity's most fundamental understanding of the Sun is as the luminous disk in the sky, whose presence above the horizon creates day and whose absence causes night. In many prehistoric and ancient cultures, the Sun was thought to be a solar deity or other supernatural entity. Worship of the Sun was central to civilizations such as the ancient Egyptians, the Inca of South America and the Aztecs of what is now Mexico. In religions such as Hinduism, the Sun is still considered a God. Many ancient monuments were constructed with solar phenomena in mind; for example, stone megaliths accurately mark the summer or winter solstice (some of the most prominent megaliths are located in Nabta Playa, Egypt; Mnajdra, Malta and at Stonehenge, England); Newgrange, a prehistoric human-built mount in Ireland, was designed to detect the winter solstice; the pyramid of El Castillo at Chichén Itzá in Mexico is designed to cast shadows in the shape of serpents climbing the pyramid at the vernal and autumn equinoxes.
The Egyptians portrayed the god Ra as being carried across the sky in a solar barque, accompanied by lesser gods, and to the Greeks, he was Helios, carried by a chariot drawn by fiery horses. From the reign of Elagabalus in the late Roman Empire the Sun's birthday was a holiday celebrated as Sol Invictus (literally "Unconquered Sun") soon after the winter solstice, which may have been an antecedent to Christmas. Regarding the fixed stars, the Sun appears from Earth to revolve once a year along the ecliptic through the zodiac, and so Greek astronomers considered it to be one of the seven planets (Greek "planetes", “wanderer”), after which the seven days of the week are named in some languages.
Development of scientific understanding.
In the early first millennium BC, Babylonian astronomers observed that the Sun's motion along the ecliptic is not uniform, though they did not know why; it is today known that this is due to the movement of Earth in an elliptic orbit around the Sun, with Earth moving faster when it is nearer to the Sun at perihelion and moving slower when it is farther away at aphelion.
One of the first people to offer a scientific or philosophical explanation for the Sun was the Greek philosopher Anaxagoras, who reasoned that it is a giant flaming ball of metal even larger than the Peloponnesus rather than the chariot of Helios, and that the Moon reflected the light of the Sun. For teaching this heresy, he was imprisoned by the authorities and sentenced to death, though he was later released through the intervention of Pericles. Eratosthenes estimated the distance between Earth and the Sun in the 3rd century BC as "of stadia myriads 400 and 80000", the translation of which is ambiguous, implying either 4,080,000 stadia (755,000 km) or 804,000,000 stadia (148 to 153 million kilometers or 0.99 to 1.02 AU); the latter value is correct to within a few percent. In the 1st century AD, Ptolemy estimated the distance as 1,210 times the radius of Earth, approximately e6km.
The theory that the Sun is the center around which the planets orbit was first proposed by the ancient Greek Aristarchus of Samos in the 3rd century BC, and later adopted by Seleucus of Seleucia (see Heliocentrism). This largely philosophical view was developed into fully predictive mathematical model of a heliocentric system in the 16th century by Nicolaus Copernicus.
Observations of sunspots were recorded during the Han Dynasty (206 BC–AD 220) by Chinese astronomers, who maintained records of these observations for centuries. Averroes also provided a description of sunspots in the 12th century. The invention of the telescope in the early 17th century permitted detailed observations of sunspots by Thomas Harriot, Galileo Galilei and other astronomers. Galileo posited that sunspots were on the surface of the Sun rather than small objects passing between Earth and the Sun.
Arabic astronomical contributions include Albatenius' discovery that the direction of the Sun's apogee (the place in the Sun's orbit against the fixed stars where it seems to be moving slowest) is changing. (In modern heliocentric terms, this is caused by a gradual motion of the aphelion of the "Earth's" orbit). Ibn Yunus observed more than 10,000 entries for the Sun's position for many years using a large astrolabe.
From an observation of a transit of Venus in 1032, the Persian astronomer and polymath Avicenna concluded that Venus is closer to Earth than the Sun. In 1672 Giovanni Cassini and Jean Richer determined the distance to Mars and were thereby able to calculate the distance to the Sun.
In 1666, Isaac Newton observed the Sun's light using a prism, and showed that it is made up of light of many colors. In 1800, William Herschel discovered infrared radiation beyond the red part of the solar spectrum. The 19th century saw advancement in spectroscopic studies of the Sun; Joseph von Fraunhofer recorded more than 600 absorption lines in the spectrum, the strongest of which are still often referred to as Fraunhofer lines. In the early years of the modern scientific era, the source of the Sun's energy was a significant puzzle. Lord Kelvin suggested that the Sun is a gradually cooling liquid body that is radiating an internal store of heat. Kelvin and Hermann von Helmholtz then proposed a gravitational contraction mechanism to explain the energy output, but the resulting age estimate was only 20 million years, well short of the time span of at least 300 million years suggested by some geological discoveries of that time. In 1890 Joseph Lockyer, who discovered helium in the solar spectrum, proposed a meteoritic hypothesis for the formation and evolution of the Sun.
Not until 1904 was a documented solution offered. Ernest Rutherford suggested that the Sun's output could be maintained by an internal source of heat, and suggested radioactive decay as the source. However, it would be Albert Einstein who would provide the essential clue to the source of the Sun's energy output with his mass-energy equivalence relation . In 1920, Sir Arthur Eddington proposed that the pressures and temperatures at the core of the Sun could produce a nuclear fusion reaction that merged hydrogen (protons) into helium nuclei, resulting in a production of energy from the net change in mass. The preponderance of hydrogen in the Sun was confirmed in 1925 by Cecilia Payne using the ionization theory developed by Meghnad Saha, an Indian physicist. The theoretical concept of fusion was developed in the 1930s by the astrophysicists Subrahmanyan Chandrasekhar and Hans Bethe. Hans Bethe calculated the details of the two main energy-producing nuclear reactions that power the Sun. In 1957, Margaret Burbidge, Geoffrey Burbidge, William Fowler and Fred Hoyle showed that most of the elements in the universe have been synthesized by nuclear reactions inside stars, some like the Sun.
Solar space missions.
The first satellites designed to observe the Sun were NASA's Pioneers 5, 6, 7, 8 and 9, which were launched between 1959 and 1968. These probes orbited the Sun at a distance similar to that of Earth, and made the first detailed measurements of the solar wind and the solar magnetic field. Pioneer 9 operated for a particularly long time, transmitting data until May 1983.
In the 1970s, two Helios spacecraft and the Skylab Apollo Telescope Mount provided scientists with significant new data on solar wind and the solar corona. The Helios 1 and 2 probes were U.S.–German collaborations that studied the solar wind from an orbit carrying the spacecraft inside Mercury's orbit at perihelion. The Skylab space station, launched by NASA in 1973, included a solar observatory module called the Apollo Telescope Mount that was operated by astronauts resident on the station. Skylab made the first time-resolved observations of the solar transition region and of ultraviolet emissions from the solar corona. Discoveries included the first observations of coronal mass ejections, then called "coronal transients", and of coronal holes, now known to be intimately associated with the solar wind.
In 1980, the Solar Maximum Mission was launched by NASA. This spacecraft was designed to observe gamma rays, X-rays and UV radiation from solar flares during a time of high solar activity and solar luminosity. Just a few months after launch, however, an electronics failure caused the probe to go into standby mode, and it spent the next three years in this inactive state. In 1984 Space Shuttle Challenger mission STS-41C retrieved the satellite and repaired its electronics before re-releasing it into orbit. The Solar Maximum Mission subsequently acquired thousands of images of the solar corona before re-entering Earth's atmosphere in June 1989.
Launched in 1991, Japan's Yohkoh ("Sunbeam") satellite observed solar flares at X-ray wavelengths. Mission data allowed scientists to identify several different types of flares, and demonstrated that the corona away from regions of peak activity was much more dynamic and active than had previously been supposed. Yohkoh observed an entire solar cycle but went into standby mode when an annular eclipse in 2001 caused it to lose its lock on the Sun. It was destroyed by atmospheric re-entry in 2005.
One of the most important solar missions to date has been the Solar and Heliospheric Observatory, jointly built by the European Space Agency and NASA and launched on 2 December 1995. Originally intended to serve a two-year mission, a mission extension through 2012 was approved in October 2009. It has proven so useful that a follow-on mission, the Solar Dynamics Observatory (SDO), was launched in February 2010. Situated at the Lagrangian point between Earth and the Sun (at which the gravitational pull from both is equal), SOHO has provided a constant view of the Sun at many wavelengths since its launch. Besides its direct solar observation, SOHO has enabled the discovery of a large number of comets, mostly tiny sungrazing comets that incinerate as they pass the Sun.
All these satellites have observed the Sun from the plane of the ecliptic, and so have only observed its equatorial regions in detail. The Ulysses probe was launched in 1990 to study the Sun's polar regions. It first travelled to Jupiter, to "slingshot" into an orbit that would take it far above the plane of the ecliptic. Serendipitously, it was well-placed to observe the collision of Comet Shoemaker–Levy 9 with Jupiter in 1994. Once Ulysses was in its scheduled orbit, it began observing the solar wind and magnetic field strength at high solar latitudes, finding that the solar wind from high latitudes was moving at about 750 km/s, which was slower than expected, and that there were large magnetic waves emerging from high latitudes that scattered galactic cosmic rays.
Elemental abundances in the photosphere are well known from spectroscopic studies, but the composition of the interior of the Sun is more poorly understood. A solar wind sample return mission, Genesis, was designed to allow astronomers to directly measure the composition of solar material. Genesis returned to Earth in 2004 but was damaged by a crash landing after its parachute failed to deploy on re-entry into Earth's atmosphere. Despite severe damage, some usable samples have been recovered from the spacecraft's sample return module and are undergoing analysis.
The Solar Terrestrial Relations Observatory (STEREO) mission was launched in October 2006. Two identical spacecraft were launched into orbits that cause them to (respectively) pull further ahead of and fall gradually behind Earth. This enables stereoscopic imaging of the Sun and solar phenomena, such as coronal mass ejections.
The Indian Space Research Organisation has scheduled the launch of a 100 kg satellite named Aditya for 2015–16. Its main instrument will be a coronagraph for studying the dynamics of the Solar corona.
Observation and effects.
The brightness of the Sun can cause pain from looking at it with the naked eye; however, doing so for brief periods is not hazardous for normal non-dilated eyes. Looking directly at the Sun causes phosphene visual artifacts and temporary partial blindness. It also delivers about 4 milliwatts of sunlight to the retina, slightly heating it and potentially causing damage in eyes that cannot respond properly to the brightness. UV exposure gradually yellows the lens of the eye over a period of years, and is thought to contribute to the formation of cataracts, but this depends on general exposure to solar UV, and not whether one looks directly at the Sun. Long-duration viewing of the direct Sun with the naked eye can begin to cause UV-induced, sunburn-like lesions on the retina after about 100 seconds, particularly under conditions where the UV light from the Sun is intense and well focused; conditions are worsened by young eyes or new lens implants (which admit more UV than aging natural eyes), Sun angles near the zenith, and observing locations at high altitude.
Viewing the Sun through light-concentrating optics such as binoculars may result in permanent damage to the retina without an appropriate filter that blocks UV and substantially dims the sunlight. When using an attenuating filter to view the Sun, the viewer is cautioned to use a filter specifically designed for that use. Some improvised filters that pass UV or IR rays, can actually harm the eye at high brightness levels.
Herschel wedges, also called Solar Diagonals, are effective and inexpensive for small telescopes. The sunlight that is destined for the eyepiece is reflected from an unsilvered surface of a piece of glass. Only a very small fraction of the incident light is reflected. The rest passes through the glass and leaves the instrument. If the glass breaks because of the heat, no light at all is reflected, making the device fail-safe. Simple filters made of darkened glass allow the full intensity of sunlight to pass through if they break, endangering the observer's eyesight. Unfiltered binoculars can deliver hundreds of times as much energy as using the naked eye, possibly causing immediate damage. It is claimed that even brief glances at the midday Sun through an unfiltered telescope can cause permanent damage.
Partial solar eclipses are hazardous to view because the eye's pupil is not adapted to the unusually high visual contrast: the pupil dilates according to the total amount of light in the field of view, "not" by the brightest object in the field. During partial eclipses most sunlight is blocked by the Moon passing in front of the Sun, but the uncovered parts of the photosphere have the same surface brightness as during a normal day. In the overall gloom, the pupil expands from ~2 mm to ~6 mm, and each retinal cell exposed to the solar image receives up to ten times more light than it would looking at the non-eclipsed Sun. This can damage or kill those cells, resulting in small permanent blind spots for the viewer. The hazard is insidious for inexperienced observers and for children, because there is no perception of pain: it is not immediately obvious that one's vision is being destroyed.
During sunrise and sunset, sunlight is attenuated due to Rayleigh scattering and Mie scattering from a particularly long passage through Earth's atmosphere, and the Sun is sometimes faint enough to be viewed comfortably with the naked eye or safely with optics (provided there is no risk of bright sunlight suddenly appearing through a break between clouds). Hazy conditions, atmospheric dust, and high humidity contribute to this atmospheric attenuation.
An optical phenomenon, known as a green flash, can sometimes be seen shortly after sunset or before sunrise. The flash is caused by light from the Sun just below the horizon being bent (usually through a temperature inversion) towards the observer. Light of shorter wavelengths (violet, blue, green) is bent more than that of longer wavelengths (yellow, orange, red) but the violet and blue light is scattered more, leaving light that is perceived as green.
Ultraviolet light from the Sun has antiseptic properties and can be used to sanitize tools and water. It also causes sunburn, and has other medical effects such as the production of vitamin D. Ultraviolet light is strongly attenuated by Earth's ozone layer, so that the amount of UV varies greatly with latitude and has been partially responsible for many biological adaptations, including variations in human skin color in different regions of the globe.

</doc>
<doc id="26829" url="http://en.wikipedia.org/wiki?curid=26829" title="Category of sets">
Category of sets

In the mathematical field of category theory, the category of sets, denoted as Set, is the category whose objects are sets. The arrows or morphisms between sets "A" and "B" are all triples ("f", "A", "B") where "f" is a function from "A" to "B".
Many other categories (such as the category of groups, with group homomorphisms as arrows) add structure to the objects of the category of sets and/or restrict the arrows to functions of a particular kind.
Properties of the category of sets.
The epimorphisms in Set are the surjective maps, the monomorphisms are the injective maps, and the isomorphisms are the bijective maps.
The empty set serves as the initial object in Set with empty functions as morphisms. Every singleton is a terminal object, with the functions mapping all elements of the source sets to the single target element as morphisms. There are thus no zero objects in Set. 
The category Set is complete and co-complete. The product in this category is given by the cartesian product of sets. The coproduct is given by the disjoint union: given sets "A""i" where "i" ranges over some index set "I", we construct the coproduct as the union of "A""i"×{"i"} (the cartesian product with "i" serves to ensure that all the components stay disjoint).
Set is the prototype of a concrete category; other categories are concrete if they "resemble" Set in some well-defined way.
Every two-element set serves as a subobject classifier in Set. The power object of a set "A" is given by its power set, and the exponential object of the sets "A" and "B" is given by the set of all functions from "A" to "B". Set is thus a topos (and in particular cartesian closed).
Set is not abelian, additive or preadditive. Its right zero morphisms are the empty functions ∅ → "X".
Every object in Set which is not initial is injective and (assuming the axiom of choice) also projective.
Foundations for the category of sets.
In Zermelo–Fraenkel set theory the collection of all sets is not a set; this follows from the axiom of foundation. One refers to collections that are not sets as proper classes. One can't handle proper classes as one handles sets; in particular, one can't write that those proper classes belong to a collection (either a set or a proper class). This is a problem: it means that the category of sets cannot be formalized straightforwardly in this setting.
One way to resolve the problem is to work in a system that gives formal status to proper classes, such as NBG set theory. In this setting, categories formed from sets are said to be "small" and those (like Set) that are formed from proper classes are said to be "large".
Another solution is to assume the existence of Grothendieck universes. Roughly speaking, a Grothendieck universe is a set which is itself a model of ZF(C) (for instance if a set belongs to a universe, its elements and its powerset will belong to the universe). The existence of Grothendieck universes (other than the empty set and the set formula_1 of all hereditarily finite sets) is not implied by the usual ZF axioms; it is an additional, independent axiom, roughly equivalent to the existence of strongly inaccessible cardinals. Assuming this extra axiom, one can limit the objects of Set to the elements of a particular universe. (There is no "set of all sets" within the model, but one can still reason about the class "U" of all inner sets, i. e., elements of "U".)
In one variation of this scheme, the class of sets is the union of the entire tower of Grothendieck universes. (This is necessarily a proper class, but each Grothendieck universe is a set because it is an element of some larger Grothendieck universe.) However, one does not work directly with the "category of all sets". Instead, theorems are expressed in terms of the category Set"U" whose objects are the elements of a sufficiently large Grothendieck universe "U", and are then shown not to depend on the particular choice of "U". As a foundation for category theory, this approach is well matched to a system like Tarski–Grothendieck set theory in which one cannot reason directly about proper classes; its principal disadvantage is that a theorem can be true of all Set"U" but not of Set.
Various other solutions, and variations on the above, have been proposed.
The same issues arise with other concrete categories, such as the category of groups or the category of topological spaces.

</doc>
<doc id="26833" url="http://en.wikipedia.org/wiki?curid=26833" title="Scientific method">
Scientific method

The scientific method is a body of techniques for investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge. To be termed scientific, a method of inquiry is commonly based on empirical or measurable evidence subject to specific principles of reasoning. The "Oxford English Dictionary" defines the scientific method as "a method or procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses."
The scientific method is an ongoing process, which usually begins with observations about the natural world. Human beings are naturally inquisitive, so they often come up with questions about things they see or hear and often develop ideas (hypotheses) about why things are the way they are. The best hypotheses lead to predictions that can be tested in various ways, including making further observations about nature. In general, the strongest tests of hypotheses come from carefully controlled and replicated experiments that gather empirical data. Depending on how well the tests match the predictions, the original hypothesis may require refinement, alteration, expansion or even rejection. If a particular hypothesis becomes very well supported a general theory may be developed. 
Although procedures vary from one field of inquiry to another, identifiable features are frequently shared in common between them. The overall process of the scientific method involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions. An hypothesis is a conjecture, based on knowledge obtained while formulating the question. The hypothesis might be very specific or it might be broad. Scientists then test hypotheses by conducting experiments. Under modern interpretations, a scientific hypothesis must be falsifiable, implying that it is possible to identify a possible outcome of an experiment that conflicts with predictions deduced from the hypothesis; otherwise, the hypothesis cannot be meaningfully tested.
The purpose of an experiment is to determine whether observations agree with or conflict with the predictions derived from a hypothesis. Experiments can take place in a college lab, on a kitchen table, at CERN's Large Hadron Collider, at the bottom of an ocean, on Mars, and so on. There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, it represents rather a set of general principles.
Not all steps take place in every scientific inquiry (or to the same degree), and are not always in the same order.
Overview.
The scientific method is the process by which science is carried out. As in other areas of inquiry, science (through the scientific method) can build on previous knowledge and develop a more sophisticated understanding of its topics of study over time. This model can be seen to underlay the scientific revolution. One thousand years ago, Alhazen argued the importance of forming questions and subsequently testing them, an approach which was advocated by Galileo in 1638 with the publication of "Two New Sciences". The current method is based on a hypothetico-deductive model formulated in the 20th century, although it has undergone significant revision since first proposed (for a more formal discussion, see below).
Process.
The overall process involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions to determine whether the original conjecture was correct. There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, they are better considered as general principles. Not all steps take place in every scientific inquiry (or to the same degree), and are not always in the same order. As noted by William Whewell (1794–1866), "invention, sagacity, [and] genius" are required at every step.
Formulation of a question.
The question can refer to the explanation of a specific "observation", as in "Why is the sky blue?", but can also be open-ended, as in "How can I design a drug to cure this particular disease?" This stage frequently involves looking up and evaluating evidence from previous experiments, personal scientific observations or assertions, and/or the work of other scientists. If the answer is already known, a different question that builds on the previous evidence can be posed. When applying the scientific method to scientific research, determining a good question can be very difficult and affects the final outcome of the investigation.
Hypothesis.
An hypothesis is a conjecture, based on knowledge obtained while formulating the question, that may explain the observed behavior of a part of our universe. The hypothesis might be very specific, e.g., Einstein's equivalence principle or Francis Crick's "DNA makes RNA makes protein", or it might be broad, e.g., unknown species of life dwell in the unexplored depths of the oceans. A statistical hypothesis is a conjecture about some population. For example, the population might be people with a particular disease. The conjecture might be that a new drug will cure the disease in some of those people. Terms commonly associated with statistical hypotheses are null hypothesis and alternative hypothesis. A null hypothesis is the conjecture that the statistical hypothesis is false, e.g., that the new drug does nothing and that any cures are due to chance effects. Researchers normally want to show that the null hypothesis is false. The alternative hypothesis is the desired outcome, e.g., that the drug does better than chance. A final point: a scientific hypothesis must be falsifiable, meaning that one can identify a possible outcome of an experiment that conflicts with predictions deduced from the hypothesis; otherwise, it cannot be meaningfully tested.
Prediction.
This step involves determining the logical consequences of the hypothesis. One or more predictions are then selected for further testing. The more unlikely that a prediction would be correct simply by coincidence, then the more convincing it would be if the prediction were fulfilled; evidence is also stronger if the answer to the prediction is not already known, due to the effects of hindsight bias (see also postdiction). Ideally, the prediction must also distinguish the hypothesis from likely alternatives; if two hypotheses make the same prediction, observing the prediction to be correct is not evidence for either one over the other. (These statements about the relative strength of evidence can be mathematically derived using Bayes' Theorem).
Testing.
This is an investigation of whether the real world behaves as predicted by the hypothesis. Scientists (and other people) test hypotheses by conducting experiments. The purpose of an experiment is to determine whether observations of the real world agree with or conflict with the predictions derived from an hypothesis. If they agree, confidence in the hypothesis increases; otherwise, it decreases. Agreement does not assure that the hypothesis is true; future experiments may reveal problems. Karl Popper advised scientists to try to falsify hypotheses, i.e., to search for and test those experiments that seem most doubtful. Large numbers of successful confirmations are not convincing if they arise from experiments that avoid risk. Experiments should be designed to minimize possible errors, especially through the use of appropriate scientific controls. For example, tests of medical treatments are commonly run as double-blind tests. Test personnel, who might unwittingly reveal to test subjects which samples are the desired test drugs and which are placebos, are kept ignorant of which are which. Such hints can bias the responses of the test subjects. Furthermore, failure of an experiment does not necessarily mean the hypothesis is false. Experiments always depend on several hypotheses, e.g., that the test equipment is working properly, and a failure may be a failure of one of the auxiliary hypotheses. (See the Duhem-Quine thesis.) Experiments can be conducted in a college lab, on a kitchen table, at CERN's Large Hadron Collider, at the bottom of an ocean, on Mars (using one of the working rovers), and so on. Astronomers do experiments, searching for planets around distant stars. Finally, most individual experiments address highly specific topics for reasons of practicality. As a result, evidence about broader topics is usually accumulated gradually.
Analysis.
This involves determining what the results of the experiment show and deciding on the next actions to take. The predictions of the hypothesis are compared to those of the null hypothesis, to determine which is better able to explain the data. In cases where an experiment is repeated many times, a statistical analysis such as a chi-squared test may be required. If the evidence has falsified the hypothesis, a new hypothesis is required; if the experiment supports the hypothesis but the evidence is not strong enough for high confidence, other predictions from the hypothesis must be tested. Once a hypothesis is strongly supported by evidence, a new question can be asked to provide further insight on the same topic. Evidence from other scientists and experience are frequently incorporated at any stage in the process. Depending on the complexity of the experiment, many iterations may be required to gather sufficient evidence to answer a question with confidence, or to build up many answers to highly specific questions in order to answer a single broader question.
DNA example.
The discovery became the starting point for many further studies involving the genetic material, such as the field of molecular genetics, and it was awarded the Nobel Prize in 1962. Each step of the example is examined in more detail later in the article.
Other components.
The scientific method also includes other components required even when all the iterations of the steps above have been completed:
Replication.
If an experiment cannot be repeated to produce the same results, this implies that the original results might have been in error. As a result, it is common for a single experiment to be performed multiple times, especially when there are uncontrolled variables or other indications of experimental error. For significant or surprising results, other scientists may also attempt to replicate the results for themselves, especially if those results would be important to their own work.
External review.
The process of peer review involves evaluation of the experiment by experts, who typically give their opinions anonymously. Some journals request that the experimenter provide lists of possible peer reviewers, especially if the field is highly specialized. Peer review does not certify correctness of the results, only that, in the opinion of the reviewer, the experiments themselves were sound (based on the description supplied by the experimenter). If the work passes peer review, which occasionally may require new experiments requested by the reviewers, it will be published in a peer-reviewed scientific journal. The specific journal that publishes the results indicates the perceived quality of the work.
Data recording and sharing.
Scientists typically are careful in recording their data, a requirement promoted by Ludwik Fleck (1896–1961) and others. Though not typically required, they might be requested to supply this data to other scientists who wish to replicate their original results (or parts of their original results), extending to the sharing of any experimental samples that may be difficult to obtain.
Scientific inquiry.
Scientific inquiry generally aims to obtain knowledge in the form of testable explanations that can be used to
predict the results of future experiments. This allows scientists to gain a better understanding of the topic being studied, and later be able to use that understanding to intervene in its causal mechanisms (such as to cure disease). The better an explanation is at making predictions, the more useful it frequently can be, and the more likely it is to continue explaining a body of evidence better than its alternatives. The most successful explanations, which explain and make accurate predictions in a wide range of circumstances, are often called scientific theories.
Most experimental results do not produce large changes in human understanding; improvements in theoretical scientific understanding is typically the result of a gradual process of development over time, sometimes across different domains of science. Scientific models vary in the extent to which they have been experimentally tested and for how long, and in their acceptance in the scientific community. In general, explanations become accepted over time as evidence accumulates on a given topic, and the explanation in question is more powerful than its alternatives at explaining the evidence. Often the explanations are altered over time, or explanations are combined to produce new explanations.
Properties of scientific inquiry.
Scientific knowledge is closely tied to empirical findings, and can remain subject to falsification if new experimental observation incompatible with it is found. That is, no theory can ever be considered final, since new problematic evidence might be discovered. If such evidence is found, a new theory may be proposed, or (more commonly) it is found that modifications to the previous theory are sufficient to explain the new evidence. The strength of a theory can be argued to be related to how long it has persisted without major alteration to its core principles.
Theories can also subject to subsumption by other theories. For example, thousands of years of scientific observations of the planets were explained by Newton's laws. However, these laws were then determined to be special cases of a more general theory (relativity), which explained both the (previously unexplained) exceptions to Newton's laws and predicting and explaining other observations such as the deflection of light by gravity. Thus, in certain cases independent, unconnected, scientific observations can be connected to each other, unified by principles of increasing explanatory power.
Since new theories might be more comprehensive than what preceded them, and thus be able to explain more than previous ones, successor theories might be able to meet a higher standard by explaining a larger body of observations than their predecessors. For example, the theory of evolution explains the diversity of life on Earth, how species adapt to their environments, and many other patterns observed in the natural world; its most recent major modification was unification with genetics to form the modern evolutionary synthesis. In subsequent modifications, it has also subsumed aspects of many other fields such as biochemistry and molecular biology.
Beliefs and biases.
Scientific methodology often directs that hypotheses be tested in controlled conditions wherever possible. This is frequently possible in certain areas, such as in the biological sciences, and more difficult in other areas, such as in astronomy. The practice of experimental control and reproducibility can have the effect of diminishing the potentially harmful effects of circumstance, and to a degree, personal bias. For example, pre-existing beliefs can alter the interpretation of results, as in confirmation bias; this is a heuristic that leads a person with a particular belief to see things as reinforcing their belief, even if another observer might disagree (in other words, people tend to observe what they expect to observe).
A historical example is the belief that the legs of a galloping horse are splayed at the point when none of the horse's legs touches the ground, to the point of this image being included in paintings by its supporters. However, the first stop-action pictures of a horse's gallop by Eadweard Muybridge showed this to be false, and that the legs are instead gathered together. Another important human bias that plays a role is a preference for new, surprising statements (see appeal to novelty), which can result in a search for evidence that the new is true. In contrast to this standard in the scientific method, poorly attested beliefs can be believed and acted upon via a less rigorous heuristic, sometimes taking advantage of the narrative fallacy that when narrative is constructed its elements become easier to believe. Sometimes, these have their elements assumed "a priori", or contain some other logical or methodological flaw in the process that ultimately produced them.
Elements of the scientific method.
There are different ways of outlining the basic method used for scientific inquiry. The scientific community and philosophers of science generally agree on the following classification of method components. These methodological elements and organization of procedures tend to be more characteristic of natural sciences than social sciences. Nonetheless, the cycle of formulating hypotheses, testing and analyzing the results, and formulating new hypotheses, will resemble the cycle described below.
Each element of the scientific method is subject to peer review for possible mistakes. These activities do not describe all that scientists do (see below) but apply mostly to experimental sciences (e.g., physics, chemistry, and biology). The elements above are often taught in the educational system as "the scientific method".
The scientific method is not a single recipe: it requires intelligence, imagination, and creativity. In this sense, it is not a mindless set of standards and procedures to follow,
but is rather an ongoing cycle, constantly developing more useful, accurate and comprehensive models and methods. For example, when Einstein developed the Special and General Theories of Relativity, he did not in any way refute or discount Newton's "Principia". On the contrary, if the astronomically large, the vanishingly small, and the extremely fast are removed from Einstein's theories – all phenomena Newton could not have observed – Newton's equations are what remain. Einstein's theories are expansions and refinements of Newton's theories and, thus, increase our confidence in Newton's work.
A linearized, pragmatic scheme of the four points above is sometimes offered as a guideline for proceeding:
The iterative cycle inherent in this step-by-step method goes from point 3 to 6 back to 3 again.
While this schema outlines a typical hypothesis/testing method, it should also be noted that a number of philosophers, historians and sociologists of science (perhaps most notably Paul Feyerabend) claim that such descriptions of scientific method have little relation to the ways that science is actually practiced.
The "operational" paradigm combines the concepts of operational definition, instrumentalism, and utility:
The essential elements of scientific method are operations, observations, models, and a utility function for evaluating models.
Characterizations.
The scientific method depends upon increasingly sophisticated characterizations of the subjects of investigation. (The "subjects" can also be called or the "unknowns".) For example, Benjamin Franklin conjectured, correctly, that St. Elmo's fire was electrical in nature, but it has taken a long series of experiments and theoretical changes to establish this. While seeking the pertinent properties of the subjects, careful thought may also entail some definitions and observations; the observations often demand careful measurements and/or counting.
The systematic, careful collection of measurements or counts of relevant quantities is often the critical difference between pseudo-sciences, such as alchemy, and science, such as chemistry or biology. Scientific measurements are usually tabulated, graphed, or mapped, and statistical manipulations, such as correlation and regression, performed on them. The measurements might be made in a controlled setting, such as a laboratory, or made on more or less inaccessible or unmanipulatable objects such as stars or human populations. The measurements often require specialized scientific instruments such as thermometers, spectroscopes, particle accelerators, or voltmeters, and the progress of a scientific field is usually intimately tied to their invention and improvement.
 I am not accustomed to saying anything with certainty after only one or two observations.
 — Andreas Vesalius, (1546)
Uncertainty.
Measurements in scientific work are also usually accompanied by estimates of their uncertainty. The uncertainty is often estimated by making repeated measurements of the desired quantity. Uncertainties may also be calculated by consideration of the uncertainties of the individual underlying quantities used. Counts of things, such as the number of people in a nation at a particular time, may also have an uncertainty due to data collection limitations. Or counts may represent a sample of desired quantities, with an uncertainty that depends upon the sampling method used and the number of samples taken.
Definition.
Measurements demand the use of "operational definitions" of relevant quantities. That is, a scientific quantity is described or defined by how it is measured, as opposed to some more vague, inexact or "idealized" definition. For example, electrical current, measured in amperes, may be operationally defined in terms of the mass of silver deposited in a certain time on an electrode in an electrochemical device that is described in some detail. The operational definition of a thing often relies on comparisons with standards: the operational definition of "mass" ultimately relies on the use of an artifact, such as a particular kilogram of platinum-iridium kept in a laboratory in France.
The scientific definition of a term sometimes differs substantially from its natural language usage. For example, mass and weight overlap in meaning in common discourse, but have distinct meanings in mechanics. Scientific quantities are often characterized by their units of measure which can later be described in terms of conventional physical units when communicating the work.
New theories are sometimes developed after realizing certain terms have not previously been sufficiently clearly defined. For example, Albert Einstein's first paper on relativity begins by defining simultaneity and the means for determining length. These ideas were skipped over by Isaac Newton with, "I do not define , space, place and motion, as being well known to all." Einstein's paper then demonstrates that they (viz., absolute time and length independent of motion) were approximations. Francis Crick cautions us that when characterizing a subject, however, it can be premature to define something when it remains ill-understood. In Crick's study of consciousness, he actually found it easier to study awareness in the visual system, rather than to study free will, for example. His cautionary example was the gene; the gene was much more poorly understood before Watson and Crick's pioneering discovery of the structure of DNA; it would have been counterproductive to spend much time on the definition of the gene, before them.
DNA-characterizations.
 The history of the discovery of the structure of DNA is a classic example of the elements of the scientific method: in 1950 it was known that genetic inheritance had a mathematical description, starting with the studies of Gregor Mendel, and that DNA contained genetic information (Oswald Avery's "transforming principle"). But the mechanism of storing genetic information (i.e., genes) in DNA was unclear. Researchers in Bragg's laboratory at Cambridge University made X-ray diffraction pictures of various molecules, starting with crystals of salt, and proceeding to more complicated substances. Using clues painstakingly assembled over decades, beginning with its chemical composition, it was determined that it should be possible to characterize the physical structure of DNA, and the X-ray images would be the vehicle. .."2. DNA-hypotheses"
Another example: precession of Mercury.
The characterization element can require extended and extensive study, even centuries. It took thousands of years of measurements, from the Chaldean, Indian, Persian, Greek, Arabic and European astronomers, to fully record the motion of planet Earth. Newton was able to include those measurements into consequences of his laws of motion. But the perihelion of the planet Mercury's orbit exhibits a precession that cannot be fully explained by Newton's laws of motion (see diagram to the right), as Leverrier pointed out in 1859. The observed difference for Mercury's precession between Newtonian theory and observation was one of the things that occurred to Einstein as a possible early test of his theory of General Relativity. His relativistic calculations matched observation much more closely than did Newtonian theory. The difference is approximately 43 arc-seconds per century.
Hypothesis development.
An hypothesis is a suggested explanation of a phenomenon, or alternately a reasoned proposal suggesting a possible correlation between or among a set of phenomena.
Normally hypotheses have the form of a mathematical model. Sometimes, but not always, they can also be formulated as existential statements, stating that some particular instance of the phenomenon being studied has some characteristic and causal explanations, which have the general form of universal statements, stating that every instance of the phenomenon has a particular characteristic.
Scientists are free to use whatever resources they have – their own creativity, ideas from other fields, induction, Bayesian inference, and so on – to imagine possible explanations for a phenomenon under study. Charles Sanders Peirce, borrowing a page from Aristotle ("Prior Analytics", 2.25) described the incipient stages of inquiry, instigated by the "irritation of doubt" to venture a plausible guess, as "abductive reasoning". The history of science is filled with stories of scientists claiming a "flash of inspiration", or a hunch, which then motivated them to look for evidence to support or refute their idea. Michael Polanyi made such creativity the centerpiece of his discussion of methodology.
William Glen observes that
In general scientists tend to look for theories that are "elegant" or "beautiful". In contrast to the usual English use of these terms, they here refer to a theory in accordance with the known facts, which is nevertheless relatively simple and easy to handle. Occam's Razor serves as a rule of thumb for choosing the most desirable amongst a group of equally explanatory hypotheses.
DNA-hypotheses.
 Linus Pauling proposed that DNA might be a triple helix. This hypothesis was also considered by Francis Crick and James D. Watson but discarded. When Watson and Crick learned of Pauling's hypothesis, they understood from existing data that Pauling was wrong and that Pauling would soon admit his difficulties with that structure. So, the race was on to figure out the correct structure (except that Pauling did not realize at the time that he was in a race) "..3. DNA-predictions"
Predictions from the hypothesis.
Any useful hypothesis will enable predictions, by reasoning including deductive reasoning. It might predict the outcome of an experiment in a laboratory setting or the observation of a phenomenon in nature. The prediction can also be statistical and deal only with probabilities.
It is essential that the outcome of testing such a prediction be currently unknown. Only in this case does a successful outcome increase the probability that the hypothesis is true. If the outcome is already known, it is called a consequence and should have already been considered while formulating the hypothesis.
If the predictions are not accessible by observation or experience, the hypothesis is not yet testable and so will remain to that extent unscientific in a strict sense. A new technology or theory might make the necessary experiments feasible. Thus, much scientifically based speculation might convince one (or many) that the hypothesis that other intelligent species exist is true. But since there no experiment now known which can test this hypothesis, science itself can have little to say about the possibility. In future, some new technique might lead to an experimental test and the speculation would then become part of accepted science.
DNA-predictions.
 James D. Watson, Francis Crick, and others hypothesized that DNA had a helical structure. This implied that DNA's X-ray diffraction pattern would be 'x shaped'. This prediction followed from the work of Cochran, Crick and Vand (and independently by Stokes). The Cochran-Crick-Vand-Stokes theorem provided a mathematical explanation for the empirical observation that diffraction from helical structures produces x shaped patterns.
In their first paper, Watson and Crick also noted that the double helix structure they proposed provided a simple mechanism for DNA replication, writing, "It has not escaped our notice that the specific pairing we have postulated immediately suggests a possible copying mechanism for the genetic material". " ..4. DNA-experiments"
Another example: general relativity.
Einstein's theory of General Relativity makes several specific predictions about the observable structure of space-time, such as that light bends in a gravitational field, and that the amount of bending depends in a precise way on the strength of that gravitational field. Arthur Eddington's observations made during a 1919 solar eclipse supported General Relativity rather than Newtonian gravitation.
Experiments.
Once predictions are made, they can be sought by experiments. If the test results contradict the predictions, the hypotheses which entailed them are called into question and become less tenable. Sometimes the experiments are conducted incorrectly or are not very well designed, when compared to a crucial experiment. If the experimental results confirm the predictions, then the hypotheses are considered more likely to be correct, but might still be wrong and continue to be subject to further testing. The experimental control is a technique for dealing with observational error. This technique uses the contrast between multiple samples (or observations) under differing conditions to see what varies or what remains the same. We vary the conditions for each measurement, to help isolate what has changed. Mill's canons can then help us figure out what the important factor is. Factor analysis is one technique for discovering the important factor in an effect.
Depending on the predictions, the experiments can have different shapes. It could be a classical experiment in a laboratory setting, a double-blind study or an archaeological excavation. Even taking a plane from New York to Paris is an experiment which tests the aerodynamical hypotheses used for constructing the plane.
Scientists assume an attitude of openness and accountability on the part of those conducting an experiment. Detailed record keeping is essential, to aid in recording and reporting on the experimental results, and supports the effectiveness and integrity of the procedure. They will also assist in reproducing the experimental results, likely by others. Traces of this approach can be seen in the work of Hipparchus (190–120 BCE), when determining a value for the precession of the Earth, while controlled experiments can be seen in the works of Jābir ibn Hayyān (721–815 CE), al-Battani (853–929) and Alhazen (965–1039).
DNA-experiments.
 Watson and Crick showed an initial (and incorrect) proposal for the structure of DNA to a team from Kings College – Rosalind Franklin, Maurice Wilkins, and Raymond Gosling. Franklin immediately spotted the flaws which concerned the water content. Later Watson saw Franklin's detailed X-ray diffraction images which showed an and was able to confirm the structure was helical. This rekindled Watson and Crick's model building and led to the correct structure. "..1. DNA-characterizations"
Evaluation and improvement.
The scientific method is iterative. At any stage it is possible to refine its accuracy and precision, so that some consideration will lead the scientist to repeat an earlier part of the process. Failure to develop an interesting hypothesis may lead a scientist to re-define the subject under consideration. Failure of a hypothesis to produce interesting and testable predictions may lead to reconsideration of the hypothesis or of the definition of the subject. Failure of an experiment to produce interesting results may lead a scientist to reconsider the experimental method, the hypothesis, or the definition of the subject.
Other scientists may start their own research and enter the process at any stage. They might adopt the characterization and formulate their own hypothesis, or they might adopt the hypothesis and deduce their own predictions. Often the experiment is not done by the person who made the prediction, and the characterization is based on experiments done by someone else. Published results of experiments can also serve as a hypothesis predicting their own reproducibility.
DNA-iterations.
 After considerable fruitless experimentation, being discouraged by their superior from continuing, and numerous false starts, Watson and Crick were able to infer the essential structure of DNA by concrete modeling of the physical shapes of the nucleotides which comprise it. They were guided by the bond lengths which had been deduced by Linus Pauling and by Rosalind Franklin's X-ray diffraction images. .."DNA Example"
Confirmation.
Science is a social enterprise, and scientific work tends to be accepted by the scientific community when it has been confirmed. Crucially, experimental and theoretical results must be reproduced by others within the scientific community. Researchers have given their lives for this vision; Georg Wilhelm Richmann was killed by ball lightning (1753) when attempting to replicate the 1752 kite-flying experiment of Benjamin Franklin.
To protect against bad science and fraudulent data, government research-granting agencies such as the National Science Foundation, and science journals, including "Nature" and "Science", have a policy that researchers must archive their data and methods so that other researchers can test the data and methods and build on the research that has gone before. Scientific data archiving can be done at a number of national archives in the U.S. or in the World Data Center.
Models of scientific inquiry.
Classical model.
The classical model of scientific inquiry derives from Aristotle, who distinguished the forms of approximate and exact reasoning, set out the threefold scheme of abductive, deductive, and inductive inference, and also treated the compound forms such as reasoning by analogy.
Pragmatic model.
In 1877, Charles Sanders Peirce ( like "purse"; 1839–1914) characterized inquiry in general not as the pursuit of truth "per se" but as the struggle to move from irritating, inhibitory doubts born of surprises, disagreements, and the like, and to reach a secure belief, belief being that on which one is prepared to act. He framed scientific inquiry as part of a broader spectrum and as spurred, like inquiry generally, by actual doubt, not mere verbal or hyperbolic doubt, which he held to be fruitless. He outlined four methods of settling opinion, ordered from least to most successful:
Peirce held that slow, stumbling ratiocination can be dangerously inferior to instinct and traditional sentiment in practical matters, and that the scientific method is best suited to theoretical research, which in turn should not be trammeled by the other methods and practical ends; reason's "first rule" is that, in order to learn, one must desire to learn and, as a corollary, must not block the way of inquiry. The scientific method excels the others by being deliberately designed to arrive – eventually – at the most secure beliefs, upon which the most successful practices can be based. Starting from the idea that people seek not truth "per se" but instead to subdue irritating, inhibitory doubt, Peirce showed how, through the struggle, some can come to submit to truth for the sake of belief's integrity, seek as truth the guidance of potential practice correctly to its given goal, and wed themselves to the scientific method.
For Peirce, rational inquiry implies presuppositions about truth and the real; to reason is to presuppose (and at least to hope), as a principle of the reasoner's self-regulation, that the real is discoverable and independent of our vagaries of opinion. In that vein he defined truth as the correspondence of a sign (in particular, a proposition) to its object and, pragmatically, not as actual consensus of some definite, finite community (such that to inquire would be to poll the experts), but instead as that final opinion which all investigators "would" reach sooner or later but still inevitably, if they were to push investigation far enough, even when they start from different points. In tandem he defined the real as a true sign's object (be that object a possibility or quality, or an actuality or brute fact, or a necessity or norm or law), which is what it is independently of any finite community's opinion and, pragmatically, depends only on the final opinion destined in a sufficient investigation. That is a destination as far, or near, as the truth itself to you or me or the given finite community. Thus, his theory of inquiry boils down to "Do the science." Those conceptions of truth and the real involve the idea of a community both without definite limits (and thus potentially self-correcting as far as needed) and capable of definite increase of knowledge. As inference, "logic is rooted in the social principle" since it depends on a standpoint that is, in a sense, unlimited.
Paying special attention to the generation of explanations, Peirce outlined the scientific method as a coordination of three kinds of inference in a purposeful cycle aimed at settling doubts, as follows (in §III–IV in "A Neglected Argument" except as otherwise noted):
1. Abduction (or retroduction). Guessing, inference to explanatory hypotheses for selection of those best worth trying. From abduction, Peirce distinguishes induction as inferring, on the basis of tests, the proportion of truth in the hypothesis. Every inquiry, whether into ideas, brute facts, or norms and laws, arises from surprising observations in one or more of those realms (and for example at any stage of an inquiry already underway). All explanatory content of theories comes from abduction, which guesses a new or outside idea so as to account in a simple, economical way for a surprising or complicative phenomenon. Oftenest, even a well-prepared mind guesses wrong. But the modicum of success of our guesses far exceeds that of sheer luck and seems born of attunement to nature by instincts developed or inherent, especially insofar as best guesses are optimally plausible and simple in the sense, said Peirce, of the "facile and natural", as by Galileo's natural light of reason and as distinct from "logical simplicity". Abduction is the most fertile but least secure mode of inference. Its general rationale is inductive: it succeeds often enough and, without it, there is no hope of sufficiently expediting inquiry (often multi-generational) toward new truths. Coordinative method leads from abducing a plausible hypothesis to judging it for its testability and for how its trial would economize inquiry itself. Peirce calls his pragmatism "the logic of abduction". His pragmatic maxim is: "Consider what effects that might conceivably have practical bearings you conceive the objects of your conception to have. Then, your conception of those effects is the whole of your conception of the object". His pragmatism is a method of reducing conceptual confusions fruitfully by equating the meaning of any conception with the conceivable practical implications of its object's conceived effects – a method of experimentational mental reflection hospitable to forming hypotheses and conducive to testing them. It favors efficiency. The hypothesis, being insecure, needs to have practical implications leading at least to mental tests and, in science, lending themselves to scientific tests. A simple but unlikely guess, if uncostly to test for falsity, may belong first in line for testing. A guess is intrinsically worth testing if it has instinctive plausibility or reasoned objective probability, while subjective likelihood, though reasoned, can be misleadingly seductive. Guesses can be chosen for trial strategically, for their caution (for which Peirce gave as example the game of Twenty Questions), breadth, and incomplexity. One can hope to discover only that which time would reveal through a learner's sufficient experience anyway, so the point is to expedite it; the economy of research is what demands the leap, so to speak, of abduction and governs its art.
2. Deduction. Two stages:
3. Induction. The long-run validity of the rule of induction is deducible from the principle (presuppositional to reasoning in general) that the real is only the object of the final opinion to which adequate investigation would lead; anything to which no such process would ever lead would not be real. Induction involving ongoing tests or observations follows a method which, sufficiently persisted in, will diminish its error below any predesignate degree. Three stages:
Communication and community.
Frequently the scientific method is employed not only by a single person, but also by several people cooperating directly or indirectly. Such cooperation can be regarded as an important element of a scientific community. Various standards of scientific methodology are used within such an environment.
Peer review evaluation.
Scientific journals use a process of "peer review", in which scientists' manuscripts are submitted by editors of scientific journals to (usually one to three) fellow (usually anonymous) scientists familiar with the field for evaluation. In certain journals, the journal itself selects the referees; while in others (especially journals that are extremely specialized), the manuscript author might recommend referees. The referees may or may not recommend publication, or they might recommend publication with suggested modifications, or sometimes, publication in another journal. This standard is practiced to various degrees by different journals, and can have the effect of keeping the literature free of obvious errors and to generally improve the quality of the material, especially in the journals who use the standard most rigorously. The peer review process can have limitations when considering research outside the conventional scientific paradigm: problems of "groupthink" can interfere with open and fair deliberation of some new research.
Documentation and replication.
Sometimes experimenters may make systematic errors during their experiments, veer from standard methods and practices (Pathological science) for various reasons, or, in rare cases, deliberately report false results. Occasionally because of this then, other scientists might attempt to repeat the experiments in order to duplicate the results.
Archiving.
Researchers sometimes practice scientific data archiving, such as in compliance with the policies of government funding agencies and scientific journals. In these cases, detailed records of their experimental procedures, raw data, statistical analyses and source code can be preserved in order to provide evidence of the methodology and practice of the procedure and assist in any potential future attempts to reproduce the result. These procedural records may also assist in the conception of new experiments to test the hypothesis, and may prove useful to engineers who might examine the potential practical applications of a discovery.
Data sharing.
When additional information is needed before a study can be reproduced, the author of the study might be asked to provide it. They might provide it, or if the author refuses to share data, appeals can be made to the journal editors who published the study or to the institution which funded the research.
Limitations.
Since it is impossible for a scientist to record "everything" that took place in an experiment, facts selected for their apparent relevance are reported. This may lead, unavoidably, to problems later if some supposedly irrelevant feature is questioned. For example, Heinrich Hertz did not report the size of the room used to test Maxwell's equations, which later turned out to account for a small deviation in the results. The problem is that parts of the theory itself need to be assumed in order to select and report the experimental conditions. The observations are hence sometimes described as being 'theory-laden'.
Dimensions of practice.
The primary constraints on contemporary science are:
It has not always been like this: in the old days of the "gentleman scientist" funding (and to a lesser extent publication) were far weaker constraints.
Both of these constraints indirectly require scientific method – work that violates the constraints will be difficult to publish and difficult to get funded. Journals require submitted papers to conform to "good scientific practice" and to a degree this can be enforced by peer review. Originality, importance and interest are more important – see for example the for "Nature".
Philosophy and sociology of science.
Philosophy of science looks at the underpinning logic of the scientific method, at what separates science from non-science, and the ethic that is implicit in science. There are basic assumptions, derived from philosophy by at least one prominent scientist, that form the base of the scientific method – namely, that reality is objective and consistent, that humans have the capacity to perceive reality accurately, and that rational explanations exist for elements of the real world. These assumptions from methodological naturalism form a basis on which science may be grounded. Logical Positivist, empiricist, falsificationist, and other theories have criticized these assumptions and given alternative accounts of the logic of science, but each has also itself been criticized.
Thomas Kuhn examined the history of science in his "The Structure of Scientific Revolutions", and found that the actual method used by scientists differed dramatically from the then-espoused method. His observations of science practice are essentially sociological and do not speak to how science is or can be practiced in other times and other cultures.
Norwood Russell Hanson, Imre Lakatos and Thomas Kuhn have done extensive work on the "theory laden" character of observation. Hanson (1958) first coined the term for the idea that all observation is dependent on the conceptual framework of the observer, using the concept of gestalt to show how preconceptions can affect both observation and description. He opens Chapter 1 with a discussion of the Golgi bodies and their initial rejection as an artefact of staining technique, and a discussion of Brahe and Kepler observing the dawn and seeing a "different" sun rise despite the same physiological phenomenon. Kuhn and Feyerabend acknowledge the pioneering significance of his work.
Kuhn (1961) said the scientist generally has a theory in mind before designing and undertaking experiments so as to make empirical observations, and that the "route from theory to measurement can almost never be traveled backward". This implies that the way in which theory is tested is dictated by the nature of the theory itself, which led Kuhn (1961, p. 166) to argue that "once it has been adopted by a profession ... no theory is recognized to be testable by any quantitative tests that it has not already passed".
Paul Feyerabend similarly examined the history of science, and was led to deny that science is genuinely a methodological process. In his book "Against Method" he argues that scientific progress is "not" the result of applying any particular method. In essence, he says that for any specific method or norm of science, one can find a historic episode where violating it has contributed to the progress of science. Thus, if believers in scientific method wish to express a single universally valid rule, Feyerabend jokingly suggests, it should be 'anything goes'. Criticisms such as his led to the strong programme, a radical approach to the sociology of science.
The postmodernist critiques of science have themselves been the subject of intense controversy. This ongoing debate, known as the science wars, is the result of conflicting values and assumptions between the postmodernist and realist camps. Whereas postmodernists assert that scientific knowledge is simply another discourse (note that this term has special meaning in this context) and not representative of any form of fundamental truth, realists in the scientific community maintain that scientific knowledge does reveal real and fundamental truths about reality. Many books have been written by scientists which take on this problem and challenge the assertions of the postmodernists while defending science as a legitimate method of deriving truth.
Role of chance in discovery.
Somewhere between 33% and 50% of all scientific discoveries are estimated to have been "stumbled upon", rather than sought out. This may explain why scientists so often express that they were lucky. Louis Pasteur is credited with the famous saying that "Luck favours the prepared mind", but some psychologists have begun to study what it means to be 'prepared for luck' in the scientific context. Research is showing that scientists are taught various heuristics that tend to harness chance and the unexpected. This is what Nassim Nicholas Taleb calls "Anti-fragility"; while some systems of investigation are fragile in the face of human error, human bias, and randomness, the scientific method is more than resistant or tough – it actually benefits from such randomness in many ways (it is anti-fragile). Taleb believes that the more anti-fragile the system, the more it will flourish in the real world.
Psychologist Kevin Dunbar says the process of discovery often starts with researchers finding bugs in their experiments. These unexpected results lead researchers to try to fix what they "think" is an error in their method. Eventually, the researcher decides the error is too persistent and systematic to be a coincidence. The highly controlled, cautious and curious aspects of the scientific method are thus what make it well suited for identifying such persistent systematic errors. At this point, the researcher will begin to think of theoretical explanations for the error, often seeking the help of colleagues across different domains of expertise.
History.
The development of the scientific method is inseparable from the history of science itself. Ancient Egyptian documents describe empirical methods in astronomy, mathematics, and medicine. The ancient Greek philosopher Thales in the 6th century BCE refused to accept supernatural, religious or mythological explanations for natural phenomena, proclaiming that every event had a natural cause. The development of deductive reasoning by Plato was an important step towards the scientific method. Empiricism seems to have been formalized by Aristotle, who believed that universal truths could be reached via induction.
For the beginnings of scientific method: Karl Popper writes of Parmenides ("fl." 5th century BCE): "So what was really new in Parmenides was his axiomatic-deductive method, which Leucippus and Democritus turned into a hypothetical-deductive method, and thus made part of scientific methodology."
According to David Lindberg, Aristotle (4th century BCE) wrote about the scientific method even if he and his followers did not actually follow what he said. Lindberg also notes that Ptolemy (2nd century CE) and Ibn al-Haytham (11th century CE) are among the early examples of people who carried out scientific experiments.
 Also, John Losee writes that "the "Physics" and the "Metaphysics" contain discussions of certain aspects of scientific method", of which, he says "Aristotle viewed scientific inquiry as a progression from observations to general principles and back to observations."
Early Christian leaders such as Clement of Alexandria (150–215) and Basil of Caesarea (330–379) encouraged future generations to view the Greek wisdom as "handmaidens to theology" and science was considered a means to more accurate understanding of the Bible and of God. Augustine of Hippo (354–430) who contributed great philosophical wealth to the Latin Middle Ages, advocated the study of science and was wary of philosophies that disagreed with the Bible, such as astrology and the Greek belief that the world had no beginning. This Christian accommodation with Greek science "laid a foundation for the later widespread, intensive study of natural philosophy during the Late Middle Ages." However, the division of Latin-speaking Western Europe from the Greek-speaking East, followed by barbarian invasions, the Plague of Justinian, and the Islamic invasion, resulted in the West largely losing access to Greek wisdom.
By the 8th century Islam had overrun the Christian lands of Syria, Iraq, Iran and Egypt This swift occupation further severed Western Europe from many of the great works of Aristotle, Plato, Euclid and others, many of which were housed in the great library of Alexandria. Having come upon such a wealth of knowledge, the Arabs, who viewed non-Arab languages as inferior, even as a source of pollution, employed conquered Christians and Jews to translate these works from the native Greek and Syriac into Arabic
Thus equipped, Arab philosopher Alhazen (Ibn al-Haytham) performed optical and physiological experiments, reported in his manifold works, the most famous being "Book of Optics" (1021). He was thus a forerunner of scientific method, having understood that a controlled environment involving experimentation and measurement is required in order to draw educated conclusions. Other Arab polymaths of the same era produced copious works on mathematics, philosophy, astronomy and alchemy. Most stuck closely to Aristotle, being hesitant to admit that some of Aristotle's thinking was errant, while others strongly criticized him.
During these years, occasionally a paraphrased translation from the Arabic, which itself had been translated from Greek and Syriac, might make its way to the West for scholarly study. It was not until 1204, during which the Latins conquered and took Constantinople from the Byzantines in the name of the fourth Crusade, that a renewed scholarly interest in the original Greek manuscripts began to grow. Due to the new easier access to the libraries of Constantinople by Western scholars, a certain revival in the study and analysis of the original Greek texts by Western scholars began. From that point a functional scientific method that would launch modern science was on the horizon.
Grosseteste (1175–1253), an English statesman, scientist and Christian theologian, was "the principal figure" in bringing about "a more adequate method of scientific inquiry" by which "medieval scientists were able eventually to outstrip their ancient European and Muslim teachers" (Dales 1973:62). ... His thinking influenced Roger Bacon, who spread Grosseteste's ideas from Oxford to the University of Paris during a visit there in the 1240s. From the prestigious universities in Oxford and Paris, the new experimental science spread rapidly throughout the medieval universities: "And so it went to Galileo, William Gilbert, Francis Bacon, William Harvey, Descartes, Robert Hooke, Newton, Leibniz, and the world of the seventeenth century" (Crombie 1962:15). So it went to us as well.| Hugh G. Gauch, 2003.
Roger Bacon (1214–1294), an English thinker and experimenter, is recognized by many to be the father of modern scientific method. His view that mathematics was essential to a correct understanding of natural philosophy was considered to be 400 years ahead of its time. He was viewed as "a lone genius proclaiming the truth about time," having correctly calculated the calendar His work in optics provided the platform on which Newton, Descartes, Huygens and others later transformed the science of light. Bacon's groundbreaking advances were due largely to his discovery that experimental science must be based on mathematics. (186–187) His works Opus Majus and De Speculis Comburentibus contain many "carefully drawn diagrams showing Bacon's meticulous investigations into the behavior of light." He gives detailed descriptions of systematic studies using prisms and measurements by which he shows how a rainbow functions.
Others who advanced scientific method during this era included Albertus Magnus (c. 1193 – 1280), Theodoric of Freiberg, (c. 1250 – c. 1310), William of Ockham (c. 1285 – c. 1350), and Jean Buridan (c. 1300 – c. 1358). These were not only scientists but leaders of the church – Christian archbishops, friars and priests.
By the late 15th century, the physician-scholar Niccolò Leoniceno was finding errors in Pliny's "Natural History". As a physician, Leoniceno was concerned about these botanical errors propagating to the materia medica on which medicines were based. To counter this, a botanical garden was established at Orto botanico di Padova, University of Padua (in use for teaching by 1546), in order that medical students might have empirical access to the plants of a pharmacopia. The philosopher and physician Francisco Sanches was led by his medical training at Rome, 1571–73, and by the philosophical skepticism recently placed in the European mainstream by the publication of Sextus Empiricus' "Outlines of Pyrrhonism", to search for a true method of knowing ("modus sciendi"), as nothing clear can be known by the methods of Aristotle and his followers – for example, syllogism fails upon circular reasoning. Following the physician Galen's "method of medicine", Sanches lists the methods of judgement and experience, which are faulty in the wrong hands, and we are left with the bleak statement "That Nothing is Known" (1581). This challenge was taken up by René Descartes in the next generation (1637), but at the least, Sanches warns us that we ought to refrain from the methods, summaries, and commentaries on Aristotle, if we seek scientific knowledge. In this, he is echoed by Francis Bacon, also influenced by skepticism; Sanches cites the humanist Juan Luis Vives who sought a better educational system, as well as a statement of human rights as a pathway for improvement of the lot of the poor.
The modern scientific method crystallized no later than in the 17th and 18th centuries. In his work "Novum Organum" (1620) – a reference to Aristotle's "Organon" – Francis Bacon outlined a new system of logic to improve upon the old philosophical process of syllogism. Then, in 1637, René Descartes established the framework for scientific method's guiding principles in his treatise, "Discourse on Method". The writings of Alhazen, Bacon and Descartes are considered critical in the historical development of the modern scientific method, as are those of John Stuart Mill.
In the late 19th century, Charles Sanders Peirce proposed a schema that would turn out to have considerable influence in the development of current scientific methodology generally. Peirce accelerated the progress on several fronts. Firstly, speaking in broader context in , Peirce outlined an objectively verifiable method to test the truth of putative knowledge on a way that goes beyond mere foundational alternatives, focusing upon both "deduction" and "induction". He thus placed induction and deduction in a complementary rather than competitive context (the latter of which had been the primary trend at least since David Hume, who wrote in the mid-to-late 18th century). Secondly, and of more direct importance to modern method, Peirce put forth the basic schema for hypothesis/testing that continues to prevail today. Extracting the theory of inquiry from its raw materials in classical logic, he refined it in parallel with the early development of symbolic logic to address the then-current problems in scientific reasoning. Peirce examined and articulated the three fundamental modes of reasoning that, as discussed above in this article, play a role in inquiry today, the processes that are currently known as abductive, deductive, and inductive inference. Thirdly, he played a major role in the progress of symbolic logic itself – indeed this was his primary specialty.
Beginning in the 1930s, Karl Popper argued that there is no such thing as inductive reasoning. All inferences ever made, including in science, are purely deductive according to this view. Accordingly, he claimed that the empirical character of science has nothing to do with induction – but with the deductive property of falsifiability that scientific hypotheses have. Contrasting his views with inductivism and positivism, he even denied the existence of the scientific method: "(1) There is no method of discovering a scientific theory (2) There is no method for ascertaining the truth of a scientific hypothesis, i.e., no method of verification; (3) There is no method for ascertaining whether a hypothesis is 'probable', or probably true". Instead, he held that there is only one universal method, a method not particular to science: The negative method of criticism, or colloquially termed trial and error. It covers not only all products of the human mind, including science, mathematics, philosophy, art and so on, but also the evolution of life. Following Peirce and others, Popper argued that science is fallible and has no authority. In contrast to empiricist-inductivist views, he welcomed metaphysics and philosophical discussion and even gave qualified support to myths and pseudosciences. Popper's view has become known as critical rationalism.
Although science in a broad sense existed before the modern era, and in many historical civilizations (as described above), modern science is so distinct in its approach and successful in its results that it now defines what science is in the strictest sense of the term.
Relationship with mathematics.
Science is the process of gathering, comparing, and evaluating proposed models against observables. A model can be a simulation, mathematical or chemical formula, or set of proposed steps. Science is like mathematics in that researchers in both disciplines can clearly distinguish what is "known" from what is "unknown" at each stage of discovery. Models, in both science and mathematics, need to be internally consistent and also ought to be "falsifiable" (capable of disproof). In mathematics, a statement need not yet be proven; at such a stage, that statement would be called a conjecture. But when a statement has attained mathematical proof, that statement gains a kind of immortality which is highly prized by mathematicians, and for which some mathematicians devote their lives.
Mathematical work and scientific work can inspire each other. For example, the technical concept of time arose in science, and timelessness was a hallmark of a mathematical topic. But today, the Poincaré conjecture has been proven using time as a mathematical concept in which objects can flow (see Ricci flow).
Nevertheless, the connection between mathematics and reality (and so science to the extent it describes reality) remains obscure. Eugene Wigner's paper, "The Unreasonable Effectiveness of Mathematics in the Natural Sciences", is a very well known account of the issue from a Nobel Prize-winning physicist. In fact, some observers (including some well known mathematicians such as Gregory Chaitin, and others such as Lakoff and Núñez) have suggested that mathematics is the result of practitioner bias and human limitation (including cultural ones), somewhat like the post-modernist view of science.
George Pólya's work on problem solving, the construction of mathematical proofs, and heuristic show that the mathematical method and the scientific method differ in detail, while nevertheless resembling each other in using iterative or recursive steps.
In Pólya's view, "understanding" involves restating unfamiliar definitions in your own words, resorting to geometrical figures, and questioning what we know and do not know already; "analysis", which Pólya takes from Pappus, involves free and heuristic construction of plausible arguments, working backward from the goal, and devising a plan for constructing the proof; "synthesis" is the strict Euclidean exposition of step-by-step details of the proof; "review" involves reconsidering and re-examining the result and the path taken to it.
Gauss, when asked how he came about his theorems, once replied "durch planmässiges Tattonieren" (through systematic palpable experimentation).
Imre Lakatos argued that mathematicians actually use contradiction, criticism and revision as principles for improving their work. In like manner to science, where truth is sought, but certainty is not found, in "Proofs and refutations" (1976), what Lakatos tried to establish was that no theorem of informal mathematics is final or perfect. This means that we should not think that a theorem is ultimately true, only that no counterexample has yet been found. Once a counterexample, i.e. an entity contradicting/not explained by the theorem is found, we adjust the theorem, possibly extending the domain of its validity. This is a continuous way our knowledge accumulates, through the logic and process of proofs and refutations. (If axioms are given for a branch of mathematics, however, Lakatos claimed that proofs from those axioms were tautological, i.e. logically true, by rewriting them, as did Poincaré ("Proofs and Refutations", 1976).)
Lakatos proposed an account of mathematical knowledge based on Polya's idea of heuristics. In "Proofs and Refutations", Lakatos gave several basic rules for finding proofs and counterexamples to conjectures. He thought that mathematical 'thought experiments' are a valid way to discover mathematical conjectures and proofs.
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="26893" url="http://en.wikipedia.org/wiki?curid=26893" title="Telecommunications in Sweden">
Telecommunications in Sweden

This article covers telecommunications in Sweden.
Telecommunications.
Sweden liberalized its telecommunications industry starting in 1980s and being formally liberalized in 1993. This was three years ahead of USA and five years before the European common policy introduced in January 1998 allowed for an open and competitive telecommunication market. The Swedes, most of who are computer literate, enjoy a continuous growth in the Internet market and the availability of technologies such as Metro Ethernet, fiber, satellite, WAN access technologies and even the availability of 3G services. Statistically, 6.447 (2004) million telephone main lines are in use, 8.0436 (2005) million mobile cellular telephones are in use and 6.7 million Swedes are regular internet users.
This abundance of telecommunication technology is a result of promoting a competitive industry that was made possible by deregulation. Since Sweden was the first to take on this arduous task the government had to come up with “a regulatory framework of its own”. The processes that went about resulting in the liberalization of the telecommunications’ industry can be structured into three phases: “Phase 1 of monopoly to Phase 2 with a mix of monopoly and competition to a “mature” Phase 3 with extensive competition”.
During the period of 1993-2000 there is rise in competition with legislation of the regulatory body being changed several times. In the case of the POTS, Telia in 2000 still held monopoly in the fixed-line access market. Whereas, mobile phone and Internet penetration in the household market ended up being one of the highest in the world with more than 50 percent of the revenue coming from these two industries. There were three major organizations providing GSM services and 120 internet service providers. One of the major causes that lead competitions thrive in areas that did not have a history of monopoly was the light handed approach taken towards the interconnection issue by the regulatory body initially. Telia held very high interconnection charges, making it very difficult for new entrants to enter. But what it did do was push the new entrants to enter other markets. Tele2 did just that by taking out a massive marketing campaign to attract a huge number of customers to its internet access service. This campaign was successful enough to bring back Telia to the negotiation table over the interconnection issue . This process eventually lead to the abolition of the light handed regulatory approach towards interconnection and put more power in the hands of the regulatory body. The intensity of regulation kept increasing around 1999 in areas other than POTS, especially the mobile market.
Signals intelligence.
In 2009, the Riksdag passed new legislation regulating the National Defence Radio Establishment (FRA), enabling them to collect information from both wireless and cable bound signals passing the Swedish border. Since most communications in Sweden pass through its borders at one point or another, this monitoring in practice affects most traffic within Sweden as well.

</doc>
<doc id="27014" url="http://en.wikipedia.org/wiki?curid=27014" title="Svenska Dagbladet">
Svenska Dagbladet

Svenska Dagbladet (common abbreviation "SvD"; the title translates as "the Swedish daily paper") is a daily newspaper published in Stockholm, Sweden. 
History and profile.
The first issue of "Svenska Dagbladet" appeared on 18 December 1884. Ivar Anderson is among its former editors-in-chief who assumed the post in 1940.
The paper is published in Stockholm and provides coverage of national and international news as well as local coverage of the Greater Stockholm region. Its subscribers are concentrated in the capital, but it is distributed in most of Sweden. During the beginning of the 1900s the paper was one of the right-wing publications in Stockholm.
"Svenska Dagbladet" is owned by Schibsted which purchased it in the late 1990s. The stated position of the editorial page is "independently moderate" ("oberoende moderat"), which means it is independent but adheres to the liberal conservatism of the Moderate Party. On the other hand, the paper is also regarded as conservative.
In November 2000 "Svenska Dagbladet" changed its format from broadsheet to tabloid. In 2005 the paper started a Web portal for business news as a joint venture with "Aftonbladet".
Since 1925 "Svenska Dagbladet" has awarded an individual sportsperson or a team the Svenska Dagbladet Gold Medal at the end of each year.
Circulation.
The circulation of "Svenska Dagbladet" was 185,000 copies in 2003. The paper had a circulation of 187,100 copies on weekdays in 2005. Among Swedish morning newspapers "Svenska Dagbladet" had the third largest circulation with 195,200 copies in 2007 after "Dagens Nyheter" and "Göteborgs-Posten". In 2008 "Svenska Dagbladet" had a circulation of 123,383 copies. The circulation of the paper was 185,600 copies in 2011. It was 159,600 copies in 2012 and 143,400 copies in 2013.

</doc>
<doc id="27016" url="http://en.wikipedia.org/wiki?curid=27016" title="Sture Allén">
Sture Allén

Sture Allén (born 31 December 1928 in Gothenburg) is a Swedish retired professor of computational linguistics at the University of Gothenburg, who was the permanent secretary of the Swedish Academy between 1986 and 1999. He was elected to chair 3 of the Swedish Academy in 1980. He is a member of the Norwegian Academy of Science and Letters.

</doc>
<doc id="27019" url="http://en.wikipedia.org/wiki?curid=27019" title="South Korea">
South Korea

</table>
South Korea ( ), officially the Republic of Korea (Hangul: 대한민국; Hanja: 大韓民國; "Daehan Minguk"  , "The Republic of Great "Han""; ROK), and commonly referred to as Korea, is a sovereign state in East Asia, constituting the southern part of the Korean Peninsula. The name "Korea" is derived from the Kingdom of Goryeo, also spelled as "Koryŏ". It shares land borders with North Korea to the north, and oversea borders with Japan to the east and China to the west. Roughly half of the country's 50 million people reside in the metropolitan area surrounding its capital, the Seoul Capital Area, which is the second largest in the world with over 25 million residents.
Korea was inhabited as early as the Lower Paleolithic period and its civilization began with the founding of Gojoseon. After the unification of the Three Kingdoms of Korea in 668, Korea enjoyed over a millennium of relative tranquility under dynasties lasting for centuries in which its trade, culture, literature, science and technology flourished. In 1910 it was annexed by the Japanese Empire, after whose surrender in 1945, Korea was divided into Soviet and U.S. zones of occupation, with the latter becoming the Republic of Korea in August 1948. Although the United Nations passed a resolution declaring the Republic to be the only lawful government of Korea, a communist regime was soon set up in the North that invaded the South in 1950, leading to the Korean War that ended in 1953 with an armistice, with peace and prosperity settling-in thereafter.
Between 1962 and 1994, South Korea's tiger economy soared at an average of 10% annually, fueled by annual export growth of 20%, in a period called the Miracle on the Han River that rapidly and successfully transformed it into a high-income advanced economy and the world's 11th largest economy by 1995. Today, South Korea is the world's fifth largest exporter and seventh largest importer, a regional power with the world's 10th largest defence budget and member of the G-20 and OECD's Development Assistance Committee. Since the first free election in 1987, South Koreans have enjoyed high civil liberties and one of the world's most developed democracies, ranked second in Asia on the Democracy Index. Its pop culture has considerable influence in Asia and expanding globally in a process called the Korean Wave.
South Korea is East Asia's highest ranked developed country in the Human Development Index. Its citizens enjoy a very high standard of living, having Asia's highest median per-capita income and average wage with the world's 8th highest household income. Globally, it ranks among the highest in education, quality of healthcare and ease of doing business. It benefits from a highly skilled workforce, leading OECD countries in student skills with the highest percentage of youths holding a tertiary education degree. Ranked as the world's most innovative country in the Bloomberg Innovation Index, it is the world's most research and development intensive country, driven by high-tech chaebols such as Samsung, Hyundai-Kia and LG. A world leading information society, South Korea has the world's fastest Internet connection speed, ranking first in e-Government, 4G LTE penetration and second in the ICT Development Index and smartphone usage.
Etymology.
The name "Korea" derives from Goryeo, itself referring to the ancient kingdom of Goguryeo, the first Korean dynasty visited by Persian merchants who referred to Koryŏ (Goryeo; 고려) as Korea. The term "Koryŏ" also widely became used to refer to Goguryeo, which renamed itself "Koryŏ" in the 5th century. (The modern spelling, "Korea", first appeared in late 17th century in the travel writings of the Dutch East India Company's Hendrick Hamel.). Despite the coexistence of the spellings "Corea" and "Korea" in 19th century publications, some Koreans believe that Japan, around the time of the Japanese occupation, intentionally standardised the spelling on "Korea", making Japan appear first alphabetically. Other commentators have pointed out that Japan continued to refer to Korea as "Corea" and "Chosen," even after Japan absorbed Korea, and that Japan would have had no need to concern itself with Korea's alphabetical position in international forums, considering that Japan had absorbed Korea, and thus Korea ceased to appear as an independent entity in international forums.<ref name=http://www.monster-island.net></ref>
After Goryeo fell in 1392, "Joseon" became the official name for the entire territory, though it was not universally accepted. The new official name has its origin in the ancient country of Gojoseon (Old Joseon). In 1897, the Joseon dynasty changed the official name of the country from "Joseon" to "Daehan Jeguk" (Korean Empire). The name "Daehan", which means "great Han" literally, derives from Samhan (Three Hans). However, the name "Joseon" was still widely used by Koreans to refer to their country, though it was no longer the official name. Under Japanese rule, the two names "Han" and "Joseon" coexisted. There were several groups who fought for independence, the most notable being the "Provisional Government of the Republic of Korea" (대한민국 임시정부).
Following the surrender of Japan, in 1945, the "Republic of Korea" ("Daehan Minguk") was adopted as the legal name for the new country. Since the government only controlled the southern part of the Korean Peninsula, the informal term "South Korea" was coined, becoming increasingly common in the Western world. While South Koreans use "Han" (or "Hanguk") to refer to the entire country, North Koreans use "Joseon" as the name of the country.
History.
Before the division.
Korean history begins with the founding of Joseon (often known as "Gojoseon" to prevent confusion with another dynasty founded in the 13th century; the prefix Go- means 'older,' 'before,' or 'earlier') in 2333 BC by Dangun, according to Korean foundation mythology. Gojoseon expanded until it controlled northern Korean Peninsula and some parts of Manchuria. The Gija Joseon was purportedly founded in 12th century BC, and its existence and role have been controversial in the modern era. In the 2nd century BC, Wiman Joseon which fell to the Han China near the end of the century. Later the Han Dynasty defeated the Wiman Joseon and set up Four Commanderies of Han in 108 BC. There was a significant Chinese presence in northern parts of the Korean peninsula during the next century, and the Lelang Commandery persisted for about 400 years until it was conquered by Goguryeo. After many conflicts with the Chinese Han Dynasty, Gojoseon disintegrated, leading to the Proto–Three Kingdoms of Korea period.
In the early centuries of the Common Era, Buyeo, Okjeo, Dongye, and the Samhan confederacy occupied the peninsula and southern Manchuria. Of the various states, Goguryeo, Baekje, and Silla grew to control the peninsula as Three Kingdoms of Korea. The unification of the Three Kingdoms by Silla in 676 led to the North South States Period, in which much of the Korean Peninsula was controlled by Unified Silla, while Balhae succeeded to have the control of northern parts of Goguryeo.
In Unified Silla, poetry and art was encouraged, and Buddhist culture thrived. Relationships between Korea and China remained relatively peaceful during this time. However, Unified Silla weakened under internal strife, and surrendered to Goryeo in 935. Balhae, Silla's neighbor to the north, was formed as a successor state to Goguryeo. During its height, Balhae controlled most of Manchuria and parts of Russian Far East. It fell to the Khitan in 926.
The peninsula was united by King Taejo of Goryeo in 936. Like Silla, Goryeo was a highly cultural state and created the Jikji in 1377, using the world's oldest movable metal type printing press. The Mongol invasions in the 13th century greatly weakened Goryeo. After nearly 30 years of war, Goryeo continued to rule Korea, though as a tributary ally to the Mongols. After the Mongolian Empire collapsed, severe political strife followed and the Goryeo Dynasty was replaced by the Joseon Dynasty in 1392, following a rebellion by General Yi Seong-gye.
King Taejo declared the new name of Korea as "Joseon" in reference to Gojoseon, and moved the capital to Hanseong (old name of Seoul). The first 200 years of the Joseon Dynasty were marked by relative peace and saw the creation of Hangul by King Sejong the Great in the 15th century and the rise in influence of Confucianism in the country.
Between 1592 and 1598, Japan invaded Korea. Toyotomi Hideyoshi led the Japanese forces, but his advance was halted by Korean forces with assistance from Righteous army militias and Ming Dynasty Chinese troops. Through a series of successful battles of attrition, the Japanese forces were eventually forced to withdraw, and subsequently signed a peace agreement with diplomats of Ming China. This war also saw the rise of Admiral Yi Sun-sin and his renowned "turtle ship". In the 1620s and 1630s, Joseon suffered from invasions by the Manchu which eventually extended to China as well.
After another series of wars against Manchuria, Joseon experienced a nearly 200-year period of peace. King Yeongjo and King Jeongjo particularly led a new renaissance of the Joseon Dynasty.
However, the latter years of the Joseon Dynasty were marked by a dependence on China for external affairs and isolation from the outside world. During the 19th century, Korea's isolationist policy earned it the name the "Hermit Kingdom". The Joseon Dynasty tried to protect itself against Western imperialism, but was eventually forced to open trade. After the First Sino-Japanese War and the Russo-Japanese War, Korea was occupied by Japan (1910–45). At the end of World War II, the Japanese surrendered to Soviet and U.S. forces who occupied the northern and southern halves of Korea, respectively.
After the division.
Despite the initial plan of a unified Korea in the 1943 Cairo Declaration, escalating Cold War antagonism between the Soviet Union and the United States eventually led to the establishment of separate governments, each with its own ideology, leading to Korea's division into two political entities in 1948: North Korea and South Korea.
In the South Syngman Rhee, an opponent of communism, who had been backed and appointed by the United States as head of the provisional government, won the first presidential elections of the newly declared Republic of Korea in May. In the North, a former anti-Japanese guerrilla and communist activist, Kim Il-sung was appointed premier of the Democratic People's Republic of Korea in September.
In October the Soviet Union declared Kim Il-sung's government as sovereign over both parts. The UN declared Rhee's government as "a lawful government having effective control and jurisdiction over that part of Korea where the UN Temporary Commission on Korea was able to observe and consult" and the Government "based on elections which was observed by the Temporary Commission" in addition to a statement that "this is the only such government in Korea."
Both leaders began an authoritarian repression of their political opponents inside their region, seeking for a unification of Korea under their control. While South Korea's request for military support was denied by the United States, North Korea's military was heavily reinforced by the Soviet Union.
On June 25, 1950, North Korea invaded South Korea, sparking the Korean War, the Cold War's first major conflict that continued until 1953. At the time, the Soviet Union had boycotted the United Nations (UN), thus forfeiting their veto rights. This allowed the UN to intervene in a civil war when it became apparent that the superior North Korean forces would unify the entire country. The Soviet Union and China backed North Korea, with the later participation of millions of Chinese troops. After an ebb and flow that saw both sides almost pushed to the brink of extinction, and massive losses among Korean civilians in both the north and the south, the war eventually reached a stalemate. The 1953 armistice, never signed by South Korea, split the peninsula along the demilitarized zone near the original demarcation line. No peace treaty was ever signed, resulting in the two countries remaining technically at war. Over 1.2 million people died during the Korean war.
In 1960, a student uprising (the "April 19 Revolution") led to the resignation of the autocratic, corrupt President Syngman Rhee. A period of political instability followed, broken by General Park Chung-hee's May 16 coup against the weak and ineffectual government the next year. Park took over as president until his assassination in 1979, overseeing rapid export-led economic growth as well as implementing political repression. Park was heavily criticised as a ruthless military dictator, who in 1972 extended his rule by creating a new constitution, which gave the president sweeping (almost dictatorial) powers and permitted him to run for an unlimited number of six-year terms. However the Korean economy developed significantly during Park's tenure and the government developed the nationwide expressway system, the Seoul subway system, and laid the foundation for economic development during his 17-year tenure.
The years after Park's assassination were marked again by political turmoil, as the previously suppressed opposition leaders all campaigned to run for president in the sudden political void. In 1979 there came the Coup d'état of December Twelfth led by General Chun Doo-hwan. Following the Coup d'état, Chun Doo-hwan planned to rise to power through several measures. On May 17, Chun Doo-hwan forced the Cabinet to expand martial law to the whole nation, which had previously not applied to the island of Jeju-do. The expanded martial law closed universities, banned political activities and further curtailed the press. Chun's assumption of the presidency in the events of May 17, triggered nationwide protests demanding democracy, in particular in the city of Gwangju, to which Chun sent special forces to violently suppress the Gwangju Democratization Movement.
Chun subsequently created the National Defense Emergency Policy Committee and took the presidency according to his political plan. Chun and his government held South Korea under a despotic rule until 1987, when a Seoul National University student, Park Jong-chul, was tortured to death. On June 10, the Catholic Priests Association for Justice revealed the incident, igniting the June Democracy Movement around the country. Eventually, Chun's party, the Democratic Justice Party, and its leader, Roh Tae-woo announced the 6.29 Declaration, which included the direct election of the president. Roh went on to win the election by a narrow margin against the two main opposition leaders, Kim Dae-Jung and Kim Young-Sam.
In 1988, Seoul hosted the 1988 Summer Olympics. It became a member of the Organization for Economic Co-operation and Development (OECD) in 1996. It was adversely affected by the 1997 Asian Financial Crisis. However, the country recovered and continue its economic growth, albeit at a slower pace.
In June 2000, as part of president Kim Dae-jung's "Sunshine Policy" of engagement, a North–South summit took place in Pyongyang, the capital of North Korea. Later that year, Kim received the Nobel Peace Prize "for his work for democracy and human rights in South Korea and in East Asia in general, and for peace and reconciliation with North Korea in particular." However, because of discontent among the population for fruitless approaches to the North under the previous administrations and, amid North Korean provocations, a conservative government was elected in 2007 led by President Lee Myung-bak, former mayor of Seoul. More recently, Park Geun-hye won the South Korean presidential election, 2012.
In 2002, South Korea and Japan jointly co-hosted the 2002 FIFA World Cup. However, South Korean and Japanese relations later soured because of conflicting claims of sovereignty over the Liancourt Rocks ("Dokdo" in Korea), in what became known as the Liancourt Rocks dispute.
Geography, climate and environment.
Geography.
South Korea occupies the southern portion of the Korean Peninsula, which extends some 1,100 km from the Asian mainland. This mountainous peninsula is flanked by the Yellow Sea to the west, and Sea of Japan (East Sea) to the east. Its southern tip lies on the Korea Strait and the East China Sea.
The country, including all its islands, lies between latitudes 33° and 39°N, and longitudes 124° and 130°E. Its total area is 100,032 km2.
South Korea can be divided into four general regions: an eastern region of high mountain ranges and narrow coastal plains; a western region of broad coastal plains, river basins, and rolling hills; a southwestern region of mountains and valleys; and a southeastern region dominated by the broad basin of the Nakdong River.
South Korea's terrain is mostly mountainous, most of which is not arable. Lowlands, located primarily in the west and southeast, make up only 30% of the total land area.
About three thousand islands, mostly small and uninhabited, lie off the western and southern coasts of South Korea. Jeju-do is about 100 kilometres (about 60 mi) off the southern coast of South Korea. It is the country's largest island, with an area of 1,845 km2. Jeju is also the site of South Korea's highest point: Hallasan, an extinct volcano, reaches 1,950 meters (6,398 ft) above sea level. The easternmost islands of South Korea include Ulleungdo and Liancourt Rocks (Dokdo), while Marado and Socotra Rock are the southernmost islands of South Korea.
South Korea has 20 national parks and popular nature places like the Boseong Tea Fields, Suncheon Bay Ecological Park, and the first national park of Jirisan.
Climate.
South Korea tends to have a humid continental climate and a humid subtropical climate, and is affected by the East Asian monsoon, with precipitation heavier in summer during a short rainy season called "jangma" (), which begins end of June through the end of July. Winters can be extremely cold with the minimum temperature dropping below -20 °C in the inland region of the country: in Seoul, the average January temperature range is -7 to, and the average August temperature range is 22 to. Winter temperatures are higher along the southern coast and considerably lower in the mountainous interior. Summer can be uncomfortably hot and humid, with temperatures exceeding 30 °C in most parts of the country.
South Korea has four distinct seasons; spring, summer, autumn and winter. Spring usually lasts from late-March to early- May, summer from mid-May to early-September, autumn from mid-September to early-November, and winter from mid-November to mid-March.
Rainfall is concentrated in the summer months of June through September. The southern coast is subject to late summer typhoons that bring strong winds and heavy rains. The average annual precipitation varies from 1370 mm in Seoul to 1470 mm in Busan. There are occasional typhoons that bring high winds and floods.
Environment.
During the first 20 years of South Korea's growth surge, little effort was made to preserve the environment. Unchecked industrialization and urban development have resulted in deforestation and the ongoing destruction of wetlands such as the Songdo Tidal Flat. However, there have been recent efforts to balance these problems, including a government run $84 billion five-year green growth project that aims to boost energy efficiency and green technology.
The green-based economic strategy is a comprehensive overhaul of South Korea's economy, utilizing nearly two percent of the national GDP. The greening initiative includes such efforts as a nationwide bike network, solar and wind energy, lowering oil dependent vehicles, backing daylight savings and extensive usage of environmentally friendly technologies such as LEDs in electronics and lighting. The country – already the world's most wired – plans to build a nationwide next-generation network which will be 10 times faster than broadband facilities in order to reduce energy usage.
The renewable portfolio standard program with renewable energy certificates runs from 2012 to 2022.
Quota systems favor large, vertically integrated generators and multinational electric utilities, if only because certificates are generally denominated in units of one megawatt-hour. They are also more difficult to design and implement than an a Feed-in tariff. Around 350 residential micro combined heat and power units were installed in 2012.
Seoul's tap water recently became safe to drink, with city officials branding it "Arisu" in a bid to convince the public. Efforts have also been made with afforestation projects. Another multi-billion dollar project was the restoration of Cheonggyecheon, a stream running through downtown Seoul that had earlier been paved over by a motorway.
One major challenge is air quality, with acid rain, sulfur oxides, and annual yellow dust storms being particular problems. It is acknowledged that many of these difficulties are a result of South Korea's proximity to China, which is a major air polluter.
South Korea is a member of the Antarctic-Environmental Protocol, Antarctic Treaty, Biodiversity Treaty, Kyoto Protocol (forming the Environmental Integrity Group (EIG), regarding UNFCCC, with Mexico and Switzerland), Desertification, Endangered Species, Environmental Modification, Hazardous Wastes, Law of the Sea, Marine Dumping, Comprehensive Nuclear-Test-Ban Treaty (not into force), Ozone Layer Protection, Ship Pollution, Tropical Timber 83, Tropical Timber 94, Wetlands, and Whaling.
Government.
Under its current constitution the state is sometimes referred to as the Sixth Republic of South Korea. Like many democratic states, South Korea has a government divided into three branches: executive, judicial, and legislative. The executive and legislative branches operate primarily at the national level, although various ministries in the executive branch also carry out local functions. Local governments are semi-autonomous, and contain executive and legislative bodies of their own. The judicial branch operates at both the national and local levels. South Korea is a constitutional democracy.
The South Korean government's structure is determined by the Constitution of the Republic of Korea. This document has been revised several times since its first promulgation in 1948 at independence. However, it has retained many broad characteristics and with the exception of the short-lived Second Republic of South Korea, the country has always had a presidential system with an independent chief executive. The first direct election was also held in 1948. Although South Korea experienced a series of military dictatorships from the 1960s up until the 1980s, it has since developed into a successful liberal democracy. Today, the CIA World Factbook describes South Korea's democracy as a "fully functioning modern democracy".
Administrative divisions.
The major administrative divisions in South Korea are provinces, metropolitan cities (self-governing cities that are not part of any province), one special city and one special autonomous city.
a Revised Romanisation; b See Names of Seoul; c As of 2013 year-end.
Demographics.
South Korea is noted for its population density, which is 487 per square kilometer, more than 10 times the global average. Most South Koreans live in urban areas, because of rapid migration from the countryside during the country's quick economic expansion in the 1970s, 1980s and 1990s. The capital city of Seoul is also the country's largest city and chief industrial center. According to the 2005 census, Seoul had a population of 9.8 million inhabitants. The Seoul National Capital Area has 24.5 million inhabitants (about half of South Korea's entire population) making it the world's second largest metropolitan area and easily the most densely populated city in the OECD. Other major cities include Busan (3.5 million), Incheon (2.5 million), Daegu (2.5 million), Daejeon (1.4 million), Gwangju (1.4 million) and Ulsan (1.1 million).
The population has also been shaped by international migration. After World War II and the division of the Korean Peninsula, about four million people from North Korea crossed the border to South Korea. This trend of net entry reversed over the next 40 years because of emigration, especially to the United States and Canada. South Korea's total population in 1955 was 21.5 million, and today it is roughly 50,062,000.
South Korea is one of the most ethnically homogeneous societies in the world, with more than 99% of inhabitants having Korean ethnicity. Koreans call their society 단일민족국가, "Dan-il minjok guk ga", "the single race society".
The percentage of foreign nationals has been growing rapidly. s of 2009[ [update]], South Korea had 1,106,884 foreign residents, 2.7% of the population; however, more than half of them are ethnic Koreans with a foreign citizenship. For example, migrants from China (PRC) make up 56.5% of foreign nationals, but approximately 30% of the Chinese citizens in Korea are "Joseonjok (조선족 in Korean)", PRC citizens of Korean ethnicity.
Regardless of the ethnicity, there are 28,500 US military personnel serving in South Korea, most serving a one-year unaccompanied tour (though approximately 10% serve longer tours accompanied by family), according to the Korea National Statistical Office. In addition, about 43,000 English teachers from English-speaking countries reside temporarily in Korea.
Currently, South Korea has one of the highest rate of growth of foreign born population, with about 30,000 foreign born residences obtaining South Korean citizenship every year since 2010.
South Korea's birthrate was the world's lowest in 2009. If this continues, its population is expected to decrease by 13% to 42.3 million in 2050. South Korea's annual birthrate is approximately 9 births per 1000 people. However, the birthrate has increased by 5.7% in 2010 and Korea no longer has the world's lowest birthrate. According to a 2011 report from Chosun Ilbo, South Korea's total fertility rate (1.23 children born per woman) is higher than those of Taiwan (1.15) and Japan (1.21). The average life expectancy in 2008 was 79.10 years, which is 34th in the world.
South Korea has the steepest decline in working age population of the OECD nations.
Education.
Education in South Korea is highly valued in the national culture and is regarded as crucial to socioeconomic success in South Korean society. Competition is consequently fierce, with many participating in intense outside tutoring to supplement classes to gain a competitive edge over their peers. In 2012, South Korea scored fifth in reading and mathematics and seventh in science on the PISA 2012 Tests. South Korea's education system is technologically advanced and it is the world's first country to bring high-speed fibre-optic broadband internet access to every primary and secondary school nationwide. Using this infrastructure, the country has developed the first digital textbooks in the world, which will be distributed for free to every primary and secondary school nationwide by 2013.
The South Korean education system has been praised for various reasons, including its comparatively high results and its major role in bringing Korea's economic development. Many international political figures such as the U.S. President Barack Obama have praised South Korea for its rigorous education system, where more than 85 percent of South Korean high school graduates go on to college thus establishing a highly motivated and educated populace. In addition, 65 percent of South Koreans aged 25–34 hold at least a bachelor's degree, the most in the OECD, while the global OECD average is 39 percent.
A centralized administration in South Korea oversees the process for the education of children from kindergarten to the third and final year of high school. South Korea has adopted a new educational program to increase the number of their foreign students through 2010. According to Ministry of Education, Science and Technology estimate, by that time, the number of scholarships for foreign students in South Korea will be doubled, and the number of foreign students will reach 100,000.
The school year is divided into two semesters, the first of which begins in the beginning of March and ends in mid-July, the second of which begins in late August and ends in mid-February. The schedules are not uniformly standardized and vary from school to school.
Most South Korean middle schools and high schools have school uniforms, modeled on western-style uniforms. Boys' uniforms usually consists of trousers and white shirts, and girls wear skirts and white shirts (this only applies in middle schools and high schools).
Religion.
As of 2005, just under half of the South Korean population expressed no religious preference. Of the rest, most are Buddhist or Christian. According to the 2007 census, 29.2% of the population at that time was Christian (18.3% identified themselves as Protestants, 10.9% as Roman Catholics), and 22.8% were Buddhist. Other religions include Islam and various new religious movements such as Jeungsanism, Cheondoism and Wonbuddhism. The earliest religion practiced was Korean shamanism. Today, freedom of religion is guaranteed by the constitution, and there is no state religion. Ironically, there are 628 Jehovah's Witnesses currently imprisoned in South Korea with the average of 40 newly imprisoned every month. The UN Human Rights Committee has condemned the government of South Korea for the arbitrary detention of conscientious objectors and for violating the right of Witnesses to freedom of conscience.
Christianity is South Korea's largest religion, accounting for more than half of all South Korean religious adherents. There are approximately 13.7 million Christians in South Korea today; about 63% of Korean Christians belong to Protestant churches, while 37% belong to the Roman Catholic Church. The number of Protestant Christians has slightly decreased since the 1990s, while the number of Roman Catholics has rapidly increased since the 1980s. Presbyterian denominations are the biggest Christian denominations in South Korea. About nine million people belong to one of the 100 different Presbyterian churches; among the biggest denominations are the HapDong Presbyterian Church, TongHap Presbyterian Church, the Koshin Presbyterian Church. For more information see Presbyterianism in South Korea. South Korea is also the second-largest missionary-sending nation, after the United States.
Buddhism was introduced to Korea in the year 372. According to the national census as of 2005, South Korea has over 10.7 million Buddhists. Today, about 90% of Korean Buddhists belong to Jogye Order. Most of the National Treasures of South Korea are Buddhist artifacts. Buddhism became the state religion in some of Korean kingdoms since the Three Kingdoms Period, when Goguryeo adopted it as the state religion in 372, followed by Baekche (528). Buddhism had been the state religion of Unified Korea from North South States Period (not to be confused with the modern division of Korea) to Goryeo before suppression under the Joseon dynasty in favor of Neo-Confucianism.
Fewer than 30,000 South Koreans are thought to be Muslims, but the country has some 100,000 resident foreign workers from Islamic countries, Bangladesh and Pakistan.
Korean shamanism, today known as Muism (religion of the mu [shamans]) or Sinism (religion of the gods) encompasses a variety of indigenous religious beliefs and practices of the Korean people and the Korean sphere. In contemporary South Korea, the most used term is "Muism" and a shaman is known as a "mudang" (무당, 巫堂) or Tangol (당골). Since the early 2000s, this religion has regained popularity among Koreans.
Public health and safety.
Although life expectancy has increased significantly since 1950, South Korea faces a number of important health-care issues. Foremost is the impact of environmental pollution on an increasingly urbanized population. According to the Ministry of Health and Welfare, chronic diseases account for the majority of diseases in South Korea, a condition exacerbated by the health care system's focus on treatment rather than prevention. The incidence of chronic disease in South Korea hovers around 24 percent. Approximately 33 percent of all adults smoke. The human immunodeficiency virus (HIV) rate of prevalence at the end of 2003 was less than 0.1 percent. In 2001 central government expenditures on health care accounted for about 6 percent of gross domestic product (GDP). The suicide rate in the nation was 26 per 100,000 in 2008, the highest in the industrialized world.
Young South Korean males were found to be the tallest in all of East Asia, resulting from healthy living conditions, economic development and changes in food culture.
Based on the Asia-Pacific Advisory Committee on Influenza (APACI), South Korea ranked the highest of influenza vaccination in Asia with 311 vaccines per 1,000 people.
Foreign relations.
South Korea maintains diplomatic relations with more than 188 countries. The country has also been a member of the United Nations since 1991, when it became a member state at the same time as North Korea. On January 1, 2007, South Korean Foreign Minister Ban Ki-moon assumed the post of UN Secretary-General. It has also developed links with the Association of Southeast Asian Nations as both a member of "ASEAN Plus three," a body of observers, and the East Asia Summit (EAS).
In 2010, South Korea and the European Union concluded a free trade agreement (FTA) to reduce trade barriers. South Korea is also negotiating a Free Trade Agreement with Canada, and another with New Zealand. In November 2009 South Korea joined the OECD Development Assistance Committee, marking the first time a former aid recipient country joined the group as a donor member. South Korea hosted the G-20 Summit in Seoul in November 2010.
China.
Historically, Korea has had close relations with China. Before the formation of South Korea, Korean independence fighters worked with Chinese soldiers during the Japanese occupation. However, after World War II, the People's Republic of China embraced Maoism while South Korea sought close relations with the United States. The PRC assisted North Korea with manpower and supplies during the Korean War, and in its aftermath the diplomatic relationship between South Korea and the PRC almost completely ceased. Relations thawed gradually and South Korea and the PRC re-established formal diplomatic relations on August 24, 1992. The two countries sought to improve bilateral relations and lifted the forty-year-old trade embargo, and South Korean–Chinese relations have improved steadily since 1992. The Republic of Korea broke off official relations with the Republic of China (Taiwan) upon gaining official relations with the People's Republic of China, which doesn't recognise Taiwan's sovereignty.
European Union.
The European Union (EU) and South Korea are important trading partners, having negotiated a free trade agreement for many years since South Korea was designated as a priority FTA partner in 2006. The free trade agreement was approved in September 2010, and took effect on July 1, 2011. South Korea is the EU's eighth largest trade partner, and the EU has become South Korea's second largest export destination. EU trade with South Korea exceeded €65 billion in 2008 and has enjoyed an annual average growth rate of 7.5% between 2004 and 2008.
The EU has been the single largest foreign investor in South Korea since 1962, and accounted for almost 45% of all FDI inflows into Korea in 2006. Nevertheless, EU companies have significant problems accessing and operating in the South Korean market because of stringent standards and testing requirements for products and services often creating barriers to trade. Both in its regular bilateral contacts with South Korea and through its FTA with Korea, the EU is seeking to improve this situation.
Japan.
Although there were no formal diplomatic ties between South Korea and Japan after the end of World War II, South Korea and Japan signed the Treaty on Basic Relations between Japan and the Republic of Korea in 1965 to establish diplomatic ties. There is heavy anti-Japanese sentiment in South Korea because of a number of unsettled Japanese-Korean disputes, many of which stem from the period of Japanese occupation after the Japanese annexation of Korea. During World War II, more than 100,000 Koreans served in the Imperial Japanese Army. Korean women were forced to the war front to serve the Imperial Japanese Army as sexual slaves, called comfort women.
Longstanding issues such as Japanese war crimes against Korean civilians, visits by Japanese politicians to the Yasukuni Shrine honoring Japanese soldiers killed at war (including some class A war criminals), the negationist re-writing of Japanese textbooks relating Japanese atrocities during World War II, and the territorial disputes over Liancourt Rocks, known in South Korea as "Dokdo", continue to trouble Korean-Japanese relations. Although Dokdo is claimed by both South Korea and Japan, the islets are administered by South Korea, which had its coast guard stationed there.
In response to then-Prime Minister Junichiro Koizumi's repeated visits to the Yasukuni Shrine, former President Roh Moo-hyun suspended all summit talks between South Korea and Japan in 2009.
North Korea.
Both North and South Korea continue to officially claim sovereignty over the entire peninsula and any outlying islands. Despite the animosity, reconciliation efforts have been present from the very beginning of the separation between North and South Korea. Political figures such as Kim Koo worked to reconcile the two governments even after the Korean War. With longstanding animosity following the Korean War from 1950 to 1953, North Korea and South Korea signed an agreement to pursue peace. On October 4, 2007, Roh Moo-Hyun and North Korean leader Kim Jong-il signed an eight-point agreement on issues of permanent peace, high-level talks, economic cooperation, renewal of train services, highway and air travel, and a joint Olympic cheering squad.
Despite the Sunshine Policy and efforts at reconciliation, the progress was complicated by North Korean missile tests in 1993, 1998, 2006, 2009, and 2013. As of early 2009[ [update]], relationships between North and South Korea were very tense; North Korea had been reported to have deployed missiles, ended its former agreements with South Korea, and threatened South Korea and the United States not to interfere with a satellite launch it had planned.
North and South Korea are still technically at war (having never signed a peace treaty after the Korean War) and share the world's most heavily fortified border. On May 27, 2009, North Korean media declared that the Armistice is no longer valid because of the South Korean government's pledge to "definitely join" the Proliferation Security Initiative. To further complicate and intensify strains between the two nations, the sinking of the South Korean warship Cheonan in March 2010, is affirmed by the South Korean government to have been caused by a North Korean torpedo, which the North denies. President Lee Myung-bak declared in May 2010 that Seoul would cut all trade with North Korea as part of measures primarily aimed at striking back at North Korea diplomatically and financially, except for the joint Kaesong Industrial Project, and humanitarian aid. North Korea initially threatened to sever all ties, to completely abrogate the previous pact of non-aggression, and to expel all South Koreans from a joint industrial zone in Kaesong, but backtracked on its threats and decided to continue its ties with South Korea. Despite the continuing ties, Kaesong industrial zone has seen a large decrease in investment and manpower as a result of this military conflict.
United States.
The United States engaged in the decolonization of Korea (mainly in the South, with the Soviet Union engaged in North Korea) from Japan after World War II. After three years of military administration by the United States, the South Korean government was established. Upon the onset of the Korean War, U.S. forces were sent to defend against an invasion from North Korea of the South. Following the Armistice, South Korea and the U.S. agreed to a "Mutual Defense Treaty", under which an attack on either party in the Pacific area would summon a response from both. In 1967, South Korea obliged the mutual defense treaty, by sending a large combat troop contingent to support the United States in the Vietnam War. The U.S. Eighth Army, Seventh Air Force, and U.S. Naval Forces Korea are stationed in South Korea. The two nations have strong economic, diplomatic, and military ties, although they have at times disagreed with regard to policies towards North Korea, and with regard to some of South Korea's industrial activities that involve usage of rocket or nuclear technology. There had also been strong anti-American sentiment during certain periods, which has largely moderated in the modern day. In 2007, a free trade agreement known as the Republic of Korea-United States Free Trade Agreement (KORUS FTA) was signed between South Korea and the United States, but its formal implementation was repeatedly delayed, pending approval by the legislative bodies of the two countries. On October 12, 2011, the U.S. Congress passed the long-stalled trade agreement with South Korea. It went into effect on March 15, 2012.
Military.
A long history of invasions by neighbors and the unresolved tension with North Korea have prompted South Korea to allocate 2.6% of its GDP and 15% of all government spending to its military (Government share of GDP: 14.967%), while maintaining compulsory conscription for men. Consequently, South Korea has the world's sixth largest number of active troops (650,000 in 2011), the world's second-largest number of reserve troops (3,200,000 in 2011) and the eleventh largest defense budget. The Republic of Korea, with both regular and reserve military force numbering 3.7 million regular personnel among a total national population of 50 million people, has the second highest number of soldiers per capita in the world, after the Democratic People's Republic of Korea.
The South Korean military consists of the Army (ROKA), the Navy (ROKN), the Air Force (ROKAF), and the Marine Corps (ROKMC), and reserve forces. Many of these forces are concentrated near the Korean Demilitarized Zone. All South Korean males are constitutionally required to serve in the military, typically 21 months. Previously, Koreans of mixed race were exempt from military duty but no exception from 2011.
In addition to male conscription in South Korea's sovereign military, 1,800 Korean males are selected every year to serve 21 months in the KATUSA Program to further augment the United States Forces Korea (USFK). In 2010, South Korea was spending ₩1.68 trillion in a cost-sharing agreement with the US to provide budgetary support to the US forces in Korea, on top of the ₩29.6 trillion budget for its own military.
The South Korean army has 2,500 tanks in operation, including the K1A1 and K2 Black Panther, which form the backbone of the South Korean army's mechanized armor and infantry forces. A sizable arsenal of many artillery systems, including 1,700 self-propelled K55 and K9 Thunder howitzers and 680 helicopters and UAVs of numerous types, are assembled to provide additional fire, reconnaissance, and logistics support. South Korea's smaller but more advanced artillery force and wide range of airborne reconnaissance platforms are pivotal in the counter-battery suppression of North Korea's large artillery force, which operates more than 13,000 artillery systems deployed in various state of fortification and mobility.
The South Korean navy has made its first major transformation into a blue-water navy through the formation of the Strategic Mobile Fleet, which includes a battle group of Chungmugong Yi Sun-sin class destroyers, Dokdo class amphibious assault ship, AIP-driven Type 214 submarines, and King Sejong the Great class destroyers, which is equipped with the latest baseline of Aegis fleet-defense system that allows the ships to track and destroy multiple cruise missiles and ballistic missiles simultaneously, forming an integral part of South Korea's indigenous missile defense umbrella against the North Korean military's missile threat.
The South Korean air force operates 840 aircraft, making it world's ninth largest air force, including several types of advanced fighters like F-15K, heavily modified KF-16C/D, and the indigenous F/A-50, supported by well-maintained fleets of older fighters such as F-4E and KF-5E/F that still effectively serve the air force alongside the more modern aircraft. In an attempt to gain strength in terms of not just numbers but also modernity, the commissioning of four Boeing 737 AEW&C aircraft, under Project Peace Eye for centralized intelligence gathering and analysis on a modern battlefield, will enhance the fighters' and other support aircraft's ability to perform their missions with awareness and precision.
In May 2011, Korea Aerospace Industries Ltd., South Korea's largest plane maker, signed a $400 million deal to sell 16 T-50 Golden Eagle trainer jets to Indonesia, making South Korea the first country in Asia to export supersonic jets.
From time to time, South Korea has sent its troops overseas to assist American forces. It has participated in most major conflicts that the United States has been involved in the past 50 years. South Korea dispatched 325,517 troops to fight alongside American, Australian, Filipino, New Zealand and South Vietnamese soldiers in the Vietnam War, with a peak strength of 50,000. In 2004, South Korea sent 3,300 troops of the Zaytun Division to help re-building in northern Iraq, and was the third largest contributor in the coalition forces after only the US and Britain. Beginning in 2001, South Korea had so far deployed 24,000 troops in the Middle East region to support the War on Terrorism. A further 1,800 were deployed since 2007 to reinforce UN peacekeeping forces in Lebanon.
The United States have stationed a substantial contingent of troops in South Korea since the Korean War to defend South Korea in case of East Asian military crises. There are approximately 28,500 U.S. Military personnel stationed in Korea, most of them serving one year of unaccompanied tours. The American troops, which are primarily ground and air units, are assigned to USFK and mainly assigned to the Eighth United States Army of the US Army & Seventh Air Force of the US Air Force. They are stationed in installations at Osan, Kunsan, Yongsan, Dongducheon, Sungbuk, Camp Humphreys, and Daegu, as well as at Camp Bonifas in the DMZ Joint Security Area . A still functioning UN Command is technically the top of the chain of command of all forces in South Korea, including the US forces and the entire South Korean military – if a sudden escalation of war between North and South Korea were to occur the United States would assume control of the South Korean armed forces in all military and paramilitary moves. However, in September 2006, the Presidents of the United States and the Republic of Korea agreed that South Korea should assume the lead for its own defense. In early 2007, the U.S. Secretary of Defense and ROK Minister of National Defense determined that South Korea will assume wartime operational control of its forces on December 1, 2015. USFK will transform into a new joint-warfighting command, provisionally described as Korea Command (KORCOM).
Economy.
South Korea's market economy ranks 13th in the world by both nominal and purchasing power parity GDP, identifying it as one of the G-20 major economies. It is a developed country with a high-income economy and is the most industrialized member country of the OECD.
South Korea's economy was one of the world's fastest-growing from the early 1960s to the late 1990s, and South Korea is still one of the fastest-growing developed countries in the 2000s, along with Hong Kong, Singapore, and Taiwan, the other three Asian Tigers. South Koreans refer to this growth as the Miracle on the Han River. The South Korean economy is heavily dependent on international trade, and in 2013, South Korea was the 8th largest exporter and 7th largest importer in the world.
South Korea hosted the fifth G20 summit in its capital city, Seoul, in November 2010. The two-day summit was expected to boost South Korea's economy by 31 trillion won, or 4% of South Korea's 2010 GDP, in economic effects, and create over 160,000 jobs in South Korea. It may also help improve the country's sovereign credit rating.
Despite the South Korean economy's high growth potential and apparent structural stability, the country suffers damage to its credit rating in the stock market because of the belligerence of North Korea in times of deep military crises, which has an adverse effect on South Korean financial markets. The International Monetary Fund compliments the resilience of the South Korean economy against various economic crises, citing low state debt and high fiscal reserves that can quickly be mobilized to address financial emergencies. Although it was severely harmed by the Asian economic crisis of the late 1990s, the South Korean economy managed a rapid recovery and subsequently tripled its GDP.
Furthermore, South Korea was one of the few developed countries that were able to avoid a recession during the global financial crisis. Its economic growth rate reached 6.2 percent in 2010 (the fastest growth for eight years after significant growth by 7.2 percent in 2002), a sharp recovery from economic growth rates of 2.3% in 2008 and 0.2% in 2009, when the global financial crisis hit. The unemployment rate in South Korea also remained low in 2009, at 3.6%.
The following list includes the largest South Korean companies by revenue in 2013 who are all listed as part of the Fortune Global 500:
Transportation, energy and infrastructure.
South Korea has a technologically advanced transport network consisting of high-speed railways, highways, bus routes, ferry services, and air routes that criss-cross the country. Korea Expressway Corporation operates the toll highways and service amenities en route.
Korail provides frequent train services to all major South Korean cities. Two rail lines, Gyeongui and Donghae Bukbu Line, to North Korea are now being reconnected. The Korean high-speed rail system, KTX, provides high-speed service along Gyeongbu and Honam Line. Major cities including Seoul, Busan, Incheon, Daegu, Daejeon and Gwangju have urban rapid transit systems. Express bus terminals are available in most cities.
South Korea's largest airport, Incheon International Airport, was completed in 2001. By 2007, it was serving 30 million passengers a year. Other international airports include Gimpo, Busan and Jeju. There are also seven domestic airports, and a large number of heliports.
Korean Air, founded in 1962, served 21,640,000 passengers, including 12,490,000 international passengers in 2008. A second carrier, Asiana Airlines, established in 1988, also serves domestic and international traffic. Combined, South Korean airlines serve 297 international routes. Smaller airlines, such as Jeju Air, provide domestic service with lower fares.
South Korea is the world's fifth-largest nuclear power producer and the second-largest in Asia as of 2010. Nuclear power in South Korea supplies 45% of electricity production, and research is very active with investigation into a variety of advanced reactors, including a small modular reactor, a liquid-metal fast/transmutation reactor and a high-temperature hydrogen generation design. Fuel production and waste handling technologies have also been developed locally. It is also a member of the ITER project.
South Korea is an emerging exporter of nuclear reactors, having concluded agreements with the UAE to build and maintain four advanced nuclear reactors, with Jordan for a research nuclear reactor, and with Argentina for construction and repair of heavy-water nuclear reactors. As of 2010, South Korea and Turkey are in negotiations regarding construction of two nuclear reactors. South Korea is also preparing to bid on construction of a light-water nuclear reactor for Argentina.
South Korea is not allowed to enrich uranium or develop traditional uranium enrichment technology on its own, because of US political pressure, unlike most major nuclear powers such as Japan, Germany, and France, competitors of South Korea in the international nuclear market. This impediment to South Korea's indigenous nuclear industrial undertaking has sparked occasional diplomatic rows between the two allies. While South Korea is successful in exporting its electricity-generating nuclear technology and nuclear reactors, it cannot capitalize on the market for nuclear enrichment facilities and refineries, preventing it from further expanding its export niche. South Korea has sought unique technologies such as pyroprocessing to circumvent these obstacles and seek a more advantageous competition. The US has recently been wary of South Korea's burgeoning nuclear program, which South Korea insists will be for civilian use only.
South Korea is the third highest ranked Asian country in the World Economic Forum's Network Readiness Index (NRI) after Singapore and Hong Kong respectively – an indicator for determining the development level of a country’s information and communication technologies. South Korea ranked number 10 overall in the 2014 NRI ranking, up from 11 in 2013.
Tourism.
In 2012, 11.1 million foreign tourists visited South Korea, making it the 20th most visited country in the world, up from 8.5 million in 2010. Due to Hallyu, South Korea welcomed more than 12 million visitors in 2013 with 6 million tourists coming from China alone. With rising tourist prospects, especially from foreign countries outside of Asia, the South Korean government has set a target of attracting 20 million foreign tourists a year by 2017. Hallyu's positive effects of the nation's entertainment industry are not limited to within its culture industry, according to a study by the Hyundai Research Institute. The Hyundai Research Institute reported that the Korean Wave has a direct impact in encouraging direct foreign investment back into the country through demand for products, and the tourism industry. Among Asian countries, China was the most receptive, investing 1.4 billion in South Korea, with much of the investment within its service sector, a sevenfold increase from 2001. According to economist, Han Sang-Wan, shown an analysis that a 1 percent increase of exports of Korean cultural content pushes consumer goods exports up 0.083 percent while a 1 percent increase in Korean pop content exports to a country produces a 0.019 percent bump in tourism.
Science and technology.
Cyber security.
Following cyberattacks in the first half of 2013, whereby government, news-media, television station, and bank websites were compromised, the national government committed to the training of 5,000 new cybersecurity experts by 2017. The South Korean government blamed its northern counterpart for these attacks, as well as incidents that occurred in 2009, 2011 and 2012, but Pyongyang denies the accusations.
In late September 2013, a computer-security competition jointly sponsored by the defense ministry and the National Intelligence Service was announced. The winners will be announced on September 29, 2013 and will share a total prize pool of 80 million won (US$74,000).
Aerospace research.
South Korea has sent up 10 satellites from 1992, all using foreign rockets and overseas launch pads, notably Arirang-1 in 1999, and Arirang-2 in 2006 as part of its space partnership with Russia. Arirang-1 was lost in space in 2008, after nine years in service.
In April 2008, Yi So-yeon became the first Korean to fly in space, aboard the Russian Soyuz TMA-12.
In June 2009, the first spaceport of South Korea, Naro Space Center, was completed at Goheung, Jeollanam-do. The launch of Naro-1 in August 2009 resulted in a failure. The second attempt in June 2010 was also unsuccessful. However the third launch of the Naro 1 in January 2013 was successful. The government plans to develop Naro-2 by the year 2018.
South Korea's efforts to build an indigenous space launch vehicle is marred because of persistent political pressure of the United States, who had for many decades hindered South Korea's indigenous rocket and missile development programs in fear of their possible connection to clandestine military ballistic missile programs, which Korea many times insisted did not violate the research and development guidelines stipulated by US-Korea agreements on restriction of South Korean rocket technology research and development. South Korea has sought the assistance of foreign countries such as Russia through MTCR commitments to supplement its restricted domestic rocket technology. The two failed KSLV-I launch vehicles were based on the Universal Rocket Module, the first stage of the Russian Angara rocket, combined with a solid-fueled second stage built by South Korea.
Robotics.
Robotics has been included in the list of main national R&D projects in Korea since 2003. In 2009, the government announced plans to build robot-themed parks in Incheon and Masan with a mix of public and private funding.
In 2005, Korea Advanced Institute of Science and Technology (KAIST) developed the world's second walking humanoid robot, HUBO. A team in the Korea Institute of Industrial Technology developed the first Korean android, EveR-1 in May 2006.
EveR-1 has been succeeded by more complex models with improved movement and vision.
Plans of creating English-teaching robot assistants to compensate for the shortage of teachers were announced in February 2010, with the robots being deployed to most preschools and kindergartens by 2013. Robotics are also incorporated in the entertainment sector as well; the "Korean Robot Game Festival" has been held every year since 2004 to promote science and robot technology.
Biotechnology.
Since the 1980s, the Korean government has invested in the development of a domestic biotechnology industry, and the sector is projected to grow to $6.5 billion by 2010. The medical sector accounts for a large part of the production, including production of hepatitis vaccines and antibiotics.
Recently, research and development in genetics and cloning has received increasing attention, with the first successful cloning of a dog, Snuppy (in 2005), and the cloning of two females of an endangered species of wolves by the Seoul National University in 2007.
The rapid growth of the industry has resulted in significant voids in regulation of ethics, as was highlighted by the scientific misconduct case involving Hwang Woo-Suk.
Culture.
South Korea shares its traditional culture with North Korea, but the two Koreas have developed distinct contemporary forms of culture since the peninsula was divided in 1945. Historically, while the culture of Korea has been heavily influenced by that of neighboring China, it has nevertheless managed to develop a unique cultural identity that is distinct from its larger neighbor. The South Korean Ministry of Culture, Sports and Tourism actively encourages the traditional arts, as well as modern forms, through funding and education programs.
The industrialization and urbanization of South Korea have brought many changes to the way Korean people live. Changing economics and lifestyles have led to a concentration of population in major cities, especially the capital Seoul, with multi-generational households separating into nuclear family living arrangements. A 2014 Euromonitor study found that South Koreans drink the most alcohol on a weekly basis compared to the rest of the world. South Koreans drink 13.7 shots of liquor per week on average and, of the 44 other countries analyzed, Russia, the Philippines, and Thailand follow.
Art.
Korean art has been highly influenced by Buddhism and Confucianism, which can be seen in the many traditional paintings, sculptures, ceramics and the performing arts. Korean pottery and porcelain, such as Joseon's "baekja" and buncheong, and Goryeo's celadon are well known throughout the world. The Korean tea ceremony, pansori, talchum and buchaechum are also notable Korean performing arts.
Post-war modern Korean art started to flourish in the 1960s and 1970s, when South Korean artists took interest in geometrical shapes and intangible subjects. Establishing a harmony between man and nature was also a favorite of this time. Because of social instability, social issues appeared as main subjects in the 1980s. Art was influenced by various international events and exhibits in Korea, and with it brought more diversity. The Olympic Sculpture Garden in 1988, the transposition of the 1993 edition of the Whitney Biennial to Seoul, the creation of the Gwangju Biennale and the Korean Pavilion at the Venice Biennale in 1995 were notable events.
Architecture.
Because of South Korea's tumultuous history, construction and destruction has been repeated endlessly, resulting in an interesting melange of architectural styles and designs.
Korean traditional architecture is characterized by its harmony with nature. Ancient architects adopted the bracket system characterized by thatched roofs and heated floors called "ondol". People of the upper classes built bigger houses with elegantly curved tiled roofs with lifting eaves. Traditional architecture can be seen in the palaces and temples, preserved old houses called "hanok", and special sites like Hahoe Folk Village, Yangdong Village of Gyeongju and Korean Folk Village. Traditional architecture may also be seen at the nine UNESCO World Heritage Sites in South Korea.
Western architecture was first introduced to Korea at the end of the 19th century. Churches, offices for foreign legislation, schools and university buildings were built in new styles. With the annexation of Korea by Japan in 1910 the colonial regime intervened in Korea's architectural heritage, and Japanese-style modern architecture was imposed. The anti-Japanese sentiment, and the Korean War, led to the destruction of most buildings constructed during that time.
Korean architecture entered a new phase of development during the post-Korean War reconstruction, incorporating modern architectural trends and styles. Stimulated by the economic growth in the 1970s and 1980s, active redevelopment saw new horizons in architectural design. In the aftermath of the 1988 Seoul Olympics, South Korea has witnessed a wide variation of styles in its architectural landscape due, in large part, to the opening up of the market to foreign architects. Contemporary architectural efforts have been constantly trying to balance the traditional philosophy of "harmony with nature" and the fast-paced urbanization that the country has been going through in recent years.
Cuisine.
Korean cuisine, "hanguk yori" (한국요리; 韓國料理), or "hansik" (한식; 韓食), has evolved through centuries of social and political change. Ingredients and dishes vary by province. There are many significant regional dishes that have proliferated in different variations across the country in the present day. The Korean royal court cuisine once brought all of the unique regional specialties together for the royal family. Meals consumed both by the royal family and ordinary Korean citizens have been regulated by a unique culture of etiquette.
Korean cuisine is largely based on rice, noodles, tofu, vegetables, fish and meats. Traditional Korean meals are noted for the number of side dishes, "banchan" (반찬), which accompany steam-cooked short-grain rice. Every meal is accompanied by numerous banchan. Kimchi (김치), a fermented, usually spicy vegetable dish is commonly served at every meal and is one of the best known Korean dishes. Korean cuisine usually involves heavy seasoning with sesame oil, "doenjang" (된장), a type of fermented soybean paste, soy sauce, salt, garlic, ginger, and "gochujang" (고추장), a hot pepper paste. Other well-known dishes are "Bulgogi" (불고기), grilled marinated beef, "Gimbap" (김밥), and "Tteokbokki" (떡볶이), a spicy snack consisting of rice cake seasoned with gochujang or a spicy chili paste.
Soups are also a common part of a Korean meal and are served as part of the main course rather than at the beginning or the end of the meal. Soups known as "guk" (국) are often made with meats, shellfish and vegetables. Similar to guk, "tang" (탕; 湯) has less water, and is more often served in restaurants. Another type is "jjigae" (찌개), a stew that is typically heavily seasoned with chili pepper and served boiling hot.
South Korean snack companies, such as Lotte, are famous for making a wide range of Korean or other Asian-inspired snacks. One example is Pepero, a snack similar to Pocky, which originates from Japan. Pepero is manufactured by Lotte Confectionery.
Popular Korean alcoholic beverages include Soju, Makgeolli and Bokbunja ju.
Contemporary music, film and television.
In addition to domestic consumption, South Korean mainstream culture, including televised drama, films, and popular music, also generates significant exports to various parts of the world. This phenomenon, often called "Hallyu" or the "Korean Wave", has swept many countries in Asia and other parts of the world.
Until the 1990s, trot and ballads dominated Korean popular music. The emergence of the rap group Seo Taiji and Boys in 1992 marked a turning point for Korean popular music, also known as K-pop, as the group incorporated elements of popular musical genres of rap, rock, and techno into its music. Hip hop, dance and ballad oriented acts have become dominant in the Korean popular music scene, though trot is still popular among older Koreans. Many K-pop stars and groups are also well known abroad, especially in other parts of Asia. A solo artist known as Psy has rescently topped charts around the world with his hit Gangnam Style.
Since the success of the film "Shiri" in 1999, Korean film has begun to gain recognition internationally. Domestic film has a dominant share of the market, partly because of the existence of screen quotas requiring cinemas to show Korean films at least 73 days a year.
Korean television shows have become popular outside of Korea. Dramas have tended to have a romantic focus, such as "Princess Hours", "You're Beautiful", "Playful Kiss", "My Name is Kim Sam Soon", "Boys Over Flowers", "Winter Sonata", "Autumn in My Heart", "Full House", "City Hunter", "All About Eve", "Secret Garden", "I Can Hear Your Voice", "Master's Sun", and "My Love from the Star". Historical dramas have included "Faith", "Dae Jang Geum", "The Legend", "Dong Yi", "Moon Embracing the Sun", and "Sungkyunkwan Scandal".
Holidays.
There are many official public holidays in South Korea. Korean New Year's Day, or "Seollal," is celebrated on the first day of the Korean lunar calendar. Korean Independence Day falls on March 1, and commemorates the March 1st Movement of 1919. Memorial Day is celebrated on June 6, and its purpose is to honor the men and women who died in South Korea's independence movement. Constitution Day is on July 17, and it celebrates the promulgation of Constitution of the Republic of Korea. Liberation Day, on August 15, celebrates Korea's liberation from the Empire of Japan in 1945. Every 15th day of the 8th lunar month, Koreans celebrate the Midautumn Festival, in which Koreans visit their ancestral hometowns and eat a variety of traditional Korean foods. On October 1, Armed Forces day is celebrated, honoring the military forces of South Korea. October 3 is National Foundation Day. Hangul Day, on October 9 commemorates the invention of hangul, the native alphabet of the Korean language. There are also unofficial holidays celebrated in Korea, such as Pepero Day, a day to celebrate the Korean snack of Pepero.
Technology culture.
South Korean corporations Samsung and LG were ranked first and third largest mobile phone companies in the world in the first quarter of 2012, respectively. An estimated 90% of South Koreans own a mobile phone. Aside from placing/receiving calls and text messaging, mobile phones in the country are widely used for watching Digital Multimedia Broadcasting (DMB) or viewing websites. Over one million DMB phones have been sold and the three major wireless communications providers SK Telecom, KT, and LG U+ provide coverage in all major cities and other areas. South Korea has the fastest Internet download speeds in the world, with an average download speed of 25.3 Mbit/s.
Sports.
The martial art taekwondo originated in Korea. In the 1950s and 1960s, modern rules were standardized, with taekwondo becoming an official Olympic sport in 2000. Other Korean martial arts include taekkyeon, hapkido, Tang Soo Do, Kuk Sool Won, kumdo and subak.
Football has traditionally been regarded as the most popular sport in Korea. Recent polling indicates that a majority, 41% of South Korean sports fans continue to self-identify as football fans, with baseball ranked 2nd at 25% of respondents. However, the polling did not indicate the extent to which respondents follow both sports. The national football team became the first team in the Asian Football Confederation to reach the FIFA World Cup semi-finals in the 2002 FIFA World Cup, jointly hosted by South Korea and Japan. The Korea Republic national team (as it is known) has qualified for every World Cup since Mexico 1986, and has broken out of the group stage twice: first in 2002, and again in 2010, when it was defeated by eventual semi-finalist Uruguay in the Round of 16. At the 2012 Summer Olympics, South Korea won the Bronze Medal for football.
Baseball was first introduced to Korea in 1905 and has since become increasingly popular, with some sources claiming it has surpassed football as the most popular sport in the country. Recent years have been characterized by increasing attendance and ticket prices for professional baseball games. The Korea Professional Baseball league, a 9-team circuit, was established in 1982. The South Korea national team finished third in the 2006 World Baseball Classic and second in the 2009 tournament. The team's 2009 final game against Japan was widely watched in Korea, with a large screen at Gwanghwamun crossing in Seoul broadcasting the game live. In the 2008 Summer Olympics, South Korea won the gold medal in baseball. Also in 1982, at the Baseball Worldcup, Korea won the gold medal. At the 2010 Asian Games, the Korean National Baseball team won the gold medal. Three notable Korean baseball players are Chan Ho Park, Shin-Soo Choo, and Hyun-Jin Ryu.
Basketball is a popular sport in the country as well. South Korea has traditionally had one of the top basketball teams in Asia and one of the continent's strongest basketball divisions. Seoul hosted the 1967 and 1995 Asian Basketball Championship. The Korea national basketball team has won a record number of 23 medals at the event to date.
South Korea hosted the Asian Games in 1986 (Seoul), 2002 (Busan) and 2014 (Incheon). It also hosted the Winter Universiade in 1997, the Asian Winter Games in 1999 and the Summer Universiade in 2003. In 1988, South Korea hosted the Summer Olympics in Seoul, coming fourth with 12 gold medals, 10 silver medals and 11 bronze medals. South Korea regularly performs well in archery, shooting, table tennis, badminton, short track speed skating, handball, hockey, freestyle wrestling, Greco-Roman wrestling, baseball, judo, taekwondo, speed skating, figure Skating, and weightlifting. The Seoul Olympic Museum is a museum in Seoul, South Korea, dedicated to the 1988 Summer Olympics. On July 6, 2011 Pyeongchang was chosen by the IOC to host the 2018 Winter Olympics.
South Korea has won more medals in the Winter Olympics than any other Asian country with a total of 45 medals (23 gold, 14 silver, and 8 bronze). At the 2010 Winter Olympics, South Korea ranked fifth in the overall medal rankings. South Korea is especially strong in short track speed skating. However, speed skating and figure skating are very popular, too, and ice hockey is an emerging sport with Anyang Halla winning their first ever Asia League Ice Hockey title in March 2010.
Seoul hosted a professional triathlon race, which is part of the International Triathlon Union (ITU) World Championship Series in May 2010. In 2011, the South Korean city of Daegu hosted the 2011 IAAF World Championships in Athletics.
In October 2010, South Korea hosted its first Formula One race at the Korea International Circuit in Yeongam, about 400 km south of Seoul. The Korean Grand Prix was held from 2010 to 2013, but was not placed on the 2014 F1 calendar.
Domestic horse racing events are also followed by South Koreans and Seoul Race Park in Gwacheon, Gyeonggi-do is located closest to Seoul out of the country's three tracks.
Electronic sports, also called eSports or competitive gaming, has become a staple of South Korea in recent years. It has become something of a career for many young people. This is managed by the Korean e-Sports Association (KeSPA for short), and top players can make large sums of money. The two most popular games are League of Legends and . Some players in Starcraft II and its standalone expansions can end up making six figures.
Korean e-Sports Association
Korea Professional Sports League
International Championship Host
Further reading.
</dl>

</doc>
<doc id="27105" url="http://en.wikipedia.org/wiki?curid=27105" title="Leonard McCoy">
Leonard McCoy

Leonard H. "Bones" McCoy is a character in the American science fiction franchise "Star Trek". First portrayed by DeForest Kelley in the , McCoy also appears in the , six Star Trek movies, the pilot episode of "", and in numerous books, comics, and video games. Karl Urban assumed the role of the character in the 2009 film "Star Trek", and its 2013 sequel "Star Trek Into Darkness".
Depiction.
McCoy was born in Georgia, January 20, 2227. The son of David,:257–258 he attended the University of Mississippi and is a divorcé. In 2266, McCoy was posted as chief medical officer of the USS "Enterprise" under Captain James T. Kirk who often calls him "Bones". McCoy and Kirk are good friends, even "brotherly".:146 The passionate, sometimes cantankerous McCoy frequently argues with Kirk's other confidant, science officer Spock, and occasionally is bigoted toward Spock's Vulcan heritage. McCoy often plays the role of Kirk's conscience, offering a counterpoint to Spock's logic. McCoy is suspicious of technology, especially the transporter; as a physician, he prefers less intrusive treatment and believes in the body's innate recuperative powers. The character's nickname, "Bones", is a play on "sawbones", an epithet for physicians, in particular, those qualified as surgeons. 
Kirk orders McCoy's commission reactivated in ' (1979); a resentful McCoy complains of being "drafted". Spock transfers his "katra"—his knowledge and experience—into McCoy's mind before dying in ' (1982). This causes mental anguish for McCoy, who in ' (1984) helps restore Spock's katra to his reanimated body. McCoy rejoins Kirk's crew aboard the USS "Enterprise"-A in ' (1986). In ', McCoy (through the intervention of Spock's half-brother Sybok) reveals that he helped his father commit suicide to relieve him of his pain. Shortly after the suicide, a cure was found for his father's disease, and McCoy carried the guilt about it with him for the rest of his life. In ' (1991), McCoy and Kirk escape from a Klingon prison world, and the "Enterprise" crew stops a plot to prevent peace between the United Federation of Planets and the Klingon Empire. Kelley reprised the role for the "Encounter at Farpoint" pilot episode of " (1987), insisting upon no more than the minimum Screen Actors Guild payment for his appearance.
In the " episode "", McCoy mentions he has a daughter. Chekov's friend Irina in the original series episode "The Way to Eden" was originally written as Dr. McCoy's daughter Joanna, but changed before the episode was shot.
Reboot series.
In the 2009 "Star Trek" film, which takes place in an "alternate, parallel" reality, McCoy and Kirk become friends at Starfleet Academy, which McCoy joins after a divorce that he says "left [him] nothing but [his] bones." This line, improvised by Urban, explains how McCoy came to be known as "Bones". McCoy later helps get Kirk posted aboard the USS "Enterprise".
Development.
"Star Trek" creator Gene Roddenberry had worked with Kelley on previous television pilots, and Kelley was Roddenberry's first choice to play the doctor aboard the USS "Enterprise". However, for the rejected pilot "" (1964), Roddenberry went with director Robert Butler's choice of John Hoyt to play Dr. Philip Boyce. For the second pilot, "Where No Man Has Gone Before" (1966), Roddenberry accepted director James Goldstone's decision to have Paul Fix play Dr. Mark Piper. Although Roddenberry wanted Kelley to play the character of ship's doctor, he didn't put Kelley's name forward to NBC; the network never "rejected" the actor as Roddenberry sometimes suggested.
Kelley's first broadcast appearance as Doctor Leonard McCoy was in "The Man Trap" (1966). Despite his character's prominence, Kelley's contract granted him only a "featuring" credit; it was not until the second season that he was given "starring" credit, at the urging of producer Robert Justman. Kelley was apprehensive about "Star Trek"‍ '​s future, telling Roddenberry that the show was "going to be the biggest hit or the biggest miss God ever made".:146 Kelley portrayed McCoy throughout the original "Star Trek" series and voiced the character in the animated "Star Trek".
Kelley, who in his youth wanted to become a doctor, in part drew upon his real-life experiences in creating McCoy: a doctor's "matter-of-fact" delivery of news of Kelley's mother's terminal cancer was the "abrasive sand" Kelley used in creating McCoy's demeanor.:145 "Star Trek" writer D. C. Fontana said that while Roddenberry created the series, Kelley essentially created McCoy; everything done with the character was done with Kelley's input.:156
"Exquisite chemistry" among Kelley, William Shatner, and Leonard Nimoy manifested itself in their performances as McCoy, Captain James T. Kirk and science officer Spock, respectively.:154 Nichelle Nichols, who played Uhura, referred to Kelley as her "sassy gentleman friend";:154 the friendship between the African-American Nichols and Southern Kelley was a real-life demonstration of the message Roddenberry hoped to convey through "Star Trek".:154
For the 2009 film "Star Trek", writers Roberto Orci and Alex Kurtzman saw McCoy as an "arbiter" in Kirk and Spock's relationship. While Spock represented "extreme logic, extreme science" and Kirk symbolized "extreme emotion and intuition", McCoy's role as "a very colorful doctor, essentially a very humanistic scientist" represented the "two extremes that often served as the glue that held the trio together." They chose to reveal McCoy befriended Kirk first, explaining the "bias" in their friendship and why he would often be a "little dismissive" of Spock. Urban said the script was "very faithful" to the original character, including the "great compassion for humanity and that sense of irascibility" with which Kelley imbued the character. Urban trained with a dialect coach to create McCoy's accent. Urban reprised the role in its 2013 sequel "Star Trek Into Darkness".
Reception and cultural impact.
McCoy is someone to whom Kirk unburdens himself and is a foil to Spock. He is Kirk's "friend, personal bartender, confidant, counselor, and priest". Spock and McCoy's bickering became so popular that Roddenberry wrote in a 1968 memo "we simply didn't realize ... how much the fans loved the bickering between our Arrowsmith and our Alien". Urban said McCoy has a "sense of irascibility with real passion for life and doing the right thing", and that "Spock's logic and McCoy's moral standing gave Kirk the benefit of having three brains instead of just one." Jennifer Porter and Darcee McLaren wrote that McCoy is an "unintentional" example of how "irrational prejudices and fixations, wishful thinking and emotional reasoning, denial and repression, and unresolved neurotic disturbances" compromise "scientific rationality" in "Star Trek".
An Illinois con artist scammed $25 million in investments for a non-existent "McCoy Home Health Tablet" medical device.
Kelley said that his greatest thrill at "Star Trek" conventions was the number of people who told him they entered the medical profession because of the McCoy character.
With regard to the 2009 film, "The Guardian" called Urban's performance of McCoy an "unqualified success", and "The New York Times" called the character "wild-eyed and funny". Slate.com said Urban came closer than the other actors to impersonating a character's original depiction.
"He's dead, Jim!".
Twenty times on the original "Star Trek", McCoy declares someone or something deceased with the line, "He's dead", "He's dead, Jim", or something similar. The phrase so became a catchphrase of the character that Kelley joked that the line would appear on his tombstone, but disliked repeating such lines:166 and refused to say it in "" when Spock is near death. Kelley and James Doohan (Scotty) agreed to swap their lines, so McCoy warns Kirk against opening the engineering doors while Scotty says "He's dead already".:249
The line has entered popular culture as a general metaphor, with uses as diverse as descriptions of an unresponsive electronic circuit, an example of how to add an audio file to function as an alert sound in a computer system, and an illustrative quote regarding how to know if one's opponent has been destroyed in an action hero game. USC Literature Professor Henry Jenkins cited Dr. McCoy's "He's dead, Jim" line as an example of fans actively participating in the creation of an underground culture in which they derive pleasure by repeating memorable lines as part of constructing new mythologies and alternative social communities. Google Chrome uses the phrase as an error message when Google Chrome either is terminated with the task manager, or Chrome runs out of memory, and is a common error.
"I'm a doctor, not a...".
Another of McCoy's catchphrases is his "I'm a doctor, (Jim) not a(n)..." statements, delivered by Kelley 11 times,:166 and twice (by Karl Urban) in later films. McCoy repeats the line when he must perform some task beyond his medical skills, such as the "classic moment" when he is confronted with the unusual silicon-based Horta alien in "The Devil in the Dark" (1967), saying, "I'm a doctor, not a bricklayer." The phrase also appears in the 2009 film, in which McCoy (Karl Urban) says "I'm a doctor, not an astrophysicist!" to Spock. Similarly, in 2013's "Star Trek Into Darkness", McCoy (Urban again) tells Spock, "Damn it, man, I'm a doctor, not a torpedo technician!"
The line or some variation has been used by Dr. Julian Bashir (Alexander Siddig) from ', (Robert Picardo) from ', two other Emergency Medical Holograms (one in ' (Picardo) and the other in the "Voyager" episode "" (Andy Dick) and Dr. Phlox (John Billingsley) from '. It has also made its way into many other shows such as "Stargate Atlantis", "Robot Chicken", "Terra Nova", "Family Guy", "Once Upon a Time", and "Friends", as well as "". In a parody sketch titled "The Restaurant Enterprise", on an episode of "Saturday Night Live", Kirk (guest host William Shatner) directs McCoy (Phil Hartman) to help a man who's choking. McCoy snaps, "Dammit, Jim! I'm a doctor, not a ... (suddenly realizes the situation; slightly embarrassed) Oh ... oh, sure." On an episode of "In Living Color", one parody sketch lampoons the advanced age of the principal Star Trek actors. McCoy appears as a skeleton in a wheelchair, and quips, "Dammit, Jim! I'm a corpse, not a doctor!" DeForest Kelley himself parodied the phrase for a "Trivial Pursuit" commercial ("How should I know? I'm an actor, not a doctor").

</doc>
<doc id="27119" url="http://en.wikipedia.org/wiki?curid=27119" title="Silver">
Silver

Silver is a chemical element with symbol Ag (Greek: άργυρος "árguros", Latin: "argentum", both from the Indo-European root "*h₂erǵ-" for "grey" or "shining") and atomic number 47. A soft, white, lustrous transition metal, it possesses the highest electrical conductivity, thermal conductivity and reflectivity of any metal. The metal occurs naturally in its pure, free form (native silver), as an alloy with gold and other metals, and in minerals such as argentite and chlorargyrite. Most silver is produced as a byproduct of copper, gold, lead, and zinc refining.
Silver has long been valued as a precious metal. More abundant than gold, silver metal has in many premodern monetary systems functioned as coinable specie, sometimes even alongside gold. In addition, silver has numerous applications beyond currency, such as in solar panels, water filtration, jewelry and ornaments, high-value tableware and utensils (hence the term silverware), and also as an investment in the forms of coins and bullion. Silver is used industrially in electrical contacts and conductors, in specialized mirrors, window coatings and in catalysis of chemical reactions. Its compounds are used in photographic film and X-rays. Dilute silver nitrate solutions and other silver compounds are used as disinfectants and microbiocides (oligodynamic effect), added to bandages and wound-dressings, catheters and other medical instruments.
Characteristics.
Silver is produced during certain types of supernova explosions by nucleosynthesis from lighter elements through the r-process, a form of nuclear fusion that produces many elements heavier than iron, of which silver is one.
Silver is a very ductile, malleable (slightly less so than gold), univalent coinage metal, with a brilliant white metallic luster that can take a high degree of polish. Protected silver has higher optical reflectivity than aluminium at all wavelengths longer than ~450 nm. At wavelengths shorter than 450 nm, silver's reflectivity is inferior to that of aluminium and drops to zero near 310 nm.
The electrical conductivity of silver is the highest of all metals, even higher than copper, but its greater cost has prevented it from being widely used in place of copper for electrical purposes. An exception to this is in radio-frequency engineering, particularly at VHF and higher frequencies, where silver plating is employed to improve electrical conductivity of parts, including wires. Silver also has the lowest contact resistance of any metal. During World War II in the US, 13,540 tons were used in the electromagnets used for enriching uranium, mainly because of the wartime shortage of copper.
Pure silver has the highest thermal conductivity of any metal, although that of the nonmetal carbon in the form of diamond and superfluid helium II are higher.
Silver halides are photosensitive and are remarkable for their ability to record a latent image that can later be developed chemically. Silver is stable in pure air and water, but tarnishes when it is exposed to air or water containing ozone or hydrogen sulfide, the latter forming a black layer of silver sulfide which can be cleaned off with dilute hydrochloric acid. The most common oxidation state of silver is +1 (for example, silver nitrate, AgNO3); the less common +2 compounds (for example, silver(II) fluoride, AgF2), and the even less common +3 (for example, potassium tetrafluoroargentate(III), KAgF4) and even +4 compounds (for example, potassium hexafluoroargentate(IV), K2AgF6) are also known.
Isotopes.
Naturally occurring silver is composed of two stable isotopes, 107Ag and 109Ag, with 107Ag being slightly more abundant (51.839% natural abundance). Their almost equal abundance is rare in the periodic table. Silver's atomic weight is 107.8682(2) g/mol.
Twenty-eight radioisotopes have been characterized, the most stable being 105Ag with a half-life of 41.29 days, 111Ag with a half-life of 7.45 days, and 112Ag with a half-life of 3.13 hours. This element has numerous meta states, the most stable being 108mAg (t1/2 = 418 years), 110mAg (t1/2 = 249.79 days) and 106mAg (t1/2 = 8.28 days). All of the remaining radioactive isotopes have half-lives of less than an hour, and the majority of these have half-lives of less than three minutes.
Isotopes of silver range in relative atomic mass from 93.943 (94Ag) to 126.936 (127Ag); the primary decay mode before the most abundant stable isotope, 107Ag, is electron capture and the primary mode after is beta decay. The primary decay products before 107Ag are palladium (element 46) isotopes, and the primary products after are cadmium (element 48) isotopes.
The palladium isotope 107Pd decays by beta emission to 107Ag with a half-life of 6.5 million years. Iron meteorites are the only objects with a high-enough palladium-to-silver ratio to yield measurable variations in 107Ag abundance. Radiogenic 107Ag was first discovered in the Santa Clara meteorite in 1978. The discoverers suggest the coalescence and differentiation of iron-cored small planets may have occurred 10 million years after a nucleosynthetic event. 107Pd–107Ag correlations observed in bodies that have clearly been melted since the accretion of the solar system must reflect the presence of unstable nuclides in the early solar system.
Compounds.
Silver metal dissolves readily in nitric acid (HNO3) to produce silver nitrate (AgNO3), a transparent crystalline solid that is photosensitive and readily soluble in water. Silver nitrate is used as the starting point for the synthesis of many other silver compounds, as an antiseptic, and as a yellow stain for glass in stained glass. Silver metal does not react with sulfuric acid, which is used in jewelry-making to clean and remove copper oxide firescale from silver articles after silver soldering or annealing. Silver reacts readily with sulfur or hydrogen sulfide H2S to produce silver sulfide, a dark-colored compound familiar as the tarnish on silver coins and other objects. Silver sulfide Ag2S also forms silver whiskers when silver electrical contacts are used in an atmosphere rich in hydrogen sulfide.
Silver chloride (AgCl) is precipitated from solutions of silver nitrate in the presence of chloride ions, and the other silver halides used in the manufacture of photographic emulsions are made in the same way, using bromide or iodide salts. Silver chloride is used in glass electrodes for pH testing and potentiometric measurement, and as a transparent cement for glass. Silver iodide has been used in attempts to seed clouds to produce rain. Silver halides are highly insoluble in aqueous solutions and are used in gravimetric analytical methods.
Silver oxide (Ag2O), produced when silver nitrate solutions are treated with a base, is used as a positive electrode (anode) in watch batteries. Silver carbonate (Ag2CO3) is precipitated when silver nitrate is treated with sodium carbonate (Na2CO3).
Silver fulminate (AgONC), a powerful, touch-sensitive explosive used in percussion caps, is made by reaction of silver metal with nitric acid in the presence of ethanol (C2H5OH). Other dangerously explosive silver compounds are silver azide (AgN3), formed by reaction of silver nitrate with sodium azide (NaN3), and silver acetylide, formed when silver reacts with acetylene gas.
Latent images formed in silver halide crystals are developed by treatment with alkaline solutions of reducing agents such as hydroquinone, metol (4-(methylamino)phenol sulfate) or ascorbate, which reduce the exposed halide to silver metal. Alkaline solutions of silver nitrate can be reduced to silver metal by reducing sugars such as glucose, and this reaction is used to silver glass mirrors and the interior of glass Christmas ornaments. Silver halides are soluble in solutions of sodium thiosulfate (Na2S2O3) which is used as a photographic fixer, to remove excess silver halide from photographic emulsions after image development.
Silver metal is attacked by strong oxidizers such as potassium permanganate (KMnO4) and potassium dichromate (K2Cr2O7), and in the presence of potassium bromide (KBr); these compounds are used in photography to bleach silver images, converting them to silver halides that can either be fixed with thiosulfate or redeveloped to intensify the original image. Silver forms cyanide complexes (silver cyanide) that are soluble in water in the presence of an excess of cyanide ions. Silver cyanide solutions are used in electroplating of silver.
Although silver normally has oxidation state +1 in compounds, other oxidation states are known, such as +3 in AgF3, produced by the reaction of elemental silver or silver fluoride with krypton difluoride.
Silver artifacts primarily under go three forms of deterioration. Silver sulfide tarnish is the most common form of silver degradation. Fresh silver chloride is pale yellow colored, becoming purplish on exposure to light and projects slightly from the surface of the artifact or coin. The precipitation of copper in ancient silver can be used to date artifacts.
Applications.
Many well-known uses of silver involve its precious metal properties, including currency, decorative items, and mirrors. The contrast between its bright white color and other media makes it very useful to the visual arts. By contrast, fine silver particles form the dense black in photographs and in silverpoint drawings. It has also long been used to confer high monetary value as objects (such as silver coins and investment bars) or make objects symbolic of high social or political rank. Silver salts have been used since the Middle Ages to produce a yellow or orange colors to stained glass, and more complex decorative color reactions can be produced by incorporating silver metal in blown, kilnformed or torchworked glass.
Currency.
Silver, in the form of electrum (a gold–silver alloy), was coined to produce money around 700 BC by the Lydians. Later, silver was refined and coined in its pure form. Many nations used silver as the basic unit of monetary value. In the modern world, silver bullion has the ISO currency code XAG. The name of the pound sterling (£) reflects the fact it originally represented the value of one pound Tower weight of sterling silver; other historical currencies, such as the French livre, have similar etymologies. During the 19th century, the bimetallism that prevailed in most countries was undermined by the discovery of large deposits of silver in the Americas; fearing a sharp decrease in the value of silver and thus the currency, most states switched to a gold standard by 1900. In some languages, such as Sanskrit, Spanish, French, and Hebrew, the same word means both silver and money.
The 20th century saw a gradual movement to fiat currency, with most of the world monetary system losing its link to precious metals after Richard Nixon took the United States dollar off the gold standard in 1971; the last currency backed by gold was the Swiss franc, which became a pure fiat currency on 1 May 2000. During this same period, silver gradually ceased to be used in circulating coins. In 1964, the United States stopped minting their silver dime and quarter. They minted their last circulating silver coin in 1970 in its 40% half-dollar.
In 1968, Canada minted their last circulating silver coins which were the 50% dime and the 50% quarter. The Royal Canadian Mint still makes many collectible silver coins with various dollar denominations. In addition to Canada, the United States and many other countries continue to mint silver coins that are collected for their bullion and numismatic value. The U.S. coin is known as the "Silver Eagle".
Silver is used as a currency by many individuals, and is legal tender in the US state of Utah. Silver coins and bullion are also used as an investment to guard against inflation and devaluation.
Jewelry and silverware.
Jewelry and silverware are traditionally made from sterling silver (standard silver), an alloy of 92.5% silver with 7.5% copper. In the US, only an alloy consisting of at least 90.0% fine silver can be marketed as "silver" (thus frequently stamped 900). Sterling silver (stamped 925) is harder than pure silver, and has a lower melting point (893 °C) than either pure silver or pure copper. Britannia silver is an alternative, hallmark-quality standard containing 95.8% silver, often used to make silver tableware and wrought plate. With the addition of germanium, the patented modified alloy Argentium sterling silver is formed, with improved properties, including resistance to firescale.
Sterling silver jewelry is often plated with a thin coat of .999 fine silver to give the item a shiny finish. This process is called "flashing". Silver jewelry can also be plated with rhodium (for a bright, shiny look) or gold (to produce silver gilt).
Silver is a constituent of almost all colored carat gold alloys and carat gold solders, giving the alloys paler color and greater hardness. White 9 carat gold contains 62.5% silver and 37.5% gold, while 22 carat gold contains a minimum of 91.7% gold and 8.3% silver or copper or other metals.
Historically, the training and guild organization of goldsmiths included silversmiths, as well, and the two crafts remain largely overlapping. Unlike blacksmiths, silversmiths do not shape the metal while it is red-hot, but instead, work it at room temperature with gentle and carefully placed hammer blows. The essence of silversmithing is to take a flat piece of metal and to transform it into a useful object using different hammers, stakes and other simple tools.
While silversmiths specialize in, and principally work silver, they also work with other metals, such as gold, copper, steel, and brass. They make jewelry, silverware, armor, vases, and other artistic items. Because silver is such a malleable metal, silversmiths have a large range of choices with how they prefer to work the metal. Historically, silversmiths are mostly referred to as goldsmiths, which was usually the same guild. In the western Canadian silversmith tradition, guilds do not exist; however, mentoring through colleagues becomes a method of professional learning within a community of craftspeople.
Traditionally, silversmiths mostly made "silverware" (cutlery, tableware, bowls, candlesticks and such). Only in more recent times has silversmithing become mainly work in jewelry, as much less solid silver tableware is now handmade.
Solar energy.
Silver is used in the manufacture of crystalline solar photovoltaic panels. Silver is also used in plasmonic solar cells. 100 million ounces of silver are projected for use by solar energy in 2015.
Silver is the reflective coating of choice for concentrated solar power reflectors. In 2009, scientists at the National Renewable Energy Laboratory (NREL) and SkyFuel teamed to develop large curved sheets of metal that have the potential to be 30% less expensive than today's best collectors of concentrated solar power by replacing glass-based models with a silver polymer sheet that has the same performance as the heavy glass mirrors, but at much lower cost and weight. It also is much easier to deploy and install. The glossy film uses several layers of polymers, with an inner layer of pure silver.
Air conditioning.
In 2014 researchers invented a mirror-like panel that, when mounted on a building, acts like an air conditioner. The mirror is built from several layers of wafer-thin materials. The first layer is silver, the most reflective substance on Earth. On top of this are alternating layers of silicon dioxide and hafnium oxide. These layers improve the reflectivity, but also turn the mirror into a thermal radiator.
Water purification.
Silver is used in water purifiers. It prevents bacteria and algae from building up in filters. The catalytic action of silver, in concert with oxygen, sanitizes water and eliminates the need for chlorine. Silver ions are also added to water purification systems in hospitals, community water systems, pools and spas, displacing chlorine.
Dentistry.
Silver can be alloyed with mercury at room temperature to make amalgams that are widely used for dental fillings. To make dental amalgam, a mixture of powdered silver and other metals such as tin and gold is mixed with mercury to make a stiff paste that can be adapted to the shape of a cavity. The dental amalgam achieves initial hardness within minutes, and sets hard in a few hours.
Photography and electronics.
The use of silver in photography, in the form of silver nitrate and silver halides, has rapidly declined due to the lower demand for consumer color film from the advent of digital technology. From the peak global demand for photographic silver in 1999 (267,000,000 troy ounces or 8304.6 metric tonnes) the market had contracted almost 70% by 2013.
Some electrical and electronic products use silver for its superior conductivity, even when tarnished. The primary example of this is in high quality RF connectors. The increase in conductivity is also taken advantage of in RF engineering at VHF and higher frequencies, where conductors often cannot be scaled by 6%, due to tuning requirements, e.g. cavity filters. As an additional example, printed circuits and RFID antennas can be made using silver paints, and computer keyboards use silver electrical contacts. Silver cadmium oxide is used in high-voltage contacts because it can withstand arcing.
Some manufacturers produce audio connector cables, speaker wires, and power cables using silver conductors, which have a 6% higher conductivity than ordinary copper ones of identical dimensions, but cost much more. Though debatable, many hi-fi enthusiasts believe silver wires improve sound quality.
Small devices, such as hearing aids and watches, commonly use silver oxide batteries due to their long life and high energy-to-weight ratio. Another usage is high-capacity silver-zinc and silver-cadmium batteries.
In World War II, there was a shortage of copper and silver borrowed from the United States Treasury for electrical windings for several production facilities including those of the Manhattan Project; see below under History, WWII.
Glass coatings.
Mirrors in almost all reflective telescopes use vacuum aluminum coatings. However thermal or infrared telescopes use silver coated mirrors because of silver's ability to reflect some types of infrared radiation more effectively than aluminum, as well as silver's ability to reduce the amount of radiation actually emitted from the mirror (its thermal emissivity).
Silver, in protected or enhanced coatings, is seen as the next generation metal coating for reflective telescope mirrors.
Using a process called sputtering, silver, along with other optically transparent layers, is applied to glass, creating low emissivity coatings used in high-performance insulated glazing. The amount of silver used per window is small because the silver layer is only 10–15 nanometers thick. However, the amount of silver-coated glass worldwide is hundreds of millions of square meters per year, leading to silver consumption on the order of 10 cubic meters or 100 metric tons/year. Silver color seen in architectural glass and tinted windows on vehicles is produced by sputtered chrome, stainless steel or other alloys.
Silver-coated polyester sheets, used to retrofit windows, are another popular method for reducing light transmission.
Other industrial and commercial applications.
Silver and silver alloys are used in the construction of high-quality musical wind instruments of many types. Flutes, in particular, are commonly constructed of silver alloy or silver plated, both for appearance and for the frictional surface properties of silver. Brass instruments, such as Trumpets and Baritones, are also commonly plated in silver.
Silver's catalytic properties make it ideal for use as a catalyst in oxidation reactions, for example, the production of formaldehyde from methanol and air by means of silver screens or crystallites containing a minimum 99.95 weight-percent silver. Silver (upon some suitable support) is probably the only catalyst available today to convert ethylene to ethylene oxide (later hydrolyzed to ethylene glycol, used for making polyesters)— an important industrial reaction. It is also used in the Oddy test to detect reduced sulfur compounds and carbonyl sulfides.
Because silver readily absorbs free neutrons, it is commonly used to make control rods to regulate the fission chain reaction in pressurized water nuclear reactors, generally in the form of an alloy containing 80% silver, 15% indium, and 5% cadmium.
Silver is used to make solder and brazing alloys, and as a thin layer on bearing surfaces can provide a significant increase in galling resistance and reduce wear under heavy load, particularly against steel.
Biology.
Silver stains are used in biology to increase the contrast and visibility of cells and organelles in microscopy. Camillo Golgi used silver stains to study cells of the nervous system and the Golgi apparatus. Silver stains are used to stain proteins in gel electrophoresis and polyacrylamide gels, either as primary stains or to enhance the visibility and contrast of colloidal gold stain. Different yeasts from Brazilian gold mines, bioaccumulate free and complexed silver ions. A sample of the fungus "Aspergillus niger" was found growing from gold mining solution; and was found to contain cyano metal complexes; such as gold, silver, copper iron and zinc. The fungus also plays a role in the solubilization of heavy metal sulfides.
Medicine.
The medical uses of silver include its incorporation into wound dressings, and its use as an antibiotic coating in medical devices. Wound dressings containing silver sulfadiazine or silver nanomaterials may be used to treat external infections. Silver is also used in some medical applications, such as urinary catheters and endotracheal breathing tubes, where there is tentative evidence that it is effective in reducing catheter-related urinary tract infections and ventilator-associated pneumonia respectively. The silver ion (Ag+) is bioactive and in sufficient concentration readily kills bacteria "in vitro". Silver and silver nanoparticles are used as an antimicrobial in a variety of industrial, healthcare and domestic applications.
Investing.
Silver coins and bullion are used for investing. Various types of silver investments can be made on the stock markets, including mining or silver streaming stocks, or silver-backed exchange-traded funds.
Clothing.
Silver inhibits the growth of bacteria and fungi on clothing, such as socks, so is sometimes added to reduce odors and the risk of bacterial and fungal infections. It is incorporated into clothing or shoes either by integrating silver nanoparticles into the polymer from which yarns are made or by coating yarns with silver. The loss of silver during washing varies between textile technologies, and the resultant effect on the environment is not yet fully known.
History.
Silver has been used for thousands of years for ornaments and utensils, trade, and as the basis for many monetary systems. Its value as a precious metal was long considered second only to gold. The word "silver" appears in Anglo-Saxon in various spellings, such as "seolfor" and "siolfor". A similar form is seen throughout the Germanic languages (compare Old High German "silabar" and "silbir"). The chemical symbol Ag is from the Latin word for "silver", "argentum" (compare Greek άργυρος, "árgyros"), from the Indo-European root "*arg-", meaning "white" or "shining". Silver has been known since ancient times; it is mentioned in the Book of Genesis. Slag heaps found in Asia Minor and on the islands of the Aegean Sea indicate silver was being separated from lead as early as the 4th millennium BC using surface mining. One of the earliest silver extraction centres in Europe was Sardinia in early Chalcolithic.
The stability of the Roman currency relied to a high degree on the supply of silver bullion, which Roman miners produced on a scale unparalleled before the discovery of the New World. Reaching a peak production of 200 t per year, an estimated silver stock of 10,000 t circulated in the Roman economy in the middle of the second century AD, five to ten times larger than the combined amount of silver available to medieval Europe and the Caliphate around 800 AD. Financial officials of the Roman Empire worried about the loss of silver to pay for highly demanded silk from Sinica (China).
Mines were made in Laureion during 483 BC.
In the Gospels, Jesus' disciple Judas Iscariot is infamous for having taken a bribe of 30 coins of silver from religious leaders in Jerusalem to turn Jesus of Nazareth over to soldiers of the High Priest Caiaphas.
The Chinese Empire during most of its history primarily used silver as a means of exchange. In the 19th century, the threat to the balance of payments of the United Kingdom from Chinese merchants demanding payment in silver in exchange for tea, silk, and porcelain led to the Opium War because Britain had to find a way to address the imbalance in payments, and they decided to do so by selling opium produced in their colony of British India to China.
Islam permits Muslim men to wear silver rings on the little finger of either hand. Muhammad himself wore a silver signet ring.
In the Americas, high temperature silver-lead cupellation technology was developed by pre-Inca civilizations as early as AD 60–120.
World War II.
During World War II, the shortage of copper led to the substitution of silver in many industrial applications. The United States government loaned out silver from its massive reserve located in the West Point vaults to a wide range of industrial users. One very important use was for bus bars for new aluminum plants needed to make aircraft. During the war, many electrical connectors and switches were silver plated. Another use was aircraft master rod bearings and other types of bearings. Since silver can replace tin in solder at a lower volume, a large amount of tin was freed up for other uses by substituting government silver. Silver was also used as the reflector in searchlights and other types of lights. Silver was used in nickels during the war to save that metal for use in steel alloys.
The Manhattan Project to develop the atomic bomb used about 14,700 tons of silver borrowed from the United States Treasury for calutron windings for the electromagnetic separation process in the Y-12 National Security Complex at the Oak Ridge National Laboratory. The oval "racetracks" had silver bus bars with a cross-section of one square foot. 
After the war ended, the silver was returned to the vaults.
Occurrence and extraction.
Silver is found in native form, as an alloy with gold (electrum), and in ores containing sulfur, arsenic, antimony or chlorine. Ores include argentite (Ag2S), chlorargyrite (AgCl) which includes horn silver, and pyrargyrite (Ag3SbS3). The principal sources of silver are the ores of copper, copper-nickel, lead, and lead-zinc obtained from Peru, Bolivia, Mexico, China, Australia, Chile, Poland and Serbia. Peru, Bolivia and Mexico have been mining silver since 1546, and are still major world producers. Top silver-producing mines are Cannington (Australia), Fresnillo (Mexico), San Cristobal (Bolivia), Antamina (Peru), Rudna (Poland), and Penasquito (Mexico). Top near-term mine development projects through 2015 are Pascua Lama (Chile), Navidad (Argentina), Jaunicipio (Mexico), Malku Khota (Bolivia), and Hackett River (Canada). In Central Asia, Tajikistan is known to have some of the largest silver deposits in the world.
The metal is primarily produced as a byproduct of electrolytic copper refining, gold, nickel, and zinc refining, and by application of the Parkes process on lead metal obtained from lead ores that contain small amounts of silver. Commercial-grade fine silver is at least 99.9% pure, and purities greater than 99.999% are available. In 2011, Mexico was the top producer of silver (4,500 tonnes or 19% of the world's total), closely followed by Peru (4,000 t) and China (4,000 t).
Price.
As of 5 August 2014, the price of silver is US$649.28 per kilogram (US$20.1950 per troy ounce). This equates to approximately 1⁄64 the price of gold. The ratio has varied from 1⁄15 to 1⁄100 in the past 100 years. Physical silver bullion prices are higher than the paper prices, with premiums increasing when demand is high and local shortages occur.
In 1980, the silver price rose to a peak for modern times of US$49.45 per troy ounce (ozt) due to market manipulation of Nelson Bunker Hunt and Herbert Hunt. Inflation-adjusted to 2012, this is approximately US$138 per troy ounce. Some time after Silver Thursday, the price was back to $10/ozt. From 2001 to 2010, the price moved from $4.37 to $20.19 (average London US$/oz). According to the Silver Institute, silver's recent gains have greatly stemmed from a rise in investor interest and an increase in fabrication demand. In late April 2011, silver reached an all-time high of $49.76/ozt.
In earlier times, silver has commanded much higher prices. In the early 15th century, the price of silver is estimated to have surpassed $1,200 per ounce, based on 2011 dollars. The discovery of massive silver deposits in the New World during the succeeding centuries has been stated as a cause for its price to have diminished greatly.
The price of silver is important in Judaic law. The lowest fiscal amount a Jewish court, or Beth Din, can convene to adjudicate a case over is a "shova pruta" (value of a Babylonian "pruta" coin). This is fixed at .025 g of pure, unrefined silver, at market price. In a Jewish tradition, still continuing today, on the first birthday of a first-born son, the parents pay the price of five pure-silver coins to a "Kohen" (priest). Today, the Israel mint fixes the coins at 117 g of silver. The "Kohen" will often give those silver coins back as a gift for the child to inherit.
Human exposure and consumption.
Silver plays no known natural biological role in humans, and possible health effects of silver are a disputed subject. Silver itself is not toxic to humans, but most are. In large doses, silver and compounds containing it can be absorbed into the circulatory system and become deposited in various body tissues, leading to argyria, which results in a blue-grayish pigmentation of the skin, eyes, and mucous membranes. Argyria is rare, and although, so far as known, this condition does not otherwise harm a person's health, it is disfiguring and usually permanent. Mild forms of argyria are sometimes mistaken for cyanosis.
Monitoring exposure.
Overexposure to silver can occur in workers in the metallurgical industry, persons taking silver-containing dietary supplements, patients who have received silver sulfadiazine treatment, and individuals who accidentally or intentionally ingest silver salts. Silver concentrations in whole blood, plasma, serum, or urine may be measured to monitor for safety in exposed workers, to confirm the diagnosis in potential poisoning victims, or to assist in the forensic investigation in a case of fatal overdosage.
Use in food.
Silver is used in food coloring; it has the E174 designation and is approved in the European Union.
Traditional Indian dishes sometimes include the use of decorative silver foil known as "vark", and in various cultures, silver "dragée" are used to decorate cakes, cookies, and other dessert items. The use of silver as a food additive is not approved in the United States.
External links.
Listen to this article ()
This audio file was created from a revision of the "Silver" article dated 2005-09-01, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="27275" url="http://en.wikipedia.org/wiki?curid=27275" title="Transport in Saudi Arabia">
Transport in Saudi Arabia

With the arrival of petrodollar over the period of time, Kingdom of Saudi Arabia has initiated many mega infrastructure development projects in the country, and extensive development of transportation network has followed suit to support various economic developments. As a result, the country now boasts an extensive transportation network.
Transportation in Saudi Arabia.
There are several transportation modes in the Saudi Arabia.
Road transportation.
<br>Total:
221,372 km 
<br>Paved:
47,529 km (includes 3,891 km of expressways)
<br>Unpaved:
173,843 km (2006)
Roads in Saudi Arabia vary from eight laned roads to small two laned roads in rural areas. The city highways and other major highways are well maintained, specially the roads in the capital Riyadh. The roads have been constructed to resist the consistently high temperatures and do not reflect the strong sunshine. The outer city highways such as the one linking from coast to coast are not as great as the inner-city highways but the government is now working on rebuilding those roads. In October 2013, a group of auto enthusiasts drove some 2000 km through Saudi Arabia in search of best driving road, and named the Jeddah - Taif - Al-Hada highway as motoring nirvana.
Saudi Arabia encourages road transport as it has maintained one of the lowest petrol prices in the world, at $0.48 per gallon ($0.13 per liter).
Some of the important inter-city highways include the following:<br>
Sea transportation.
Saudi Arabia has a well development sea transport network developed primarily to support the transport of petrochemicals. Saudi Ports Authority is the ports management organization in the country, overseeing the operations.
The major ports in the country are as follows;
Air transportation.
There are an estimated 204 airports in Saudi Arabia (2003 est.).
Airports with paved runways.
<br>"total:"
71
<br>"over 3,047 m:"
32
<br>"2,438 to 3,047 m:"
13
<br>"1,524 to 2,437 m:"
12
<br>"914 to 1,523 m:"
2
<br>"under 914 m:"
2 (2003 est.)
Airports with unpaved runways.
<br>"total:"
133
<br>"over 3047 m:"
1
<br>"2,438 to 3,047 m:"
5
<br>"1,524 to 2,437 m:"
75
<br>"914 to 1,523 m:"
38
<br>"under 914 m:"
14 (2003 est.) (Saudia is the nation's flag carrier airline)
Heliports.
9 (2009 est.)
Rail transport.
As a result of over-reliance on road and air travel, the rail transport has not received a similar level of investment in Saudi Arabia. However, there are now plans to add more tracks and develop new railway routes.
The Saudi Railways Organization (SRO) is a state-owned company that operates Saudi Arabia's rail network.
SRO provides freight services on two main lines totalling 1018 km. These connect Riyadh with the port of Dammam on the coast of the Persian Gulf. SRO passenger trains operate between Riyadh and Dammam.
Land Bridge will connect Jeddah with Dammam in the east.
There are plans to extend the network to the Red Sea port of Jeddah and, eventually to the borders of Jordan, Yemen, and perhaps all the way to Egypt.
There is a large scale railway project Haramain High Speed Rail Project underway currently in the Western province, connecting Makkah with Jeddah and Madinah city. The primary objective of this railway line is to provide an alternative for the Muslim pilgrims travelling between the 3 cities.
Riyadh Metro is a metro being built and will be done in 2017.
Jeddah Metro will start soon.
A recent addition is the Makkah Metro Line B is part of the Makkah Metro rail transit system, which was developed in Makkah city. This is a 18.1 km track developed to shuttle a forecasted 8 million pilgrims between Mecca, Mount Arafat, Muzdalifa and Mina in the annual Hajj pilgrimage.

</doc>
<doc id="27303" url="http://en.wikipedia.org/wiki?curid=27303" title="Economy of Seychelles">
Economy of Seychelles

The economy of Seychelles is based on fishing, tourism, the processing of coconuts and vanilla, coir (coconut fiber) rope, boat building, printing, furniture and beverages. Agricultural products include cinnamon, sweet potatoes, cassava (tapioca), bananas, poultry and tuna.
The public sector, comprising the government and state-owned enterprises, dominates the economy in terms of employment and gross revenue, employing two-thirds of the labor force. Government consumption absorbs over one-third of the GDP.
Economic history.
The French originally settled the Seychelles in 1770, setting up plantations which relied heavily on slave labour to produce cotton, sugar, rice, and maize. The British took control of the Seychelles during the Napoleonic Wars without removing the French upper class. 
After the British prohibited slavery in 1835, the influx of African workers did not end because British warships captured Arab slavers and forced the liberated slaves to work on plantations as apprentices without pay.
In the 1960s, about 33% of the working population worked at plantations, and 20% worked in the public or government sector.
Advent of the tourist industry (1971).
Plantations were the main industry of the Seychelles until 1971, when the international airport opened. Overnight, tourism became a serious industry, basically dividing the economy into plantations and tourism. The tourism sector paid better, and the plantation economy could only expand so far. 
The plantation sector of the economy declined in prominence, and tourism and fishing became the primary industries of Seychelles. In the 1960s, about 33% of the working population worked at plantations, but by 2006 it was less than 3%.
While the tourism and industrial fishing industries were on a roll in the late 1990s, the traditional plantation economy atrophied. Cinnamon barks and copra—traditional export crops—dwindled to negligible amounts by 1991. There were no exports of copra in 1996; 318 tons of cinnamon bark was exported in 1996, reflecting a decrease of 35% in cinnamon bark exports from 1995.
The Indian Ocean Tracking Station on Mahé, was closed in August 1996 after the Seychelles government attempted to raise the rent to more than $10,000,000 per year.
Current economy.
Since Seychelles' independence in 1976, per capita output has expanded to roughly seven times the old near-subsistence level. Growth has been led by the tourist sector, which employs about 30% of the labor force and provides more than 70% of hard currency earnings, followed by tuna fishing. In recent years the government has encouraged foreign investment in order to upgrade hotels and other services. 
At the same time, the government has moved to reduce the dependence on tourism by promoting the development of farming, fishing, small-scale manufacturing and most recently the offshore sector. The vulnerability of the tourist sector was illustrated by the sharp drop in 1991-92 due largely to the Gulf War. Although the industry has rebounded, the government recognizes the continuing need for upgrading the sector in the face of stiff international competition. Other issues facing the government are the curbing of the budget deficit and further privatization of public enterprises.
Despite attempts to improve its agricultural base and emphasize locally manufactured products and indigenous materials, Seychelles imports 90% of the food it consumes. The exceptions are some fruits and vegetables, fish, poultry, pork, beer, cigarettes, paint, and a few locally made plastic items. Imports of all kind are controlled by the Seychelles Marketing Board (SMB), a government parastatal which operates all the major supermarkets and is the distributor and licensor of most other imports.
In an effort to increase agricultural self-sufficiency, Seychelles has undertaken steps to make the sector more productive and to provide incentives to farmers. Much of the state holdings in the agricultural sector have been privatized, while the role of the government has been reduced to conducting research and providing infrastructure.
Tourism and fishing.
Tourism is one of the most important sectors of the economy, accounting for approximately 16.6% (2000) of GDP. Employment, foreign earnings, construction, banking, and commerce are all dominated by tourism-related industries. Tourism earned $631 million in 1999-2000. About 130,046 tourists visited Seychelles in 2000, 80.1% of them from Europe (United Kingdom, Italy, France, Germany, and Switzerland).
In 2000, industrial fishing surpassed tourism as the most important foreign exchange earner. Manufacturing, construction, and industrial fishing, notably tuna fishing, account for about 28.8% of the GDP. Earnings are growing annually from licensing fees paid by foreign trawlers fishing in Seychelles' territorial waters. 
In 1995, Seychelles saw the privatization of the Seychelles Tuna Canning Factory, 60% of which was purchased by the American food company Heinz. Similarly, some port operations have been privatized, a trend that has been accompanied by a fall in transshipment fees and an increase in efficiency. Overall, this has sparked a recovery in port services following a drastic fall in 2009.
Manufacturing.
Many of the other industrial activities are limited to small scale manufacturing, particularly agro-processing and import substitution. Agriculture (including artisanal and forestry), once the backbone of the economy, now accounts for around 3% of the GDP. 
Public sector.
The public sector, comprising the government and state-owned enterprises, dominates the economy in terms of employment and gross revenue, employing two-thirds of the labor force. Public consumption absorbs over one-third of the GDP.
Vulnerability to external shocks.
The Seychelles economy is extremely vulnerable to external shocks. Not only does it depend on tourism, but it imports over 90% of its total primary and secondary production inputs. Any decline in tourism quickly translates into a fall in GDP, a decline in foreign exchange receipts, and budgetary difficulties. 
Economic growth.
Growth slowed in 1998–2001, due to sluggish tourist and tuna sectors. Also, tight controls on exchange rates and the scarcity of foreign exchange have impaired short-term economic prospects. The black market value of the Seychellois rupee is anywhere from two thirds to one half the official exchange rate. The next few years were also a bit slow due to the worldwide economic downturn and the fear of flying brought on by September 11, 2001. 
More recently though, tourism has roared back at a record pace setting successive records in 2006 and again in 2007 for number of visitors. The increased availability of flights to and from the archipelago due in part to new entrants Emirates and Qatar airlines is also beginning to show. New five star properties and the devaluation of the currency by nearly 33% by the Seychelles Government is having a positive influence on the tourism sector as well. Both at official exchange rates and at purchasing power parity (PPP), Seychelles remains the richest territory in Africa in terms of GDP per capita (US$9,440.095 at real exchange rates and US$17,560.062 at PPP 2008 estimate),
Because of economic contraction (the economy declined by about 2% in 2004 and 2005 and lost another 1.4% in 2006 according to the International Monetary Fund) the country was moving downwards in terms of per capita income. However, the economy bounced back in 2007, growing by 5.3% due in part to record tourism numbers and the booming building and offshore industries. The IMF forecast further growth in 2008 with continuing increase in the GDP per capita.
In October 2008, as tourism and fishing revenue began slowing, the Seychelles defaulted on a $230 million debt. The International Monetary Fund stepped with a two-year, $26 million rescue package. The rescue package came with a few stipulations; The country laid off 1,800 government workers, floated its currency, lifted foreign exchange controls and sold off state assets. At the time, the country's $800 million external foreign debt was equivalent to almost 175 percent of its gross domestic product.
The decision to let the currency trade freely as part of the IMF rescue package means that Seychelles is the smallest country in the world that has a completely independent currency - one that is neither pegged, nor an adopted foreign currency, nor a common currency used within a larger monetary union. When the Seychellois rupee became freely floated on November 3, 2008, its value quickly fell drastically, decreasing from eight per U.S. dollar to 16, effectively doubled the prices of imports.
The rupee traded at an average 19.97 per euro by noon in the capital Victoria, compared with 11.3421 last week, according to Caroline Abel, head of monetary analysis and statistics at the Central Bank of Seychelles. It traded at 15.58 per dollar, from 8.9090, she said. Against the pound, it dropped to 25.02, from 14.3227.
The global recession and piracy in the Indian Ocean hit Seychelles hard in 2009, with the GDP projected to contract by 7.5 percent. However the government exceeded its fiscal targets, with a primary surplus of 13.4% of GDP in the first nine months of 2009 according to the IMF. They report expenditure has been tightly controlled and revenue has held up well despite the difficult economic environment.
In May 2010 an International Monetary Fund mission visited Seychelles and concluded the country is making progress. The head of the mission, Mr. Jean Le Dem, said at the conclusion of the visit:
The economy is recovering from a recession that put real gross domestic product (GDP) to almost a stand in 2009. Real GDP is projected to grow at 4 percent in 2010, reflecting primarily a rebound in tourism earnings. Twelve-month inflation, which was negative during the past few months, is expected to return to about 1 percent by year-end
Financial Services.
In addition to the now booming tourism and building/real estate markets, Seychelles has renewed its commitment to developing its financial services sector. Government officials and industry participants believe this could overtake the tourism industry as the chief pillar of the economy by 2017. The recent passage of a revised Mutual Fund Act 2007, Securities Act 2007 and Insurance Act 2007 are meant to be the catalysts to move Seychelles from just another offshore jurisdiction to a full-fledged Offshore Financial Center (OFC).
The Ministry of Finance is responsible for economic decisions and budgetary policy. A separate Monetary Authority supervises the banking system and manages the money supply. Although foreign banks operate branches in Seychelles, the government owns the two local banks—the Development Bank of Seychelles, which mobilizes resources to fund development programs, and the Seychelles Saving Bank, a bank for savings and current accounts.
The Seychelles International Business Authority (SIBA) is charged with overseeing the quickly growing offshore industry.
Offshore oil and gas.
New detailed studies and exploration shows that the Seychelles potentially have large off-shore petroleum reservoirs which are yet to be discovered. Drills have proven the presence of:
However, to date all exploratory and stratigraphic test wells (a total of 9 since the 1970s) in the Seychelles have failed to find commercial hydrocarbons. The most recent wildcat by Enterprise Oil in 1995 detected gas but failed to find hydrocarbons.
Several oil and gas exploration companies are active in the Seychelles offshore. These include East African Exploration (EAX) (a subsidiary of Afren), Avana Petroleum (a subsidiary of Vanoil Energy) and .
Beginning at the turn of the millennium the Seychelles Petroleum Company (SEPEC) started to develop the first fleet of modern petroleum double-hull tankers (five vessels), which was completed by late 2007/early 2008 with the possibility to build more in the near future. The Seychelles President claims that this has opened the door to a new industry for his country and encouraged economic growth by further removing over-reliance on traditional trades like fisheries and tourism, which is now falling rapidly as the country's main income but nevertheless, has experienced significant growth in recent years.
Economic statistics.
GDP: <br>
official exchange rate - $0,921 billion (2008) <br>
purchasing power parity - $1,864 billion (2008)
GDP - real growth rate: <br>
+4,0% (2010 est.)<br>
-0,8% (2008)
GDP - per capita: <br>
official exchange rate - $12 068 (2007) <br>
purchasing power parity - $21 653 (2007)
GDP - composition by sector:
"agriculture:"
3,2%
"industry:"
30,4%
"services:"
66,4% (2005 estimate)
Population below poverty line: <br>
NA%
Household income or consumption by percentage share:
"lowest 10%:"
NA%
"highest 10%:"
NA%
Inflation rate (consumer prices): <br>
31,8% (2009) <br>
3,2% (2010 est.)
Labor force: <br>
39,560 (2006) <br>
26,000 (1996)
Labor force - by occupation: <br>
services: 74%, industry: 23%, agriculture: 3% (2006)<br>
services: 57%, industry: 19%, government: 14%, fishing, agriculture, and forestry: 10% (1989)
Unemployment rate: <br>
2% (2006 est.)
Budget: <br>
revenues: $183,9 million <br>
expenditures: $195,8 million (2009 est.)
Industries:<br>
fishing, tourism, processing of coconuts and vanilla, coir (coconut fiber) rope, boat building, printing, furniture; beverages
Industrial production growth rate: <br>
-2% (2009 est.)
Agriculture - products:<br>
coconuts, cinnamon, vanilla, sweet potatoes, cassava (tapioca), bananas; poultry; tuna
Exports: <br>
$495 million (2008 est.) <br>
$312 million (1998 est.)
Exports - commodities:<br>
canned tuna, frozen fish, cinnamon bark, copra, petroleum products (re-exports)
Exports - partners:<br>
UK 21.1%, France 19.1%, Mauritius 10.1%, Japan 7.9%, Italy 7.8%, Netherlands 6% (2008) <br>
UK 27.6%, France 15.8%, Spain 12.7%, Japan 8.6%, Italy 7.5%, Germany 5.6% (2004)
Imports:<br>
$658 million (2009 est.) <br>
$460 million (2005 est.)
Imports - commodities:<br>
machinery and equipment, foodstuffs, petroleum products
Imports - partners:<br>
Saudi Arabia 17.2%, Singapore 12.2%, France 10.1%, Spain 8%, Germany 7%, India 6.9%, South Africa 4.6% (2008)
Debt - external:<br>
$1,250 billion (2009 est.)<br>
$0,276 billion (2005 est.)
Economic aid - recipient:<br>
$16,4 million (1995)
Currency:<br>
1 Seychelles rupee (SCR or SRe) = 100 cents
Exchange rates:<br>
Seychelles rupees (SCR) per US$1 – 11.85 (May 2010), 16.7 (February 2009), 8.0 (2008), 5.8 (2007), 5.5 (2006), 5.3 (1999), 4.7 (1995)
Seychelles rupees (SCR) per Pound Sterling £1 –- 17.50 (May 2010), 24.11 (February 2009)
Seychelles rupees (SCR) per Euro €1 –- 15.04 (May 2010), 21.55 (February 2009)
Fiscal year:<br>
calendar year

</doc>
<doc id="27310" url="http://en.wikipedia.org/wiki?curid=27310" title="Geography of Sierra Leone">
Geography of Sierra Leone

Sierra Leone is located on the west coast of Africa, between the 7th and 10th parallels north of the equator. Sierra Leone is bordered by Guinea to the north and northeast, Liberia to the south and southeast, and the Atlantic Ocean to the west. 
Sierra Leone has a total area of 71740 km2, divided into a land area of 71620 km2 and water of 120 km2. 
Sierra Leone has four distinct geographical regions: coastal Guinean mangroves, the wooded hill country, an upland plateau, and the eastern mountains. Eastern Sierra Leone is an interior region of large plateaus interspersed with high mountains, where Mount Bintumani rises to 1948 m.
Physical geography.
Sierra Leone is located on the west coast of Africa, between the 7th and 10th parallels north of the equator. Sierra Leone is bordered by Guinea to the north and northeast, Liberia to the south and southeast, and the Atlantic Ocean to the west. The country has a total area of 71740 km2, divided into a land area of 71620 km2 and water of 120 km2. 
Sierra Leone has four distinct geographical regions: coastal Guinean mangroves, the wooded hill country, an upland plateau, and the eastern mountains. Eastern Sierra Leone is an interior region of large plateaus interspersed with high mountains, where Mount Bintumani rises to 1948 m.
Geology.
Sierra Leone can be split into three geological areas, in the east is part of the West African craton, the western area consists of the Rokelides, an orogenic belt, and a 20- to 30-km coastal strip of sediments.
Extreme points.
This is a list of the extreme points of Sierra Leone, the points that are farther north, south, east or west than any other location.
Climate.
The climate is tropical; although it could be classified as a tropical monsoon climate, it could also be describe as a climate that is transitional between a continually wet tropical rainforest climate and a tropical savanna climate. 
There are two seasons determining the agricultural cycle: the rainy season from May to November, and a dry season from December to May, which includes harmattan, when cool, dry winds blow in off the Sahara Desert and the night-time temperature can be as low as 16 °C. The average temperature is 26 °C and varies from around 26 °C to 36 °C during the year. 
Average rainfall is highest at the coast, 3000–5000 mm per year; moving inland this decreases and at the eastern border of the country, the average rainfall is 2000-2500mm.
Environment issues.
Rapid population growth in Sierra Leone has put pressure upon the natural environment. Environmental problems include the overharvesting of timber, the expansion of cattle grazing and slash and burn agriculture have resulted in deforestation and soil exhaustion, and overfishing.
Sierra Leone is party to several environmental agreements:
Signed, but not ratified:
General information.
Geographic coordinates: 
Land boundaries:
<br>"total:" 958 km
<br>"border countries:" Guinea 652 km, Liberia 306 km
Coastline: 402 km
Maritime claims:
<br>"territorial sea:" 200 nmi.
<br>"continental shelf:" 200 m depth or to the depth of exploitation.
Terrain: coastal belt of mangrove swamps, wooded hill country, upland plateau, mountains in east.
Elevation extremes:
<br>"lowest point:" Atlantic Ocean 0 m
<br>"highest point:" Loma Mansa (Bintimani) 1,948 m
Natural resources: diamonds, titanium ore, bauxite, iron ore, gold, chromite.
Land use:
<br>"arable land:" 7.95%
<br>"permanent crops:" 1.05%
<br>"other:" 91% (2005)
Irrigated land: 300 km²; (2003)
Natural hazards: dry, sand-laden harmattan winds blow from the Sahara (December to February); dust storms.

</doc>
<doc id="27321" url="http://en.wikipedia.org/wiki?curid=27321" title="Demographics of Singapore">
Demographics of Singapore

This article is about the demographic features of the population of Singapore, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
By end of June 2012, the island's population stood at 5.31 million. It is the second densest sovereign state in the world, after the microstate Monaco. Singapore is a multiracial and multicultural country with a majority population of Chinese (74.2% of the resident population), with substantial Malay (13.2%) and Indian minorities (9.2%). The Malays are recognised as the indigenous community although most are the descendants of post-1945 immigrants from Indonesia and Malaysia. 
Mahayana Buddhism is most widely adhered to in Singapore though followers do not represent the majority, with significant numbers following Islam, Christianity, Hinduism, Sikhism or no religion at all. The annual total population growth rate for the year 2012 was about 2.5%. The constitution says Malay is the national language. The other three official languages are English, Mandarin and Tamil. English is the main working language and is the mandatory first language in all schools in Singapore. 
Singapore’s resident total fertility rate (TFR) was of 1.2 in 2011; the Chinese, Malay and Indian fertility rate was 1.08, 1.64 and 1.09 respectively. In 2010, the Malay fertility rate was about 70% higher than that of Chinese and Indians. Singapore has attempted to boost the fertility rate for years to the replacement level of 2.1 births per woman.
Population.
An average population density of 53km² was found for the "World (land only, excluding Antarctica)" 
at Wikipedia's "List of sovereign states and dependent territories by population density" based
on data from July 5, 2014.
Population growth and population control.
After World War II, from 1947 to 1957, Singapore had a massive population increase. The birth rate rose and the death rate fell; the average annual growth rate was 4.4%, of which 1% was due to immigration; Singapore experienced its highest birth rate in 1957 at 42.7 per thousand individuals. (This was also the same year the United States saw its peak birth rate.) 
By 1960, the government publicly funded and supported family planning programmes; after independence in 1965, the birth rate had fallen to 29.5 per thousand individuals, and the natural growth rate had fallen to 2.5%.
Birth rates in the 1960s were still perceived as high by the government; on average, a baby was born every 11 minutes in 1965. Kandang Kerbau Hospital (KKH) — which specialised in women's health and was the most popular hospital to have children — saw over 100 deliveries per day in 1962. In 1966, KKH delivered 39835 babies, earning it a place in the Guinness Book of World Records for "largest number of births in a single maternity facility" for ten years. Because there was generally a massive shortage of beds in that era, mothers with routine deliveries were discharged from hospitals within 24 hours.
In September 1965 the Minister for Health, Yong Nyuk Lin, submitted a white paper to Parliament, recommending a "Five-year Mass Family Planning programme" that would reduce the birth rate to 20.0 per thousand individuals by 1970. In 1966, the Family Planning and Population Board (FPPB) had been established based on the findings of the white paper, providing clinical services and public education on family planning. 
By 1970, the "Stop at Two" campaign was firmly established, implementing incentives, disincentives and public exhortation to discourage families from having more than two children. After 1975, the fertility rate declined below replacement level, in a sign that Singapore was undergoing the demographic transition. In 1983, the "Graduate Mothers' Scheme" was implemented in an attempt to get educated women, especially women with a university degree, to marry and procreate, while the government encouraged women without an O-level degree to get sterilised. This was done out of the Lee Kuan Yew government's belief that for the nation to best develop and avoid hardship, the educated classes should be encouraged to contribute to the nation's breeding pool, while the uneducated should not, sparking the "Great Marriage Debate".
In 1986, the government reversed its population policy — except its stance on low-income, lowly-educated women — and initiated the Have Three or More (if you can afford it) campaign, offering cash and public administration incentives to have children. In 2001, the Singapore government started its Baby Bonus scheme.
As of 2012, Singapore total fertility rate (TFR) is 1.20 children born per woman, which represents a sub-replacement fertility rate and is one of the lowest in the world. Ethnic Chinese had a ferlility of 1.07 in 2004 (1.65 in 1990), while Malays had a TFR of 2.10 (2.69 in 1990). Both figures declined further in 2006. TFR for Indians was 1.30 in 2004 and 1.89 in 1990. The Singapore government has launched several highly publicized attempts to raise the fertility rate and increase awareness of the negative effects of an aging population, the elderly (65 and above) had constituted 9.9% of its population in 2012; this proportion is still significantly lower than that of many other developed nations, such as the United States (12%) and Japan (21.2%) .
Due to the continued low TFR, amongst other reasons, the Singapore government has varied its immigration policy over the years. As the demand for labour grew with industrialization, foreign talent with professional qualifications as well as less-skilled foreign workers has made up a significant and increasing proportion of Singapore's total population since the 2000s and 2010s.
2013 Population White Paper.
In early 2013, the Singapore parliament debated over the policies recommended by the Population White Paper entitled "A Sustainable Population for a Dynamic Singapore". Citing that Singapore's 900,000 Baby Boomers would comprise a quarter of the citizen population by 2030 and that its workforce would shrink "from 2020 onwards", the White Paper projected that by 2030, Singapore's "total population could range between 6.5 and 6.9 million", with resident population between 4.2 and 4.4 million and citizen population between 3.6 and 3.8 million. The White Paper called for an increase in the number of foreign workers so as to provide balance between the number of skilled and less-skilled workers, as well as provide healthcare and domestic services. It also claimed that foreign workers help businesses thrive when the economy is good. The motion was passed albeit after amendments made to leave out "population policy" and add focus on infrastructure and transport development.
The White Paper was criticized by opposition parties. Member of Parliament Low Thia Khiang of the Workers' Party of Singapore had criticized current measures of increasing the fertility rate, claiming that the high cost of living and lack of family and social support discouraged young couples from having babies. As for current immigration policies, he had noted that immigrants were a source of friction for Singaporeans and that an increased population would put more stress on the already strained urban infrastructure. On February 16, 2013, nearly 3,000 people rallied to protest the White Paper and raise concerns that the increased population would lead to the deterioration of public service and the increase of the cost of living in the future.
Ethnic groups.
Singapore became numerically dominated by immigrant ethnic groups soon after the British annexed the island in the 19th century. It is estimated that in January 1819, Singapore had about 880 Malays and aboriginal tribes and about 20 to 30 Chinese. In 1821, it was estimated that there were nearly 3,000 Malays and more than 1,000 Chinese.
While the Singapore Department of Statistics reports overall population figures for Singapore (4.48 million in 2006), as a matter of policy, it only provides more detailed demographic breakdown analysis for the approximately 80% of the population who are Singapore citizens and Permanent Residents (collectively termed 'residents'). Of this group of about 3.6 million people, Chinese form 75.2%, Malays form 13.6%, Indians form 8.8%, while Eurasians and other groups form 2.4%. No breakdown by ethnicity is released for the non-resident population.
Official figures show that the number of foreigners on short-term permits (termed 'non-residents') has grown from 30,900 in 1970 to 797,900 in 2005, which translate roughly to a 24-fold increase in 35 years, or from 1% of the population in 1970 to 18.3% in 2005. Despite this huge increase, no further breakdown is given by Singstat.
Some studies attempted to cast light on the demographic profile of Singapore's non-residents. According to 'The Encyclopedia of the Indian Diaspora' (published in 2006), "independent surveys approximate the number of South Asians on work permits to be between 30-35 per cent of the total 'Indian' population in Singapore, or approximately 90,000-100,000." Based on this, it can be estimated that, as of June 2006, the Indian population formed 12.5% of the non-resident population, and therefore numbered between 415,000 and 430,000, or about 9.5% of the total population of about 4.5 million. It is likely the population of 'others' is similarly greater than suggested by the figures for the 'resident' population. Conversely, it is likely that the Chinese form significantly less than 75% of the total population of 4.5 million.
A figure released by the Straits Times on 20 July 2010 shows that the total population of non- resident Singaporeans (PRs and foreigners) is around 1.79 million of which Indians are 400,000 (22.35%). The number of Indian PRs and foreigners had doubled in the previous 2 years.
Languages.
There are four official languages: English, Malay, Mandarin and Tamil.
Malay is the national language of the country, although English is mainly used. English serves as the link between the different ethnic groups and is the language of the educational system and the administration. The colloquial English used in everyday life is often referred to as Singlish.
The government of Singapore has been promoting the use of Mandarin, the official form of Chinese in Singapore as well as mainland China and Taiwan, with its Speak Mandarin Campaign among the Chinese population. The use of other Chinese dialects, like Hokkien, Teochew, Cantonese, Hainanese and Hakka, has been declining over the last two decades, although they are still being used especially by the older generations of the Chinese population.
About 60% of Singapore's Indian population speaks Tamil as their native language. Other widely spoken Indian languages are Punjabi, Malayalam, Hindi and Telugu.
Around 5,000 Peranakans, the early Chinese population of the region, still use the Hokkien-influenced Malay dialect called Baba Malay.
Religion.
Singapore generally allows religious freedom, although the authorities restrict or ban some religious sects (such as Jehovah's Witnesses, due to their opposition to National Service). The majority of Malays are Muslim, the plurality of Chinese practise Buddhism and syncretic Chinese folk traditions. Christianity is growing among the Chinese, having overtaken Taoism as second most important religion among this ethnic group. Indians are mostly Hindus though many are Muslims, Sikhs, Buddhists and Christians. People who practise no religion form the third-largest group in Singapore.
Religions of the main ethnic groups (2000):
"Source: Census 2000."
Marriage and divorce.
Source: Singapore Department of Statistics.
The divorce rate has doubled over the last decade, and as of 2003, for every ten marriages registered in Singapore, almost three ended in divorce. The Women's Charter protects the women's financial interests during a divorce, often requiring the husband to contribute to his divorced wife and their children.
Literacy and education.
Among residents aged 25–39 years, the percentage of university graduates increased from 23.7% in 2001 to 45.9% in 2011 while that who had attained a diploma or professional qualification increased from 15.9% to 22.9% over the same period.
Employment.
In 2005, the unemployment rate for persons aged 15 years and over was 2.5%, the lowest in the last four years, with a labour force of 2.3 million people.
Household income.
Average household monthly income.
The average household monthly income was SGD 4,943 in 2000, which was an increase of $3,080 in 1990 at an average annual rate of 4.9%. The average household income experienced a drop of 2.7% in 1999 due to economic slowdown. Measured in 1990 dollars, the average household monthly income rose from SGD$3,080 in 1990 to SGD$4,170 in 2000 at an average annual rate of 2.8%.
Growth in household income by decile.
With the recovery from the 1998 economic slowdown, household income growth had resumed for the majority of households in 2000. However, for the lowest two deciles, the average household income in 2000 had declined compared with 1999. This was mainly due to the increase in the proportion of households with no income earner from 75% in 1999 to 87% in 2000 for the lowest 10%. Households with no income earner include those with retired elderly persons as well as unemployed members.
Household income disparity.
The disparity in household income had widened in 2000, reflecting the faster income growth for the higher-income households. The Gini coefficient, a measure of income inequality, rose from 0.446 in 1998 to 0.481 in 2000. Other measures of income inequality also indicated similar trend of increasing disparity in household income. In the United Nations Development Programme Report 2004, Singapore's Gini coefficient based on income is 0.425 in 1998, which is ranked 78 among 127 countries in income equality (see list of countries by income equality).

</doc>
<doc id="27349" url="http://en.wikipedia.org/wiki?curid=27349" title="History of Solomon Islands">
History of Solomon Islands

Solomon Islands is a sovereign state in the Melanesia subregion of Oceania in the western Pacific Ocean.
This page is about the history of the nation state rather than the broader geographical area of the Solomon Islands archipelago, which covers both Solomon Islands and Bougainville Island, a province of Papua New Guinea. For the history of the archipelago not covered here refer to the former administration of the British Solomon Islands Protectorate, the North Solomon Islands and the History of Bougainville.
Earliest inhabitants.
The human history of Solomon Islands begins with the first Papuan settlement at least 30,000 years ago from New Guinea. They represented the furthest expansion of humans into the Pacific until the expansion of Austronesian-language speakers through the area around 4000 BC, bringing new agricultural and maritime technology. Most of the languages spoken today in Solomon Islands derive from this era, but some thirty languages of the pre-Austronesian settlers survive "(see East Papuan languages)."
There are preserved numerous pre-European cultural monuments in Solomon Islands, notably Bao megalithic shrine complex (13th century AD), Nusa Roviana fortress and shrines (14th - 19th century), Vonavona Skull island - all in Western province. Nusa Roviana fortress, shrines and surrounding villages served as a hub of regional trade networks in 17th - 19th centuries. Skull shrines of Nusa Roviana are sites of legends. Better known is Tiola shrine - site of legendary stone dog which turned towards the direction where enemy of Roviana was coming from. This complex of archaeological monuments characterises fast development of local Roviana culture, through trade and head hunting expeditions turning into regional power in 17th - 18th centuries.
European contact.
Ships of the Spanish explorer Álvaro de Mendaña de Neira first sighted Santa Isabel island on 6 February 1568. Finding signs of alluvial gold on Guadalcanal, Mendaña believed he had found the source of King Solomon's wealth, and consequently named the islands "The Islands of Solomon". 
In 1595 and 1605 Spain again sent several expeditions to find the islands and establish a colony, however these were unsuccessful. In 1767 Captain Philip Carteret rediscovered the Santa Cruz Islands and Malaita. Later, Dutch, French and British navigators visited the islands; their reception was often hostile.
Colonization.
Sikaiana, then known as the Stewart Islands, was annexed to the Hawaiian Kingdom in 1856. Hawai'i did not formalize the annexation, and the United States refused to recognize Hawaiian sovereignty over Sikaiana when the United States annexed Hawai'i in 1898.
Missionary activity then started at the mid 19th century and European colonial ambitions led to the establishment of a German Protectorate over the North Solomon Islands, which covered parts of what is now Solomon Islands, following an Anglo-German Treaty of 1886. A British Solomon Islands Protectorate over the southern islands was proclaimed in June 1893. German interests were transferred to the United Kingdom under the Samoa Tripartite Convention of 1899, in exchange for recognition of the German claim to Western Samoa.
World War II.
Japanese forces occupied the Solomon Islands in January 1942. The counter-attack was led by the United States; the 1st Division of the US Marine Corps landed on Guadalcanal and Tulagi in August 1942. Some of the most bitter fighting of World War II took place on the islands for almost three years.
Tulagi, the seat of the British administration on the island of Nggela Sule in Central Province was destroyed in the heavy fighting following landings by the US Marines. Then the tough battle for Guadalcanal, which was centred on the capture of the airfield, Henderson field, led to the development of the adjacent town of Honiara as the United States logistics centre.
Biuku Gasa and Eroni Kumana.
Islanders Biuku Gasa (deceased 2005) and Eroni Kumana (Gizo) were Allies scouts during the war. They became famous when they were noted by National Geographic for being the first men to find the shipwrecked John F. Kennedy and his crew of the PT-109 using a traditional dugout canoe. They suggested the idea of using a coconut which was later kept on the desk of the president to write a rescue message for delivery. Their names had not been credited in most movie and historical accounts, and they were turned back before they could visit President Kennedy's inauguration, though the Australian coastwatcher would also meet the president. They were visited by a member of the Kennedy family in 2002, where they lived in traditional huts without electricity.
War consequences.
The impact of the war on islanders was profound. The destruction caused by the fighting and the longer-term consequences of the introduction of modern materials, machinery and western cultural artefacts, transformed traditional isolated island ways of life. The reconstruction was slow in the absence of war reparations and with the destruction of the pre-war plantations, formerly the mainstay of the economy. Significantly, Solomon Islanders experience as labourers with the Allies led some to a new appreciation of the importance of economic organisation and trade as the basis for material advancement. Some of these ideas were put into practice in the early post-war political movement "Maasina Ruru" - often corrupted to "Marching Rule".
Post war (1945-1978).
Stability was restored during the 1950s, as the British colonial administration built a network of official local councils. On this platform Solomon Islanders with experience on the local councils started participation in central government, initially through the bureaucracy and then, from 1960, through the newly established Legislative and Executive Councils. Positions on both Councils were initially appointed by the High Commissioner of the British Protectorate but progressively more of the positions were directly elected or appointed by electoral colleges formed by the local councils. The first national election was held in 1964 for the seat of Honiara, and by 1967 the first general election was held for all but one of the 15 representative seats on the Legislative Council (the one exception was the seat for the Eastern Outer Islands, which was again appointed by electoral college).
Elections were held again in 1970 and a new constitution was introduced. The 1970 constitution replaced the Legislative and Executive Councils with a single Governing Council. It also established a 'committee system of government' where all members of the Council sat on one or more of five committees. The aim of this system was to reduce divisions between elected representatives and the colonial bureaucracy, provide opportunities for training new representatives in managing the responsibilities of government. 
It was also claimed that this system was more consistent with the Melanesian style of government, however this was quickly undermined by opposition to the 1970 constitution and the committee system by elected members of the council. As a result, a new constitution was introduced in 1974 which established a standard Westminster form of government and gave the Islanders both Chief Ministerial and Cabinet responsibilities. Solomon Mamaloni became the country's first Chief Minister in July 1974.
Independence (1978).
As late as 1970, the British Protectorate did not envisage independence for Solomon Islands in the foreseeable future. Shortly thereafter, the financial costs of supporting the Protectorate became more trying, as the world economy was hit by the first oil price shock of 1973. The imminent independence of Papua New Guinea (in 1975) was also thought to have influenced the Protectorate's administrators. 
Outside of a very small educated elite in Honiara, there was little in the way of an indigenous independence movement in the Solomons. Self-government was granted in January 1976 and after July 1976, Sir Peter Kenilorea became the Chief Minister who would lead the country to independence. Independence was granted on 7 July 1978, and Kenilorea automatically became the country's first Prime Minister.
Ethnic violence (1999-2003).
In early 1999 long-simmering tensions between the local Gwale people on Guadalcanal and more recent migrants from the neighbouring island of Malaita erupted into violence. The ‘Guadalcanal Revolutionary Army’, later called Isatabu Freedom Movement (IFM), began terrorising Malaitans in the rural areas of the island to make them leave their homes. About 20,000 Malaitans fled to the capital and others returned to their home island; Gwale residents of Honiara fled. The city became a Malaitan enclave.
Meanwhile, the Malaita Eagle Force (MEF) was formed to uphold Malaitan interests. The Government appealed to the Commonwealth Secretary General for assistance. The Honiara Peace Accord was agreed on 28 June 1999. Despite this apparent success the underlying problems remained unresolved and had already resulted in the death or serious injury of 30,000 civilians. The accord soon broke down and fighting broke out again in June 2000.
Malaitans took over some armouries at their home island and Honiara and helped by that, on 5 June 2000 the MEF seized the parliament by force. Through their spokesman Andrew Nori, they claimed that the government of the then Prime Minister, Bartholomew Ulufa'alu, had failed to secure compensation for loss of Malaitan life and property. Ulufa’alu was forced to step down.
On 30 June 2000 Parliament elected by a narrow margin a new Prime Minister, Manasseh Sogavare. He established a Coalition for National Unity, Reconciliation and Peace, which released a program of action focused on resolving the ethnic conflict, restoring the economy and distributing the benefits of development more equally. However, Sogavare’s government was deeply corrupt and its actions led to the downward economic spiral and the deterioration of law and order.
The conflict was foremost about access to land and other resources and was centered around Honiara. Since the beginning of the civil war it is estimated that 100 have been killed. About 30,000 refugees, mainly Malaitans, had to leave their homes, and economic activity on Guadalcanal was severely disrupted.
Continuing civil unrest led to an almost complete breakdown in normal activity: civil servants remained unpaid for months at a time, and cabinet meetings had to be held in secret to prevent local warlords from interfering. The security forces were unable to reassert control, largely because many police and security personnel are associated with one or another of the rival gangs.
In July 2003 the Governor General of Solomon Islands issued an official request for international help, which was subsequently endorsed by a unanimous vote of the parliament. Technically, only the Governor General's request for troops was necessary. However, the government then passed legislation to provide the international force with greater powers and resolve some legal ambiguities.
On 6 July 2003, in response to a proposal to send 300 police and 2,000 troops from Australia, New Zealand, Fiji and Papua New Guinea to Guadalcanal, warlord Harold Keke announced a ceasefire by faxing a signed copy of the announcement to the Solomons Prime Minister, Allan Kemakeza. Keke ostensibly leads the Guadalcanal Liberation Front, but has been described as marauding bandit based on the isolated southwestern coast (Weather Coast) of Guadalcanal. Despite this ceasefire, on 11 July 2003 the Solomon Islands Broadcasting Corporation broadcast unconfirmed reports that supporters of Harold Keke razed two villages.
In mid-July 2003, the Solomons parliament voted unanimously in favour of the proposed intervention. The international force began gathering at a training facility in Townsville. In August 2003, an international peacekeeping force, known as the Regional Assistance Mission to Solomon Islands (RAMSI) and Operation Helpem Fren, entered the islands. Australia committed the largest number of security personnel, but with substantial numbers also from other South Pacific Forum countries such as New Zealand, Fiji, and Papua New Guinea (PNG). It acts as an interim police force and is responsible for restoring law and order in the country because the Royal Solomon Islands Police force failed to do so for a variety of reasons. Peacekeeping forces have been successful in improving the country's overall security conditions, including brokering the surrender of a notorious warlord Harold Keke in August 2003.
In 2009, the government is scheduled to set up a Truth and Reconciliation Commission, with the assistance of South African Archbishop Desmond Tutu, to "address people’s traumatic experiences during the five year ethnic conflict on Guadalcanal".
The government continues to face serious problems, including an uncertain economic outlook, deforestation, and malaria control. At one point, prior to the deployment of RAMSI forces, the country was facing a serious financial crisis. While economic conditions are improving, the situation remains unstable.
Cyclones.
In 1992, Cyclone Tia struck the island of Tikopia, wiping out most housing and food crops.
In 1997, the Government asked for help from the USA and Japan to clean up more than 50 sunken World War II shipwrecks polluting coral reefs and killing marine life.
In December 2002, Severe Tropical Cyclone Zoe struck the island of Tikopia and Anuta, cutting off contact with the 3,000 inhabitants. Due to funding problems, the Solomon Islands government could not send relief until the Australian government provided funding.
Cyclone Ita.
In April 2014 the islands were struck by the tropical low that later became Cyclone Ita. 
Throughout the Solomons, at least 23 people were killed while up to 40 others remained unaccounted for as of 6 April. An estimated 49,000 people were affected by the floods, of whom 9,000 were left homeless.
As the precursor tropical low to Ita affected the Islands, local authorities issued heavy flood warnings, tropical disturbance and cyclone watches.
Nearly two days of continuous heavy rains from the storm caused flash flooding in the Islands. Over a four-day span, more than 1000 mm fell at the Gold Ridge mine in Guadalcanal, with 500 mm falling in a 24 hour-span. The Matanikau River, which runs through the capital city Honiara, broke its banks on 3 April and devastated nearby communities. Thousands of homes along with the city's two main bridges were washed away, stranding numerous residents. The national hospital had to evacuate 500 patients to other facilities due to flooding. Graham Kenna from Save the Children stated that, "the scale of destruction is like something never seen before in the Solomon Islands." According to Permanent Secretary Melchoir Mataki, the majority of homes destroyed in Honiara were built on a flood plain where construction was not allowed.
Severe flooding took place on Guadalcanal. Immediately following the floods, Honiara and Guadalcanal were declared disaster areas by the Solomon Government. Debris left behind by the floods initially hampered relief efforts, with the runway at Honiara International Airport blocked by two destroyed homes. Food supplies started running low as the Red Cross provided aid to the thousands homeless. The airport was reopened on 6 April, allowing for supplies from Australia and New Zealand to be delivered. Roughly 20 percent of Honiara's population relocated to evacuation centers as entire communities were swept away. There were fears that the flooding could worsen an already ongoing dengue fever outbreak and cause outbreaks of diarrhea and conjunctivitis.
New Zealand offered an immediate NZ$300,000 in funds and deployed a C-130 Hercules with supplies and emergency response personnel. Australia donated A$250,000 on 6 April and sent engineers and response teams to aid in relief efforts. On 8 April, Australia increased its aid package to A$3 million while New Zealand provided an additional NZ$1.2 million. Taiwan provided US$200,000 in funds.
On 4 April (5 April local time) a 6.0 MW earthquake, with its epicenter on Makira island, struck the Islands. Though no reports of damage were received in relation to it, officials were concerned about possible landslides.

</doc>
<doc id="27351" url="http://en.wikipedia.org/wiki?curid=27351" title="Demographics of the Solomon Islands">
Demographics of the Solomon Islands

This article is about the demographic features of the population of Solomon Islands, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
The Solomon Islanders comprise diverse cultures, languages, and customs. Of its 552,438 persons, 94.5% are Melanesian, 3% Polynesian, and 1.2% Micronesian. In addition, small numbers of Europeans and Chinese are registered. About 120 vernaculars are spoken.
Most people reside in small, widely dispersed settlements along the coasts. Sixty percent live in localities with fewer than 200 persons, and only 10% reside in urban areas.
The capital city of Honiara, situated on Guadalcanal, the largest island, has over 30,000 inhabitants. The other principal towns are Gizo, Auki, and Kirakira.
Most Solomon Islanders are Christian, with the Anglican, Methodist, Roman Catholic, South Seas Evangelical, and Seventh-day Adventist faiths predominating. About 5% of the population maintain traditional beliefs.
The chief characteristics of the traditional Melanesian social structure are:
Most Solomon Islanders maintain this traditional social structure and find their roots in village life.
The World Factbook demographic statistics.
The following demographic statistics are from The World Factbook, unless otherwise indicated.
Population statistics.
571,890 (2011 est.)
Population growth rate.
2.17% (2012 est.)
Birth rate.
27.46 births/1,000 population (2012 est.)
Death rate.
3.91 deaths/1,000 population (2012 est.)
Net migration rate.
-1.86 migrant(s)/1,000 population (2012 est.)
Urbanization.
"urban population:" 19% of total population (2010)
"rate of urbanization:" 4.2% annual rate of change (2010-2015)
Human sex ratio.
"at birth:"
1.05 male(s)/female
"under 15 years:"
1.06 male(s)/female
"15-64 years:"
1.04 male(s)/female
"65 years and over:"
0.95 male(s)/female
"total population:"
1.04 male(s)/female (2012 est.)
Infant mortality rate.
17.25 deaths/1,000 live births (2012 est.)
Life expectancy at birth.
"total population:" 74.24 years
"male:" 71.83 years
"female:" 77.14 years (2012 est.)
Total fertility rate.
3.51 children born/woman (2012 est.)
Health expenditure.
5.4% of GDP (2009)
Physicians density.
0.186 physicians/1,000 population (2005)
Hospital bed density.
1.4 beds/1,000 population (2005)
Nationality.
"noun:"
Solomon Islander(s)
"adjective:"
Solomon Islander
Ethnic groups.
Melanesian 94.5%, Polynesian 3%, Micronesian 1.2%, European 0.8%, Chinese 0.3%, other 0.4%
Religions.
Anglican 34%, Roman Catholic 19%, Baptist 17%, United (Methodist/Presbyterian) 11%, Seventh-day Adventist 10%, other Protestant 5%, indigenous beliefs 4% - see Religion in the Solomon Islands
Languages.
Pijin is the lingua franca in much of the country. English is the official language but is spoken by only 1–2% of the population.
There are 68 .
Literacy.
"definition:"
NA
"total population:"
NA
"male:"
NA
"female:"
NA

</doc>
<doc id="27354" url="http://en.wikipedia.org/wiki?curid=27354" title="Telecommunications in the Solomon Islands">
Telecommunications in the Solomon Islands

Communications in the Solomon Islands.
Communications by type.
Telephones - main lines in use:
13,000 (2009)
Telephones - mobile cellular:
55,000 (2009). A new mobile telecommunications operator, Bemobile, commenced operations in 2010.
Telephone system:
<br>"domestic:"
GSM mobile phone network covering all provincial capitals along with several other townships and villages and a landline system covering all provincial capitals and some townships.
<br>Backhaul is principally via satellite along with some microwave and fibre optic.
<br>"international:"
satellite earth stations based in Honiara and Gizo - Intelsat (Pacific Ocean).
<br>Solomon Islands has no undersea fibre connection.
Radio broadcast stations:
AM 3, FM 3 Paoa FM, ZFM100, Wan FM and Gold FM, shortwave 1 (2002). The Solomon Islands Broadcasting Corporation, founded in 1976, transmits regular programming.
Radio
Receivers: 57,000 (1997)
In addition to regular broadcast stations, several shortwave utility station networks exist, such as the Church of Melanesia network.
Television broadcast stations:
1 (2009). Transmitters are located in Honiara, Auki and Gizo, programming is a mixture of local news and other locally generated content in Solomon Islands Pidgin and English, along with content from Australia Network, Television New Zealand and Aljazeera.
<br>In Honiara BBC World and Australia Network are also broadcast free-to-air on separate channels.
Televisions:
3,000 (1997)
Internet Service Providers (ISPs):
Solomon Telekom http://www.telekom.com.sb
SATSOL http://www.satsol.tv/internet.html
People First Network (PFnet) operate a community email network with 17 rural access points (Apr 2005) http://www.peoplefirst.net.sb/general/pfnet.htm 
PFnet is also establishing a VSAT network of distance learning centres in rural community schools under an EU-funded project http://www.peoplefirst.net.sb/dlcp 
It is hoped each host school will contribute to the Wikipedia or create their own wikis.
Country code (Top level domain): SB

</doc>
<doc id="27417" url="http://en.wikipedia.org/wiki?curid=27417" title="Transport in Sri Lanka">
Transport in Sri Lanka

Transport in Sri Lanka is based mainly on the road network which is centred on Sri Lanka's capital, Colombo. There is also a railway network, but it is largely a legacy of British colonial rule and today only handles a small fraction of the country's transport needs. There are navigable waterways, harbours and two international airports located in Katunayake, 22 miles north of Colombo and in Hambantota. The highways and roadways around the country are in very good condition and are being upgraded.
Railway network.
Rail transport in Sri Lanka consists of a heavy-rail intercity network connecting major population centres and commuter rail serving Colombo commuter traffic. Sri Lanka Railways operates the country’s railway network, which includes about 1450 km of track. Colombo is the main node of the network. Train routes connect the main cities of all nine provinces in the country.
Most of the railways were developed during the British colonial period, with the first line from Colombo to Kandy opening on 26 April 1867. The British introduced the railway as a cheap means of transporting the goods produced in the British-owned tea, rubber and coconut plantations, situated away from the main port in Colombo. Hence, the legacy rail network was suited for the distribution from plantations.
After independence from Britain, the Sri Lankan economy became focused more on industries than plantation agriculture. The road network also grew, and with the introduction of lorries, which were a faster means of transporting goods, the amount of goods transported by the railways declined. As the railway network is more focused on plantation areas and not on population and service centres, the railways have become an enterprise generating a heavy loss.
The railway is currently modernising and extending the Coast Line to facilitate faster trains and improved efficiency. Electrification of the busiest sections of the network was proposed in 2010, to improve energy efficiency and sustainability, but no work was carried out. The Railway is currently extending the Coastal line from Matara to Kataragama, via Hambantota.
Destinations.
The Sri Lankan railway network covers one of the most scenic landscapes in the world, the best of which is the Colombo-Badulla main line which runs hugging the steep mountains of the Sri Lankan highlands. The railways connect the main cities of Kandy, Galle, Matara, Anuradhapura, Gampaha, Negombo, Kurunegala, Avissawella, Kalutara, Polonnaruwa, Batticaloa, Trincomalee, Badulla, Gampola, Nawalapitiya, Matale, Vavuniya, Puttalam and Chilaw with the Capital Colombo. The lines to Jaffna, Kankesanturai and Mannar have been destroyed by the LTTE. There were also narrow gauge lines from Nanu Oya to Nuwara Eliya, Avissawella to Yatiyantota and Avissawella to Ratnapura and Opanayaka, which were dismantled due to financial losses from their operation.
The narrow-gauge Kelani Valley Line, from Colombo to Avissawella, was converted to broad gauge. In the 1970s the bridges and culverts on the line were strengthened to make the change to broad gauge, but the actual conversion was not made until the 1990s.
The potential for expansion was revealed when in 1974 the Minister of Transport, Leslie Goonewardena, opened an extension of the Coastal Line from Puttalam to Aruvakalu, to serve the cement factory there. 
In 2007, the Sri Lankan government announced plans for Matara - Kataragama (113 km), Padukka - Hambantota - Ratnapura (210 km), Kurunegala - Dambulla - Habarana (80 km) and Panadura - Horana (18 km) lines by 2014.
Road transport.
Road transport accounts for about 93 per cent of the land transport in Sri Lanka. There are 12,000 km of A class and B class roads and 151.8 km of expressways, as of Oct 2013.
Classification.
The Road transport network is divided in A, B, C and E class roads.
Expressways.
The Colombo–Matara Expressway is a 126 km long motorway linking Colombo, Galle, and Matara. It was built to bolster the economy of the Southern Province. Other expressways are either under construction or proposed. The Colombo–Katunayake Expressway, Colombo-Kandy Expressway, and Outer Circular Expressway (Colombo bypass road) are currently under construction. Colombo–Padeniya Expressway has been proposed. Also, Sri Lankan government has proposed to build three elevated highways connecting the three main expressways.
National Highways.
The National Highwys of the country are classified as 'A' and 'B' class roads. A class roads are further divided into 'AA', 'AB' & 'AC' classes.
The road density is highest in the southwest, especially in the area around Colombo.
Highways are in good condition, with a smooth bitumen surface and road markings. The road network is at its densest around Colombo and its suburbs. Some rural roads are in poor condition. The roads that are most widely used across the country are being upgraded and repaved. In many rural areas, public transport is widely available, even in areas where operation is unprofitable.
Bus transport.
Buses are the principal mode of public transport. Bus services are provided by the state-run Sri Lanka Transport Board (SLTB) and by privately run buses. SLTB serves both urban and rural routes. In many rural areas, it provides services in unprofitable areas that would be unattractive to private operators.
Colombo has an extensive public transport system based on buses. The "Central Bus Stand" in Pettah functions as the primary hub for bus transport in Colombo. The road network in Colombo consists of radial links (or arterial routes), which link the city centre and district centres, and orbital links, which intersect the arterial routes; most bus routes run along the radial links without the benefit of dedicated bus lanes, owing to the high volume of traffic at peak times. A BRT system for Colombo has been proposed, but has yet to be implemented.
Inter-city routes connect many of the major population centres in the country. A few services are available on the E01 Expressway. Expressway services use modern Lanka Ashok Leyland buses.
In 2011, the SLTB began introducing new buses to replace part of its aging fleet. These new Volvo 8400 buses from Volvo India, serve on major routes in Colombo city.
Waterways.
Sri Lanka has 160 km of inland waterways (primarily on rivers in the southwest), navigable by shallow-draught boats.
Pipelines.
Sri Lanka has 62 km of pipelines for crude oil and petroleum products (1987 figures).
Ports and Harbours.
Sri Lanka has deep-water ports at Colombo, Hambantota, Galle, and Trincomalee. Of these, Colombo handles the highest volume of cargo.
Colombo Port.
Capacity of the port is estimated at 4.1 TEU's In 2008, the port commenced a large-scale expansion project at a cost of US$1.2 billion, which is expected dramatically increase the port's capacity and capabilities. The project, which is headed by the Sri Lanka Ports Authority and built by the Hyundai Engineering & Construction Company, is expected to be completed by 11 April 2012. The expansion project will consist of four new terminals that are 1200m in length and can accommodate 3 berths each, alongside a depth of 18 m (59 ft) (which can be deepened to 23 m (75 ft)). The channel width of the harbour is to be 560m and depth of 20m, with harbour basin depth of 18m and a 600m turning circle. Once completed, it will increase the annual container handling capacity from 4 million TEUs to approximately 12 million TEUs. It will also be able to accommodate larger container vessels, carrying around 12,000 TEUs.
Hambantota Port.
Construction of the Hambantota port began in January 2008. It will be Sri Lanka’s largest port, after the Port of Colombo. The Port of Hambantota will serve ships travelling along one of world's most busiest shipping lines - the east-west shipping route which passes six to ten nautical miles (19 km) south of Hambantota. The first phase of the Port of Hambantota will consist of two 600 m general purpose berths, a 310 m bunkering berth and a 120 m small craft berth. It will also contain a bunkering facility and tank farm which will include 8 tanks for marine fuel, 3 tanks containing aviation fuel and 3 for Liquid Petroleum Gas (LPG). A 15 floor administrative complex will also be constructed as part of the project. Later phases will raise capacity of the port up to 20 million TEUs per year. When completed, the port will be the biggest port constructed on land to date in the 21st century.
Dikkowitta Fishery Harbour.
It is located in Wattala, Gampaha in Western Province. The project cost is estimated as $73 million. It is strategically located with close proximity to Colombo port and airport. It will be the largest fisheries harbour in Asia. It provides unloading and packing facilities to cater fish importing countries (EU, Japan, U.S.A) requirements. It will serve as an alternative site for Mutwal fishery harbour.
Main Facilities include Southern basin for export oriented fishing vessels, Northern basin for local registered fishing vessels, a servicing facility for boat repairs, cleaning and lifting and a fish processing facility with 3 cold rooms.
Kankesanthurai Port.
There is a harbour at Kankesanturai, north of Jaffna, navigable by ships of relatively shallow draught which was not active during the civil war period. The Kankesanturai Port is being restored and deepened with the help of India.
Merchant marine.
Total: 21 ships ( gross register tons (GRT) or over) totaling  GRT/ tonnes deadweight (DWT)
Ships by type: bulk carrier 4, cargo ship 13, chemical tanker 1, container ship 1, petroleum tanker 2 (2010).
Aviation.
Sri Lanka's international airports include Colombo Bandaranaike International Airport, Mattala Rajapaksa International Airport and the under-renovation Ratmalana International Airport.
Sri Lankan Airlines.
Sri Lankan Airlines is the national airline. Founded in 1979 as Air Lanka, the airline changed its name when it came under partial foreign ownership in 1998.It operates to destinations in Asia and Europe from its base and hub at Bandaranaike International Airport in the Sri Lankan capital, Colombo. The airline's head office is in the Airline Centre, on the grounds of Bandaranaike International Airport. The airline is set to join the Oneworld alliance in 2013.
SriLankan currently serves 62 destinations in 34 countries. Mihin Lanka is a low-fare airline owned by SriLankan.
Airports.
Bandaranaike International Airport is located in Katunayake, 35 km (22 mi) north of Colombo. Mattala Rajapaksa International Airport located in Mattala, north of Hambantota. After the ongoing renovations Ratmalana Airport will also resume operating international flights, after a half-century absence.
Domestic Aviation.
Flights connect the airport in Ratmalana to various domestic destinations.
The total number of airports in the country is 18 (2012 figure).

</doc>
<doc id="27463" url="http://en.wikipedia.org/wiki?curid=27463" title="Demographics of Switzerland">
Demographics of Switzerland

This article is about the demographic features of the population of Switzerland, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
Switzerland had a population of 8.02 million as of 2012. Its population quadrupled over the period 1800 to 1990 (average doubling time 95 years). Population growth was steepest in the period after World War II (1.4% per annum during 1950-1970, doubling time 50 years), it slowed down during the 1970s to 1980s and has since again picked up to 1% during the 2000s (doubling time 70 years).
More than 75% of the population live in the central plain, which stretches between the Alps and the Jura Mountains and from Geneva in the southwest to the Rhine River and Lake Constance in the northeast. Foreigners with permanent residency (which does not include temporary foreign workers) make up about 23% of the population.
Census.
The "Federal Population Census" (German: "Eidgenössische Volkszählung", French: "Recensement fédéral de la population", Italian: "Censimento federale della popolazione", Romansh: "Dumbraziun federala dal pievel") has been carried out every 10 years starting in 1850. The census was initiated by Federal Councillor Stefano Franscini, who evaluated the data of the first census all by himself after Parliament failed to provide the necessary funds. The census is now being conducted by the Swiss Federal Statistical Office, which makes most results available on its .
Collected data includes population data (citizenship, place of residence, place of birth, position in household, number of children, religion, languages, education, profession, place of work, etc.), household data (number of individuals living in the household, etc.), accommodation data (surface area, amount of rent paid, etc.) and building data (geocoordinates, time of construction, number of floors, etc.). Participation is compulsory and reached 99.87% of the population in 2000.
Since 2010, the population census has been carried out and analysed annually in a new format by the Federal Statistical Office (FSO). In order to ease the burden on the population, the information is primarily drawn from population registers and supplemented by sample surveys. Only a small proportion of the population (about 5%) are surveyed in writing or by telephone. The first reference day for the new census was 31 December 2010.
Population.
Total of registered residents (numbers relate to 31 December):
Growth rate.
During the 19th and 20th centuries, population growth rate has been at 0.7% to 0.8%, with a doubling time of ca. 90 years. In the later 20th century, the growth rate has fallen below 0.7% (1980s: 0.64%; 1990s: 0.65%), and in the 2000s it has risen again slightly (2000–2006: 0.69%), mostly due to immigration. In 2007 the population grew at a much higher 1.1% rate, again mostly due to immigration. For 2008, the population grew 1.6%, a level not seen since the early 1960s.
Total fertility rate
Vital statistics since 1900.
Data according to Statistik Schweiz and United Nations.
Age structure.
Data: Swiss Federal Statistics Office</dl>
As population growth curbs, the percentage of elderly people increases. In July 2006, the Swiss Federal Office of Statistics published a projection estimating that by 2050, one in three adult Swiss will be of retirement age (as opposed to one in five in 2005). Total population was projected to stagnate in 2036 at around 8.1 million and fall slightly to 8 million in 2050. The predicted age structure for 2050 is:
Sex ratio.
Data: Swiss Federal Statistics Office 2007</dl>
Life expectancy at birth.
According to statistics released by the federal government in 2008, life expectancy stands at 79.7 years for men and 84.4 years for women, for an overall average of 82.1 years for the populace as a whole.
Nationality.
Encompassing the Central Alps, Switzerland sits at the crossroads of several major European cultures. Its population includes a two-thirds majority of Alemannic German speakers and a one-quarter Latin minority (French, Italian and Romansh), see linguistic geography of Switzerland. 10% of the population natively speak an immigrant language.
Switzerland consistently ranks high on quality of life indices, including per capita income, concentration of computer and internet usage per capita, insurance coverage per individual, and health care rates. For these and many other reasons, such as the four languages, it serves as an excellent test market for businesses hoping to introduce new products into Europe.
Permanent residents by nationality.
The number of registered resident foreigners was 1,001,887 (16.17%) in 1970. This amount decreased to 904,337 (14.34%) in 1979, and has increased steadily since that time, passing the 20% mark during 2001 and rising to 1,524,663 (20.56%) in 2004. The number of Swiss citizens thus numbered about 5.9 million in that year.
In 2013 there were a total of 1,937,447 permanent residents (23.8% of the total population of 8.14 million) in Switzerland. Of these, 1.65 million resident foreigners (85.0%, or 20.2% of the 8.14 Million total population), had European citizenship (Italian: 298,875; German: 292,291; Portuguese: 253,227; French: 110,103; Serbian: 90,704; Kosovan: 86,976; Spanish: 75,333, Macedonian: 62,633; British: 40,898; Austrian: 39,494; Bosnian and Herzegovinian: 33,002; Croatian: 30,471). From other continents; 122,941 residents were from Asia; 83,873 from Africa; 78,433 from the Americas; and 4,145 from Oceania.
The following chart shows permanent resident numbers from selected regions and countries every 5 years.
Source:
Tamil refugees fleeing from war in Sri Lanka are the largest number of Asians, while Albanians and other former Yugoslavians continue to grow in number. Switzerland is also the second largest European country in number of acceptance of Iraqi refugees fleeing from the violence in Iraq since 2003, but behind Great Britain, Germany and Sweden in the number of Iraqis taken residence for a European country.
Naturalization.
In 2004, 35,700 people acquired Swiss citizenship according to Swiss nationality law, a figure slightly larger than that of the previous year (35,424), and four times larger than the 1990 figure (8,658). About a third of those naturalized are from a successor state of Former Yugoslavia: 7,900 Serbia-Montenegro, 2,400 Bosnia-Herzegowina, 2,000 Macedonia, 1,600 Croatia. 4,200 were from Italy, 3,600 from Turkey, 1,600 from Sri Lanka, 1,200 from Portugal, and 1,200 from France.
The yearly rate of naturalization has quintupled over the 1990s and 2000s, from roughly 9,000 to 45,000. Relative to the population of resident foreigners, this amounts to an increase from 8% in 1990 to 27% in 2007, or relative to the number of Swiss citizens from 1.6% in 1990 to 7.3% in 2007.
The following table shows the historical development of naturalization from selected countries.
Emigration.
In 2004, 623,100 Swiss citizens (8.9%) lived abroad, the largest group in France (166,200), followed by the USA (71,400) and Germany (70,500). (see Swiss diaspora).
Religions.
Switzerland has no state religion, though most of the cantons (except for Geneva and Neuchâtel) recognize official churches (Landeskirchen), in all cases including the Catholic Church and the Swiss Reformed Church. These churches, and in some cantons also the Old Catholic Church and Jewish congregations, are financed by official taxation of adherents. Christianity is the predominant religion of Switzerland, but is slowly shrinking over the decades (74% of total resident population in 2010). 20% of the total population were irreligious in 2010. The largest minority religion is Islam: 4,5% (2010 census).
In 2000, 5.78 million residents (79.2%, compared to 93.8% in 1980) were Christian (Roman Catholic 41.8%, Protestant 35.3%, Orthodox 1.8%). 809,800 (11.1%, compared to 3.8% in 1980) were without any religious affiliation. 310,800 (4.3%) were Muslim (compared to 0.9% in 1980), 17,900 (0.2%) were Jewish. The 2005 Eurobarometer poll found 48% of Swiss residents to be theist, 39% expressing belief in "some sort of spirit or life force", 9% atheist and 4% said that they "don't know".
As of 2012, there were 918,126 people aged 15–24 in Switzerland, of which 347,328 (37.8%) were Catholics, 232,634 (25.4%) Protestants, 57,033 (6.2%) other Christian denominations, 2,005 (0.2%) Jewish, 76,502 (8.3%) Muslims, 12,992 (1.4%) other religious communities and 176,969 (19.3%) without religion.
Languages.
The four national languages of Switzerland are German, French, Italian and Romansh.
Native speakers number about 64% (4.6 million) for German (mostly Swiss German dialects), 20% (1.5 million) for French (mostly Swiss French, but including some Franco-Provençal dialects), 7% (0.5 million, mostly Swiss Italian, but including Insubric dialects) for Italian and less than 0.5% (35,000) for Romansh.
The non-official language with the largest group of native speakers is Serbo-Croatian with 103,000 speakers in 2000, followed by Albanian with 95,000, Portuguese with 89,500, Spanish with 77,500, English with 73,000, Macedonian 61,300, and a total of 173,000 speakers of other languages, amounting to roughly 10% of the population with a native language not among the four official languages.
Education.
Almost all Swiss are literate. Switzerland's 13 institutes of higher learning enrolled 99,600 students in the academic year of 2001-02. About 25% of the adult population hold a diploma of higher learning. According to the CIA World Factbook data for 2003, 99% of the Swiss population aged 15 and over could read and write, with the rate being identical for both sexes.
During the 2008/09 school year there were 1,502,257 students in the entire Swiss educational system. In kindergarten or pre-school, there were 152,919 students (48.6% female). These students were taught by 13,592 teachers (96.0% female) in 4,949 schools, of which 301 were private schools. There were 777,394 students (48.6% female) in the obligatory schools, which include primary and lower secondary schools. These students were taught by 74,501 teachers (66.3% female) in 6,083 schools, of which 614 were private. The upper secondary school system had 337,145 students (46.9% female). They were taught by 13,900 teachers (42.3% female) in 730 schools, of which 240 were private. The tertiary education system had 234,799 students (49.7% female). They were taught by 37,546 teachers (32.8% female) in 367 schools.
Crime.
The police registered a total of 553,421 criminal offences in 2009, including 51 killings and 185 attempted murders. There were 666 cases of rape. In the same year, 94,574 adults (85% of them male, 47.4% of them Swiss citizens) were convicted under criminal law. 57.3% of convictions were for traffic offences.
In the same year, 15,064 minors (78.3% of them male, 68.2% of them of Swiss nationality, 76.3% aged between 15 and 18) were convicted.
The number of convictions in the last five years is given in the following table. Each class of crime references the relevant section of the Swiss penal code ("Strafgesetzbuch", abbreviated StGB) or the Swiss traffic laws ("Strassenverkehrsgesetz", abbr. SVG).

</doc>
<doc id="27478" url="http://en.wikipedia.org/wiki?curid=27478" title="Transport in Syria">
Transport in Syria

This article deals with the system of transport in Syria, both public and private.
Railways.
"total:"
2,052 km
<br>"standard gauge:"
1,801 km of gauge
<br>"narrow gauge:"
251 km of gauge (2006)
See also Hejaz railway.
Road Transport.
An overland trans-desert bus service between Beirut, Haifa, Damascus and Baghdad was established by the "Nairn Transport Company" of Damascus in 1923.
Roads.
"total:"
68,157 km
<br>"paved:"
61,514 km (including 1,103 km of expressways)
<br>"unpaved:"
6,643 km (2006)
Motorways.
Syria has a well-developed system of motorways in the western half of the country. The eastern part nevertheless has only connection through two lanes roads due to the sparsity of the population. The main motorways in Syria are the following:
Waterways.
900 km; minimal economic importance
Pipelines.
crude oil 1,997 km; petroleum products 3,161 km (2010)
Ports and Harbors.
Main international seaport at Latakia with additional ports at Baniyas, Jablah, and Tartus.
Merchant marine.
"total:"
19 ships ( gross register tons (GRT) or over) totaling  GRT/ tonnes deadweight (DWT)
<br>"ships by type:"
bulk carrier 4, cargo ship 14, carrier 1 (2010)
Airports.
As of 2012, Syria had a total of 99 airports. The major airports are: Aleppo International Airport, Bassel Al-Assad International Airport, Damascus International Airport, Deir ez-Zor Airport, Kamishly Airport, and Palmyra Airport.
Airports (with paved runways).
"total:" 29<br>
"over 3,047 meters in length:" 5<br>
"2,438 to 3,047 meters:" 16<br>
"914 to 1,523 meters:" 3<br>
"under 914 meters:" 5
Airports (with unpaved runways).
"total:"
70
<br>"1,524 to 2,437 m:"
1
<br>"914 to 1,523 m:"
14
<br>"under 914 m:"
55 (2012)

</doc>
<doc id="27481" url="http://en.wikipedia.org/wiki?curid=27481" title="Section 508 Amendment to the Rehabilitation Act of 1973">
Section 508 Amendment to the Rehabilitation Act of 1973

In 1998 the US Congress amended the Rehabilitation Act to require Federal agencies to make their electronic and information technology accessible to people with disabilities. Section 508 was enacted to eliminate barriers in information technology, to make available new opportunities for people with disabilities, and to encourage development of technologies that will help achieve these goals. The law applies to all Federal agencies when they develop, procure, maintain, or use electronic and information technology. Under Section 508 (#redirect ), agencies must give disabled employees and members of the public access to information that is comparable to the access available to others.
History.
Section 508 was originally added as an amendment to the "Rehabilitation Act of 1973" in 1986. The original section 508 dealt with electronic and information technologies, in recognition of the growth of this field.
In 1997, The Federal Electronic and Information Technology Accessibility and Compliance Act was proposed in the U.S. legislature to correct the shortcomings of the original section 508; the original Section 508 had turned out to be mostly ineffective, in part due to the lack of enforcement mechanisms. In the end, this Federal Electronic and Information Technology Accessibility and Compliance Act, with revisions, was enacted as the "new" Section 508 of the Rehabilitation Act of 1973, in 1998.
Section 508 addresses legal compliance through the process of market research and government procurement and also has technical standards against which products can be evaluated to determine if they meet the technical compliance. Because technology can meet the legal provisions and be legally compliant (e.g., no such product exists at time of purchase) but may not meet the United States Access Board's technical accessibility standards, users are often confused between these two issues. Additionally, evaluation of compliance can be done only when reviewing the procurement process and documentation used when making a purchase or contracting for development, the changes in technologies and standards themselves, it requires a more detailed understanding of the law and technology than at first seems necessary.
There is nothing in section 508 that requires private web sites to comply unless they are receiving federal funds or under contract with a federal agency. Commercial best practices include voluntary standards and guidelines as the World Wide Web Consortium's (W3C) Web Accessibility Initiative (WAI). Automatic accessibility checkers (engines) such as "IBM Rational Policy Tester" and AccVerify, refer to Section 508 guidelines but have difficulty in accurately testing content for accessibility.
In 2006, the United States Access Board organized the Telecommunications and Electronic and Information Technology Advisory Committee (TEITAC) to review and recommend updates to its Section 508 standards and Telecommunications Act Accessibility Guidelines. TEITAC issued its report to the Board in April 2008. The Board released drafts of proposed rules based on the committee’s recommendations in 2010 and 2011 for public comment. In February 2015, the Board released a notice of proposed rulemaking for the Section 508 standards.
The law.
Provisions.
The original legislation mandated that the Architectural and Transportation Barriers Compliance Board, known as the Access Board, establish a draft for their Final Standards for accessibility for such electronic and information technologies in December 2001. The final standards were approved in April 2001 and became enforceable on June 25, 2001.
The latest information about these standards and about support available from the Access Board in implementing them, as well as the results of surveys conducted to assess compliance, is available from the Board's newsletter Access Currents. The Section 508 standards, tools, and resources are available from the Center for Information Technology Accommodation (CITA), in the U.S. General Services Administration's Office of Government-wide Policy.

</doc>
<doc id="27485" url="http://en.wikipedia.org/wiki?curid=27485" title="Slartibartfast">
Slartibartfast

Slartibartfast is a character in "The Hitchhiker's Guide to the Galaxy", a comedy/science fiction series created by Douglas Adams. The character appears in the first and third novels, the first and third radio series (and the LP adaptation of the first radio series), the 1981 television series and the 2005 feature film. The character was modelled after actor John Le Mesurier.
Character overview.
Slartibartfast is a Magrathean, and a designer of planets. His favourite part of the job is creating coastlines, the most notable of which are the fjords found on the coast of Norway on planet Earth, for which he won an award. While trapped on prehistoric Earth, Arthur Dent and Ford Prefect see Slartibartfast's signature deep inside a glacier in ancient Norway.
When "Earth Mk. II" is being made, Slartibartfast is assigned to the continent of Africa. He is unhappy about this because he wants to make more fjords (arguing that they give a continent a baroque feel), and fjords in Africa would be hard for him to explain without natural glacial movement.
In any event, the new Earth is not required and, much to Slartibartfast's disgust, its owners suggested that he take a quick skiing holiday on his glaciers before dismantling them.
In "Life, the Universe and Everything" Slartibartfast has joined the Campaign for Real Time (or CamTim as the volunteers casually refer to it, a reference to CAMRA) which tries to preserve events as they happened before time travelling was invented. He picks up Arthur and Ford from Lord's Cricket Ground with his "Starship Bistromath", after which they head out to stop the robots of Krikkit from bringing together the pieces of the Wikkit Gate.
Origin of name.
Douglas Adams writes in the notes accompanying that he wanted Slartibartfast's name to sound very rude, but still actually be broadcastable. He therefore started with the name "Phartiphukborlz", and changed bits of it until it would be acceptable to the BBC. He came closer to achieving this goal in the following episode, with the double-act Lunkwill and Fook. He adds to this statement in "", an analysis by Neil Gaiman:
Portrayals.
Slartibartfast was first portrayed in the 1978 radio serial, in which he was voiced by Richard Vernon, who also portrayed him in the 1981 live-action miniseries. Richard Griffiths voiced him in the 2004 radio series. Finally, he was portrayed by Bill Nighy in the 2005 film adaptation of the first novel.

</doc>
<doc id="27490" url="http://en.wikipedia.org/wiki?curid=27490" title="Sense and Sensibility">
Sense and Sensibility

Sense and Sensibility is a novel by Jane Austen, and was her first published work when it appeared in 1811 under the pseudonym "A Lady". A work of romantic fiction, better known as a comedy of manners, "Sense and Sensibility" is set in southwest England, London and Kent between 1792 and 1797, and portrays the life and loves of the Dashwood sisters, Elinor and Marianne. The novel follows the young ladies to their new home, a meagre cottage on a distant relative's property, where they experience love, romance and heartbreak. The philosophical resolution of the novel is ambiguous: the reader must decide whether sense and sensibility have truly merged.
Title.
Jane Austen wrote the first draft of the novel in the form of a novel-in-letters (epistolary form) sometime around 1795 when she was about 19 years old, and gave it the title "Elinor and Marianne". She later changed the form to a narrative and the title to "Sense and Sensibility". "Sense" in the book means good judgment or prudence, and "sensibility" means sensitivity or emotionality. "Sense" is identified with the character of Elinor, while "sensibility" is identified with the character of Marianne. By changing the title, Austen added "philosophical depth" to what began as a sketch of two characters. The title of the book, and that of her next published novel, "Pride and Prejudice" (1813), may be suggestive of political conflicts of the 1790s.
Plot discussion.
Resolution.
Austen biographer Claire Tomalin argues that "Sense and Sensibility" has a "wobble in its approach," which developed because Austen, in the course of writing the novel, gradually became less certain about whether sense or sensibility should triumph. Austen characterises Marianne as a sweet lady with attractive qualities: intelligence, musical talent, frankness, and the capacity to love deeply. She also acknowledges that Willoughby, with all his faults, continues to love and, in some measure, appreciate Marianne. For these reasons, some readers find Marianne's ultimate marriage to Colonel Brandon an unsatisfactory ending.
Other interpretations, however, have argued that Austen's intention was not to debate the superior value of either sense or sensibility in good judgment, but rather to demonstrate that both are equally important but must be applied with good balance to one another.
Dashwood extracts a promise from his son, that he will take care of his half-sisters; however, John's selfish and greedy wife, Fanny, soon persuades him to renege. John and Fanny immediately take up their place as the new owners of Norland, while the Dashwood women are reduced to the position of unwelcome guests. Mrs. Dashwood begins looking for somewhere else to live.
In the meantime, Fanny's brother, Edward Ferrars, a pleasant, unassuming, intelligent but reserved young man, visits Norland and soon forms an attachment with Elinor. Fanny disapproves the match and offends Mrs. Dashwood with the implication that Elinor is motivated by money rather than love. Mrs. Dashwood indignantly speeds her search for a new home.
Mrs. Dashwood moves her family to Barton Cottage in Devonshire, near the home of her cousin, Sir John Middleton. Their new home lacks many of the conveniences that they have been used to; however, they are warmly received by Sir John, and welcomed into the local society—meeting his wife, Lady Middleton, his mother-in-law, Mrs. Jennings and his friend, the grave, quiet and gentlemanly Colonel Brandon. It soon becomes apparent that Colonel Brandon is attracted to Marianne, and Mrs. Jennings teases them about it. Marianne is not pleased as she considers the thirty-five-year-old Colonel Brandon an old bachelor, incapable of falling in love or inspiring love in anyone else.
Marianne, out for a walk, gets caught in the rain, slips and sprains her ankle. The dashing, handsome John Willoughby sees the accident and assists her. Marianne quickly comes to admire his good looks and outspoken views on poetry, music, art and love. Mr. Willoughby's attentions are so overt that Elinor and Mrs. Dashwood begin to suspect that the couple are secretly engaged. Elinor cautions Marianne against her unguarded conduct, but Marianne refuses to check her emotions, believing that it is a falsehood. Unexpectedly one day, Mr. Willoughby informs the Dashwoods that his aunt is sending him to London on business, indefinitely. Marianne is distraught and abandons herself to her sorrow.
Edward Ferrars then pays a short visit to Barton Cottage but seems unhappy and out of sorts. Elinor fears that he no longer has feelings for her, but feels compelled, by a sense of duty, to protect her family from knowing her heartache. Soon after Edward departs, Anne and Lucy Steele, the vulgar and uneducated cousins of Lady Middleton, come to stay at Barton Park. Lucy informs Elinor of her secret four-year engagement to Edward Ferrars, displaying proofs of her veracity. Elinor comes to understand the inconsistencies of Edward's behaviour to her and acquits him of blame. She is charitable enough to pity Edward for being held to a loveless engagement by his gentlemanly honour.
As winter approaches, Elinor and Marianne accompany Mrs. Jennings to London. On arriving, Marianne rashly writes a series of personal letters to Willoughby, which go unanswered. When they finally meet, Mr. Willoughby greets Marianne reluctantly and coldly, to her extreme distress. Soon Marianne receives a curt letter enclosing their former correspondence and love tokens, including a lock of her hair and informing her of his engagement to a young lady of large fortune. Marianne is devastated, and admits to Elinor that she and Willoughby were never engaged, but she loved him and he led her to believe he loved her. In sympathy for Marianne, and to illuminate Willoughby's true character, Colonel Brandon reveals to Elinor that Willoughby had seduced Brandon's fifteen-year-old ward, Miss Williams, then abandoned her when she became pregnant. Brandon had been in love with her mother, who was his father's ward and forced into an unhappy marriage to his brother; Marianne strongly reminds him of her.
In the meantime, the Steele sisters have come to London as guests of John and Fanny Dashwood. Lucy sees her invitation to the Dashwoods' as a personal compliment, rather than what it is, a slight to Elinor. In the false confidence of their popularity, Anne Steele betrays Lucy's secret. As a result the Misses Steele are turned out of the house, and Edward is entreated to break the engagement on pain of disinheritance. Edward, honourably, refuses to comply and is immediately disinherited in favour of his brother, gaining widespread respect for his gentlemanly conduct, and sympathy from Elinor and Marianne who understand how much he has sacrificed. Colonel Brandon shows this admiration by offering him the living of Delaford parsonage.
Mrs. Jennings takes Elinor and Marianne to the country to visit her second daughter who has just given birth to her first child. In her misery over Willoughby's marriage, Marianne neglects her health and becomes dangerously ill. Traumatised by rumours of her impending death, Willoughby arrives to repent and reveals to Elinor that his love for Marianne was genuine. When his aunt learned of his behaviour towards Miss Williams and disinherited him, he felt he had to marry for money rather than love. But he elicits Elinor's pity because his choice has made him unhappy.
When Marianne recovers, Elinor tells her of Willoughby's visit. Marianne comes to assess what has passed with sense rather than emotion, and sees that she could never have been happy with Willoughby's immoral and expansive nature. She comes to value Elinor's conduct in a similar situation and resolves to model herself after Elinor's courage and good sense.
On learning that Lucy has married Mr. Ferrars, Elinor grieves, until Edward arrives and reveals that, after his disinheritance, Lucy jilted him in favour of his now wealthy brother, Robert Ferrars. Edward and Elinor soon marry, and in a very few years Marianne marries Colonel Brandon, having gradually fallen deeply in love with him.
Publication.
In 1811, Thomas Egerton of the Military Library publishing house in London accepted the manuscript for publication in three volumes. Austen paid to have the book published and paid the publisher a commission on sales. The cost of publication was more than a third of Austen's annual household income of £460 (about £15,000 in 2008 currency). She made a profit of £140 (almost £5,000 in 2008 currency) on the first edition, which sold all 750 printed copies by July 1813. A second edition was advertised in October 1813.
Adaptations.
The book has been adapted for film and television a number of times, including a 1981 serial for TV directed by Rodney Bennett; a 1995 movie adapted by Emma Thompson and directed by Ang Lee; a version in Tamil called "Kandukondain Kandukondain", released in 2000, starring Aishwarya Rai Bachchan; and a 2008 TV series on BBC adapted by Andrew Davies and directed by John Alexander. Sense & Sensibility, The Musical (book and lyrics by Jeffrey Haddow and music by Neal Hampton) received its world premier by the Denver Center Theatre Company in April 2013 staged by Tony-nominated director Marcia Milgrom Dodge, and in 2014, the Utah Shakespeare Festival presented Joseph Hanreddy and J.R. Sullivan's adaptation of the novel. In 2013, author Joanna Trollope published her own version of the story, bringing the characters into the present day and providing modern satire.

</doc>
<doc id="27533" url="http://en.wikipedia.org/wiki?curid=27533" title="September 28">
September 28

September 28 is the day of the year in the Gregorian calendar.

</doc>
<doc id="27560" url="http://en.wikipedia.org/wiki?curid=27560" title="Solar deity">
Solar deity

A solar deity (also sun god or sun goddess) is a sky deity who represents the Sun, or an aspect of it, usually by its perceived power and strength. Solar deities and sun worship can be found throughout most of recorded history in various forms. Hence, many beliefs have formed around this worship, such as the "missing sun" found in many cultures.
Overview.
The Neolithic concept of a "solar barge" (also "solar bark", "solar barque", "solar boat" and "sun boat", a mythological representation of the sun riding in a boat) is found in the later myths of ancient Egypt, with Ra and Horus. Predynasty Egyptian beliefs attribute Atum as the sun-god and Horus as a god of the sky and sun. As the Old Kingdom theocracy gained power, early beliefs were incorporated with the expanding popularity of Ra and the Osiris-Horus mythology. Atum became Ra-Atum, the rays of the setting sun. Osiris became the divine heir to Atum's power on Earth and passes his divine authority to his son Horus. Early Egyptian myths imply the sun is within the lioness, Sekhmet, at night and is reflected in her eyes; or that it is within the cow, Hathor, during the night, being reborn each morning as her son ("bull").
Mesopotamian Shamash plays an important role during the Bronze Age, and "my Sun" is eventually used as an address to royalty. Similarly, South American cultures have a tradition of Sun worship, as with the Incan Inti. Svarog is the Slavic god sun and spirit of fire.
Proto-Indo-European religion has a solar chariot, the sun as traversing the sky in a chariot. In Germanic mythology this is "Sol", in Vedic Surya, and in Greek Helios (occasionally referred to as Titan) and (sometimes) as Apollo.
During the Roman Empire, a festival of the birth of the "Unconquered Sun" (or "Dies Natalis Solis Invicti") was celebrated on the winter solstice—the "rebirth" of the sun—which occurred on December 25 of the Julian calendar. In late antiquity, the theological centrality of the sun in some Imperial religious systems suggest a form of a "solar monotheism". The religious commemorations on December 25 were replaced under Christian domination of the Empire with the birthday of Christ.
Africa.
The Tiv people consider the Sun to be the son of the supreme being Awondo and the Moon Awondo's daughter. The Barotse tribe believes that the Sun is inhabited by the sky god Nyambi and the Moon is his wife. Even where the sun god is equated with the supreme being, in some African mythologies he or she does not have any special functions or privileges as compared to other deities. The Ancient Egyptian god of creation, Amun is also believed to reside inside the sun. So is the Akan creator deity, Nyame and the Dogon deity of creation, Nommo. Also in Egypt, there was a religion that worshiped the sun directly, and was among the first monotheistic religions: Atenism.
Aztec mythology.
In Aztec mythology, "Tonatiuh" (Nahuatl: "Ollin Tonatiuh", "Movement of the Sun") was the sun god. The Aztec people considered him the leader of "Tollan" (heaven). He was also known as the fifth sun, because the Aztecs believed that he was the sun that took over when the fourth sun was expelled from the sky. According to their cosmology, each sun was a god with its own cosmic era. According to the Aztecs, they were still in Tonatiuh's era. According to the Aztec creation myth, the god demanded human sacrifice as tribute and without it would refuse to move through the sky. The Aztecs were fascinated by the sun and carefully observed it, and had a solar calendar similar to that of the Maya. Many of today's remaining Aztec monuments have structures aligned with the sun.
In the Aztec calendar, Tonatiuh is the lord of the thirteen days from 1 Death to 13 Flint. The preceding thirteen days are ruled over by Chalchiuhtlicue, and the following thirteen by Tlaloc.
Buddhism.
In Buddhist cosmology, the bodhisattva of the Sun is known as Ri Gong Ri Guang Pu Sa (The Bright Solar Bodhisattva of the Solar Palace) / Ri Gong Ri Guang Tian Zi (The Bright Solar Prince of the Solar Palace) / Ri Gong Ri Guang Zun Tian Pu Sa (The Greatly Revered Bright Solar Prince of the Solar Palace / one of the 20 or 24 guardian devas). In Sanskrit, He is known as Suryaprabha. He is usually depicted with Yue Gong Yue Guang Pu Sa (The Bright Lunar Bodhisattva of the Lunar Palace) / Yue Gong Yue Guang Tian Zi ( The Bright Lunar Prince of the Lunar Palace) / Yue Gong Yue Guang Zun Tian Pu Sa (The Greatly Revered Bright Lunar Prince of the Lunar Palace / one of the 20 or 24 guardian "devas" known as "Candraprabha" in Sanskrit. With Yao Shi Fo / Bhaisajyaguru Buddha (Medicine Buddha), these two "bodhisattvas" create the Dong Fang San Sheng or the Three Holy Sages of the East.
Chinese mythology.
In Chinese mythology (cosmology), there were originally ten suns in the sky, who were all brothers. They were supposed to emerge one at a time as commanded by the Jade Emperor. They were all very young and loved to fool around. Once they decided to all go into the sky to play, all at once. This made the world too hot for anything to grow. A hero named Hou Yi shot down nine of them with a bow and arrow to save the people of the earth. He is still honored this very day. In another myth, the solar eclipse was caused by the magical dog of heaven biting off a piece of the sun. The referenced event is said to have occurred around 2,160BCE. There was a tradition in China to make lots of loud celebratory sounds during a solar eclipse to scare the sacred "dog" away.
The Deity of the Sun in Chinese mythology is Ri Gong Tai Yang Xing Jun (Tai Yang Gong / Grandfather Sun) or Star Lord of the Solar Palace, Lord of the Sun. In some mythologies, Tai Yang Xing Jun is believed to be Hou Yi. Tai Yang Xing Jun is usually depicted with the Star Lord of the Lunar Palace, Lord of the Moon, Yue Gong Tai Yin Xing Jun (Tai Yin Niang Niang / Lady Tai Yin).
Baltic mythology.
Those whom practice Dievturība, beliefs of traditional Latvian culture, celebrate the Sun goddess, Saulė and known in traditional Lithuanian beliefs as Saulé. Saule/Saulé is among the most important deities in Baltic mythology/traditions.
Ancient Egypt.
Sun worship was prevalent in ancient Egyptian religion. The earliest deities associated with the sun are all goddesses: Wadjet, Sekhmet, Hathor, Nut, Bast, Bat, and Menhit. First Hathor, and then Isis, give birth to and nurse Horus and Ra. Hathor the horned-cow is one of the 12 daughters of Ra, gifted with joy and is a wet-nurse to Horus.
The Sun's movement across the sky represents a struggle between the Pharaoh's soul and an avatar of Osiris. Ra travels across the sky in his solar-boat; at dawn he drives away the demon Apep of darkness. The "solarisation" of several local gods (Hnum-Re, Min-Re, Amon-Re) reaches its peak in the period of the fifth dynasty.
Rituals to the god Amun who became identified with the sun god Ra were often carried out on the top of temple pylons. A Pylon mirrored the hieroglyph for 'horizon' or "akhet", which was a depiction of two hills "between which the sun rose and set", associated with recreation and rebirth. On the first Pylon of the temple of Isis at Philae, the pharaoh is shown slaying his enemies in the presence of Isis, Horus and Hathor.
In the eighteenth dynasty, Akhenaten changed the polytheistic religion of Egypt to a monotheistic one, Atenism of the solar-disk and is the first recorded state monotheism. All other deities were replaced by the Aten, including Amun-Ra, the reigning sun god of Akhenaten's own region. Unlike other deities, the Aten did not have multiple forms. His only image was a disk—a symbol of the sun.
Soon after Akhenaten's death, worship of the traditional deities was reestablished by the religious leaders (Ay the High-Priest of Amen-Ra, mentor of Tutankhaten/Tutankhamen) who had adopted the Aten during the reign of Akhenaten.
Hinduism.
The Ādityas are one of the principal deities of the Vedic classical Hinduism belonging to Solar class. In the Vedas, numerous hymns are dedicated to Mitra, Varuna, Savitr etc.
Even the Gayatri mantra, which is regarded as one of the most sacred of the Vedic hymns is dedicated to Savitr, one of the principal Ādityas. The Adityas are a group of solar deities, from the Brahmana period numbering twelve. The ritual of "sandhyavandanam", performed by Hindus, is an elaborate set of hand gestures and body movements, designed to greet and revere the Sun.
The sun god in Hinduism is an ancient and revered deity. In later Hindu usage, all the Vedic Ādityas lost identity and metamorphosed into one composite deity, Surya, the Sun. The attributes of all other Ādityas merged into that of Surya and the names of all other Ādityas became synonymous with, or epithets of, Surya.
The Ramayana has Rama as a descendant of the Surya, thus belonging to the Suryavansha or the clan of the Sun. The Mahabharata describes one of its warrior heroes, Karna, as being the son of the Pandava mother Kunti and Surya.
The sun god is said to be married to the goddess Ranaadeh, also known as Sanjnya. She is depicted in dual form, being both sunlight and shadow, personified. The goddess is revered in Gujarat and Rajasthan.
The charioteer of Surya is Aruna, who is also personified as the redness that accompanies the sunlight in dawn and dusk. The sun god is driven by a seven-horsed Chariot depicting the seven days of the week.
In India, at Konark, in the state of Odisha, a temple is dedicated to Surya. The Konark Sun Temple has been declared a UNESCO World Heritage Site. Surya is the most prominent of the "navagrahas" or nine celestial objects of the Hindus. "Navagrahas" can be found in almost all Hindu temples. There are further temples dedicated to Surya, one in Arasavilli, Srikakulam District in AndhraPradesh, one in Gujarat at Modhera and another in Rajasthan. The temple at Arasavilli was constructed in such a way that on the day of Radhasaptami, the sun's rays directly fall on the feet of the Sri Suryanarayana Swami, the deity at the temple.
Chhath (Hindi: छठ, also called "Dala Chhath") is an ancient Hindu festival dedicated to Surya, the chief solar deity, unique to Bihar, Jharkhand and the Terai. This major festival is also celebrated in the northeast region of India, Madhya Pradesh, Uttar Pradesh, and parts of Chhattisgarh. Hymns to the sun can be found in the Vedas, the oldest sacred texts of Hinduism. Practiced in different parts of India, the worship of the sun has been described in the Rigveda. There is another festival called Sambha-Dasami, which is celebrated in the state of Odisha for the "surya".
The Gurjars (or Gujjars), were Sun-worshipers and are described as devoted to the feet of the sun god Surya. Their copper-plate grants bear an emblem of the Sun and on their seals too, this symbol is depicted.
Indonesian mythology.
Solar gods have a stronger presence in Indonesian mythology. In some cases the Sun is revered as a "father" or "founder" of the tribe. This may apply for the whole tribe or only for the royal and ruling families. This practise is more common in Australia and on the island of Timor, where the tribal leaders are seen as direct heirs to the sun god.
Some of the initiation rites include the second reincarnation of the rite's subject as a "son of the Sun", through a symbolic death and a rebirth in the form of a Sun. These rituals hint that the Sun may have an important role in the sphere of funerary beliefs. Watching the Sun's path has given birth to the idea in some societies that the deity of the Sun descends in to the underworld without dying and is capable of returning afterward. This is the reason for the Sun being associated with functions such as guide of the deceased tribe members to the underworld, as well as with revival of perished. The Sun is a mediator between the planes of the living and the dead.
Theosophy.
The primary local deity in Theosophy is the Solar Logos, "the consciousness of the sun".
Solar myth.
Three theories exercised great influence on nineteenth and early twentieth century mythography, beside the Tree worship of Mannhardt and the Totemism of J. F. McLennan, the "Sun myth" of Alvin Boyd Kuhn and Max Müller.
R. F. Littledale criticized the Sun myth theory when he illustrated that Max Müller on his own principles was himself only a Solar myth, whilst Alfred Lyall delivered a still stronger attack on the same theory and its assumption that tribal gods and heroes, such as those of Homer, were mere reflections of the Sun myth by proving that the gods of certain Rajput clans were really warriors who founded the clans not many centuries ago, and were the ancestors of the present chieftains.
Solar barge and sun chariot.
A "solar barge" (also "solar bark", "solar barque", "solar boat" and "sun boat") is a mythological representation of the sun riding in a boat. The "Khufu ship", a 43.6-meter-long vessel that was sealed into a pit in the Giza pyramid complex at the foot of the Great Pyramid of Giza around 2500 BC, is a full-size surviving example which may have fulfilled the symbolic function of a solar barque. This boat was rediscovered in May 1954 when archeologist Kamal el-Mallakh and inspector Zaki Nur found two ditches sealed off by about 40 blocks weighing 17 to 20 tonnes each. This boat was disassembled into 1,224 pieces and took over 10 years to reassemble. A nearby museum was built to house this boat.
Other sun boats were found in Egypt dating to different pharonic dynasties.
Examples include:
A "sun chariot" is a mythological representation of the sun riding in a chariot. The concept is younger than that of the solar barge, and typically Indo-European, corresponding with the Indo-European expansion after the invention of the chariot in the 2nd millennium BC.
Examples include these:
The sun itself also was compared to a wheel, possibly in Proto-Indo-European, Greek "hēliou kuklos", Sanskrit "suryasya cakram", Anglo-Saxon "sunnan hweogul" (PIE *swelyosyo kukwelos).
Male and female.
Among modern English speakers, solar deities are popularly thought of as male counterparts of the lunar deity (usually female); however, sun goddesses are found on every continent (e.g. Amaterasu in Japanese belief) paired with male lunar deities. Among the earliest records of human beliefs, the early goddesses of the Egyptian pantheon carried a sun above their head as a symbol of dignity (as daughters of Ra). The sun was a major aspect of Egyptian symbols and hieroglyphs, all the lunar deities of that pantheon were male deities. The cobra (of Pharaoh Son of Ra), the lioness (daughter of Ra), the cow (daughter of Ra), the dominant symbols of the most ancient Egyptian deities, carried their relationship to the sun atop their heads; they were female and their cults remained active throughout the history of the culture. Later a "sun god" (Aten) was established in the eighteenth dynasty on top of the other solar deities, before the "aberration" was stamped out and the old pantheon re-established. When male deities became associated with the sun in that culture, they began as the offspring of a mother (except Ra, King of the Gods who gave birth to himself).
Some mythologists, such as Brian Branston, Patricia Monaghan and Janet McCrickard, contend that sun goddesses are as common as, or even more common, worldwide than their male counterparts. They also claim that the belief that solar deities are primarily male is linked to the fact that a few better known mythologies (such as those of late classical Greece and late Roman mythology) rarely break from this rule, although closer examination of the earlier myths of those cultures reveal a very different distribution than the contemporary popular belief. The dualism of sun/male/light and moon/female/darkness is found in many (but not all) late southern traditions in Europe that derive from Orphic and Gnostic philosophies.
In Germanic mythology the Sun is female and the Moon is male. The corresponding Old English name is Siȝel ], continuing Proto-Germanic *Sôwilô or *Saewelô. The Old High German Sun goddess is Sunna. In the Norse traditions, every day, Sól rode through the sky on her chariot, pulled by two horses named Arvak and Alsvid. Sól also was called Sunna and Frau Sunne, from which are derived the words "sun" and "Sunday".
Other cultures that have sun goddesses include: The Lithuanians and Latvians (Saule), the Finns (Päivätär, Beiwe) and the related Hungarians . Sun goddesses are found around the world; in Arabia (Al-Lat), Australia (Bila, Walo), India (Bisal-Mariamna, Bomong, Kn Sgni) and Sri Lanka (Pattini); among the Hittites (Wurusemu), Egyptians (Sekhmet) and Babylonians (Shapash); in Native America, among the Cherokee (Unelanuhi), Natchez (Wal Sil), Inuit (Malina) and Miwok (Hekoolas).
It appears that the original Proto-Indo-European solar deity was female. Accordingly, most of the unambiguous solar deities in the Indo-European paradigm are feminine, with masculine ones being mostly a product of Hellenic and Hindu derivations, as well as syncretism of originally unrelated deities like Apollo with the sun.
Missing sun.
The missing sun is a theme in the myths of many cultures, sometimes including the themes of imprisonment, exile, or death. The missing sun is often used to explain various natural phenomena, including the disappearance of the sun at night, shorter days during the winter, and solar eclipses.
Some other tales are similar, such as the Sumerian story of the goddess Inanna's descent into the underworld. These may have parallel themes, but do not fit in this motif unless they concern a solar deity.
In late Egyptian mythology, Ra passes through Duat (the underworld) every night. Apep has to be defeated in the darkness hours for Ra and his solar barge to emerge in the east each morning.
In Japanese mythology, the sun goddess Amaterasu is startled by the behavior of her brother Susanoo and hides in a cave, plunging the world into darkness until she is willing to emerge. It has been suggested that the story is allegorical, symbolising that the sun goddess hiding in a cave is a metaphor for the sun exhibiting quiet periods such as the Maunder Minimum. This allegory has been used in popular fiction such as Masks of the Lost Kings.
In Norse mythology, the gods Odin and Tyr both have attributes of a sky father, and they are doomed to be devoured by wolves (Fenrir and Garm, respectively) at Ragnarok. Sól, the Norse sun goddess, will be devoured by the wolf Skoll.
In Hindu astronomy, Rahu and Ketu ate the sun or moon to cause lunar and solar eclipses. In later, more scientific years, their names were given to the Lunar nodes.

</doc>
<doc id="27566" url="http://en.wikipedia.org/wiki?curid=27566" title="Summer Olympic Games">
Summer Olympic Games

The Summer Olympic Games or the Games of the Olympiad (French: "Jeux olympiques d'été" ), first held in 1896, are an international multi-sport event, occurring every four years, organized by the International Olympic Committee. Medals are awarded in each event, with gold medals for first place, silver for second and bronze for third, a tradition that started in 1904. The Winter Olympic Games were also created due to the success of the Summer Olympics.
The Olympics have increased from a 42-event competition with fewer than 250 male competitors from 14 nations to a 300-event sporting celebration with over 10,000 competitors from 205 nations. Organizers for the 2008 Summer Olympics in Beijing expected approximately 10,500 competitors to take part in the 302 events on the program for the games.
Eighteen countries have hosted the Summer Olympics, with England 2012 being the most recent. The United States has hosted four Summer Olympics, more than any other nation, and the United Kingdom has hosted three Summer Olympics. In 2016, Rio de Janeiro will host the first Summer Games in South America. London has hosted the Summer Olympics three times, and three cities have hosted two Summer Olympics: Los Angeles, Paris and Athens. 
Five countries – Greece, Australia, France, Great Britain and Switzerland – have been represented at all Summer Olympic Games. The only country to have won at least one gold medal at every Summer Olympic Games is Great Britain, ranging from one gold in 1904, 1952 and 1996 to fifty-six golds in 1908.
Qualification.
Qualification rules for each of the Olympic sports are set by the International Sports Federations (IFs) that governs that sport's international competition.
For individual sports, competitors typically qualify through attaining a certain place in a major international event or on the IF's ranking list. There is a general rule that maximum three individual athletes may represent each nation per competition. National Olympic committees may enter a limited number of qualified competitors in each event, and the NOC decides which qualified competitors to select as representatives in each event if more have attained the benchmark than can be entered.
Nations most often qualify teams for team sports through continental qualifying tournaments, in which each continental association is given a certain number of sports in the Olympic tournament. Each nation may be represented by no more than one team per competition.
Hosting.
The United States has hosted four Summer Olympic Games, more than any other nation. The United Kingdom hosted the 2012 Olympic games, its third Summer Olympic Games, in its capital London, making London the first city to host the Summer Olympic Games three times. Australia, France, Germany and Greece have all hosted the Summer Olympic Games twice. Other countries that have hosted the Summer Olympics are Belgium, China, Canada, Finland, Italy, Japan, Mexico, Netherlands, South Korea, Spain, the Soviet Union and Sweden. In 2016, Rio de Janeiro will host the first Summer Games in South America. Three cities have hosted two Summer Olympic Games: Los Angeles, Paris and Athens. Stockholm, Sweden, has hosted events at two Summer Olympic Games, having hosted the games in 1912 and the equestrian events at the 1956 Summer Olympics—which they are usually listed as jointly hosting. Events at the Summer Olympics have also been held in Hong Kong and the Netherlands, with the equestrian events at the 2008 Summer Olympics being held in Sha Tin and Kwu Tung, Hong Kong and two sailing races at the 1920 Summer Olympics being held in Amsterdam, the Netherlands.
History.
Early years.
The modern Olympic Games were founded in 1894 when Pierre de Coubertin sought to promote international understanding through sporting competition. He based his Olympics on the Wenlock Olympian Society Annual Games, which had been contested in Much Wenlock since 1850. The first edition of de Coubertin's games, held in Athens in 1896, attracted just 245 competitors, of whom more than 200 were Greek, and only 14 countries were represented. Nevertheless, no international events of this magnitude had been organized before. Female athletes were not allowed to compete, though one woman, Stamata Revithi, ran the marathon course on her own, saying "[i]f the committee doesn’t let me compete I will go after them regardless".
The 1896 Summer Olympics, officially known as the Games of the I Olympiad, was an international multi-sport event which was celebrated in Athens, Greece, from 6 to 15 April 1896. It was the first Olympic Games held in the Modern era. Ancient Greece was the birthplace of the Olympic Games, consequently Athens was perceived to be an appropriate choice to stage the inaugural modern Games. It was unanimously chosen as the host city during a congress organized by Pierre de Coubertin, a French pedagogue and historian, in Paris, on 23 June 1894. The International Olympic Committee (IOC) was also established during this congress.
Despite many obstacles and setbacks, the 1896 Olympics were regarded as a great success. The Games had the largest international participation of any sporting event to that date. Panathinaiko Stadium, the first big stadium in the modern world, overflowed with the largest crowd ever to watch a sporting event. The highlight for the Greeks was the marathon victory by their compatriot Spiridon Louis, a water carrier. He won at the Olympics in 2 hours 58 minutes and 50 seconds, setting off wild celebrations at the stadium. The most successful competitor was German wrestler and gymnast Carl Schuhmann, who won four gold medals.
After the Games, Coubertin and the IOC were petitioned by several prominent figures including Greece's King George and some of the American competitors in Athens, to hold all the following Games in Athens. However, the 1900 Summer Olympics were already planned for Paris and, except for the 1906 Intercalated Games, the Olympics did not return to Greece until the 2004 Summer Olympics.
Four years later the 1900 Summer Olympics in Paris attracted more than four times as many athletes, including 20 women, who were allowed to officially compete for the first time, in croquet, golf, sailing, and tennis. The Games were integrated with the Paris World's Fair and lasted over 5 months. It is still disputed which events exactly were "Olympic", since few or maybe even none of the events were advertised as such at the time.
Numbers declined for the 1904 Games in St. Louis, Missouri, United States, due in part to the lengthy transatlantic boat trip required of the European competitors, and the integration with the Louisiana Purchase Exposition World's Fair, which again spread the event out over an extended period. In contrast with Paris 1900, the word "Olympic" was used for practically every contest, including those exclusively for school boys or for Irish-Americans.
A series of smaller games were held in Athens in 1906. The IOC does not currently recognize these games as being official Olympic Games, although many historians do. The 1906 Athens games were the first of an alternating series of games to be held in Athens, but the series failed to materialize. The games were more successful than the 1900 and 1904 games, with over 900 athletes competing, and contributed positively to the success of future games.
The 1908 London Games saw numbers rise again, as well as the first running of the marathon over its now-standard distance of 42.195 km (26 miles 385 yards). The first Olympic Marathon in 1896 (a male-only race) was raced at a distance of 40 km (24 miles 85 yards). The new marathon distance was chosen to ensure that the race finished in front of the box occupied by the British royal family. Thus the marathon had been 40 km for the first games in 1896, but was subsequently varied by up to 2 km due to local conditions such as street and stadium layout. At the six Olympic games between 1900 and 1920, the marathon was raced over six different distances.
At the end of the 1908 marathon the Italian runner Dorando Pietri was first to enter the stadium, but he was clearly in distress, and collapsed of exhaustion before he could complete the event. He was helped over the finish line by concerned race officials, but later he was disqualified and the gold medal was awarded to John Hayes, who had trailed him by around 30 seconds.
The Games continued to grow, attracting 2,504 competitors, to Stockholm in 1912, including the great all-rounder Jim Thorpe, who won both the decathlon and pentathlon. Thorpe had previously played a few games of baseball for a fee, and saw his medals stripped for this breach of amateurism after complaints from Avery Brundage. They were reinstated in 1983, 30 years after his death. The Games at Stockholm were the first to fulfill Pierre de Coubertin's original idea. For the first time since the Games started in 1896 were all five inhabited continents represented with athletes competing in the same stadium.
The scheduled 1916 Summer Olympics were cancelled following the onset of World War I.
Interwar era.
The 1920 Antwerp games in war-ravaged Belgium were a subdued affair, but again drew a record number of competitors. This record only stood until 1924, when the Paris Games involved 3,000 competitors, the greatest of whom was Finnish runner Paavo Nurmi. The "Flying Finn" won three team gold medals and the individual 1,500 and 5,000 meter runs, the latter two on the same day.
The 1928 Amsterdam games were notable for being the first games which allowed females to compete at track & field athletics, and benefited greatly from the general prosperity of the times alongside the first appearance of sponsorship of the games, from Coca-Cola. The 1928 games saw the introduction of a standard medal design with the IOC choosing Giuseppe Cassioli's depiction of Greek goddess Nike and a winner being carried by a crowd of people. This design was used up until 1972.
The 1932 Los Angeles games were affected by the Great Depression, which contributed to the low number of competitors (the fewest since the St. Louis games). The 1936 Berlin Games were seen by the German government as a golden opportunity to promote their ideology. The ruling Nazi Party commissioned film-maker Leni Riefenstahl to film the games. The result, "Olympia", was a masterpiece, despite Hitler's theories of Aryan racial superiority being repeatedly shown up by "non-Aryan" athletes. In particular, African-American sprinter and long jumper Jesse Owens won four gold medals. The tale of Hitler snubbing Owens at the ensuing medal ceremony is a myth. The 1936 Berlin Games also saw the reintroduction of the Torch Relay.
Due to World War II, the Games of 1940 (due to be held in Tokyo and temporarily relocated to Helsinki upon the outbreak of war) were cancelled. The Games of 1944 were due to be held in London but were also cancelled; instead, London hosted the first games after the end of the war, in 1948.
After World War II.
The first post-war Games were held in 1948 in London, with both Germany and Japan excluded. Dutch sprinter Fanny Blankers-Koen won four gold medals on the track, emulating Owens' achievement in Berlin.
At the 1952 Games in Helsinki the USSR team competed for the first time and immediately became one of the dominant teams. Finland made a legend of an amiable Czechoslovak army lieutenant named Emil Zátopek, who was intent on improving on his single gold and silver medals from 1948. Having first won both the 10,000 and 5,000 meter races, he also entered the marathon, despite having never previously raced at that distance. Pacing himself by chatting with the other leaders, Zátopek led from about half way, slowly dropping the remaining contenders to win by two and a half minutes, and completed a trio of wins.
The 1956 Melbourne Games were largely successful, barring a water polo match between Hungary and the Soviet Union, which political tensions caused to end as a pitched battle between the teams. Due to a foot-and-mouth disease outbreak in Britain at the time and the strict quarantine laws of Australia, the equestrian events were held in Stockholm.
At the 1960 Rome Games a young light-heavyweight boxer named Cassius Clay, later known as Muhammad Ali, arrived on the scene. Ali would later throw his gold medal away in disgust after being refused service in a whites-only restaurant in his home town of Louisville, Kentucky. Soviet women's artistic gymnastics team members won 15 of 16 possible medals. Other performers of note in 1960 included Wilma Rudolph, a gold medalist in the 100 meters, 200 meters and 4x100 meters relay events.
The 1964 Games held in Tokyo are notable for heralding the modern age of telecommunications. These games were the first to be broadcast worldwide on television, enabled by the recent advent of communication satellites. The 1964 Games were thus a turning point in the global visibility and popularity of the Olympics. Judo debuted as an official sport, and Dutch judoka Anton Geesink created quite a stir when he won the final of the open weight division, defeating Akio Kaminaga in front of his home crowd.
Performances at the 1968 Mexico City games were affected by the altitude of the host city, specifically the long jump, in which American athlete Bob Beamon jumped 8.90 meters. Beamon's world record would stand for 23 years. The 1968 Games also introduced the now-universal Fosbury flop, a technique which won American high jumper Dick Fosbury the gold medal. Politics took center stage in the medal ceremony for the men's 200 meter dash, where Tommie Smith and John Carlos made a protest gesture on the podium against the segregation in the United States; their political act was condemned within the Olympic Movement, but was praised in the American Civil Rights Movement.
Politics again intervened at Munich in 1972, with lethal consequences. A Palestinian terrorist group named Black September invaded the Olympic village and broke into the apartment of the Israeli delegation. They killed two Israelis and held 9 others as hostages. The terrorists demanded that Israel release numerous prisoners. When the Israeli government refused their demand, a tense stand-off ensued while negotiations continued. Eventually the captors, still holding their hostages, were offered safe passage and taken to an airport, where they were ambushed by German security forces. In the firefight that followed, 15 people, including the nine Israeli athletes and five of the terrorists, were killed. After much debate, it was decided that the Games would continue, but proceedings were obviously dominated by these events. Some memorable athletic achievements did occur during these Games, notably the winning of a then-record seven gold medals by United States swimmer Mark Spitz, Lasse Virén (of Finland)'s back-to-back gold in the 5,000 meters and 10,000 meters (defeating American distance running great Steve Prefontaine in the former), and the winning of three gold medals by 16-year-old Soviet gymnastic sensation Olga Korbut - who thrilled the world with an historic backflip off the high bar. Korbut, however, failed to win the all-around, losing to her teammate Ludmilla Tourischeva.
There was no such tragedy in Montreal in 1976, but bad planning and fraud led to the Games' cost far exceeding the budget. The Montreal Games were the most expensive in Olympic history, until the 2008 Summer Olympics, costing over $5 billion (equivalent to $20 billion in 2006). For a time, it seemed that the Olympics might no longer be a viable financial proposition. In retrospect, the belief that contractors (suspected of being members of the Montreal Mafia) skimmed large sums of money from all levels of contracts while also profiting from the substitution of cheaper building materials of lesser quality, may have contributed to the delays, poor construction and excessive costs. In 1988, one such contractor, Giuseppe Zappia "was cleared of fraud charges that resulted from his work on Olympic facilities after two key witnesses died before testifying at his trial." There was also a boycott by African nations to protest against a recent tour of apartheid-run South Africa by the New Zealand national rugby union team. The Romanian gymnast Nadia Comăneci won the women's individual all around gold medal with two of four possible perfect scores, thus giving birth to a gymnastics dynasty in Romania. Another female gymnast to earn the perfect score and three gold medals there was Nellie Kim of the USSR. Lasse Virén repeated his double gold in the 5,000 meters and 10,000 meters, making him the only athlete to ever win the distance double twice.
End of the 20th century.
Following the Soviet Union's 1979 invasion of Afghanistan, 66 nations, including the United States, Canada, West Germany and Japan, boycotted the 1980 games held in Moscow. The boycott contributed to the 1980 Games being a less publicised and less competitive affair, which was dominated by the host country.
In 1984 the Soviet Union, and 13 Soviet Allies, reciprocated by boycotting the 1984 Summer Olympics in Los Angeles. Romania, notably, was one of the nations in the Eastern Bloc which did attend the 1984 Olympics. These games were perhaps the first games of a new era to make a profit. The games were again viable, but had become more commercial. Again, without the participation of the Eastern European countries, the 1984 Games were dominated by their host country. The game was also the first time Mainland China (People's Republic) participated.
The 1988 games, in Seoul, were very well planned but the games were tainted when many of the athletes, most notably men's 100 metres winner Ben Johnson, failed mandatory drug tests. Despite splendid drug-free performances by many individuals, the number of people who failed screenings for performance-enhancing chemicals overshadowed the games.
The 1992 Barcelona Games featured increased professionalism among Olympic athletes, exemplified by US basketball's "Dream Team". The 1992 games also saw the reintroduction to the Games of several smaller European states which had been incorporated into the Soviet Union since World War II. These games also saw gymnast Vitaly Scherbo equal the record for most individual gold medals at a single Games set by Eric Heiden in the 1980 Winter Games, with five.
By then the process of choosing a location for the Games had itself become a commercial concern; allegations of corruption rocked the International Olympic Committee. In the Atlanta games in 1996, the highlight was 200 meters runner Michael Johnson annihilating the world record in front of a home crowd. Canadians savored Donovan Bailey's record-breaking gold medal run in the 100-meter dash. This was popularly felt to be an appropriate recompense for the previous national disgrace involving Ben Johnson. There were also emotional scenes, such as when Muhammad Ali, clearly affected by Parkinson's disease, lit the Olympic torch and received a replacement medal for the one he had discarded in 1960. The latter event took place not at the boxing ring but in the basketball arena, at the demand of US television. The atmosphere at the Games was marred, however, when a bomb exploded during the celebration in Centennial Olympic Park. In June 2003, the principal suspect in this bombing, Eric Robert Rudolph, was arrested.
The 2000 Games were held in Sydney, Australia, and showcased individual performances by local favorite Ian Thorpe in the pool, Briton Steve Redgrave who won a rowing gold medal in an unprecedented fifth consecutive Olympics, and Cathy Freeman, an Indigenous Australian whose triumph in the 400 meters united a packed stadium. Eric "the Eel" Moussambani, a swimmer from Equatorial Guinea, had a memorably slow 100 meter freestyle swim that showed that, even in the commercial world of the twentieth century, some of de Coubertin's original vision still remained. The Sydney Games were also memorable for the first appearance of a joint North and South Korean contingent (to a standing ovation) at the opening ceremonies, even if they competed as different countries. Controversy did not escape the 2000 Games in Women's Artistic Gymnastics, in which the vaulting horse was set to the wrong height during the All Around Competition. Several athletes faltered, including Russian Svetlana Khorkina, who had been favored to win gold after qualifying for the competition in first place.
New millennium.
In 2004 the Games returned to their birthplace in Athens, Greece. Greece spent at least $7.2 billion on the Games, including $1.5 billion on security alone. Nonetheless, the Men's Gymnastics events were mired in controversy when it was discovered that Korean gymnast Yang Tae Young had been incorrectly credited with a lower start value, which placed him third behind American Paul Hamm, who won the competition. Later in the event finals, fans halted the Men's High Bar competition with chants of disapproval following the release of the score for Russian Alexei Nemov. Allegations of corrupt judging also marred the event finals in men's still rings. Although unfounded and wildly sensationalized reports of potential terrorism drove crowds away from the preliminary competitions of the first weekend of the games (14–15 August), attendance picked up as the games progressed. Still, a third of the tickets failed to sell. The Athens Games witnessed all 202 NOCs participate with over 11,000 participants.
The 2008 Summer Olympics were held in Beijing, People's Republic of China. This Olympics was the subject of much controversy, especially following the March Tibetan riots. Human rights activists unsuccessfully called for a boycott, and some even compared the 2008 Olympics to the 1936 ones held in Nazi Germany. Several new events were held, including the new discipline of BMX for both men and women. For the first time, women competed in the steeplechase. The fencing program was expanded to include all six events for both men and women. Women had not previously been able to compete in team foil or saber events (although women's team épée and men's team foil were dropped for these Games). Marathon swimming events, over the distance of 10 km, were added. In addition, the doubles events in table tennis were replaced by team events. American swimmer Michael Phelps set a record for gold medals at a single Games with eight, and tied the record of most gold medals by a single competitor previously held by both Heiden and Scherbo. Another major star of the Games was Jamaican sprinter Usain Bolt, who became the first male athlete ever to set world records in the finals of both the 100 and 200 metres in the same Games. Equestrian events were held in Hong Kong. 
London held the 2012 Summer Olympics, becoming the first city to host the Games three times. In his closing address the IOC President, Jacques Rogge described the Games as "Happy and Glorious". They certainly were for the host nation who won 29 Gold Medals, the best haul for Great Britain since the 1908 Games in London. It was also notable that the United States returned to the top of the medal table after China dominated in 2008. The International Olympic Committee had removed baseball and softball from the 2012 program. On a commercial level the Games were successful as they were the first in history to completely sell out every ticket with as many as 1 million applications for 40,000 tickets for both the Opening Ceremony and the 100m Mens Sprint Final. Such was the demand for tickets to all levels of each event, there was controversy when seats set aside for sponsors and National Delegations went unused in the early days. A system of reallocation was put in place so the empty seats were filled throughout the Games.
Rio de Janeiro, Brazil, will host the 2016 Summer Olympics, becoming the third city in the Southern Hemisphere to host the Olympic Games (after Melbourne, Australia, in 1956 and Sydney, Australia, in 2000), and the first South American city to host the Olympics. In October 2009, the IOC included golf and rugby sevens as part of the Olympic program for Rio de Janeiro. Tokyo, Japan, will host the 2020 Summer Olympics.
All-time medal table.
With reference to the top ten nations and according to official data of the International Olympic Committee.
     Past nations
List of Olympic sports.
Forty-two different sports, spanning 55 different disciplines, have been part of the Olympic program at one point or another. Twenty-eight sports have comprised the schedule for three of the recent games, 2000, 2004, and 2008 Summer Olympics. Due to the removal of baseball and softball, there was a total of 26 sports in the 2012 Games.
The various Olympic Sports federations are grouped under a common umbrella association, called the Association of Summer Olympic International Federations (ASOIF).
Popularity of Olympic sports.
Summer Olympic sports are divided into categories based on popularity, gauged by six categories: television viewing figures (40%), internet popularity (20%), public surveys (15%), ticket requests (10%), press coverage (10%), and number of national federations (5%). The category determines the share the sport's International Federation receives of Olympic revenue. Sports that are new to the 2016 Olympics (rugby and golf) have been placed in Category E.
The current categories are:

</doc>
<doc id="27640" url="http://en.wikipedia.org/wiki?curid=27640" title="Samuel Beckett">
Samuel Beckett

Samuel Barclay Beckett (; 13 April 1906 – 22 December 1989) was an Irish avant-garde novelist, playwright, theatre director, and poet, who lived in Paris for most of his adult life and wrote in both English and French. His work offers a bleak, tragicomic outlook on human nature, often coupled with black comedy and gallows humour.
Beckett is widely regarded as among the most influential writers of the 20th century. He is considered one of the last modernists. As an inspiration to many later writers, he is also sometimes considered one of the first postmodernists. He is one of the key writers in what Martin Esslin called the "Theatre of the Absurd". His work became increasingly minimalist in his later career.
Beckett was awarded the 1969 Nobel Prize in Literature "for his writing, which—in new forms for the novel and drama—in the destitution of modern man acquires its elevation". He was elected Saoi of Aosdána in 1984.
Biography.
Early life and education.
The Becketts were members of the Anglican Church of Ireland. The family home, Cooldrinagh in the Dublin suburb of Foxrock, was a large house and garden complete with tennis court built in 1903 by Samuel's father, William. The house and garden, together with the surrounding countryside where he often went walking with his father, the nearby Leopardstown Racecourse, the Foxrock railway station and Harcourt Street station at the city terminus of the line, all feature in his prose and plays.
Samuel Beckett was born on Good Friday, 13 April 1906 to William Frank Beckett, quantity surveyor and Maria Jones Roe, a nurse, when both were 35. They had married in 1901. Beckett had one older brother, Frank Edward Beckett (born 1902). At the age of five, Beckett attended a local playschool, where he started to learn music, and then moved to Earlsfort House School in the city centre near Harcourt Street. In 1919, Beckett went to Portora Royal School in Enniskillen, County Fermanagh (which Oscar Wilde had also attended). A natural athlete, Beckett excelled at cricket as a left-handed batsman and a left-arm medium-pace bowler. Later, he was to play for Dublin University and played two first-class games against Northamptonshire. As a result, he became the only Nobel laureate to have an entry in "Wisden Cricketers' Almanack", the "bible" of cricket.
Early writings.
Beckett studied French, Italian, and English at Trinity College, Dublin from 1923 to 1927 (one of his tutors was the eminent Berkeley scholar A. A. Luce). Beckett graduated with a BA and, after teaching briefly at Campbell College in Belfast, took up the post of "lecteur d'anglais" in the École Normale Supérieure in Paris. While there, he was introduced to renowned Irish author James Joyce by Thomas MacGreevy, a poet and close confidant of Beckett who also worked there. This meeting had a profound effect on the young man. Beckett assisted Joyce in various ways, one of which was research towards the book that became "Finnegans Wake".
In 1929, Beckett published his first work, a critical essay entitled "Dante... Bruno. Vico.. Joyce". The essay defends Joyce's work and method, chiefly from allegations of wanton obscurity and dimness, and was Beckett's contribution to "Our Exagmination Round His Factification for Incamination of Work in Progress" (a book of essays on Joyce which also included contributions by Eugene Jolas, Robert McAlmon, and William Carlos Williams). Beckett's close relationship with Joyce and his family cooled, however, when he rejected the advances of Joyce's daughter Lucia owing to her progressing schizophrenia. Beckett's first short story, "Assumption", was published in Jolas's periodical "transition". The next year he won a small literary prize for his hastily composed poem "Whoroscope", which draws on a biography of René Descartes that Beckett happened to be reading when he was encouraged to submit.
In 1930, Beckett returned to Trinity College as a lecturer. In November 1930, he presented a paper in French to the Modern Languages Society of Trinity on the Toulouse poet Jean du Chas, founder of a movement called "le Concentrisme". It was a literary parody, for Beckett had in fact invented the poet and his movement that claimed to be "at odds with all that is clear and distinct in Descartes." Beckett later insisted that he had not intended to fool his audience. When Beckett resigned from Trinity at the end of 1931, his brief academic career was terminated. He commemorated it with the poem "Gnome", which was inspired by his reading of Johann Wolfgang von Goethe's "Wilhelm Meister's Apprenticeship" and eventually published in the "Dublin Magazine" in 1934:
Spend the years of learning squanderingCourage for the years of wanderingThrough a world politely turningFrom the loutishness of learning
Beckett travelled in Europe. He spent some time in London, where in 1931 he published "Proust", his critical study of French author Marcel Proust. Two years later, following his father's death, he began two years' treatment with Tavistock Clinic psychoanalyst Dr. Wilfred Bion. Aspects of it became evident in Beckett's later works, such as "Watt" and "Waiting for Godot". In 1932, he wrote his first novel, "Dream of Fair to Middling Women", but after many rejections from publishers decided to abandon it (it was eventually published in 1992). Despite his inability to get it published, however, the novel served as a source for many of Beckett's early poems, as well as for his first full-length book, the 1933 short-story collection "More Pricks Than Kicks".
Beckett published essays and reviews, including "Recent Irish Poetry" (in "The Bookman", August 1934) and "Humanistic Quietism", a review of his friend Thomas MacGreevy's "Poems" (in "The Dublin Magazine", July–September 1934). They focused on the work of MacGreevy, Brian Coffey, Denis Devlin and Blanaid Salkeld, despite their slender achievements at the time, comparing them favourably with their Celtic Revival contemporaries and invoking Ezra Pound, T. S. Eliot, and the French symbolists as their precursors. In describing these poets as forming "the nucleus of a living poetic in Ireland", Beckett was tracing the outlines of an Irish poetic modernist canon.
In 1935—the year that Beckett successfully published a book of his poetry, "Echo's Bones and Other Precipitates"—Beckett worked on his novel "Murphy". In May, he wrote to MacGreevy that he had been reading about film and wished to go to Moscow to study with Sergei Eisenstein at the Gerasimov Institute of Cinematography in Moscow. In mid-1936 he wrote to Sergei Eisenstein and Vsevolod Pudovkin to offer himself as their apprentices. Nothing came of this, however, as Beckett's letter was lost owing to Eisenstein's quarantine during the smallpox outbreak, as well as his focus on a script re-write of his postponed film production. In 1936, a friend had suggested him to look up the works of Arnold Geulincx, which Beckett did and he took many notes. The philosopher's name is mentioned in "Murphy" and the reading apparently left a strong impression. "Murphy" was finished in 1936 and Beckett departed for extensive travel around Germany, during which time he filled several notebooks with lists of noteworthy artwork that he had seen and noted his distaste for the Nazi savagery that was overtaking the country. Returning to Ireland briefly in 1937, he oversaw the publication of "Murphy" (1938), which he translated into French the following year. He fell out with his mother, which contributed to his decision to settle permanently in Paris. Beckett remained in Paris following the outbreak of World War II in 1939, preferring, in his own words, "France at war to Ireland at peace". His was soon a known face in and around Left Bank cafés, where he strengthened his allegiance with Joyce and forged new ones with artists Alberto Giacometti and Marcel Duchamp, with whom he regularly played chess. Sometime around December 1937, Beckett had a brief affair with Peggy Guggenheim, who nicknamed him "Oblomov" (after the character in Ivan Goncharov's novel).
In January 1938 in Paris, Beckett was stabbed in the chest and nearly killed when he refused the solicitations of a notorious pimp (who went by the name of Prudent). Joyce arranged a private room for Beckett at the hospital. The publicity surrounding the stabbing attracted the attention of Suzanne Dechevaux-Dumesnil, who previously knew Beckett slightly from his first stay in Paris. This time, however, the two would begin a lifelong companionship. At a preliminary hearing, Beckett asked his attacker for the motive behind the stabbing. Prudent replied: "Je ne sais pas, Monsieur. Je m'excuse" ["I do not know, sir. I'm sorry"]. Beckett eventually dropped the charges against his attacker—partially to avoid further formalities, partly because he found Prudent likeable and well-mannered.
World War II.
Beckett joined the French Resistance after the 1940 occupation by Germany, in which he worked as a courier. On several occasions over the next two years he was nearly caught by the Gestapo. In August 1942, his unit was betrayed and he and Suzanne fled south on foot to the safety of the small village of Roussillon, in the Vaucluse "département" in Provence-Alpes-Côte d'Azur. There he continued to assist the Resistance by storing armaments in the back yard of his home. During the two years that Beckett stayed in Roussillon he indirectly helped the Maquis sabotage the German army in the Vaucluse mountains, though he rarely spoke about his wartime work in later life.
Beckett was awarded the Croix de guerre and the Médaille de la Résistance by the French government for his efforts in fighting the German occupation; to the end of his life, however, Beckett would refer to his work with the French Resistance as "boy scout stuff". While in hiding in Roussillon, he continued work on the novel "Watt" (begun in 1941 and completed in 1945, but not published until 1953, though an extract had appeared in the Dublin literary periodical "Envoy").
Fame: novels and the theatre.
In 1945, Beckett returned to Dublin for a brief visit. During his stay, he had a revelation in his mother’s room: his entire future direction in literature appeared to him. Beckett had felt that he would remain forever in the shadow of Joyce, certain to never best him at his own game. His revelation prompted him to change direction and to acknowledge both his own stupidity and his interest in ignorance and impotence:
"I realized that Joyce had gone as far as one could in the direction of knowing more, [being] in control of one’s material. He was always adding to it; you only have to look at his proofs to see that. I realized that my own way was in impoverishment, in lack of knowledge and in taking away, in subtracting rather than in adding."
Knowlson argues that "Beckett was rejecting the Joycean principle that knowing more was a way of creatively understanding the world and controlling it ... In future, his work would focus on poverty, failure, exile and loss – as he put it, on man as a 'non-knower' and as a 'non-can-er.'" The revelation "has rightly been regarded as a pivotal moment in his entire career." Beckett fictionalised the experience in his play "Krapp's Last Tape" (1958). While listening to a tape he made earlier in his life, Krapp hears his younger self say "clear to me at last that the dark I have always struggled to keep under is in reality my most...", at which point Krapp fast-forwards the tape (before the audience can hear the complete revelation). Beckett later explained to Knowlson that the missing words on the tape are "precious ally".
In 1946, Jean-Paul Sartre’s magazine "Les Temps Modernes" published the first part of Beckett’s short story ""Suite" (later to be called "La Fin"", or "The End"), not realizing that Beckett had only submitted the first half of the story; Simone de Beauvoir refused to publish the second part. Beckett also began to write his fourth novel, "Mercier et Camier", which was not published until 1970. The novel presaged his most famous work, the play "Waiting for Godot", which was written not long afterwards. More importantly, the novel was Beckett’s first long work that he wrote in French, the language of most of his subsequent works which were strongly supported by Jérôme Lindon director of his parisian publishing house Les Éditions de Minuit, including the poioumenon "trilogy" of novels: "Molloy" (1951); "Malone meurt" (1951), "Malone Dies" (1958); "L'innommable" (1953), "The Unnamable", (1960). Despite being a native English speaker, Beckett wrote in French because—as he himself claimed—it was easier for him thus to write "without style".
Beckett is most famous for his play "En attendant Godot" (1953) ("Waiting for Godot"). In a much-quoted article, the critic Vivian Mercier wrote that Beckett "has achieved a theoretical impossibility—a play in which nothing happens, that yet keeps audiences glued to their seats. What's more, since the second act is a subtly different reprise of the first, he has written a play in which nothing happens, twice." Like most of his works after 1947, the play was first written in French with the title "En attendant Godot". Beckett worked on the play between October 1948 and January 1949. He published it in 1952 and it premièred in 1953; an English translation appeared two years later. Directed by Roger Blin, the play was a critical, popular, and controversial success in Paris. It opened in London in 1955 to mainly negative reviews, but the tide turned with positive reactions from Harold Hobson in "The Sunday Times" and, later, Kenneth Tynan. In the United States, it flopped in Miami and had a qualified success in New York City. After this, the play became extremely popular, with highly successful performances in the US and Germany. It is frequently performed today.
Beckett translated all of his works into English himself, with the exception of "Molloy", for which he collaborated with Patrick Bowles. The success of "Waiting for Godot" opened up a career in theatre for its author. Beckett went on to write successful full-length plays, including "Fin de partie" ("Endgame") (1957), "Krapp's Last Tape" (1958, written in English), "Happy Days" (1961, also written in English), and "Play" (1963). In 1961, Beckett received the International Publishers' Formentor Prize in recognition of his work, which he shared that year with Jorge Luis Borges.
Later life and death.
The 1960s was a period of change for Beckett, both on a personal level and as a writer. In 1961, he married Suzanne in a secret civil ceremony in England (its secrecy due to reasons relating to French inheritance law). The success of his plays led to invitations to attend rehearsals and productions around the world, leading eventually to a new career as a theatre director. In 1956, he had his first commission from the BBC Third Programme for a radio play, "All That Fall". He continued writing sporadically for radio and extended his scope to include cinema and television. He began to write in English again, although he also wrote in French until the end of his life.
Beckett bought some land in 1953 near a hamlet around forty miles northeast of Paris and built a cottage for himself with the help of some locals. One of the locals that helped him build the cottage was a Bulgarian-born farmer named Boris Rousimoff, who Beckett befriended. Rousimoff’s son was André the Giant and when Beckett found out that Rousimoff was having trouble getting his son to school due to his size, Beckett offered to drive André to school in his truck — a vehicle that could fit André. When André recounted the drives with Beckett, he revealed they rarely talked about anything other than cricket.
From the late 1950s until his death, Beckett had a relationship with Barbara Bray, a widow who worked as a script editor for the BBC. Knowlson wrote of them: "She was small and attractive, but, above all, keenly intelligent and well-read. Beckett seems to have been immediately attracted by her and she to him. Their encounter was highly significant for them both, for it represented the beginning of a relationship that was to last, in parallel with that with Suzanne, for the rest of his life".
In October 1969 while on holiday in Tunis with Suzanne, Beckett heard that he had won the Nobel Prize for Literature. Anticipating that her intensely private husband would be saddled with fame from that moment on, Suzanne called the award a "catastrophe". In true ascetic fashion, he gave away all of the prize money. While Beckett did not devote much time to interviews, he sometimes met the artists, scholars, and admirers who sought him out in the anonymous lobby of the Hotel PLM St. Jacques in Paris near his Montparnasse home. Although Beckett was an intensely private man, a review of the second volume of his letters by Roy Foster in the 15 December 2011 issue of "The New Republic", reveals Beckett to be not only unexpectedly amiable but frequently prepared to talk about his work and the process behind it.
Suzanne died on 17 July 1989. Confined to a nursing home and suffering from emphysema and possibly Parkinson's disease, Beckett died on 22 December. The two were interred together in the Cimetière du Montparnasse in Paris and share a simple granite gravestone that follows Beckett's directive that it should be "any colour, so long as it's grey."
Barbara Bray died in Edinburgh on 25 February 2010.
Works.
Beckett's career as a writer can be roughly divided into three periods: his early works, up until the end of World War II in 1945; his middle period, stretching from 1945 until the early 1960s, during which period he wrote what are probably his best-known works; and his late period, from the early 1960s until Beckett's death in 1989, during which his works tended to become shorter and his style more minimalist.
Early works.
Beckett's earliest works are generally considered to have been strongly influenced by the work of his friend James Joyce. They are erudite and seem to display the author's learning merely for its own sake, resulting in several obscure passages. The opening phrases of the short-story collection "More Pricks than Kicks" (1934) affords a representative sample of this style:
It was morning and Belacqua was stuck in the first of the canti in the moon. He was so bogged that he could move neither backward nor forward. Blissful Beatrice was there, Dante also, and she explained the spots on the moon to him. She shewed him in the first place where he was at fault, then she put up her own explanation. She had it from God, therefore he could rely on its being accurate in every particular.
The passage makes reference to Dante's "Commedia", which can serve to confuse readers not familiar with that work. It also anticipates aspects of Beckett's later work: the physical inactivity of the character Belacqua; the character's immersion in his own head and thoughts; the somewhat irreverent comedy of the final sentence.
Similar elements are present in Beckett's first published novel, "Murphy" (1938), which also explores the themes of insanity and chess (both of which would be recurrent elements in Beckett's later works). The novel's opening sentence hints at the somewhat pessimistic undertones and black humour that animate many of Beckett's works: "The sun shone, having no alternative, on the nothing new". "Watt", written while Beckett was in hiding in Roussillon during World War II, is similar in terms of themes but less exuberant in its style. It explores human movement as if it were a mathematical permutation, presaging Beckett's later preoccupation—in both his novels and dramatic works—with precise movement.
Beckett's 1930 essay "Proust" was strongly influenced by Schopenhauer's pessimism and laudatory descriptions of saintly asceticism. At this time Beckett began to write creatively in the French language. In the late 1930s, he wrote a number of short poems in that language and their sparseness—in contrast to the density of his English poems of roughly the same period, collected in "Echo's Bones and Other Precipitates" (1935)—seems to show that Beckett, albeit through the medium of another language, was in process of simplifying his style, a change also evidenced in "Watt".
Middle period.
<poem>
who may tell the tale
of the old man?
weigh absence in a scale?
mete want with a span?
the sum assess
of the world's woes?
nothingness
in words enclose?
</poem>
”
From "Watt" (1953)
After World War II, Beckett turned definitively to the French language as a vehicle. It was this, together with the "revelation" experienced in his mother's room in Dublin—in which he realized that his art must be subjective and drawn wholly from his own inner world—that would result in the works for which Beckett is best remembered today.
During the 15 years following the war, Beckett produced four major full-length stage plays: "En attendant Godot" (written 1948–1949; "Waiting for Godot"), "Fin de partie" (1955–1957; "Endgame"), "Krapp's Last Tape" (1958), and "Happy Days" (1961). These plays—which are often considered, rightly or wrongly, to have been instrumental in the so-called "Theatre of the Absurd"—deal in a very blackly humorous way with themes similar to those of the roughly contemporary existentialist thinkers. The term "Theatre of the Absurd" was coined by Martin Esslin in a book of the same name; Beckett and "Godot" were centerpieces of the book. Esslin claimed these plays were the fulfilment of Albert Camus's concept of "the absurd"; this is one reason Beckett is often falsely labeled as an existentialist (this is based on the assumption that Camus was an existentialist, though he in fact broke off from the existentialist movement and founded his own philosophy). Though many of the themes are similar, Beckett had little affinity for existentialism as a whole.
Broadly speaking, the plays deal with the subject of despair and the will to survive in spite of that despair, in the face of an uncomprehending and incomprehensible world. The words of Nell—one of the two characters in "Endgame" who are trapped in ashbins, from which they occasionally peek their heads to speak—can best summarize the themes of the plays of Beckett's middle period: "Nothing is funnier than unhappiness, I grant you that. ... Yes, yes, it's the most comical thing in the world. And we laugh, we laugh, with a will, in the beginning. But it's always the same thing. Yes, it's like the funny story we have heard too often, we still find it funny, but we don't laugh any more."
Beckett's outstanding achievements in prose during the period were the three novels "Molloy" (1951), "Malone meurt" (1951; "Malone Dies") and "L'innommable" (1953: "The Unnamable"). In these novels—sometimes referred to as a "trilogy", though this is against the author's own explicit wishes—the prose becomes increasingly bare and stripped down. "Molloy", for instance, still retains many of the characteristics of a conventional novel (time, place, movement, and plot) and it makes use of the structure of a detective novel. In "Malone Dies", movement and plot are largely dispensed with, though there is still some indication of place and the passage of time; the "action" of the book takes the form of an interior monologue. Finally, in "The Unnamable", almost all sense of place and time are abolished, and the essential theme seems to be the conflict between the voice's drive to continue speaking so as to continue existing, and its almost equally strong urge towards silence and oblivion. Despite the widely held view that Beckett's work, as exemplified by the novels of this period, is essentially pessimistic, the will to live seems to win out in the end; witness, for instance, the famous final phrase of "The Unnamable": 'I can't go on, I'll go on'.
After these three novels, Beckett struggled for many years to produce a sustained work of prose, a struggle evidenced by the brief "stories" later collected as "Texts for Nothing". In the late 1950s, however, he created one of his most radical prose works, "Comment c'est" (1961; "How It Is". An early variant version of "Comment c'est", "L'Image", was published in the British arts review, "X: A Quarterly Review" (1959), and is the first appearance of the novel in any form.). This work relates the adventures of an unnamed narrator crawling through the mud while dragging a sack of canned food. It was written as a sequence of unpunctuated paragraphs in a style approaching telegraphese: "You are there somewhere alive somewhere vast stretch of time then it's over you are there no more alive no more than again you are there again alive again it wasn't over an error you begin again all over more or less in the same place or in another as when another image above in the light you come to in hospital in the dark" Following this work, it was almost another decade before Beckett produced a work of non-dramatic prose. "How It Is" is generally considered to mark the end of his middle period as a writer. 
Late works.
<poem>
time she stopped
sitting at her window
quiet at her window
only window
facing other windows
other only windows
all eyes
all sides
high and low
time she stopped
</poem>
”
From "Rockaby" (1980)
Throughout the 1960s and into the 1970s, Beckett's works exhibited an increasing tendency—already evident in much of his work of the 1950s—towards compactness. This has led to his work sometimes being described as minimalist. The extreme example of this, among his dramatic works, is the 1969 piece "Breath", which lasts for only 35 seconds and has no characters (though it was likely intended to offer ironic comment on "Oh! Calcutta!", the theatrical revue for which it served as an introductory piece).
In his theatre of the late period, Beckett's characters—already few in number in the earlier plays—are whittled down to essential elements. The ironically titled "Play" (1962), for instance, consists of three characters immersed up to their necks in large funeral urns. The television drama "Eh Joe" (1963), which was written for the actor Jack MacGowran, is animated by a camera that steadily closes in to a tight focus upon the face of the title character. The play "Not I" (1972) consists almost solely of, in Beckett's words, "a moving mouth with the rest of the stage in darkness". Following from "Krapp's Last Tape", many of these later plays explore memory, often in the form of a forced recollection of haunting past events in a moment of stillness in the present. They also deal with the theme of the self confined and observed, with a voice that either comes from outside into the protagonist's head (as in "Eh Joe") or else another character comments on the protagonist silently, by means of gesture (as in "Not I"). Beckett's most politically charged play, "Catastrophe" (1982), which was dedicated to Václav Havel, deals relatively explicitly with the idea of dictatorship. After a long period of inactivity, Beckett's poetry experienced a revival during this period in the ultra-terse French poems of "mirlitonnades", with some as short as six words long. These defied Beckett's usual scrupulous concern to translate his work from its original into the other of his two languages; several writers, including Derek Mahon, have attempted translations, but no complete version of the sequence has been published in English.
Beckett's prose pieces during the late period were not so prolific as his theatre, as suggested by the title of the 1976 collection of short prose texts "Fizzles" (which the American artist Jasper Johns illustrated). Beckett experienced something of a renaissance with the novella "Company" (1980), which continued with "Ill Seen Ill Said" (1982) and "Worstward Ho" (1984), later collected in "Nohow On". In these three "'closed space' stories", Beckett continued his preoccupation with memory and its effect on the confined and observed self, as well as with the positioning of bodies in space, as the opening phrases of "Company" make clear: "A voice comes to one in the dark. Imagine." "To one on his back in the dark. This he can tell by the pressure on his hind parts and by how the dark changes when he shuts his eyes and again when he opens them again. Only a small part of what is said can be verified. As for example when he hears, You are on your back in the dark. Then he must acknowledge the truth of what is said."
In the hospital and nursing home where he spent his final days, Beckett wrote his last work, the 1988 poem "What is the Word" ("Comment dire"). The poem grapples with an inability to find words to express oneself, a theme echoing Beckett's earlier work, though possibly amplified by the sickness he experienced late in life.
Collaborators.
Jack MacGowran.
Jack MacGowran was the first actor to do a one-man show based on the works of Beckett. He debuted "End of Day" in Dublin in 1962, revising it as "Beginning To End" (1965). The show went through further revisions before Beckett directed it in Paris in 1970; MacGowran won the 1970-71 Obie for Best Performance By an Actor when he performed the show off-Broadway as "Jack MacGowran in the Works of Samuel Beckett." Beckett wrote the radio play "Embers" and the teleplay "Eh Joe" specifically for MacGowran. The actor also appeared in various productions of "Waiting for Godot" and "Endgame," and did several readings of Beckett's plays and poems on BBC Radio; he also recorded the LP, "MacGowran Speaking Beckett."
Billie Whitelaw.
Billie Whitelaw worked with Beckett for 25 years on such plays as "Not I", "Eh Joe", "Krapp's Last Tape", and "Footfalls" and "Rockaby.". She first met Beckett in 1963. In her autobiography she describes their first meeting in 1963 was "trust at first sight". Beckett went on to write many of his experimental theatre works for her. She came to be regarded as his muse, the "supreme interpreter of his work", perhaps most famous for her role as the mouth in "Not I". She said of the play "Rockabye": "I put the tape in my head. And I sort of look in a particular way, but not at the audience. Sometimes as a director Beckett comes out with absolute gems and I use them a lot in other areas. We were doing "Happy Days" and I just did not know where in the theatre to look during this particular section. And I asked, and he thought for a bit and then said, 'Inward' ". She said of her role in "Footfalls": "I felt like a moving, musical Edvard Munch painting and, in fact, when Beckett was directing "Footfalls" he was not only using me to play the notes but I almost felt that he did have the paintbrush out and was painting." "Sam knew that I would turn myself inside out to give him what he wanted", she explained. "With all of Sam's work, the scream was there, my task was to try to get it out." She stopped performing his plays in 1989 when he died.
Jocelyn Herbert.
The English stage designer Jocelyn Herbert was a close friend and influence on Beckett until his death. She worked with him on such plays as "Happy Days" (their third project) and "Krapp's Last Tape" at the Royal Court Theatre. Beckett said that Herbert became his closest friend in England: "She has a great feeling for the work and is very sensitive and doesn't want to bang the nail on the head. Generally speaking, there is a tendency on the part of designers to overstate, and this has never been the case with Jocelyn."
Legacy.
Of all the English-language modernists, Beckett's work represents the most sustained attack on the realist tradition. He opened up the possibility of theatre and fiction that dispense with conventional plot and the unities of time and place in order to focus on essential components of the human condition. Václav Havel, John Banville, Aidan Higgins, Tom Stoppard, Harold Pinter and Jon Fosse have publicly stated their indebtedness to Beckett's example. He has had a wider influence on experimental writing since the 1950s, from the Beat generation to the happenings of the 1960s and after. In an Irish context, he has exerted great influence on poets such as Derek Mahon and Thomas Kinsella, as well as writers like Trevor Joyce and Catherine Walsh who proclaim their adherence to the modernist tradition as an alternative to the dominant realist mainstream.
Many major 20th-century composers including Luciano Berio, György Kurtág, Morton Feldman, Pascal Dusapin, Scott Fields, Philip Glass, Roman Haubenstock-Ramati and Heinz Holliger have created musical works based on Beckett's texts. His work has also influenced numerous international writers, artists and filmmakers including Edward Albee, Avigdor Arikha, Paul Auster, Richard Kalich, J.M. Coetzee, Douglas Gordon, Bruce Nauman, Anthony Minghella, and Damian Pettigrew.
Beckett is one of the most widely discussed and highly prized of 20th-century authors, inspiring a critical industry to rival that which has sprung up around James Joyce. He has divided critical opinion. Some early philosophical critics, such as Sartre and Theodor Adorno, praised him, one for his revelation of absurdity, the other for his works' critical refusal of simplicities; others such as Georg Lukács condemn for 'decadent' lack of realism.
Since Beckett's death, all rights for performance of his plays are handled by the Beckett estate, currently managed by Edward Beckett (the author's nephew). The estate has a controversial reputation for maintaining firm control over how Beckett's plays are performed and does not grant licenses to productions that do not adhere to the writer's stage directions.
Historians interested in tracing Beckett's blood line were, in 2004, granted access to confirmed trace samples of his DNA to conduct molecular genealogical studies to facilitate precise lineage determination.
Some of the best-known pictures of Beckett were taken by photographer John Minihan, who photographed him between 1980 and 1985 and developed such a good relationship with the writer that he became, in effect, his official photographer. Some consider one of these to be among the top three photographs of the 20th century. It was the theater photographer John Haynes, however, who took possibly the most widely reproduced image of Beckett: it is used on the cover of the Knowlson biography, for instance. This portrait was taken during rehearsals of the San Quentin Drama Workshop at the Royal Court Theatre in London, where Haynes photographed many productions of Beckett's work.
An Post, the Irish postal service, issued a commemorative stamp of Beckett in 1994.
The Central Bank of Ireland launched two Samuel Beckett Centenary commemorative coins on 26 April 2006: €10 Silver Coin and €20 Gold Coin.
On 10 December 2009, the new bridge across the River Liffey in Dublin was opened and named the Samuel Beckett Bridge in his honour. Reminiscent of a harp on its side, it was designed by the celebrated Spanish architect Santiago Calatrava, who had also designed the James Joyce Bridge further upstream opened on Bloomsday (16 June) 2003. Attendees at the official opening ceremony included Beckett’s niece Caroline Murphy, his nephew Edward Beckett, poet Seamus Heaney and Barry McGovern.
The newest ship of the Irish Naval Service, the LE Samuel Beckett, is named for Beckett. An Ulster History Circle blue plaque in his memory is located at Portora Royal School, Enniskillen, County Fermanagh.
Enniskillen International Beckett Festival (also known as Happy Days) is an annual multi-arts festival celebrating the work and influence of Beckett. The festival, founded in 2011, is held at Enniskillen, Northern Ireland where Beckett spent his formative years studying at Portora Royal School.

</doc>
<doc id="27650" url="http://en.wikipedia.org/wiki?curid=27650" title="September 16">
September 16

September 16 is the day of the year in the Gregorian calendar.

</doc>
<doc id="27669" url="http://en.wikipedia.org/wiki?curid=27669" title="Spanish cuisine">
Spanish cuisine

Spanish cuisine is a way of preparing varied dishes, which is enriched by the culinary contributions of the various regions that make up the country. It is a cuisine influenced by the people who, throughout history, have conquered the territory of that country.
History of Spain.
Spain as a territory of the Roman Empire.
The Romans introduced the custom of collecting and eating mushrooms, which is still preserved in many parts of Spain, especially in the north. The Romans along with the Greeks introduced viticulture; it also appears that the extension of the vine along the Mediterranean seems to be due to colonization of the Greeks.
Middle Ages.
The Visigoths introduced brewing. The change came in 711 AD, when Muslim troops composed of Arabs and Berbers crossed the Strait of Gibraltar, invading the Iberian Peninsula. The Muslim conquest brought new ingredients to Spanish cuisine from the Muslim world, such as Persia and India.
The cuisine of Al-Andalus included such ingredients as: rice, sorghum, sugar cane, spinach, eggplant, watermelon, lemon, peach, orange and almonds. It is common for modern dishes to possess Berber and Arab roots.
The "New World".
The discovery of America, in 1492, initiated the advent of new culinary elements, such as tomato, cucumber, potato, corn, bell pepper, spicy pepper, paprika, vanilla and cocoa or chocolate. The latter caused a furor in the Spanish society in the sixteenth and seventeenth centuries; Spain was where it was first mixed with sugar to remove its natural bitterness. Other ingredients traveled to the Americas, such as rice, grapes, olives and many types of cereals.
Spanish regional variation: typical dishes and meal routines.
La comida, the large midday meal in Spain contains several courses. It spans about two hours from 2:00 pm to 4:00 pm, and is usually followed by Sobremesa, which refers to the tabletalk that Spanish people undertake. Menus are organized according to these courses and include five or six choices in each course. At home, Spanish meals wouldn't be too fancy, and would contain soup or a pasta dish, salad, a meat or a fish dish and a dessert such as fruit or cheese. Green salad with the meat or fish courses.
The following is a list of traditional Spanish meals:
Andalucia.
Andalusian cuisine is twofold: rural and coastal. Of all the Spanish regions, this region uses the most olive oil in its cuisine. The dish that has achieved the most international fame is Gazpacho. It is a kind of cold soup made with five vegetables, bread, vinegar, water, salt and olive oil. Other cold soups include: pulley, Zoque, salmorejo, etc.
Snacks made with olives are common. Meat dishes include: flamenquín, pringá, oxtail and often gypsy (also called Andalusian tripe). Among the hot soups are: cat soup (made with bread), dog stew (fish soup with orange juice) and Migas Canas. Fish dishes include: fried fish, cod pavías, and parpandúas. A culinary custom is the typical Andalusian breakfast, considered to be a traditional characteristic of laborers and today extending throughout Spain.
Cured meats include: Serrano Ham and Jabugo. Typical drinks in the area include: anise, wine (Malaga, Jerez, Pedro Ximénez, etc..) and sherry brandy.
Aragon.
The Aragonese cuisine has a rural and mountainous origin. The central part of Aragon, the flattest, is the richest in culinary specialties. Being a land of lambs raised on the slopes of the Pyrenees, one of its most famous dishes is roast lamb (asado de ternasco) (with garlic, salt and bacon fat), having the lamb to the shepherd, the heads of lamb and Highlanders asparagus (lamb tails). Pork dishes are also very popular, among them: Magras con tomate, roasted pork leg and Almojábanas de Cerdo. Among the recipes made with bread are: migas de Pastor, migas con chocolate, Regañaos (cakes with sardines or herring) and goguera. The most notable condiment is garlic-oil.
Legumes are very important and the most popular vegetables are borage and thistle. In terms of cured meats, ham from Teruel and Huesca are famous. Among the cheeses Tronchon is notable. Fruit-based cuisine includes the very popular Fruits of Aragon (Spanish: Frutas de Aragón) and Maraschino cherries.
Asturias.
Asturian cuisine has a long and rich history, deeply rooted in Celtic traditions of northern Europe. One of its most famous dishes is the Asturian bean stew, which is the traditional stew of the region, made with white beans, sausages such as chorizo and morcilla and pork. Another well-known recipe is beans with clams, hare and partridge. Also of note are Asturian stew and vigil. Pork-based foods, for example chosco, tripe Asturias and bollos preñaos are popular.
Common meat dishes include: carne gobernada, cachopo and stew. Asturian cheeses are very popular in the rest of Spain. Among them, the most representative is Cabrales Cheese a strong-smelling cheese developed in the regions near the Picos de Europa. This can be enjoyed with the local cider. Notable desserts are frisuelos, rice pudding and carbayones.
Balearic Islands.
The Balearic cuisine has purely Mediterranean characteristics. The islands have been conquered several times throughout their history by the French and the English, which has left some culinary influences. At present are well known: the spicy sausage and rice brut, cheese Mahon, Mahon Gin ("pellofa") and mayonnaise. Among the dishes are tumbet, variat frit and roast suckling pig.
Among the desserts are: Ensaimadas, drum almond, sighs of Manacor.
Basque Country.
The cuisine of the Basque Country is a wide and varied range of ingredients and preparations. The culture of eating is very strong among the inhabitants of this region. Highlights include meat and fish dishes. Among fish, cod is produced in various preparations: bacalao al pil pil, cod Bilbao, etc.. Are also common anchovy, bream, bonito, etc.. Among the most famous dishes is the seafood changurro. Among the meats are: the beef steaks, pork loin with milk, fig leaf quail, marinated goose, etc.
Canary Islands.
The Canary Islands have a unique cuisine because of their insular nature and location in the Atlantic. It is based on the gofio food of the Guanches, the result of different toasted grains.
Among the most typical fruits are: bananas, yams, mangoes and persimmons. The fish dishes are well placed and are usually accompanied by a sauce called mojo, known as mojo picon. Stew is one of many similarly-prepared dishes. Prominent among the gastronomy are: wrinkled potatoes, almogrote, frangollo, rabbit in salmorejo, stewed goat, etc. The most popular sweets are trout with potato or pumpkin, roasted maize meal nougat, etc. Among the wines, the best known is the Malvasia wine.
Cantabria.
A popular Cantabrian dish is "cocido montañés", a rich stew made with beans, cabbage and pork. Seafood is widely used and bonito is present in the typical "sorropotún" or marmite. Recognized quality meats are Tudanca veal and game meat. Cantabrian pastries include "sobaos" and "quesadas pasiegas". Dairy products include Cantabrian cream cheese, smoked cheeses, "picón Bejes-Tresviso" and "quesucos de Liébana". "Orujo" is the Cantabrian pomace brandy. Cider ("sidra") and "chacoli" wine are increasing in popularity.
Cantabria has two wines labelled DOC: Costa de Cantabria and Liébana.
Castile-La Mancha.
In this region, the culinary habits reflect the origin of foods eaten by shepherds and peasants. "Al-Manchara" means, in Arabic, "Dry Land" indicating the arid lands and the quality of its dishes. It is said that the best La Mancha cuisine cookbook is the novel "Don Quixote" by Miguel de Cervantes. Wheat and grains are dominant, used in bread, soups, gazpacho manchego, crumbs, porridge, etc.. One of the most abundant ingredients in Manchego cuisine is garlic, leading to dishes such as: ajoarriero, ajopuerco and garlic marinade.
Some traditional recipes are gazpacho manchego, pisto manchego and migas ruleras. Also popular is morteruelo, a kind of foie gras manchego. Manchego cheese is renowned.
Given the fact that its lands are dry, and thus unable to substain big amounts of cattle living on grass, an abundance of small animals, such as rabbit, and especially birds (pheasant, quail, partridge, squab) can be found. This has led to game meat being incorporated into traditional dishes, such as Conejo al Ajillo (rabbit in garlic sauce), Perdiz Escabechada (marinated partridge) or Huevos de Codorniz (Quail's eggs).
Castile and León.
In Castile and León characteristic dishes include morcilla, Valladolid (a black pudding made with special spices), "judión de la granja", "sopa de ajo" (garlic soup), "Cochinillo asado" (roast piglet), "lechazo" (roast lamb), "botillo del Bierzo", "hornazo" from Salamanca, "Jamón de Guijuelo" (a cured ham from Guijuelo, Salamanca), "Salchichas de Zaratán" and other sausages, Serrada cheese, Burgos's soft cheese, and Ribera del Duero wines.
Major wines in Castilian-Leonese cuisine include the robust wine of Toro, reds from Ribera del Duero, whites from Rueda, and clarets from Cigales.
Catalonia.
The cuisine of Catalonia is based in a rural culture; it is very extensive and a great culinary wealth. Notably, it was in Catalonia where the first cookbook was written in Spain. It has a triple cuisine: seafood, mountain and interior. Among the most popular dishes include: escudella and tomato bread. Bean tortilla, Coca de recapte, samfaina, farigola soup and snails are famous dishes. Notable sauces are: romesco sauce, aioli, bouillabaisse of Catalan origin and picada.
Cured pork cuisine boasts sausage (white and black) and the salami and pepperoni of Vic. Among the fish dishes are: suquet, stewed cod and black rice. Among the vegetable dishes, the most famous are calçots and the Escalivada (roasted vegetables). Among the desserts are: Catalan cream, carquiñoles, panellets, Kings Tortel, kink and neulas.
La Rioja.
La Rioja is recognized by the use of meats such as pork, and their cold cuts made after the traditional slaughter. The lamb is perhaps the second most popular meat product in this region (Sarmiento chops) and finally, veal is common in mountain areas. The most famous dish is Rioja potatoes and Fritada. Lesser known are: Holy lunch and Ajo huevo (garlic eggs).
Another well-known dish is Rioja stew. Pimientos asados (roasted peppers) is a notable vegetable dish. Rioja wine has designated origin status.
Extremadura.
The cuisine of Extremadura is austere, with dishes prepared by shepherds. It is very similar to the cuisine of Castilla. Extremaduran cuisine is abundant in pork; it is said that the region is one of the best for breeding pigs in Spain, thanks to the acorns that grow in their fields: Iberian pig herds raised in the fields of Montánchez are characterized by dark skin and black, thin legs. This breed of pig is found exclusively in Spain and Portugal. Iberian pork sausages are common, such as pork stews (cocido extremeño).
Another meat dishes is lamb stew. It is also known that lizard is often cooked in Extremadura. Highlights include game meats such as wild boar, partridge, pheasant or venison. Famous cheeses are Torta de la Serena and Torta de casar. Among the desserts are: Leche frita, perrunillas and fritters, as well as many sweets that have their origins in convents.
Galicia.
Galician cuisine is known in Spanish territory because of the emigration of its inhabitants. One of the most noted is Galician soup. Also notable is pork with turnip tops, a popular component of the Galician carnival meal laconadas. Another remarkable recipe is Caldo de castañas (a chestnut broth), which is commonly consumed during winter. Pork products are also popular.
The seafood dishes are very famous and rich in variety. Among these are: the Galician empanada, Galician octopus, scallops, crab and barnacles. Among the many dairy products is Queso de tetilla. Orujo is one of Galicia's alcoholic drinks. Sweets that are famous throughout the Iberian Peninsula are the Tarta de Santiago and Filloas (pancakes made with blood).
Madrid.
Madrid did not gain its own identity in the Court until 1561, when Philip II moved the capital to Madrid. Since then, due to immigration, many of Madrid's culinary dishes have been made from modifications to dishes from other Spanish regions. Madrid, due to the influx of visitors from the nineteenth century onwards, was one of the first cities to introduce the concept of the restaurant, hosting some of the earliest examples.
Notable dairy products are: rice pudding, meringue milk, cheese and curd. Some important fruits and vegetables are Aranjuez strawberries and melons. Madrid is rich in religious confectionery, with sweets such as chocolate con churros and buñuelos.
Murcia.
The cuisine of the region of Murcia has two sides with the influence of Manchego cuisine. The region of Murcia is famous for its varied fruit production. Among the most outstanding dishes are: Murcia tortilla, zarangollo, mojete, eggplants cream, pipirrana, etc.. A typical sauce of this area is the cabañil garlic, used to accompany meat dishes.
Among the culinary preparations are: the michirones (dried beans cooked with bay leaves, hot peppers and garlic). Among the cooked include: the gypsy pot, cooked with balls, mondongo, etc.. Among meat products Murcia find black pudding, which is flavored with oregano, and Murcia cake that is made with ground beef. Among the fish and seafood are: the golden salt, the Mar Menor prawns and octopus baked. Rices are common and among them are: the cauldron, the pavement rice, rice with rabbit and snails, rice scribe, and the widower rice.
Among confectionary products are: the Salteadores(Robbers) and Pastel de Cierva(Doe Cake, them are some typical cakes in Murcia gastronomy,they are found in almost all pastry shop in Murcia,are both sweet and savory at the same time.
The desserts are very abundant, among them are: paparajotes Orchard, stufed pastries and various pastries. This region also has wine appellation of origin, as the wine from Jumilla, Bullas wine and wine Yecla.
Navarre.
The gastronomy of Navarra has many similarities with the Aragonese cuisine. Two of his dishes flag are: trout to Navarre and cochifrito, although we must not forget the lamb chilindrón. Among the dishes is the Garbure. There are very curious recipes such as the Carlists eggs.
Salted products are common and, between them, include: chorizo de Pamplona, stuffing and sausage. The lamb and beef have, at present, designations of origin. Within the dairy include: Roncal cheese, the curd or Idiazabal cheese. Among the most typical alcoholic drinks are: the claret and pacharán.
Valencia.
The cuisine of Valencia has two components: the rural (products of the field) and the other coastal, which is seafood. One of the most popular dishes is paella, but there are many other rice dishes, such as Arroz con costra, fideuá and throw rice, Arroz al horno, and rice with beans and turnips.
Coastal towns supply the region with fish, leading to popular dishes like "all i pebre" typical of the Albufera of Valencia, or fish stew. Among the desserts are: coffee liqueur, chocolate Alicante, arnadí and horchata. Notably, during Christmas, nougat is made in Alicante and Jijona; also well-known are peladillas (almonds wrapped in a thick layer of caramel).
Other Spanish dishes:
See also.
Derivatives:

</doc>
<doc id="27694" url="http://en.wikipedia.org/wiki?curid=27694" title="Satan">
Satan

Satan (Hebrew: שָּׂטָן "satan", meaning "adversary"; Arabic: شيطان "shaitan", meaning "astray" or "distant," sometimes "devil") is a figure appearing in the texts of the Abrahamic religions who brings evil and temptation, and is known as the deceiver who leads humanity astray. Some religious groups teach that he originated as an angel who fell out of favor with God, seducing humanity into the ways of sin, and who has power in the fallen world. In the Hebrew Bible and the New Testament, Satan is primarily an accuser and adversary, a decidedly malevolent entity, also called the devil, who possesses demonic qualities.
In Theistic Satanism, Satan is considered a positive force and deity who is either worshipped or revered. In LaVeyan Satanism, Satan is regarded as holding virtuous characteristics.
Judaism.
Hebrew Bible.
The original Hebrew term "satan" is a noun from a verb meaning primarily "to obstruct, oppose", as it is found in Numbers 22:22, 1 Samuel 29:4, Psalms 109:6. "Ha-Satan" is traditionally translated as "the accuser" or "the adversary". The definite article "ha-" (English: "the") is used to show that this is a title bestowed on a being, versus the name of a being. Thus, this being would be referred to as "the satan".
Thirteen occurrences.
"Ha-Satan" with the definite article occurs 13 times in the Masoretic Text, in two books of the Hebrew Bible: Job ch.1–2 (10x) and Zechariah 3:1–2 (3x).
"Satan" without the definite article is used in 10 instances, of which two are translated "diabolos" in the Septuagint and "Satan" in the King James Version:
The other eight instances of "satan" without the definite article are traditionally translated (in Greek, Latin and English) as "an adversary", etc., and taken to be humans or obedient angels:
Book of Job.
At the beginning of the book, Job is a good person "who revered God and turned away from evil" (Job 1:1), and has therefore been rewarded by God. When the angels present themselves to God, Satan comes as well. God informs Satan about Job's blameless, morally upright character. Between Job 1:9–10 and 2:4–5, Satan points out that God has given Job everything that a man could want, so of course Job would be loyal to God; Satan suggests that Job's faith would collapse if all he has been given (even his health) were to be taken away from him. God therefore gives Satan permission to test Job. In the end, Job remains faithful and righteous, and there is the implication that Satan is shamed in his defeat.
Second Temple period.
Septuagint.
In the Septuagint, the Hebrew "ha-Satan" in Job and Zechariah is translated by the Greek word "diabolos" (slanderer), the same word in the Greek New Testament from which the English word devil is derived. Where "satan" is used of human enemies in the Hebrew Bible, such as Hadad the Edomite and Rezon the Syrian, the word is left untranslated but transliterated in the Greek as "satan", a neologism in Greek. In Zechariah 3, this changes the vision of the conflict over Joshua the High Priest in the Septuagint into a conflict between "Jesus and the devil", identical with the Greek text of Matthew.
Dead Sea scrolls and Pseudepigrapha.
In Enochic Judaism, the concept of Satan being an opponent of God and a chief evil figure in among demons seems to have taken root in Jewish pseudepigrapha during the Second Temple period, particularly in the "apocalypses". 
The Book of Enoch contains references to Satariel, thought also to be Sataniel and Satan'el (etymology dating back to Babylonian origins). The similar spellings mirror that of his angelic brethren Michael, Raphael, Uriel, and Gabriel, previous to the fall from Heaven.
The Second Book of Enoch, also called the "Slavonic Book of Enoch", contains references to a Watcher (Grigori) called Satanael. It is a pseudepigraphic text of an uncertain date and unknown authorship. The text describes Satanael as being the prince of the Grigori who was cast out of heaven and an evil spirit who knew the difference between what was "righteous" and "sinful". A similar story is found in the book of 1 Enoch; however, in that book, the leader of the Grigori is called Semjâzâ.
In the Book of Wisdom, the devil is represented as the being who brought death into the world.
In the Book of Jubilees, Mastema induces God to test Abraham through the sacrifice of Isaac. He is identical to Satan in both name and nature.
Rabbinical Judaism.
In Judaism, Satan is a term used since its earliest biblical contexts to refer to a "human opponent". Occasionally, the term has been used to suggest "evil influence" opposing human beings, as in the Jewish exegesis of the Yetzer hara ("evil inclination" Genesis 6:5). Micaiah's "lying spirit" in 1 Kings 22:22 is sometimes related. Thus, Satan is personified as a character in three different places of the Tenakh, serving as an accuser (Zechariah 3:1–2), a seducer (1 Chronicles 21:1), or as a heavenly persecutor who is "among the sons of God" (Job 2:1). In any case, Satan is always subordinate to the power of God, having a role in the divine plan. Satan is rarely mentioned in Tannaitic literature, but is found in Babylonian aggadah.
In medieval Judaism, the Rabbis rejected these Enochic literary works into the Biblical canon, making every attempt to root them out. Traditionalists and philosophers in medieval Judaism adhered to rational theology, rejecting any belief in rebel or fallen angels, and viewing evil as abstract. The Yetzer hara ("evil inclination" Genesis 6:5) is a more common motif for evil in rabbinical texts. Rabbinical scholarship on the Book of Job generally follows the Talmud and Maimonides as identifying the "Adversary" in the prologue of Job as a metaphor.
In Hasidic Judaism, the Kabbalah presents Satan as an agent of God whose function is to tempt one into sin, then turn around and accuse the sinner on high. The Chasidic Jews of the 18th century associated ha-Satan with "Baal Davar".
Dualism and Zoroastrianism.
Some scholars see contact with religious dualism in Babylon, and early Zoroastrianism in particular, as being influenced by Second Temple period Judaism, and consequently early Christianity. Subsequent development of Satan as a "deceiver" has parallels with the evil spirit in Zoroastrianism, known as the Lie, who directs forces of darkness.
Christianity.
Satan is traditionally identified as the serpent who tempted Eve to eat the forbidden fruit, as he was in Judaism. Thus Satan has often been depicted as a serpent. Christian agreement with this can be found in the works of Justin Martyr, in Chapters 45 and 79 of "Dialogue with Trypho", where Justin identifies Satan and the serpent. Other early church fathers to mention this identification include Theophilus and Tertullian.
From the fourth century, Lucifer is sometimes used in Christian theology to refer to Satan, as a result of identifying the fallen "son of the dawn" of with the "accuser" of other passages in the Old Testament.
For most Christians, Satan is believed to be an angel who rebelled against God. His goal is to lead people away from the love of God; i.e., to lead them to evil.
In the New Testament he is called "the ruler of the demons" (), "the ruler of the world", and "the god of this world" (). The Book of Revelation describes how Satan was cast out of Heaven, having "great anger" and waging war against "those who obey God's commandments". Ultimately, Satan will be thrown into the lake of fire.
The early Christian church encountered opposition from pagans such as Celsus, who claimed that "it is blasphemy...to say that the greatest God...has an adversary who constrains his capacity to do good" and said that Christians "impiously divide the kingdom of God, creating a rebellion in it, as if there were opposing factions within the divine, including one that is hostile to God".
Terminology.
In Christianity, there are many synonyms for Satan. The most common English synonym for "Satan" is "Devil", which descends from Middle English "devel," from Old English "dēofol," that in turn represents an early Germanic borrowing of Latin "diabolus" (also the source of "diabolical"). This in turn was borrowed from Greek "diabolos" "slanderer", from "diaballein" "to slander": "dia-" "across, through" + "ballein" "to hurl". In the New Testament, "Satan" occurs more than 30 times in passages alongside "Diabolos" (Greek for "the devil"), referring to the same person or thing as Satan.
Beelzebub, meaning "Lord of Flies", is the contemptuous name given in the Hebrew Bible and New Testament to a Philistine god whose original name has been reconstructed as most probably "Ba'al Zabul", meaning "Baal the Prince". This pun was later used to refer to Satan as well.
The Book of Revelation twice refers to "the dragon, that ancient serpent, who is called the devil and Satan" (12:9, 20:2). The Book of Revelation also refers to "the deceiver", from which is derived the common epithet "the great deceiver".
Islam.
"Shaitan" (شيطان) is the equivalent of Satan in Islam. While "Shaitan" (شيطان, from the root šṭn شط⁬ن) is an adjective (meaning "astray" or "distant", sometimes translated as "devil") that can be applied to both man ("al-ins", الإنس) and Jinn, Iblis (]) is the personal name of the Devil who is mentioned in the Qur'anic account of Genesis. According to the Qur'an, Iblis (the Arabic name used) disobeyed an order from Allah to bow to Adam, and as a result Iblis was forced out of heaven. However, he was given respite from further punishment until the day of judgment.
When Allah commanded all of the angels to bow down before Adam (the first Human), Iblis, full of hubris and jealousy, refused to obey God's command (he could do so because he had free will), seeing Adam as being inferior in creation due to his being created from clay as compared to him (created of fire).
It is We Who created you and gave you shape; then We bade the angels prostrate to Adam, and they prostrate; not so Iblis (Lucifer); He refused to be of those who prostrate.
(Allah) said: "What prevented thee from prostrating when I commanded thee?" He said: "I am better than he: Thou didst create me from fire, and him from clay."—Qur'an 7:11–12
It was after this that the title of "Shaitan" was given, which can be roughly translated as "Enemy", "Rebel", "Evil", or "Devil". Shaitan then claims that, if the punishment for his act of disobedience is to be delayed until the Day of Judgment, then he will divert many of Adam's own descendants from the straight path during his period of respite. God accepts the claims of Iblis and guarantees recompense to Iblis and his followers in the form of Hellfire. In order to test mankind and jinn alike, Allah allowed Iblis to roam the earth to attempt to convert others away from his path. He was sent to earth along with Adam and Eve, after eventually luring them into eating the fruit from the forbidden tree.
Yazidism.
An alternative name for the main deity in the tentatively Indo-European pantheon of the Yazidis, Melek Taus, is Shaitan. However, rather than being Satanic, Yazidism is better understood as a remnant of a pre-Islamic Middle Eastern Indo-European religion, and/or a ghulat Sufi movement founded by Shaykh Adi. The connection with Satan, originally made by Muslim outsiders, attracted the interest of 19th-century European travelers and esoteric writers.
Bahá'í Faith.
In the Bahá'í Faith, "Satan" is not regarded as an independent evil power as he is in some faiths, but signifies the "lower nature" of humans. `Abdu'l-Bahá explains: "This lower nature in man is symbolized as Satan — the evil ego within us, not an evil personality outside." All other evil spirits described in various faith traditions—such as fallen angels, demons, and jinns—are also metaphors for the base character traits a human being may acquire and manifest when he turns away from God.
Satanism.
Within Satanism, two major trends exists, theistic Satanism and atheistic Satanism, both having different views regarding the essence of Satan.
Theistic Satanism.
Theistic Satanism, commonly referred to as 'devil-worship', holds that Satan is an actual deity or force to revere or worship that individuals may contact and supplicate to, and represents loosely affiliated or independent groups and cabals which hold the belief that Satan is a real entity rather than an archetype.
Among non-Satanists, much modern Satanic folklore does not originate with the beliefs or practices of theistic or atheistic Satanists, but a mixture of medieval Christian folk beliefs, political or sociological conspiracy theories, and contemporary urban legends. An example is the Satanic ritual abuse scare of the 1980s—beginning with the memoir "Michelle Remembers"—which depicted Satanism as a vast conspiracy of elites with a predilection for child abuse and human sacrifice. This genre frequently describes Satan as physically incarnating in order to receive worship.
Atheistic Satanism.
Atheistic Satanism, most commonly referred to as LaVeyan Satanism, holds that Satan does not exist as a literal anthropomorphic entity, but rather a symbol of pride, carnality, liberty, enlightenment, undefiled wisdom, and of a cosmos which Satanists perceive to be permeated and motivated by a force that has been given many names by humans over the course of time. To adherents, he also serves as a conceptual framework and an external metaphorical projection of [the Satanists] highest personal potential.
In his essay, "Satanism: The Feared Religion", the current High Priest of the Church of Satan, Peter H. Gilmore, further expounds that "...Satan is a symbol of Man living as his prideful, carnal nature dictates. The reality behind Satan is simply the dark evolutionary force of entropy that permeates all of nature and provides the drive for survival and propagation inherent in all living things. Satan is not a conscious entity to be worshiped, rather a reservoir of power inside each human to be tapped at will."
References.
</dl>

</doc>
<doc id="27701" url="http://en.wikipedia.org/wiki?curid=27701" title="String (computer science)">
String (computer science)

In computer programming, a string is traditionally a sequence of characters, either as a literal constant or as some kind of variable. The latter may allow its elements to be mutated and the length changed, or it may be fixed (after creation). A string is generally understood as a data type and is often implemented as an array of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding. A string may also denote more general arrays or other sequence (or list) data types and structures.
Depending on programming language and precise data type used, a variable declared to be a string may either cause storage in memory to be statically allocated for a predetermined maximum length or employ dynamic allocation to allow it to hold variable number of elements.
When a string appears literally in source code, it is known as a string literal or an anonymous string.
In formal languages, which are used in mathematical logic and theoretical computer science, a string is a finite sequence of symbols that are chosen from a set called an alphabet.
Formal theory.
Let Σ be a non-empty finite set of symbols (alternatively called characters), called the "alphabet". No assumption is made about the nature of the symbols. A string (or word) over Σ is any finite sequence of symbols from Σ. For example, if Σ = {0, 1}, then "01011" is a string over Σ.
The "length" of a string "s" is the number of symbols in "s" (the length of the sequence) and can be any non-negative integer; it is often denoted as |"s"|. The "empty string" is the unique string over Σ of length 0, and is denoted "ε" or "λ".
The set of all strings over Σ of length "n" is denoted Σ"n". For example, if Σ = {0, 1}, then Σ2 = {00, 01, 10, 11}. Note that Σ0 = {ε} for any alphabet Σ.
The set of all strings over Σ of any length is the Kleene closure of Σ and is denoted Σ*. In terms of Σ"n",
For example, if Σ = {0, 1}, then Σ* = {ε, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, ...}. Although the set Σ* itself is countably infinite, each element of Σ* is a string of finite length.
A set of strings over Σ (i.e. any subset of Σ*) is called a "formal language" over Σ. For example, if Σ = {0, 1}, the set of strings with an even number of zeros, {ε, 1, 00, 11, 001, 010, 100, 111, 0000, 0011, 0101, 0110, 1001, 1010, 1100, 1111, ...}, is a formal language over Σ.
Concatenation and substrings.
"Concatenation" is an important binary operation on Σ*. For any two strings "s" and "t" in Σ*, their concatenation is defined as the sequence of symbols in "s" followed by the sequence of characters in "t", and is denoted "st". For example, if Σ = {a, b, ..., z}, "s" = bear, and "t" = hug, then "st" = bearhug and "ts" = hugbear.
String concatenation is an associative, but non-commutative operation. The empty string ε serves as the identity element; for any string "s", ε"s" = "s"ε = "s". Therefore, the set Σ* and the concatenation operation form a monoid, the free monoid generated by Σ. In addition, the length function defines a monoid homomorphism from Σ* to the non-negative integers (that is, a function formula_2, such that formula_3).
A string "s" is said to be a "substring" or "factor" of "t" if there exist (possibly empty) strings "u" and "v" such that "t" = "usv". The relation "is a substring of" defines a partial order on Σ*, the least element of which is the empty string.
Prefixes and suffixes.
A string "s" is said to be a prefix of "t" if there exists a string "u" such that "t" = "su". If "u" is nonempty, "s" is said to be a "proper" prefix of "t". Symmetrically, a string "s" is said to be a suffix of "t" if there exists a string "u" such that "t" = "us". If "u" is nonempty, "s" is said to be a "proper" suffix of "t". Suffixes and prefixes are substrings of "t". Both the relations "is a prefix of" and "is a suffix of" are prefix orders.
Rotations.
A string "s" = "uv" is said to be a rotation of "t" if "t" = "vu". For example, if Σ = {0, 1} the string 0011001 is a rotation of 0100110, where u = 00110 and v = 01.
Reversal.
The reverse of a string is a string with the same symbols but in reverse order. For example, if "s" = abc (where a, b, and c are symbols of the alphabet), then the reverse of "s" is cba. A string that is the reverse of itself (e.g., "s" = madam) is called a palindrome, which also includes the empty string and all strings of length 1.
Lexicographical ordering.
It is often useful to define an ordering on a set of strings. If the alphabet Σ has a total order (cf. alphabetical order) one can define a total order on Σ* called lexicographical order. For example, if Σ = {0, 1} and 0 < 1, then the lexicographical order on Σ* includes the relationships ε < 0 < 00 < 000 < ... < 0001 < 001 < 01 < 010 < 011 < 0110 < 01111 < 1 < 10 < 100 < 101 < 111 < 1111 < 11111 ... The lexicographical order is total if the alphabetical order is, but isn't well-founded for any nontrivial alphabet, even if the alphabetical order is.
See Shortlex for an alternative string ordering that preserves well-foundedness.
String operations.
A number of additional operations on strings commonly occur in the formal theory. These are given in the article on string operations.
Topology.
Strings admit the following interpretation as nodes on a graph:
The natural topology on the set of fixed-length strings or variable-length strings is the discrete topology, but the natural topology on the set of infinite strings is the limit topology, viewing the set of infinite strings as the inverse limit of the sets of finite strings. This is the construction used for the "p"-adic numbers and some constructions of the Cantor set, and yields the same topology.
Isomorphisms between string representations of topologies can be found by normalizing according to the lexicographically minimal string rotation.
String datatypes.
A string datatype is a datatype modeled on the idea of a formal string. Strings are such an important and useful datatype that they are implemented in nearly every programming language. In some languages they are available as primitive types and in others as composite types. The syntax of most high-level programming languages allows for a string, usually quoted in some way, to represent an instance of a string datatype; such a meta-string is called a "literal" or "string literal".
String length.
Although formal strings can have an arbitrary (but finite) length, the length of strings in real languages is often constrained to an artificial maximum. In general, there are two types of string datatypes: "fixed-length strings", which have a fixed maximum length to be determined at compile time and which use the same amount of memory whether this maximum is needed or not, and "variable-length strings", whose length is not arbitrarily fixed and which can use varying amounts of memory depending on the actual requirements at run time. Most strings in modern programming languages are variable-length strings. Of course, even variable-length strings are limited in length – theoretically by the number of bits available to a pointer, practically by the current size of memory. The string length can be stored as a separate integer (which may put an artificial limit on the length) or implicitly through a termination character, usually a character value with all bits zero. See also "Null-terminated" below.
Character encoding.
String datatypes have historically allocated one byte per character, and, although the exact character set varied by region, character encodings were similar enough that programmers could often get away with ignoring this, since characters a program treated specially (such as period and space and comma) were in the same place in all the encodings a program would encounter. These character sets were typically based on ASCII or EBCDIC.
Logographic languages such as Chinese, Japanese, and Korean (known collectively as CJK) need far more than 256 characters (the limit of a one 8-bit byte per-character encoding) for reasonable representation. The normal solutions involved keeping single-byte representations for ASCII and using two-byte representations for CJK ideographs. Use of these with existing code led to problems with matching and cutting of strings, the severity of which depended on how the character encoding was designed. Some encodings such as the EUC family guarantee that a byte value in the ASCII range will represent only that ASCII character, making the encoding safe for systems that use those characters as field separators. Other encodings such as ISO-2022 and Shift-JIS do not make such guarantees, making matching on byte codes unsafe. These encodings also were not "self-synchronizing", so that locating character boundaries required backing up to the start of a string, and pasting two strings together could result in corruption of the second string (these problems were much less with EUC as any ASCII character did synchronize the encoding).
Unicode has simplified the picture somewhat. Most programming languages now have a datatype for Unicode strings. Unicode's preferred byte stream format UTF-8 is designed not to have the problems described above for older multibyte encodings. UTF-8, UTF-16 and UTF-32 require the programmer to know that the fixed-size code units are different than the "characters", the main difficulty currently is incorrectly designed APIs that attempt to hide this difference (UTF-32 does make "code points" fixed-sized, but these are not "characters" due to composing codes).
Implementations.
Some languages like C++ implement strings as templates that can be used with any datatype, but this is the exception, not the rule.
Some languages, such as C++ and Ruby, normally allow the contents of a string to be changed after it has been created; these are termed "mutable" strings. In other languages, such as Java and Python, the value is fixed and a new string must be created if any alteration is to be made; these are termed "immutable" strings.
Strings are typically implemented as arrays of bytes, characters, or code units, in order to allow fast access to individual units or substrings—including characters when they have a fixed length. A few languages such as Haskell implement them as linked lists instead.
Some languages, such as Prolog and Erlang, avoid implementing a dedicated string datatype at all, instead adopting the convention of representing strings as lists of character codes.
Representations.
Representations of strings depend heavily on the choice of character repertoire and the method of character encoding. Older string implementations were designed to work with repertoire and encoding defined by ASCII, or more recent extensions like the ISO 8859 series. Modern implementations often use the extensive repertoire defined by Unicode along with a variety of complex encodings such as UTF-8 and UTF-16.
The term "byte string" usually indicates a general-purpose string of bytes, rather than strings of only (readable) characters, strings of bits, or such. Byte strings often imply that bytes can take any value and any data can be stored as-is, meaning that there should be no value interpreted as a termination value.
Most string implementations are very similar to variable-length arrays with the entries storing the character codes of corresponding characters. The principal difference is that, with certain encodings, a single logical character may take up more than one entry in the array. This happens for example with UTF-8, where single codes (UCS code points) can take anywhere from one to four bytes, and single characters can take an arbitrary number of codes. In these cases, the logical length of the string (number of characters) differs from the logical length of the array (number of bytes in use). UTF-32 avoids the first part of the problem.
Null-terminated.
The length of a string can be stored implicitly by using a special terminating character; often this is the null character (NUL), which has all bits zero, a convention used and perpetuated by the popular C programming language. Hence, this representation is commonly referred to as a C string. This representation of an "n"-character string takes "n" + 1 space (1 for the terminator), and is thus an implicit data structure.
In terminated strings, the terminating code is not an allowable character in any string. Strings with "length" field do not have this limitation and can also store arbitrary binary data. In C two things are needed to handle binary data: a character pointer and the length of the data.
An example of a "null-terminated string" stored in a 10-byte buffer, along with its ASCII (or more modern UTF-8) representation as 8-bit hexadecimal numbers is:
The length of the string in the above example, "codice_1", is 5 characters, but it occupies 6 bytes. Characters after the terminator do not form part of the representation; they may be either part of another string or just garbage. (Strings of this form are sometimes called "ASCIZ strings", after the original assembly language directive used to declare them.)
Length-prefixed.
The length of a string can also be stored explicitly, for example by prefixing the string with the length as a byte value (a convention used in many Pascal dialects): as a consequence, some people call it a Pascal string or P-string. Storing the string length as byte limits the maximum string length to 255. To avoid such limitations, improved implementations of P-strings use 16-, 32-, or 64-bit words to store the string length. When the "length" field covers the address space, strings are limited only by the available memory. Encoding the length "n" takes log("n") space (see fixed-length code), so length-prefixed strings are a succinct data structure, encoding a string of length "n" in log("n") + "n" space. However, if the length is bounded, then the length can be encoded in constant space, typically a machine word, and thus is an implicit data structure, taking "n" + "k" space, where "k" is the number of characters in a word (8 for 8-bit ASCII on a 64-bit machine, 1 for 32-bit UTF-32/UCS-4 on a 32-bit machine, etc.).
Here is the equivalent Pascal string stored in a 10-byte buffer, along with its ASCII / UTF-8 representation:
Strings as records.
Many languages, including object-oriented ones, implement strings as records in a structure like:
Although this implementation is hidden, and accessed through member functions. The "text" will be a dynamically allocated memory area, that might be expanded if needed. See also string (C++).
Linked-list.
Both character termination and length codes limit strings: For example, C character arrays that contain null (NUL) characters cannot be handled directly by C string library functions: Strings using a length code are limited to the maximum value of the length code.
Both of these limitations can be overcome by clever programming, but such workarounds are by definition not standard.
Rough equivalents of the C termination method have historically appeared in both hardware and software. For example, "data processing" machines like the IBM 1401 used a special word mark bit to delimit strings at the left, where the operation would start at the right. This meant that, while the IBM 1401 had a seven-bit word in "reality", almost no-one ever thought to use this as a feature, and override the assignment of the seventh bit to (for example) handle ASCII codes.
It is possible to create data structures and functions that manipulate them that do not have the problems associated with character termination and can in principle overcome length code bounds. It is also possible to optimize the string represented using techniques from run length encoding (replacing repeated characters by the character value and a length) and Hamming encoding.
While these representations are common, others are possible. Using ropes makes certain string operations, such as insertions, deletions, and concatenations more efficient.
Security concerns.
The differing memory layout and storage requirements of strings can affect the security of the program accessing the string data. String representations requiring a terminating character are commonly susceptible to buffer overflow problems if the terminating character is not present, caused by a coding error or an attacker deliberately altering the data. String representations adopting a separate length field are also susceptible if the length can be manipulated. In such cases, program code accessing the string data requires bounds checking to ensure that it does not inadvertently access or change data outside of the string memory limits.
String data is frequently obtained from user-input to a program. As such, it is the responsibility of the program to validate the string to ensure that it represents the expected format. Performing limited or no validation of user-input can cause a program to be vulnerable to code injection attacks. 
Text file strings.
In computer readable text files, for example programming language source files or configuration files, strings can be represented. The NUL byte is normally not used as terminator since that does not correspond to the ASCII text standard, and the length is usually not stored, since the file should be human editable without bugs.
Two common representations are:
Non-text strings.
While character strings are very common uses of strings, a string in computer science may refer generically to any sequence of homogeneously typed data. A string of bits or bytes, for example, may be used to represent non-textual binary data retrieved from a communications medium. This data may or may not be represented by a string-specific datatype, depending on the needs of the application, the desire of the programmer, and the capabilities of the programming language being used. If the programming language's string implementation is not 8-bit clean, data corruption may ensue.
String processing algorithms.
There are many algorithms for processing strings, each with various trade-offs. Some categories of algorithms include:
Advanced string algorithms often employ complex mechanisms and data structures, among them suffix trees and finite state machines.
The name "stringology" was coined in 1984 by computer scientist Zvi Galil for the issue of algorithms and data structures used for string processing.
Character string-oriented languages and utilities.
Character strings are such a useful datatype that several languages have been designed in order to make string processing applications easy to write. Examples include the following languages:
Many Unix utilities perform simple string manipulations and can be used to easily program some powerful string processing algorithms. Files and finite streams may be viewed as strings.
Some APIs like Multimedia Control Interface, embedded SQL or printf use strings to hold commands that will be interpreted.
Recent scripting programming languages, including Perl, Python, Ruby, and Tcl employ regular expressions to facilitate text operations. Perl is particularly noted for its regular expression use, and many other languages and applications implement Perl compatible regular expressions.
Some languages such as Perl and Ruby support string interpolation, which permits arbitrary expressions to be evaluated and included in string literals.
Character string functions.
String functions are used to manipulate a string or change or edit the contents of a string. They also are used to query information about a string. They are usually used within the context of a computer programming language.
The most basic example of a string function is the string length function – the function that returns the length of a string (not counting any terminator characters or any of the string's internal structural information) and does not modify the string. This function is often named codice_2 or codice_3. For example, codice_4 would return 11.
String buffers.
In some programming languages, a string buffer is an alternative to a string. It has the ability to be altered through adding or appending, whereas a String is normally fixed or immutable.
In Java.
Theory.
Java's standard way to handle text is to use its codice_5 class. Any given codice_6 in Java is an immutable object, which means its state cannot be changed. A codice_6 has an array of characters. Whenever a codice_6 must be manipulated, any changes require the creation of a new codice_6 (which, in turn, involves the creation of a new array of characters, and copying of the original array). This happens even if the original codice_6's value or intermediate codice_6s used for the manipulation are not kept. 
Java provides an alternate class for string manipulation, called a codice_12. A codice_12, like a codice_6, has an array to hold characters. It, however, is mutable (its state can be altered). Its array of characters is not necessarily completely filled (as opposed to a String, whose array is always the exact required length for its contents). Thus, it has the capability to add, remove, or change its state without creating a new object (and without the creation of a new array, and array copying). The exception to this is when its array is no longer of suitable length to hold its content. In this case, it is required to create a new array, and copy the contents.
For these reasons, Java would handle an expression like
like this:
Implications.
Generally, a codice_12 is more efficient than a String in string handling. However, this is not necessarily the case, since a StringBuffer will be required to recreate its character array when it runs out of space. Theoretically, this is possible to happen the same number of times as a new String would be required, although this is unlikely (and the programmer can provide length hints to prevent this). Either way, the effect is not noticeable in modern desktop computers.
As well, the shortcomings of arrays are inherent in a codice_12. In order to insert or remove characters at arbitrary positions, whole sections of arrays must be moved. 
The method by which a codice_12 is attractive in an environment with low processing power takes this ability by using too much memory, which is likely also at a premium in this environment. This point, however, is trivial, considering the space required for creating many instances of Strings in order to process them. As well, the StringBuffer can be optimized to "waste" as little memory as possible.
The class, introduced in J2SE 5.0, differs from codice_12 in that it is unsynchronized. When only a single thread at a time will access the object, using a codice_19 processes more efficiently than using a codice_12.
codice_12 and codice_19 are included in the package.
In .NET.
Microsoft's .NET Framework has a codice_19 class in its Base Class Library.

</doc>
<doc id="27707" url="http://en.wikipedia.org/wiki?curid=27707" title="Socialist law">
Socialist law

Socialist law or Soviet law denotes a general type of legal system which has been used in communist and formerly communist states. It is based on the civil law system, with major modifications and additions from Marxist-Leninist ideology. There is controversy as to whether socialist law ever constituted a separate legal system or not. If so, prior to the end of the Cold War, "socialist law" would be ranked among the major legal systems of the world.
While civil law systems have traditionally put great pains in defining the notion of private property, how it may be acquired, transferred, or lost, socialist law systems provide for most property to be owned by the state or by agricultural co-operatives, and having special courts and laws for state enterprises.
Many scholars argue that socialist law was not a separate legal classification. Although the command economy approach of the communist states meant that most types of property could not be owned, the Soviet Union always had a civil code, courts that interpreted this civil code, and a civil law approach to legal reasoning (thus, both legal process and legal reasoning were largely analogous to the French or German civil code system). Legal systems in all socialist states preserved formal criteria of the Romano-Germanic civil law; for this reason, law theorists in post-socialist states usually consider the Socialist law as a particular case of the Romano-Germanic civil law. Cases of development of common law into Socialist law are unknown because of incompatibility of basic principles of these two systems (common law presumes influential rule-making role of courts while courts in socialist states play a dependent role).
Soviet legal theory.
Soviet law displayed many special characteristics that derived from the socialist nature of the Soviet state and reflected Marxist-Leninist ideology. Vladimir Lenin accepted the Marxist conception of the law and the state as instruments of coercion in the hands of the bourgeoisie and postulated the creation of popular, informal tribunals to administer revolutionary justice. One of the main theoreticians of Soviet socialist legality in this early phase was Pēteris Stučka.
Alongside this utopian trend was one more critical of the concept of "proletarian justice", represented by Evgeny Pashukanis. A dictatorial trend developed that advocated the use of law and legal institutions to suppress all opposition to the regime. This trend reached its zenith under Joseph Stalin with the ascendancy of Andrey Vyshinsky, when the administration of justice was carried out mainly by the security police in special tribunals.
During the de-Stalinization of the Nikita Khrushchev era, a new trend developed, based on socialist legality, that stressed the need to protect the procedural and statutory rights of citizens, while still calling for obedience to the state. New legal codes, introduced in 1960, were part of the effort to establish legal norms in administering laws. Although socialist legality remained in force after 1960, the dictatorial and utopian trends continued to influence the legal process. Persecution of political and religious dissenters continued, but at the same time there was a tendency to decriminalize lesser offenses by handing them over to people's courts and administrative agencies and dealing with them by education rather than by incarceration.
By late 1986, the Mikhail Gorbachev era was stressing anew the importance of individual rights in relation to the state and criticizing those who violated procedural law in implementing Soviet justice. This signaled a resurgence of socialist legality as the dominant trend. It should be noted, however, that socialist legality itself still lacked features associated with Western jurisprudence.
Characteristic traits.
Socialist law is similar to common law or civil law but with a greatly increased public law sector and decreased private law sector.
A specific institution characteristic to Socialist law was the so-called burlaw court (or, verbally, "court of comrades", Russian товарищеский суд) which decided on minor offences.
Chinese Socialist law.
Among the remaining communist governments, some (most notably the People's Republic of China) have added extensive modifications to their legal systems. In general, this is a result of their market-oriented economic changes. However, some communist influence can still be seen. For example, in Chinese real estate law there is no unified concept of real property; the state owns all land but often not the structures that sit on that land. A rather complex "ad hoc" system of use rights to land property has developed, and these use rights are the things being officially traded (rather than the property itself). In some cases (for example in the case of urban residential property), the system results in something that resembles real property transactions in other legal systems.
In other cases, the Chinese system results in something quite different. For example, it is a common misconception that reforms under Deng Xiaoping resulted in the privatization of agricultural land and a creation of a land tenure system similar to those found in Western countries. In actuality, the village committee owns the land and contracts the right to use this land to individual farmers who may use the land to make money from agriculture. Hence the rights that are normally unified in Western economies are split up between the individual farmer and the village committee.
This has a number of consequences. One of them is that, because the farmer does not have an absolute right to transfer the land, he cannot borrow against his use rights. On the other hand, there is some insurance against risk in the system, in that the farmer can return his land to the village committee if he wants to stop farming and start some other sort of business. Then, if this business does not work, he can get a new contract with the village committee and return to farming. The fact that the land is redistributable by the village committee also ensures that no one is left landless; this creates a form of social welfare.
There have been a number of proposals to reform this system and they have tended to be in the direction of fully privatizing rural land for the alleged purpose of increasing efficiency. These proposals have usually not received any significant support, largely because of the popularity of the current system among the farmers themselves. There is little risk that the village committee will attempt to impose a bad contract on the farmers, since this would reduce the amount of money the village committee receives. At the same time, the farmer has some flexibility to decide to leave farming for other ventures and to return at a later time.

</doc>
<doc id="27715" url="http://en.wikipedia.org/wiki?curid=27715" title="Saint Louis">
Saint Louis

Saint Louis, Saint-Louis or St. Louis may refer to:

</doc>
<doc id="27718" url="http://en.wikipedia.org/wiki?curid=27718" title="Super Bowl">
Super Bowl

The Super Bowl is the annual championship game of the National Football League (NFL), the highest level of professional football in the United States, culminating a season that begins in the late summer of the previous calendar year. The Super Bowl normally uses Roman numerals to identify each game, rather than the year in which it is held. For example, Super Bowl I was played on January 15, 1967, following the 1966 regular season. The lone exception will be the next game, Super Bowl 50, which will be played on February 7, 2016, following the 2015 season.
The game was created as part of a merger agreement between the NFL and its then-rival league, the American Football League (AFL). It was agreed that the two leagues' champion teams would play in the AFL–NFL World Championship Game until the merger was to officially begin in 1970. After the merger, each league was redesignated as a "conference", and the game has since been played between the conference champions to determine the NFL's league champion. Currently, the National Football Conference (NFC) leads the league with 26 wins to 23 wins for the American Football Conference (AFC). The Pittsburgh Steelers hold the record for Super Bowl victories with six.
The day on which the Super Bowl is played, now considered by some an unofficial American national holiday, is called "Super Bowl Sunday". It is the second-largest day for U.S. food consumption, after Thanksgiving Day. In addition, the Super Bowl has frequently been the most-watched American television broadcast of the year; the four most-watched broadcasts in U.S. television history are Super Bowls. In 2015, Super Bowl XLIX became the most-watched American television program in history with an average audience of 114.4 million viewers, the fifth time in six years the game had set a record, starting with the 2010 Super Bowl, which itself had taken over the number-one spot held for 27 years by the final episode of "M*A*S*H". The Super Bowl is also among the most-watched sporting events in the world, almost all audiences being North American, and is second to soccer's UEFA Champions League final as the most watched "annual" sporting event worldwide.
The NFL restricts the use of its "Super Bowl" trademark; it is frequently called the Big Game or other generic terms by non-sponsoring corporations. Because of the high viewership, commercial airtime during the Super Bowl broadcast is the most expensive of the year, leading to companies regularly developing their most expensive advertisements for this broadcast. As a result, watching and discussing the broadcast's commercials has become a significant aspect of the event. In addition, popular singers and musicians including Michael Jackson, Madonna, Prince, The Rolling Stones, The Who, and Whitney Houston have performed during the event's pre-game and halftime ceremonies.
Origin.
For four decades after its 1920 inception, the NFL successfully fended off several rival leagues. However, in 1960, it encountered its most serious competitor when the American Football League (AFL) was formed. The AFL vied heavily with the NFL for both players and fans, but by the middle of the decade the strain of competition led to serious merger talks between the two leagues. Prior to the 1966 season, the NFL and AFL reached a merger agreement that was to take effect for the 1970 season. As part of the merger, the champions of the two leagues agreed to meet in a world championship game for professional American football until the merger was effected.
Lamar Hunt, owner of the AFL's Kansas City Chiefs, first used the term "Super Bowl" to refer to this game in the merger meetings. Hunt would later say the name was likely in his head because his children had been playing with a Super Ball toy (a vintage example of the ball is on display at the Pro Football Hall of Fame in Canton, Ohio). In a July 25, 1966, letter to NFL commissioner Pete Rozelle, Hunt wrote, "I have kiddingly called it the 'Super Bowl,' which obviously can be improved upon." Although the leagues' owners decided on the name "AFL-NFL Championship Game," the media immediately picked up on Hunt's "Super Bowl" name, which would become official beginning with the third annual game.
The "Super Bowl" name was derived from the bowl game, a post-season college football game. The original "bowl game" was the Rose Bowl Game in Pasadena, California, which was first played in 1902 as the "Tournament East-West football game" as part of the Pasadena Tournament of Roses and moved to the new Rose Bowl Stadium in 1923. The stadium got its name from the fact that the game played there was part of the Tournament of Roses and that it was shaped like a bowl, much like the Yale Bowl in New Haven, Connecticut; the Tournament of Roses football game itself eventually came to be known as the Rose Bowl Game. Exploiting the Rose Bowl Game's popularity, post-season college football contests were created for Miami (the Orange Bowl), New Orleans (the Sugar Bowl), and El Paso, TX (the Sun Bowl) in 1935, and for Dallas (the Cotton Bowl) in 1937. Thus, by the time the first Super Bowl was played, the term "bowl" for any big-time American football game was well established.
After the NFL's Green Bay Packers won the first two Super Bowls, some team owners feared for the future of the merger. At the time, many doubted the competitiveness of AFL teams compared with their NFL counterparts, though that perception changed when the AFL's New York Jets defeated the NFL's Baltimore Colts in Super Bowl III in Miami. One year later, the AFL's Kansas City Chiefs defeated the NFL's Minnesota Vikings 23–7 in Super Bowl IV in New Orleans, which was the final AFL-NFL World Championship Game played before the merger. Beginning with the 1970 season, the NFL realigned into two conferences; the former AFL teams plus three NFL teams (the Colts, Pittsburgh Steelers, and Cleveland Browns) would constitute the American Football Conference (AFC), while the remaining NFL clubs would form the National Football Conference (NFC). The champions of the two conferences would play each other in the Super Bowl.
The winning team receives the Vince Lombardi Trophy, named after the coach of the Green Bay Packers, who won the first two Super Bowl games and three of the five preceding NFL championships in 1961, 1962, and 1965. Following Lombardi's death in September 1970, the trophy was named the Vince Lombardi Trophy, and was the first awarded as such to the Baltimore Colts following their win in Super Bowl V in Miami.
Date.
The game is played annually on a Sunday as the final game of the NFL Playoffs. Originally, the game took place in early to mid-January, following a fourteen-game regular season and two rounds of playoffs. Over the years, the date of the Super Bowl has progressed from the second Sunday in January, to the third, and then the fourth Sunday in January; the game is currently played on the first Sunday in February, given the current seventeen-week (sixteen games and one bye week) regular season and three rounds of playoffs. Also, February is television's "sweeps" month, thus affording the television network carrying the game an immense opportunity to pad its viewership when negotiating for advertising revenue. The progression of the dates of the Super Bowl was caused by several factors: the expansion of the NFL's regular season in 1978 from fourteen games to sixteen; the expansion of the pre-Super Bowl playoff field from four teams (two AFL and two NFL) in the 1966–67 season, to six in 1967–68 (two AFL and four NFL), to eight (four AFL and four NFL) in the 1969–70 season which was continued after the merger, then to ten in 1978–79, and finally twelve in 1990–91, necessitating additional rounds of playoffs; the addition of the regular season bye-week in the 1990s; and the decision to start the regular season the week following Labor Day.
To date, 36 games have been played in January, and 13 in February. The earliest game played was Super Bowl XI on January 9, 1977. The latest played was Super Bowl XLIV on February 7, 2010. The most frequent date for the game has been January 26, with four games played. Between January 9 and February 7, the only dates not to feature the game have been January 10, 19 and 23. Super Bowl XLVIII was the first Super Bowl played on February 2, a date commonly celebrated as Groundhog Day.
Game history.
The Pittsburgh Steelers have won six Super Bowls, the most of any team; the Dallas Cowboys and San Francisco 49ers have five victories each, while the Green Bay Packers, New York Giants and New England Patriots have four Super Bowl championships. Thirteen other NFL franchises have won at least one Super Bowl. Nine teams have appeared in Super Bowl games without a win. The Minnesota Vikings were the first team to have appeared a record four times without a win. The Buffalo Bills played in a record four Super Bowls in a row, and lost every one. Four teams (the Cleveland Browns, Detroit Lions, Jacksonville Jaguars, and Houston Texans) have never appeared in a Super Bowl. The Browns and Lions both won NFL Championships prior to the Super Bowl's creation, while the Jaguars (1995) and Texans (2002) are both recent NFL expansion teams. The Minnesota Vikings won the last NFL Championship before the merger, but lost to the AFL champion Kansas City Chiefs in Super Bowl IV.
1960s: Early history.
The Green Bay Packers won the first two Super Bowls, defeating the Kansas City Chiefs and Oakland Raiders following the 1966 and 1967 seasons, respectively. The Packers were led by quarterback Bart Starr, who was named the Most Valuable Player (MVP) for both games. These two championships, coupled with the Packers' NFL championships in 1961, 1962, and 1965, amount to the most successful stretch in NFL History; five championships in seven years.
In Super Bowl III, the AFL's New York Jets defeated the eighteen-point favorite Baltimore Colts of the NFL, 16–7. The Jets were led by quarterback Joe Namath (who had famously guaranteed a Jets win prior to the game) and former Colts head coach Weeb Ewbank, and their victory proved that the AFL was the NFL's competitive equal. This was reinforced the following year, when the AFL's Kansas City Chiefs defeated the NFL's Minnesota Vikings 23–7 in Super Bowl IV.
1970s: Dominant franchises.
After the AFL–NFL merger was completed in 1970, three franchises – the Dallas Cowboys, Miami Dolphins, and Pittsburgh Steelers – would go on to dominate the 1970s, winning a combined eight Super Bowls in the decade.
The Baltimore Colts, now a member of the AFC, would start the decade by defeating the Cowboys in Super Bowl V, a game which is notable as being the only Super Bowl to date in which a player from the losing team won the Super Bowl MVP (Cowboys' linebacker Chuck Howley). This Super Bowl, as well as all Super Bowls since, have served as the NFL's league championship game.
The Cowboys, coming back from a loss the previous season, won Super Bowl VI over the Dolphins. However, this would be the Dolphins' final loss in over a year, as the next year, the Dolphins would go 14–0 in the regular season and eventually win all of their playoff games, capped off with a 14-7 victory in Super Bowl VII, becoming the first and only team to finish an entire perfect regular and post season. The Dolphins would repeat as league champions by winning Super Bowl VIII a year later.
In the late 1970s, the Steelers became the first NFL dynasty of the post-merger era by winning four Super Bowls (IX, X, XIII, and XIV) in six years. They were led by head coach Chuck Noll, the play of offensive stars Terry Bradshaw, Franco Harris, Lynn Swann, John Stallworth, and Mike Webster, and their dominant "Steel Curtain" defense, led by "Mean" Joe Greene, L.C. Greenwood, Ernie Holmes, Mel Blount, Jack Ham, and Jack Lambert. The coaches and administrators also were part of the dynasty's greatness as evidenced by the team's "final pieces" being part of the famous 1974 draft. The selections in that class have been considered the best by any pro franchise ever, as Pittsburgh selected four future Hall of Famers, the most for any team in any sport in a single draft. The Steelers were the first team to win three and then four Super Bowls and appeared in six AFC Championship Games during the decade, making the playoffs in eight straight seasons. Nine players and three coaches and administrators on the team have been inducted into the Pro Football Hall of Fame. Pittsburgh still remains the only team to win back-to-back Super Bowls twice and four Super Bowls in a six-year period.
The Steelers' dynasty was interrupted only by the Cowboys winning their second Super Bowl of the decade and the Oakland Raiders' Super Bowl XI win.
1980s and 1990s: The NFC's winning streak.
In the 1980s and 1990s, the tables turned for the AFC, as the NFC dominated the Super Bowls of the new decade and most of those of the 1990s. The NFC won 16 of the 20 Super Bowls during these two decades, including 13 straight from Super Bowl XIX to Super Bowl XXXI.
The most successful team of the 1980s was the San Francisco 49ers, which featured the West Coast offense of Hall of Fame head coach Bill Walsh. This offense was led by three-time Super Bowl MVP and Hall of Fame quarterback Joe Montana, Super Bowl MVP and Hall of Fame wide receiver Jerry Rice, and tight end Brent Jones. Under their leadership, the 49ers won four Super Bowls in the decade (XVI, XIX, XXIII, and XXIV) and made nine playoff appearances between 1981 and 1990, including eight division championships, becoming the second dynasty of the post-merger NFL.
The 1980s also produced the 1985 Chicago Bears, who posted an 18–1 record under head coach Mike Ditka; colorful quarterback Jim McMahon; and Hall of Fame running back Walter Payton. Their team won Super Bowl XX in dominating fashion. The Washington Redskins and New York Giants were also top teams of this period; the Redskins won Super Bowls XVII, XXII, and XXVI. The Giants claimed Super Bowls XXI and XXV. As in the 1970s, the Oakland Raiders were the only team to interrupt the Super Bowl dominance of other teams; they won Super Bowls XV and XVIII (the latter as the Los Angeles Raiders).
Following several seasons with poor records in the 1980s, the Dallas Cowboys rose back to prominence in the 1990s. During this decade, the Cowboys made post-season appearances every year except for the seasons of 1990 and 1997. From 1992 to 1996, the Cowboys won their division championship each year. In this same period, the Buffalo Bills had made their mark reaching the Super Bowl for 4 consecutive years, only to lose in all of them. After Super Bowl championships by division rivals New York (1990) and Washington (1991), the Cowboys won three of the next four Super Bowls (XXVII, XXVIII, and XXX) led by quarterback Troy Aikman, running back Emmitt Smith, and wide receiver Michael Irvin. All three of these players went to the Hall of Fame. The Cowboys' streak was interrupted by the 49ers, who won their league-leading fifth title overall with Super Bowl XXIX in dominating fashion under Super Bowl MVP and Hall of Fame quarterback Steve Young, Hall of Fame wide receiver Jerry Rice, and Hall of Fame cornerback Deion Sanders; however, the Cowboys' victory in Super Bowl XXX the next year also gave them five titles overall and they did so with Deion Sanders after he won the Super Bowl the previous year with the San Francisco 49ers. The NFC's winning streak was continued by the Green Bay Packers who, under quarterback Brett Favre, won Super Bowl XXXI, their first championship since Super Bowl II in the late 1960s.
1997–2009: AFC resurgence.
Super Bowl XXXII saw quarterback John Elway and running back Terrell Davis lead the Denver Broncos to an upset victory over the defending champion Packers, snapping the NFC's 13 year winning streak. The following year, the Broncos defeated the Atlanta Falcons in Super Bowl XXXIII, Elway's fifth Super Bowl appearance, his second NFL championship, and his final NFL game. The back-to-back victories heralded a change in momentum in which AFC teams would win 10 out of 13 Super Bowls. In the years between 2001 and 2011, three teams – the Patriots, Steelers, and Colts – accounted for ten of the AFC Super Bowl appearances, with those same teams often meeting each other earlier in the playoffs. In contrast, the NFC saw a different representative in the Super Bowl every season from 2001 through 2010.
The year following the Denver Broncos' second victory, however, a surprising St. Louis Rams led by undrafted quarterback Kurt Warner would close out the 1990s in a wild battle against the Tennessee Titans in Super Bowl XXXIV. The tense game came down to the final play in which Tennessee had the opportunity to tie the game and send it to overtime. The Titans nearly pulled it off, but the tackle of receiver Kevin Dyson by linebacker Mike Jones kept the ball out of the end zone by a matter of inches. In 2007, ESPN would rank "The Tackle" as the 2nd greatest moment in Super Bowl history.
Super Bowl XXXV was played by the AFC's Baltimore Ravens and the NFC's New York Giants. The Ravens defeated the Giants by the score of 34–7. The game was played on January 28, 2001, at Raymond James Stadium in Tampa, Florida.
The New England Patriots became the dominant team throughout the early 2000s, winning the championship three out of four years early in the decade. They would become only the second team in the history of the NFL to do so (after the 1990s Dallas Cowboys). In Super Bowl XXXVI, first-year starting quarterback Tom Brady led his team to a 20–17 upset victory over the St. Louis Rams. Brady would go on to win the MVP award for this game. The Patriots also won Super Bowls XXXVIII and XXXIX defeating the Carolina Panthers and the Philadelphia Eagles respectively. This four-year stretch of Patriot dominance was only interrupted by the Tampa Bay Buccaneers' 48-21 Super Bowl XXXVII victory over the Oakland Raiders.
The Pittsburgh Steelers and Indianapolis Colts continued the era of AFC dominance by winning Super Bowls XL and XLI in 2005-06 and 2006–07, respectively defeating the Seattle Seahawks and Chicago Bears.
In the 2007 season, the Patriots became the second team in NFL history to have a perfect regular season record, after the 1972 Miami Dolphins, and the first to finish 16–0. They easily marched through the AFC playoffs and were heavy favorites in Super Bowl XLII. However, they lost that game to Eli Manning and the New York Giants 17–14, leaving the Patriots' 2007 record at 18-1.
The following season, the Steelers logged their record sixth Super Bowl title (XLIII) in a 27-23, final-minute victory against the Arizona Cardinals.
2010–present: The NFC re-emerges.
The 2010s have seen a return to dominance by NFC teams. Between 2010 and 2015, four of the six Super Bowl winners hailed from the NFC.
The Giants won another title after the 2011 season, again defeating the Patriots in Super Bowl XLVI. Prior to that Super Bowl victory, the New Orleans Saints won their first (XLIV) by defeating the Indianapolis Colts in February 2010, and the Green Bay Packers won their fourth Super Bowl (XLV) and record thirteenth NFL championship overall by defeating the Pittsburgh Steelers in February 2011.
The Baltimore Ravens interrupted the NFC's streak by winning Super Bowl XLVII in a 34-31 nail-biter over the San Francisco 49ers.
Super Bowl XLVIII, played at New Jersey's MetLife Stadium in February 2014, was the first Super Bowl held outdoors in a cold weather environment. The Seattle Seahawks won their first NFL title with a 43-8 defeat of the Denver Broncos, in a highly touted matchup that pitted Seattle's top-ranked defense against a Peyton-Manning-led Denver offense that had broken the NFL's single-season scoring record.
In Super Bowl XLIX, the New England Patriots, the AFC champions, beat the NFC and defending Super Bowl champions, the Seattle Seahawks.
The Super Bowls of the 2000s and early 2010s are notable for the performances (and the pedigrees) of several of the participating quarterbacks. During that era, Tom Brady (six Super Bowl appearances, four wins), Ben Roethlisberger (three appearances, two wins), Peyton Manning (three appearances, one win), Eli Manning (two appearances, two wins), Kurt Warner (three appearances, one win), Drew Brees (one appearance, one win), Aaron Rodgers (one appearance, one win), Joe Flacco (one appearance, one win), and Russell Wilson (two appearances, one win) have all added Super Bowl championships to their lists of individual accomplishments.
Television coverage and ratings.
The Super Bowl is one of the most watched annual sporting events in the world. The only other annual events that gather more viewers are the UEFA Champions League final, and "El Clásico" in Spain. For many years, the Super Bowl has possessed a large US and global television viewership, and it is often the most watched United States originating television program of the year. The game tends to have high Nielsen television ratings, which is usually around a 40 rating and 60 share. This means that on average, more than 100 million people from the United States alone are tuned into the Super Bowl at any given moment.
In press releases preceding each year's event, the NFL typically claims that that year's Super Bowl will have a potential worldwide audience of around one billion people in over 200 countries. This figure refers to the number of people "able" to watch the game, not the number of people "actually" watching. However the statements have been frequently misinterpreted in various media as referring to the latter figure, leading to a common misperception about the game's actual global audience. The New York-based media research firm Initiative measured the global audience for the 2005 Super Bowl at 93 million people, with 98 percent of that figure being viewers in North America, which meant roughly 2 million people outside North America watched the Super Bowl that year.
2015's Super Bowl XLIX holds the record for total number of U.S. viewers, with a final number of 114.4 million, making the game the most-viewed television broadcast of any kind in American history. The halftime show was the most watched ever with 118.5 million viewers tuning in, and an all-time high of 168 million viewers in the United States had watched several portions of the Super Bowl 2015 broadcast. The game set a record for total viewers for the fifth time in six years.
The highest-rated game according to Nielsen was Super Bowl XVI in 1982, which was watched in 49.1 percent of households (73 share), or 40,020,000 households at the time. Ratings for that game, a San Francisco victory over Cincinnati, may have been aided by a large blizzard that had affected much of the northeastern United States on game day, leaving residents to stay at home more than usual. Super Bowl XVI still ranks fourth on Nielsen's list of top-rated programs of all time, and three other Super Bowls, XII, XVII, and XX, made the top ten.
Famous commercial campaigns include the Budweiser "Bud Bowl" campaign and the 1999 and 2000 dot-com ads. Prices have increased every year, with advertisers paying as much as $3.5 million for a thirty-second spot during Super Bowl XLVI in 2012. A segment of the audience tunes into the Super Bowl solely to view commercials. In 2010, Nielsen reported that 51 percent of Super Bowl viewers tune in for the commercials. The Super Bowl halftime show has spawned another set of alternative entertainment such as the Lingerie Bowl, the Beer Bottle Bowl, and others.
Since 1991, the Super Bowl has begun between 6:19 and 6:40 PM EST so that most of the game is played during the primetime hours on the East Coast.
Super Bowl on TV.
<br>
^ *: Not currently broadcasting NFL games.
^ **: The extended current TV contracts with the networks expire after the 2022 season (or Super Bowl LVII in early 2023) and the Super Bowl is rotated annually between CBS, Fox and NBC in that order.
^ ***: The first Super Bowl was simultaneously broadcast by CBS and NBC, with each network using the same video feed, but providing its own commentary.
Super Bowls I–VI were blacked out in the television markets of the host cities, due to league restrictions then in place.
Lead-out programming.
The Super Bowl provides an extremely strong lead-in to programming following it on the same channel, the effects of which can last for several hours. For instance, in discussing the ratings of a local TV station, Buffalo television critic Alan Pergament noted on the coattails from Super Bowl XLVII, which aired on CBS: "A paid program that ran on Channel 4 (WIVB-TV) at 2:30 in the morning had a 1.3 rating. That’s higher than some CW prime time shows get on WNLO-TV, Channel 4’s sister station."
Because of this strong coattail effect, the network that airs the Super Bowl typically takes advantage of the large audience to air an episode of a hit series, or to premiere the pilot of a promising new one in the lead-out slot, which immediately follows the Super Bowl and post-game coverage.
Entertainment.
 Initially, it was sort of a novelty and so it didn't quite feel right. But it was just like, this is the year. ... Bands of our generation, you can sort of be seen on a stage like this or, like, not seen. There's not a lot of middle places. It is a tremendous venue.
 — Bruce Springsteen on why he turned down several invitations to perform at the Super Bowl before finally agreeing to appear in Super Bowl XLIII.
Early Super Bowls featured a halftime show consisting of marching bands from local colleges or high schools; but as the popularity of the game increased, a trend where popular singers and musicians performed during its pre-game ceremonies and the halftime show, or simply sang the national anthem of the United States, emerged. Unlike regular season or playoff games, thirty minutes are allocated for the Super Bowl halftime. The first halftime show to have featured only one star performer was Super Bowl XXVII in 1993, at which Michael Jackson performed. The NFL specifically went after him to increase viewership and to continue expanding the Super Bowl's realm. Sports bloggers have ranked Jackson's appearance as the No. 1 Super Bowl halftime show since its inception. Another notable performance came during Super Bowl XXXVI in 2002, when U2 performed; during their third song, "Where the Streets Have No Name", the band played under a large projection screen which scrolled through names of the victims of the September 11 attacks.
The halftime show of Super Bowl XXXVIII in 2004 generated controversy when Justin Timberlake removed a piece of Janet Jackson's top, exposing her right breast with a star-shaped pastie around the nipple. Timberlake and Jackson have maintained that the incident was accidental, calling it a "wardrobe malfunction". The game was airing live on CBS, and MTV had produced the halftime show. Immediately after the moment, the footage jump-cut to a wide-angle shot and went to a commercial break; however, video captures of the moment in detail circulated quickly on the internet. The NFL, embarrassed by the incident, permanently banned MTV from conducting future halftime shows. This also led to the FCC tightening controls on indecency and fining CBS and CBS-owned stations a total of $550,000 for the incident. The fine was later reversed in July 2008. CBS and MTV eventually split into two separate companies in part because of the fiasco, with CBS going under the control of CBS Corporation and MTV falling under the banner of Viacom (although both corporations remain under the ownership of National Amusements). For six years following the incident, all of the performers in Super Bowl halftime shows were artists associated with the classic rock genre of the 1970s and 1980s (including three acts from the British Invasion of the 1960s), with only one act playing the entire halftime show. Paul McCartney (formerly of The Beatles) played Super Bowl XXXIX in 2005, The Rolling Stones played Super Bowl XL in 2006, and The Who played Super Bowl XLIV in 2010. The halftime show returned to a modern act in 2011 with The Black Eyed Peas. But during the halftime show of Super Bowl XLVI in 2012, M.I.A. gave the middle finger during a performance of "Give Me All Your Luvin'" with Madonna, which was caught by TV cameras. An attempt to censor the gesture by blurring the entire screen came late.
Excluding Super Bowl XXXIX, the famous "I'm going to Disney World!" advertising campaign took place at every Super Bowl since Super Bowl XXI, when quarterback Phil Simms from the New York Giants became the first player to say the tagline.
Venue.
As of Super Bowl XLVIII, 27 of 49 Super Bowls have been played in three cities: New Orleans (ten times), the Greater Miami area (ten times), and the Greater Los Angeles area (seven times). Stadiums that do not host an NFL franchise are not, by rule, prohibited from hosting the Super Bowl, and non-NFL stadiums have hosted the game nine times, with the Rose Bowl accounting for five of these. To date, however, no market or region without an NFL franchise has ever hosted a Super Bowl; all five Rose Bowl Super Bowls were hosted before the Los Angeles Rams and Los Angeles Raiders left for St. Louis and Oakland respectively in 1995.
No team has ever played the Super Bowl in its home stadium. The closest have been the San Francisco 49ers who played Super Bowl XIX in Stanford Stadium, rather than Candlestick Park, and the Los Angeles Rams who played Super Bowl XIV in the Rose Bowl, rather than the Los Angeles Memorial Coliseum. In both cases, the stadium in which the Super Bowl was held was perceived to be a better stadium for a large, high-profile event than the stadiums the Rams and 49ers were playing in at the time; this situation has not arisen since 1993, in part because the league has traditionally awarded the Super Bowl in modern times to the newest stadiums. Besides those two, the only other Super Bowl venue that was not the home stadium to an NFL team at the time was Rice Stadium in Houston: the Houston Oilers had played there previously, but moved to the Astrodome several years prior to Super Bowl VIII. The Orange Bowl was the only AFL stadium to host a Super Bowl and the only stadium to host consecutive Super Bowls, hosting Super Bowls II and III.
Traditionally, the NFL does not award Super Bowls to stadiums that are located in climates with an expected average daily temperature less than 50 °F (10 °C) on game day unless the field can be completely covered by a fixed or retractable roof. Five Super Bowls have been played in northern cities: two in the Detroit area—Super Bowl XVI at Pontiac Silverdome in Pontiac, Michigan and Super Bowl XL at Ford Field in Detroit, one in Minneapolis—Super Bowl XXVI, one in Indianapolis at Lucas Oil Stadium for Super Bowl XLVI, and one in the New York area—Super Bowl XLVIII at MetLife Stadium. Only MetLife Stadium did not have a roof (be it fixed or retractable) but it was still picked as the host stadium for Super Bowl XLVIII in an apparent waiver of the warm-climate rule. A sixth Super Bowl is planned in a northern city as Minneapolis has been picked to host Super Bowl LII in 2018 in the under-construction New Minnesota Stadium.
There have been a few instances where the league has yanked the Super Bowl from cities. Super Bowl XXVII in 1993 was originally awarded to Sun Devil Stadium in Tempe, Arizona, but after Arizona voted to not recognize Martin Luther King, Jr. Day in 1990, the NFL moved the game to the Rose Bowl in Pasadena, California in protest. After Arizona opted to create the holiday by ballot in 1992, Super Bowl XXX in 1996 was awarded to Tempe. Super Bowl XXXIII was awarded first to Candlestick Park in San Francisco, but when plans to renovate the stadium fell through the game was moved to Pro Player Stadium in greater Miami. Super Bowl XXXVII was awarded to a new stadium not yet built in San Francisco, when that stadium failed to be built, the game was moved to San Diego. Super Bowl XLIV, slated for February 7, 2010, was withdrawn from New York City's proposed West Side Stadium, because the city, state, and proposed tenants New York Jets could not agree on funding. Super Bowl XLIV was then eventually awarded to Sun Life Stadium in Miami Gardens, Florida. And Super Bowl XLIX in 2015 was originally given to Arrowhead Stadium in Kansas City, Missouri, but after two sales taxes failed to pass at the ballot box, and opposition by local business leaders and politicians increased, Kansas City eventually withdrew its request to host the game. Super Bowl XLIX was then eventually awarded to University of Phoenix Stadium in Glendale, Arizona.
In 2011, Texas Attorney General Greg Abbott said, "It's commonly known as the single largest human trafficking incident in the United States." According to Forbes, 10,000 prostitutes were brought to Miami in 2010 for the Super Bowl.
Selection process.
The location of the Super Bowl is chosen by the NFL well in advance, usually three to five years before the game. Cities place bids to host a Super Bowl and are evaluated in terms of stadium renovation and their ability to host. In 2014, a document listing the specific requirements of Super Bowl hosts was leaked, giving a clear list of what was required for a Super Bowl host. Much of the cost of the Super Bowl is to be assumed by the host community, although some costs are enumerated within the requirements to be assumed by the NFL. Some of the host requirements include:
The NFL owners meet to make a selection on the site, usually three years prior to the event. In 2007, NFL commissioner Roger Goodell suggested that a Super Bowl might be played in London, England, perhaps at Wembley Stadium. The game has never been played in a region that lacks an NFL franchise; seven Super Bowls have been played in Los Angeles, but none since the Los Angeles Raiders and Los Angeles Rams relocated to Oakland and St. Louis respectively in 1995. New Orleans, the site of the 2013 Super Bowl, invested more than $1 billion in infrastructure improvements in the years leading up to the game.
Home team designation.
The designated "home team" alternates between the NFC team in odd-numbered games and the AFC team in even-numbered games. This alternation was initiated with the first Super Bowl, when the Green Bay Packers were the designated home team. Regardless of being the home or away team of record, each team has their team wordmark painted in one of the end zones along with their conference designation. Designated away teams have won 29 of 49 Super Bowls to date (59.2%).
Since Super Bowl XIII in January 1979, the home team is given the choice of wearing their colored or white jerseys. Formerly, the designated home team was specified to wear their colored jerseys, which resulted in Dallas donning their less familiar dark blue jerseys for Super Bowl V. While most of the home teams in the Super Bowl have chosen to wear their colored jerseys, there have been four exceptions; the Cowboys during Super Bowl XIII and XXVII, the Washington Redskins during Super Bowl XVII, and the Pittsburgh Steelers during Super Bowl XL. The Cowboys, since 1965, and Redskins, since the arrival of coach Joe Gibbs in 1981, have traditionally worn white jerseys at home. Meanwhile, the Steelers, who have always worn their black jerseys at home since the AFL-NFL merger in 1970, opted for the white jerseys after winning three consecutive playoff games on the road, wearing white. The Steelers' decision was compared with the New England Patriots in Super Bowl XX; the Patriots had worn white jerseys at home during the 1985 season, but after winning road playoff games against the New York Jets and Miami Dolphins wearing red jerseys, New England opted to switch to red for the Super Bowl as the designated home team. White-shirted teams have won 31 of 49 Super Bowls to date (63.2%).
Host cities/regions.
Fifteen different regions have hosted Super Bowls.
Host stadiums.
A total of twenty-four different stadiums have hosted, or are scheduled to host, Super Bowls. Years listed in the table below are the years the game was actually played "(will be played)" rather than what NFL season it is considered to have been.
^ Stadium is now demolished.<br>
† The original Stanford Stadium, which hosted Super Bowl XIX, was demolished and replaced with a new stadium in 2006.<br>
The game has never been played in a region that lacked an NFL franchise, though cities without NFL teams are not categorically ineligible to host the event. London, England has occasionally been mentioned as a host city for a Super Bowl in the near future. Wembley Stadium has hosted several NFL games as part of the NFL International Series and is specifically designed for large, individual events. NFL Commissioner Roger Goodell has openly discussed the possibility on different occasions. Time zone complications are a significant obstacle to a Super Bowl in London; a typical 6:30 p.m. Eastern Time start would result in the game beginning at 11:30 p.m. local time in London, an unusually late hour to be holding spectator sports (the NFL has never in its history started a game later than 9:15 p.m. local time).
Super Bowl trademark.
The NFL is vigilant on stopping what it says is unauthorized commercial use of its trademarked terms "NFL", "Super Bowl", and "Super Sunday". As a result, many events and promotions tied to the game, but not sanctioned by the NFL, are asked to refer to it with colloquialisms such as "The Big Game", or other generic descriptions. A radio spot for Planters nuts parodied this, by saying "it would be "super"...to have a "bowl"...of Planters nuts while watching the big game!" and comedian Stephen Colbert began referring to the game in 2014 as the "Superb Owl". The NFL claims that the use of the phrase "Super Bowl" implies an NFL affiliation, and on this basis the league asserts broad rights to restrict how the game may be shown publicly; for example, the league says Super Bowl showings are prohibited in churches or at other events that "promote a message", while venues that do not regularly show sporting events cannot show the Super Bowl on any television screen larger than 55 inches. Some critics say the NFL is exaggerating its ownership rights by stating that "any use is prohibited", as this contradicts the broad doctrine of fair use in the United States. Legislation was proposed by Utah Senator Orrin Hatch in 2008 "to provide an exemption from exclusive rights in copyright for certain nonprofit organizations to display live football games", and "for other purposes".
In 2006, the NFL made an attempt to trademark "The Big Game" as well; however, it withdrew the application in 2007 due to growing commercial and public-relations opposition to the move, mostly from Stanford University and the University of California, Berkeley and their fans, as the Stanford Cardinal football and California Golden Bears football teams compete in the "Big Game", which has been played since 1892 (28 years before the formation of the NFL and 75 years before Super Bowl I). Additionally, the Mega Millions lottery game was known as The Big Game from 1996 to 2002.
Use of the phrase "world champions".
Like the other major professional leagues in the United States, the winner of the Super Bowl is usually declared "world champions", a title often mocked by non-Americans. Others feel the title is fitting, since it is the only professional league of its kind.
The practice by the U.S. major leagues of using the "World Champion" moniker originates from the World Series, and it was later used during the first three Super Bowls when they were referred to as AFL-NFL World Championship Games. The phrase is still engraved on the Super Bowl rings.

</doc>
<doc id="27761" url="http://en.wikipedia.org/wiki?curid=27761" title="School choice">
School choice

School choice is a term or label given to a wide array of programs offering students and their families alternatives to publicly provided schools, to which students are generally assigned by the location of their family residence. In the United States, the most common—both by number of programs and by number of participating students—school choice programs are scholarship tax credit programs, which allow individuals or corporations to receive tax credits toward their state taxes in exchange for donations made to non-profit organizations that grant private school scholarships. In other cases, a similar subsidy may be provided by the state through a school voucher program. Other school choice options include open enrollment laws (which allow students to attend public schools outside of the district in which the students live), charter schools, magnet schools, virtual schools, homeschooling, education savings accounts (ESAs), and individual tax credits or deductions for educational expenses.
Forms.
Scholarship Tax Credits.
States with scholarship tax credit programs grant individuals and/or businesses a credit, whether full or partial, toward their taxes for donations made to scholarship granting organizations (also called school tuition organizations). SGOs/STOs use the donations to create scholarships that are then given to help pay for the cost of tuition for students. These scholarships allow students to attend private schools or out-of-district public schools that would otherwise be prohibitively expensive for many families. These programs currently exist in fourteen states: Alabama, Arizona, Florida, Georgia, Illinois, Iowa, Kansas, Louisiana, Minnesota, New Hampshire, Oklahoma, Pennsylvania, Rhode Island, and Virginia in the United States.
Vouchers.
In a traditional public education system, schools receive funding from the state on a per student basis. Under a voucher system, eligible "students" receive state funding ("vouchers") which can be spent at whatever eligible private schools the parents choose for their children. The two most common voucher designs are universal vouchers and means-tested vouchers. Means-tested vouchers are directed towards low-income families and constitute the bulk of voucher plans in the United States.
Charter schools.
Charter schools are independent public schools which are exempt from many of the state and local regulations which govern most public schools. These exemptions grant charter schools some autonomy and flexibility with decision-making, such as teacher union contracts, hiring, and curriculum. In return, charter schools are subject to stricter accountability on spending and academic performance. The majority of states (and the District of Columbia) have charter school laws, though they vary in how charter schools are approved. Minnesota was the first state to have a charter school law and the first charter school in the United States, City Academy High School, opened in St. Paul, Minnesota in 1992.
22-26% of Dayton, Ohio children are in charter schools. This is the highest percentage in the nation. Other hotbeds for charter schools are Kansas City (24%), Washington, D.C. (20-24%), and Arizona. Almost 1 in 4 public schools in Arizona are charter schools, comprising about 8% of total enrollment.
Charter schools can also come in the form of cyber charters. Cyber charter schools deliver the majority of their instruction over the internet instead of in a school building. And, like all charter schools, cyber charters are public schools, but they are free from some of the rules and regulations that conventional public schools must follow.
Magnet schools.
Magnet schools are public schools that often have a specialized function like science, technology, or art. These magnet schools, unlike charter schools, are not open to all children. Much like many private schools, some (but not all) magnet schools require a test to get in.
Home schooling.
"Home education" or "home schooling" is instruction in a child's home, or provided primarily by a parent, or under direct parental control. Informal home education has always taken place, and formal instruction in the home has at times also been very popular. As public education grew in popularity during the 1900s, however, the number of people educated at home using a planned curriculum dropped. In the last 20 years, in contrast, the number of children being formally educated at home has grown tremendously, in particular in the United States. The laws relevant to home education differ throughout the country. In some states the parent simply needs to notify the state that the child will be educated at home. In other states the parents are not free to educate at home unless at least one parent is a certified teacher and yearly progress reports are reviewed by the state. Such laws are not always enforced however. According to the federal government, about 1.1 million children were home educated in 2003.
Education Savings Accounts.
Education Savings Accounts (ESAs) are somewhat similar to vouchers: a percentage of the funds that the state would otherwise spend to educate a student in a public school are instead given to the student's family to spend on private school tuition. However, ESAs give parents additional flexibility to customize their children's educations. For example, in addition to private school tuition, ESA funds may be used for private tutoring or online learning. Alternatively, ESA funds may be saved to pay for future higher education costs. Currently, there are ESA programs in two states: Arizona ("Empowerment Savings Accounts") and Florida (the "Personal Learning Scholarship Account Program").
Tax Credit/Deduction For Educational Expenses.
Certain states allow parents to claim a tax credit or deduction as a means to provide relief for certain educational expenses. These can include private school tuition, textbooks, school supplies and equipment, tutoring, and transportation. Currently, Alabama, Illinois, Indiana, Iowa, Louisiana, Minnesota, and Wisconsin have such programs.
Debate.
Support.
The goal of school choice programs is to give parents more control over their child's education and to allow parents to pursue the most appropriate learning environments for children. For example, school choice may enable parents to choose a school that provides religious instruction, stronger discipline, better foundational skills (including reading, writing, mathematics, and science), everyday skills (from handling money to farming), or other desirable foci.
Supporters of voucher models of school choice argue that choice creates competition between schools for students. Schools that fail to attract students can be closed. Advocates of school choice argue that this competition for students (and the dollars that come with them) create a catalyst for schools to create innovative programs, become more responsive to parental demands, and to increase student achievement. Caroline Hoxby suggests that this competition increases the productivity of a school. Hoxby describes a productive school as being one that produces high achievements in its student for each dollar that is spends. Others suggest that this competition gives parents more power to influence their child's school in the school marketplace. Parents and students become the consumers and schools must work to attract new students with new programs. Parents also have the ability to punish schools that they judge to be inferior by leaving the 'bad' school for a better, more highly ranked school. Parents look for schools that will advocate for the needs of their child and if the school does not meet the needs required for that child, parents have the choice to find a school that will be more suitable
Another argument in favor of school choice is based on cost-effectiveness. Studies undertaken by the Cato Institute and other libertarian and conservative thinktanks conclude that privately run education both costs less and produces superior outcomes compared to public education.
Others argue that since children from impoverished families almost exclusively attend D or F ranked public schools, school choice programs would give parents the power to opt their children out of poorly-performing schools assigned by zip code and seek better education elsewhere. Supporters say this would level the playing field by broadening opportunities for low-income students—particularly minorities—to attend high-quality schools that would otherwise be accessible only to higher-income families.
The Organisation Internationale pour le Droit à l'Education et la Liberté d'Enseignement (OIDEL), an international non-profit organization for the development of freedom of education, maintains that the right to education is a fundamental human right which cannot exist without the presence of State benefits and the protection of individual liberties. According to the organization, freedom of education notably implies the freedom for parents to choose a school for their children without discrimination on the basis of finances. To advance freedom of education, OIDEL promotes a greater parity between public and private schooling systems.
Opposition.
Some school choice measures are criticized by public school entities, organizations opposed to church-state entanglement, and self-identified liberal advocacy groups. Known plaintiffs who have filed suit to challenge the constitutionality of state sponsored school choice laws are as follows: School Boards Associations, Public School Districts, Federations for Teachers, Associations of School Business Officials, Education Associations/Associations of Educators (unions for public school teachers), the American Civil Liberties Union, Freedom From Religion Foundation, and People for the American Way.
Public school entities are chiefly concerned that these school choice measures are taking funding away from public schools and therefore depleting their already strained resources. Alternatively, public school entities may argue that non-traditional forms of education lack sufficient regulation and oversight. Other opponents of certain school choice policies (particularly vouchers) have cited the Establishment Clause and individual state Blaine amendments, which forbid, to one degree or another, the use of direct government aid to religiously affiliated entities. This is of particular concern in the voucher debate because voucher dollars are often spent at parochial schools.
International overview and major institutional options.
France.
The French government subsidizes most private primary and secondary schools, including those affiliated with religious denominations, under contracts stipulating that education must follow the same curriculum as public schools and that schools cannot discriminate on grounds of religion or force pupils to attend religion classes.
This system of "école libre" (Free Schooling) is mostly used not for religious reasons, but for practical reasons (private schools may offer more services, such as after-class tutoring) as well as the desire of parents living in disenfranchised areas to send their children away from the local schools, where they perceive that the youth are too prone to delinquency or have too many difficulties keeping up with schooling requirements that the educational content is bound to suffer. The threatened repealing of that status in the 1980s triggered mass street demonstrations in favor of the status. 
Sweden.
Sweden reformed its school system in 1992. Its system of school choice is one of the freest in the world, allowing students to use public funds for the publicly or privately run school of their choice, including religious and for-profit schools. Fifteen years after the reform, private school enrolment had increased from 1% to 10% of the student population.
Canada.
Ontario is the only large province in Canada with limited school choice funding, Catholic, Secular and one Protestant school receive funding and are open to all students. In 2003, following an international human rights ruling, the provincial Conservative government gradually introduced a tax credit over 5 years, (when it would have been fully implemented it would have been worth up to 50% of tuition to a maximum of $3,500 at any independent school in Ontario) in order to meet the human rights norms and expand funded choice to all interested parents. However, the tax credit was retroactively canceled by the subsequent Liberal government when it had been only been in place for two years to the $1,000 point. Currently there are over 900 independent schools in Ontario. The only school choice program available to non-rich parents who wish to send their children to an independent school is a privately funded program called , a program of The Fraser Institute.
Chile.
In Chile, there is an extensive voucher system in which the state pays private and municipal schools directly, based on average attendance (90% of the country students utilize such a system). The result has been a steady increase in the number and recruitment of private schools that show consistently better results in standardized testing than municipal schools. The reduction of students in municipal schools has gone from 78% of all students in 1981, to 57% in 1990, and to less than 50% in 2005.
Regarding vouchers in Chile, researchers have found that when controls for the student's background (parental income and education) are introduced, the difference in performance between public and private subsectors is not significant. There is also greater variation within each subsector than between the two systems.
United States.
A variety of forms of school choice exist in the United States.
Scholarship Tax Credits.
Scholarship tax credit programs currently exist in Alabama, Arizona, Florida, Georgia, Illinois, Iowa, Kansas, Louisiana, Minnesota, New Hampshire, Oklahoma, Pennsylvania, Rhode Island, and Virginia.
Arizona has a well-known and fast-growing tax credit program. In the Arizona Individual Private School Tuition Tax Credit Program, in accordance with A.R.S. §43-1089 and §1089.03, individuals can claim up to $1,053 and couples filing joint returns can claim up to $2106 (for 2014, amounts are indexed annually). Nearly 24,000 children received scholarships in the 2011-2012 school year. Since the program has started in 1998, over 77,500 taxpayers have participated in the program, providing over $500 million in scholarship money for children at private schools across the state.
The Arizona program was challenged in court in "ACSTO v Winn" by a group of state taxpayers on the grounds that the tax credit violated the First Amendment because the tuition grants could go to students who attend private schools with religious affiliations. The suit was initially brought against the state until the Arizona Christian School Tuition Organization (ACSTO), one of the largest School Tuition Organizations in the state, voluntarily stepped in to represent the defense with the help of the Alliance Defending Freedom (formerly Alliance Defense Fund). Typically, taxpayers are not allowed to bring suit against the government regarding how taxes are spent because injury would be purely speculative. In addition, insomuch as a donation to a School Tuition Organization is still a charitable act, just like any donation to a charity, there would be no standing unless all charitable deduction programs nationwide were brought under scrutiny. The Court ruled 5-4 to let the tax credit program stand. In April 2011, a Fairleigh Dickinson University PublicMind poll found that a majority of American voters (60%) felt that the tax credits support school choice for parents whereas 26% felt as it the tax credits support religion.
In Iowa, the Educational Opportunities Act was signed into law in 2006, creating a pool of tax credits for eligible donors to student tuition organizations (STOs). At first, these tax caps were $5 million but in 2007, Governor Chet Culver increased the total amount to $7.5 million. The Iowa Alliance for Choice in Education (Iowa ACE) oversees the STOs and advocates for school choice in Iowa.
Greater Opportunities for Access to Learning (GOAL) is the Georgia program which offers a state income tax credit to donors of scholarships to private schools. Representative David Casas was responsible for passing the Georgia version of the school choice legislation.
Vouchers.
Vouchers currently exist in Wisconsin, Ohio, Florida, and, most recently, the District of Columbia and Georgia.
The largest and oldest Voucher program is in Milwaukee. Started in 1990, and expanded in 1995, it currently allows no more than 15% of the district's public school enrollment to use vouchers. As of 2005 over 14,000 students use vouchers and they are nearing the 15% cap.
School vouchers are legally controversial in some states. In 2014 a lawsuit sought to challenge the legality of the Florida voucher program.
In the U.S., the legal and moral precedents for vouchers may have been set by the G.I. bill, which includes a voucher program for university-level education of veterans. The G.I. bill permits veterans to take their educational benefits at religious schools, an extremely divisive issue when applied to primary and secondary schools.
In "Zelman v. Simmons-Harris", 536 U.S. 639 (2002), the Supreme Court of the United States held that school vouchers could be used to pay for education in sectarian schools without violating the Establishment Clause of the First Amendment. As a result, states are basically free to enact voucher programs that provide funding for any school of the parent's choosing.
The Supreme Court has not decided, however, whether states can provide vouchers for secular schools only, excluding sectarian schools. Proponents of funding for parochial schools argue that such an exclusion would violate the free exercise clause. However, in "Locke v. Davey", 540 U.S. 712 (2004), the Court held that states could exclude majors in "devotional theology" from an otherwise generally available college scholarship. The Court has not indicated, however, whether this holding extends to the public school context, and it may well be limited to the context of individuals training to enter the ministry.
Charter schools.
The majority of states (and the District of Columbia) have charter school laws. Minnesota was the first state to have a charter school law and the first charter school in the United States, City Academy, opened in St. Paul, Minnesota in 1992.
Dayton, Ohio has between 22–26% of all children in charter schools. This is the highest percentage in the nation. Other hotbeds for charter schools are Kansas City (24%), Washington, D.C. (20-24%) and the State of Arizona. Almost 1 in 4 public schools in Arizona are charter schools, comprising about 8% of total enrollment.
Charter schools can also come in the form of Cyber Charters. Cyber charter schools deliver the majority of their instruction over the internet instead of in a school building. And, like charter schools, they are public schools, but free of many of the rules and regulations that public schools must follow.
Magnet schools.
Magnet schools are public schools that often have a specialized function like science, technology or art. These magnet schools, unlike charter schools, are not open to all children. Much like many private schools, the students must test into the school.
Home schooling.
The laws relevant to homeschooling differ between US states. In some states the parent simply needs to notify the state that the child will be educated at home. In other states the parents are not free to educate at home unless at least one parent is a certified teacher and yearly progress reports are reviewed by the state. Such laws are not always enforced however. According to the Federal Government, about 1.1 million children were Home Educated in 2003.
College.
The United States has school choice at the university level. College students can get subsidized tuition by attending "any" public college or university within their state of residence. Furthermore, the U.S. federal government provides tuition assistance for both public and private colleges via the G.I. Bill and federally guaranteed student loans.

</doc>
<doc id="27763" url="http://en.wikipedia.org/wiki?curid=27763" title="Structuralism">
Structuralism

In sociology, anthropology and linguistics, structuralism is the theory that elements of human culture must be understood in terms of their relationship to a larger, overarching system or structure. It works to uncover the structures that underlie all the things that humans do, think, perceive, and feel. Alternatively, as summarized by philosopher Simon Blackburn, structuralism is "the belief that phenomena of human life are not intelligible except through their interrelations. These relations constitute a structure, and behind local variations in the surface phenomena there are constant laws of abstract culture".
Structuralism in Europe developed in the early 1900s, in the structural linguistics of Ferdinand de Saussure and the subsequent Prague, Moscow and Copenhagen schools of linguistics. In the late 1950s and early '60s, when structural linguistics was facing serious challenges from the likes of Noam Chomsky and thus fading in importance, an array of scholars in the humanities borrowed Saussure's concepts for use in their respective fields of study. French anthropologist Claude Lévi-Strauss was arguably the first such scholar, sparking a widespread interest in Structuralism.
The structuralist mode of reasoning has been applied in a diverse range of fields, including anthropology, sociology, psychology, literary criticism, economics and architecture. The most prominent thinkers associated with structuralism include Lévi-Strauss, linguist Roman Jakobson, and psychoanalyst Jacques Lacan. As an intellectual movement, structuralism was initially presumed to be the heir apparent to existentialism. However, by the late 1960s, many of structuralism's basic tenets came under attack from a new wave of predominantly French intellectuals such as the philosopher and historian Michel Foucault, the philosopher and social commentator Jacques Derrida, the Marxist philosopher Louis Althusser, and the literary critic Roland Barthes. Though elements of their work necessarily relate to structuralism and are informed by it, these theorists have generally been referred to as post-structuralists.
In the 1970s, structuralism was criticised for its rigidity and ahistoricism. Despite this, many of structuralism's proponents, such as Jacques Lacan, continue to assert an influence on continental philosophy and many of the fundamental assumptions of some of structuralism's post-structuralist critics are a continuation of structuralism.
Overview.
The term "structuralism" is a belated term that describes a particular philosophical/literary movement or moment. The term appeared in the works of French anthropologist Claude Lévi-Strauss and gave rise, in France, to the "structuralist movement." Influencing the thinking of writers such as Louis Althusser, the psychoanalyst Jacques Lacan, as well as the structural Marxism of Nicos Poulantzas, most of whom disavowed themselves as being a part of this movement.
The origins of structuralism connect with the work of Ferdinand de Saussure on linguistics, along with the linguistics of the Prague and Moscow schools. In brief, de Saussure's structural linguistics propounded three related concepts.
Proponents of structuralism would argue that a specific domain of culture may be understood by means of a structure—modelled on language—that is distinct both from the organizations of reality and those of ideas or the imagination—the "third order". In Lacan's psychoanalytic theory, for example, the structural order of "the Symbolic" is distinguished both from "the Real" and "the Imaginary"; similarly, in Althusser's Marxist theory, the structural order of the capitalist mode of production is distinct both from the actual, real agents involved in its relations and from the ideological forms in which those relations are understood.
Blending Freud and de Saussure, the French (post)structuralist Jacques Lacan applied structuralism to psychoanalysis and, in a different way, Jean Piaget applied structuralism to the study of psychology. But Jean Piaget, who would better define himself as constructivist, considers structuralism as "a method and not a doctrine" because for him "there exists no structure without a construction, abstract or genetic".
Although the French theorist Louis Althusser is often associated with a brand of structural social analysis which helped give rise to "structural Marxism", such association was contested by Althusser himself in the Italian foreword to the second edition of "Reading Capital". In this foreword Althusser states the following: 
"Despite the precautions we took to distinguish ourselves from the 'structuralist' ideology ..., despite the decisive intervention of categories foreign to 'structuralism' ..., the terminology we employed was too close in many respects to the 'structuralist' terminology not to give rise to an ambiguity. With a very few exceptions ... our interpretation of Marx has generally been recognized and judged, in homage to the current fashion, as 'structuralist'... We believe that despite the terminological ambiguity, the profound tendency of our texts was not attached to the 'structuralist' ideology."
In a later development, feminist theorist Alison Assiter enumerated four ideas that she says are common to the various forms of structuralism. First, that a structure determines the position of each element of a whole. Second, that every system has a structure. Third, structural laws deal with co-existence rather than change. Fourth, structures are the "real things" that lie beneath the surface or the appearance of meaning.
Structuralism in linguistics.
In Ferdinand de Saussure's "Course in General Linguistics" (written by Saussure's colleagues after his death and based on student notes), the analysis focuses not on the "use" of language (called ""parole", or speech), but rather on the underlying system of language (called "langue""). This approach examines how the elements of language relate to each other in the present, synchronically rather than diachronically. Saussure argued that linguistic signs were composed of two parts:
This was quite different from previous approaches that focused on the relationship between words and the things in the world that they designate. Other key notions in structural linguistics include paradigm, syntagm, and value (though these notions were not fully developed in Saussure's thought). A structural "idealism" is a class of linguistic units (lexemes, morphemes or even constructions) that are possible in a certain position in a given linguistic environment (such as a given sentence), which is called the "syntagm". The different functional role of each of these members of the paradigm is called "value" ("valeur" in French).
Saussure's "Course" influenced many linguists between World War I and World War II. In the United States, for instance, Leonard Bloomfield developed his own version of structural linguistics, as did Louis Hjelmslev in Denmark and Alf Sommerfelt in Norway. In France Antoine Meillet and Émile Benveniste continued Saussure's project. Most importantly, however, members of the Prague school of linguistics such as Roman Jakobson and Nikolai Trubetzkoy conducted research that would be greatly influential. However, by the 1950s Saussure's linguistic concepts were under heavy criticism and were soon largely abandoned by practicing linguists: 
"Saussure's views are not held, so far as I know, by modern linguists, only by literary critics and the occasional philosopher. [Strict adherence to Saussure] has elicited wrong film and literary theory on a grand scale. One can find dozens of books of literary theory bogged down in signifiers and signifieds, but only a handful that refer to Chomsky."
The clearest and most important example of Prague school structuralism lies in phonemics. Rather than simply compiling a list of which sounds occur in a language, the Prague school sought to examine how they were related. They determined that the inventory of sounds in a language could be analyzed in terms of a series of contrasts. Thus in English the sounds /p/ and /b/ represent distinct phonemes because there are cases (minimal pairs) where the contrast between the two is the only difference between two distinct words (e.g. 'pat' and 'bat'). Analyzing sounds in terms of contrastive features also opens up comparative scope—it makes clear, for instance, that the difficulty Japanese speakers have differentiating /r/ and /l/ in English is because these sounds are not contrastive in Japanese. Phonology would become the paradigmatic basis for structuralism in a number of different fields.
Structuralism in anthropology.
According to structural theory in anthropology and social anthropology, meaning is produced and reproduced within a culture through various practices, phenomena and activities that serve as systems of signification. A structuralist approach may study activities as diverse as food-preparation and serving rituals, religious rites, games, literary and non-literary texts, and other forms of entertainment to discover the deep structures by which meaning is produced and reproduced within the culture. For example, Lévi-Strauss analyzed in the 1950s cultural phenomena including mythology, kinship (the alliance theory and the incest taboo), and food preparation. In addition to these studies, he produced more linguistically focused writings in which he applied Saussure's distinction between "langue" and "parole" in his search for the fundamental structures of the human mind, arguing that the structures that form the "deep grammar" of society originate in the mind and operate in people unconsciously. Lévi-Strauss took inspiration from mathematics.
Another concept used in structural anthropology came from the Prague school of linguistics, where Roman Jakobson and others analyzed sounds based on the presence or absence of certain features (such as voiceless vs. voiced). Lévi-Strauss included this in his conceptualization of the universal structures of the mind, which he held to operate based on pairs of binary oppositions such as hot-cold, male-female, culture-nature, cooked-raw, or marriageable vs. tabooed women.
A third influence came from Marcel Mauss (1872–1950), who had written on gift-exchange systems. Based on Mauss, for instance, Lévi-Strauss argued that kinship systems are based on the exchange of women between groups (a position known as 'alliance theory') as opposed to the 'descent'-based theory described by Edward Evans-Pritchard and Meyer Fortes. While replacing Marcel Mauss at his "Ecole Pratique des Hautes Etudes" chair, Lévi-Strauss' writing became widely popular in the 1960s and 1970s and gave rise to the term "structuralism" itself.
In Britain, authors such as Rodney Needham and Edmund Leach were highly influenced by structuralism. Authors such as Maurice Godelier and Emmanuel Terray combined Marxism with structural anthropology in France. In the United States, authors such as Marshall Sahlins and James Boon built on structuralism to provide their own analysis of human society. Structural anthropology fell out of favour in the early 1980s for a number of reasons. D'Andrade suggests that this was because it made unverifiable assumptions about the universal structures of the human mind. Authors such as Eric Wolf argued that political economy and colonialism should be at the forefront of anthropology. More generally, criticisms of structuralism by Pierre Bourdieu led to a concern with how cultural and social structures were changed by human agency and practice, a trend which Sherry Ortner has referred to as 'practice theory'.
Some anthropological theorists, however, while finding considerable fault with Lévi-Strauss's version of structuralism, did not turn away from a fundamental structural basis for human culture. The Biogenetic Structuralism group for instance argued that some kind of structural foundation for culture must exist because all humans inherit the same system of brain structures. They proposed a kind of Neuroanthropology which would lay the foundations for a more complete scientific account of cultural similarity and variation by requiring an integration of cultural anthropology and neuroscience—a program that theorists such as Victor Turner also embraced.
Structuralism in literary theory and criticism.
In literary theory, structuralist criticism relates literary texts to a larger structure, which may be a particular genre, a range of intertextual connections, a model of a universal narrative structure, or a system of recurrent patterns or motifs. Structuralism argues that there must be a structure in every text, which explains why it is easier for experienced readers than for non-experienced readers to interpret a text. Hence, everything that is written seems to be governed by specific rules, or a "grammar of literature", that one learns in educational institutions and that are to be unmasked.
A potential problem of structuralist interpretation is that it can be highly reductive, as scholar Catherine Belsey puts it: "the structuralist danger of collapsing all difference." An example of such a reading might be if a student concludes the authors of "West Side Story" did not write anything "really" new, because their work has the same structure as Shakespeare's "Romeo and Juliet". In both texts a girl and a boy fall in love (a "formula" with a symbolic operator between them would be "Boy + Girl") despite the fact that they belong to two groups that hate each other ("Boy's Group - Girl's Group" or "Opposing forces") and conflict is resolved by their death. Structuralist readings focus on how the structures of the single text resolve inherent narrative tensions. If a structuralist reading focuses on multiple texts, there must be some way in which those texts unify themselves into a coherent system. The versatility of structuralism is such that a literary critic could make the same claim about a story of two "friendly" families ("Boy's Family + Girl's Family") that arrange a marriage between their children despite the fact that the children hate each other ("Boy - Girl") and then the children commit suicide to escape the arranged marriage; the justification is that the second story's structure is an 'inversion' of the first story's structure: the relationship between the values of love and the two pairs of parties involved have been reversed.
Structuralistic literary criticism argues that the "literary banter of a text" can lie only in new structure, rather than in the specifics of character development and voice in which that structure is expressed. Literary structuralism often follows the lead of Vladimir Propp, Algirdas Julien Greimas, and Claude Lévi-Strauss in seeking out basic deep elements in stories, myths, and more recently, anecdotes, which are combined in various ways to produce the many versions of the ur-story or ur-myth.
There is considerable similarity between structural literary theory and Northrop Frye's archetypal criticism, which is also indebted to the anthropological study of myths. Some critics have also tried to apply the theory to individual works, but the effort to find unique structures in individual literary works runs counter to the structuralist program and has an affinity with New Criticism.
History and background.
Throughout the 1940s and 1950s, existentialism, such as that propounded by Jean-Paul Sartre, was the dominant European intellectual movement. Structuralism rose to prominence in France in the wake of existentialism, particularly in the 1960s. The initial popularity of structuralism in France led to its spread across the globe.
Structuralism rejected the concept of human freedom and choice and focused instead on the way that human experience and thus, behavior, is determined by various structures. The most important initial work on this score was Claude Lévi-Strauss's 1949 volume "The Elementary Structures of Kinship". Lévi-Strauss had known Jakobson during their time together at the New School in New York during WWII and was influenced by both Jakobson's structuralism as well as the American anthropological tradition. In "Elementary Structures" he examined kinship systems from a structural point of view and demonstrated how apparently different social organizations were in fact different permutations of a few basic kinship structures. In the late 1950s he published "Structural Anthropology", a collection of essays outlining his program for structuralism.
By the early 1960s structuralism as a movement was coming into its own and some believed that it offered a single unified approach to human life that would embrace all disciplines. Roland Barthes and Jacques Derrida focused on how structuralism could be applied to literature.
The so-called "Gang of Four" of structuralism was Lévi-Strauss, Lacan, Barthes, and Foucault.
Interpretations and general criticisms.
Structuralism is less popular today than other approaches, such as post-structuralism and deconstruction. Structuralism has often been criticized for being ahistorical and for favoring deterministic structural forces over the ability of people to act. As the political turbulence of the 1960s and 1970s (and particularly the student uprisings of May 1968) began affecting academia, issues of power and political struggle moved to the center of people's attention.
In the 1980s, deconstruction—and its emphasis on the fundamental ambiguity of language rather than its crystalline logical structure—became popular. By the end of the century structuralism was seen as an historically important school of thought, but the movements that it spawned, rather than structuralism itself, commanded attention.
Several social thinkers and academics have strongly criticized structuralism or even dismissed it "in toto". The French hermeneutic philosopher Paul Ricœur (1969) criticized Lévi-Strauss for constantly overstepping the limits of validity of the structuralist approach, ending up in what Ricoeur described as "a Kantianism without a transcendental subject". Anthropologist Adam Kuper (1973) argued that "'Structuralism' came to have something of the momentum of a millennial movement and some of its adherents felt that they formed a secret society of the seeing in a world of the blind. Conversion was not just a matter of accepting a new paradigm. It was, almost, a question of salvation." Philip Noel Pettit (1975) called for an abandoning of "the positivist dream which Lévi-Strauss dreamed for semiology" arguing that semiology is not to be placed among the natural sciences. Cornelius Castoriadis (1975) criticized structuralism as failing to explain symbolic mediation in the social world; he viewed structuralism as a variation on the "logicist" theme, and he argued that, contrary to what structuralists advocate, language—and symbolic systems in general—cannot be reduced to logical organizations on the basis of the binary logic of oppositions. Critical theorist Jürgen Habermas (1985) accused structuralists, such as Foucault, of being positivists; he remarked that while Foucault is not an ordinary positivist, he nevertheless paradoxically uses the tools of science to criticize science (see "Performative contradiction" and "Foucault–Habermas debate"). Sociologist Anthony Giddens (1993) is another notable critic; while Giddens draws on a range of structuralist themes in his theorizing, he dismisses the structuralist view that the reproduction of social systems is merely "a mechanical outcome".

</doc>
<doc id="27774" url="http://en.wikipedia.org/wiki?curid=27774" title="Scanning tunneling microscope">
Scanning tunneling microscope

A scanning tunneling microscope (STM) is an instrument for imaging surfaces at the atomic level. Its development in 1981 earned its inventors, Gerd Binnig and Heinrich Rohrer (at IBM Zürich), the Nobel Prize in Physics in 1986. For an STM, good resolution is considered to be 0.1 nm lateral resolution and 0.01 nm depth resolution. With this resolution, individual atoms within materials are routinely imaged and manipulated. The STM can be used not only in ultra-high vacuum but also in air, water, and various other liquid or gas ambients, and at temperatures ranging from near zero kelvin to a few hundred degrees Celsius.
The STM is based on the concept of quantum tunneling. When a conducting tip is brought very near to the surface to be examined, a bias (voltage difference) applied between the two can allow electrons to tunnel through the vacuum between them. The resulting "tunneling current" is a function of tip position, applied voltage, and the local density of states (LDOS) of the sample. Information is acquired by monitoring the current as the tip's position scans across the surface, and is usually displayed in image form. STM can be a challenging technique, as it requires extremely clean and stable surfaces, sharp tips, excellent vibration control, and sophisticated electronics, but nonetheless many hobbyists have built their own.
US4,343,993, written by Gerd Binnig and Heinrich Rohrer, is the basic patent of STM.
Procedure.
First, a voltage bias is applied and the tip is brought close to the sample by coarse sample-to-tip control, which is turned off when the tip and sample are sufficiently close. At close range, fine control of the tip in all three dimensions when near the sample is typically piezoelectric, maintaining tip-sample separation W typically in the 4-7 Å (0.4-0.7 nm) range, which is the equilibrium position between attractive (3<W<10Å) and repulsive (W<3Å) interactions. In this situation, the voltage bias will cause electrons to tunnel between the tip and sample, creating a current that can be measured. Once tunneling is established, the tip's bias and position with respect to the sample can be varied (with the details of this variation depending on the experiment) and data are obtained from the resulting changes in current.
If the tip is moved across the sample in the x-y plane, the changes in surface height and density of states cause changes in current. These changes are mapped in images. This change in current with respect to position can be measured itself, or the height, z, of the tip corresponding to a constant current can be measured. These two modes are called constant height mode and constant current mode, respectively. In constant current mode, feedback electronics adjust the height by a voltage to the piezoelectric height control mechanism. This leads to a height variation and thus the image comes from the tip topography across the sample and gives a constant charge density surface; this means contrast on the image is due to variations in charge density. In constant height mode, the voltage and height are both held constant while the current changes to keep the voltage from changing; this leads to an image made of current changes over the surface, which can be related to charge density. The benefit to using a constant height mode is that it is faster, as the piezoelectric movements require more time to register the height change in constant current mode than the current change in constant height mode. All images produced by STM are grayscale, with color optionally added in post-processing in order to visually emphasize important features.
In addition to scanning across the sample, information on the electronic structure at a given location in the sample can be obtained by sweeping voltage and measuring current at a specific location. This type of measurement is called scanning tunneling spectroscopy (STS) and typically results in a plot of the local density of states as a function of energy within the sample. The advantage of STM over other measurements of the density of states lies in its ability to make extremely local measurements: for example, the density of states at an impurity site can be compared to the density of states far from impurities.
Framerates of at least 1 Hz enable so called Video-STM (up to 50 Hz is possible). This can be used to scan surface diffusion.
Instrumentation.
The components of an STM include scanning tip, piezoelectric controlled height and x,y scanner, coarse sample-to-tip control, vibration isolation system, and computer.
The resolution of an image is limited by the radius of curvature of the scanning tip of the STM. Additionally, image artifacts can occur if the tip has two tips at the end rather than a single atom; this leads to “double-tip imaging,” a situation in which both tips contribute to the tunneling. Therefore it has been essential to develop processes for consistently obtaining sharp, usable tips. Recently, carbon nanotubes have been used in this instance.
The tip is often made of tungsten or platinum-iridium, though gold is also used. Tungsten tips are usually made by electrochemical etching, and platinum-iridium tips by mechanical shearing.
Due to the extreme sensitivity of tunnel current to height, proper vibration isolation or an extremely rigid STM body is imperative for obtaining usable results. In the first STM by Binnig and Rohrer, magnetic levitation was used to keep the STM free from vibrations; now mechanical spring or gas spring systems are often used. Additionally, mechanisms for reducing eddy currents are sometimes implemented.
Maintaining the tip position with respect to the sample, scanning the sample and acquiring the data is computer controlled. The computer may also be used for enhancing the image with the help of image processing as well as performing quantitative measurements.
Probe tips.
STM tips are usually made from tungsten metal or a platinum-iridium alloy where at the very end of the tip (called apex) there is one atom of the material.
Other STM related studies.
Many other microscopy techniques have been developed based upon STM. These include photon scanning microscopy (PSTM), which uses an optical tip to tunnel photons; scanning tunneling potentiometry (STP), which measures electric potential across a surface; spin polarized scanning tunneling microscopy (SPSTM), which uses a ferromagnetic tip to tunnel spin-polarized electrons into a magnetic sample, and atomic force microscopy (AFM), in which the force caused by interaction between the tip and sample is measured.
Other STM methods involve manipulating the tip in order to change the topography of the sample. This is attractive for several reasons. Firstly the STM has an atomically precise positioning system which allows very accurate atomic scale manipulation. Furthermore, after the surface is modified by the tip, it is a simple matter to then image with the same tip, without changing the instrument. IBM researchers developed a way to manipulate xenon atoms adsorbed on a nickel surface. This technique has been used to create electron "corrals" with a small number of adsorbed atoms, which allows the STM to be used to observe electron Friedel oscillations on the surface of the material. Aside from modifying the actual sample surface, one can also use the STM to tunnel electrons into a layer of electron beam photoresist on a sample, in order to do lithography. This has the advantage of offering more control of the exposure than traditional electron beam lithography. Another practical application of STM is atomic deposition of metals (gold, silver, tungsten, etc.) with any desired (pre-programmed) pattern, which can be used as contacts to nanodevices or as nanodevices themselves.
Variable temperature STM was used to investigate temperature dependendy of molecular rotations on single crystalline surfaces.
Rotating molecules appear blurred compared to non-rotating ones.
Recently groups have found they can use the STM tip to rotate individual bonds within single molecules. The electrical resistance of the molecule depends on the orientation of the bond, so the molecule effectively becomes a molecular switch.
Principle of operation.
Tunneling is a functioning concept that arises from quantum mechanics. Classically, an object hitting an impenetrable barrier will not pass through. In contrast, objects with a very small mass, such as the electron, have wavelike characteristics which permit such an event, referred to as tunneling.
Electrons behave as beams of energy, and in the presence of a potential "U"("z"), assuming 1-dimensional case, the energy levels "ψn"("z") of the electrons are given by solutions to Schrödinger’s equation,
where "ħ" is the reduced Planck’s constant, "z" is the position, and "m" is the mass of an electron. If an electron of energy "E" is incident upon an energy barrier of height "U"("z"), the electron wave function is a traveling wave solution,
where
if "E" > "U"("z"), which is true for a wave function inside the tip or inside the sample. Inside a barrier, "E" < "U"("z") so the wave functions which satisfy this are decaying waves,
where
quantifies the decay of the wave inside the barrier, with the barrier in the +"z" direction for formula_6.
Knowing the wave function allows one to calculate the probability density for that electron to be found at some location. In the case of tunneling, the tip and sample wave functions overlap such that when under a bias, there is some finite probability to find the electron in the barrier region and even on the other side of the barrier. Let us assume the bias is "V" and the barrier width is "W". This probability, "P", that an electron at "z"=0 (left edge of barrier) can be found at "z"="W" (right edge of barrier) is proportional to the wave function squared,
If the bias is small, we can let "U" − "E" ≈ "φM" in the expression for "κ", where "φM", the work function, gives the minimum energy needed to bring an electron from an occupied level, the highest of which is at the Fermi level (for metals at "T"=0 kelvins), to vacuum level. When a small bias "V" is applied to the system, only electronic states very near the Fermi level, within "eV" (a product of electron charge and voltage, not to be confused here with electronvolt unit), are excited. These excited electrons can tunnel across the barrier. In other words, tunneling occurs mainly with electrons of energies near the Fermi level.
However, tunneling does require that there is an empty level of the same energy as the electron for the electron to tunnel into on the other side of the barrier. It is because of this restriction that the tunneling current can be related to the density of available or filled states in the sample. The current due to an applied voltage "V" (assume tunneling occurs sample to tip) depends on two factors: 1) the number of electrons between "E"f and "eV" in the sample, and 2) the number among them which have corresponding free states to tunnel into on the other side of the barrier at the tip. The higher the density of available states the greater the tunneling current. When "V" is positive, electrons in the tip tunnel into empty states in the sample; for a negative bias, electrons tunnel out of occupied states in the sample into the tip.
Mathematically, this tunneling current is given by
One can sum the probability over energies between "E"f − "eV" and "E"f to get the number of states available in this energy range per unit volume, thereby finding the local density of states (LDOS) near the Fermi level. The LDOS near some energy "E" in an interval "ε" is given by
and the tunnel current at a small bias V is proportional to the LDOS near the Fermi level, which gives important information about the sample. It is desirable to use LDOS to express the current because this value does not change as the volume changes, while probability density does. Thus the tunneling current is given by
where ρs(0,"E"f) is the LDOS near the Fermi level of the sample at the sample surface. This current can also be expressed in terms of the LDOS near the Fermi level of the sample at the tip surface,
The exponential term in the above equations means that small variations in W greatly influence the tunnel current. If the separation is decreased by 1 Ǻ, the current increases by an order of magnitude, and vice versa.
This approach fails to account for the "rate" at which electrons can pass the barrier. This rate should affect the tunnel current, so it can be treated using the Fermi's golden rule with the appropriate tunneling matrix element. John Bardeen solved this problem in his study of the metal-insulator-metal junction. He found that if he solved Schrödinger’s equation for each side of the junction separately to obtain the wave functions ψ and χ for each electrode, he could obtain the tunnel matrix, M, from the overlap of these two wave functions. This can be applied to STM by making the electrodes the tip and sample, assigning ψ and χ as sample and tip wave functions, respectively, and evaluating M at some surface S between the metal electrodes, where z=0 at the sample surface and z=W at the tip surface.
Now, Fermi’s Golden Rule gives the rate for electron transfer across the barrier, and is written
where δ(Eψ–Eχ) restricts tunneling to occur only between electron levels with the same energy. The tunnel matrix element, given by
is a description of the lower energy associated with the interaction of wave functions at the overlap, also called the resonance energy.
Summing over all the states gives the tunneling current as
where "f" is the Fermi function, ρs and ρT are the density of states in the sample and tip, respectively. The Fermi distribution function describes the filling of electron levels at a given temperature T.
Early invention.
An earlier, similar invention, the "Topografiner" of R. Young, J. Ward, and F. Scire from the NIST, relied on field emission. However, Young is credited by the Nobel Committee as the person who realized that it should be possible to achieve better resolution by using the tunnel effect.

</doc>
<doc id="27791" url="http://en.wikipedia.org/wiki?curid=27791" title="Sophie Germain">
Sophie Germain

Marie-Sophie Germain (]; 1 April 1776 – 27 June 1831) was a French mathematician, physicist, and philosopher. Despite initial opposition from her parents and difficulties presented by society, she gained education from books in her father's library including ones by Leonhard Euler and from correspondence with famous mathematicians such as Lagrange, Legendre, and Gauss. One of the pioneers of elasticity theory, she won the grand prize from the Paris Academy of Sciences for her essay on the subject. Her work on Fermat's Last Theorem provided a foundation for mathematicians exploring the subject for hundreds of years after. Because of prejudice against her sex, she was unable to make a career out of mathematics, but she worked independently throughout her life. In recognition of her contribution towards advancement of mathematics, an honorary degree was also conferred upon her by the University of Göttingen six years after her death. At the centenary of her life, a street and a girls' school were named after her. The Academy of Sciences established The Sophie Germain Prize in her honor.
Early life.
Family.
Marie-Sophie Germain was born on 1 April 1776, in Paris, France, in a house on Rue Saint-Denis. According to most sources, her father, Ambroise-Franҫois, was a wealthy silk merchant, though some believe he was a goldsmith. In 1789, he was elected as a representative of the bourgeoisie to the États-Généraux, which he saw change into the Constitutional Assembly. It is therefore assumed that Sophie witnessed many discussions between her father and his friends on politics and philosophy. Gray proposes that after his political career, Ambroise-Franҫois became the director of a bank; at least, the family remained well-off enough to support Germain throughout her adult life.
Marie-Sophie had one younger sister, named Angélique-Ambroise, and one older sister, named Marie-Madeline. Her mother was also named Marie-Madeline, and this plethora of "Maries" may have been the reason she went by Sophie. Germain's nephew Armand-Jacques Lherbette, Marie-Madeline's son, published some of Germain's work after she died (see Work in Philosophy).
Introduction to mathematics.
When Germain was 13, the Bastille fell, and the revolutionary atmosphere of the city forced her to stay inside. For entertainment she turned to her father's library. Here she found J. E. Montucla's "L'Histoire des Mathématiques", and his story of the death of Archimedes intrigued her.
Germain decided that if geometry, which at that time referred to all of pure mathematics, could hold such fascination for Archimedes, it was a subject worthy of study. So she pored over every book on mathematics in her father's library, even teaching herself Latin and Greek so she could read works like those of Sir Isaac Newton and Leonhard Euler. She also enjoyed "Traité d'Arithmétique" by Étienne Bézout and "Le Calcul Différentiel" by Jacques Antoine-Joseph Cousin. Later, Cousin visited her in her house, encouraging her in her studies.
Germain's parents did not at all approve of her sudden fascination with mathematics, which was then thought inappropriate for a woman. When night came, they would deny her warm clothes and a fire for her bedroom to try to keep her from studying, but after they left she would take out candles, wrap herself in quilts and do mathematics. As Lynn Osen describes, when her parents found Sophie "asleep at her desk in the morning, the ink frozen in the ink horn and her slate covered with calculations," they realized that their daughter was serious and relented. After some time, her mother even secretly supported her.
l'École Polytechnique.
In 1794, when Germain was 18, the École Polytechnique opened. As a woman, Germain was barred from attending, but the new system of education made the "lecture notes available to all who asked." The new method also required the students to "submit written observations." Germain obtained the lecture notes and began sending her work to Joseph Louis Lagrange, a faculty member. She used the name of a former student Monsieur Antoine-August Le Blanc, "fearing," as she later explained to Gauss, "the ridicule attached to a female scientist." When Lagrange saw the intelligence of M. LeBlanc, he requested a meeting, and thus Sophie was forced to disclose her true identity. Fortunately, Lagrange did not mind that Germain was a woman, and he became her mentor. He too visited her in her home, giving her moral support.
Early work in number theory.
Correspondence with Legendre.
Germain first became interested in number theory in 1798 when Adrien-Marie Legendre published "Essai sur la théorie des nombres". After studying the work, she opened correspondence with him on number theory, and later, elasticity. Legendre showed some of Germain's work in the "Supplément" to his second edition of the "Théorie des Nombres", where he calls it "très ingénieuse" ["very ingenious"] (See Her work on Fermat's Last Theorem).
Correspondence with Gauss.
Germain's interest in number theory was renewed when she read Carl Friedrich Gauss' monumental work "Disquisitiones Arithmeticae". After three years of working through the exercises and trying her own proofs for some of the theorems, she wrote, again under the pseudonym of M. LeBlanc, to the author himself, who was one year younger than she. The first letter, dated 21 November 1804, discussed Gauss' "Disquisitiones" and presented some of Germain's work on Fermat's Last Theorem. In the letter, Germain claimed to have proved the theorem for "n" = "p" – 1, where "p" is a prime number of the form "p" = 8"k" + 7. However, her proof contained a weak assumption, and Gauss' reply did not comment on Germain's proof.
Around 1807 (sources differ) the French were occupying the German town of Braunschweig, where Gauss lived. Germain, concerned that he might suffer the fate of Archimedes, wrote to General Pernety, a family friend, requesting that he ensure Gauss' safety. General Pernety sent a chief of a battalion to meet with Gauss personally to see that he was safe. As it turned out, Gauss was fine, but he was confused by the mention of Sophie's name.
Three months after the incident, Germain disclosed her true identity to Gauss. He replied,
How can I describe my astonishment and admiration on seeing my esteemed correspondent M leBlanc metamorphosed into this celebrated person. . . when a woman, because of her sex, our customs and prejudices, encounters infinitely more obstacles than men in familiarising herself with [number theory's] knotty problems, yet overcomes these fetters and penetrates that which is most hidden, she doubtless has the most noble courage, extraordinary talent, and superior genius.
Gauss' letters to Olbers show that his praise for Germain was sincere. In the same 1807 letter, Sophie claimed that if "x""n" + "y""n" is of the form "h"2 + "nf"2, then "x" + "y" is also of that form. Gauss replied with a counterexample: 1511 + 811 can be written as "h"2 + 11"f"2, but 15 + 8 cannot.
Although Gauss thought well of Germain, his replies to her letters were often delayed, and he generally did not review her work. Eventually his interests turned away from number theory, and in 1809 the letters ceased. Despite the friendship of Germain and Gauss, they never met.
Work in elasticity.
Germain's first attempt for the Academy Prize.
When Germain's correspondence with Gauss ceased, she took interest in a contest sponsored by the Paris Academy of Sciences concerning Ernst Chladni's experiments with vibrating metal plates. The object of the competition, as stated by the Academy, was "to give the mathematical theory of the vibration of an elastic surface and to compare the theory to experimental evidence." Lagrange's comment that a solution to the problem would require the invention of a new branch of analysis deterred all but two contestants, Denis Poisson and Germain. Then Poisson was elected to the Academy, thus becoming a judge instead of a contestant, and leaving Germain as the only entrant to the competition.
In 1809 Germain began work. Legendre assisted by giving her equations, references, and current research. She submitted her paper early in the fall of 1811, and did not win the prize. The judging commission felt that "the true equations of the movement were not established," even though "the experiments presented ingenious results." Lagrange was able to use Germain's work to derive an equation that was "correct under special assumptions."
Subsequent attempts for the Prize.
The contest was extended by two years, and Germain decided to try again for the prize. At first Legendre continued to offer support, but then he refused all help. Germain's anonymous 1813 submission was still littered with mathematical errors, especially involving double integrals, and it received only an honorable mention because "the fundamental base of the theory [of elastic surfaces] was not established." The contest was extended once more, and Germain began work on her third attempt. This time she consulted with Poisson. In 1814 he published his own work on elasticity, and did not acknowledge Germain's help (although he had worked with her on the subject and, as a judge on the Academy commission, had had access to her work).
Germain submitted her third paper, "Recherches sur la théorie des surfaces élastiques" under her own name, and on 8 January 1816 she became the first woman to win a prize from the Paris Academy of Sciences. She did not appear at the ceremony to receive her award. Although Germain had at last been awarded the "prix extraordinaire", the Academy was still not fully satisfied. Sophie had derived the correct differential equation, but her method did not predict experimental results with great accuracy, as she had relied on an incorrect equation from Euler, which led to incorrect boundary conditions. Here is Germain's final equation:
where "N"2 is a constant.
After winning the Academy contest, she was still not able to attend its sessions because of the Academy's tradition of excluding women other than the wives of members. Seven years later this tradition was broken when she made friends with Joseph Fourier, a secretary of the Academy, who obtained tickets to the sessions for her.
Later work in elasticity.
Germain published her prize-winning essay at her own expense in 1821, mostly because she wanted to present her work in opposition to that of Poisson. In the essay she pointed out some of the errors in her method.
In 1826 she submitted a revised version of her 1821 essay to the Academy. According to Andrea Del Centina, the revision included attempts to clarify her work by "introducing certain simplifying hypotheses." This put the Academy in an awkward position, as they felt the paper to be "inadequate and trivial," but they did not want to "treat her as a professional colleague, as they would any man, by simply rejecting the work." So Augustin-Louis Cauchy, who had been appointed to review her work, recommended she publish it, and she followed his advice.
One further work of Germain's on elasticity was published posthumously in 1831: her "Mémoire sur la courbure des surfaces." She used the mean curvature in her research (see Honors in Number Theory).
Later work in number theory.
Renewed interest.
Germain's best work was in number theory, and her most significant contribution to number theory dealt with Fermat's Last Theorem. In 1815, after the elasticity contest, the Academy offered a prize for a proof of Fermat's Last Theorem. It reawakened Germain's interest in number theory, and she wrote to Gauss again after ten years of no correspondence.
In the letter, Germain said that number theory was her preferred field, and that it was in her mind all the time she was studying elasticity. She outlined a strategy for a general proof of Fermat's Last Theorem, including a proof for a special case. Germain's letter to Gauss contained her substantial progress toward a proof. She asked Gauss if her approach to the theorem was worth pursuing. Gauss never answered.
Her work on Fermat's Last Theorem.
Fermat's Last Theorem can be divided into two cases. Case 1 involves all "p" that do not divide any of "x", "y", or "z". Case 2 includes all "p" that divide at least one of "x", "y", or "z". Germain proposed the following, commonly called "Sophie Germain's Theorem":
Let "p" be an odd prime. If there exists an auxiliary prime "P" = 2"Np" + 1 (N any positive integer not divisible by 3) such that:
Then the first case of Fermat's Last Theorem holds true for "p".
Germain used this result to prove the first case of Fermat's Last Theorem for all odd primes "p"<100, but according to Andrea Del Centina, "she had actually shown that it holds for every exponent "p"<197." L. E. Dickson later used Germain's theorem to prove Fermat's Last Theorem for odd primes less than 1700.
In an unpublished manuscript entitled "Remarque sur l'impossibilité de satisfaire en nombres entiers a l'équation xp + yp = zp", Germain showed that any counterexamples to Fermat's theorem for "p">5 must be numbers "whose size frightens the imagination," around 40 digits long. Sophie did not publish this work. Her brilliant theorem is known only because of the footnote in Legendre's treatise on number theory, where he used it to prove Fermat's Last Theorem for "p" = 5 (see Correspondence with Legendre). Germain also proved or nearly proved several results that were attributed to Lagrange or were rediscovered years later. Del Centina states that "after almost two hundred years her ideas were still central", but ultimately her method did not work.
Work in philosophy.
In addition to mathematics, Germain studied philosophy and psychology. She wanted to classify facts and generalize them into laws that could form a system of psychology and sociology, which were then just coming into existence. Her philosophy was highly praised by Auguste Comte.
Two of her philosophical works, "Pensées diverses" and "Considérations générales sur l'état des sciences et des lettres, aux différentes époques de leur culture", were published, both posthumously. This was due in part to the efforts of Lherbette, her nephew, who collected her philosophical writings and published them. "Pensées" is a history of science and mathematics with Sophie's commentary. In "Considérations", the work admired by Comte, Sophie argues that there are no differences between the sciences and the humanities.
Final years.
In 1829 Germain learned she had breast cancer. Despite the pain, she continued to work. In 1831 "Crelle's Journal" published her paper on the curvature of elastic surfaces and "a note about finding "y" and "z" in formula_2." Mary Gray records, "She also published in "Annales de chimie et de physique" an examination of principles which led to the discovery of the laws of equilibrium and movement of elastic solids." On June 27 of 1831, she died in the house at 13 rue de Savoie.
Despite Germain's intellectual achievements, her death certificate lists her as a "rentière – annuitant" (property holder, not a "mathematicienne." But her work was not unappreciated by everyone. When the matter of honorary degrees came up at the University of Göttingen six years after Germain's death, Gauss lamented, "she [Germain] proved to the world that even a woman can accomplish something worthwhile in the most rigorous and abstract of the sciences and for that reason would well have deserved an honorary degree."
Honors.
Memorials.
Germain's resting place in the Père Lachaise Cemetery in Paris is marked by a crumbling gravestone. At the centennial celebration of her life, a street and a girls' school were named after her, and a plaque was placed at the house where she died. The school houses a bust commissioned by the Paris City Council.
Honors in number theory.
E. Dubouis defined a "sophien" of a prime "n" to be a prime θ where θ = "kn" + 1, for such "n" that yield θ such that "x""n" = "y""n" + 1 (mod θ) has no solutions when "x" and "y" are prime to "n".
A Sophie Germain prime is a prime "p" such that 2"p" + 1 is also prime.
The "Germain curvature" (also called mean curvature) is formula_3, when "k"1 and "k"2 are the maximum and minimum values of the normal curvature.
"Sophie Germain's Identity" states that for any {"x,y"}, then,
Criticisms.
Contemporary praise and criticisms.
Vesna Petrovich found that the educated world's response to the publication in 1821 of Germain's prize-winning essay "ranged from polite to indifferent". Yet, some critics had high praise for it. Of her essay in 1821, Cauchy said, "[it] was a work for which the name of its author and the importance of the subject both deserved the attention of mathematicians." Germain was also included in H. J. Mozans' book "Woman in Science", although Marilyn Bailey Ogilvie claims that the biography "is inaccurate and the notes and bibliography are unreliable". Nevertheless, it quotes the mathematician Claude-Louis Navier as saying, "it is a work which few men are able to read and which only one woman was able to write."
Germain's contemporaries also had good things to say relating to her work in mathematics. Osen relates that "Baron de Prony called her the Hypatia of the nineteenth century," and "J.J Biot wrote, in the "Journal de Savants", that she had probably penetrated the science of mathematics more deeply than any other of her sex." Gauss certainly thought highly of her, and he recognized that European culture presented special difficulties to a woman in mathematics (see Correspondence with Gauss).
Modern praise and criticisms.
The modern view generally acknowledges that although Germain had great talent as a mathematician, her haphazard education had left her without the strong base she needed to truly excel. As explained by Gray, "Germain's work in elasticity suffered generally from an absence of rigor, which might be attributed to her lack of formal training in the rudiments of analysis." Petrovich adds, "This proved to be a major handicap when she could no longer be regarded as a young prodigy to be admired but was judged by her peer mathematicians."
Not withstanding the problems with Germain's theory of vibrations, Gray states that "Germain's work was fundamental in the development of a general theory of elasticity." Mozans writes, however, that when the Eiffel tower was built and the architects inscribed the names of 72 great French scientists, Germain's name was not among them: despite the salience of her work to the tower's construction. Mozans asked, "Was she excluded from this list... because she was a woman? It would seem so."
Concerning her early work in number theory, J. H. Sampson states, "She was clever with formal algebraic manipulations; but there is little evidence that she really understood the "Disquisitiones", and her work of that period that has come down to us seems to touch only on rather superficial matters." Gray adds that "The inclination of sympathetic mathematicians to praise her work rather than to provide substantive criticism from which she might learn was crippling to her mathematical development." Yet Marilyn Bailey Ogilvie recognizes that "Sophie Germain's creativity manifested itself in pure and applied mathematics...[she] provided imaginative and provocative solutions to several important problems," and, as Petrovich proposes, it may have been her very lack of training that gave her unique insights and approaches. Louis Bucciarelli and Nancy Dworsky, Germain's biographers, summarize as follows: "All the evidence argues that Sophie Germain had a mathematical brilliance that never reached fruition due to a lack of rigorous training available only to men."
Germain in popular culture.
Germain was referenced and quoted in David Auburn's 2001 play "Proof." The protagonist is a young struggling female mathematician, Catherine, who found great inspiration in the work of Germain. Germain was also mentioned in John Madden's film adaptation of the same play in a conversation between Catherine (Gwyneth Paltrow) and Hal (Jake Gyllenhaal).
In the fictional work "The Last Theorem" by Arthur C. Clarke and Frederik Pohl, Sophie Germain was credited with inspiring Ranjit Subramanian to solve Fermat's Last Theorem.
Sophie Germain Prize.
The Sophie Germain Prize (Prix Sophie Germain), awarded annually by the Foundation Sophie Germain is conferred by the Academy of Sciences in Paris. Its purpose is to honour a French mathematician for research in the foundations of mathematics. This award, in the amount of €8,000 was established in 2003, under the auspices of the Institut de France. Previous winners have included:
External links.
 

</doc>
<doc id="27840" url="http://en.wikipedia.org/wiki?curid=27840" title="Senryū">
Senryū

Senryū (川柳, literally 'river willow') is a Japanese form of short poetry similar to haiku in construction: three lines with 17 or fewer total morae (or "on", often translated as syllables, but see the article on onji for distinctions). Senryū tend to be about human foibles while haiku tend to be about nature, and senryū are often cynical or darkly humorous while haiku are more serious. Unlike haiku, senryū do not include a kireji (cutting word), and do not generally include a "kigo", or season word.
Form and content.
Senryū is named after Edo period haikai poet Senryū Karai (柄井川柳, 1718-1790), whose collection "Haifūyanagidaru" (誹風柳多留) launched the genre into the public consciousness. A typical example from the collection:
This senryū, which can also be translated "Catching him / you see the robber / is your son," is not so much a personal experience of the author as an example of a type of situation (provided by a short comment called a maeku or fore-verse, which usually prefaces a number of examples) and/or a brief or witty rendition of an incident from history or the arts (plays, songs, tales, poetry, etc.). In this case, there was a historical incident of legendary proportion.
Some senryū skirt the line between haiku and senryū. The following senryū by Shūji Terayama copies the haiku structure faithfully, down to a blatantly obvious kigo, but on closer inspection is absurd in its content:
Terayama, who wrote about playing hide-and-seek in the graveyard as a child, thought of himself as the odd one out, the one who was always "it" in hide-and-seek. Indeed, the original haiku included the theme "oni" (the "it" in Japanese is a demon, though in some parts a very young child forced to play "it" was called a "sea slug" (namako)). To him, seeing a game of hide-and-seek, or recalling it as it grew cold would be a chilling experience. Terayama might also have recalled opening his eyes and finding himself all alone, feeling the cold more intensely than he did a minute before among other children. Either way, any genuinely personal experience would be haiku and not senryū in the classic sense. If you think Terayama's poem uses a child's game to express in hyperbolic metaphor how, in retrospect, life is short, and nothing more, then this would indeed work as a senryū. Otherwise, it is a bona-fide haiku. There is also the possibility that it is a joke about playing hide and seek, only to realize (winter having arrived during the months spent hiding) that no one wants to find you.
English-language senryū publications.
In the 1970s, Michael McClintock edited "Seer Ox: American Senryu Magazine". In 1993, Michael Dylan Welch edited and published "Fig Newtons: Senryū to Go", the first anthology of English-language senryū.
Additionally, one can regularly find senryū and related articles in some haiku publications. For example:
Senryū regularly appear in the pages of "Modern Haiku", "Frogpond", "Tundra", and other haiku journals, often unsegregated from haiku.
Senryū awards.
The Haiku Society of America holds the annual Gerald Brady Memorial Award for best unpublished senryū.
Since about 1990, the Haiku Poets of Northern California has been running a senryū contest, as part of its San Francisco International Haiku and Senryu Contest.

</doc>
<doc id="27850" url="http://en.wikipedia.org/wiki?curid=27850" title="Saxons">
Saxons

The Saxons (Latin: "Saxones", Old English: "Seaxe", Old Saxon: "Sahson", Low German: "Sassen", German: "Sachsen", Dutch: "Saksen") were a confederation of Germanic tribes on the North German Plain. They settled in large parts of Great Britain in the early Middle Ages and formed part of the merged group of Anglo-Saxons who eventually organized the first united Kingdom of England. Many Saxons however remained in Germany, where they resisted the expanding Frankish Empire through the leadership of the semi-legendary Saxon hero, Widukind.
The Saxons' earliest area of settlement is believed to have been Northern Albingia, an area approximately that of modern Holstein. This general area also included the probable homeland of the Angles. Saxons, along with the Angles and other continental Germanic tribes, participated in the Anglo-Saxon settlement of Britain during and after the 5th century. The British-Celtic inhabitants of the isles tended to refer to all these groups collectively as Saxons. It is unknown how many Saxons migrated from the continent to Britain, though estimates for the total number of Anglo-Saxon settlers are around 200,000. During the Middle Ages, because of international Hanseatic trading routes and contingent migration, Saxons mixed with and had strong influences upon the languages and cultures of the North Germanic, Baltic peoples, Finnic peoples, Polabian Slavs and Pomeranian West Slavic people.
Etymology.
The Saxons may have derived their name from "seax", a kind of knife for which they were known. The seax has a lasting symbolic impact in the English counties of Essex and Middlesex, both of which feature three seaxes in their ceremonial emblem. Their names, along with those of Sussex and Wessex, contain a remnant of the word "Saxon".
The Elizabethan-era play, "Edmund Ironside," suggests the Saxon name as deriving from the Latin "saxa" (stone).
<poem>
"Their names discover what their natures are,
more hard than stones, and yet not stones indeed."
Saxon as a demonym.
Celtic languages.
In the Celtic languages, the word for the English nationality is derived from the Latin "Saxones". The most prominent example, a loan word in English, is the Scottish Gaelic Sassenach ("Saxon"), often used disparagingly in Scottish English/Scots. It derives from the Scottish Gaelic "Sasunnach" meaning, originally, "Saxon", from the Latin "Saxones". As employed by Scots or Scottish English-speakers today, it is usually used in jest, as a (friendly) term of abuse. The "Oxford English Dictionary" (OED) gives 1771 as the date of the earliest written use of the word in English.
"Sasanach", the Irish word for an Englishman, has the same derivation, as do the words used in Welsh to describe the English people ("Saeson", sing. "Sais") and the language and things English in general: "Saesneg" and "Seisnig".
Cornish terms the English as "Sawsnek," from the same derivation. In the 16th century, the phrase, "Meea navidna cowza sawzneck"!, to feign ignorance of the English language, was used in Cornish.
"England", in Scottish Gaelic, is "Sasainn" (Saxony). Other examples are the Welsh "Saesneg" (the English language), Irish "Sasana" (England), Breton "saoz(on)" (English, "saozneg" "the English language", Bro-saoz "England"), and Cornish "Sowson" (English people) and "Sowsnek" (English language), "Pow Sows" for 'Land [Pays] of Saxons'.
Romance languages.
The label "Saxons" (in Romanian "Saşi") was also applied to German settlers who migrated during the 13th century to southeastern Transylvania. From Transylvania, some Saxons migrated to the neighbouring Moldavia, as the name of the town, Sas-cut, shows. Sascut is located in the part of Moldavia that is today part of Romania.
During Georg Friederich Händel's visit to Italy, much was made of his being from Saxony; in particular, the Venetians greeted the 1709 performance of his opera "Agrippina" with the cry "Viva il caro Sassone", "Cheers for the beloved Saxon!"
Non-Indo-European languages.
The Finns and Estonians have changed their usage of the term "Saxony" over the centuries to denote now the whole country of Germany ("Saksa" and "Saksamaa" respectively) and the Germans ("saksalaiset" and "sakslased", respectively). The Finnish word "sakset" scissors reflects the name of the old Saxon single-edged sword Seax from which 'Saxon' is supposedly derived. In Estonian, "saks" means a nobleman or, colloquially, a wealthy or powerful person. As a result of the Northern Crusades in the Middle Ages, Estonia's upper class had been mostly of German origin until well into the 20th century.
Related surnames.
The word also survives as the surnames of Saß/Sass, Sachse and Sachs. The Dutch female first name, "Saskia," originally meant "A Saxon woman" (alteration of "Saxia").
Saxony as a toponym.
Following the downfall of Henry the Lion (1129-1195, Duke of Saxony 1142-1180), and the subsequent splitting of the Saxon tribal duchy into several territories, the name of the Saxon duchy was transferred to the lands of the Ascanian family. This led to the differentiation between "Lower Saxony", lands settled by the Saxon tribe, and "Upper Saxony", the lands belonging to the House of Wettin. When Upper Saxony dropped the "Upper" designation, a different region became known as "Saxony", ultimately usurping the name's original meaning. The area formerly known as Upper Saxony now lies in Central Germany.
History.
Early history.
In prehistoric times, the territory of Saxony was the site of some of the largest of the ancient Central European monumental temples, dating from thousands of years ago. Notable archaeological sites of this period have been discovered in Dresden and the villages of Eythra and Zwenkau near Leipzig. The Slavic and Germanic occupation of the territory of today's Saxony is thought to have begun in the 1st century BC. Ptolemy's "Geographia," written in the 2nd century, is sometimes considered to contain the first mentioning of the Saxons. Some copies of this text mention a tribe called "Saxones" in the area to the north of the lower River Elbe. However, other versions refer to the same tribe as "Axones." This may be a misspelling of the tribe that Tacitus in his "Germania" called "Aviones". According to this theory, "Saxones" was the result of later scribes trying to correct a name that meant nothing to them. On the other hand, Schütte, in his analysis of such problems in "Ptolemy's Maps of Northern Europe", believed that "Saxones" is correct. He notes that the loss of first letters occurs in numerous places in various copies of Ptolemy's work, and also that the manuscripts without "Saxones" are generally inferior overall.
Schütte also remarks that there was a medieval tradition of calling this area "Old Saxony". In contrast, other scholars note that sources such as Bede who mention Old Saxony, might be interpreted as saying it was near the Rhine, somewhere to the north of the river Lippe, and were in any case not personally familiar with the area.
In AD 441–442, Saxons are mentioned for the first time as inhabitants of Britain, when an unknown Gaulish historian wrote: "The British provinces...have been reduced to Saxon rule".
The first undisputed mention of the Saxon name in its modern form is from AD 356, when Julian, later the Roman Emperor, mentioned them in a speech as allies of Magnentius, a rival emperor in Gaul. Zosimus also mentions a specific tribe of Saxons, called the "Kouadoi", which have been interpreted as the Chauci. They entered the Rhineland and displaced the recently settled Salian Franks from Batavi, whereupon some of the Salians began to move into the Belgian territory of Toxandria, supported by Julian.
In order to defend against Saxon raiders, the Romans created a military district called the "Litus Saxonicum" ("Saxon Coast") on both sides of the English Channel.
Saxons as inhabitants of present-day Northern Germany are first mentioned in 555, when the Frankish king Theudebald died, and the Saxons used the opportunity for an uprising. The uprising was suppressed by Chlothar I, Theudebald's successor. Some of their Frankish successors fought against the Saxons, others were allied with them; Chlothar II won a decisive victory against the Saxons. The Thuringians frequently appeared as allies of the Saxons.
Continental Saxons.
Saxony.
The Continental Saxons living in what was known as "Old Saxony" appear to have become consolidated by the end of the 8th century. After subjugation by the Emperor Charlemagne, a political entity called the Duchy of Saxony appeared.
The Saxons long resisted becoming Christians and being incorporated into the orbit of the Frankish kingdom. In 776 the Saxons promised to convert to Christianity and vow loyalty to the king, but once Charlemagne went to Spain, they staged further attacks in 778. This was an often repeated pattern when Charlemagne was distracted by other matters. During Charlemagne's campaign in Hispania (778), the Saxons advanced to Deutz on the Rhine and plundered along the river.
They were decisively conquered by Charlemagne in a long series of annual campaigns, the Saxon Wars (772 – 804) With defeat came enforced baptism and conversion as well as the union of the Saxons with the rest of the Germanic, Frankish empire. Their sacred tree or pillar, a symbol of Irminsul, was destroyed. Charlemagne also deported 10,000 of them to Neustria and gave their now vacant lands to the loyal king of the Abotrites. Einhard, Charlemagne's biographer, says on the closing of this grand conflict:
The war that had lasted so many years was at length ended by their acceding to the terms offered by the King; which were renunciation of their national religious customs and the worship of devils, acceptance of the sacraments of the Christian faith and religion, and union with the Franks to form one people.
Under Carolingian rule, the Saxons were reduced to tributary status. There is evidence that the Saxons, as well as Slavic tributaries such as the Abodrites and the Wends, often provided troops to their Carolingian overlords. The dukes of Saxony became kings (Henry I, the Fowler, 919) and later the first emperors (Henry's son, Otto I, the Great) of Germany during the 10th century, but they lost this position in 1024. The duchy was divided up in 1180 when Duke Henry the Lion, Emperor Otto's grandson, refused to follow his cousin, Emperor Frederick Barbarossa, into war in Lombardy.
During the High Middle Ages, under the Salian emperors and, later, under the Teutonic Knights, German settlers moved east of the River Saale into the area of a western Slavic tribe, the Sorbs. The Sorbs were gradually Germanised. This region subsequently acquired the name Saxony through political circumstances, though it was initially called the March of Meissen. The rulers of Meissen acquired control of the Duchy of Saxony in 1423; they eventually applied the name "Saxony" to the whole of their kingdom. Since then, this part of eastern Germany has been referred to as Saxony (German: "Sachsen"), a source of some misunderstanding about the original homeland of the Saxons, with a central part in the present-day German state of Lower Saxony (German: "Niedersachsen").
Netherlands.
In the Netherlands, Saxons occupied the territory south of the Frisians and north of the Franks. In the west it reached as far as the Gooi region, in the south as far as the Lower Rhine. After the conquest of Charlemagne, this area formed the main part of the Bishopric of Utrecht. The Saxon duchy of Hamaland played an important role in the formation of the duchy of Guelders.
The local language, although strongly influenced by standard Dutch, is still officially recognised as Dutch Low Saxon.
Italy and Provence.
In 569, some Saxons accompanied the Lombards into Italy under the leadership of Alboin and settled there. In 572, they raided southeastern Gaul as far as "Stablo", now Estoublon. Divided, they were easily defeated by the Gallo-Roman general Mummolus. When the Saxons regrouped, a peace treaty was negotiated whereby the Italian Saxons were allowed to settle with their families in Austrasia. Gathering their families and belongings in Italy, they returned to Provence in two groups in 573. One group proceeded by way of Nice and another via Embrun, joining up at Avignon. They plundered the territory and were as a consequence stopped from crossing the Rhone by Mummolus. They were forced to pay compensation for what they had robbed before they could enter Austrasia. These people are known only by documents, and their settlement cannot be compared to the archeological artifacts and remains that attest to Saxon settlements in northern and western Gaul.
Gaul.
A Saxon king named Eadwacer conquered Angers in 463 only to be dislodged by Childeric I and the Salian Franks, allies of the Roman Empire. It is possible that Saxon settlement of Great Britain began only in response to expanding Frankish control of the Channel coast.
Some Saxons already lived along the Saxon shore of Gaul. They can be traced in documents, but also in archeology and in toponymy. The "Notitia Dignitatum" mentions the "Tribunus cohortis primae novae Armoricanae, Grannona in litore Saxonico". The location of "Grannona" is uncertain and was identified by the historians and toponymists at different places, mainly with the town known today as Granville (in Normandy) or nearby. The "Notitia Dignitatum" does not explain where these "Roman" soldiers came from. Some toponymists have proposed Graignes ("Grania" 1109 - 1113) as the location for "Grannona"/"Grannonum". It could be the same element "*gran", that is recognised in Guernsey ("Greneroi" 11th century). This location is closer to Bayeux, where Gregory of Tours evokes otherwise the "Saxones Bajocassini" (Bessin Saxons), which were ineffective to defeat the Breton Waroch II in 579.
A Saxon unit of "laeti" settled at Bayeux—the "Saxones Baiocassenses". These Saxons became subjects of Clovis I late in the 5th century. The Saxons of Bayeux comprised a standing army and were often called upon to serve alongside the local levy of their region in Merovingian military campaigns. They were ineffective against the Breton Waroch in this capacity in 579. In 589, the Saxons wore their hair in the Breton fashion at the orders of Fredegund and fought with them as allies against Guntram. Beginning in 626, the Saxons of the Bessin were used by Dagobert I for his campaigns against the Basques. One of their own, Aeghyna, was created a "dux" over the region of Vasconia.
In 843 and 846 under king Charles the Bald, other official documents mention a "pagus" called "Otlinga Saxonia" in the Bessin region, but the meaning of "Otlinga" is unclear. Different Bessin toponyms were identified as typically Saxon, ex : Cottun ("Coltun" 1035 - 1037 ; "Cola" 's "town"). It is the only place-name in Normandy that can be interpreted as a "-tun" one (English "-ton"; cf. Colton). In contrast to this one example in Normandy are numerous "-thun" villages in the north of France, in Boulonnais, ex : Alincthun, Verlincthun, Pelingthun, etc. showing with other toponyms, an important Saxon or Anglo-Saxon settlement. comparing the concentration of "-ham" / "-hem" (Anglo-Saxon "hām" > home) toponyms in the Bessin and in the Boulonnais gives more examples of Saxon settlement. In the area known today as Normandy, the "-ham" cases of Bessin are unique, they do not exist elsewhere. Other cases were considered, but there is no determining example, f.e. : Canehan ("Kenehan" 1030 / "Canaan" 1030 - 1035) could be the biblical name "Canaan" or Airan ("Heidram" 9th century), the Germanic masculine name "Hairammus".
The Bessin examples are clear. f. e. Ouistreham ("Oistreham" 1086), Étréham ("Oesterham" 1350 ?), Huppain ("*Hubbehain" ; "Hubba" 's "home"), Surrain ("Surrehain" 11th century), etc. Another significant example can be found in the Norman onomastics: the widespread surname Lecesne, with variant spellings : Le Cesne, Lesène, Lecène and Cesne. It comes from Gallo-Romance "the Saxon" > "saisne" in Old French. These examples are not derived from more recent Anglo-Scandinavian toponyms, because in that case they would have been numerous in the Norman regions (pays de Caux, Basse-Seine, North-Cotentin) settled by the Nordic peoples. That is not the case, nor does Bessin belong to the "pagii," which were affected by an important wave of Anglo-Scandinavian immigration.
In addition, archeological finds add evidence to the documents and the results of toponymic research. All around the city of Caen and in the Bessin (Vierville-sur-Mer, Bénouville, Giverville, Hérouvillette), excavations have shown numerous Anglo-Saxon jewelry, design elements, settings and weapons. All these things were discovered in cemeteries in a context of the 5th, 6th and 7th centuries AD.
The oldest and most spectacular Saxon site found in France to date is Vron, in Picardy. There, archeologists excavated a large cemetery with tombs dating from the Roman Empire until the 6th century. Furniture and other gravegoods, as well as the human remains, revealed a group of people buried in the 4th and 5th centuries AD. Physically different from the usual local inhabitants found before this period, they instead resembled the Germanic populations of the North. At the beginning (4th century) 92% were buried, sometimes with typical Germanic weapons. Then, they were ranked to the east, when they were buried in the 5th and later to the beginning of the 6th century. A strong Anglo-Saxon influence became obvious in the middle of the period, but it disappeared later. Archeological material, neighbouring toponymy, and texts support the same conclusions: settlement of Saxon foederati with their families. Further anthropological research by Joël Blondiaux shows these people were from Low Saxony.
Saxons in Britain.
Saxons, along with Angles, Frisians and Jutes, invaded or migrated to the island of Great Britain (Britannia) around the time of the collapse of Roman authority in the west. Saxon raiders had been harassing the eastern and southern shores of Britannia for centuries before, prompting the construction of a string of coastal forts called the "Litora Saxonica" or Saxon Shore. Before the end of Roman rule in Britannia, many Saxons and other folk had been permitted to settle in these areas as farmers.
According to tradition, the Saxons (and other tribes) first entered Britain en masse as part of a deal to protect the Britons from the incursions of the Picts, Gaels and others. The story, as reported in such sources as the "Historia Brittonum" and Gildas, indicates that the British king Vortigern allowed the Germanic warlords, later named as Hengist and Horsa by Bede, to settle their people on the Isle of Thanet in exchange for their service as mercenaries. According to Bede, Hengist manipulated Vortigern into granting more land and allowing for more settlers to come in, paving the way for the Germanic settlement of Britain.
Historians are divided about what followed: some argue that the takeover of southern Great Britain by the Anglo-Saxons was peaceful. The known account from a native Briton who lived in the mid-5th century AD, Gildas, described events as a forced takeover by armed attack:
For the fire...spread from sea to sea, fed by the hands of our foes in the east, and did not cease, until, destroying the neighbouring towns and lands, it reached the other side of the island, and dipped its red and savage tongue in the western ocean. In these assaults...all the columns were levelled with the ground by the frequent strokes of the battering-ram, all the husbandmen routed, together with their bishops, priests, and people, whilst the sword gleamed, and the flames crackled around them on every side. Lamentable to behold, in the midst of the streets lay the tops of lofty towers, tumbled to the ground, stones of high walls, holy altars, fragments of human bodies, covered with livid clots of coagulated blood, looking as if they had been squeezed together in a press; and with no chance of being buried, save in the ruins of the houses, or in the ravening bellies of wild beasts and birds; with reverence be it spoken for their blessed souls, if, indeed, there were many found who were carried, at that time, into the high heaven by the holy angels... Some, therefore, of the miserable remnant, being taken in the mountains, were murdered in great numbers; others, constrained by famine, came and yielded themselves to be slaves for ever to their foes, running the risk of being instantly slain, which truly was the greatest favour that could be offered them: some others passed beyond the seas with loud lamentations instead of the voice of exhortation...Others, committing the safeguard of their lives, which were in continual jeopardy, to the mountains, precipices, thickly wooded forests, and to the rocks of the seas (albeit with trembling hearts), remained still in their country.
Gildas described how the Saxons were later slaughtered at the battle of Mons Badonicus 44 years before he wrote his history, and Britain reverted to Romano-British rule. The 8th-century English historian Bede disagreed with Gildas, stating that the Saxon invasions continued after the battle of Mons Badonicus, including also Jutish and Anglic expeditions. He said these resulted in a swift overrunning of the entirety of South-Eastern Britain, and the foundation of the Anglo-Saxon kingdoms.
Four separate Saxon realms emerged:
During the period of the reigns from Egbert to Alfred the Great, the kings of Wessex emerged as Bretwalda, unifying the country. They eventually organized it as the kingdom of England in the face of Viking invasions.
Culture.
Social structure.
Bede, a Northumbrian, writing around the year 730, remarks that "the old (that is, the continental) Saxons have no king, but they are governed by several ealdormen (or "satrapa") who, during war, cast lots for leadership but who, in time of peace, are equal in power." The "regnum Saxonum" was divided into three provinces – Westphalia, Eastphalia and Angria – which comprised about one hundred "pagi" or "Gaue". Each "Gau" had its own satrap with enough military power to level whole villages that opposed him.
In the mid-9th century, Nithard first described the social structure of the Saxons beneath their leaders. The caste structure was rigid; in the Saxon language the three castes, excluding slaves, were called the "edhilingui" (related to the term aetheling), "frilingi", and "lazzi". These terms were subsequently Latinised as "nobiles" or "nobiliores"; "ingenui", "ingenuiles", or "liberi"; and "liberti", "liti", or "serviles". According to very early traditions that are presumed to contain a good deal of historical truth, the "edhilingui" were the descendants of the Saxons who led the tribe out of Holstein and during the migrations of the 6th century. They were a conquering warrior elite. The "frilingi" represented the descendants of the "amicii", "auxiliarii", and "manumissi" of that caste. The "lazzi" represented the descendants of the original inhabitants of the conquered territories, who were forced to make oaths of submission and pay tribute to the "edhilingui".
The "Lex Saxonum" regulated the Saxons' unusual society. Intermarriage between the castes was forbidden by the "Lex," and wergilds were set based upon caste membership. The "edhilingui" were worth 1,440 solidi, or about 700 head of cattle, the highest wergild on the continent; the price of a bride was also very high. This was six times as much as that of the "frilingi" and eight times as much as the "lazzi". The gulf between noble and ignoble was very large, but the difference between a freeman and an indentured labourer was small.
According to the "Vita Lebuini antiqua", an important source for early Saxon history, the Saxons held an annual council at Marklo where they "confirmed their laws, gave judgment on outstanding cases, and determined by common counsel whether they would go to war or be in peace that year." All three castes participated in the general council; twelve representatives from each caste were sent from each "Gau". In 782, Charlemagne abolished the system of "Gaue" and replaced it with the "Grafschaftsverfassung", the system of counties typical of Francia. By prohibiting the Marklo councils, Charlemagne pushed the "frilingi" and "lazzi" out of political power. The old Saxon system of "Abgabengrundherrschaft", lordship based on dues and taxes, was replaced by a form of feudalism based on service and labour, personal relationships, and oaths.
Religion.
Paganism.
Saxon religious practices were closely related to their political practices. The annual councils of the entire tribe began with invocations of the gods. The procedure by which dukes were elected in wartime, by drawing lots, is presumed to have had religious significance, i. e. in giving trust to divine providence – it seems – to guide the random decision making. There were also sacred rituals and objects, such as the pillars called Irminsul; these were believed to connect heaven and earth, as with other examples of trees or ladders to heaven in numerous religions. Charlemagne had one such pillar chopped down in 772 close to the Eresburg stronghold.
Early Saxon religious practices in Britain can be gleaned from place names and the Germanic calendar in use at that time. The Germanic gods Woden, Frigg, Tiw, and Thunor, who are attested to in every Germanic tradition, were worshipped in Wessex, Sussex, and Essex. They are the only ones directly attested to, though the names of the third and fourth months (March and April) of the Old English calendar bear the names "Hrethmonath" and "Eosturmonath", meaning "month of Hretha" and "month of Ēostre." It is presumed that these are the names of two goddesses who were worshipped around that season. The Saxons offered cakes to their gods in February ("Solmonath"). There was a religious festival associated with the harvest, "Halegmonath" ("holy month" or "month of offerings", September). The Saxon calendar began on 25 December, and the months of December and January were called Yule (or "Giuli"). They contained a "Modra niht" or "night of the mothers", another religious festival of unknown content.
The Saxon freemen and servile class remained faithful to their original beliefs long after their nominal conversion to Christianity. Nursing a hatred of the upper class, which, with Frankish assistance, had marginalised them from political power, the lower classes (the "plebeium vulgus" or "cives") were a problem for Christian authorities as late as 836. The "Translatio S. Liborii" remarks on their obstinacy in pagan "ritus et superstitio" (usage and superstition).
Conversion and resistance.
The conversion of the Saxons in England from their original Germanic religion to Christianity occurred in the early to late 7th century under the influence of the already converted Jutes of Kent. In the 630s, Birinus became the "apostle to the West Saxons" and converted Wessex, whose first Christian king was Cynegils. The West Saxons begin to emerge from obscurity only with their conversion to Christianity and keeping written records. The Gewisse, a West Saxon people, were especially resistant to Christianity; Birinus exercised more efforts against them and ultimately succeeded in conversion. In Wessex, a bishopric was founded at Dorchester. The South Saxons were first evangelised extensively under Anglian influence; Aethelwalh of Sussex was converted by Wulfhere, King of Mercia, and allowed Wilfrid, Archbishop of York, to evangelise his people beginning in 681. The chief South Saxon bishopric was that of Selsey. The East Saxons were more pagan than the southern or western Saxons; their territory had a superabundance of pagan sites. Their king, Saeberht, was converted early and a diocese was established at London. Its first bishop, Mellitus, was expelled by Saeberht's heirs. The conversion of the East Saxons was completed under Cedd in the 650s and 660s.
The continental Saxons were evangelised largely by English missionaries in the late 7th and early 8th centuries. Around 695, two early English missionaries, Hewald the White and Hewald the Black, were martyred by the "vicani", that is, villagers. Throughout the century that followed, villagers and other peasants proved to be the greatest opponents of Christianisation, while missionaries often received the support of the "edhilingui" and other noblemen. Saint Lebuin, an Englishman who between 745 and 770 preached to the Saxons, mainly in the eastern Netherlands, built a church and made many friends among the nobility. Some of them rallied to save him from an angry mob at the annual council at Marklo. Social tensions arose between the Christianity-sympathetic noblemen and the pagan lower castes, staunchly faithful to their traditional religion.
Under Charlemagne, the Saxon Wars had as their chief object the conversion and integration of the Saxons into the Frankish empire. Though much of the highest caste converted readily, forced baptisms and forced tithing made enemies of the lower orders. Even some contemporaries found the methods employed to win over the Saxons wanting, as this excerpt from a letter of Alcuin of York to his friend Meginfrid, written in 796, shows:
If the light yoke and sweet burden of Christ were to be preached to the most obstinate people of the Saxons with as much determination as the payment of tithes has been exacted, or as the force of the legal decree has been applied for fault of the most trifling sort imaginable, perhaps they would not be averse to their baptismal vows.
Charlemagne's successor, Louis the Pious, reportedly treated the Saxons more as Alcuin would have wished, and as a consequence they were faithful subjects. The lower classes, however, revolted against Frankish overlordship in favour of their old paganism as late as the 840s, when the "Stellinga" rose up against the Saxon leadership, who were allied with the Frankish emperor Lothair I. After the suppression of the "Stellinga", in 851 Louis the German brought relics from Rome to Saxony to foster a devotion to the Roman Catholic Church. The Poeta Saxo, in his verse "Annales" of Charlemagne's reign (written between 888 and 891), laid an emphasis on his conquest of Saxony. He celebrated the Frankish monarch as on par with the Roman emperors and as the bringer of Christian salvation to people. References are made to periodic outbreaks of pagan worship, especially of Freya, among the Saxon peasantry as late as the 12th century.
Vernacular Christianity.
In the 9th century, the Saxon nobility became vigorous supporters of monasticism and formed a bulwark of Christianity against the existing Slavic paganism to the east and the Nordic paganism of the Vikings to the north. Much Christian literature was produced in the vernacular Old Saxon, the notable ones being a result of the literary output and wide influence of Saxon monasteries such as Fulda, Corvey, and Verden; and the theological controversy between the Augustinian Gottschalk and the semipelagian Rabanus Maurus.
From an early date, Charlemagne and Louis the Pious supported Christian vernacular works in order to evangelise the Saxons more efficiently. The "Heliand", a verse epic of the life of Christ in a Germanic setting, and "Genesis", another epic retelling of the events of the first book of the Bible, were commissioned in the early 9th century by Louis to disseminate scriptural knowledge to the masses. A council of Tours in 813 and then a synod of Mainz in 848 both declared that homilies ought to be preached in the vernacular. The earliest preserved text in the Saxon language is a baptismal vow from the late 8th or early 9th centuries; the vernacular was used extensively in an effort to Christianise the lowest castes of Saxon society.

</doc>
<doc id="27885" url="http://en.wikipedia.org/wiki?curid=27885" title="Savoy">
Savoy

Savoy (; Arpitan: "Savouè", ]; French: "Savoie", ]; Italian: "Savoia") is a region of France. It comprises roughly the territory of the Western Alps between Lake Geneva in the north and Dauphiné in the south.
The historical land of Savoy emerged as the feudal territory of the House of Savoy during the 11th to 14th centuries. The historical territory is shared between the modern republics of France, Italy, and Switzerland.
Installed by Rudolph III, King of Burgundy, officially in 1003, the House of Savoy became the longest surviving royal house in Europe. It ruled the County of Savoy to 1416 and then the Duchy of Savoy from 1416 to 1714.
The territory of Savoy was annexed to France in 1792 under the French First Republic, before being returned to the Kingdom of Piedmont-Sardinia in 1815. Savoy, along with the county of Nice, was finally annexed to France by a plebiscite, under the Second French Empire in 1860, as part of a political agreement (Treaty of Turin) brokered between the French emperor Napoleon III and King Victor Emmanuel II of the Kingdom of Sardinia that began the process of unification of Italy. Victor Emmanuel's dynasty, the House of Savoy, retained its Italian lands of Piedmont and Liguria and became the ruling dynasty of Italy.
Geography.
In modern France, Savoy is part of the Rhône-Alpes region. Following its annexation to France in 1860 by a plebiscite, the territory of Savoy was divided administratively into two separate departments, Savoie and Haute-Savoie.
The traditional capital remains Chambéry (Ciamberì), on the rivers Leysse and Albane, hosting the castle of the House of Savoy and the Savoyard senate. The state included six districts:
The County and Duchy of Savoy incorporated Turin and other territories in Piedmont, a region in northwestern Italy that borders Savoy, which were also possessions of the House of Savoy. The capital of the Duchy remained at the traditional Savoyard capital of Chambéry until 1563, when it was moved to Turin.
History.
Early history.
The region occupied by the Allobroges, a Celtic people became part of the Roman Empire. The name "Savoy" stems from the Late Latin "Sapaudia", referring to a fir forest. It is first recorded in Ammianus Marcellinus (354), to describe the southern part of "Maxima Sequanorum". According to the Gallic Chronicle of 452, it was separated from the rest of Burgundian territories in 443, after the Burgundian defeat by Flavius Aetius.
Early and High Middle Ages.
By the 8th century, the territory that would later become known as Savoy was part of the Kingdom of the Franks, and at the division of Francia at the Treaty of Verdun in 843, it became part of the short-lived kingdom of Middle Francia. After only 12 years, at the death of Lothair I in 855, 
Middle Francia was divided into Lotharingia north of the Alps, Italy south of the Alps, and the parts of Burgundy in the Western Alps, inherited by Charles son of Lothair. This latter territory comprised what would become known as Savoy and Provence.
From the 10th to 14th century, parts of what would ultimately become Savoy remained within the Kingdom of Arles. Beginning in the 11th century, the gradual rise to power of the House of Savoy is reflected in the increasing territory of their County of Savoy between 1003 and 1416.
The County of Savoy was detached "de jure" from the Kingdom of Arles by Emperor Charles IV in 1361. It acquired the County of Nice in 1388, and in 1401 added the County of Genevois, the area of Geneva except for the city proper, which was ruled by its prince-bishop, nominally under the duke's rule: the bishops of Geneva, by unspoken agreement, came from the House of Savoy; this agreement came to an end in 1533.
Duchy of Savoy.
On February 19, 1416, Sigismund, Holy Roman Emperor, made the County of Savoy an independent duchy, with Amadeus VIII as the first duke. Straddling the Alps, Savoy lay within two competing spheres of influence, a French sphere and a North Italian one. At the time of the Renaissance, Savoy showed only modest development. Its towns were few and small. Savoy derived its subsistence from agriculture. The geographic location of Savoy was also of military importance. During the interminable wars between France and Spain over the control of northern Italy, Savoy was important to France because it provided access to Italy. Savoy was important to Spain because it served as a buffer between France and the Spanish held lands in Italy. In 1563 Emmanuel Philibert moved the capital from Chambéry to Turin, which was less vulnerable to French interference.
Vaud was annexed by Bern in 1536, and Savoy officially ceded Vaud to Bern in the Treaty of Lausanne of 30 October 1564.
In 1714, as a consequence of the War of the Spanish Succession, Savoy was technically subsumed into the Kingdom of Sicily, then (after that island was traded to Austria for Sardinia) the Kingdom of Sardinia from 1720.
While the heads of the House of Savoy were known as the Kings of Sardinia, Turin remained their capital.
French Revolutionary Wars.
Savoy was occupied by French revolutionary forces between 1792 and 1815. The region was first added to the département of Mont-Blanc, then in 1798 was divided between the départements of Mont-Blanc and Léman (French name of Lake Geneva.) In 1801, Savoy officially left the Holy Roman Empire. On September 13, 1793 the combined forces of Savoy, Piedmont and Aosta Valley fought against and lost to the occupying French forces at the Battle of Méribel (Sallanches). Two-thirds of Savoy was restored to the Kingdom of Sardinia in the First Restoration of 1814 following Napoleon's abdication; approximately one-third of Savoy, including the two most important cities of Chambéry and Annecy, remained in France. Following Napoleon's brief return to power during the Hundred Days and subsequent defeat at Waterloo, the remaining one-third of Savoy was restored to the Kingdom of Sardinia at the Congress of Vienna to strengthen Sardinia as a buffer state on France's southeastern border.
Modern history.
From 1815 until 1860 Savoy was part of the Kingdom of Piedmont-Sardinia.
Annexation to France.
The French Second Republic first attempted to annex Savoy in 1848. Corps were dispatched from Lyons and invaded the capital of Savoy, Chambéry, and proclaimed the annexation to France. On learning about the invasion countrymen rushed to Chambéry. The corps were chased away by the local population and many were massacred.
In order to secure an alliance against Austria in the wars of unification of Italy, the Prime Minister of the Kingdom of Sardinia Camillo Cavour met in secret with the French emperor Napoleon III on July 21, 1858 in Plombières (Vosges). During the discussion, Cavour promised that Sardinia would cede the County of Nice and Duchy of Savoy to France in exchange for military support in a planned war against Austria. Though this was a secret arrangement, it quickly became widely known.
The treaty annexing Nice and Savoy to France was signed in Turin on March 24, 1860 (Treaty of Turin). In the northern provinces of the Chablais and Faucigny, there was some sympathy for annexation to neighboring Switzerland, with which the northern provinces had longstanding economic ties. To help reduce the attractiveness of Switzerland, the French government conceded a free-trade Zone that maintained the longstanding duty-free relationship of northern Savoyard communes to Geneva. The treaty was followed on April 22–23 by a plebiscite employing universal male suffrage, in which voters were offered the option of voting "yes" to approve the treaty and join France or rejecting the treaty with a no vote. The disallowed options of either joining Switzerland, remaining with Italy, or regaining its independence, were the source of some opposition. With a 99.8% vote in favour of joining France, there were allegations of vote-rigging, notably by the British government, which opposed continental expansion by its traditional French enemy.
The correspondent of "The Times" in Savoy who was in Bonneville on April 22 called the vote "the lowest and most immoral farce(s) which was ever played in the history of nations". He finished his letter with those words: I leave you to draw your own conclusions from this trip, which will show clearly what the vote was in this part of Savoy. The vote was the bitterest irony ever made on popular suffrage. The ballot-box in the hands of those very authorities who issued the proclamations; no control possible; even travellers suspected and dogged lest they should pry into the matter; all opposition put down by intimidation, and all liberty of action completely taken away. One can really scarcely reproach the Opposition with having given up the game; there was too great force used against them. As for the result of the vote, therefore, no one need trouble himself about it; it will be just as brilliant as that in Nice. The only danger is lest the Savoy authorities in their zeal should fare as some of the French did in the vote of 1852, finding to their surprise rather more votes than voters inscribed on the list. In his letter to the ambassador of Vienna Lord A. Loftus, the then Foreign Secretary Lord John Russell said "Voting in Savoy and Nice a farce ... we are neither entertained or edified".
The annexation was promulgated on June 14, 1860. On August 23, 1860 and March 7, 1861, two agreements were signed between the French Empire and the Kingdom of Sardinia to settle the remaining issues concerning the annexation.
20th century.
In 1919, France officially (but contrary to the annexation treaty) ended the military neutrality of the parts of the country of Savoy that had originally been agreed to at the Congress of Vienna, and also eliminated the free trade zone – both treaty articles having been broken unofficially in World War I. France was condemned in 1932 by the international court for noncompliance with the measures of the Treaty of Turin regarding the provinces of Savoy and Nice.
In 1960, the term "annexation" having acquired negative connotations in France, particularly after Germany's 1871 annexation of Alsace-Lorraine, the annexation was renamed "Rattachement de la Savoie à la France" (Incorporation of Savoy to France). It was the latter term which was used by the French authorities during the festivities celebrating the 100th anniversary of the annexation. Daniel Rops of the French Academy justified the new title with these words:
Savoy has begun to solemnize the feasts in 1960, commemorating the centenary of its incorporation ("rattachement") to France. It is on purpose that the word incorporation ("rattachement") is highlighted here: the Savoyards attach great value to it, and it is the only one they have resolved to use in the official terminology of the Centenary. In that, they are infinitely right.
Yesterday another term that was used: annexation. Looking at it more closely it was wrong! Can we say annexation when we talk about a decision which was approved by 130,889 voters over 135,449? [...]. Savoy was not annexed [...] but actually incorporated freely and by the will of its inhabitants.
A former French deputy, P. Taponnier, spoke of the annexation:
In late March 1860, the betrothal ceremony of Savoy to France took place in Tuileries Palace [...], a ceremony which was a pact of love and fidelity [...] it is with free consent that she [Savoy] gave itself to France by a solemn plebiscite which our leaders can ignore neither the terms nor the commitments. [...] May the bells of our cities [...] in Savoy vibrate in unison to glorify, in this magnificent Centenary, the indefectible commitment of Savoy to France.
The Savoyards did not feel Italian. Besides, they spoke French. This explains why in 1858–1859 when rumours ran of the Plombières secret agreement, where Napoleon III and Cavour decided of the fate of Savoy, the Savoyards themselves took the initiative to ask for the incorporation ("rattachement"). [...] Incorporation, not annexation [...] The incorporation was an act of free will, in the logical order of geography and history [...].
Modern regionalist politics.
Since the mid-20th century, regionalist movements have appeared in Savoy much as in other historic provinces of France. The "Mouvement Région Savoie" (Savoy Regional Movement) was founded in December 1971 as a 'movement' (rather than a traditional political party) in favour of regional autonomy. Unlike other historic provinces, including Alsace and Brittany, Savoy does not currently have its own region within France and is part of the Rhône-Alpes region. In the 1996 local elections, the Savoy Regional Movement received 19,434 votes; it received 4,849 in the 1998 regional elections. A new non-party organisation, "La Région Savoie, j’y crois !" ("I believe in the Savoy Region!"), was founded in 1998. The organisation campaigns for the replacement of the Savoie and Haute-Savoie departments with a regional government, separate from the Rhône-Alpes region, with greater devolved powers.
A very marginal separatist movement has also appeared in Savoy within the past twenty years, most prominently represented by the "Ligue Savoisienne", founded in 1994. In the March 1998 regional elections, 1 seat (out of 23) was won by Patrice Abeille, leader of the Ligue, which won a total of 17,865 votes across the two departments. In 2004, "Waiting for Freedom in Savoy" was founded to promote the peaceful separatist cause to young people.
According to surveys conducted in 2000, between 41% and 55% of the population were in favour of the proposal for a separate Savoy region, while 19% to 23% were in favour of separation from France. Towards the end of 2005, Hervé Gaymard called for Savoie to be given special status, similar to a French region, under his proposed "Conseil des Pays de Savoie".
Modern historiographical debates.
In recent years, sparked by the admittedly tiny Savoyard separatist movement, much attention has been focused on questioning the validity of the 1860 annexation. The Ligue Savoisienne, for example, rejects the Treaty of Turin and subsequent plebiscite as null and void, arguing that the plebiscite did not meet the standards of a free and fair vote. Today, historians generally acknowledge that the plebiscite of 1860 did feature irregularities, but they also affirm that the annexation instrument was the Treaty of Turin and not the plebiscite, whose main purpose was to demonstrate favorable public opinion in Savoy for the annexation after the signature of the treaty. In an interview for the newspaper "Le Dauphiné Libéré", Sylvain Milbach, a historian at the University of Savoy, qualifies the vote as Napoleonic, but also argues that a completely free and fair vote would not have dramatically changed the outcome, as the majority of Savoyards wished to become French. This is today the official stance of the General Council of Savoie.

</doc>
<doc id="27940" url="http://en.wikipedia.org/wiki?curid=27940" title="Culture in Stockholm">
Culture in Stockholm

Apart from being a large city with an active cultural life, Stockholm, the capital of Sweden, houses many national cultural institutions. There are two UNESCO World Heritage sites in the Stockholm County area: the Royal Palace Drottningholm (within Ekerö Municipality) and the Skogskyrkogården (The Woodland Cemetery). 
Stockholm was the 1998 European City of Culture.
Literature.
Authors connected to Stockholm include the poet and songwriter Carl Michael Bellman (1740–1795), novelist and dramatist August Strindberg (1849–1912), and novelist Hjalmar Söderberg (1869–1941), all of whom made Stockholm part of their works. Other authors with notable heritage in Stockholm were the Nobel Prize laureate Eyvind Johnson (1900–1976) and the popular poet and composer Evert Taube (1890–1976). The novelist Per Anders Fogelström (1917–1998) wrote a popular series of historical novels depicting life in Stockholm from the 19th to the mid-20th century.
Architecture.
The city's oldest section is “Gamla Stan” (Old Town), located on the original small islands of the city's earliest settlements and still featuring the medieval street layout. Some notable buildings of Gamla Stan are the large German Church ("Tyska kyrkan") and several mansions and palaces: the "Riddarhuset" (the House of Nobles), the Bonde Palace, the Tessin Palace and the Oxenstierna Palace. The oldest building in Stockholm is the Riddarholmskyrkan from the late 13th century. After a fire in 1697 when the original medieval castle was destroyed, Stockholm Palace was erected in a baroque style. Storkyrkan Cathedral, the episcopal seat of the Bishop of Stockholm, stands next to the castle. It was founded in the 13th century but is clad in a baroque exterior dating to the 18th century.
As early as the 15th century, the city had expanded outside of its original borders. Some pre-industrial, small-scale buildings from this era can still be found in Södermalm. During the 19th century and the age of industrialization Stockholm grew rapidly, with plans and architecture inspired by the large cities of the continent such as Berlin and Vienna. Notable works of this time period include public buildings such as the Royal Swedish Opera and private developments such as the luxury housing developments on Strandvägen.
In the 20th century, a nationalistic push spurred a new architectural style inspired by medieval and renaissance ancestry as well as influences of the Jugend / Art Nouveau style. A key landmark of Stockholm, the Stockholm City Hall, was erected 1911–1923 by architect Ragnar Östberg. Other notable works of these times are the Stockholm Public Library and the Forest Cemetery, Skogskyrkogården
Modernism characterized the style of the Stockholm International Exhibition (1930) and the development of the city as it grew in that decade. New residential areas sprang up such as the development on Gärdet while industrial development added to the growth, such as the KF manufacturing industries on Kvarnholmen located in the Nacka Municipality. In the 1950s, suburban development entered a new phase with the introduction of the Stockholm metro. The modernist developments of Vällingby and Farsta where internationally praised. In the 1960s this suburban development continued but with the aesthetic of the times, the industrialised and mass-produced blocks of flats received a large amount of criticism.
At the same time that this suburban development was happening the most central areas of the inner city were being redesigned. Sergels Torg, with its five high-rise office towers was created in the 1960s, followed by the total clearance of large areas to make room for new development projects. The most notable buildings from this period is the ensemble of the House of Culture, City Theatre and National Bank at Sergels Torg, designed by architect Peter Celsing.
Museums.
Stockholm is one of the most crowded museum-towns in the world with some 70 museums, visited by over 9 million people per year.
One of the most renowned museums is the Nationalmuseum, with the largest national collection of art: 16,000 paintings and 30,000 objects of art handicraft. The collection dates back to the days of Gustav Vasa in the 16th century, and has since been expanded with works by artists such as Rembrandt, and Antoine Watteau, as well as constituting a main part of Sweden's art heritage, manifested in the works of Alexander Roslin, Anders Zorn, Johan Tobias Sergel, Carl Larsson, Carl Fredrik Hill and Ernst Josephson.
The Museum of Modern Art, or Moderna Museet, is Sweden's national museum of modern art. It has works by famous modern artists such as Picasso and Salvador Dalí. 
Other notable museums in Stockholm include:
Theatres.
Distinguished among Stockholm's many theatres are the Royal Dramatic Theatre ("Dramaten"), one of Europe's most renowned theatres, and the Royal Swedish Opera, inaugurated in 1773. 
Other notable theatres are the Stockholm City Theatre, the Peoples Opera ("Folkoperan"), the Modern Theatre of Dance ("Moderna dansteatern"), the China Theatre, the Göta Lejon Theatre, the Mosebacke Theatre, and the Oscar Theatre.
The stages of Stockholm number in their fifties at the least, and a wide variety of plays are constantly on, from classical to newly written.
Media.
Stockholm is basically the media center of Sweden. It has four nation-wide daily newspapers, is also the central location of the publicly funded radio (SR) and television (SVT); in addition, all other major television channels have their base in Stockholm (TV4 TV3, TV6 and Kanal 5). All major magazines are also located to Stockholm, as are the largest literature publisher, the Bonnier group.
Sports.
The most popular spectator sports are football and ice hockey. The three most popular football teams are AIK, Hammarby IF and Djurgårdens IF.
Historically, the city was the host of the 1912 Summer Olympics. From those days stem the Stockholms Olympiastadion which has since hosted numerous sports events, notably football and athletics. Stadion is the previous home arena of AIK and is the current home arena of Djurgårdens IF. For Sweden men's national ice hockey team, the home arena is Ericsson Globe, one of the largest spherical building in the world, but it is also hosting concerts and other events. 
Stockholm also hosted all but one of the Nordic Games, a winter multi-sport event that predated the Winter Olympics.

</doc>
<doc id="27948" url="http://en.wikipedia.org/wiki?curid=27948" title="September 6">
September 6

September 6 is the day of the year in the Gregorian calendar.

</doc>
<doc id="27983" url="http://en.wikipedia.org/wiki?curid=27983" title="Surd">
Surd

Surd may be:

</doc>
<doc id="27990" url="http://en.wikipedia.org/wiki?curid=27990" title="September 5">
September 5

September 5 is the day of the year in the Gregorian calendar.

</doc>
<doc id="27999" url="http://en.wikipedia.org/wiki?curid=27999" title="Human swimming">
Human swimming

Human swimming is the self-propulsion of a person through water or another liquid, usually for the purpose of recreation, sport, exercise, or survival. Locomotion is achieved through coordinated movement of the limbs, the body, or both. Humans are able to hold their breath underwater and undertake rudimentary locomotive swimming within weeks of birth, as an evolutionary response.
Swimming is consistently found to be among the top recreational activities undertaken by the public, and in some countries, swimming lessons are a compulsory part of the educational curriculum. As a formalized sport, swimming features in a range of local, national, and international competitions, including every modern summer Olympics, which occurs every four years.
Science.
Swimming relies on the natural buoyancy of the human body. On average, the body has a relative density of 0.98 compared to water, which causes the body to float. However, buoyancy varies on the basis of both body composition and the salinity of the water. Higher levels of body fat and saltier water both lower the relative density of the body and increase its floatation.
Since the human body is only slightly less dense than water, water supports the weight of the body during swimming. As a result, swimming is "low-impact" compared to land activities such as running. The density and viscosity of water also create resistance for objects moving through the water. Swimming strokes use this resistance to create propulsion, but this same resistance also generates drag on the body.
This means that hydrodynamics are an important factor in stroke technique in terms of swimming faster, and swimmers wishing to swim faster, or wishing to tire less, will try to reduce the drag caused by the body's motion through the water. In order to be more hydrodynamic, swimmers can either increase the power of their strokes or reduce their water resistance, although increasing power to overcome resistance needs to increase by a factor of three to achieve the same effect as reducing resistance.
Efficient swimming by reducing water resistance involves having a horizontal water position, rolling the body in order to reduce the breadth of the body in the water, and extending the arms as far as possible in order to reduce wave resistance.
Infant swimming.
Human babies demonstrate an innate swimming or diving reflex from newborn until the age of approximately 6 months. Other mammals also demonstrate this phenomenon (see mammalian diving reflex). The diving response involves apnoea, reflex bradycardia, and peripheral vasoconstriction; in other words a baby immersed in water will spontaneously hold his or her breath, slow his/her heartrate, and reduce blood circulation to the extremities such as fingers and toes.
Technique.
Swimming can be undertaken using a wide range of different styles, known as 'strokes,' and these strokes are used for different purposes, or to distinguish between classes in competitive swimming. It is not necessary to use a defined stroke for propulsion through the water, and untrained swimmers may use a 'doggy paddle' of arm and leg movements, similar to the way four-legged animals swim.
There are four main strokes used in competition and recreation swimming: the front crawl, the breaststroke, the backstroke and the butterfly. Competitive swimming in Europe started around 1800, mostly using the breaststroke. In 1873 John Arthur Trudgen introduced the trudgen to Western swimming competitions, after copying the front crawl used by Native Americans, but substituting a scissor kick for the traditional flutter kick in order to reduce splashing. The butterfly stroke was developed in the 1930s and was considered a variant of the breaststroke until it was accepted as a separate style in 1952.
Other strokes exist for specific purposes, such as training or rescue, and it is also possible to adapt strokes to avoid using parts of the body, either to isolate certain body parts, such as swimming with arms only or legs only to train them harder, or for use by amputees or those suffering from paralysis.
Historic record.
Swimming has been recorded since prehistoric times, and the earliest records of swimming date back to Stone Age paintings from around 7,000 years ago. Written references date from 2000 BC. Some of the earliest references include the Epic of Gilgamesh, the Iliad, the Odyssey, the Bible (Ezekiel 47:5, Acts 27:42, Isaiah 25:11), Beowulf, and other sagas. In 1538, Nikolaus Wynmann, a German professor of languages, wrote the first swimming book, "The Swimmer or A Dialogue on the Art of Swimming" ("Der Schwimmer oder ein Zweigespräch über die Schwimmkunst").
Purpose.
There are many reasons why people swim, from swimming as a recreational pursuit to swimming as a necessary part of a job or other activity. Swimming may also be used to rehabilitate injuries, especially various cardiovascular injuries and muscle injuries. 
Recreation.
Many swimmers swim for recreation, with swimming consistently ranking as one of the physical activities people are most likely to take part in. Recreational swimming can also be used for exercise, relaxation, or rehabilitation. The support of the water, and the reduction in impact, makes swimming accessible for people who are unable to undertake activities such as running.
Health.
Swimming is primarily a cardiovascular/aerobic exercise due to the long exercise time, requiring a constant oxygen supply to the muscles, except for short sprints where the muscles work anaerobically. As with most aerobic exercise, swimming is believed to reduce the harmful effects of stress. Swimming can also improve posture.
Sport.
Swimming as a sport predominantly involves competition among participants to be the fastest over a given distance under self-propulsion. Different distances are swum in different levels of competition. For example, swimming has been an Olympic sport since 1896, and the current program includes events from 50m to 1500m in length, across all four main strokes and medley. 
The sport is governed internationally by the Fédération Internationale de Natation (FINA), and competition pools for FINA events are 25 or 50 meters in length. In the United States of America, USA Swimming is the governing body and a pool 25 yards in length is commonly used for competition. 
Other swimming and water-related sporting disciplines including diving, synchronized swimming and water polo, as well as sports which include a swimming element, such as the triathlon and the modern pentathlon.
Occupation.
Some occupations require the workers to swim. For example, abalone- and pearl-divers swim and dive to obtain an economic benefit, as do spear fishermen.
Swimming is used to rescue people in the water who are in distress, including exhausted swimmers, non-swimmers who have accidentally entered the water, and others who have come to harm on the water. Lifeguards or volunteer lifesavers are deployed at many pools and beaches worldwide to fulfill this purpose, and they, as well as rescue swimmers, may use specific swimming styles for rescue purposes.
Swimming is also used in marine biology to observe plants and animals in their natural habitat. Other sciences use swimming, for example Konrad Lorenz swam with geese as part of his studies of animal behavior.
Swimming also has military purposes. Military swimming is usually done by special forces, such as Navy SEALs. Swimming is used to approach a location, gather intelligence, engage in sabotage or combat, and subsequently depart. This may also include airborne insertion into water or exiting a submarine while it is submerged. Due to regular exposure to large bodies of water, all recruits in the United States Navy, Marine Corps, and Coast Guard are required to complete basic swimming or water survival training.
Swimming is also a professional sport. Companies sponsor swimmers who have the skills to compete at the international level. Many swimmers compete competitively in order to represent their home country in the Olympics. Cash awards are also given at many of the major competitions for breaking records. Professional swimmers may also earn a living as entertainers, performing in water ballets.
Locomotion.
Locomation by swimming over brief distances is frequent when alternatives are precluded. There have been cases of political refugees swimming in the Baltic Sea and of people jumping in the water and swimming ashore from vessels not intended to reach land where they planned to go. Swimming travel is central to the plot of the motion picture "Welcome". US president John F. Kennedy led his sailors swimming island to island after his torpedo boat was sunk in World War II, and his senator brother Ted Kennedy claimed to have left Chappaquiddick Island by swimming.
Risks.
There are many risks associated with voluntary or involuntary human presence in water, which may result in death directly or through drowning asphyxiation. Swimming is both the goal of much voluntary presence, and the prime means of regaining land in accidental situations.
Most recorded water deaths fall into these categories:
An adult with fully developed and extended lungs has generally positive or at least neutral buoyancy, and can float with modest effort when calm and in still water. A small child has negative buoyancy and will either sink rapidly or have to make a sustained effort to stay near the surface.
Hypothermia and dehydration also kill directly, without causing drowning, even when the person wears a life vest.
Less common are 
Around any pool area, safety equipment is often considered important and is a zoning requirement for most residential pools in the United States. Supervision by personnel trained in rescue techniques is required at most competitive swimming meets and public pools.
Lessons.
Traditionally, children were considered not able to swim independently until 4 years of age,
although now infant swimming lessons are recommended to prevent drowning.
In Sweden, Denmark, Norway, Estonia and Finland, the curriculum for the fifth grade (fourth grade in Estonia) states that all children should learn how to swim as well as how to handle emergencies near water. Most commonly, children are expected to be able to swim 200 m – of which at least 50 m on their back – after first falling into deep water and getting their head under water. Even though about 95 percent of Swedish school children know how to swim, drowning remains the third most common cause of death among children.
In both the Netherlands and Belgium swimming lessons under school time ("schoolzwemmen", school swimming) are supported by the government. Most schools provide swimming lessons. There is a long tradition of swimming lessons in the Netherlands and Belgium, the Dutch translation for the breaststroke swimming style is even "schoolslag" (schoolstroke). The children learn a variant of the breaststroke, which is technically not entirely correct. In France, swimming is a compulsory part of the curriculum for primary schools. Children usually spend one semester per year learning swimming during CE1/CE2/CM1 (2nd, 3rd and 4th grade). 
In many places, swimming lessons are provided by local swimming pools, both those run by the local authority and by private leisure companies. Many schools also include swimming lessons into their Physical Education curricula, provided either in the schools' own pool, or in the nearest public pool. 
In the UK, the "Top-ups scheme" calls for school children who cannot swim by the age of 11 to receive intensive daily lessons. These children who have not reached Great Britain's National Curriculum standard of swimming 25 metres by the time they leave primary school will be given a half-hour lesson every day for two weeks during term-time.
In Canada and Mexico there has been a call for swimming to be included in the public school curriculum.
In USA there is the Infant Swimming Resource (ISR) initiative that provides lessons for infant children, to cope with emergency situation when they have fallen into water. They are taught how to roll-back-to-float (hold their breath underwater, to roll onto their back, to float unassisted, rest and breathe until help arrives).
Clothing and equipment.
Swimsuits.
Standard everyday clothing is usually impractical for swimming and is unsafe under some circumstances. Most cultures today expect swimsuits to be worn for aquatic activities.
Men's swimsuits commonly resemble shorts, or briefs. Casual men's swimsuits (for example, boardshorts) are rarely skintight, unlike competitive swimwear, like jammers or diveskins. In most cases, boys and men swim with their upper body exposed, except in countries where custom or law prohibits it in a public setting, or for practical reasons such as sun protection.
Modern women's swimsuits are generally skintight, covering the pubic region and the breasts (See bikini). Women's swimwear may also cover the midriff as well. Women's swimwear is often a fashion statement, and whether it is modest or not is a subject of debate by many groups, religious and secular.
Competitive swimwear is built so that the wearer can swim faster and more efficiently. Modern competitive swimwear is skintight and lightweight. There are many kinds of competitive swimwear for each gender. It is used in aquatic competitions, such as water polo, swim racing, diving, and rowing.
Wetsuits provide both thermal insulation and floatation. Many swimmers lack buoyancy in the leg. The wetsuit reduces density, and therefore improves buoyancy while swimming. It provides insulation by absorbing some of the surrounding water, which then heats up when in direct contact with skin. The wetsuit is the usual choice for those who swim in cold water for long periods of time, as it reduces susceptibility to hypothermia.
Some people also choose to wear no clothing while swimming. This is known as skinny dipping. It was common for males to swim naked in a public setting up to the early 20th century. Today, skinny dipping can be a rebellious activity, or merely a casual one.

</doc>
<doc id="28011" url="http://en.wikipedia.org/wiki?curid=28011" title="Subgroup">
Subgroup

In mathematics, given a group "G" under a binary operation ∗, a subset "H" of "G" is called a subgroup of "G" if "H" also forms a group under the operation ∗. More precisely, "H" is a subgroup of "G" if the restriction of ∗ to "H" × "H" is a group operation on "H". This is usually denoted "H" ≤ "G", read as ""H" is a subgroup of "G"".
A proper subgroup of a group "G" is a subgroup "H" which is a proper subset of "G" (i.e. "H" ≠ "G"). This is usually represented notationally by "H" < "G", read as ""H" is a proper subgroup of "G"".
The trivial subgroup of any group is the subgroup {"e"} consisting of just the identity element.
If "H" is a subgroup of "G", then "G" is sometimes called an overgroup of "H".
The same definitions apply more generally when "G" is an arbitrary semigroup, but this article will only deal with subgroups of groups. The group "G" is sometimes denoted by the ordered pair ("G", ∗), usually to emphasize the operation ∗ when "G" carries multiple algebraic or other structures.
This article will write "ab" for "a" ∗ "b", as is usual.
Cosets and Lagrange's theorem.
Given a subgroup "H" and some "a" in G, we define the left coset "aH" = {"ah" : "h" in "H"}. Because "a" is invertible, the map φ : "H" → "aH" given by φ("h") = "ah" is a bijection. Furthermore, every element of "G" is contained in precisely one left coset of "H"; the left cosets are the equivalence classes corresponding to the equivalence relation "a"1 ~ "a"2 if and only if "a"1−1"a"2 is in "H". The number of left cosets of "H" is called the index of "H" in "G" and is denoted by ["G" : "H"].
Lagrange's theorem states that for a finite group "G" and a subgroup "H", 
where |"G"| and |"H"| denote the orders of "G" and "H", respectively. In particular, the order of every subgroup of "G" (and the order of every element of "G") must be a divisor of |"G"|.
Right cosets are defined analogously: "Ha" = {"ha" : "h" in "H"}. They are also the equivalence classes for a suitable equivalence relation and their number is equal to ["G" : "H"].
If "aH" = "Ha" for every "a" in "G", then "H" is said to be a normal subgroup. Every subgroup of index 2 is normal: the left cosets, and also the right cosets, are simply the subgroup and its complement. More generally, if "p" is the lowest prime dividing the order of a finite group "G," then any subgroup of index "p" (if such exists) is normal.
Example: Subgroups of Z8.
Let "G" be the cyclic group Z8 whose elements are
and whose group operation is addition modulo eight. Its Cayley table is
This group has two nontrivial subgroups: "J"={0,4} and "H"={0,2,4,6}, where "J" is also a subgroup of "H". The Cayley table for "H" is the top-left quadrant of the Cayley table for "G". The group "G" is cyclic, and so are its subgroups. In general, subgroups of cyclic groups are also cyclic.
Example: Subgroups of S4 (the symmetric group on 4 elements).
Every group has as many small subgroups as neutral elements on the main diagonal:
The trivial group and two-element groups Z2. These small subgroups are not counted in the following list.

</doc>
<doc id="28016" url="http://en.wikipedia.org/wiki?curid=28016" title="Steiner system">
Steiner system

In combinatorial mathematics, a Steiner system (named after Jakob Steiner) is a type of block design, specifically a with λ = 1 and "t" ≥ 2.
A Steiner system with parameters "t", "k", "n", written S("t","k","n"), is an "n"-element set "S" together with a set of "k"-element subsets of "S" (called blocks) with the property that each "t"-element subset of "S" is contained in exactly one block. In an alternate notation for block designs, an S("t","k","n") would be a "t"-("n","k",1) design.
This definition is relatively modern, generalizing the "classical" definition of Steiner systems which in addition required that "k" = "t" + 1. An S(2,3,"n") was (and still is) called a "Steiner triple" (or "triad") "system", while an S(3,4,"n") was called a "Steiner quadruple system", and so on. With the generalization of the definition, this naming system is no longer strictly adhered to.
As of 2015, an outstanding problem in design theory is if any nontrivial (formula_1) Steiner systems have "t" ≥ 6. It is also unknown if infinitely many have "t" = 4 or 5.
Examples.
Finite projective planes.
A finite projective plane of order "q", with the lines as blocks, is an formula_2, since it has formula_3 points, each line passes through formula_4 points, and each pair of distinct points lies on exactly one line.
Finite affine planes.
A finite affine plane of order "q", with the lines as blocks, is an S(2, "q", "q"2). An affine plane of order "q" can be obtained from a projective plane of the same order by removing one block and all of the points in that block from the projective plane. Choosing different blocks to remove in this way can lead to non-isomorphic affine planes.
Classical Steiner systems.
Steiner triple systems.
An S(2,3,"n") is called a Steiner triple system, and its blocks are called triples. It is common to see the abbreviation STS("n") for a Steiner triple system of order "n". 
The number of triples is "n"("n"−1)/6. A necessary and sufficient condition for the existence of an S(2,3,"n") is that "n" formula_5 1 or 3 (mod 6). The projective plane of order 2 (the Fano plane) is an STS(7) and the affine plane of order 3 is an STS(9).
Up to isomorphism, the STS(7) and STS(9) are unique, there are two STS(13)s, 80 STS(15)s, and 11,084,874,829 STS(19)s.
We can define a multiplication on the set "S" using the Steiner triple system by setting "aa" = "a" for all "a" in "S", and "ab" = "c" if {"a","b","c"} is a triple. This makes "S" an idempotent, commutative quasigroup. It has the additional property that "ab" = "c" implies "bc" = "a" and "ca" = "b". Conversely, any (finite) quasigroup with these properties arises from a Steiner triple system. Commutative idempotent quasigroups satisfying this additional property are called "Steiner quasigroups".
Steiner quadruple systems.
An S(3,4,"n") is called a Steiner quadruple system. A necessary and sufficient condition for the existence of an S(3,4,"n") is that "n" formula_5 2 or 4 (mod 6). The abbreviation SQS("n") is often used for these systems.
Up to isomorphism, SQS(8) and SQS(10) are unique, there are 4 SQS(14)s and 1,054,163 SQS(16)s.
Steiner quintuple systems.
An S(4,5,"n") is called a "Steiner quintuple system". A necessary condition for the existence of such a system is that "n" formula_5 3 or 5 (mod 6) which comes from considerations that apply to all the classical Steiner systems. An additional necessary condition is that "n" formula_8 4 (mod 5), which comes from the fact that the number of blocks must be an integer. Sufficient conditions are not known.
There is a unique Steiner quintuple system of order 11, but none of order 15 or order 17. Systems are known for orders 23, 35, 47, 71, 83, 107, 131, 167 and 243. The smallest order for which the existence is not known (as of 2011) is 21.
Properties.
It is clear from the definition of S("t","k","n") that formula_9. (Equalities, while technically possible, lead to trivial systems.)
If S("t","k","n") exists, then taking all blocks containing a specific element and discarding that element gives a "derived system" S("t"−1,"k"−1,"n"−1). Therefore the existence of S("t"−1,"k"−1,"n"−1) is a necessary condition for the existence of S("t","k","n").
The number of "t"-element subsets in S is formula_10, while the number of "t"-element subsets in each block is formula_11. Since every "t"-element subset is contained in exactly one block, we have formula_12, or formula_13, where "b" is the number of blocks. Similar reasoning about "t"-element subsets containing a particular element gives us formula_14, or formula_15, where "r" is the number of blocks containing any given element. From these definitions follows the equation formula_16. It is a necessary condition for the existence of S("t","k","n") that "b" and "r" are integers. As with any block design, Fisher's inequality formula_17 is true in Steiner systems.
Given the parameters of a Steiner system S(t,k,n) and a subset of size formula_18, contained in at least one block, one can compute the number of blocks intersecting that subset in a fixed number of elements by constructing a Pascal triangle. In particular, the number of blocks intersecting a fixed block in any number of elements is independent of the chosen block.
It can be shown that if there is a Steiner system S(2,"k","n"), where "k" is a prime power greater than 1, then "n" formula_5 1 or "k" (mod "k"("k"−1)). In particular, a Steiner triple system S(2,3,"n") must have "n" = 6"m"+1 or 6"m"+3. It is known that this is the only restriction on Steiner triple systems, that is, for each natural number "m", systems S(2,3,6"m"+1) and S(2,3,6"m"+3) exist.
History.
Steiner triple systems were defined for the first time by W.S.B. Woolhouse in 1844 in the Prize question #1733 of Lady's and Gentlemen's Diary. The posed problem was solved by Thomas Kirkman (1847). In 1850 Kirkman posed a variation of the problem known as Kirkman's schoolgirl problem, which asks for triple systems having an additional property (resolvability). Unaware of Kirkman's work, Jakob Steiner (1853) reintroduced triple systems, and as this work was more widely known, the systems were named in his honor.
Mathieu groups.
Several examples of Steiner systems are closely related to group theory. In particular, the finite simple groups called Mathieu groups arise as automorphism groups of Steiner systems:
The Steiner system S(5, 6, 12).
There is a unique S(5,6,12) Steiner system; its automorphism group is the Mathieu group M12, and in that context it is denoted by W12.
Constructions.
There are different ways to construct an S(5,6,12) system.
Projective line method.
This construction is due to Carmichael (1937).
Add a new element, call it ∞, to the 11 elements of the finite field F11 (that is, the integers mod 11). This set, "S", of 12 elements can be formally identified with the points of the projective line over F11. Call the following specific subset of size 6,
a "block". From this block, we obtain the other blocks of the S(5,6,12) system by repeatedly applying the linear fractional transformations:
With the usual conventions of defining "f" (−"d"/"c") = ∞ and "f" (∞) = "a"/"c", these functions map the set "S" onto itself. In geometric language, they are projectivities of the projective line. They form a group under composition which is the projective special linear group PSL(2,11) of order 660. There are exactly five elements of this group that leave the starting block fixed setwise, so there will be 132 images of that block. As a consequence of the multiply transitive property of this group acting on this set, any subset of five elements of "S" will appear in exactly one of these 132 images of size six.
Kitten method.
An alternative construction of W12 is obtained by use of the 'kitten' of R.T. Curtis, which was intended as a "hand calculator" to write down blocks one at a time. The kitten method is based on completing patterns in a 3x3 grid of numbers, which represent an affine geometry on the vector space F3xF3, an S(2,3,9) system.
Construction from K6 graph factorization.
The relations between the graph factors of the complete graph K6 generate an S(5,6,12). A K6 graph has 6 different 1-factorizations (ways to partition the edges into disjoint perfect matchings), and also 6 vertices. The set of vertices and the set of factorizations provide one block each. For every distinct pair of factorizations, there exists exactly one perfect matching in common. Take the set of vertices and replace the two vertices corresponding to an edge of the common perfect matching with the labels corresponding to the factorizations; add that to the set of blocks. Repeat this with the other two edges of the common perfect matching. Similarly take the set of factorizations and replace the labels corresponding to the two factorizations with the end points of an edge in the common perfect matching. Repeat with the other two edges in the matching. There are thus 3+3 = 6 blocks per pair of factorizations, and there are 6C2 = 15 pairs among the 6 factorizations, resulting in 90 new blocks. Finally take the full set of 12C6 = 924 combinations of 6 objects out of 12, and discard any combination that has 5 or more objects in common with any of the 92 blocks generated so far. Exactly 40 blocks remain, resulting in 2+90+40 = 132 blocks of the S(5,6,12).
The Steiner system S(5, 8, 24).
The Steiner system S(5, 8, 24), also known as the Witt design or Witt geometry, was first described by Carmichael (1931) and rediscovered by Witt (1938). This system is connected with many of the sporadic simple groups and with the exceptional 24-dimensional lattice known as the Leech lattice.
The automorphism group of S(5, 8, 24) is the Mathieu group M24, and in that context the design is denoted W24 ("W" for "Witt")
Constructions.
There are many ways to construct the S(5,8,24). Two methods are described here:
Method based on 8-combinations of 24 elements.
All 8-element subsets of a 24-element set are generated in lexicographic order, and any such subset which differs from some subset already found in fewer than four positions is discarded.
The list of octads for the elements 01, 02, 03, ..., 22, 23, 24 is then:
Each single element occurs 253 times somewhere in some octad. Each pair occurs 77 times. Each triple occurs 21 times. Each quadruple (tetrad) occurs 5 times. Each quintuple (pentad) occurs once. Not every hexad, heptad or octad occurs.
Method based on 24-bit binary strings.
All 24-bit binary strings are generated in lexicographic order, and any such string that differs from some earlier one in fewer than 8 positions is discarded. The result looks like this:
The list contains 4096 items, which are each code words of the extended binary Golay code. They form a group under the XOR operation. One of them has zero 1-bits, 759 of them have eight 1-bits, 2576 of them have twelve 1-bits, 759 of them have sixteen 1-bits, and one has twenty-four 1-bits. The 759 8-element blocks of the S(5,8,24) (called ) are given by the patterns of 1's in the code words with eight 1-bits.

</doc>
<doc id="28047" url="http://en.wikipedia.org/wiki?curid=28047" title="Memorials and services for the September 11 attacks">
Memorials and services for the September 11 attacks

The first memorials to the victims of the September 11, 2001, attacks began to take shape online, as hundreds of webmasters posted their own thoughts, links to the Red Cross, and other rescue agencies, photos and eyewitness accounts. Numerous online September 11 memorials began appearing a few hours after the attacks, although many of these memorials were only temporary.
Around the world, U.S. embassies and consulates became makeshift memorials as people came out to pay their respects. Many U.S. ambassadors have said that they will never forget the outpouring of people as they showed their sympathy to the American people and their opposition to terrorism.
The Tribute in Light was the first major physical memorial at the World Trade Center site. A permanent memorial and museum, the National September 11 Memorial & Museum at the World Trade Center, are being built as part of the design by overall WTC site redevelopment. The Memorial consists of two massive pools set within the original footprints of the Twin Towers with 30 ft waterfalls cascading down their sides. The names of the victims of the attacks have been inscribed around the edges of the waterfalls.
Permanent memorials are being constructed around the world.
One of the places that saw many memorials and candlelight vigils was Pier A in Hoboken, New Jersey, where many people saw the events of September 11 (Pier A had a good view of the World Trade Center.) There was also a memorial service on March 11, 2002, at dusk on Pier A when the Tribute in Light first turned on, marking the half-year anniversary of the terrorist attack. A permanent September 11 memorial for Hoboken, called Hoboken Island, was chosen in September 2004.
List.
Temporary memorials.
Soon after the attacks, temporary memorials were set up in New York and elsewhere.
Performances and benefits.
2001 events.
The Raoul Wallenberg Award was given to New York City in 2001 "For all of its citizens who searched for the missing, cared for the injured, gave comfort to loved ones of the missing or lost, and provided sustenance and encouragement to those who searched through the rubble at Ground Zero."
2002 and later events.
On February 3, 2002, during the Halftime Show of Super Bowl XXXVI, rock group U2 performed Where the Streets Have No Name, while the names of the victims were projected onto banners. Bono opened his jacket to reveal a U.S. flag pattern sewn in the inside lining.
On February 23, 2003, the 45th Annual Grammy Awards were held at Madison Square Garden and paid tribute to those who died during the 9/11 attacks, to whom the ceremony was dedicated. Ceremony host Bruce Springsteen performed "The Rising" at the Awards.
American country singer Darryl Worley paid tribute to the people with his 2003 single, "Have You Forgotten?" from the album of the same name.
Newark International Airport was renamed "Newark Liberty International Airport".
On September 11, 2002, representatives from over 90 countries came to Battery Park City as New York City Mayor Michael Bloomberg lit an eternal flame to mark the first anniversary of the attacks. Leading the dignitaries were Canadian Prime Minister Jean Chrétien, U.N. Secretary General Kofi Annan, Bloomberg, and Secretary of State Colin Powell. The same day, the Victims of Terrorist Attack on the Pentagon Memorial was dedicated at Arlington National Cemetery near the Pentagon. The memorial is dedicated to the five individuals at the Pentagon whose remains were never found, and the partial remains of another 25 victims are buried beneath the memorial. The names of the 184 victims of the Pentagon attack are inscribed on the memorial's side.
10th anniversary memorial services.
Many organizations held memorial services and events for the 10th anniversary of the attacks.
Annual commemorations.
Every year on September 11 a commemoration is held at the National September 11 Memorial. Family members read the names of victims of the attacks, as well as victims of the 1993 World Trade Center truck bombing. Elected officials and other dignitaries attend, but since the 2012 event they have not given speeches.
Memorial flags.
The National 9/11 Flag was made from a tattered remains of a 30 ft American flag found by recovery workers in the early morning of September 12, 2001. It was hanging precariously from some scaffolding at a construction site next to Ground Zero. Because of safety reasons the flag could not be taken down until late October 2001. Charlie Vitchers, a construction superintendent for the Ground Zero cleanup effort, had a crew recover the flag. It was placed in storage for seven years.
The flag has made a number appearances across the country including a Boston Red Sox Game, a New York Giants Home Opener, and the USS New York Commissioning Ceremony. It also appeared on the CBS Evening News and on ABC World News Tonight "Persons of the Week."
The flag began a national tour on Flag day, which was on June 14, 2009. It will visit all 50 states where service heroes, veterans, and other honorees will each add stitching and material from other retired American flags in order to restore the original 13 stripes of the flag. The flag will have a permanent home at the National September 11 Memorial and Museum.
The 9-11 Remembrance Flag was created to be a permanent reminder of the thousands of people lost in the September 11 attacks. The purpose of keeping the memories of September 11 alive is not to be forever mourning, but for "learning from the circumstances and making every effort to prevent similar tragedies in our future." The flag is also meant to be a reminder of how the people of this country came together to help each other after the attacks. The red background of the flag represents the blood shed by Americans for their country. The stars represent the lost airplanes and their passengers. The blue rectangles stand for the twin towers and the white pentagon represents the Pentagon building. The blue circle symbolizes the unity of this country after the attacks.
The 9/11 National Remembrance Flag was designed by Stephan and Joanne Galvin soon after September 11, 2001. They wanted to do something to help and were inspired by a neighbor's POW/MIA flag. They wanted sell the flag so people would remember the September 11 attacks and in order to raise money for relief efforts. The blue represents the colors of the state flags that were involved in the attacks. The black represents sorrow for innocent lives lost. The four stars stand for the four planes that crashed and the lives lost, both in the crash and in the rescue efforts, as well as the survivors. The blue star is a representation of American Airlines Flight 77 and the Pentagon. The two white stars represent American Airlines flight 11 and United Airlines flight 175, as well as the twin towers. The red star stands for United Flight 93 that crashed in Shanksville, Pennsylvania and all those who sacrifice their lives to protect the innocent. The colors of the stars represent the American flag. The four stars are touching each other and the blue parts of the flag in order to symbolize the unity of the people of the United States.
The National Flag of Honor and the National Flag of Heroes were created by John Michelotti for three main reasons: (1)"To immortalize the individual victims that were killed in the terrorist attacks of September 11, 2001." (2)"To give comfort to the families left behind knowing that their loved one will be forever honored and remembered." (2)"To create an enduring symbol, recognized by the world, of the human sacrifice that occurred on September 11, 2001."
The Flag of Honor and the Flag of Heroes are based on the American flag. They both have the names of all the innocent people who were killed in the September 11 attacks printed on the red and white stripes of the American Flag. Both flags have a white space across the bottom with the name of the flag and a description printed in black. The Flag of Honor reads: "This flag contains the names of those killed in the terrorist attacks of September 11. Now and forever it will represent their immortality. We shall never forget them" The Flag of Heroes reads: " This flag contains the names of the emergency service personnel who gave their lives to save others in the terrorist attacks of September 11. Now and forever it will represent their immortality. We shall never forget them."
The Flag of Honor and the Flag of Heroes were featured at the NYC 9/11 Memorial Field 5th Anniversary in Manhattan's Inwood Hill Park September 8–12, 2006. There 3,000 flags which represented those who died in the September 11 attacks. The flags were also featured on the msnbc Today Show and on ABC 13 News, Norfolk, VA.
The Remembrance Flag has a white background with large, black Roman numerals IX/XI in the center and four black stars across the top. The IX/XI are the Roman numerals for 9/11. The four stars represent World Trade Center North, World Trade Center South, the Pentagon, and Shanksville, PA.
The 10th Anniversary September 11 Memorial Flag was designed by Carrot-Top Industries, a privately owned company in Hillsborough, NC. The exclusive 9/11 memorial flag was designed with the two World Trade Towers set inside a pentagon decorated with a ribbon to commemorate all of the Americans that lost their lives on September 11, 2001.
Virtual memorials.
The growing popularity of virtual worlds such as Secondlife has led to the construction of permanent virtual memorials and exhibits. Examples include:

</doc>
<doc id="28156" url="http://en.wikipedia.org/wiki?curid=28156" title="Salem al-Hazmi">
Salem al-Hazmi

Salem al-Hazmi (Arabic: سالم الحازمي‎, "Sālam al-Ḥāzmī", also transliterated as Alhazmi) (February 2, 1981 – September 11, 2001) was one of five hijackers of American Airlines Flight 77 as part of the September 11 attacks. 
A Saudi, Hazmi had a relatively long history with al-Qaeda before being selected for the attacks. He obtained a tourist visa through the Visa Express program and arrived in the United States in June 2001 where he would settle in New Jersey with other American 77 hijackers up until the attacks. 
On September 11, 2001, Hazmi boarded American Airlines Flight 77 and helped subdue the passengers and crew for Hani Hanjour, the pilot among the hijackers, to crash the plane into west facade of the Pentagon. His older brother, Nawaf al-Hazmi, was another hijacker aboard the same flight. At the age of 20 years and 221 days he was the youngest hijacker who participated in the attacks.
History.
Hazmi was born on February 2, 1981 to Muhammad Salim al-Hazmi, a grocer, in Mecca, Saudi Arabia. His father described Salem as a quarrelsome teenager who had problems with alcohol and petty theft. However, he stopped drinking and began to attend the mosque about three months before he left his family.
There are reports that he fought in Afghanistan with his brother, Nawaf al-Hazmi, and other reports say the two fought together in Chechnya. Salem al-Hazmi was an al-Qaeda veteran by the time he was selected for participation in the 9/11 attacks. U.S intelligence learned of Hazmi's involvement with al-Qaeda as early as 1999, but he was not placed on any watchlists. 
Known as "Bilal" during the preparations, both he and Ahmed al-Ghamdi flew to Beirut in November 2000, though on separate flights.
Along with Nawaf al-Hazmi and several other future hijackers, Salem al-Hazmi may have attended the 2000 Al Qaeda Summit in Kuala Lumpur, Malaysia. It was there that the details of the 9/11 attacks were decided upon.
In the United States.
According to the FBI and the 9/11 Commission report, Hazmi first entered the United States on June 29, 2001, although there are numerous unconfirmed reports that he was living in San Antonio, Texas with fellow hijacker Satam al-Suqami much earlier. Hazmi used the controversial Visa Express program to gain entry into the country. 
Hazmi moved to Paterson, New Jersey where he lived with Hani Hanjour. Both were among the five hijackers who applied for Virginia identity cards at the Arlington office of the Virginia Department of Motor Vehicles on August 2, 2001, although Salem already held an NJ identity card.
On August 27, brothers Nawaf and Salem purchased flight tickets through Travelocity.com using Nawaf's visa card.
With the four other Flight 77 hijackers, he worked out at a Gold's Gym in Greenbelt, Maryland from September 2 to September 6 of the same year.
Attacks.
On September 11, 2001, Hazmi boarded American Airlines Flight 77. Airport surveillance video from Washington's Dulles Airport shows two of the five hijackers, including Salem al-Hazmi, being pulled aside to undergo additional scrutiny after setting off metal detectors.
The flight was scheduled to depart at 08:10, but ended up departing 10 minutes late from Gate D26 at Dulles. The last normal radio communications from the aircraft to air traffic control occurred at 08:50:51. At 08:54, Flight 77 began to deviate from its normal, assigned flight path and turned south, and then hijackers set the flight's autopilot heading for Washington, D.C. Passenger Barbara Olson called her husband, United States Solicitor General Theodore Olson, and reported that the plane had been hijacked and that the assailants had box cutters and knives. At 09:37, American Airlines Flight 77 crashed into the west facade of the Pentagon, killing all 64 aboard (including the hijackers), along with 125 on the ground in the Pentagon. In the recovery process at the Pentagon, remains of all five Flight 77 hijackers were identified through a process of elimination, as not matching any DNA samples for the victims, and put into custody of the FBI. Forensics teams confirmed that it seemed two of the hijackers were brothers, based on their DNA similarities.
Mistaken identity.
Shortly after the attacks, several sources reported that Salem al-Hazmi, 26, was alive and working at a petrochemical plant in Yanbu, Saudi Arabia. He claimed that his passport had been stolen by a pickpocket in Cairo three years before, and that the pictures and details such as date of birth released to the public by the FBI were his own. He also stated that he had never visited the United States, but volunteered to fly to the U.S. to prove his innocence. On September 19, "Al-Sharq Al-Awsat" published his photograph alongside Badr Alhazmi's, whom they claimed was the actual hijacker who had stolen his identity.
Muhammad Salim al-Hazmi, father of the two suspects, Nawaf and Salim Muhammad al-Hazmi, said that the published photos may be doctored or faked somehow. Hazmi continued, "As a father, I have a feeling that the two of them are still alive and unhurt, and will come back home in the near future when the truth is uncovered and the real culprits are found."
After some confusion and doubt Saudi Arabia admitted that in fact the names of the hijackers were correct. "The names that we got confirmed that," Interior Minister Prince Nayef said in an interview with The Associated Press. "Their families have been notified." Nayef said the Saudi leadership was shocked to learn 15 of the hijackers were from Saudi Arabia and said it was natural that the kingdom had not noticed their involvement beforehand.

</doc>
<doc id="28163" url="http://en.wikipedia.org/wiki?curid=28163" title="Sagitta">
Sagitta

Sagitta is a constellation. Its name is Latin for "arrow", and it should not be confused with the larger constellation Sagittarius, the archer. Although Sagitta is an ancient constellation, it has no star brighter than 3rd magnitude and has the third-smallest area of all constellations (only Equuleus and Crux are smaller). It was included among the 48 constellations listed by the 2nd century astronomer Ptolemy, and it remains one of the 88 modern constellations defined by the International Astronomical Union. Located to the north of the equator, Sagitta can be seen from every location on Earth except within the Antarctic circle.
The red giant Gamma Sagittae is the constellation's brightest star, with an apparent magnitude of 3.47. Two star systems have been found to have planets.
Characteristics.
Covering 79.9 square degrees and hence 0.194% of the sky, Sagitta ranks 86th of the 88 modern constellations by area. Its position in the Northern Celestial Hemisphere means that the whole constellation is visible to observers north of 69°S. It is bordered by Vulpecula to the north, Hercules to the west, Aquila to the south, and Delphinus to the east. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'Sge'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of twelve segments ("illustrated in infobox"). In the equatorial coordinate system, the right ascension coordinates of these borders lie between 18h 57.2m and 20h 20.5m, while the declination coordinates are between 16.08° and 21.64°.
Notable features.
Stars.
Johann Bayer gave Bayer designations to eight stars, labelling them Alpha to Theta. What was viewed by Bayer, Friedrich Wilhelm Argelander and Heis as a single star Theta was in fact three stars, and is now equated to HR 7705. John Flamsteed added the letters x (mistaken as Chi), y and z to 13, 14 and 15 Sagittae in his Catalogus Britannicus. All three were dropped by Bevis and Baily. 
In his "Uranometria", Bayer depicted Alpha, Beta and Epsilon Sagittae as the fins of the arrow. Also known as Sham, Alpha is a yellow bright giant star of spectral class G1 II (with an apparent magnitude of 4.38, which lies at a distance of 430 ± 10 light-years from Earth. Originally 4 times as massive as the Sun, it has swollen and brightened to 20 times its diameter and 340 times its luminosity. Also of magnitude 4.38, Beta is a G-type giant located 440 ± 20 light-years distant from Earth. Epsilon Sagittae is G8 III, 5.66m, multiple star (4 components; component B is optical)
Ptolemy saw the constellation's brightest star Gamma Sagittae as marking the arrow's head, while Bayer saw Gamma, Eta and Theta as depicting the arrow's shaft. Gamma Sagittae is a red giant of spectral type M0III, and magnitude 3.47. It lies at a distance of 258 ± 4 light-years from Earth. Eta Sagittae is a star of spectral class K2 III with a magnitude of 5.1, which belongs to the Hyades Stream. Theta Sagittae is a multiple star system. 
Delta and Zeta depicted the spike according to Bayer. Delta Sagittae is a suspected visual double - M2 II+A0 V probably single image, composite spectrum), magnitude 3.82. Zeta Sagittae is a triple system, approximately 326 light-years from Earth, the primary an A-type star.
FG Sagittae is a remote highly luminous star around 8000 light years distant from Earth. WR 124 is a Wolf-Rayet star moving at great speed surrounded by a nebula of ejected gas.
R Sagittae is a member of the rare RV Tauri variable class of star. It ranges in magnitude from 8.2 to 10.4.
S Sagittae is a Classical Cepheid variable that varies from magnitude 5.24 to 6.04 in 8.38 days. It is a yellow-white supergiant that varies between spectral types F6Ib and G5Ib. Around 6 or 7 times as massive and 3500 time as luminous as the Sun, it is located around 2300 light-years away from Earth.
Located near 18 Sagittae is V Sagittae.
WZ Sagittae is a dwarf nova. The black widow pulsar (B1957+20) is a millisecond pulsar with a brown dwarf companion.
HD 231701 is a yellow-white main sequence star hotter and larger than our Sun, which was found to have a planet in 2007. 15 Sagittae is a solar analog that has a brown dwarf substellar companion.
Deep-sky objects.
Messier 71 is a very loose globular cluster mistaken for quite some time for a dense open cluster. It lies at a distance of about 13,000 light-years from Earth and was first discovered by the French astronomer Philippe Loys de Chéseaux in the year 1745 or 1746.
History.
The Greeks who may have originally identified this constellation called it "Oistos". The Romans named it Sagitta.
Mythology.
Sagitta's shape is reminiscent of an arrow, and many cultures have interpreted it thus, among them the Persians, Hebrews, Greeks and Romans. The Arabs called it "as-Sahm", a name that was transferred "Sham" and now refers to α Sge only.
Ancient Greece.
In ancient Greece, Sagitta was regarded as the weapon that Hercules used to kill the eagle (Aquila) of Jove that perpetually gnawed Prometheus' liver. The Arrow is located beyond the north border of Aquila, the Eagle. Others believe the Arrow to be the one shot by Hercules towards the adjacent Stymphalian birds (6th labor) who had claws, beaks and wings of iron, and who lived on human flesh in the marshes of Arcadia - Aquila the Eagle and Cygnus the Swan, and the Vulture - and still lying between them, whence the title Herculea. Eratosthenes claimed it as the arrow with which Apollo exterminated the Cyclopes.
External links.
Coordinates: 

</doc>
<doc id="28176" url="http://en.wikipedia.org/wiki?curid=28176" title="Willis Tower">
Willis Tower

The Willis Tower, built as and still commonly referred to as Sears Tower, is a 108-story, 1451 ft skyscraper in Chicago, Illinois, United States. At completion in 1973, it surpassed the World Trade Center towers in New York to become the tallest building in the world, a title it held for nearly 25 years. The Willis Tower is the second-tallest building in the United States and the 12th-tallest in the world. More than one million people visit its observation deck each year, making it one of Chicago's most popular tourist destinations. The structure was renamed in 2009 by the Willis Group as part of its lease on a portion of the tower's space.
s of 2013[ [update]], the building's largest tenant is United Airlines, which moved its corporate headquarters from the United Building at 77 West Wacker Drive in 2012 and today occupies around 20 floors with its headquarters and operations center.
The building's official address is 233 South Wacker Drive, Chicago, Illinois 60606.
History.
Planning and construction.
In 1969, Sears, Roebuck & Co. was the largest retailer in the world, with about 350,000 employees. Sears executives decided to consolidate the thousands of employees in offices distributed throughout the Chicago area into one building on the western edge of Chicago's Loop. Sears asked its outside counsel, Arnstein, Gluck, Weitzenfeld & Minow (now known as Arnstein & Lehr, LLP) to suggest a location. The firm consulted with local and federal authorities and the applicable law, then offered Sears two options: an area known as Goose Island and a two-block area bounded by Franklin Street on the east, Jackson Boulevard on the south, Wacker Drive on the west and Adams Street on the north, with Quincy Street running through the middle from east to west.
This latter site was decided upon, and preliminary inquiries determined that the necessary permits could be obtained and Quincy Street vacated. The next step was to acquire the property; a team of attorneys from the Arnstein law firm, headed by Andrew Adsit, began buying the property parcel by parcel. Sears purchased 15 old buildings from 100 owners and paid $2.7 million to the City of Chicago for the portion of Quincy Street that divided the property.
Sears, which needed 3000000 sqft of office space for its planned consolidation and predicted that growth would require yet more, commissioned architects Skidmore, Owings & Merrill (SOM) to produce a structure to be one of the largest office buildings in the world. Their team of architect Bruce Graham and structural engineer Fazlur Rahman Khan designed the building as nine square "tubes" (each essentially a separate building), clustered in a 3×3 matrix forming a square base with 225 ft sides. All nine tubes would rise up to the 50th floor of the building. At the 50th floor, the northwest and southeast tubes end, and the remaining seven continue up. At the 66th floor, the northeast and the southwest tubes end. At the 90th floor, the north, east, and south tubes end. The remaining west and center tubes continue up to the 108th floor.
The Willis Tower was the first building to use Khan's bundled tube structure. This innovative design was structurally efficient and economic: at 1,450 feet, it provided more space and rose higher than the Empire State Building, yet cost much less per unit area. This structural system would prove highly influential in skyscraper construction. It has been used in most supertall buildings since then, including the world's tallest building, the Burj Khalifa. To honor Khan's contributions, the Structural Engineers Association of Illinois commissioned a sculpture of him for the lobby of the Willis Tower.
Sears executives decided that the space they would immediately occupy should be efficiently designed to house their Merchandise Group, and that floor space for future growth would be rented out to smaller firms and businesses until Sears could retake it. The latter floor areas had to be designed to a smaller plate, with a high window-space to floor-space ratio, to be attractive and marketable to prospective lessees. Smaller floorplates required a taller structure to yield sufficient square footage. Skidmore architects proposed a tower with large 55000 sqft floors in the lower part of the building, and gradually tapered areas of floorplates in a series of setbacks, which would give the Sears Tower its distinctive look.
As Sears continued to offer optimistic projections for growth, the tower's proposed floor count rapidly increased into the low hundreds, surpassing the height of New York's unfinished World Trade Center to become the world's tallest building. The height was restricted by a limit imposed by the Federal Aviation Administration (FAA) to protect air traffic. The financing of the tower was provided by the Sears company. It was topped with two antennas to permit local television and radio broadcasts. Sears and the City of Chicago approved the design, and the first steel was put in place in April 1971. The structure was completed in May 1973. The construction cost about US$150 million at the time, equivalent to $ million in 2015. By comparison, Taipei 101, built in 2004 in Taiwan, cost around the equivalent of US$2.14 billion in 2014 dollars.
Black bands appear on the tower around the 29th–32nd, 64th–65th, 88th–89th, and 104th–108th floors. These are louvres that allow ventilation for service equipment and obscure the structure's belt trusses. Even though regulations did not require a fire sprinkler system, the building was equipped with one from the beginning. There are around 40,000 sprinkler heads in the building, installed at a cost of $4 million.
In February 1982, two television antennas were added to the structure, increasing its total height to 1707 ft. The western antenna was later extended, bringing the overall height to 1730 ft on June 5, 2000 to improve reception of local NBC station WMAQ-TV.
Suits filed to halt construction.
As the construction of the building neared the 50th floor, lawsuits for an injunction were filed seeking to stop the building from exceeding 67 floors. The suits alleged that above that point television reception would deteriorate and cause property values to plummet. The first suit was filed by the State's Attorney in neighboring Lake County on March 17, 1972. A second suit was filed on March 28 in the Cook County Circuit Court by the Villages of Skokie, Northbrook and Deerfield, Illinois.
Sears filed motions to dismiss the Lake County and the Cook County lawsuits and on May 17, 1972, Judge LaVerne Dickson, Chief of the Lake County Circuit Court dismissed the suit, saying, "I find nothing that gives television viewers the right to reception without interference. They will have to find some other means of ensuring reception such as taller antennas." The Lake County State's Attorney filed a Notice of Appeal and the Supreme Court agreed to permit bypassing the appellate court and to hear the matter on an expedited basis. The State's Attorney then asked the Illinois Supreme Court for a temporary injunction to stop the construction and his request was denied. On June 12, Judge Charles R. Barrett granted Sears' motion to dismiss the suit filed by three Chicago suburbs on the ground that interference with television reception caused by construction of the Sears building did not violate constitutional rights and that the suburbs involved in the suit do not have any right to undistorted television reception. This decision, too, also was appealed and consolidated with the Lake County appeal with the Supreme Court of Illinois.
Meanwhile, an Illinois Citizens Committee for Broadcasting requested the Federal Communications Commission to halt construction on the grounds so that it would not interfere with area television reception. On May 26, 1972, the Commission declined to take action on the ground that it did not have jurisdiction to do so.
On June 30, 1972, the Illinois Supreme Court affirmed the previous rulings by Lake and Cook County Circuit Courts, by a letter order with a written opinion to follow. On September 8, 1972, the United States Court of Appeals for the Seventh Circuit upheld the decision by the Federal Communications Commission to dismiss the complaint brought by the Illinois Citizens Committee for Broadcasting charging that the building would drastically affect reception in Chicago land and requesting the FCC to halt construction. The Supreme Court of Illinois written opinion was filed on September 20, 1972. In affirming the judgments of lower courts the Court held, "Considering the foregoing, it is clear to us that absent legislation to the contrary defendant has a propriety right to construct a building to its desired height and that completion of the project would not constitute a nuisance under the circumstances of this case." 
Post-opening.
Sears' optimistic growth projections were not met. Competition from its traditional rivals (like Montgomery Ward) continued, with new competition by retailing giants such as Kmart, Kohl's, and Walmart. The fortunes of Sears & Roebuck declined in the 1970s as the company lost market share; its management grew more cautious. Nor did the Sears Tower draw as many tenants as Sears had hoped. The tower stood half-vacant for a decade as a surplus of office space was erected in Chicago in the 1980s.
In 1990, the law firm of Keck, Mahin & Cate decided to move out of its space in the Sears Tower and into a development that would become 77 West Wacker Drive, rebuffing Sears' attempts to entice the firm to stay. Two years later, Sears began moving its own offices out of the Sears Tower.
In 1994, Sears sold the building to Boston-based AEW Capital Management, with financing from MetLife. At the time, it was one-third vacant. By 1995, Sears had completely left the building, moving to a new office campus in Hoffman Estates, Illinois.
In 1997, Toronto-based TrizecHahn Corporation (the owner of the CN Tower at the time) purchased the building for $110 million, and assumption of $4 million in liabilities, and a $734 million mortgage. In 2003, Trizec surrendered the building to lender MetLife.
In 2004, MetLife sold the building to a group of investors, including New York-based Joseph Chetrit, Joseph Moinian, Lloyd Goldman, Joseph Cayre and Jeffrey Feil, and Skokie, Illinois-based American Landmark Properties. The quoted price was $840 million, with $825 million held in a mortgage.
In June 2006, seven men were arrested by the FBI and charged with plotting to destroy the tower. Deputy FBI Director John Pistole described their plot as "more aspirational than operational". The case went to court in October 2007; after three trials, five of the suspects were convicted and two were acquitted. The alleged leader of the group, Narseal Batiste, was sentenced to 13½ years in prison in November 2009.
Plans.
In February 2009, the owners announced they were considering a plan to paint the structure silver; this plan was later dropped. The paint would have "rebranded" the building and highlighted its advances in energy efficiency. The estimated cost was $50 million.
Since 2007, the building owners have been considering building a hotel on the north side of Jackson, between Wacker and Franklin, at the plaza that is the entrance to the tower's observation deck. The tower's parking garage is beneath the plaza. Building owners say the second building was considered in the original design. The plan was eventually cancelled as city zoning does not permit construction of such a tall tower there.
Although Sears' naming rights expired in 2003, the building continued to be called the Sears Tower for several years. In March 2009, London-based insurance broker Willis Group Holdings agreed to lease a portion of the building, and obtained the building's naming rights. On July 16, 2009, the building was officially renamed Willis Tower. On August 13, 2012, United Airlines announced it would move its corporate headquarters from 77 West Wacker Drive to Willis Tower.
Skydeck.
The Willis Tower observation deck, called the Skydeck, opened on June 22, 1974. Located on the 103rd floor of the tower, it is 1353 ft high and is one of the most famous tourist attractions in Chicago. Tourists can experience how the building sways on a windy day. They can see far over the plains of Illinois and across Lake Michigan to Indiana, Michigan and Wisconsin on a clear day. Elevators take tourists to the top in about 60 seconds, and allow tourists to feel the pressure change as they rise up. The Skydeck competes with the John Hancock Center's observation floor a mile and a half away, which is 323 ft lower. Some 1.3 million tourists visit the Skydeck annually. A second Skydeck on the 99th floor is also used if the 103rd floor is closed. The tourist entrance can be found on the south side of the building along Jackson Boulevard.
In January 2009, Willis Tower's owners began a major renovation of the Skydeck, including the installation of retractable glass balconies, which can be extended approximately 4 ft from the facade of the 103rd floor, overlooking South Wacker Drive. The all-glass boxes, informally dubbed "The Ledge", allow visitors to look through the glass floor to the street 1353 ft below. The boxes, which can bear 5 ST of weight, opened to the public on July 2, 2009. However, on May 29, 2014 the laminated glass covering the floor of one of the glass boxes shattered while visitors were sitting on it, but caused no injuries. The broken glass was replaced within days, and tourist operations resumed as before.
Panorama of Chicago skyline as seen from Willis Tower Skydeck
Height.
Willis Tower remains the second tallest building in the Americas (after One World Trade Center) and the Western Hemisphere. With a pinnacle height of 1729 ft, it is the third tallest freestanding structure in the Americas, as it is 86 ft shorter than Toronto's CN Tower. Willis Tower is the eighth-tallest freestanding structure in the world by pinnacle height.
At 1482.6 ft tall, including decorative spires, the Petronas Twin Towers in Kuala Lumpur, Malaysia, laid claim to replacing the Sears Tower as the tallest building in the world in 1998. Not everyone agreed, and in the ensuing controversy four different categories of "tallest building" were created. Of these, Petronas was the tallest in the first category (height to top of architectural elements, meaning spires but not antennas) giving it the title of world's tallest building.
Taipei 101 in Taiwan claimed the record in three of the four categories in 2004 to become recognized as the tallest building in the world. Taipei 101 surpassed the Petronas Twin Towers in spire height and the Sears Tower in roof height and highest occupied floor. The Sears Tower retained one record: its antenna exceeded Taipei 101's spire in height. In 2008, the Shanghai World Financial Center claimed the records of tallest building by roof and highest occupied floor.
On August 12, 2007, the Burj Khalifa in Dubai, United Arab Emirates was reported by its developers to have surpassed the Sears Tower in all height categories.
Upon completion, One World Trade Center in New York City surpassed Willis Tower through its structural and pinnacle heights, but not by roof, observation deck elevation or highest occupied floor.
Until 2000, the Sears Tower did not hold the record for the tallest building by pinnacle height. From 1969 to 1978, this record was held by the John Hancock Center, whose antenna reached a height of 1500 ft, or 49 ft taller than the Sears Tower's original height of 1451 ft. In 1978, One World Trade Center became taller by pinnacle height due to the addition of a 359 ft antenna, which brought its total height to 1727 ft. In 1982, two antennas were installed on top of the Sears Tower which brought its total height to 1707 ft, making it taller than the John Hancock Center but not One World Trade Center. However, the extension of the Sears Tower's western antenna in June 2000 to 1730 ft allowed it to just barely claim the title of tallest building by pinnacle height.
Climbing.
On May 25, 1981, Dan Goodwin, wearing a homemade Spider-Man suit while using suction cups, camming devices, and sky hooks, and despite several attempts by the Chicago Fire Department to stop him, made the first successful outside ascent of the Sears Tower. Goodwin was arrested at the top after the seven-hour climb and charged with trespassing. Goodwin stated that the reason he made the climb was to call attention to shortcomings in high-rise rescue and firefighting techniques. After a lengthy interrogation by Chicago's District Attorney and Fire Commissioner, Goodwin was released.
In August 1999, French urban climber Alain "Spiderman" Robert, using only his bare hands and bare feet, scaled the building's exterior glass and steel wall all the way to the top. A thick fog settled in near the end of his climb, making the last 20 stories of the building's glass and steel exterior slippery.
Naming rights.
Although Sears sold the Tower in 1994 and had completely vacated it by 1995, the company retained the naming rights to the building through 2003. The new owners were rebuffed in renaming deals with CDW Corp in 2005 and the U.S. Olympic Committee in 2008. London-based insurance broker Willis Group Holdings, Ltd. leased more than 140000 sqft of space on three floors in 2009. A Willis spokesman said the naming rights were obtained as part of the negotiations at no cost to Willis, and the building was renamed Willis Tower on July 16, 2009.
The naming rights are valid for 15 years, so it is possible that the building's name could change again in 2024 or later. The "Chicago Tribune" joked that the building's new name reminded them of the oft-repeated "What you talkin' 'bout, Willis?" catchphrase from the 1980s American television sitcom "Diff'rent Strokes" and considered the name-change ill-advised in "a city with a deep appreciation of tradition and a healthy ego, where some Chicagoans still mourn the switch from Marshall Field's to Macy's". This feeling was confirmed in a July 16, 2009 CNN article in which some Chicago area residents expressed reluctance to accept the Willis Tower name, and in an article that appeared in the October 2010 issue of "Chicago" magazine that ranked the building among Chicago's 40 most important, the author pointedly refused to acknowledge the name change and referred to the building as the "Sears Tower". "Time" magazine called the name change one of the top 10 worst corporate name changes and pointed to negative press coverage by local news outlets and online petitions from angry residents. The naming rights issue continued into 2013, when Eric Zorn noted in the "Chicago Tribune" that "We're stubborn about such things. This month marked four years since the former Sears Tower was re-christened Willis Tower, and the new name has yet to stick."
Broadcasting.
Many broadcast station transmitters are located at the top of Willis Tower. Each list is ranked by height from the top down. Stations at the same height on the same mast indicate the use of a diplexer into the same shared antenna. Due to its extreme height, FM stations (all class B) are very limited in power output.
Radio stations.
Also, NOAA Weather Radio station KWO39 transmits off the top of Willis Tower, at 162.550 MHz. KWO39, programmed by the National Weather Service Weather Forecast Office in Chicago, is equipped with Specific Area Message Encoding (SAME), which sets off a siren on specially-programmed weather radios to alert of an impending hazard, such as a tornado or civil emergency.
Cultural depictions.
Film and television.
The building has appeared in numerous films and television shows set in Chicago such as "Ferris Bueller's Day Off", where Ferris and company watch the streets of Chicago from the observation deck. The television show "Late Night with Conan O'Brien" introduced a character called The Sears Tower Dressed In Sears Clothing when the show visited Chicago in 2006. The building is also featured in History Channel's "Life After People", in which it and other human-made landmarks suffer from neglect without humans around, and it collapses two hundred years after people are gone. In an episode of the television series "Monk", Adrian Monk tries to conquer his fear of heights by imagining that he is on top of the Sears Tower. Also, in an episode of "Kenan and Kel", Kenan Rockmore and Kel Kimble decide to climb to the top of the Sears Tower, so that Kenan can declare his love for a girl.
In the movie "", the tower is damaged by a tornado.
In "1969", a Season 2 episode of the science-fiction series "Stargate SG-1", the SG-1 team accidentally travels back in time to the titular year. At one point, the team travels though Chicago and the Sears Tower is shown (erroneously, since construction did not begin on the tower until two years later in 1971).
In the 2004 film "I, Robot", the tower is shown updated in the year 2035 with new triangular antennas. The tower is shown surpassed in height by the USR (United States Robotics) Building.
In the 2011 film "", the tower is featured in a number of scenes. The most notable one is when the N.E.S.T team tries to enter the city using V-22 Osprey helicopters. They use Willis Tower for cover before using wing suits to descend into the city streets. In the movie, the tower is shown to be severely damaged by the Decepticon invasion of the city.
In the 2013 film "Man of Steel", the tower's interior and parts of its exterior portrayed the offices of the "Daily Planet".
In the film "Divergent", the tower is shown abandoned and decayed in a future Chicago.
Other.
Older versions of "Microsoft Flight Simulator" would begin with the player on the runway of Meigs Field, facing a virtual version of the tower.
In Sufjan Stevens' 2005 album "Illinois", the tower is referenced in the track "Seer's Tower", whose title is a play on the tower's now-former name, Sears Tower.

</doc>
<doc id="28236" url="http://en.wikipedia.org/wiki?curid=28236" title="Space Shuttle Enterprise">
Space Shuttle Enterprise

The Space Shuttle "Enterprise" (NASA Orbiter Vehicle Designation: OV-101) was the first Space Shuttle. It was built for NASA as part of the Space Shuttle program to perform test flights in the atmosphere, aided by a modified Boeing 747. It was constructed without engines or a functional heat shield, and was therefore not capable of spaceflight. It was unveiled on September 17, 1976.
Originally, "Enterprise" had been intended to be refitted for orbital flight to become the second space-rated orbiter in service. However, during the construction of "Columbia", details of the final design changed, particularly with regard to the weight of the fuselage and wings. Refitting "Enterprise" for spaceflight would have involved dismantling the orbiter and returning the sections to subcontractors across the country. As this was an expensive proposition, it was determined to be less costly to build "Challenger" around a body frame (STA-099) that had been built as a test article. Similarly, "Enterprise" was considered for refit to replace "Challenger" after the latter was destroyed, but "Endeavour" was built from structural spares instead.
Differences between "Enterprise" and future shuttles.
The design of OV-101 was not the same as that planned for OV-102, the first flight model; the aft fuselage was constructed differently, and it did not have the interfaces to mount OMS pods. A large number of subsystems—ranging from main engines to radar equipment—were not installed on "Enterprise", but the capacity to add them in the future was retained, as NASA originally intended to refit the orbiter for spaceflight at the conclusion of its testing. Instead of a thermal protection system, its surface was primarily covered with simulated tiles made from polyurethane foam. Fiberglass was used for the leading edge panels in place of the reinforced carbon-carbon ones of spaceflight-worthy orbiters. Only a few sample thermal tiles and some Nomex blankets were real. "Enterprise" used fuel cells to generate its electrical power, but these were not sufficient to power the orbiter for spaceflight.
"Enterprise" also lacked RCS thrusters (which were useless in atmospheric flight) and hydraulic mechanisms for the landing gear; the landing gear doors were simply opened through the use of explosive bolts and the gear dropped down solely by gravity. As it was only used for atmospheric testing, "Enterprise" featured a large air spike mounted on its nose cap which had been used on the U-2 spy plane.
"Enterprise" was equipped with Lockheed-manufactured zero-zero ejection seats like those its sister "Columbia" had carried on its first four missions.
Service.
Construction began on "Enterprise" on June 4, 1974. Designated OV-101, it was originally planned to be named "Constitution" and unveiled on Constitution Day, September 17, 1976. A letter-writing campaign by fans to President Gerald Ford asked that the orbiter be named after the Starship "Enterprise". Although Ford did not mention the campaign, the president, saying he was "partial to the name" "Enterprise", directed NASA officials to change the name.
In mid-1976 the orbiter was used for ground vibration tests, allowing engineers to compare data from an actual flight vehicle with theoretical models.
On September 17, 1976, "Enterprise" was rolled out of Rockwell's plant at Palmdale, California. In recognition of its fictional namesake, "Star Trek" creator Gene Roddenberry and most of the principal cast of the original series of "Star Trek" were on hand at the dedication ceremony.
Approach and landing tests (ALT).
On January 31, 1977, it was taken by road to Dryden Flight Research Center at Edwards Air Force Base, to begin operational testing.
While at NASA Dryden "Enterprise" was used by NASA for a variety of ground and flight tests intended to validate aspects of the shuttle program. The initial nine-month testing period was referred to by the acronym ALT, for "Approach and Landing Test". These tests included a maiden "flight" on February 18, 1977, atop a Boeing 747 Shuttle Carrier Aircraft (SCA) to measure structural loads and ground handling and braking characteristics of the mated system. Ground tests of all orbiter subsystems were carried out to verify functionality prior to atmospheric flight.
The mated "Enterprise"/SCA combination was then subjected to five test flights with "Enterprise" unmanned and unactivated. The purpose of these test flights was to measure the flight characteristics of the mated combination. These tests were followed with three test flights with "Enterprise" manned to test the shuttle flight control systems.
On August 12, 1977, the space shuttle "Enterprise" flew on its own for the first time. "Enterprise" underwent four more free flights where the craft separated from the SCA and was landed under astronaut control. These tests verified the flight characteristics of the orbiter design and were carried out under several aerodynamic and weight configurations. The first three flights were flown with a tailcone placed at the end of "Enterprise's" aft fuselage, which reduced drag and turbulence when mated to the SCA. The final two flights saw the tailcone removed and mockup main engines installed. On the fifth and final glider flight, pilot-induced oscillation problems were revealed, which had to be addressed before the first orbital launch occurred.
Mated Vertical Ground Vibration Test (MGVT).
Following the conclusion of the ALT test flight program, on March 13, 1978, "Enterprise" was flown once again, but this time half way across the country to NASA's Marshall Space Flight Center (MSFC) in Alabama for the Mated Vertical Ground Vibration Testing (MGVT). The orbiter was lifted up on a sling very similar to the one used at Kennedy Space Center and placed inside the Dynamic Test Stand building, and there mated to the Vertical Mate Ground Vibration Test tank (VMGVT-ET), which in turn was attached to a set of inert Solid Rocket Boosters (SRB) to form a complete shuttle launch stack, and marked the first time in the program's history that all Space Shuttle elements, an Orbiter, an External Tank (ET), and two SRBs, were mated together. During the course of the program, "Enterprise" and the rest of the launch stack would be exposed to a punishing series of vibration tests simulating as closely as possible those expected during various phases of launch, some tests with and others without the SRBs in place.
Planned preparations for spaceflight.
At the conclusion of this testing, "Enterprise" was supposed to be taken back to Palmdale for retrofitting as a fully spaceflight capable vehicle. Under this arrangement, "Enterprise" would be launched on its maiden spaceflight in July 1981 to launch a communications satellite and retrieve the Long Duration Exposure Facility, then planned for a 1980 release on the first operational orbiter, "Columbia". Afterwards, "Enterprise" would conduct two Spacelab missions. However, in the period between the rollout of "Enterprise" and the rollout of "Columbia", a number of significant design changes had taken place that meant retrofitting the prototype would end up being a much more intensive and expensive process than previously realized. As a consequence, NASA took the option to convert the incomplete Structural Test Article, numbered STA-099, which had been built to undergo a variety of stress tests, into a full up flight worthy orbiter, which became "Challenger".
Preparation for STS-1.
Following the MGVT program and with the decision to not use "Enterprise" for orbital missions, it was ferried on April 10, 1979, to KSC. By June 1979, it was again mated with an external tank and solid rocket boosters (known as a boilerplate configuration) and tested in a launch configuration at Kennedy Space Center Launch Pad 39A for a series of fit checks of the facilities there.
Retirement.
With the completion of critical testing, "Enterprise" was returned to Rockwell's plant in Palmdale in October 1979 and was partially disassembled to allow certain components to be reused in other shuttles. After this period, "Enterprise" was returned to NASA's Dryden Flight Research Facility in September 1981. During 1983 and 1984, "Enterprise" underwent an international tour visiting France, Germany, Italy, the UK, Canada, and the American states of California, Alabama, and Louisiana (during the 1984 Louisiana World Exposition). It was also used to fit-check the never-used shuttle launch pad at Vandenberg AFB, California. Finally, on November 18, 1985, "Enterprise" was ferried to Washington, D.C., where it became property of the Smithsonian Institution.
Post-"Challenger".
After the "Challenger" disaster, NASA considered using "Enterprise" as a replacement. Refitting the shuttle with all of the necessary equipment for it to be used in space was considered, but NASA decided to use spares constructed at the same time as "Discovery" and "Atlantis" to build "Endeavour".
Post-"Columbia".
In 2003 after the breakup of "Columbia" during re-entry, the Columbia Accident Investigation Board conducted tests at Southwest Research Institute, which used an air gun to shoot foam blocks of similar size, mass and speed to that which struck "Columbia" at a test structure which mechanically replicated the orbiter wing leading edge. They removed a fiberglass panel from "Enterprise"'s wing to perform analysis of the material and attached it to the test structure, then shot a foam block at it. While the panel was not broken as a result of the test, the impact was enough to permanently deform a seal. Since the reinforced carbon-carbon (RCC) panel on "Columbia" had only 40% of the strength of the test panel from "Enterprise," this result suggested that the RCC leading edge would have been shattered. Additional tests on the fiberglass were canceled in order not to risk damaging the test apparatus, and a panel from "Discovery" was tested to determine the effects of the foam on a similarly-aged RCC leading edge. On July 7, 2003, a foam impact test created a hole 41 by in the protective RCC panel. The tests clearly demonstrated that a foam impact of the type "Columbia" sustained could seriously breach the protective RCC panels on the wing leading edge.
The board determined that the probable cause of the accident was that the foam impact caused a breach of a reinforced carbon-carbon panel along the leading edge of "Columbia's" left wing, allowing hot gases generated during re-entry to enter the wing and cause structural collapse. This caused "Columbia" to tumble out of control, breaking up with the loss of the entire crew.
Museum exhibit.
Washington, D.C..
"Enterprise" was stored at the Smithsonian's hangar at Washington Dulles International Airport before it was restored and moved to the newly built Smithsonian's National Air and Space Museum's Steven F. Udvar-Hazy Center at Dulles International Airport, where it was the centerpiece of the space collection. On April 12, 2011, NASA announced that Space Shuttle "Discovery", the most traveled orbiter in the fleet, would be added to the collection once the Shuttle fleet was retired. On April 17, 2012, "Discovery" was transported by Shuttle Carrier Aircraft to Dulles from Kennedy Space Center, where it made several passes over the Washington D.C. metro area.
New York.
On December 12, 2011, ownership of the "Enterprise" was officially transferred to the Intrepid Sea, Air & Space Museum in New York City. In preparation for the anticipated relocation, engineers evaluated the vehicle in early 2010 and determined that it was safe to fly on the Shuttle Carrier Aircraft once again. At approximately 9:40 am Eastern Daylight Time on April 27, 2012 "Enterprise" took off from Dulles International Airport en-route to a fly-by over the Hudson River, New York's JFK International Airport, the Statue of Liberty, the George Washington and Verrazano-Narrows Bridges, and several other landmarks in the city, in an approximately 45-minute "final tour". At 11:23 am Eastern Daylight Time "Enterprise" touched down at JFK International Airport.
The mobile Mate-Demate Device and cranes were transported from Dulles to the ramp at JFK and the shuttle was removed from the SCA overnight on May 12, 2012, placed on a specially designed flat bed trailer and returned to Hangar 12. On June 3 a Weeks Marine barge took Enterprise to Jersey City. The Shuttle sustained cosmetic damage to a wingtip when a gust of wind blew the barge towards a piling. It was hoisted June 6 onto the Intrepid Museum in Manhattan.
The "Enterprise" went on public display on July 19, 2012, at the Intrepid Museum's new Space Shuttle Pavilion, a temporary shelter consisting of a pressurized, air-supported fabric bubble constructed on the aft end of the carrier's flight deck.
On October 29, 2012, storm surges from Hurricane Sandy caused Pier 86, including the Intrepid Museum's visitor center, to flood, and knocked out the museum's electrical power and both backup generators. The loss of power caused the Space Shuttle Pavilion to deflate, and high winds from the hurricane caused the fabric of the Pavilion to tear and collapse around the orbiter. Minor damage was spotted on the vertical stabilizer of the orbiter, as a portion of the tail fin above the rudder/speedbrake had broken off. The broken section was recovered by museum staff. While the pavilion itself could not be replaced for some time in 2013, the museum erected scaffolding and sheeting around "Enterprise" to protect it from the environment.
By April 2013, the damage sustained to "Enterprise"'s vertical stabilizer had been fully repaired, and construction work on the structure for a new pavilion was under way. The exhibit was closed due to damage from Hurricane Sandy. The pavilion and exhibit reopened on July 10, 2013.
The "Enterprise" was listed on the National Register of Historic Places on March 13, 2013, reference number 13000071, in recognition of its role in the development of the Space Shuttle Program. The historic significance criteria are in space exploration, transportation, and engineering.
References.
 This article incorporates  from websites or documents of the .
</dl>

</doc>
<doc id="28343" url="http://en.wikipedia.org/wiki?curid=28343" title="South African Republic">
South African Republic

 |style="width:1.0em; padding:0 0 0 0.6em;"| - 
 |style="padding-left:0;text-align:left;"| 1857–1863
 |- class="mergedbottomrow"
 | style="width:1.0em; padding:0 0 0 0.6em;"|  -  ||style="padding-left:0;text-align:left;"| 1870 
 |  km² ( sq mi)
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |style="padding-left:0;text-align:left;"| 1870 est.
 |- class="mergedbottomrow"
 |colspan="2"| Density
 |style="white-space:nowrap;"| /km²  ( /sq mi)
 |  South Africa
The South African Republic (Dutch: "Zuid-Afrikaansche Republiek", ZAR), often referred to as the Transvaal and sometimes as the Republic of Transvaal, was an independent and internationally recognised country in Southern Africa from 1852 to 1902. The country defeated the British in what is often referred to as the First Boer War and remained independent until the end of the Second Boer War on 31 May 1902, when it was forced to surrender to the British. The territory of the ZAR became known after this war as the Transvaal Colony. After the outbreak of the First World War a small number of Boers staged the Maritz Rebellion and aligned themselves with the Central Powers in a failed gambit to regain independence.
Name and etymology.
Zuid-Afrikaansche Republiek (ZAR).
Constitutionally the name of the country was "Zuid-Afrikaansche Republiek" (South African Republic or ZAR). Many people also called the "ZAR Transvaal", in reference to the area over (or trans) the Vaal River including the British press and the press in Europe. In fact the name "Transvaal" was later so often used that later the British objected to the use of the real name (The South African Republic). The British pointed out that the Convention of Pretoria of 3 August 1881 referred to the 'Transvaal Territory' and that the Transvaal and the South African Republic did not have the same boundaries. However, in the London Convention dated 27 February 1884,:469–474 a subsequent treaty between Britain and the ZAR, Britain acquiesced and reverted to the use of the true name, "The South African Republic". 
Transvaal.
The name of the South African Republic was of such importance that on 1 September 1900 the British declared by special proclamation that the name of the South African Republic be changed:514 from "South African Republic" to "The Transvaal" and that the entire territory shall henceforth and forever be known as "The Transvaal". This proclamation was issued during the second boer war and whilst the ZAR was still an independent country.
On 31 May 1902, the Treaty of Vereeniging was signed, with the South African Government, Orange Free State Government and the British Government which also converted the ZAR into the Transvaal Colony. On 20 May 1903 an Inter Colonial Council:516 was established, to manage the colonies of the British Government. The name "Transvaal" was finally changed in 1994, when the ANC government broke up the Transvaal area and renamed the core, to "Gauteng".
History.
Early history.
In paleolithic times, between 2.2 to 3.3 million years ago, hominids lived within the geographic area of the ZAR. The earliest hominid bones, between 2.2 to 3.3 million years old, was discovered at Sterkfontein in 1994. In 1938 Paranthropus robustus bones were found at Kromdraai, and during 1947 several more examples of Australopithecus africanus were uncovered in Sterkfontein. The pastoral San People lived on the lands of Southern Africa and they were later joined by Bantu people from North Africa and then Europeans. The Bantu, who displaced the San, were themselves decimated and scattered by the internecine warfare known as the Mfecane which left most of the lands of the ZAR abandoned and vacant. The greater portion of the territory south of the twenty-second parallel of latitude was literally without any inhabitants. With the arrival of the Europeans and their defense of Matabele raids, the Bantu tribes inhabiting the mountains and deserts could settle on open country, make gardens and sleep in safety. The Europeans were masters and owners of the land, but in accordance with the ancient Dutch custom, they permitted each newly settled Bantu community to be governed by its own chief. The Europeans subjected the Bantu community to a kraal tax, which was fully accepted by the Bantu communities and loyally paid while the Europeans provided peace and security, according to historian George McCall Theal.:335
Formation.
The Zuid-Afrikaansche Republiek came into existence on 17 January 1852:357–359 when the United Kingdom signed the Sand River Convention treaty with about 40,000 Boer people, recognising their independence in the region to the north of the Vaal River.
The first president of the Zuid-Afrikaansche Republiek was Marthinus Wessel Pretorius, elected in 1857, son of Boer leader Andries Pretorius, who commanded the Boers to victory at the Battle of Blood River. The capital was established at Potchefstroom and later moved to Pretoria. The parliament was called the Volksraad and had 24 members.
Independence.
The South African Republic became fully independent on the 27 February 1884 when the London Convention was signed. The country independently also entered into various agreements with other foreign countries after that date. On 3 November 1884 the country signed a Postal convention with the government of the Cape Colony and later similarly with the Orange Free State.:477
Expansion.
On the November 1859:420–422 the independent Republic of Lijdenburg merged with the Zuid Afrikaansche Republiek. On 9 May 1887, burghers from the territories of Stellaland and Goosen:479 (sometimes referred to as Goshen) were granted rights to the ZAR franchise. On the 25th of July 1895 the burghers that took part in the battle at Zoutpansberg,:505 were granted citizenship of the ZAR.
Constitution and laws.
The constitution of the Zuid Afrikaansche Republiek has been referred to as legally interesting for its time. It contained provisions for the division between the political leadership and office bearers in government administration. The legal system consisted of higher and lower courts and had adopted a jury system. The laws were enforced by the South African Republic Police (Zuid-Afrikaansche Republiek Politie or ZARP) which were divided into Mounted Police (Rijdende Politie) and Foot Police. On 10 April 1902 the Magistrates Court powers were extended to increase the civil ceiling amounts and to expand criminal jurisdiction to include all criminal cases not punishable by death or banishment. Also established was a Municipal Government, Witwatersrand District court and the High Court of Transvaal.:515
Religion.
Initially the State and Church were not separated in the constitution of the ZAR. To be a citizen of the ZAR you had to be a member of the Dutch Reformed Church. In 1858 these clauses were altered in the constitution to allow for the Volksraad to approve other Dutch Christian churches.:358–359 The Reformed Church was approved by the Volksraad in 1858, which had the effect of allowing Paul Kruger, of the Gereformeerde Kerk to remain a citizen of the ZAR. The Bible itself was also often used to interpret the intention of legal documents. The Bible was also used to interpret a prisoner exchange agreement, reached in terms of the Sand River Convention, between a commando of the ZAR, led by Paul Kruger and a Commando of the Orange Free State. President Boshoff had issued a death sentence over two ZAR citizens, for treason. Paul Kruger argued with President Boshoff that the Bible said punishment does not mean a death sentence and at the prisoner exchange, it was agreed that the accused would be punished if found guilty. After double checking Commandant Paul Krugers Bible, President Boshoff commuted the sentences to lashes with a sjambok.
Citizenship.
Citizenship of the ZAR was legislated by the constitution as well as Law no 7 of 1882, as amended on 23 June 1890.:495 Citizenship was gained by being born in the republic or by naturalization. The voting age was 16 years. Persons not born in the Republic (Foreigners) could become citizens by taking the prescribed oath and procuring the letters of naturalization. The oath involved abandoning, discarding and renouncing all allegiance and subjugation towards foreign sovereignties and in particular their previous citizenship. Foreigners had to have been residing in the Republic for a period of two years, be of good character and have been accepted as member of the Dutch Reformed or Reformed Church. On 20 September 1893 the ZAR Constitution was amended so that two thirds of the Volksraad would have to agree to changes to the citizenship law. This proclamation, number 224, also changed Law no 7 with regards to voting.:501 All citizens who was born in the ZAR or whom obtained their franchise prior to 23 June 1890 would have the right to vote for both the first and second volksraad and in all other elections. Citizens who obtained their franchise through naturalization after 23 June 1890, would be able to vote in all elections, except those for the first Volksraad.
Racism.
The constitution promoted racialism as it treated European (white) people differently from Native (black) people. Although slavery was illegal in the constitution and foreigners (white and black) were both discriminated against, black foreigners had fewer rights than their white counterparts. Black and Asian foreigners could never become citizens of the ZAR, at this time in history, this was very similar to many other European countries as well as in the new world.
Discrimination on the basis of race was prevalent in the ZAR and black British subjects were forced to reside in ghettos outside cities with Asian and black races, whilst white races were free to live anywhere. One of the racist motivations often used by the ZAR Government was "sanitation and regard to public health necessitated that measure of segregation"
Language and culture.
Language.
The language spoken and written by the citizens of the ZAR was high Dutch. This high Dutch was carried over into the Transvaal Colony and later the Union of South Africa. Up to 1925, the high Dutch language was still in use. In fact there were four main languages: High Dutch, Low or South African Dutch (Afrikaans), High Afrikaans and English. After 1925, High Dutch was removed and only Afrikaans and English remained as official languages. 
On 3 October 1884 the Volksraad stated that they had reason to believe that in certain schools impure Dutch was being used. The Volksraad issued Proclamation 207 and compelled the Superintendent of Education to apply the language law enforcing the exclusive use of the Dutch Language.:477
On the 30 July 1888 the high Dutch language was declared the only language:481–482 to be used in the country, not only in government but also in schools, trade and general use. All other languages were declared "foreign". These changes to the ZAR laws made the use of all other foreign languages illegal in the ZAR. Use of any foreign language was subject to criminal penalty:483 and fine of 20 ZAR Pond for each offense. 
The British similarly had declared English to be the only language spoken in the Cape Colony some decades earlier to outlaw the Dutch Language. 
The discovery of gold in 1885 led to a major influx of foreigners. 
By 1896 the language of government and citizens remained Dutch but in many market places, shops and homes the English language was spoken.
Boer Wars.
War with Mapela and Makapaan, 1854.
Hendrik Potgieter was elected at the assembly of 1849, as Commandant General for life and it became necessary, to avoid strife, to appoint three commandants general, all possessing equal powers.:41 Commandant General A.W.J. Pretorius became Commandant General of the Potchefstroom and Rustenburg districts. 16 December 1852, Commandant General Potgieter passed away and his son, Piet Potgieter, was appointed in his stead as Commandant General of the Lydenburg and Zoutpansberg districts of the ZAR. There was some disputes over cattle which Mapela was raising on behalf of Potgieter and earlier Commandant Scholtz had confiscated a large amount of rifles, ammunition, rifle repair equipment and materials of war from the home of English missionary, Reverend Livingstone. Livingstone admitted to storing these for Secheli and by this he was acting in breach of the Sand River Convention of 1852, which prescribed that neither arms nor ammunition should be supplied to the natives.:40
In 1853, the brother of Hendrik Potgieter, Herman Potgieter was called to Mapela to come and cull the elephant population.:42 When Potgieter arrived, Maphela took Potgieter, his son, his groom and a few other burghers to show them where the elephants were. On the way, Mapela and hundreds of natives attacked the Potgieter party. They killed Andries Potgieter, the son of Herman Potgieter and then dragged Potgieter up a hill, where they proceeded to skin him alive. They stopped once they had torn the entrails from his body.:43 At the same time of these events, Makapaan attacked and killed an entire convoy of woman and children traveling to Pretoria. The two chiefs had concluded an agreement to murder all the Europeans in their respective districts:44 and to keep the cattle that they were raising for the Europeans. General Piet Potgieter set out with 100 men from Zoutpansberg and Commandant General Pretorius left Pretoria with 200 men. After the commandos met up, they first attacked Makapaan and the natives were driven back to their caves in the mountains where they lived before. The Boers held them at siege in their caves and eventually hundreds of women and children came out.
Orphan children of the native tribes were "ingeboekt" at "autorisatie voor den landrost" or translated into English, "booked in" strictly controlled by legal process, at appointed Boer families to look after them until they came of age.:47 (The administration was similar to the system of indentured workers, (which was simply another form of slavery) with the exception that children so registered had to be released at age 16) The commando would return all such children to the nearest landrost district, for registration and allocation to a Boer family. As there were slavers and other criminals dealing in children any burgher found in possession of an unregistered minor child was guilty of a criminal offense. These children were also often called "oorlams" in reference to being overly used to the Dutch culture, and in reference to a hand raised orphan sheep, or "hanslam". These children, even after their 16th birthday, and being free to come and go as they please, never re-connected with their own culture and own language and except for surviving and being cared for in terms of food and shelter, were basically forcefully divorced from their native tribe forever.
Among the casualties of this war was Commandant General Potgieter.:46 The natives were armed with rifles and were good shots. The general was killed by native sniper on the ridge of a trench and his body recovered by then commandant Paul Kruger whilst under heavy fire from the natives. What remained of the joint commando, now under command of General Pretorius focussed their attention on Mapela. By the time the commando had reached Mapela, the natives had fled. A few wagons, bloody clothes, chests and other goods were discovered at a kop near Mapela's town. Mapela and his soldiers escaped and with their rifles and ammunition intact and Mapela was only captured much later, in 1858.
Civil War, 1861–1864.
Commandant-General Schoeman did not accept the 20 September 1858 proclamation by the Volksraad, where the members of the Christelijk Gereformeerde Church, would be entitled to citizenship of the ZAR. Consequently, Paul Kruger was not accepted as a citizen and disallowed from political intercourse. Acting President van Rensburg called a special meeting of the general council of the Hervormde kerk (Dutch Reformed Church) which then voted in a special resolution to allow members of the Reformed Church access to the franchise.
Sekhukhune War of 1876.
In 1876, a war between the ZAR and the Bapedi broke out over cattle theft and land encroachment. The Volksraad declared war on the Pedi leader, Sekhukhune on 16 May 1876. The war only began in July 1876. The president of the ZAR, Burgers led an army of 2000 burghers and was joined by a strong force of Swazi warriors. The Swazis joined the war to aid Mampuru, who was ousted from his position of chieftain by Sekhukhune. One of the early battles occurred at Botsabelo Mission Station on 13 July 1876, against Johannes Dinkwanyane, who was Sekhukhune's brother. The Boer forces were led by Commandant Coetzee and accompanied by Swazi warriors. The Swazi warriors launched a surprise and successful attack while the Boers held back. Seeing this, the Swazis refused to hand over to the Boers any spoils from the battle, thereafter leaving and returning to Swaziland. Dinkwanyane's followers also surrendered after this campaign.
First Boer War, 1880–1881.
On 12 April 1877, Britain issued a proclamation called: "ANNEXATION OF THE S.A. REPUBLIC TO THE BRITISH EMPIRE":448–449 In the proclamation, the British claim that the country is unstable, ungovernable, bankrupt and facing civil war. The unsuccessful annexation did not suspend self-government and attempted to convert the ZAR into a British colony.:448–453
The Zuid Afrikaansche Republiek viewed this proclamation as an act of aggression,:454–455 and resisted. Instead of declaring war, the country decided to send a delegation to United Kingdom and the USA, to protest. This did not have any effect and the First Boer War formally broke out on 20 December 1880. The First Boer War was the first conflict since the American Revolution in which the British had been decisively defeated and forced to sign a peace treaty under unfavourable terms. It would see the introduction of the khaki uniform, marking the beginning of the end of the famous Redcoat. The Battle of Laing's Nek would be the last occasion where a British regiment carried its official regimental colours into battle. The Pretoria Convention of 1881 was signed on 3 August 1881 and ratified on 25 October 1881 by the Zuid Afrikaansche Republiek (where the Zuid-Afrikaansche Republiek is referred to by the name "Transvaal Territory"). The Pretoria Convention of 1881:456–457 was superseded in 1884 by the London Convention,:469–470 and in which the British suzerainty over the South African Republic, was relinquished. The British Government, in the London Convention, accepted the name of the country as The South African Republic. The convention was signed in duplicate, in London on 27 February 1884 by Hercules Robinson, S.JP. Kruger, S.J. Du Toit and N.J. Smit and later ratified by the South African Republic (Zuid-Afrikaansche Republiek) Volksraad.
In 1885 extremely rich gold reefs were discovered in the ZAR. The South African Republic burghers were farmers and not miners and much of the mining fell to immigrants. The immigrants were also referred to as "outlanders" (uitlanders). By 1897 the immigrants had invested over 300 000 000 British Pounds in the ZAR goldfields.
Second Boer War, 1899–1902.
Britain first attacked the independent country of South Africa in December 1895, the Jameson Raid. After that failed attack the British started building up massive amounts of troops and resources at the borders of the ZAR. Then they demanded voting rights for the 50,000 British nationals and the 10, 000 other nationals in South Africa, even though none of these nationals were at that time South African citizens. Kruger rejected the British demand and called for the withdrawal of British troops from the ZAR's borders. When the British refused, Kruger declared war against Britain. Britain received assistance from Australia, Canada and New Zealand as well as forces and citizens of colonies like the Colony of Natal and the Cape Colony.
The Second Boer War was a watershed for the British Army in particular and for the British Empire as a whole. The British used concentration camps where women and children were held without adequate food or medical care. The abhorrent conditions in these camps caused the death of 4,177 women and 22,074 children under sixteen; death rates were between 344 and 700 per 1000. There is #Redirect .
The Treaty of Vereeniging was signed on 31 May 1902. The treaty ended the existence of the ZAR and the Orange Free State as independent Boer republics and placed them within the British Empire. The Boers were promised eventual limited self-government, which was granted in 1906 and 1907. The Union of South Africa was established in 1910.
Political structure.
Divisions.
The country was divided into 17 districts: 
Flag.
The national flag of the ZAR featured three horizontal stripes of red, white, and blue (mirroring the Dutch national flag), with a vertical green stripe at the hoist, and was known as the "Vierkleur" (lit. "four colours"). The former national flag of South Africa (from 1927 to 1994) had, as part of a feature contained within its central white bar, a horizontal flag of the Transvaal Republic (ZAR).

</doc>
<doc id="28407" url="http://en.wikipedia.org/wiki?curid=28407" title="Stephen Báthory">
Stephen Báthory

Stephen Báthory (Hungarian: "Báthory István"; Polish: "Stefan Batory"; Lithuanian: "Steponas Batoras"; Romanian: "Ştefan Báthory"; 27 September 1533 – 12 December 1586) was Voivode of Transylvania (1571–76), Prince of Transylvania (1576–86), from 1576 Queen Anna Jagiellon's husband and "jure uxoris" King of Poland.
The son of Stephen VIII Báthory and a member of the Hungarian Báthory noble family, Báthory was a ruler of Transylvania in the 1570s, defeating another challenger for that title, Gáspár Bekes. In 1576 Báthory became the third elected king of Poland. He worked closely with chancellor Jan Zamoyski. The first years of his reign were focused on establishing power, defeating a fellow claimant to the throne, Maximilian II, Holy Roman Emperor, and quelling rebellions, most notably, the Danzig rebellion. He reigned only a decade, but is considered one of the most successful kings in Polish history, particularly in the realm of military history. His signal achievement was his victorious campaign in Livonia against Russia in the middle part of his reign, in which he repulsed a Russian invasion of Commonwealth borderlands and secured a highly favorable treaty of peace (the Peace of Jam Zapolski).
Life.
Stephen Báthory was born on 27 September 1533 in the castle at Somlyó, also known as Szilágysomlyó (today's Şimleu Silvaniei). He was the son of Stephen VIII Báthory (d. 1534) of the noble Hungarian Báthory family and his wife Catherine Telegdi. He had at least five siblings: two brothers and three sisters.
Little is known about his childhood. Around 1549-1550 he briefly visited Italy and likely spent a few months attending lectures at the Padua University. Upon his return, he joined the army of Ferdinand I, Holy Roman Emperor, and took part in his military struggle against the Turks. Some time after 1553 Báthory was captured by the Turks, and after Ferdinand I refused to pay his ransom, Báthory joined the opposing side, supporting John II Sigismund Zápolya in his struggle for power in the Eastern Hungarian Kingdom. As Zápolya's supporter, Báthory acted both as a feudal lord, military commander and a diplomat. During one of his trips to Vienna he was put under house arrest for two years. During this time Báthory fell out of favour at Zápolya's court, and his position was largely assumed by another Hungarian noble, Gáspár Bekes. Báthory briefly retired from politics, but he still wielded considerable influence and was seen as a possible successor to Zápolya.
After Zápolya's death in 1571, the Transylvanian estates elected Báthory Voivode of Transylvania. Bekes, supported by the Habsburgs, disputed his election, but by 1573 Báthory emerged victorious in the resulting civil war and drove Bekes out of Transylvania. He subsequently attempted to play the Ottomans and the Holy Roman Empire against one another in an attempt to strengthen the Transylvania position.
Elected king.
In 1572, the throne of the Polish-Lithuanian Commonwealth, at the time the largest and one of the most populous states in Europe, was vacated when King Sigismund II of Poland died without heirs. The Sejm was given the power to elect a new king, and did so by choosing Henry of France; Henry soon ascended the French throne and forfeited the Polish one by returning to France. Báthory decided to enter into the election; in the meantime he had to defeat another attempt by Bekes to challenge his authority in Transylvania, which he did by defeating Bekes at the Battle of Sinpaul.
On 12 December 1575, after an interregnum of roughly one and a half years, primate of Poland Jakub Uchański, representing a pro-Habsburg faction, declared Emperor Maximilian II as the new monarch. However, chancellor Jan Zamoyski and others opponents of Habsburgs persuaded many of the lesser nobility to demand a "Piast king", a Polish king. After a heated discussion, it was decided that Anna Jagiellon, sister of former King Sigismund II Augustus, should be elected King of Poland and marry Stephen Báthory. In January 1576 Báthory passed the mantle of Voivode of Transylvania to his brother Christopher Báthory and departed for Poland. On 1 May 1576 Báthory married Anna and was crowned King of Poland and Grand Duke of Lithuania. After ascending the Polish-Lithuanian throne, Báthory also began using the title of the Prince of Transylvania.
Establishing power.
Báthory's position was at first extremely difficult, as there was still some opposition to Báthory's election. Emperor Maximilian, insisting on his earlier election, fostered internal opposition and prepared to enforce his claim by military action. At first the representatives of the Grand Duchy of Lithuania refused to recognize Báthory as Grand Duke, and demanded concessions - that he return the estates of his wife Anne to the Lithuanian treasury, hold Sejm conventions in both Lithuania and Poland, and reserve the highest governmental official offices in Lithuania for Lithuanians. He accepted the conditions. In June Báthory was recognized as Grand Duke of Lithuania, Duke of Ruthenia and Samogitia.
With Lithuania secure, the other major region refusing to recognize his election was Prussia. Maximilian's sudden death improved Báthory's situation, but the city of Danzig (Gdańsk) still refused to recognize his election without significant concessions. The Hanseatic League city, bolstered by its immense wealth, fortifications, and the secret support of Maximilian, had supported the Emperor's election and decided not to recognize Báthory as legitimate ruler. The resulting conflict was known as the Danzig rebellion. Most armed opposition collapsed when the prolonged Siege of Danzig by Báthory's forces was lifted as an agreement was reached. The Danzig army was utterly defeated in a field battle on 17 April 1577. However, since Báthory's armies were unable to take the city by force, a compromise was reached. In exchange for some of Danzig's demands being favorably reviewed, the city recognised Báthory as ruler of Poland and paid the sum of 200,000 zlotys in gold as compensation. Tying up administration of the Commonwealth northern provinces, in February 1578 he acknowledged George Frederick as the ruler of Duchy of Prussia, receiving his feudal tribute.
Policies.
After securing control over Commonwealth, Báthory had a chance to devote himself to strengthening his authority, in which he was supported by his chancellor Jan Zamoyski, who would soon become one of the king's most trusted advisers. Báthory reorganised the judiciary by formation of legal tribunals (the Crown Tribunal in 1578 and the Lithuanian Tribunal in 1581). While this somewhat weakened the royal position, it was of little concern to Báthory, as the loss of power was not significant in the short term, and he was more concerned with the hereditary Hungarian throne. In exchange, the Sejm allowed him to raise taxes and push a number of reforms strengthening the military, including the establishment of the "piechota wybraniecka", an infantry formation composed of peasants. Many of his projects aimed to modernize the Commonwealth army, reforming it in a model of Hungarian troops of Transylvania. He also founded the Academy of Vilna, the third university in the Commonwealth, transforming a prior Jesuit college into a major university. He founded several other Jesuit colleges, and was active in propagating Catholicism, while at the same time being respectful of the Commonwealth policy of religious tolerance, issuing a number of decrees offering protection to Polish Jews, and denouncing any religious violence.
In external relations, Báthory sought peace through strong alliances. Though Báthory remained distrustful of the Habsburgs, he maintained the tradition of good relations that the Commonwealth had with its Western neighbor and confirmed past treaties between the Commonwealth and Holy Roman Empire with diplomatic missions received by Maximilian's successor, Rudolf II. The troublesome south-eastern border with the Ottoman Empire was temporarily quelled by truces signed in July 1577 and April 1579. The Sejm of January 1578 gathered in Warsaw was persuaded to grant Báthory subsidies for the inevitable war against Muscovy.
A number of his trusted advisers were Hungarian, and he remained interested in the Hungarian politics. He wished to recreate his native country into an independent, strong power, but the unfavorable international situation did not allow him to significantly advance any of his plans in that area. In addition to Hungarian, he was well versed in Latin, and spoke Italian and German; he never learned the Polish language.
In his personal life, he was described as rather frugal in his personal expenditures, with hunting and reading as his favorite pastimes.
War with Muscovy.
Before Báthory's election to the throne of the Commonwealth, Ivan the Terrible of Russia had begun encroaching on its sphere of interest in the northeast, eventually invading the Commonwealth borderlands in Livonia; the conflict would grow to involve a number of nearby powers (outside Russia and Poland-Lithuania, also Sweden, the Kingdom of Livonia and Denmark-Norway). Each of them was vying for control of Livonia, and the resulting conflict, lasting for several years, became known as the Livonian War. By 1577 Ivan was in control of most of the disputed territory, but his conquest was short-lived. In 1578 Commonwealth forces scored a number of victories in Liviona and begun pushing Ivan's forces back; this marked the turning point in the war. Báthory, together with his chancellor Zamoyski, led the army of the Commonwealth in a series of decisive campaigns taking Polotsk in 1579 and Velikiye Luki in 1580.
In 1581 Stephen penetrated once again into Russia and, on 22 August, laid siege to the city of Pskov. While the city held, on 13 December 1581 Ivan the Terrible began negotiations that concluded with the Truce of Jam Zapolski on 15 January 1582. The treaty was favorable to the Commonwealth, as Ivan ceded Polatsk, Veliz and most of the Duchy of Livonia in exchange for regaining Velikiye Luki and Nevel.
Final years.
In 1584 Báthory allowed Zamoyski to execute Samuel Zborowski, whose death sentence for treason and murder had been pending for roughly a decade. This political conflict between Báthory and the Zborowski family, framed as the clash between the monarch and the nobility, would be a major recurring controversy in internal Polish politics for many years. In external politics, Báthory was considering another war with Russia, but his plans were delayed to the lack of support from the Sejm, which refused to pass requested tax raises.
Báthory's health had been declining for several years. He died on 12 December 1586. A 1934 autopsy concluded that the cause of death was chronic kidney disease. He had no legitimate children, though contemporary rumours suggested he might have had several illegitimate children. None of these rumours have been confirmed by modern historians. His death was followed by an interregnum of one year. Maximilian II's son, Archduke Maximilian III, was elected king but was contested by the Swedish Sigismund III Vasa, who defeated Maximilian at the Byczyna and succeeded as ruler of the Commonwealth.
Remembrance.
Báthory actively promoted his own legend, sponsoring a number of works about his life and achievements, from historical treatises to poetry. In his lifetime, he was featured in the works of Jan Kochanowski, Mikołaj Sęp Szarzyński and many others. He became a recurring character in Polish poetry and literature and featured as a central figure in poems, novels and drama by Jakub Jasiński, Józef Ignacy Kraszewski, Julian Ursyn Niemcewicz, Henryk Rzewuski and others. He has been a subject of numerous paintings, both during his life and posthumously. Among the painters who took him as a subject were Jan Matejko and Stanisław Wyspiański.
A statue of Báthory by Giovanni Ferrari was raised in 1789 in Padua, Italy, sponsored by the last king of the Commonwealth, Stanisław August Poniatowski. Other monuments to him include one in the Łazienki Palace (1795 by Andrzej Le Brun) and one in Sniatyn (1904, destroyed in 1939). He was a patron of the Vilnius University (then known as the Stefan Batory University) and several units in the Polish Army from 1919 to 1939. His name was borne by two 20th-century passenger ships of the Polish Merchant Navy, the MS Batory and TSS Stefan Batory. In modern Poland, he is the namesake of the Batory Steelmill, a nongovernmental Stefan Batory Foundation, the Polish 9th Armored Cavalry Brigade, and numerous Polish streets and schools. One of the districts of the town of Chorzów is named after him.
Immediately after his death, he was not fondly remembered in the Commonwealth. Many nobles took his behavior in the Zborowski affair and his domestic policies as indicating an interest in curtailing the nobility's Golden Freedoms and establishing an absolute monarchy. His contemporaries were also rankled by his favoritism toward Hungarians over nationals of the Commonwealth. He was also remembered, more trivially, for his Hungarian-style cap and saber (szabla "batorówka").
His later resurgence in Polish memory and historiography can be traced to the 19th-century era of partitions of Poland, when the Polish state lost its independence. He was remembered for his military triumphs and praised as an effective ruler by many, including John Baptist Albertrandi, Jerzy Samuel Bandtkie, Michał Bobrzyński, Józef Szujski and others. Though some historians like Tadeusz Korzon, Joachim Lelewel and Jędrzej Moraczewski remained more reserved, in 1887 Wincenty Zakrzewski noted that Báthory is "the darling of both the Polish public opinion and Polish historians". During the interwar period in the Second Polish Republic he was a cult figure, often compared - with the government's approval - to the contemporary dictator of Poland, Józef Piłsudski. After the Second World War, in the communist People's Republic of Poland, he became more of a controversial figure, with historians more ready to question his internal politics and attachment to Hungary. Nonetheless his good image remained intact, reinforced by the positive views of a popular Polish historian of that period, Paweł Jasienica.

</doc>
<doc id="28427" url="http://en.wikipedia.org/wiki?curid=28427" title="Telephone switchboard">
Telephone switchboard

A telephone switchboard is a telecommunications system used in the public switched telephone network or in enterprises to interconnect circuits of telephones to establish telephone calls between the subscribers or users, or between other exchanges. The switchboard was an essential component of a manual telephone exchange, and was operated by one or more persons, called operators who either used electrical cords or switches to establish the connections.
The electromechanical automatic telephone exchange, invented by Almon Strowger in 1888, gradually replaced manual switchboards in central telephone exchanges starting in 1919 when the Bell System adopted automatic switching, but many manual branch exchanges remained operational during the last half of the 20th century in offices, hotels, or other enterprises. Later electronic devices and computer technology gave the operator access to an abundance of features. In modern businesses, a private branch exchange (PBX) often has an attendant console for the operator, or an auto-attendant, which bypasses the operator entirely.
Operation.
The switchboard is usually designed to accommodate the operator, who sits facing it. It has a high back panel, which consists of rows of female jacks, each jack designated and wired as a local extension of the switchboard (which serves an individual subscriber) or as an incoming or outgoing trunk line. The jack is also associated with a lamp.
On the table or desk area in front of the operator are columns of keys, lamps and cords. Each column consists of a front key and a rear key, a front lamp and a rear lamp, followed by a front cord and a rear cord, making up together a cord circuit. The front key is the "talk" key allowing the operator to speak with that particular cord pair. The rear key on older "manual" boards and PBXs is used to physically ring a telephone. On newer boards, the back key is used to collect (retrieve) money from coin telephones. Each of the keys has three positions: back, normal and forward. When a key is in the normal position an electrical talk path connects the front and rear cords. A key in the forward position (front key) connects the operator to the cord pair, and a key in the back position sends a ring signal out on the cord (on older manual exchanges). Each cord has a three-wire TRS phone connector: tip and ring for testing, ringing and voice; and a sleeve wire for busy signals.
When a call is received, a jack lamp lights on the back panel and the operator responds by placing the rear cord into the corresponding jack and throwing the front key forward. The operator then converses with the caller, who informs the operator to whom he or she would like to speak. If it is another extension, the operator places the front cord in the associated jack and pulls the front key backwards to ring the called party. After connecting, the operator leaves both cords "up" with the keys in the normal position so the parties can converse. The supervision lamps light to alert the operator when the parties finish their conversation and go on-hook. Either party could "flash" the operator's supervision lamps by depressing their switch hook for a second and releasing it, in case they needed assistance with a problem. When the operator pulls down a cord, a pulley weight behind the switchboard pulls it down to prevent it from tangling.
On a trunk, on-hook and off-hook signals must pass in both directions. In a one-way trunk, the originating or A board sends a short for off-hook, and an open for on-hook, while the terminating or B board sends normal polarity or reverse polarity. This "reverse battery" signaling was carried over to later automatic exchanges.
History.
The first telephones in the 1870s were rented in pairs which were limited to conversation between those two instruments. The use of a central exchange was soon found to be even more advantageous than in telegraphy. In January 1878 the Boston Telephone Dispatch company had started hiring boys as telephone operators. Boys had been very successful as telegraphy operators, but their attitude (lack of patience) and behaviour (pranks and cursing) was unacceptable for live phone contact, so the company began hiring women operators instead. Thus, on September 1, 1878, Boston Telephone Dispatch hired Emma Nutt as the first woman operator. Small towns typically had the switchboard installed in the operator's home so that he or she could answer calls on a 24 hour basis. In 1894, New England Telephone and Telegraph Company installed the first battery-operated switchboard on January 9 in Lexington, Massachusetts.
Early switchboards in large cities usually were mounted floor to ceiling in order to allow the operators to reach all the lines in the exchange. The operators were boys who would use a ladder to connect to the higher jacks. Late in the 1890s this measure failed to keep up with the increasing number of lines, and Milo G. Kellogg devised the Divided Multiple Switchboard for operators to work together, with a team on the "A board" and another on the "B." These operators were almost always women until the early 1970s, when men were once again hired. Cord switchboards were often referred to as "cordboards" by telephone company personnel. Conversion to Panel switch and other automated switching systems first eliminated the "B" operator and then, usually years later, the "A". Rural and suburban switchboards for the most part remained small and simple. In many cases, customers came to know their operator by name.
As telephone exchanges converted to automatic (dial) service, switchboards continued to serve specialized purposes. Before the advent of direct-dialed long distance calls, a subscriber would need to contact the long-distance operator in order to place a toll call. In large cities, there was often a special number, such as 112, which would ring the long-distance operator directly. Elsewhere, the subscriber would ask the local operator to ring the long-distance operator.
The long distance operator would record the name and city of the person to be called, and the operator would advise the calling party to hang up and wait for the call to be completed. Each toll center had only a limited number of trunks to distant cities, and if those circuits were busy, the operator would try alternate routings through intermediate cities. The operator would plug into a trunk for the destination city, and the inward operator would answer. The inward operator would obtain the number from the local information operator, and ring the call. Once the called party answered, the originating operator would advise him or her to stand by for the calling party, whom she'd then ring back, and record the starting time, once the conversation began.
In the 1940s, with the advent of dial pulse and multi-frequency operator dialing, the operator would plug into a tandem trunk and dial the NPA (area code) and operator code for the information operator in the distant city. For instance, the New York City information operator was 212-131. If the customer knew the number, and the point was direct-dialable, the operator would dial the call. If the distant city did not have dialable numbers, the operator would dial the code for the inward operator serving the called party, and ask her to ring the number.
In the 1960s, once most phone subscribers had direct long-distance dialing, a single type of operator began to serve both the local and long distance functions. A customer might call to request a collect call, a call billed to a third number, or a person-to-person call. All toll calls from coin phones required operator assistance. The operator was also available to help complete a local or long-distance number which did not complete. For example, if a customer encountered a reorder tone (a fast busy signal), it could indicate "all circuits busy," or a problem in the destination exchange. The operator might be able to use a different routing to complete the call. If the operator could not get through by dialing the number, she could call the inward operator in the destination city, and ask her to try the number, or to test a line to see if it was busy or out of order.
Cord switchboards used for these purposes were replaced in the 1970s and 1980s by TSPS and similar systems, which greatly reduced operator involvement in calls. The customer would, instead of simply dialing "0" for the operator, dial 0+NPA+7digits, after which an operator would answer and provide the desired service (coin collection, obtaining acceptance on a collect call, etc.), and then release the call to be automatically handled by the TSPS.
Before the late 1970s and early 1980s, it was common for many smaller cities to have their own operators. An NPA (area code) would usually have its largest city as its primary toll center, with smaller toll centers serving the secondary cities scattered throughout the NPA. TSPS allowed telephone companies to close smaller toll centers and consolidate operator services in regional centers which might be hundreds of miles from the subscriber.
Virtual switchboard.
A virtual switchboard is an automated system used to connect an incoming caller with an agent or staff member. The virtual switchboard user normally has the option of controlling how incoming calls are routed via a web interface. For example calls could be routed to different destinations according to certain criteria such as the time of day etc.
Interactive voice response (IVR) functionality is also a common feature with virtual switchboards. IVR enables incoming callers to a virtual switchboard to hear prerecorded announcements. Popular announcements instruct callers to press a number on their key pad to select which department they want to reach (for example, "press 1 for sales, 2 for accounts, 3 for support, and so on").

</doc>
<doc id="28615" url="http://en.wikipedia.org/wiki?curid=28615" title="Sequencing">
Sequencing

In genetics and biochemistry, sequencing means to determine the primary structure (sometimes falsely called primary sequence) of an unbranched biopolymer. Sequencing results in a symbolic linear depiction known as a sequence which succinctly summarizes much of the atomic-level structure of the sequenced molecule.
DNA sequencing.
DNA sequencing is the process of determining the nucleotide order of a given DNA fragment. So far, most DNA sequencing has been performed using the chain termination method developed by Frederick Sanger. This technique uses sequence-specific termination of a DNA synthesis reaction using modified nucleotide substrates. However, new sequencing technologies such as pyrosequencing are gaining an increasing share of the sequencing market. More genome data are now being produced by pyrosequencing than Sanger DNA sequencing. Pyrosequencing has enabled rapid genome sequencing. Bacterial genomes can be sequenced in a single run with several times coverage with this technique. This technique was also used to sequence the genome of James Watson recently.
The sequence of DNA encodes the necessary information for living things to survive and reproduce. Determining the sequence is therefore useful in fundamental research into why and how organisms live, as well as in applied subjects. Because of the key importance DNA has to living things, knowledge of DNA sequences are useful in practically any area of biological research. For example, in medicine it can be used to identify, diagnose, and potentially develop treatments for genetic diseases. Similarly, research into pathogens may lead to treatments for contagious diseases. Biotechnology is a burgeoning discipline, with the potential for many useful products and services.
The Carlson curve is a term coined by "The Economist" to describe the biotechnological equivalent of Moore's law, and is named after author Rob Carlson. Carlson accurately predicted the doubling time of DNA sequencing technologies (measured by cost and performance) would be at least as fast as Moore's law. Carlson curves illustrate the rapid (in some cases hyperexponential) decreases in cost, and increases in performance, of a variety of technologies, including DNA sequencing, DNA synthesis, and a range of physical and computational tools used in protein expression and in determining protein structures.
Sanger sequencing.
In chain terminator sequencing (Sanger sequencing), extension is initiated at a specific site on the template DNA by using a short oligonucleotide 'primer' complementary to the template at that region. The oligonucleotide primer is extended using a DNA polymerase, an enzyme that replicates DNA. Included with the primer and DNA polymerase are the four deoxynucleotide bases (DNA building blocks), along with a low concentration of a chain terminating nucleotide (most commonly a di-deoxynucleotide). Limited incorporation of the chain terminating nucleotide by the DNA polymerase results in a series of related DNA fragments that are terminated only at positions where that particular nucleotide is used. The fragments are then size-separated by electrophoresis in a slab polyacrylamide gel, or more commonly now, in a narrow glass tube (capillary) filled with a viscous polymer.
An alternative to the labelling of the primer is to label the terminators instead, commonly called 'dye terminator sequencing'. The major advantage of this approach is the complete sequencing set can be performed in a single reaction, rather than the four needed with the labeled-primer approach. This is accomplished by labelling each of the dideoxynucleotide chain-terminators with a separate fluorescent dye, which fluoresces at a different wavelength. This method is easier and quicker than the dye primer approach, but may produce more uneven data peaks (different heights), due to a template dependent difference in the incorporation of the large dye chain-terminators. This problem has been significantly reduced with the introduction of new enzymes and dyes that minimize incorporation variability.
This method is now used for the vast majority of sequencing reactions as it is both simpler and cheaper. The major reason for this is that the primers do not have to be separately labelled (which can be a significant expense for a single-use custom primer), although this is less of a concern with frequently used 'universal' primers. This is changing rapidly due to the increasing cost-effectiveness of second- and third-generation systems from Illumina, 454, ABI, Helicos, and Dover.
Pyrosequencing.
Pyrosequencing, which was developed by Pål Nyrén and Mostafa Ronaghi, has been commercialized by Biotage (for low-throughput sequencing) and 454 Life Sciences (for high-throughput sequencing). The latter platform sequences roughly 100 megabases [now up to 400 megabases] in a seven-hour run with a single machine. In the array-based method (commercialized by 454 Life Sciences), single-stranded DNA is annealed to beads and amplified via EmPCR. These DNA-bound beads are then placed into wells on a fiber-optic chip along with enzymes which produce light in the presence of ATP. When free nucleotides are washed over this chip, light is produced as ATP is generated when nucleotides join with their complementary base pairs. Addition of one (or more) nucleotide(s) results in a reaction that generates a light signal that is recorded by the CCD camera in the instrument. The signal strength is proportional to the number of nucleotides, for example, homopolymer stretches, incorporated in a single nucleotide flow. 
Large-scale sequencing.
Whereas the methods above describe various sequencing methods, separate related terms are used when a large portion of a genome is sequenced. Several platforms were developed to perform exome sequencing (a subset of all DNA across all chromosomes that encode genes) or whole genome sequencing (sequencing of the all nuclear DNA of a human).
RNA sequencing.
RNA is less stable in the cell, and also more prone to nuclease attack experimentally. As RNA is generated by transcription from DNA, the information is already present in the cell's DNA. However, it is sometimes desirable to sequence RNA molecules. In particular, eukaryotic RNA molecules are not necessarily co-linear with their DNA template, as introns are excised. To sequence RNA, the usual method is first to reverse transcribe the sample to generate cDNA fragments. This can then be sequenced as described above.
For more information on the capabilities of next-generation sequencing applied to whole transcriptomes see: RNA-Seq and MicroRNA Sequencing.
Protein sequencing.
Methods for performing protein sequencing
include:
If the gene encoding the protein can be identified it is currently much easier to sequence the DNA and infer the protein sequence. Determining part of a protein's amino-acid sequence (often one end) by one of the above methods may be sufficient to enable the identification of a clone carrying the gene.
Polysaccharide sequencing.
Though polysaccharides are also biopolymers, it is not so common to talk of 'sequencing' a polysaccharide, for several reasons. Although many polysaccharides are linear, many have branches. Many different units (individual monosaccharides) can be used, and bonded in different ways. However, the main theoretical reason is that whereas the other polymers listed here are primarily generated in a 'template-dependent' manner by one processive enzyme, each individual join in a polysaccharide may be formed by a different enzyme. In many cases the assembly is not uniquely specified; depending on which enzyme acts, one of several different units may be incorporated. This can lead to a family of similar molecules being formed. This is particularly true for plant polysaccharides. Methods for the structure determination of oligosaccharides and polysaccharides include NMR spectroscopy and methylation analysis.

</doc>
<doc id="28616" url="http://en.wikipedia.org/wiki?curid=28616" title="Shotgun sequencing">
Shotgun sequencing

In genetics, shotgun sequencing, also known as shotgun cloning, is a method used for sequencing long DNA strands. It is named by analogy with the rapidly expanding, quasi-random firing pattern of a shotgun.
The chain termination method of DNA sequencing (or "Sanger sequencing" for its developer Frederick Sanger) can only be used for fairly short strands of 100 to 1000 base pairs. Longer sequences are subdivided into smaller fragments that can be sequenced separately, and subsequently they are re-assembled to give the overall sequence. Two principal methods are used for this: primer walking (or "chromosome walking") which progresses through the entire strand piece by piece, and shotgun sequencing which is a faster but more complex process that uses random fragments.
In shotgun sequencing,
DNA is broken up randomly into numerous small segments, which are sequenced using the chain termination method to obtain "reads". Multiple overlapping reads for the target DNA are obtained by performing several rounds of this fragmentation and sequencing. Computer programs then use the overlapping ends of different reads to assemble them into a continuous sequence.
Shotgun sequencing was one of the precursor technologies that was responsible for enabling full genome sequencing.
Example.
For example, consider the following two rounds of shotgun reads:
In this extremely simplified example, none of the reads cover the full length of the original sequence, but the four reads can be assembled into the original sequence using the overlap of their ends to align and order them. In reality, this process uses enormous amounts of information that are rife with ambiguities and sequencing errors. Assembly of complex genomes is additionally complicated by the great abundance of repetitive sequences, meaning similar short reads could come from completely different parts of the sequence.
Many overlapping reads for each segment of the original DNA are necessary to overcome these difficulties and accurately assemble the sequence. For example, to complete the Human Genome Project, most of the human genome was sequenced at 12X or greater "coverage"; that is, each base in the final sequence was present on average in 12 different reads. Even so, current methods have failed to isolate or assemble reliable sequence for approximately 1% of the (euchromatic) human genome, as of 2004.
Whole genome shotgun sequencing.
Whole genome shotgun sequencing for small (4000- to 7000-base-pair) genomes was already in use in 1979. Broader application benefited from pairwise end sequencing, known colloquially as "double-barrel shotgun sequencing". As sequencing projects began to take on longer and more complicated DNA sequences, multiple groups began to realize that useful information could be obtained by sequencing both ends of a fragment of DNA. Although sequencing both ends of the same fragment and keeping track of the paired data was more cumbersome than sequencing a single end of two distinct fragments, the knowledge that the two sequences were oriented in opposite directions and were about the length of a fragment apart from each other was valuable in reconstructing the sequence of the original target fragment. The first published description of the use of paired ends was in 1990
as part of the sequencing of the human HGPRT locus, although the use of paired ends was limited to closing gaps after the application of a traditional shotgun sequencing approach. The first theoretical description of a pure pairwise end sequencing strategy, assuming fragments of constant length, was in 1991. At the time, there was community consensus that the optimal fragment length for pairwise end sequencing would be three times the sequence read length. In 1995 Roach et al.
introduced the innovation of using fragments of varying sizes, and demonstrated that a pure pairwise end-sequencing strategy would be possible on large targets. The strategy was subsequently adopted by The Institute for Genomic Research (TIGR) to sequence the genome of the bacterium "Haemophilus influenzae" in 1995, and then by Celera Genomics to sequence the "Drosophila melanogaster" (fruit fly) genome in 2000, 
and subsequently the human genome.
To apply the strategy, a high-molecular-weight DNA strand is sheared into random fragments, size-selected (usually 2, 10, 50, and 150 kb), and cloned into an appropriate vector. The clones are then sequenced from both ends using the chain termination method yielding two short sequences. Each sequence is called an "end-read" or "read" and two reads from the same clone are referred to as "mate pairs". Since the chain termination method usually can only produce reads between 500 and 1000 bases long, in all but the smallest clones, mate pairs will rarely overlap.
The original sequence is reconstructed from the reads using sequence assembly software. First, overlapping reads are collected into longer composite sequences known as "contigs". Contigs can be linked together into "scaffolds" by following connections between mate pairs. The distance between contigs can be inferred from the mate pair positions if the average fragment length of the library is known and has a narrow window of deviation. Depending on the size of the gap between contigs, different techniques can be used to find the sequence in the gaps. If the gap is small (5-20kb) then the use of PCR to amplify the region is required, followed by sequencing. If the gap is large (>20kb) then the large fragment is cloned in special vectors such as BAC (Bacterial artificial chromosomes) followed by sequencing of the vector.
Proponents of this approach argue that it is possible to sequence the whole genome at once using large arrays of sequencers, which makes the whole process much more efficient than more traditional approaches. Detractors argue that although the technique quickly sequences large regions of DNA, its ability to correctly link these regions is suspect, particularly for genomes with repeating regions. As sequence assembly programs become more sophisticated and computing power becomes cheaper, it may be possible to overcome this limitation.
Coverage.
Coverage (read depth or depth) is the average number of reads representing a given nucleotide in the reconstructed sequence. It can be calculated from the length of the original genome ("G"), the number of reads("N"), and the average read length("L") as formula_1. For example, a hypothetical genome with 2,000 base pairs reconstructed from 8 reads with an average length of 500 nucleotides will have 2x redundancy. This parameter also enables one to estimate other quantities, such as the percentage of the genome covered by reads (sometimes also called coverage). A high coverage in shotgun sequencing is desired because it can overcome errors in base calling and assembly. The subject of DNA sequencing theory addresses the relationships of such quantities.
Sometimes a distinction is made between "sequence coverage" and "physical coverage". Sequence coverage is the average number of times a base is read (as described above). Physical coverage is the average number of times a base is read or spanned by mate paired reads.
Hierarchical Shotgun sequencing.
Although shotgun sequencing can in theory be applied to a genome of any size, its direct application to the sequencing of large genomes (for instance, the Human Genome) was limited until the late 1990s, when technological advances made practical the handling of the vast quantities of complex data involved in the process. Historically, full-genome shotgun sequencing was believed to be limited by both the sheer size of large genomes and by the complexity added by the high percentage of repetitive DNA (greater than 50% for the human genome) present in large genomes. It was not widely accepted that a full-genome shotgun sequence of a large genome would provide reliable data. For these reasons, other strategies that lowered the computational load of sequence assembly had to be utilized before shotgun sequencing was performed.
In hierarchical sequencing, also known as top-down sequencing, a low-resolution physical map of the genome is made prior to actual sequencing. From this map, a minimal number of fragments that cover the entire chromosome are selected for sequencing. In this way, the minimum amount of high-throughput sequencing and assembly is required.
The amplified genome is first sheared into larger pieces (50-200kb) and cloned into a bacterial host using BACs or PACs. Because multiple genome copies have been sheared at random, the fragments contained in these clones have different ends, and with enough coverage (see section above) finding a scaffold of BAC contigs that covers the entire genome is theoretically possible. This scaffold is called a tiling path. Once a tiling path has been found, the BACs that form this path are sheared at random into smaller fragments and can be sequenced using the shotgun method on a smaller scale.
Although the full sequences of the BAC contigs is not known, their orientations relative to one another are known. There are several methods for deducing this order and selecting the BACs that make up a tiling path. The general strategy involves identifying the positions of the clones relative to one another and then selecting the least number of clones required to form a contiguous scaffold that covers the entire area of interest. The order of the clones is deduced by determining the way in which they overlap. Overlapping clones can be identified in several ways. A small radioactively or chemically labeled probe containing a sequence-tagged site (STS) can be hybridized onto a microarray upon which the clones are printed. In this way, all the clones that contain a particular sequence in the genome are identified. The end of one of these clones can then be sequenced to yield a new probe and the process repeated in a method called chromosome walking.
Alternatively, the BAC library can be restriction-digested. Two clones that have several fragment sizes in common are inferred to overlap because they contain multiple similarly spaced restriction sites in common. This method of genomic mapping is called restriction fingerprinting because it identifies a set of restriction sites contained in each clone. Once the overlap between the clones has been found and their order relative to the genome known, a scaffold of a minimal subset of these contigs that covers the entire genome is shotgun-sequenced.
Because it involves first creating a low-resolution map of the genome, hierarchical shotgun sequencing is slower than whole-genome shotgun sequencing, but relies less heavily on computer algorithms than whole-genome shotgun sequencing. The process of extensive BAC library creation and tiling path selection, however, make hierarchical shotgun sequencing slow and labor-intensive. Now that the technology is available and the reliability of the data demonstrated, and the speed and cost efficiency of whole-genome shotgun sequencing has made it the primary method for genome sequencing.
Shotgun and Next-generation sequencing.
The classical shotgun sequencing was based on the Sanger sequencing method: this was the most advanced technique for sequencing genomes from about 1995–2005. The shotgun strategy is still applied today, however using other sequencing technologies, called next-generation sequencing. These technologies produce shorter reads (anywhere from 25–500bp) but many hundreds of thousands or millions of reads in a relatively short time (on the order of a day).
This results in high coverage, but the assembly process is much more computationally expensive. These technologies are vastly superior to Sanger sequencing due to the high volume of data and the relatively short time it takes to sequence a whole genome.
References.
Further reading.
</dl>
External links.
 This article incorporates public domain material from the document .

</doc>
<doc id="28624" url="http://en.wikipedia.org/wiki?curid=28624" title="Selenocysteine">
Selenocysteine

Selenocysteine (abbreviated as Sec or U, in older publications also as Se-Cys) is the 21st proteinogenic amino acid. It exists naturally in all kingdoms of life as a building block of selenoproteins. Selenocysteine is a cysteine analogue with a selenium-containing selenol group in place of the sulfur-containing thiol group. It is present in several enzymes (for example glutathione peroxidases, tetraiodothyronine 5' deiodinases, thioredoxin reductases, formate dehydrogenases, glycine reductases, selenophosphate synthetase 1, methionine-R-sulfoxide reductase B1 (SEPX1), and some hydrogenases). Selenocysteine was discovered by biochemist Theresa Stadtman at the National Institutes of Health.
Structure.
Selenocysteine has a structure similar to that of cysteine, but with an atom of selenium taking the place of the usual sulfur, forming a selenol group which is deprotonated at physiological pH. Proteins that contain one or more selenocysteine residues are called selenoproteins and those with catalytic activities which depend on selenocysteine's biochemical activity are called selenoenzymes.
The structurally characterized selenoenzymes have been found to employ catalytic triad structures that influence the nucleophilicity of the active site selenocysteine.
Biology.
Selenocysteine has both a lower pKa (5.47) and a lower reduction potential than cysteine. These properties make it very suitable in proteins that are involved in antioxidant activity.
Although it is found in all kingdoms of life, it is not universal in all organisms. Unlike other amino acids present in biological proteins, selenocysteine is not coded for directly in the genetic code. Instead, it is encoded in a special way by a UGA codon, which is normally a stop codon. Such a mechanism is called translational recoding and its efficiency depends on the selenoprotein being synthesized and on translation initiation factors. When cells are grown in the absence of selenium, translation of selenoproteins terminates at the UGA codon, resulting in a truncated, nonfunctional enzyme. The UGA codon is made to encode selenocysteine by the presence of a selenocysteine insertion sequence (SECIS) in the mRNA. The SECIS element is defined by characteristic nucleotide sequences and secondary structure base-pairing patterns. In bacteria, the SECIS element is typically located immediately following the UGA codon within the reading frame for the selenoprotein. In Archaea and in eukaryotes, the SECIS element is in the 3' untranslated region (3' UTR) of the mRNA, and can direct multiple UGA codons to encode selenocysteine residues.
Again unlike the other amino acids, no free pool of selenocysteine exists in the cell. Its high reactivity would cause damage to cells. Instead, cells store selenium in the less reactive selenide form (H2Se). Selenocysteine synthesis occurs on a specialized tRNA, which also functions to incorporate it into nascent polypeptides. The primary and secondary structure of selenocysteine-specific tRNA, tRNASec, differ from those of standard tRNAs in several respects, most notably in having an 8-base (bacteria) or 10-base (eukaryotes) pair acceptor stem, a long variable region arm, and substitutions at several well-conserved base positions. The selenocysteine tRNAs are initially charged with serine by seryl-tRNA ligase, but the resulting Ser-tRNASec is not used for translation because it is not recognised by the normal translation elongation factor (EF-Tu in bacteria, eEF1A in eukaryotes). Rather, the tRNA-bound seryl residue is converted to a selenocysteine residue by the pyridoxal phosphate-containing enzyme selenocysteine synthase. Finally, the resulting Sec-tRNASec is specifically bound to an alternative translational elongation factor (SelB or mSelB (or eEFSec)), which delivers it in a targeted manner to the ribosomes translating mRNAs for selenoproteins. The specificity of this delivery mechanism is brought about by the presence of an extra protein domain (in bacteria, SelB) or an extra subunit (SBP2 for eukaryotic mSelB/eEFSec) which bind to the corresponding RNA secondary structures formed by the SECIS elements in selenoprotein mRNAs.
As of 2003, twenty-five human proteins are known to contain selenocysteine (selenoproteins).
Selenocysteine derivatives γ-glutamyl-"Se"-methylselenocysteine and "Se"-methylselenocysteine occur naturally in plants of the genera "Allium" and "Brassica".
Applications.
Biotechnological applications of selenocysteine include use of 73Se-labeled Sec (half-life of 73Se = 7.2 hours) in positron emission tomography (PET) studies and 75Se-labeled Sec (half-life of 75Se = 118.5 days) in specific radiolabeling, facilitation of phase determination by multi-wavelength anomalous diffraction in X-ray crystallography of proteins by introducing Sec alone, or Sec together with selenomethionine (SeMet), and incorporation of the stable 77Se isotope, which has a nuclear spin of 1/2 and can be used for high-resolution NMR, among others.

</doc>
<doc id="28684" url="http://en.wikipedia.org/wiki?curid=28684" title="Session Initiation Protocol">
Session Initiation Protocol

The Session Initiation Protocol (SIP) is a communications protocol for signaling and controlling multimedia communication sessions. The most common applications of SIP are in Internet telephony for voice and video calls, as well as instant messaging all over Internet Protocol (IP) networks.
The protocol defines the messages that are sent between endpoints, which govern establishment, termination and other essential elements of a call. SIP can be used for creating, modifying and terminating sessions consisting of one or several media streams. SIP is an application layer protocol designed to be independent of the underlying transport layer. It is a text-based protocol, incorporating many elements of the Hypertext Transfer Protocol (HTTP) and the Simple Mail Transfer Protocol (SMTP).
SIP works in conjunction with several other application layer protocols that identify and carry the session media. Media identification and negotiation is achieved with the Session Description Protocol (SDP). For the transmission of media streams (voice, video) SIP typically employs the Real-time Transport Protocol (RTP) or Secure Real-time Transport Protocol (SRTP). For secure transmissions of SIP messages, the protocol may be encrypted with Transport Layer Security (TLS).
History.
SIP was originally designed by Mark Handley, Henning Schulzrinne, Eve Schooler and Jonathan Rosenberg in 1996. The protocol was standardized as RFC 2543 in 1999 (SIP 1.0). In November 2000, SIP was accepted as a 3GPP signaling protocol and permanent element of the IP Multimedia Subsystem (IMS) architecture for IP-based streaming multimedia services in cellular systems. s of 2014[ [update]], the latest version (SIP 2.0) of the specification is RFC 3261, published in June 2002, with extensions and clarifications since then.
The U.S. National Institute of Standards and Technology (NIST), Advanced Networking Technologies Division provides a public-domain Java implementation that serves as a reference implementation for the standard. The implementation can work in proxy server or user agent scenarios and has been used in numerous commercial and research projects. It supports RFC 3261 in full and a number of extension RFCs including RFC 6665 (event notification) and RFC 3262 (reliable provisional responses).
While originally developed based on voice applications, the protocol was envisioned and supports a diverse array of applications, including video conferencing, streaming multimedia distribution, instant messaging, presence information, file transfer, fax over IP and online games.
Protocol operation.
SIP is independent from the underlying transport protocol. It runs on the Transmission Control Protocol (TCP), the User Datagram Protocol (UDP) or the Stream Control Transmission Protocol (SCTP). SIP can be used for two-party (unicast) or multiparty (multicast) sessions.
SIP employs design elements similar to the HTTP request/response transaction model. Each transaction consists of a client request that invokes a particular method or function on the server and at least one response. SIP reuses most of the header fields, encoding rules and status codes of HTTP, providing a readable text-based format. 
Each resource of a SIP network, such as a user agent or a voicemail box, is identified by a uniform resource identifier (URI), based on the general standard syntax also used in Web services and e-mail. The URI scheme used for SIP is codice_1 and a typical SIP URI is of the form: codice_2.
If secure transmission is required, the scheme codice_3 is used and mandates that each hop over which the request is forwarded up to the target domain must be secured with Transport Layer Security (TLS). The last hop from the proxy of the target domain to the user agent has to be secured according to local policies. TLS protects against attackers who try to listen on the signaling link but it does not provide real end-to-end security to prevent espionage and law enforcement interception, as the encryption is only hop-by-hop and every single intermediate proxy has to be trusted.
SIP works in concert with several other protocols and is only involved in the signaling portion of a communication session. SIP clients typically use TCP or UDP on port numbers 5060 and/or 5061 to connect to SIP servers and other SIP endpoints. Port 5060 is commonly used for non-encrypted signaling traffic whereas port 5061 is typically used for traffic encrypted with Transport Layer Security (TLS). SIP is primarily used in setting up and tearing down voice or video calls. It also allows modification of existing calls. The modification can involve changing addresses or ports, inviting more participants, and adding or deleting media streams. SIP has also found applications in messaging applications, such as instant messaging, and event subscription and notification. A suite of SIP-related Internet Engineering Task Force (IETF) rules define behavior for such applications. The voice and video stream communications in SIP applications are carried over another application protocol, the Real-time Transport Protocol (RTP). Parameters (port numbers, protocols, codecs) for these media streams are defined and negotiated using the Session Description Protocol (SDP), which is transported in the SIP packet body.
A motivating goal for SIP was to provide a signaling and call setup protocol for IP-based communications that can support a superset of the call processing functions and features present in the public switched telephone network (PSTN). SIP by itself does not define these features; rather, its focus is call-setup and signaling. The features that permit familiar telephone-like operations: dialing a number, causing a phone to ring, hearing ringback tones or a busy signal - are performed by proxy servers and user agents. Implementation and terminology are different in the SIP world but to the end-user, the behavior is similar.
SIP-enabled telephony networks can also implement many of the more advanced call processing features present in Signaling System 7 (SS7), though the two protocols themselves are very different. SS7 is a centralized protocol, characterized by a complex central network architecture and dumb endpoints (traditional telephone handsets). SIP is a client-server protocol, however most SIP-enabled devices may perform both the client and the server role. In general, session initiator is a client, and the call recipient performs the server function. SIP features are implemented in the communicating endpoints, contrary to traditional SS7 features, which are implemented in the network.
SIP is distinguished by its proponents for having roots in the IP community rather than in the telecommunications industry. SIP has been standardized and governed primarily by the IETF, while other protocols, such as H.323, have traditionally been associated with the International Telecommunication Union (ITU).
Network elements.
SIP defines user-agents as well as several types of server network elements. Two SIP endpoints can communicate without any intervening SIP infrastructure. However, this approach is often impractical for a public service, which needs directory services to locate available nodes on the network.
User Agent.
A "SIP user agent" (UA) is a logical network end-point used to create or receive SIP messages and thereby manage a SIP session. A SIP UA can perform the role of a "User Agent Client" (UAC), which sends SIP requests, and the "User Agent Server" (UAS), which receives the requests and returns a SIP response. These roles of UAC and UAS only last for the duration of a SIP transaction.
A SIP phone is an IP phone that implements SIP user agent and server functions, which provide the traditional call functions of a telephone, such as dial, answer, reject, hold/unhold, and call transfer. SIP phones may be implemented as a hardware device or as a softphone. As vendors increasingly implement SIP as a standard telephony platform, often driven by 4G efforts, the distinction between hardware-based and software-based SIP phones is being blurred and SIP elements are implemented in the basic firmware functions of many IP-capable devices. Examples are devices from Nokia and BlackBerry.
In SIP, as in HTTP, the user agent may identify itself using a message header field 'User-Agent', containing a text description of the software/hardware/product involved. The User-Agent field is sent in request messages, which means that the receiving SIP server can see this information. SIP network elements sometimes store this information, and it can be useful in diagnosing SIP compatibility problems.
Proxy server.
The proxy server is an intermediary entity that acts as both a server and a client for the purpose of making requests on behalf of other clients.
A proxy server primarily plays the role of routing, meaning that its job is to ensure that a request is sent to another entity closer to the targeted user. Proxies are also useful for enforcing policy (for example, making sure a user is allowed to make a call).
A proxy interprets, and, if necessary, rewrites specific parts of a request message before forwarding it.
Registrar.
A registrar is a SIP endpoint that accepts REGISTER requests and places the information it receives in those requests into a location service for the domain it handles. The location service links one or more IP addresses to the SIP URI of the registering agent. The URI uses the sip: scheme, although other protocol schemes are possible, such as tel:. More than one user agent can register at the same URI, with the result that all registered user agents receive the calls to the URI.
SIP registrars are logical elements, and are commonly co-located with SIP proxies. But it is also possible and often good for network scalability to place this location service with a redirect server.
Redirect server.
A user agent server that generates 3xx (Redirection) responses to requests it receives, directing the client to contact an alternate set of URIs. The redirect server allows proxy servers to direct SIP session invitations to external domains.
Session border controller.
Session border controllers serve as "middle boxes" between UA and SIP servers for various types of functions, including network topology hiding, and assistance in NAT traversal.
Gateway.
Gateways can be used to interface a SIP network to other networks, such as the public switched telephone network, which use different protocols or technologies.
SIP messages.
SIP is a text-based protocol with syntax similar to that of HTTP. There are two different types of SIP messages: requests and responses. The first line of a request has a "method", defining the nature of the request, and a Request-URI, indicating where the request should be sent. The first line of a response has a "response code".
SIP request.
For SIP requests, RFC 3261 defines the following methods:
A new method has been introduced in SIP in RFC 3262:
SIP response.
The SIP response types defined in RFC 3261 fall in one of the following categories:
Transactions.
SIP makes use of transactions to control the exchanges between participants and deliver messages reliably. The transactions maintain an internal state and make use of timers. "Client Transactions" send requests and "Server Transactions" respond to those requests with one-or-more responses. The responses may include zero-or-more Provisional (1xx) responses and one-or-more final (2xx-6xx) responses.
Transactions are further categorized as either "Invite" or "Non-Invite". "Invite" transactions differ in that they can establish a long-running conversation, referred to as a "Dialog" in SIP, and so include an acknowledgment (ACK) of any non-failing final response (e.g. 200 OK).
Because of these transactional mechanisms, SIP can make use of un-reliable transports such as User Datagram Protocol (UDP). 
Instant messaging and presence.
The Session Initiation Protocol for Instant Messaging and Presence Leveraging Extensions (SIMPLE) is the SIP-based suite of standards for instant messaging and presence information. MSRP (Message Session Relay Protocol) allows instant message sessions and file transfer.
Conformance testing.
TTCN-3 test specification language is used for the purposes of specifying conformance tests for SIP implementations. SIP test suite is developed by a Specialist Task Force at ETSI (STF 196). The SIP developer community meets regularly at the SIP Forum events to test interoperability and test implementations of new RFCs.
Applications.
A SIP connection is a marketing term for voice over Internet Protocol (VoIP) services offered by many Internet telephony service providers (ITSPs). The service provides routing of telephone calls from a clients private branch exchange (PBX) telephone system to the public switched telephone network (PSTN). Such services may simplify corporate information system infrastructure by sharing Internet access for voice and data, and removing the cost for Basic Rate Interface (BRI) or Primary Rate Interface (PRI) telephone circuits.
Many VoIP phone companies allow customers to use their own SIP devices, such as SIP-capable telephone sets, or softphones.
SIP-enabled video surveillance cameras can make calls to alert the owner or operator that an event has occurred; for example, to notify that motion has been detected out-of-hours in a protected area.
SIP is used in audio over IP for broadcasting applications where it provides an interoperable means for audio interfaces from different manufacturers to make connections with one another.
SIP-ISUP interworking.
SIP-I, or the Session Initiation Protocol with encapsulated ISUP, is a protocol used to create, modify, and terminate communication sessions based on ISUP using SIP and IP networks. Services using SIP-I include voice, video telephony, fax and data. SIP-I and SIP-T are two protocols with similar features, notably to allow ISUP messages to be transported over SIP networks. This preserves all of the detail available in the ISUP header, which is important as there are many country-specific variants of ISUP that have been implemented over the last 30 years, and it is not always possible to express all of the same detail using a native SIP message. SIP-I was defined by the ITU-T, whereas SIP-T was defined via the IETF RFC route.
Deployment issues.
If the call traffic runs on the same connection with other traffic, such as email or Web browsing, voice and even signaling packets may be dropped and the voice stream may be interrupted.
To mitigate this, many companies split voice and data between two separate internet connections. Alternately, some networks use the Differentiated services (DiffServ) field (previously defined as Type of Service (ToS) field) in the header of IPV4 packets to mark the relative time-sensitivity of SIP and RTP as compared to web, email, video and other types of IP traffic. This precedence marking method requires that all routers in the SIP and RTP paths support separate queues for different traffic types. Other options to control delay and loss include incorporating multiple VLANs (virtual local area networks), traffic shaping to avoid this resource conflict, but the efficacy of this solution is dependent on the number of packets dropped between the Internet and the PBX.
Registration is required if the end user has a dynamic IP address, if the provider does not support static hostnames, or if NAT is used. In order to share several DID numbers on the same registration, the IETF has defined additional headers (for example "P-Preferred-Identity", see RFC 3325). This avoids multiple registrations from one PBX to the same provider. Using this method the PBX can indicate what identity should be presented to the Called party and what identity should be used for authenticating the call. This feature is also useful when the PBX redirects an incoming call to a PSTN number, for example a cell phone, to preserve the original Caller ID.
Users should also be aware that a SIP connection can be used as a channel for attacking the company's internal networks, similar to Web and Email attacks. Users should consider installing appropriate security mechanisms to prevent malicious attacks.
Encryption.
The increasing concerns about security of calls that run over the public Internet has made SIP encryption more popular. Because VPN is not an option for most service providers, most service providers that offer secure SIP (SIPS) connections use TLS for securing signaling. The relationship between SIP (port 5060) and SIPS (port 5061), is similar to that as for HTTP and HTTPS, and uses URIs in the form "sips:user@example.com". The media streams, which occur on different connections to the signaling stream, can be encrypted with SRTP. The key exchange for SRTP is performed with SDES (RFC 4568), or the newer and often more user friendly ZRTP (RFC 6189), which can automatically upgrade RTP to SRTP using dynamic key exchange (and a verification phrase). One can also add a MIKEY (RFC 3830) exchange to SIP and in that way determine session keys for use with SRTP.

</doc>
<doc id="28692" url="http://en.wikipedia.org/wiki?curid=28692" title="Sabermetrics">
Sabermetrics

Sabermetrics is the empirical analysis of baseball, especially baseball statistics that measure in-game activity. The term is derived from the acronym SABR, which stands for the Society for American Baseball Research. It was coined by Bill James, who is one of its pioneers and is often considered its most prominent advocate and public face.
General principles.
"The Sabermetric Manifesto" by David Grabiner (1994) begins:
Bill James defined sabermetrics as "the search for objective knowledge about baseball." Thus, sabermetrics attempts to answer objective questions about baseball, such as "which player on the Red Sox contributed the most to the team's offense?" or "How many home runs will Ken Griffey hit next year?" It cannot deal with the subjective judgments which are also important to the game, such as "Who is your favorite player?" or "That was a great game."
It may, however, attempt to settle questions, such as, "Was Willie Mays faster than Mickey Mantle?" by establishing several possible parameters for examining speed in objective studies (how many triples each man hit, how many bases each man stole, how many times he was caught stealing) and then reaching a tentative conclusion on the basis of these individual studies.
Sabermetricians frequently question traditional measures of baseball skill. For instance, they doubt that batting average is as useful as conventional wisdom says it is because team batting average provides a relatively poor fit for team runs scored. Sabermetric reasoning would say that runs win ballgames, and that a good measure of a player's worth is his ability to help his team score more runs than the opposing team. This may imply that the traditional RBI (runs batted in) is an effective metric; however, sabermetricians also reject RBI, for a number of reasons. Rather, sabermetric measures are usually phrased in terms of either runs or team wins. For example, a player might be described as being worth 54 offensive runs more than a replacement-level player at the same position over the course of a full season, as the sabermetric statistic VORP can indicate.
Early history.
Sabermetrics research began in the middle of the 20th century. Earnshaw Cook was one of the earliest researchers of sabermetrics. Cook gathered the majority of his research into his 1964 book, "Percentage Baseball". The book was the first of its kind to gain national media attention, although it was widely criticized and not accepted by most baseball organizations.
While playing for the Baltimore Orioles in the early 1970s, Davey Johnson used an IBM System/360 at team owner Jerry Hofberger's brewery to write a FORTRAN baseball computer simulation, and using the results unsuccessfully proposed to manager Earl Weaver that he should bat second in the lineup. He wrote IBM BASIC programs to help him manage the Tidewater Tides, and after becoming manager of the New York Mets in 1984 arranged for a team employee to write a dBASE II application to compile and store advanced metrics on team statistics.

</doc>
<doc id="28695" url="http://en.wikipedia.org/wiki?curid=28695" title="Mercy rule">
Mercy rule

A mercy rule, also known by the term slaughter rule (or, less commonly, knockout rule and skunk rule), brings a sports event to an early end when one team has a very large and presumably insurmountable lead over the other team. It is called the "mercy" rule because it spares the losing team the humiliation of suffering a more formal loss, and denies the winning team the satisfaction thereof, and prevents running up the score, a generally discouraged practice in which the opponent continues to score beyond the point when the game has become out of hand. The mercy rule is most common in North America and primarily in North American sports such as baseball or softball, where there no game clock and play could theoretically continue forever, although it is also used in sports such as ice hockey and American football. It is very rare in competitive sports beyond the high school level.
Usage details.
The rules vary widely, depending on the level of competition, but nearly all youth leagues and high school sports associations, and many college sports associations have mercy rules for sports including baseball, softball, American football (though not college) and association football.
However, mercy rules usually do not take effect until a prescribed point in the game (e.g., the second half of an Association football game). That means one team, particularly if they are decidedly better than a weaker opponent, can still "run up the score" before the rule takes effect. For instance, in American football, one team could be ahead by 70 points with three minutes left in the first half; in baseball, the better team could have a 20-run lead in the second inning, but the game would continue.
Baseball and softball.
International competitions are sanctioned by the World Baseball Softball Confederation (WBSC), formed by the 2013 merger of the International Baseball Federation (IBAF) and International Softball Federation (ISF).
In baseball competitions, including Olympic competitions (discontinued after 2008) and the World Baseball Classic (WBC), games are currently ended when one team is ahead by 10 runs, once at least seven completed innings are played by the trailing team. In women's competition, the same applies after five innings.
The inaugural WBC in 2006 followed the IBAF mercy rule, with an additional rule stopping a game after five innings when a team is ahead by at least 15 runs. The mercy rules applied to the round-robin (now double-elimination) matches only, and not to the Semi-Finals or Final.
In Little League Baseball and Softball, rules call for the game to end if the winning team is ahead by 10 runs after four innings (3½ innings if the home team is ahead).
Softball rules are different for fast/modified fast pitch and slow pitch. In WBSC-sanctioned competitions, the run ahead rule (the WBSC terminology) is, for fast or modified fast pitch, 20 runs after three innings, 15 after four, or 7 after 5. In slow pitch, the margin is 20 runs after four innings or 15 after five. The NCAA has also adopted this rule.
In NCAA and NAIA college baseball, the game will end if a team is ahead by at least 10 runs after seven innings in a scheduled 9-inning game. Most NCAA conferences only apply the rule on the final day of a series for travel reasons or during conference tournaments in order to allow the next game to start. The rule is not allowed in NCAA tournament play where all games must be at least nine innings.
In NCAA softball, the rule is invoked if one team is ahead by at least eight runs after five innings and, unlike with college baseball, applies in the NCAA tournament as well with the exception of the championship series. In American high school softball, most states use a mercy rule of 20 runs ahead in three innings or 10 in five innings. (In either case, if the home team is ahead by the requisite number of runs, the game will end after the top half of the inning.)
Most state high school associations (where games are seven innings) have rules where a baseball game ends after the winning team has built a 10-run lead and at least five innings have been played; some associations further this rule ending a game after either three or four innings if the lead is at least 15 runs. For softball, the rule is 12 after three innings, and 10 after five. However, since the home team has the last at-bat, the rules usually allow visiting teams to score an unlimited number of runs in the top half of an inning. This can be prevented by only invoking the rule after the home team has completed their half of the inning.
Due to the untimed nature of innings, some leagues impose caps on the number of runs that can be scored in one inning, usually in the 4-8 range. This ensures that games will complete in a reasonable length of time, but it can also mean that a lead of a certain size becomes insurmountable due to the cap, however, this can be prevented by not invoking the rule in this or similar circumstances.
American football.
At the middle or high school level, 34 states use a mercy rule that may involve a "continuous clock" – that is, the clock continues to operate on most plays when the clock would normally stop, such as an incomplete pass – once a team has a certain lead (e.g., 35 points) during the second half. In most states, the clock would stop only for scores, time outs (officials', injury or charged) or the end of the quarter. Plays that would normally stop the clock, such as penalties, incomplete passes, going out of bounds or change of possession, would not stop the clock. The rule varies by state – for example, the clock does not stop upon a score in Colorado or Kansas (regular season games only) unless the score difference is reduced to below the rule-invoking amount.
In most states, once the point differential is reduced to below the mercy rule-invoking amount, normal timing procedures resume until either the end of the game or the mercy rule-invoking point differential is re-established. Most states that have mercy rules waive this rule for the championship game.
In some states, coaches and game officials may choose to end a game at their own discretion at any time during the second half if the continuous clock rule is in effect; this usually happens if a lopsided margin continues to increase or if threatening weather is imminent. Although rare, some states or high school conferences have rules where the team with a very large lead can't run a certain play for the rest of the game, such as a deep pass or outside run.
At the college level, there is no NCAA mercy rule. However, a continuous clock was used on September 5, 2013 beginning in the 4th quarter when the Georgia Tech Yellow Jackets had a 63-0 lead against the Elon Phoenix. This was at the request of the Elon coach Jason Swepsona and agreed upon by Georgia Tech coach Paul Johnson. The Yellow Jackets won the game 70-0.
A continuous clock was used on November 8, 2003 beginning in the 3rd quarter when the Oklahoma Sooners opened up with a 49-0 halftime lead against the Texas A&M Aggies. This was agreed upon by the two coaches and the game ended with the Sooners winning 77-0.
In a 1988 game, Kansas Jayhawks coach Glen Mason asked if a running clock could be used after his team trailed 49-0 at halftime to Auburn. Tigers coach Pat Dye and the officials agreed, and Auburn ended up a 56-7 winner.
In some states (where 8-man and 6-man football is widely used), the rules for 8-man and 6-man football call for a game to end when one team is ahead by a certain score (e.g., 45 or 50 points) at half time or any time thereafter. In other states with 6- or 8-man football, continuous clock rules are used, and the rule may be modified; for instance, in Iowa, the rule goes into effect if the 35-point differential is reached at any time after the first quarter.
Amateur boxing.
If a boxer trails by more than 20 points, the referee stops the fight and the boxer that is leading automatically wins; bouts which end this way may be noted as "RSC" (referee stopped contest) with notations for an outclassed opponent (RSCO), outscored opponent (RSCOS), injury (RSCI) or head injury (RSCH).
While a boxer who loses on the mercy rule is scored RSCOS, and would be similar to a technical knockout in professional boxing, it is not scored a loss by knockout, and the 28-day suspension for losing on a knockout does not apply.
Association Football (Soccer).
IBSA rules require that any time during a game that one team has scored ten (10) more goals than the other team that game is deemed completed. In United States high school soccer, most states use a mercy rule that ends the game whenever one team is ahead by 10 or more goals at any point from halftime onward. Youth soccer leagues use variations on this rule.
Paintball.
In woodsball, if you were within 10 ft of an opposing player and he was unaware of your presence, it is an etiquette to offer the opposing player a "mercy", that is to offer him a chance to surrender and call himself out of the game, instead of shooting him at close range. The opposing player, however, does not have to accept this "mercy" and can attempt to return fire. This rule, however, is not universal and different fields have different variation and interpretation of the mercy rule.
Basketball.
In high school basketball, many states have a "continuous clock" rule, similar to American football, which takes effect in the second half after a lead grows to a prescribed point (e.g., in Iowa, 35 points or more). The clock stops only for charged, officials' or injury time-outs; or at the end of the third quarter. The clock would not stop in situations where timing would normally stop, such as for fouls, free throws, out-of-bounds plays or substitutions.
The rules vary when normal timing procedures take effect after a lead is diminished (such as due to the trailing team's rally); for instance, in Iowa, normal timing procedures are enforced when the lead is lowered to 25 points, but re-instituted once the lead grows back to 35 or more points. As with other sports, some states offer provisions to allow a team to end the game early by mutual decision of the coaches (for instance, if a large lead continues to grow and the talent disparity is obvious).

</doc>
<doc id="28696" url="http://en.wikipedia.org/wiki?curid=28696" title="Sumbawa">
Sumbawa

Sumbawa is an Indonesian island, in the middle of the Lesser Sunda Islands chain, with Lombok to the west, Flores to the east, and Sumba further to the southeast. It is part of the
province of West Nusa Tenggara, but there are presently steps being taken by the Indonesian government to turn the island into a separate province. Traditionally the island is known as the source of sappanwood used to make red dye, honey and sandalwood. Its savanna-like climate and vast grassland is used to breed horses and cattle and to hunt deer.
Sumbawa has an area (including minor offshore islands) of 15,448 km2 (three times the size of Lombok) with a current population (January 2014) of around 1.39 million. It marks the boundary between the islands to the west, which were influenced by religion and culture spreading from India, and the region to the east that was less influenced. In particular this applies to both Hinduism and Islam.
History.
The 14th-century Nagarakretagama mentioned several principalities identified to be on Sumbawa; Dompu, Bima, Sape and Sang Hyang Api volcanic island just offcoast of northeast Sumbawa. Four principalities in western Sumbawa were dependencies of the Majapahit Empire of eastern Java. Because of Sumbawa's natural resources, it was regularly invaded by outside forces – from Javanese, Balinese, Makassarese, Dutch and Japanese. The Dutch first arrived in 1605, but did not effectively rule Sumbawa until the early 20th century. The Balinese kingdom of Gelgel ruled western Sumbawa for a short period as well. The eastern parts of the island also home to the Sultanate of Bima, an Islamic polity that has links to Bugis and Makasarese people of South Sulawesi, as well as other Malay-Islamic polities in the archipelago.
Historical evidence indicates that people on Sumbawa island were known in the East Indies for their honey, horses, sappan wood for producing red dye, and sandalwood used for incense and medications. The area was thought to be highly productive agriculturally. In 18th-century, the Dutch introduced coffee plantation on the western slopes of Mount Tambora, thus creating the Tambora coffee variant.
Administration.
Sumbawa is administratively divided into four regencies ("kabupaten") and one kota (city). They are:
Demographics.
Islam was introduced via the Makassarese of Sulawesi.
Sumbawa has historically had two major linguistic groups who spoke languages that were unintelligible to each other. One group centered in the western side of the island speaks Basa Semawa (Indonesian: "Bahasa Sumbawa") which is similar to the Sasak language from Lombok; the second group in the east speaks Nggahi Mbojo ("Bahasa Bima"). They were once separated by the Tambora culture, which spoke a language related to neither. After the demise of Tambora, the kingdoms located in Sumbawa Besar and Bima were the two focal points of Sumbawa. This division of the island into two parts remains today; Sumbawa Besar and Bima are the two largest towns on the island, and are the centers of distinct cultural groups that share the island.
The population of the island (including minor outliers) was 1.33 million at the latest decennial census in 2010, comprising 29.58% of the population of the entire province's with 4.5 million people. Due to lack of work opportunities on the island and its frequent droughts (unlike wet Bali), many people on the island seek work in the Middle East as laborers or domestic servants; some 500,000 workers, or over 10% of the population of West Nusa Tenggara, have left the country to work overseas.
Geography.
The island is bound by bodies of water; to the west is Alas Strait, south is the Indian Ocean, Saleh Bay creates a major north-central indentation in the island, and the Flores Sea runs the length of the northern coastline. The Sape Strait lies to the east of the island and separates Sumbawa from Flores and the Komodo Islands, there are a number bays and gulfs, most notably Bima Bay, Cempi Bay, and Waworada Bay
Sumbawa's most distinguishing feature is Saleh Bay and the Tambora Peninsula with Mount Tambora. Highlands rise in four spots on the island, as well as on Sangeang Island, the large western lobe of Sumbawa is dominated by a large central highland, there is Mt. Tambora, Dompu and Bima each have more minor highlands.
There are a number of large surrounding islands, most notably are Moyo Island, volcanically active Sangeang Island, and the tourist Komodo Islands (administered under Flores) to the east.
List of offshore islands.
There are a number of smaller offshore islands which fall within the regencies based on Sumbawa Island:
Volcanoes.
Sumbawa lies within the Pacific Ring of Fire. It is a volcanic island, including Mount Tambora (8°14’41”S, 117°59’35”E) which exploded in April 1815, the most destructive volcanic eruption in modern history (roughly four times larger than the 1883 eruption of Krakatoa, between Java and Sumatra, in terms of volume of magma ejected). The eruption killed as many as 72,000. It also apparently destroyed a small culture of Southeast Asian affinity, known to archaeologists as the Tambora culture. It launched 100 km3 of ash into the upper atmosphere, which caused 1816 to be the "year without a summer". 
Economy.
 "We want to say that there has been a decline, but a slow decline. There is no seriousness from the government." (In reference to some 20 children died from malnutrition on Sumbawa in October, 2012) —Ida, Alliance of Prosperous Villages (ADS)
 
Many of the island residents are at risk of starvation when crops fail due to lack of rainfall. The majority of the population works in agriculture. Tourism is very nascent, with a few surf spots renowned for being world class, Sekongkang and Supersuck Beaches near the mine, as well as Hu'u and Lakey Beach in the Gulf of Cempi.
Due to the mine, Sumbawa Barat Regency (statistically), along with other remote mining towns, and Jakarta, have the highest GDP per capita rates in Indonesia, Sumbawa Barat's is 156.25 million rupiah ($17,170 USD) as of 2010, yet it is an area that sees entrenched, repeated starvation deaths and severe malnutrition in children.
Newmont Mine.
The Southwestern extreme portion of Sumbawa is monopolized by American firm Newmont Mining Corporation; a large gold and copper mine, Newmont's Batu Hijau mine in Sumbawa began commercial operations in 2000, a decade after the copper and gold was discovered. Newmont holds a 45% stake in the operation through its shareholding in PT Newmont Nusa Tenggara. A local unit of Japan's Sumitomo Corporation has a 35% share.
Newmont and its partners have invested about $1.9 billion in the mine. The reserves are expected to last until 2034, making Batu Hijau one of the largest copper mines in the world. Newmont has a been involved in scandals of mercury and arsenic poisoning in Sulawesi island, as well as having been embroiled in pollution cases on four continents., and also protests on Sumbawa itself, with police firing on protesters.
Transport.
There is a road network in Sumbawa, but it is poorly maintained and has long portions of rough gravel. Frequent ferry service to Sumbawa (Poto Tano) from Lombok (Labuhan Lombok) exists, however ferry service to Flores from Sape is infrequent. Bima is the largest city on Sumbawa and has ferry and bus service directly to Java and Bali, though service breakdowns are common.

</doc>
<doc id="28732" url="http://en.wikipedia.org/wiki?curid=28732" title="Spaghetti code">
Spaghetti code

Spaghetti code is a pejorative phrase for source code that has a complex and tangled control structure, especially one using many GOTO statements, exceptions, threads, or other "unstructured" branching constructs. It is named such because program flow is conceptually like a bowl of spaghetti, i.e. twisted and tangled. Spaghetti code can be caused by several factors, such as continuous modifications by several people over a long life cycle. Structured programming greatly decreases the incidence of spaghetti code.
History.
It is not clear when the phrase spaghetti code came into common usage; however, several references appeared in 1977 including "Macaroni is Better Than Spaghetti" by Steele published in Proceedings of the 1977 symposium on artificial intelligence and programming languages. In the 1978 book "A primer on disciplined programming using PL/I, PL/CS, and PL/CT", Richard Conway used the term to describe types of programs that "have the same clean logical structure as a plate of spaghetti", a phrase repeated in the 1979 book "An Introduction to Programming" he co-authored with David Gries. In the 1988 paper "A spiral model of software development and enhancement", the term is used to describe the older practice of the "code and fix model", which lacked planning and eventually led to the development of the waterfall model. In the 1979 book "Structured programming for the COBOL programmer", author Paul Noll uses the phrases "spaghetti code" and "rat's nest" as synonyms to describe poorly structured source code.
In a 1980 publication by the United States National Bureau of Standards, the phrase spaghetti program was used to describe older programs having "fragmented and scattered files". The consequences of using codice_1 statements in programs were described in a 1980 paper, which stated that it was perceived to be "evil".
In the "Ada – Europe '93" conference, Ada was described as forcing the programmer to "produce understandable, instead of spaghetti code", because of its restrictive exception propagation mechanism.
In a 1981 computer languages spoof in "The Michigan Technic" titled "BASICally speaking...FORTRAN bytes!!", the author described FORTRAN as "proof positive that the cofounders of IBM were Italian, for it consists entirely of spaghetti code".
Examples.
Here follows what would be considered a trivial example of spaghetti code in BASIC. The program prints each of the numbers 1 to 10 to the screen along with its square. Notice that indentation is not used to differentiate the various actions performed by the code, and that the program's codice_2 statements create a reliance on line numbers. Also observe the less easily predictable way the flow of execution jumps from one area to another. Real-world occurrences of spaghetti code are more complex and can add greatly to a program's maintenance costs.
Here is the same code written in a structured programming style:
The program jumps from one area to another, but this jumping is formal and more easily predictable, because for loops and functions provide flow control whereas the "goto" statement encourages arbitrary flow control. Though this example is small, real world programs are composed of many lines of code and are difficult to maintain when written in a spaghetti code fashion.
Assembly, scripting and other languages.
When using the many forms of assembly language (and also the underlying machine code) the danger of writing spaghetti code is especially great.
This is because they are low-level programming languages where equivalents for structured control flow statements such as for loops and while loops exist, but are often poorly understood by inexperienced programmers. Some scripting languages have the same deficiencies: this applies to the batch scripting language of DOS and DCL on VMS.
Nonetheless, adopting the same discipline as in structured programming can greatly improve the readability and maintainability of such code.
This may take the form of conventions limiting the use of codice_1 to correspond to the standard structures, or use of a set of assembler macros for codice_4 and codice_5 constructs.
Most assembly languages also provide a function stack, and function call mechanisms which can be used to gain the advantages of procedural programming. Macros can again be used to support a standardized form of parameter passing, to avoid ad hoc global variables and the action at a distance anti-pattern.
Some widely used newer programming languages, such as Python and Java, do not have the codice_1 statement, and therefore strongly encourage structured programming.
Related phrases.
The phrase "spaghetti code" has inspired the coinage of other terms that similarly compare program structure to styles of pasta. The general meta-phrase is "programming pasta".
Ravioli code.
Ravioli code is a type of computer program structure, characterized by a number of very small and (ideally) loosely coupled software components. The term stems from the analogy of ravioli (small pasta pouches containing cheese, meat, or vegetables) to modules (which ideally are encapsulated, consisting of both code and data). While generally desirable from a coupling and cohesion perspective, overzealous separation and encapsulation of code can bloat call stacks and make navigation through the code for maintenance purposes more difficult.
Lasagna code.
Lasagna code, a phrase coined in 1982 by Joe Celko, refers to a type of program structure characterized by several well-defined and separable "layers", where each layer of code accesses services in the layers below through well-defined interfaces. The analogy stems from the layered structure of lasagna, where different ingredients (for example, meat, sauce, vegetables, or cheese) are each separated by strips of pasta.
One common instance of lasagna code occurs at the interface between different subsystems, such as between web application code, business logic, and a relational database. Another common programming technique, alternate hard and soft layers (use of different programming languages at different levels of the program architecture), tends to produce lasagna code. In general, client–server applications are frequently lasagna code, with well-defined interfaces between client and server.
Lasagna code generally enforces encapsulation between the different "layers", as the subsystems in question may have no means of communication other than through a well-defined mechanism, such as Structured Query Language, a foreign function interface, or remote procedure call. However, individual layers in the system may be highly unstructured or disorganized.
A similar layering may be seen in communication stacks, where a protocol (such as the OSI model) is divided into layers (in this case seven), with each layer performing a limited and well-defined function and communicating with other layers using specific and standardized methods. Such a design eases the evolutionary improvement of the entire stack through layer-specific improvements.
Again, while loosely coupled layering is generally desirable in a program's architecture because it makes objects at each layer more interchangeable with existing or possible future implementations, other types of changes to the code will actually increase in complexity as more layers are added and so an extensively layered architecture can be seen as an anti-pattern as well. Adding a new field to a UI view, for example, requires changing every object at every layer in the architecture that is required to have knowledge about this new field (generally the view itself, any underlying controller/presenter class, data transfer objects, SOA layers, data access objects or mappings, and the database schema itself). A quote usually attributed either to David Wheeler or Butler Lampson reads: "There is no problem in computer science that cannot be solved by adding another layer of indirection, "except" having too many layers of indirection".
References.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.

</doc>
<doc id="28737" url="http://en.wikipedia.org/wiki?curid=28737" title="Semaphore (disambiguation)">
Semaphore (disambiguation)

Semaphore usually refers to flag semaphore. It may also refer to;

</doc>
<doc id="28740" url="http://en.wikipedia.org/wiki?curid=28740" title="Sulawesi">
Sulawesi

Sulawesi (formerly known as Celebes or ) is an island in Indonesia. One of the four Greater Sunda Islands, and the world's eleventh-largest island, it is situated between Borneo and the Maluku Islands. In Indonesia, only Sumatra, Borneo, and Papua are larger in territory, and only Java and Sumatra have larger populations.
Sulawesi comprises four peninsulas: the northern Minahasa Peninsula; the East Peninsula; the South Peninsula; and the South-east Peninsula. Three gulfs separate these peninsulas: the Gulf of Tomini between northern Minahasa peninsula and East Peninsula; the Tolo Gulf between East and Southeast Peninsula; and the Bone Gulf between the South and Southeast Peninsula. The Strait of Makassar runs along the western side of the island and separates the island from Borneo.
Etymology.
The Portuguese were the first to refer to Sulawesi as 'Celebes'. The name 'Sulawesi' possibly comes from the words "sula" ('island') and "besi" ('iron') and may refer to the historical export of iron from the rich Lake Matano iron deposits.
Geology.
The island slopes up from the shores of the deep seas surrounding the island to a high, mostly non-volcanic, mountainous interior. Active volcanoes are found in the northern Minahassa Peninsula, stretching north to the Sangihe Islands. The northern peninsula contains several active volcanoes such as Mount Lokon, Mount Awu, Soputan, and Karangetang.
According to plate reconstructions, the island is believed to have been formed by the collision of terranes from the Asian Plate (forming the west and southwest), and from the Australian Plate (forming the southeast and Banggai), with island arcs previously in the Pacific (forming the north and east peninsulas). Because of its several tectonic origin, faults scar the land; as a result, the island is prone to earthquakes.
Sulawesi, in contrast to most of the other islands in the biogeographical region of Wallacea, is not truly oceanic, but a composite island at the centre of the Asia-Australia collision zone. Parts of the island were formerly attached to either the Asian or Australian continental margin and became separated from these areas by vicariant processes. For one. in the west, the opening of the Makassar Strait separated West Sulawesi from Sundaland in the Eocene c. 45 Mya. In the east, the traditional view of collisions of multiple micro-continental fragments sliced from New Guinea with an active volcanic margin in West Sulawesi at different times since the Early Miocene c. 20 Mya has recently been replaced by the hypothesis that extensional fragmentation has followed a single Miocene collision of West Sulawesi with the Sula Spur, the western end of an ancient folded belt of Variscan origin in the Late Paleozoic.
Prehistory.
Before October 2014, the settlement of South Sulawesi by modern humans had been dated to c. 30,000 BC on the basis of radiocarbon dates obtained from rock shelters in Maros. No earlier evidence of human occupation had at that point been found, but the island almost certainly formed part of the land bridge used for the settlement of Australia and New Guinea by at least 40,000 BCE. There is no evidence of "Homo erectus" having reached Sulawesi; crude stone tools first discovered in 1947 on the right bank of the Walennae river at Berru, which were thought to date to the Pleistocene on the basis of their association with vertebrate fossils, are now thought to date to perhaps 50,000 BC.
Following Peter Bellwood's model of a southward migration of Austronesian-speaking farmers (AN), radiocarbon dates from caves in Maros suggest a date in the mid-second millennium BC for the arrival of an AN group from east Borneo speaking a Proto-South Sulawesi language (PSS). Initial settlement was probably around the mouth of the Sa'dan river, on the northwest coast of the peninsula, although the south coast has also been suggested. Subsequent migrations across the mountainous landscape resulted in the geographical isolation of PSS speakers and the evolution of their languages into the eight families of the South Sulawesi language group. If each group can be said to have a homeland, that of the Bugis – today the most numerous group – was around lakes Témpé and Sidénréng in the Walennaé depression. Here for some 2,000 years lived the linguistic group that would become the modern Bugis; the archaic name of this group (which is preserved in other local languages) was Ugiq. Despite the fact that today they are closely linked with the Makasar, the closest linguistic neighbors of the Bugis are the Toraja.
Pre-1200 CE Bugis society was most likely organized into chiefdoms. Some anthropologists have speculated these chiefdoms would have warred and, in times of peace, exchanged women with each other. Further they have speculated that personal security would have been negligible, and head-hunting an established cultural practice. The political economy would have been a mixture of hunting and gathering and swidden or shifting agriculture. Speculative planting of wet rice may have taken place along the margins of the lakes and rivers.
In Central Sulawesi there are over 400 granite megaliths, which various archaeological studies have dated to be from 3000 BC to 1300 AD. They vary in size from a few centimetres to ca.4.5 m. The original purpose of the megaliths is unknown. About 30 of the megaliths represent human forms. Other megaliths are in form of large pots ("Kalamba") and stone plates ("Tutu'na").
In October 2014 it was announced that cave paintings in Maros had been dated as being about 40,000 years old. Dr Maxime Aubert, of Griffith University in Queensland, Australia, said that the minimum age for the outline of a hand was 39,900 years old, which made it "the oldest hand stencil in the world" and added, "Next to it is a pig that has a minimum age of 35,400 years old, and this is one of the oldest figurative depictions in the world, if not the oldest one."
History.
Starting in the 13th century, access to prestige trade goods and to sources of iron started to alter long-standing cultural patterns, and to permit ambitious individuals to build larger political units. It is not known why these two ingredients appeared together; one was perhaps the product of the other. By 1400, a number of nascent agricultural principalities had arisen in the western Cenrana valley, as well as on the south coast and on the west coast near modern Parepare.
The first Europeans to visit the island (which they believed to be an archipelago due to its contorted shape) were the Portuguese sailors Simão de Abreu, in 1523, and Gomes de Sequeira (among others) in 1525, sent from the Moluccas in search of gold, which the islands had the reputation of producing. A Portuguese base was installed in Makassar in the first decades of the 16th century, lasting until 1665, when it was taken by the Dutch. The Dutch had arrived in Sulawesi in 1605 and were quickly followed by the English, who established a factory in Makassar. From 1660, the Dutch were at war with Gowa, the major Makasar west coast power. In 1669, Admiral Speelman forced the ruler, Sultan Hasanuddin, to sign the Treaty of Bongaya, which handed control of trade to the Dutch East India Company. The Dutch were aided in their conquest by the Bugis warlord Arung Palakka, ruler of the Bugis kingdom of Bone. The Dutch built a fort at Ujung Pandang, while Arung Palakka became the regional overlord and Bone the dominant kingdom. Political and cultural development seems to have slowed as a result of the status quo. In 1905 the entire island became part of the Dutch state colony of the Netherlands East Indies until Japanese occupation in World War II. During the Indonesian National Revolution, the Dutch Captain 'Turk' Westerling led campaigns in which hundreds, maybe thousands were executed during the South Sulawesi Campaign. Following the transfer of sovereignty in December 1949, Sulawesi became part of the federal United States of Indonesia, which in 1950 became absorbed into the unitary Republic of Indonesia.
Central Sulawesi.
The Portuguese were rumoured to have a fort in Parigi in 1555 (Balinese of Parigi, Central Sulawesi (Davis 1976), however she gives no source). The Kaili were an important group based in the Palu valley and related to the Toraja. Scholars relate that their control swayed under Ternate and Makassar, but this might have been a decision by the Dutch to give their vassals a chance to govern a difficult group. Padbruge commented that in the 1700 Kaili numbers were significant and a highly militant society. In the 1850s a war erupted between the Kaili groups, including the Banawa, in which the Dutch decided to intervene. A complex conflict also involving the Sulu Island pirates and probably Wyndham (a British merchant who commented on being involved in arms dealing to the area in this period and causing a row).
In the late 19th century the Sarasins journeyed through the Palu valley as part of a major initiative to bring the Kaili under Dutch rule. Some very surprising and interesting photographs were taken of shamen called Tadulako. Further Christian religious missions entered the area to make one of the most detailed ethnographic studies in the early 20th century (Kruyt & Adriani). A Swede by the name of Walter Kaudern later studied much of the literature and produced a synthesis. Erskine Downs in the 1950s produced a summary of Kruyts and Andrianis work: "The religion of the Bare'e-Speaking Toradja of Central Celebes," which is invaluable for English-speaking researchers. One of the most recent publications is "When the bones are left," a study of the material culture of central Sulawesi (Eija-Maija Kotilainen – History – 1992), offering extensive analysis. Also worthy of study is the brilliant works of Monnig Atkinson on the Wana shamen who live in the Mori area.
Geography.
Sulawesi is the world's eleventh-largest island, covering an area of 174600 km². The island is surrounded by Borneo to the west, by the Philippines to the north, by Maluku to the east, and by Flores and Timor to the south. It has a distinctive shape, dominated by four large peninsulas: the Semenanjung Minahassa; the East Peninsula; the South Peninsula; and the South-east Peninsula. The central part of the island is ruggedly mountainous, such that the island's peninsulas have traditionally been remote from each other, with better connections by sea than by road. Three bays dominate the island: Gulf of Tomini, Tolo Gulf, and Bone Gulf, while the Strait of Makassar runs the western side of the island.
Minor islands.
The Selayar Islands make up a peninsula stretching southwards from Southwest Sulawesi into the Flores Sea are administratively part of Sulawesi. The Sangihe Islands and Talaud Islands stretch northward from the northeastern tip of Sulawesi, while Buton Island and its neighbours lie off its southeast peninsula, the Togian Islands are in the Gulf of Tomini, and Peleng Island and Banggai Islands form a cluster between Sulawesi and Maluku. All the above-mentioned islands, and many smaller ones, are administratively part of Sulawesi's six provinces.
Population.
The 2000 census population of the provinces of Sulawesi was 14,946,488, about 7.25% of Indonesia's total population. By the 2010 Census the total had reached 17,371,782, and the latest official estimate (for January 2014) is 18,455,058. The largest city is Makassar.
Religion.
Islam is the majority religion in Sulawesi. The conversion of the lowlands of the south western peninsula (South Sulawesi) to Islam occurred in the early 17th century. The kingdom of Luwu in the Gulf of Bone was the first to accept Islam in February 1605; the Makassar kingdom of Goa-Talloq, centered on the modern-day city of Makassar, followed suit in September. However, the Gorontalo and the Mongondow peoples of the northern peninsula largely converted to Islam only in the 19th century. Most Muslims are Sunnis.
Christians form a substantial minority on the island. According to the demographer Toby Alice Volkman, 17% of Sulawesi's population is Protestant and less than 2% is Roman Catholic. Christians are concentrated on the tip of the northern peninsula around the city of Manado, which is inhabited by the Minahasa, a predominantly Protestant people, and the northernmost Sangir and Talaud Islands. The famous Toraja people of Tana Toraja in Central Sulawesi have largely converted to Christianity since Indonesia's independence. There are also substantial numbers of Christians around Lake Poso in Central Sulawesi, among the Pamona speaking peoples of Central Sulawesi, and near Mamasa.
Though most people identify themselves as Muslims or Christians, they often subscribe to local beliefs and deities as well. It is not uncommon for Christians to make offerings to local gods, goddesses, and spirits.
Smaller communities of Buddhists and Hindus are also found on Sulawesi, usually among the Chinese, Balinese and Indian communities.
Administration.
The island is subdivided into six provinces: Gorontalo, West Sulawesi, South Sulawesi, Central Sulawesi, Southeast Sulawesi, and North Sulawesi. West Sulawesi is the newest province, created in 2004 from part of South Sulawesi. The largest cities on the island are Makassar, Manado, Palu, Kendari, Bitung, Gorontalo, Palopo and Bau-Bau.
Capital city of South Sulawesi, and Biggest city in Sulawesi, Makassar.
Flora and fauna.
Sulawesi is part of Wallacea, meaning that it has a mix of both Asian and Australasian species. There are 8 national parks on the island, of which 4 are mostly marine. The parks with the largest terrestrial area are Bogani Nani Wartabone with 2,871 km² and Lore Lindu National Park with 2,290 km². Bunaken National Park which protects a rich coral ecosystem has been proposed as an UNESCO World Heritage Site.
Mammals.
There are 127 known mammalian species in Sulawesi. A large percentage of these mammals, 62% (79 species) are endemic, meaning that they are found nowhere else in Indonesia or the world. The largest native mammals in Sulawesi are the two species of anoa or dwarf buffalo. Other mammalian species inhabiting Sulawesi are the babirusas, which are aberrant pigs, the Sulawesi palm civet, and primates including a number of tarsiers (the spectral, Dian's, Lariang and pygmy species) and several species of macaque, including the crested black macaque, the moor macaque and the booted macaque. Although virtually all Sulawesi's mammals are placental, and generally have close relatives in Asia, several species of cuscus, marsupials of Australasian origin, are also present.
Birds.
By contrast, Sulawesian bird species tend to be found on other nearby islands as well, such as Borneo; 31% of Sulawesi's birds are found nowhere else. One endemic bird is the largely ground-dwelling, chicken-sized maleo, a megapode which uses hot sand close to the island's volcanic vents to incubate its eggs. There are around 350 known bird species in Sulawesi. An international partnership of conservationists, donors, and local people have formed the Alliance for Tompotika Conservation, in an effort to raise awareness and protect the nesting grounds of these birds on the central-eastern arm of the island.
Freshwater fishes.
Sulawesi also has several endemic species of freshwater fish, such as those in the genus "Nomorhamphus", a species flock of viviparous halfbeaks containing 12 species that only are found on Sulawesi (others are from the Philippines). In addition to "Nomorhamphus", the majority of Sulawesi's 70+ freshwater fish species are ricefishes, gobies ("Glossogobius" and "Mugilogobius") and Telmatherinid sail-fin silversides. The last family is almost entirely restricted to Sulawesi, especially the Malili Lake system, consisting of Matano and Towuti, and the small Lontoa (Wawantoa), Mahalona and Masapi. Another unusual endemic is "Lagusia micracanthus" from rivers in South Sulawesi, which is the sole member of its genus and among the smallest grunters. The gudgeon "Bostrychus microphthalmus" from the Maros Karst is the only described species of cave-adapted fish from Sulawesi, but an apparently undescribed species from the same region and genus also exists.
Freshwater crustaceans and snails.
There are many species of "Caridina" freshwater shrimp and parathelphusid freshwater crabs ("Migmathelphusa", "Nautilothelphusa", "Parathelphusa", "Sundathelphusa" and "Syntripsa") that are endemic to Sulawesi. Several of these species have become very popular in the aquarium hobby, and since most are restricted to a single lake system, they are potentially vulnerable to habitat loss and overexploitation. There are also several endemic cave-adapted shrimp and crabs, especially in the Maros Karst. This includes "Cancrocaeca xenomorpha", which has been called the "most highly cave-adapted species of crab known in the world".
The genus "Tylomelania" of freshwater snails is also endemic to Sulawesi, with the majority of the species restricted to Lake Poso and the Malili Lake system.
Miscellaneous.
mimic octopus
Conservation.
The island was recently the subject of an Ecoregional Conservation Assessment, coordinated by The Nature Conservancy. Detailed reports about the vegetation of the island are available. The assessment produced a detailed and annotated list of 'conservation portfolio' sites. This information was widely distributed to local government agencies and nongovernmental organizations. Detailed conservation priorities have also been outlined in a recent publication.
The lowland forests on the island have mostly been removed. Because of the relative geological youth of the island and its dramatic and sharp topography, the lowland areas are naturally limited in their extent. The past decade has seen dramatic conversion of this rare and endangered habitat. The island also possesses one of the largest outcrops of serpentine soil in the world, which support an unusual and large community of specialized plant species. Overall, the flora and fauna of this unique center of global biodiversity is very poorly documented and understood and remains critically threatened.
Environment.
The largest environmental issue in Sulawesi is deforestation. In 2007, scientists found that 80 percent of Sulawesi's forest had been lost or degraded, especially centered in the lowlands and the mangroves. Forests have been felled for logging and large agricultural projects. Loss of forest has resulted in many of Sulawesi's endemic species becoming endangered. In addition, 99 percent of Sulawesi's wetlands have been lost or damaged.
Other environmental threats included bushmeat hunting and mining.
Parks.
The island of Sulawesi has six national parks and nineteen nature reserves. In addition, Sulawesi has three marine protected areas. Many of Sulawesi's parks are threatened by logging, mining, and deforestation for agriculture.
References.
This article incorporates CC-BY-4.0 text from the reference

</doc>
<doc id="28769" url="http://en.wikipedia.org/wiki?curid=28769" title="Ship transport">
Ship transport

Ship transport is watercraft carrying people (passengers) or goods (cargo).
Sea transport has been the largest carrier of freight throughout recorded history. Although the importance of sea travel for passengers has decreased due to aviation, it is effective for short trips and pleasure cruises. Transport by water is cheaper than transport by air , despite fluctuating exchange rates and CAF charges to account for such.
Ship transport can be over any distance by boat, ship, sailboat or barge, over oceans and lakes, through canals or along rivers. Shipping may be for commerce, recreation or the military purpose. Virtually any material can be moved by water; however, water transport becomes impractical when material delivery is highly time-critical.
Containerization revolutionized ship transport starting in the 1970s. "General cargo" includes goods packaged in boxes, cases, pallets, and barrels. When a cargo is carried in more than one mode, it is intermodal or co-modal.
Merchant shipping.
A nation's shipping fleet (merchant navy, merchant marine, merchant fleet) consists of the ships operated by civilian crews to transport passengers or cargo from one place to another. Professionals are merchant seaman, merchant sailor, and merchant mariner, or simply seaman, sailor, or mariner. The terms "seaman" or "sailor" may refer to a member of a country's navy.
According to the 2005 CIA World Factbook, the total number of merchant ships of at least 1,000 gross register tons in the world was 30,936. In 2010, it was 38,988, an increase of 26%. Statistics for individual countries are available at the list of merchant navy capacity by country.
Professional mariners.
A ship's complement can be divided into four categories: the deck department, the engineering department, the steward's department, and other.
Deck department.
 Officer positions in the deck department include but not limited to: Master and his Chief, Second, and Third officers. The official classifications for unlicensed members of the deck department are Able Seaman and Ordinary Seaman.
A common deck crew for a ship includes:
A deck cadet is person who is carrying out mandatory seatime to achieve their officer of the watch certificate. Their time onboard is spent learning the operations and tasks of everyday life on a merchant vessel.
Engineering department.
A ship's engineering department consists of the members of a ship's crew that operate and maintain the propulsion and other systems on board the vessel. Marine Engineering staff also deal with the "Hotel" facilities on board, notably the sewage, lighting, air conditioning and water systems. They deal with bulk fuel transfers, and require training in firefighting and first aid, as well as in dealing with the ship's boats and other nautical tasks- especially with cargo loading/discharging gear and safety systems, though the specific cargo discharge function remains the responsibility of deck officers and deck workers. On LPG and LNG tankers however, a cargo engineer works with the deck department during cargo operations, as well as being a watchkeeping engineer.
A common Engineering crew for a ship includes:
Many American ships also carry a Qualified Member of the Engine Department. Other possible positions include Motorman, Machinist, Electrician, Refrigeration Engineer, and Tankerman. Engine Cadets are trainee engineers who are completing sea time necessary before they can obtain a watchkeeping license.
Steward's department.
A typical Steward's department for a cargo ship would be composed of a Chief Steward, a Chief Cook, and a Steward's Assistant. All three positions are typically filled by unlicensed personnel.
The chief steward directs, instructs, and assigns personnel performing such functions as preparing and serving meals; cleaning and maintaining officers' quarters and steward department areas; and receiving, issuing, and inventorying stores.
On large passenger vessels, the Catering Department is headed by the Chief Purser and managed by assistant pursers. Although they enjoy the benefits of having officer rank, they generally progress through the ranks to become pursers. Under the pursers are the department heads - such as chief cook, head waiter, head barman etc. They are responsible for the administration of their own areas.
The chief steward also plans menus; compiles supply, overtime, and cost control records. May requisition or purchase stores and equipment. They may bake bread, rolls, cakes, pies, and pastries.
A chief steward's duties may overlap with those of the Steward's Assistant, the Chief Cook, and other Steward's Department crewmembers.
In the United States Merchant Marine, in order to be occupied as a chief steward a person has to have a Merchant Mariner's Document issued by the United States Coast Guard. Because of international conventions and agreements, all chief cooks who sail internationally are similarly documented by their respective countries.
Other Departments.
Staff officer positions on a ship, including Junior Assistant Purser, Senior Assistant Purser, Purser, Chief Purser, Medical Doctor, Professional Nurse, Marine Physician Assistant, and Hospital Corpsman, are considered administrative positions and are therefore regulated by Certificates of Registry issued by the United States Coast Guard. Pilots are also merchant marine officers and are licensed by the Coast Guard. Formerly, there was also a radio department, headed by a chief radio officer and supported by a number of radio officers. Since the introduction of GMDSS (Satellite communications) and the subsequent exemptions from carrying radio officers if the vessel is so equipped, this department has fallen away, although many ships do still carry specialist radio officers, particularly passenger vessels. Many radio officers became 'electro-technical officers', and transferred into the engineering department.
Life at sea.
Mariners spend much of their life beyond the reach of land. They sometime face dangerous conditions at sea. Yet men and women still go to sea. For some, the attraction is a life unencumbered with the restraints of life ashore. Seagoing adventure and a chance to see the world also appeal to many seafarers. Whatever the calling, those who live and work at sea invariably confront social isolation.
Findings by the Seafarer's International Research Center indicate a leading cause of mariners leaving the industry is "almost invariably because they want to be with their families." U.S. merchant ships typically do not allow family members to accompany seafarers on voyages. Industry experts increasingly recognize isolation, stress, and fatigue as occupational hazards. Advocacy groups such as International Labour Organization, a United Nations agency, and the Nautical Institute are seeking improved international standards for mariners. Satellite phones have improved communication and efficiency aboard sea-fairing ships. This technology has contributed to crew welfare, although both equipment and fees are expensive.
Ocean voyages are steeped in routine. Maritime tradition dictates that each day be divided into six four-hour periods. Three groups of watch keepers from the engine and deck departments work four hours on then have eight hours off watch keeping. However, there are many overtime jobs to be done daily. This cycle repeats endlessly, 24 hours a day while the ship is at sea. Members of the steward department typically are day workers who put in at least eight-hour shifts. Operations at sea, including repairs, safeguarding against piracy, securing cargo, underway replenishment, and other duties provide opportunities for overtime work. Service aboard ships typically extends for months at a time, followed by protracted shore leave. However, some seamen secure jobs on ships they like and stay aboard for years.
The quick turnaround of many modern ships, spending only a few hours in port, limits a seafarer's free-time ashore. Moreover, some foreign seamen entering U.S. ports from a watch list of 25 countries face restrictions on shore leave due to security concerns. However, shore leave restrictions while in U.S. ports impact American seamen as well. For example, the International Organization of Masters, Mates & Pilots notes a trend of U.S. shipping terminal operators restricting seamen from traveling from the ship to the terminal gate. Furthermore, in cases where transit is allowed, special "security fees" are at times assessed.
Such restrictions on shore leave, coupled with reduced time in port, translate into longer periods at sea. Mariners report that extended periods at sea living and working with shipmates, who for the most part are strangers, takes getting used to. At the same time, there is an opportunity to meet people from other ethnic and cultural backgrounds. Recreational opportunities have improved aboard some U.S. ships, which may feature gyms and day rooms for watching movies, swapping sea stories, and other activities. And in some cases, especially tankers, it is possible for a mariner to be accompanied by members of his family. However, a mariner’s off-duty time is largely a solitary affair, pursuing hobbies, reading, writing letters, and sleeping.
On modern ocean-going vessels, typically registered with a flag of convenience, life has changed immensely in the last 20 years. Most large vessels include a gym and often a swimming pool for use by the crew. Since the "Exxon Valdez" incident, the focus of leisure time activity has shifted from having officer and crew bars, to simply having lounge-style areas where officers or crew can sit to watch movies. With many companies now providing TVs and DVD players in cabins, and enforcing strict smoking policies, it is not surprising that the bar is now a much quieter place on most ships. In some instances games consoles are provided for the officers and crew. The officers enjoy a much higher standard of living on board ocean-going vessels.
Crews are generally poorly paid, poorly qualified and have to complete contracts of approximately 9 months before returning home on leave. They often come from countries where the average industrial wage is still very low, such as the Philippines or India. Officers however, come from all over the world and it is not uncommon to mix the nationality of the officers on board ships. Officers are often the recipients of university degrees and have completed vast amounts of training in order to reach their rank. Officers benefit e.g. by having larger, more comfortable cabins and table service for their meals.
Contracts average at the 4 month mark for officers, with generous leave. Most ocean-going vessels now operate an unmanned engine room system allowing engineers to work days only. The engine room is computer controlled by night, although the duty engineer will make inspections during unmanned operation. Engineers work in a hot, humid, noisy atmosphere. Communication in the engine room is therefore by hand signals and lip-reading, and good teamwork often stands in place of any communication at all.
Ships and watercraft.
Ships and other watercraft are used for ship transport. Types can be distinguished by propulsion, size or cargo type. Recreational or educational craft still use wind power, while some smaller craft use internal combustion engines to drive one or more propellers, or in the case of jet boats, an inboard water jet. In shallow draft areas, such as the Everglades, some craft, such as the hovercraft, are propelled by large pusher-prop fans.
Most modern merchant ships can be placed in one of a few categories, such as:
Ships that fall outside these categories include Semi-submersible heavy-lift ships or OHGC.
Liners and Tramps.
A ship may also be categorised as to how it is operated.
A liner will have a regular run and operate to a schedule. The scheduled operation requires that such ships are better equipped to deal with causes of potential delay such as bad weather. They are generally higher powered than tramp ships with better seakeeping qualities, thus they are significantly more expensive to build. Liners are typically built for passenger and container operation though past common uses also included mail and general cargo.
A tramp has no fixed run but will go wherever a suitable cargo takes it. Thus a ship and crew may be chartered from the ship owner to fetch a cargo of grain from Canada to Latvia, the ship may then be required to carry a cargo of coal from Britain to Melanesia. Bulk carriers and cruise ships are examples of ships built to operate in this manner.
Typical in-transit times.
A cargo ship sailing from a European port to a US one will typically take 10–12 days depending on water currents and other factors. In order to make container ship transport more economical in the face of declining demand for intercontinental shipping, ship operators sometimes reduce cruising speed, thereby increasing transit time, to reduce fuel consumption, a strategy referred to as "slow steaming".
Ship transport infrastructure.
For a port to efficiently send and receive cargo, it requires infrastructure. Harbors, seaports and marinas host watercraft, and consist of components such as piers, wharfs, docks and roadsteads.

</doc>
<doc id="28791" url="http://en.wikipedia.org/wiki?curid=28791" title="Sovereignty">
Sovereignty

Sovereignty is understood in jurisprudence as the full right and power of a governing body to govern itself without any interference from outside sources or bodies. In political theory, sovereignty is a substantive term designating supreme authority over some polity. It is a basic principle underlying the dominant Westphalian model of state foundation.
Derived from Latin through French "souveraineté", its attainment and retention, in both Chinese and Western culture, has traditionally been associated with certain moral imperatives upon any claimant.
Different approaches.
The concept of sovereignty has been discussed throughout history, from the time before recorded history through to the present day. It has changed in its definition, concept, and application throughout, especially during the Age of Enlightenment. The current notion of state sovereignty contains four aspects consisting of territory, population, authority and recognition. According to Stephen D. Krasner, the term could also be understood in four different ways: 
Often, these four aspects all appear together, but this is not necessarily the case – they are not affected by one another, and there are historical examples of states that were non-sovereign in one aspect while at the same time being sovereign in another of these aspects. According to Immanuel Wallerstein, another fundamental feature of sovereignty is that it is a claim that must be recognised by others if it is to have any meaning: "Sovereignty is more than anything else a matter of legitimacy [...that] requires reciprocal recognition. Sovereignty is a hypothetical trade, in which two potentially conflicting sides, respecting de facto realities of power, exchange such recognitions as their least costly strategy."
History.
Classical.
The Roman jurist Ulpian observed that:
Ulpian was expressing the idea that the Emperor exercised a rather absolute form of sovereignty, although he did not use the term expressly.
Medieval.
Classical Ulpian's statements were known in medieval Europe, but sovereignty was an important concept in medieval times. Medieval monarchs were "not" sovereign, at least not strongly so, because they were constrained by, and shared power with, their feudal aristocracy. Furthermore, both were strongly constrained by custom.
Sovereignty existed during the Medieval Period as the "de jure" rights of nobility and royalty, and in the "de facto" capability of individuals to make their own choices in life.
Around c. 1380–1400, the issue of feminine sovereignty was addressed in Geoffrey Chaucer's Middle English collection of "Canterbury Tales", specifically in "The Wife of Bath's Tale."
A later English Arthurian romance, "The Wedding of Sir Gawain and Dame Ragnell" (c. 1450), uses much of the same elements of the Wife of Bath's tale, yet changes the setting to the court of King Arthur and the Knights of the Round Table. The story revolves around the knight Sir Gawain granting to Dame Ragnell, his new bride, what is purported to be wanted most by women: sovereignty.
We desire most from men,
From men both lund and poor,
To have sovereignty without lies.
For where we have sovereignty, all is ours,
Though a knight be ever so fierce,
And ever win mastery.
It is our desire to have master
Over such a sir.
Such is our purpose.—The Wedding of Sir Gawain and Dame Ragnell (c. 1450), 
Reformation.
Sovereignty reemerged as a concept in the late 16th century, a time when civil wars had created a craving for stronger central authority, when monarchs had begun to gather power onto their own hands at the expense of the nobility, and the modern nation state was emerging. Jean Bodin, partly in reaction to the chaos of the French wars of religion, presented theories of sovereignty calling for strong central authority in the form of absolute monarchy. In his 1576 treatise "" ("Six Books of the Republic") Bodin argued that it is inherent in the nature of the state that sovereignty must be:
Bodin rejected the notion of transference of sovereignty from people to sovereign; natural law and divine law confer upon the sovereign the right to rule. And the sovereign is not above divine law or natural law. He is above ("ie." not bound by) only positive law, that is, laws made by humans. The fact that the sovereign must obey divine and natural law imposes ethical constraints on him. Bodin also held that the "lois royales", the fundamental laws of the French monarchy which regulated matters such as succession, are natural laws and are binding on the French sovereign.
How divine and natural law could in practice be enforced on the sovereign is a problematic feature of Bodin's philosophy: any person capable of enforcing them on him would be above him.
Despite his commitment to absolutism, Bodin held some moderate opinions on how government should in practice be carried out. He held that although the sovereign is not obliged to, it is advisable for him, as a practical expedient, to convene a senate from whom he can obtain advice, to delegate some power to magistrates for the practical administration of the law, and to use the Estates as a means of communicating with the people.
With his doctrine that sovereignty is conferred by divine law, Bodin predefined the scope of the divine right of kings.
Age of Enlightenment.
During the Age of Enlightenment, the idea of sovereignty gained both legal and moral force as the main Western description of the meaning and power of a State. In particular, the "Social Contract" as a mechanism for establishing sovereignty was suggested and, by 1800, widely accepted, especially in the new United States and France, though also in Great Britain to a lesser extent.
Thomas Hobbes, in "Leviathan" (1651) borrowed Bodin's definition of sovereignty, which had just achieved legal status in the "Peace of Westphalia", and explained its origin. He created the first modern version of the social contract (or contractarian) theory, arguing that to overcome the "nasty, brutish and short" quality of life without the cooperation of other human beings, people must join in a "commonwealth" and submit to a "Soveraigne ["sic"] Power" that is able to compel them to act in the common good. This expediency argument attracted many of the early proponents of sovereignty. Hobbes strengthened the definition of sovereignty beyond either Westphalian or Bodin's, by saying that it must be:
Hobbes' hypothesis—that the ruler's sovereignty is contracted to him by the people in return for his maintaining their physical safety—led him to conclude that if and when the ruler fails, the people recover their ability to protect themselves by forming a new contract.
Hobbes's theories decisively shape the concept of sovereignty through the medium of social contract theories. Jean-Jacques Rousseau's (1712–1778) definition of popular sovereignty (with early antecedents in Francisco Suárez's theory of the origin of power), provides that the people are the legitimate sovereign. Rousseau considered sovereignty to be inalienable; he condemned the distinction between the origin and the exercise of sovereignty, a distinction upon which constitutional monarchy or representative democracy is founded. John Locke, and Montesquieu are also key figures in the unfolding of the concept of sovereignty; their views differ with Rousseau and with Hobbes on this issue of alienability.
The second book of Jean-Jacques Rousseau's "Du Contrat Social, ou Principes du droit politique" (1762) deals with sovereignty and its rights. Sovereignty, or the general will, is inalienable, for the will cannot be transmitted; it is indivisible, since it is essentially general; it is infallible and always right, determined and limited in its power by the common interest; it acts through laws. Law is the decision of the general will in regard to some object of common interest, but though the general will is always right and desires only good, its judgment is not always enlightened, and consequently does not always see wherein the common good lies; hence the necessity of the legislator. But the legislator has, of himself, no authority; he is only a guide who drafts and proposes laws, but the people alone (that is, the sovereign or general will) has authority to make and impose them.
Rousseau, in his 1763 treatise "Of the Social Contract"
argued, "the growth of the State giving the trustees of public authority more and means to abuse their power, the more the Government has to have force to contain the people, the more force the Sovereign should have in turn in order to contain the Government," with the understanding that the Sovereign is "a collective being of wonder" (Book II, Chapter I) resulting from "the general will" of the people, and that "what any man, whoever he may be, orders on his own, is not a law" (Book II, Chapter VI) – and furthermore predicated on the assumption that the people have an unbiased means by which to ascertain the general will. Thus the legal maxim, "there is no law without a sovereign."
Definition and types.
There exists perhaps no conception the meaning of which is more controversial than that of sovereignty. It is an indisputable fact that this conception, from the moment when it was introduced into political science until the present day, has never had a meaning which was universally agreed upon. 
Lassa Oppenheim (30-03-1858 – 07-10-1919) 
Absoluteness.
An important factor of sovereignty is its degree of absoluteness. A sovereign power has absolute sovereignty when it is not restricted by a constitution, by the laws of its predecessors, or by custom, and no areas of law or policy are reserved as being outside its control. International law; policies and actions of neighboring states; cooperation and respect of the populace; means of enforcement; and resources to enact policy are factors that might limit sovereignty. For example, parents are not guaranteed the right to decide some matters in the upbringing of their children independent of societal regulation, and municipalities do not have unlimited jurisdiction in local matters, thus neither parents nor municipalities have absolute sovereignty. Theorists have diverged over the desirability of increased absoluteness.
Exclusivity.
A key element of sovereignty in a legalistic sense is that of exclusivity of jurisdiction. Specifically, the degree to which decisions made by a sovereign entity might be contradicted by another authority. Along these lines, the German sociologist Max Weber proposed that sovereignty is a community's monopoly on the legitimate use of force; and thus any group claiming the same right must either be brought under the yoke of the sovereign, proven illegitimate, or otherwise contested and defeated for sovereignty to be genuine. International law, competing branches of government, and authorities reserved for subordinate entities (such as federated states or republics) represent legal infringements on exclusivity. Social institutions such as religious bodies, corporations, and competing political parties might represent de facto infringements on exclusivity.
De jure and de facto.
De jure, or legal, sovereignty concerns the expressed and institutionally recognised right to exercise control over a territory. De facto, or actual, sovereignty is concerned with whether control in fact exists. Cooperation and respect of the populace; control of resources in, or moved into, an area; means of enforcement and security; and ability to carry out various functions of state all represent measures of de facto sovereignty. When control is practiced predominately by military or police force it is considered "coercive sovereignty".
Sovereignty and independence.
State sovereignty is sometimes viewed synonymously with independence, however, sovereignty can be transferred as a legal right whereas independence cannot. A state can achieve de facto independence long after acquiring sovereignty, such as in the case of Cambodia, Laos and Vietnam. Additionally, independence can also be suspended when an entire region becomes subject to an occupation such as when Iraq had been overrun by the forces to take part in the Iraq War of 2003, Iraq had not been annexed by any country, so its sovereignty during this period was not contested by any state including those present on the territory. Alternatively, independence can be lost completely when sovereignty itself becomes the subject of dispute. The pre-World War II administrations of Latvia, Lithuania and Estonia maintained an exile existence (and considerable international recognition) whilst the entities were annexed by the Soviet Union and governed locally by their pro-Soviet functionaries. When in 1991 Latvia, Lithuania and Estonia re-enacted independence, it was done so on the basis of continuity directly from the pre-Soviet republics. Another complicated sovereignty scenario can arise when regime itself is the subject of dispute. In the case of Poland, the People's Republic of Poland which governed Poland from 1945 to 1989 is now seen to have been an illegal entity by the modern Polish administration. The post-1989 Polish state claims direct continuity from the Second Polish Republic which ended in 1939. For other reasons however, Poland maintains its communist-era outline as opposed to its pre-World War II shape which included areas now in Belarus, Czech Republic, Lithuania, Slovakia and Ukraine but did not include some of its western regions that were then in Germany.
At the opposite end of the scale, there is no dispute regarding the independence of Republic of Abkhazia, Republic of South Ossetia or the Republic of Kosovo (see List of states with limited recognition) since the factually governing bodies are neither part of a bigger state, nor are they subjected to supervision. The sovereignty (i.e. legal right to govern) however, is disputed in all three cases as the first two entities are claimed by Georgia and the third by Serbia.
Internal.
Internal sovereignty is the relationship between a sovereign power and its own subjects. A central concern is legitimacy: by what right does a government exercise authority? Claims of legitimacy might refer to the divine right of kings or to a social contract (i.e. popular sovereignty).
With Sovereignty meaning holding supreme, independent authority over a region or state, Internal Sovereignty refers to the internal affairs of the state and the location of supreme power within it. A state that has internal sovereignty is one with a government that has been elected by the people and has the popular legitimacy. Internal sovereignty examines the internal affairs of a state and how it operates. It is important to have strong internal sovereignty in relation to keeping order and peace. When you have weak internal sovereignty organization such as rebel groups will undermine the authority and disrupt the peace. The presence of a strong authority allows you to keep agreement and enforce sanctions for the violation of laws. The ability for leadership to prevent these violations is a key variable in determining internal sovereignty. The lack of internal sovereignty can cause war in one of two ways: first, undermining the value of agreement by allowing costly violations; and second, requiring such large subsidies for implementation that they render war cheaper than peace. Leadership needs to be able to promise members, especially those like armies, police forces, or paramilitaries will abide by agreements. The presence of strong internal sovereignty allows a state to deter opposition groups in exchange for bargaining. It has been said that a more decentralized authority would be more efficient in keeping peace because the deal must please not only the leadership but also the opposition group. While the operations and affairs within a state are relative to the level of sovereignty within that state, there is still an argument between who should hold the authority in a sovereign state.
This argument between who should hold the authority within a sovereign state is called the traditional doctrine of public sovereignty. This discussion is between an internal sovereign or an authority of public sovereignty. An internal sovereign is a political body that possesses ultimate, final and independent authority; one whose decisions are binding upon all citizens, groups and institutions in society. Early thinkers believe sovereignty should be vested in the hands of a single person, a monarch. They believed the overriding merit of vesting sovereignty in a single individual was that sovereignty would therefore be indivisible; it would be expressed in a single voice that could claim final authority. An example of an internal sovereign or monarch is Louis XIV of France during the seventeenth century; Louis XIV claimed that he was the state. Jean-Jacques Rousseau rejected monarchical rule in favor of the other type of authority within a sovereign state, public sovereignty. Public Sovereignty is the belief that ultimate authority is vested in the people themselves, expressed in the idea of the general will. This means that the power is elected and supported by its members, the authority has a central goal of the good of the people in mind. The idea of public sovereignty has often been the basis for modern democratic theory.
Modern internal sovereignty.
Within the modern governmental system, internal sovereignty is usually found in states that have public sovereignty and rarely found within a state controlled by an internal sovereign. A form of government that is a little different from both is the UK parliament system. From 1790 to 1859 it was argued that sovereignty in the UK was vested neither in the Crown nor in the people but in the "Monarch in Parliament". This is the origin of the doctrine of parliamentary sovereignty and is usually seen as the fundamental principle of the British constitution. With these principles of parliamentary sovereignty majority control can gain access to unlimited constitutional authority, creating what has been called "elective dictatorship" or "modern autocracy". Public sovereignty in modern governments is a lot more common with examples like the USA, Canada, Australia and India where government is divided into different levels.
External.
External sovereignty concerns the relationship between a sovereign power and other states. For example, the United Kingdom uses the following criterion when deciding under what conditions other states recognise a political entity as having sovereignty over some territory;
"Sovereignty." A government which exercises de facto administrative control over a country and is not subordinate to any other government in that country or a foreign sovereign state. 
("The Arantzazu Mendi", [1939] A.C. 256) 
External sovereignty is connected with questions of international law – such as: when, if ever, is intervention by one country onto another's territory permissible?
Following the Thirty Years' War, a European religious conflict that embroiled much of the continent, the Peace of Westphalia in 1648 established the notion of territorial sovereignty as a norm of noninterference in the affairs of other nations, so-called Westphalian sovereignty, even though the actual treaty itself reaffirmed the multiple levels of sovereignty of the Holy Roman Empire. This resulted as a natural extension of the older principle of "cuius regio, eius religio" (Whose realm, his religion), leaving the Roman Catholic Church with little ability to interfere with the internal affairs of many European states. It is a myth, however, that the Treaties of Westphalia created a new European order of equal sovereign states.
In international law, sovereignty means that a government possesses full control over affairs within a territorial or geographical area or limit. Determining whether a specific entity is sovereign is not an exact science, but often a matter of diplomatic dispute. There is usually an expectation that both de jure and de facto sovereignty rest in the same organisation at the place and time of concern. Foreign governments use varied criteria and political considerations when deciding whether or not to recognise the sovereignty of a state over a territory. Membership in the United Nations requires that "[t]he admission of any such state to membership in the United Nations will be effected by a decision of the General Assembly upon the recommendation of the Security Council."
Sovereignty may be recognized even when the sovereign body possesses no territory or its territory is under partial or total occupation by another power. The Holy See was in this position between the annexation in 1870 of the Papal States by Italy and the signing of the Lateran Treaties in 1929, a 59-year period during which it was recognised as sovereign by many (mostly Roman Catholic) states despite possessing no territory – a situation resolved when the Lateran Treaties granted the Holy See sovereignty over the Vatican City. Another case, "sui generis", is the Sovereign Military Order of Malta, the third sovereign entity inside Italian territory (after San Marino and the Vatican City State) and the second inside the Italian capital (since in 1869 the Palazzo di Malta and the Villa Malta receive extraterritorial rights, in this way becoming the only "sovereign" territorial possessions of the modern Order), which is the last existing heir to one of several once militarily significant, crusader states of sovereign military orders. In 1607 its Grand masters were also made Reichsfürst (princes of the Holy Roman Empire) by the Holy Roman Emperor, granting them seats in the Reichstag, at the time the closest permanent equivalent to a UN-type general assembly; confirmed 1620). These sovereign rights were never deposed, only the territories were lost. 100 modern states still maintain full diplomatic relations with the order (now de facto "the most prestigious service club"), and the UN awarded it observer status.
The governments-in-exile of many European states (for instance, Norway, Netherlands or Czechoslovakia) during the Second World War were regarded as sovereign despite their territories being under foreign occupation; their governance resumed as soon as the occupation had ended. The government of Kuwait was in a similar situation "vis-à-vis" the Iraqi occupation of its country during 1990–1991. The government of Republic of China was recognized as sovereign over China from 1911 to 1971 despite that its mainland China territory became occupied by Communist Chinese forces since 1949. In 1971 it lost UN recognition to Chinese Communist-led People's Republic of China and its sovereign and political status as a state became disputed and it lost its ability to use "China" as its name and therefore became commonly known as Taiwan.
The International Committee of the Red Cross is commonly mistaken to be sovereign. It has been granted various degrees of special privileges and legal immunities in many countries, that in cases like Switzerland are considerable, which are described. The Committee is a private organisation governed by Swiss law.
Shared and pooled.
Just as the office of head of state can be vested jointly in several persons within a state, the sovereign jurisdiction over a single political territory can be shared jointly by two or more consenting powers, notably in the form of a condominium.
Likewise the member states of international organizations may voluntarily bind themselves by treaty to a supranational organization, such as a continental union. In the case of the European Union members states this is called "pooled sovereignty".
Nation-states.
A community of people who claim the right of self-determination based on a common ethnicity, history and culture might seek to establish sovereignty over a region, thus creating a nation-state. Such nations are sometimes recognised as autonomous areas rather than as fully sovereign, independent states.
Federations.
In a federal system of government, "sovereignty" also refers to powers which a constituent state or republic possesses independently of the national government. In a confederation constituent entities retain the right to withdraw from the national body, but in a federation member states or republics do not hold that right.
Different interpretations of state sovereignty in the United States of America, as it related to the expansion of slavery and fugitive slave laws, led to the outbreak of the American Civil War. Depending on the particular issue, sometimes both northern and southern states justified their political positions by appealing to state sovereignty. Fearing that slavery would be threatened by results of the federal election, eleven slave states declared their independence from the federal Union and formed a new confederation. The United States government rejected the secessions as rebellion, declaring that secession from the Union by an individual state was unconstitutional, as the states were part of an indissolvable federation. 
Acquisition.
A number of modes of acquisition of sovereignty are presently or have historically been recognised by international law as lawful methods by which a state may acquire sovereignty over territory. The classification of these modes originally derived from Roman property law and from the 15th and 16th century with the development of international law. The modes are:
  full national jurisdiction and sovereignty
  restrictions on national jurisdiction and sovereignty
  international jurisdiction per common heritage of mankind
Justifications.
There exist vastly differing views on the moral basis of sovereignty. A fundamental polarity is between theories that assert that sovereignty is vested directly in the sovereign by divine or natural right and theories that assert it originates from the people. In the latter case there is a further division into those that assert that the people transfer their sovereignty to the sovereign (Hobbes), and those that assert that the people retain their sovereignty (Rousseau).
During the brief period of Absolute monarchies in Europe, the divine right of kings was an important competing justification for the exercise of sovereignty. The Mandate of Heaven had some similar implications in China.
A republic is a form of government in which the people, or some significant portion of them, retain sovereignty over the government and where offices of state are not granted through heritage. A common modern definition of a republic is a government having a head of state who is not a monarch.
Democracy is based on the concept of "popular sovereignty". In a direct democracy the public plays an active role in shaping and deciding policy. Representative democracy permits a transfer of the exercise of sovereignty from the people to a legislative body or an executive (or to some combination of legislature, executive and Judiciary). Many representative democracies provide limited direct democracy through referendum, initiative, and recall.
Parliamentary sovereignty refers to a representative democracy where the parliament is ultimately sovereign and not the executive power nor the judiciary.
Views.
According to Matteo Laruffa "sovereignty resides in every public action and policy as the exercise of executive powers by institutions open to the participation of citizens to the decision-making processes"
Relation to rule of law.
Another topic is whether the law is held to be sovereign, that is, whether it is above political or other interference. Sovereign law constitutes a true state of law, meaning the letter of the law (if constitutionally correct) is applicable and enforceable, even when against the political will of the nation, as long as not formally changed following the constitutional procedure. Strictly speaking, any deviation from this principle constitutes a revolution or a coup d'état, regardless of the intentions.

</doc>
<doc id="28819" url="http://en.wikipedia.org/wiki?curid=28819" title="Generalissimo Francisco Franco is still dead">
Generalissimo Francisco Franco is still dead

"Generalissimo Francisco Franco is still dead" is a catchphrase that originated in 1975 during the first season of "NBC's Saturday Night" (now called "Saturday Night Live", or "SNL") and which mocked the weeks-long media reports of the Spanish dictator's impending death. It was one of the first catchphrases from the series to enter the general lexicon.
Origin.
The death of Spanish dictator Francisco Franco during the first season of "NBC's Saturday Night" served as the source of the phrase. Franco lingered near death for weeks before dying. On slow news days, United States network television newscasters sometimes noted that Franco was still alive, or not yet dead. The imminent death of Franco was a headline story on the NBC news for a number of weeks prior to his death on November 20, 1975.
After Franco's death, Chevy Chase, reader of the news on "NBC's Saturday Night"'s comedic news segment "Weekend Update", announced the dictator's death and read a quotation from Richard Nixon: "General Franco was a loyal friend and ally of the United States. He earned worldwide respect for Spain through firmness and fairness." As an ironic counterpoint to this, a picture was displayed behind Chase, showing Franco giving the fascist salute alongside Adolf Hitler.
In subsequent weeks Chase developed the joke into a parody of the earlier news coverage of Franco's illness, treating his death as the top story. "This breaking news just in", Chase would announce – "Generalissimo Francisco Franco is "still" dead!" Occasionally, Chase would change the wording slightly in order to keep the joke fresh, e.g. "Generalissimo Francisco Franco is still valiantly holding on in his fight to remain dead." The joke was sometimes combined with another running gag in which, rather than having a sign language interpreter visually presenting the news to aid the deaf, the show would provide assistance from Garrett Morris, "head of the New York School for the Hard of Hearing", whose "aid" involved cupping his hands around his mouth and shouting the news as Chase read it. The gag ran until early 1977.
The line was perceived as a slap at "NBC Nightly News" main anchor John Chancellor, who due to his background as a foreign correspondent, felt the network should weigh its news more heavily toward world events, and had kept Franco's deathwatch at the top of the headlines.
Legacy.
The phrase has remained in use since Franco's death. James Taranto's "Best of the Web Today" column at OpinionJournal.com uses the phrase as a tag for newspaper headlines that indicate something is still happening when it should be obvious. On February 8, 2007, during Jack Cafferty's segment on CNN's "The Situation Room" on the day of the death of Anna Nicole Smith, he asked of CNN correspondent Wolf Blitzer "Is Anna Nicole Smith still dead, Wolf?" It was also used now and then on "NBC News Overnight" in the early 1980s, and Keith Olbermann occasionally used it on "Countdown".
"The Wall Street Journal" used the headline: "Generalísimo Francisco Franco Is Still Dead – And His Statues Are Next" (front page; March 2, 2009).
The practice of American television networks continually reporting that ailing world leaders are still alive remains widespread. Famous examples include Yasser Arafat in 2004, Pope John Paul II in 2005, Fidel Castro in late 2006 and early 2007, Hosni Mubarak in 2012 and Hugo Chávez in 2013.
Although "SNL"'s use is perhaps the most widely known, it is predated by the "'John Garfield Still Dead' syndrome," which originated as a result of extensive coverage in the wake of the actor John Garfield's death and funeral in 1952.
After a brief "in memoriam" during "SNL"'s 40th Anniversary Special on February 15, 2015, Bill Murray ended the segment with the famous phrase which "just came in from Spain".
References.
Notes
Citations

</doc>
<doc id="28820" url="http://en.wikipedia.org/wiki?curid=28820" title="Son House">
Son House

Eddie James "Son" House, Jr. (March 21, 1902 – October 19, 1988) was an American blues singer and guitarist, noted for his highly emotional style of singing and slide guitar playing.
After years of hostility to secular music, as a preacher, and for a few years also as a church pastor, he turned to blues performance at the age of 25. He quickly developed a unique style by applying the rhythmic drive, vocal power and emotional intensity of his preaching to the newly learned idiom. In a short career interrupted by a spell in Parchman Farm penitentiary, he developed to the point that Charley Patton, the foremost blues artist of the Mississippi Delta region, invited him to share engagements, and to accompany him to a 1930 recording session for Paramount Records.
Issued at the start of The Great Depression, the records did not sell and did not lead to national recognition. Locally, Son remained popular, and in the 1930s, together with Patton's associate, Willie Brown, he was the leading musician of Coahoma County. There he was a formative influence on Robert Johnson and Muddy Waters. In 1941 and 1942, House and the members of his band were recorded by Alan Lomax and John W. Work for Library of Congress and Fisk University. The following year, he left the Delta for Rochester, New York, and gave up music.
In 1964, a group of young record collectors discovered House, whom they knew of from his records issued by Paramount and by the Library of Congress. With their encouragement, he relearned his style and repertoire and enjoyed a career as an entertainer to young white audiences in the coffee houses, folk festivals and concert tours of the American folk music revival billed as a "folk blues" singer. He recorded several albums, and some informally taped concerts have also been issued as albums. Son House died in 1988.
In addition to his early influence on Robert Johnson and Muddy Waters, he became an inspiration to John Hammond, Alan Wilson (of Canned Heat), Bonnie Raitt, The White Stripes, Dallas Green and John Mooney.
Biography.
Early life.
The middle of three brothers, House was born in the hamlet of Lyon, north of Clarksdale, Mississippi and continued to live in the rural Mississippi Delta until his parents separated. His father, Eddie House, Sr., was a musician, playing the tuba in a band with his many brothers, and sometimes playing guitar. He was a church member, but also a drinker. This caused him to leave the church for a time, before giving up drink and becoming a deacon. Young Eddie House adopted the family concern with religion and churchgoing. He also absorbed the family love of music, but confined himself to singing, showing no interest in the family instrumental band, and feeling entirely hostile to the Blues on religious grounds.
Son's parents separated when he was about seven or eight. His mother took him to Tallulah, Louisiana, across the Mississippi River from Vicksburg, Mississippi. When Son was in his early teens, they moved to Algiers, New Orleans. Recalling these years, Son would later speak of his hatred of blues and his passion for churchgoing (he described himself as "churchy" and "churchified"). At fifteen, probably while living in Algiers, he began preaching sermons.
At the age of nineteen, while living in the Delta, he married an older woman from New Orleans named Carrie Martin. This was a significant step for House; he married in church and against family opposition. The couple moved to her hometown of Centreville, Louisiana to help run Carrie's father's farm. After a couple of years, feeling used and disillusioned, House recalls "I left her hanging on the gatepost, with her father tellin' me to come back so we could plow some more." In later years, House was still angry and said of Carrie "She wasn't nothin' but one of them New Orleans whores". At around the same time, probably 1922, Son's mother died.
House's resentment of farming extended to the many menial jobs he took in his young adult years. He moved around frequently, on one occasion taking off to East Saint Louis to work in a steel plant. The one job he enjoyed was on a Louisiana horse ranch, which later he celebrated by wearing a cowboy hat in his performances. He found some relief from constant manual labor when, following a conversion experience "getting religion" in his early twenties, he was accepted as a paid pastor, first in the Baptist Church, then in the Colored Methodist Episcopal Church. However, like his father before him, he fell into habits which conflicted with his calling — drinking like his father, and probably also womanizing. This led him after several years of conflict to "leave the church" — i.e. cease his full-time commitment — although he still felt the need to preach sermons from time to time.
Blues performer.
In 1927 at the age of 25, House underwent a change of musical perspective as rapid and dramatic as a religious conversion. In a hamlet south of Clarksdale, Son heard one of his drinking companions, either James McCoy or Willie Wilson (his recollections differed), playing bottleneck guitar, a style he had never heard before. He immediately changed his attitude to blues, bought a guitar from a musician called Frank Hoskins, and within weeks was playing with Hoskins, McCoy and Wilson. Two songs he learned from McCoy would later be among his best-known: "My Black Mama" and "Preachin' The Blues". Another source of inspiration was Reuben Lacy, a much better known performer who had recorded for Columbia Records in 1927 (no titles released) and for Paramount Records in 1928 (two titles released). In an astonishing short time, with only these four musicians as models, House developed to professional standard a blues style based on his religious singing and simple bottleneck guitar style.
After allegedly killing a man in self-defense, he spent time in prison in 1928 and 1929. The official story on the killing is that sometime around 1927 or 1928, he was playing in a juke joint when a man went on a shooting spree. Son was wounded in the leg, and shot the man dead. He received a 15-year sentence at the Mississippi State Penitentiary (Parchman Farm), of which he served two years. House credited his re-examination and release to an appeal by his family, but also spoke of the intervention by the influential white planter for whom they worked. The date of the killing and the duration of his sentence are unclear. House gave different accounts to different interviewers and searches by his biographer Daniel Beaumont found no details in the court records of Coahoma County or in the archive of the Mississippi Department of Corrections.
On his release in 1929 or early 1930, Son was strongly advised to leave Clarksdale and stay away. He walked to Jonestown and caught a train to the small town of Lula, Mississippi, sixteen miles north of Clarksdale, and eight miles from the blues hub of Helena, Arkansas. Coincidentally, the great star of Delta Blues, Charley Patton, was also in virtual exile in Lula, having been expelled from his base in the Dockery Plantation. With his partner Willie Brown, Patton dominated the local market for professional blues performance. Patton watched House busking when he arrived penniless at Lula station, but did not approach him. He then observed Son's showmanship attracting a crowd to the café and bootleg whiskey business of a woman called Sara Knight, and invited him to be a regular musical partner with him and Brown. Son formed a liaison with Knight, and both musicians profited from association with her bootlegging activities. The musical partnership is disputed by Patton's biographers Stephen Calt and Gayle Dean Wardlow. They consider that House's musicianship was too limited to play with Patton and Brown, who were also rumoured to be estranged at the time. They also cite one statement by House that he did not play for dances in Lula. Beaumont concludes that Son became a firm friend of Patton, traveling with him to gigs but playing separately.
Recording.
In 1930, Art Laibly of Paramount Records traveled to Lula to convince Patton to record several more sides in Grafton, Wisconsin. Along with Patton came House, Brown, and pianist Louise Johnson, who would all end up recording sides for the label. House recorded nine songs during that session, eight of which were released; but these were commercial failures, and House would not record again commercially for 35 years. House continued to play with Patton and Brown, even after Patton's death in 1934. During this time, House worked as a tractor driver for various plantations around the Lake Cormorant area.
Alan Lomax first recorded House for the Library of Congress in 1941. Willie Brown, mandolin player Fiddlin' Joe Martin, and harmonica player Leroy Williams played with House on these recordings. Lomax returned to the area in 1942, where he recorded House once more. He then faded from the public view, moving to Rochester, New York, in 1943, working as a railroad porter for the New York Central Railroad and as a chef.
Rediscovery.
In 1964, after a long search of the Mississippi Delta region by Nick Perls, Dick Waterman and Phil Spiro, he ended up being "rediscovered" in Rochester, NY. House had been retired from the music business for many years, and was unaware of the 1960s folk blues revival and international enthusiasm regarding his early recordings.
He subsequently toured extensively in the US and Europe and recorded for CBS Records. Like Mississippi John Hurt, he was welcomed into the music scene of the 1960s and played at the Newport Folk Festival in 1964, the New York Folk Festival in July 1965, and the October 1967 European tour of the American Folk Festival along with Skip James and Bukka White.
The young guitarist Alan Wilson (Canned Heat) was a fan of Son House. The producer John Hammond Sr asked Wilson, who was just 22 years old, to teach "Son House how to play like Son House," because Alan Wilson had such a good knowledge of the blues styles. The album "The Father of Delta Blues - The Complete 1965 Sessions" was the result. Son House played with Alan Wilson live. It can be heard on the album "John the Revelator: The 1970 London Sessions".
In the summer of 1970, House toured Europe once again, including an appearance at the Montreux Jazz Festival; a recording of his London concerts was released by Liberty Records. He also played at the two Days of Blues Festival in Toronto in 1974. On an appearance on the TV arts show "Camera Three", he was accompanied by blues guitarist Buddy Guy.
Ill health plagued House's later years and in 1974 he retired once again, and later moved to Detroit, Michigan, where he remained until his death from cancer of the larynx. He was buried at the Mt. Hazel Cemetery. Members of the Detroit Blues Society raised money through benefit concerts to put a monument on his grave. He had been married five times.
Discography.
78 RPM Recordings<br>
Recorded May 28, 1930, in Grafton, Wisconsin, for Paramount Records.
For Library of Congress/Fisk University
Recorded August 1941, Clack Store in Clack, Mississippi
Recorded 17 July 1942, Robbinsonville Mississippi
The music from both sessions and most of the recorded interviews have been reissued on LP and CD.
Singles
Other albums 

</doc>
<doc id="28828" url="http://en.wikipedia.org/wiki?curid=28828" title="Poetry slam">
Poetry slam

 A poetry slam is a competition at which poets read or recite original work. These performances are then judged on a numeric scale by previously selected members of the audience.
History.
American poet Marc Smith is credited with starting the poetry slam at the Get Me High Lounge in Chicago in November 1984. In July 1986, the slam moved to its permanent home, the Green Mill Jazz Club. In August 1988, the first poetry slam was held in New York City at the Nuyorican Poet's Cafe and hosted by Bob Holman. In 1990, the first National Poetry Slam took place in Fort Mason, San Francisco, involving a team from Chicago, a team from San Francisco, and an individual poet from New York. s of 2014[ [update]], the National Poetry Slam featured 72 certified teams, culminating in five days of competition. Da Poetry Lounge was started in Hollywood, CA in 1998.
Slams have spread all over the world, with slam competitions in Ireland, Nepal, Canada, Germany, France, Sweden, Austria, Israel, Ukraine, Russia, Switzerland, the Netherlands, Portugal, United Kingdom, Australia, New Zealand, Singapore, Hungary, the Czech Republic, Poland, Serbia, Bosnia, Denmark, Latvia, South Korea, Japan, India, Greece, Spain, Mexico, Madagascar, Azerbaijan, Morocco, Moldova and Brazil.
Format.
In a poetry slam, members of the audience are chosen by an M.C. or host to act as judges for the event. In the national slam, there are five judges, but smaller slams generally have three. After each poet performs, each judge awards a score to that poem. Scores generally range between zero and ten. The highest and lowest score are dropped, giving each performance a rating between zero and thirty points.
Before the competition begins, the host will often bring up a "sacrificial" poet, whom the judges will score in order to calibrate their judging.
A single round at a standard slam consists of performances by all eligible poets. Most slams last multiple rounds, and many involve the elimination of lower-scoring poets in successive rounds. An elimination rubric might run 8-4-2; eight poets in the first round, four in the second, and two in the last. Some slams do not eliminate poets at all. The Green Mill usually runs its slams with 6 poets in the first round.
The Portland Poetry Slam (Portland, OR) takes a different approach; it uses the 8-4-2 three-round rubric, but the poets go head-to-head in separate bouts within the round. Instead of five judges giving points, the audience decides who moves on to the next round by a loud, enthusiastic popular vote.
Props, costumes, and music are always forbidden in slams, distinguishing this category from its immediate predecessor, performance poetry. Additionally, most slams enforce a time limit of three minutes (and a grace period of ten seconds), after which a poet's score may be docked according to how long the poem exceeded the limit.
Competition types.
In an "Open Slam," the most common slam type, competition is open to all who wish to compete, given the number of slots available. In an "Invitational Slam," by contrast, only those invited to do so may compete.
Poetry Slam, Inc. holds several National and World Poetry Slams, including the Individual World Poetry Slam, The National Poetry Team Slam and The Women of the World Poetry Slam. The current (2013) IWPS champion is Ed Mabrey. Ed Mabrey is the only three-time IWPS champion in the history of the event. The current (2013) National Poetry Slam Team champions are Slam New Orleans (SNO), who have won the competition for the second year in a row. The current (2014) Women of the World Poetry Slam Champion is Dominique Christina.
A "Theme Slam" is one in which all performances must conform to a specified theme, genre, or formal constraint. Themes may include Nerd, Erotica, Queer, Improv, or other conceptual limitations. In theme slams, poets can sometimes be allowed to break "traditional" slam rules. For instance, they sometimes allow performance of work by another poet (e.g. the "Dead Poet Slam", in which all work must be by a deceased poet). They can also allow changes on the restrictions on costumes or props (e.g. the Swedish "Triathlon" slams that allow for a poet, musician, and dancer to all take the stage at the same time), changing the judging structure (e.g. having a specific guest judge), or changing the time limits (e.g. a "1-2-3" slam with three rounds of one minute, two minutes, and three minutes, respectively).
Although theme slams may seem restricting in nature, slam venues frequently use them to advocate participation by particular and perhaps underrepresented demographics (which vary from slam to slam), like younger poets and women.
Poetics.
Poetry slams can feature a broad range of voices, styles, cultural traditions, and approaches to writing and performance.
Some poets are closely associated with the vocal delivery style found in hip-hop music and draw heavily on the tradition of dub poetry, a rhythmic and politicized genre belonging to black and particularly West Indian culture. Others employ an unrhyming narrative formula. Some use traditional theatric devices including shifting voices and tones, while others may recite an entire poem in ironic monotone. Some poets use nothing but their words to deliver a poem, while others stretch the boundaries of the format, tap-dancing or beatboxing or using highly choreographed movements.
What is a dominant / successful style one year may be passe the next. Cristin O'Keefe Aptowicz, slam poet and author of "Words In Your Face: A Guided Tour Through Twenty Years of the New York City Poetry Slam", was quoted in an interview on the Best American Poetry blog as saying:
One of the goals of a poetry slam is to challenge the authority of anyone who claims absolute authority over literary value. No poet is beyond critique, as everyone is dependent upon the goodwill of the audience. Since only the poets with the best cumulative scores advance to the final round of the night, the structure assures that the audience gets to choose from whom they will hear more poetry. Audience members furthermore become part of each poem's presence, thus breaking down the barriers between poet/performer, critic, and audience. Bob Holman, a poetry activist and former slammaster of the Nuyorican Poets Cafe, once called the movement "the democratization of verse." In 2005, Holman was also quoted as saying:
Slam critics.
In an interview in the "Paris Review," literary critic Harold Bloom said about slamming:
I can’t bear these accounts I read in the "Times" and elsewhere of these poetry slams, in which various young men and women in various late-spots are declaiming rant and nonsense at each other. The whole thing is judged by an applause meter which is actually not there, but might as well be. This isn’t even silly; it is the death of art. Kip Fulbeck, who teaches Spoken Word at the University of California, Santa Barbara, said, "I don’t like the idea of competition and art being put together. I think it often distills the quality of work down to a caricature of itself. Seeing poetry slams often reminds me of watching American Idol. You’ve got a series of judges, an audience that comes in looking for a certain shtick that they want to see and that’s what they’re going to cheer for."
Poet and lead singer of King Missile, John S. Hall has also long been a vocal opponent, taking issue with such factors as its inherently competitive nature and what he considers its lack of stylistic diversity. In his 2005 interview in "Words In Your Face: A Guided Tour Through Twenty Years of the New York City Poetry Slam," he recalls seeing his first slam, at the Nuyorican Poets Café:
The poet Tim Clare offers a "for and against" account of the phenomenon in "Slam: A Poetic Diaialogue".
Ironically, slam poetry movement founder Marc Smith has been critical of the commercially successful Def Poetry television and Broadway live stage shows produced by Russell Simmons, decrying it as "an exploitive entertainment [program that] diminished the value and aesthetic of performance poetry".
Academia and slam.
As of 2011, four poets who have competed at National Poetry Slam have won National Endowment of the Arts (NEA) Fellowships for Literature:
A number of poets belong to both academia and slam: as noted above Jeffrey McDaniel slammed on several poetry slam teams, and has since published several books and currently teaches at Sarah Lawrence College; Patricia Smith, a four-time national slam champion, went on to win several prestigious literary awards, including being nominated for the 2008 National Book Award, and being inducted into the International Literary Hall of Fame for Writers of African Descent in 2006; Bob Holman founded the Nuyorican Poetry Slam has taught for years at the New School, Bard, Columbia and NYU; Craig Arnold won the Yale Series of Younger Poets Competition and has competed at slams; Kip Fulbeck, a professor of Art at the University of California, Santa Barbara competed in slam in the early-1990s and initiated the first spoken word course to be taught as part of a college art program's core curriculum; and poet/academics such as Michael Salinger, Felice Belle, Javon Johnson was national slam poetry champion in 2003 and 2004, wrote his dissertation on slam poetry and recently published an article in text and performance quarterly about black masculinity and sexism in the slam communitiy, Susan Somers-Willett wrote the book The Cultural Politics of Slam poetry, exploring the relationships between slam, identity, and politics, Robbie Q. Telfer, Phil West, Ragan Fox writes about his ten years of experience as "a gay slam poet", Marie Fleischmann Timbreza, and Karyna McGlynn have devoted much attention to the merging of the poetry slam community and the academic community in their respective works.
Some renowned poets have competed in slams, with less successful results. Henry Taylor, winner of the 1985 Pulitzer Prize for Poetry, competed in the 1997 National Poetry Slam as an individual and placed 75th out of 150.
While slam poetry has often been ignored in traditional higher learning institutions, it slowly is finding its way into courses and programs of study. For example, at Berklee College of Music, in Boston, Slam Poetry is now available as a Minor course of study.
Youth poetry slam movement.
Slam poetry has found popularity as a form of self-expression among many teenagers. The World Poetry Bout Association sponsored the earliest slam poetry workshops for teenagers, through its "Poetry Education Project" in Taos, New Mexico, in the early 1990s. The first statewide competition for high school students was held at Taos High School in 1993, with the top teams and individual participants awarded plaques. Members of Taos' competitive teams earned athletic letters annually up until 2008. [cf. The Taos News, Taos, NM, articles, 1993 to present.]Youth Speaks
, a non-profit literary organization founded in 1996 by James Kass, patterned the slam competitions at the annual Brave New Voices festival after that seminal Taos event. Youth Speaks serves as one of the largest youth poetry organizations in America, offering opportunities for youth ages 13–19 to express their ideas on paper and stage.
, a non-profit literary organization founded in 1996 by James Kass, serves as one of the largest youth poetry organizations in America, offering opportunities for youth ages 13–19 to express their ideas on paper and stage.
Another group offering opportunities in education and performance to teens is out of New York City, formerly known as Youth Speaks New York. URBAN WORD NYC holds the largest youth slam in NYC annually, with over 500 young people. The non-profit organization provides free workshops for inner-city youth ran by Hip-Hop poet and mentor, Michael Cirelli.
 (YCA) provides workshops, mentoring, and competition opportunities to youth in the Chicago area. Every year YCA presents Louder Than A Bomb, the world's largest team-based youth slam and subject of a documentary by the same name.
The youth poetry slam movement was the focus of a documentary film series produced by HBO and released in 2009. It featured poets from Youth Speaks, Urban Word, Louder than a Bomb and other related youth poetry slam organizations.
In a 2005 interview, one of slam's best known poets Saul Williams praised the youth poetry slam movement, explaining:
In 2012 more than 12,000 young people took part in an England-wide youth slam "Shake the Dust", organised by Apples and Snakes as part of the London 2012 Festival.

</doc>
<doc id="28834" url="http://en.wikipedia.org/wiki?curid=28834" title="Sultan Bashiruddin Mahmood">
Sultan Bashiruddin Mahmood

Sultan Bashiruddin Mahmood(Urdu: سلطان بشیر الدین محمود‎; born 1940; "SI"), is a Pakistani nuclear engineer and a scholar on Islamic studies who was notoriously subjected for a criminal probe launched by the FIA on suspicions on unauthorized travel in Afghanistan prior to the deadliest terrorist attacks in the United States in 2001.
Having spent a distinguish career in PAEC, he founded the Ummah Tameer-e-Nau (UTN) in 1999– a right-wing organization that was banned and sanctioned by the United States in 2001. Mehmood was among those who were listed and sanctioned by the al-Qaeda sanction committee in December 2001. Having been cleared by the FIA, he has been living in "anonymity" in Islamabad, authoring books on relationship between Islam and science.
Life and education.
Mahmood was born in Amritsar, Punjab, British India to the Punjabi family. There are conflicting reports on concerning his date of birth; his personal admission noted the birth year as 1940, while the UN reports estimated as 1938. His father, Chaudhry Muhammad Sharif, was a local "Zamindar" (lit. feudal lord). His family emigrated from India to Pakistan in an events following the violent partition of India in 1947; the family settled in Lahore, Punjab.
After graduating with distinctions from a local high school standing at top of his class, Mehmood was awarded scholarship and enrolled at the famed Government College University to study electrical engineering. After spending a semester, he made a transfer to University of Engineering and Technology in Lahore, and graduated with bachelor's degree with honors in electrical engineering in 1960. His credentials led him to join the Pakistan Atomic Energy Commission (PAEC) where he gained scholarship to study in the United Kingdom.
In 1962, he went to attend the University of Manchester where he studied for double master's degree. First completing masters' program in control systems in 1965, then Mehmood received his another master's degree in nuclear engineering in 1969 from the Manchester University. While in Manchester, Mehmood was an expert on Manhattan Project and was reportedly in contacts with South African scientists in discussing the jet-nozzel method for uranium-enrichment. However, it remains unclear how much interaction was taken place during that time.
Pakistan Atomic Energy Commission.
Mehmood joined the PAEC in 1968, joining the Nuclear Physics Division at the Institute of Nuclear Science and Technology working under dr. Naeem Ahmad Khan. His collaboration took place with Samar Mubarakmand, Hafeez Qureshi and was a vital member of the group before it got discontinued in 1970. Mahmood was one of the foremost experts on civilian reactor technology and was a senior engineer at the KANUPP I— the first commercial nuclear power plant of the country. He gained notability and publicity in the physics community for inventing the scientific instrument, the "SBM probe" to detect leaks in steam pipes, a problem that was affecting nuclear plants all over the world and is still used worldwide.
After witnessing the war with India which saw the unconditional surrender of Pakistan in 1971, Mahmood attended the winter seminar at Multan and delivered a speech on atomic science. On 20 January 1972, President Zulfikar Ali Bhutto approved the crash program under Munir Ahmad Khan for a sake of "national survivor." Though, he continued his work at the KANUPP I engineering division.
In the aftermath of surprise nuclear test conducted by India, Munir Ahmad appointed Mehmood as the director of the enrichment division at the PAEC where majority of the calculations were conducted by dr. Khalil Qureshi– a physical chemist. Mehmood analyzed the diffusion, gas-centrifuge, jet-nozzle and laser methods for the uranium-enrichment; recommending the gas-centrifuge method as economical. After submitting the report, Mehmood was asked to depart to the Netherlands to interview dr. Abdul Qadeer Khan on behalf of President Bhutto in 1974. In 1975, his proposal was approved and the work on uranium project started with Mahmood being its director, a move that irked more qualified but more difficult to manage dr. Abdul Qadeer Khan who had coveted the job for himself. His relations with dr. Khan remains extremely tense and the pairs disagreed with each other and developed differences at great height. In private meetings with Munir Ahmad, Mehmood often complained and pictured him as "egomaniac". In 1976, Mahmood was removed from the enrichment division as dr. Abdul Qadeer Khan had him ejected and moving the enrichment division at the ERL under military control.
Eventually, Munir Ahmad removed him from other classified works and posted him back at the KANUPP-I with no reason given. In 1980s, Munir Ahmad secured him a job as project manager for the construction of the Khushab-I where he served as chief engineer and aided with the designing the coolant systems. In 1998, he was promoted as a director of the nuclear power division and held that position until 1999.
After the reactor went critical in April 1998, Mahmood in an interview had said: ""This reactor (can produce enough plutonium for two to three nuclear weapons per year) Pakistan had "acquired the capability to produce... boosted thermonuclear weapons and hydrogen bombs"." In 1998, Mahmood was honored with Sitara-e-Imtiaz in a colourful ceremony by the Prime Minister, Nawaz Sharif.
In 1998, he was promoted as a director of the nuclear power division and held that position until 1999.
Radical politics and Ummah Tameer-e-Nau.
Endorsing publicly the decision of nuclear tests by Prime Minister Nawaz Sharif in 1998, Mahmood began appearing in news channels as an outspoken opponent of Prime Minister Sharif, as he vehemently opposed Pakistan becoming the signatory state of the NPT and CTBT. At country's popular news channels and newspapers, Mahmood gave numerous interviews, wrote articles, and lobbied against Prime Minister Sharif when learning that Prime Minister Sharif had been willing to be a signatory of anti-nuclear weapon treaties, prompting the government forcefully transferring Mahmood at the non-technical position in the PAEC.
Seeking premature retirement from PAEC in 1999, Mahmood moved towards publishing books and articles involving the relationship between Science and Islam. Mahmood founded the Ummah Tameer-e-Nau (UTN)– a rightwing organization– with his close associates. In 2000, he began attending the lectures and religious sessions with Dr. Israr Ahmed who would later influenced in his political views and philosophy. Through UTN, he steps in the more radical politics, and began visiting Afghanistan where he wanted to be focused on rebuilding educational institutions, hospitals, and relief work.
In August 2001, Mahmood and one of his colleagues at the UTN met with Osama bin Laden and Ayman al-Zawahiri in Kandahar, Afghanistan. Describing the meeting, the "New York Times" editorial quoted:""There is little doubt that Mahmood talked to the two al-Qaeda leaders about nuclear weapons, or that Al Qaeda desperately wanted the bomb".
2001 debriefing and detention.
Since 1999 and 2000 onwards, Pakistan's intelligence community had been tracking and monitoring Mehmood whose bushy beard advertised his deep attachment to Taliban. After the terrorist attacks in the United States, the FIA launched an active criminal investigations against him, leveling charges on unauthorized traveled to Afghanistan. Director CIA George Tenet later described intelligence reports of his meeting with Al Qaeda as "frustratingly vague"." When asked by Pakistani and American investigators about nature of UTN's work and discussions, Mahmood told that he had nothing to do with the al-Qaeda and was only working on humanitarian issues like food, health and education. Investigators from ISI and CIA were astonished and surprised when finding out that Mahmood knew nothing on nuclear weapons as contrary of being a nuclear engineer, and were unable to construct one by themselves.
During his debriefing, his son Dr. Asim Mahmood, who's a family medicine doctor told ISI officials that: "My father [Mahmood] did meet with Osama bin Laden and Osama Bin Laden seemed interested in that matter but my father showed no interest in the matter as he met him for food, water and healthcare matters on which his charity was working".
The FIA criminal probe continued for four months and yielded no concrete results. Pressure from the civil society and court inquiries against FIA's criminal probe led to his release in 2001. His family did confirmed his released but had been constantly under surveillance by the FIA; his name was placed in the "Exit Control List" in which he is not allowed to travel out of Pakistan and since his release, Mehmood has been out of the public eye and lives a very quiet life in Islamabad, Pakistan devoting most of his time to write books and doing research work on Islam and science.
Dr. Bashir Syed, former president of the Association of Pakistani Scientists and Engineers of North America (APSENA), said: "I know both of these persons and can tell you there is not an iota of truth that both these respected scientists and friends will do anything to harm the interest of their own country."
Mahmood-Hoodbhoy debates.
He has written over fifteen books, the most well-known being "The Mechanics of Doomsday and Life After Death", which is an analysis of the events leading to doomsday in light of scientific theories and Quranic knowledge. However, his scientific arguments and theories have been challenged by some prominent scientists in Pakistan. His religiosity and eccentricity began troubling the Pakistan's Physics Society; his peers often quoted him as "a rather strange man".
In 1988, Mehmood was invited through an invitation at the University of Islamabad to deliver a lecture on science. During his lecture at the university's "Physics Hall", he and several other academcians have debated on his book. While debating, a well known Dr. Pervez Hoodbhoy and Sultan Bashiruddin Mahmood had an acrimonious public debate in 1988 at the University of Islamabad's Physics Hall. Dr. Pervez Hoodbhoy had severely criticised Mr. Bashiruddin Mahmood's theories and the notion of Islamic science in general, calling it "ludicrous science." Bashiruddin Mahmood protested that Dr. Pervez Hoodbhoy misrepresented his views, quoting: "This is crossing all limits of decency," he wrote. "But should one expect any honesty or decency from anti-Islamic sources?"
Literature and Cosmology.
I
n his writings and speeches, Mahmood has advocated for nuclear sharing with other Islamic nations which he believed would give rise to Muslim dominance in the world. He has also written a Tafseer of the Quran in English.
Mahmood is reported to be fascinated "with the role sunspots played in triggering the French and Russian Revolutions, World War II and assorted anti-colonial uprisings." According to his book "Cosmology and Human Destiny", Mahmood argued that sunspots have influenced major human events, including the French Revolution, the Russian Revolution, and World War II. He concluded that governments across the world ""are already being subjected to great emotional aggression under the catalytic effect of the abnormally high sunspot activity under which they are most likely to adapt aggression as the natural solution for their problems". In this book which was first published in 1998, he predicts that the period from 2007 to 2014 would be of great turmoil and destruction in the world. Other books written by him include a biography of the Islamic prophet Muhammad titled "First and the Last", while his other books are focused more on the relation between Islam and science like "Miraculous Quran", "Life After Death and Doomsday", and "Kitab-e-Zindagi" (in Urdu).
One passage of the book reportedly states: "At the international level, terrorism will rule; and in this scenario use of mass destruction weapons cannot be ruled out. Millions, by 2020, may die through mass destruction weapons, hunger, disease, street violence, terrorist attacks, and suicide.""
Mahmood's lifelong friend, Parliamentarian Farhatullah Babar, who is currently serving as a spokesperson of President of Pakistan, while talking to media, said: "Mahmood predicted in Cosmology and Human Destiny that "the year 2002 was likely to be a year of maximum sunspot activity. It means upheaval, particularly on the South Asia, with the possibility of nuclear exchanges"."
Mahmood has published papers concerning djinni, which are described in the Qur'an as beings made of fire. He has proposed that djinni could be tapped to solve the energy crisis. "I think that if we develop our souls, we can develop communication with them," Mr. Bashiruddin Mahmood said about djinni in The Wall Street Journal in an interview in 1988. "Every new idea has its opponents," he added. "But there is no reason for this controversy over Islam and science because there is no conflict between Islam and science."
New York Times comments.
The New York Times has described Mahmood as "an autodidact intellectual with grand aspirations," and noted that "his fellow scientists at PAEC began to wonder if Mahmood was mentally sound." Mahmood made it clear that he believed Pakistan's bomb was "the property of the whole Ummah," referring to the worldwide Muslim community. "This guy was our ultimate nightmare," an American intelligence official told the Times in late 2001. The US Institute of Historical biographies mentions him in their ‘Who is Who’ list and presented him a gold medal in 1998. He has also been awarded Gold Medal by the Pakistan Academy of Sciences.
References.
</dl>

</doc>
<doc id="28852" url="http://en.wikipedia.org/wiki?curid=28852" title="Syphilis">
Syphilis

Syphilis is a sexually transmitted infection caused by the spirochete bacterium "Treponema pallidum" subspecies "pallidum". The primary route of transmission is through sexual contact; it may also be transmitted from mother to fetus during pregnancy or at birth, resulting in congenital syphilis. Other human diseases caused by related "Treponema pallidum" include yaws (subspecies "pertenue"), pinta (subspecies "carateum"), and bejel (subspecies "endemicum").
The signs and symptoms of syphilis vary depending in which of the four stages it presents (primary, secondary, latent, and tertiary). The primary stage classically presents with a single chancre (a firm, painless, non-itchy skin ulceration), secondary syphilis with a diffuse rash which frequently involves the palms of the hands and soles of the feet, latent syphilis with little to no symptoms, and tertiary syphilis with gummas, neurological, or cardiac symptoms. It has, however, been known as "the great imitator" due to its frequent atypical presentations. Diagnosis is usually made by using blood tests; however, the bacteria can also be detected using dark field microscopy. Syphilis can be effectively treated with antibiotics, specifically the preferred intramuscular benzathine penicillin G (or penicillin G potassium given intravenously for neurosyphilis), or else ceftriaxone, and in those who have a severe penicillin allergy, oral doxycycline or azithromycin.
Syphilis is thought to have infected 12 million additional people worldwide in 1999, with greater than 90% of cases in the developing world. After decreasing dramatically since the widespread availability of penicillin in the 1940s, rates of infection have increased since the turn of the millennium in many countries, often in combination with human immunodeficiency virus (HIV). This has been attributed partly to increased promiscuity, prostitution, decreasing use of condoms, and unsafe sexual practices among men who have sex with men.
Signs and symptoms.
Syphilis can present in one of four different stages: primary, secondary, latent, and tertiary, and may also occur congenitally. It was referred to as "the great imitator" by Sir William Osler due to its varied presentations.
Primary.
Primary syphilis is typically acquired by direct sexual contact with the infectious lesions of another person. Approximately 3 to 90 days after the initial exposure (average 21 days) a skin lesion, called a chancre, appears at the point of contact. This is classically (40% of the time) a single, firm, painless, non-itchy skin ulceration with a clean base and sharp borders between 0.3 and 3.0 cm in size. The lesion, however, may take on almost any form. In the classic form, it evolves from a macule to a papule and finally to an erosion or ulcer. Occasionally, multiple lesions may be present (~40%), with multiple lesions more common when coinfected with HIV. Lesions may be painful or tender (30%), and they may occur outside of the genitals (2–7%). The most common location in women is the cervix (44%), the penis in heterosexual men (99%), and anally and rectally relatively commonly in men who have sex with men (34%). Lymph node enlargement frequently (80%) occurs around the area of infection, occurring seven to 10 days after chancre formation. The lesion may persist for three to six weeks without treatment.
Secondary.
Secondary syphilis occurs approximately four to ten weeks after the primary infection. While secondary disease is known for the many different ways it can manifest, symptoms most commonly involve the skin, mucous membranes, and lymph nodes. There may be a symmetrical, reddish-pink, non-itchy rash on the trunk and extremities, including the palms and soles. The rash may become maculopapular or pustular. It may form flat, broad, whitish, wart-like lesions known as condyloma latum on mucous membranes. All of these lesions harbor bacteria and are infectious. Other symptoms may include fever, sore throat, malaise, weight loss, hair loss, and headache. Rare manifestations include liver inflammation, kidney disease, joint inflammation, periostitis, inflammation of the optic nerve, uveitis, and interstitial keratitis. The acute symptoms usually resolve after three to six weeks; however, about 25% of people may present with a recurrence of secondary symptoms. Many people who present with secondary syphilis (40–85% of women, 20–65% of men) do not report previously having had the classic chancre of primary syphilis.
Latent.
Latent syphilis is defined as having serologic proof of infection without symptoms of disease. It is further described as either early (less than 1 year after secondary syphilis) or late (more than 1 year after secondary syphilis) in the United States. The United Kingdom uses a cut-off of two years for early and late latent syphilis. Early latent syphilis may have a relapse of symptoms. Late latent syphilis is asymptomatic, and not as contagious as early latent syphilis.
Tertiary.
Tertiary syphilis may occur approximately 3 to 15 years after the initial infection, and may be divided into three different forms: gummatous syphilis (15%), late neurosyphilis (6.5%), and cardiovascular syphilis (10%). Without treatment, a third of infected people develop tertiary disease. People with tertiary syphilis are not infectious.
Gummatous syphilis or late benign syphilis usually occurs 1 to 46 years after the initial infection, with an average of 15 years. This stage is characterized by the formation of chronic gummas, which are soft, tumor-like balls of inflammation which may vary considerably in size. They typically affect the skin, bone, and liver, but can occur anywhere.
Neurosyphilis refers to an infection involving the central nervous system. It may occur early, being either asymptomatic or in the form of syphilitic meningitis, or late as meningovascular syphilis, general paresis, or tabes dorsalis, which is associated with poor balance and lightning pains in the lower extremities. Late neurosyphilis typically occurs 4 to 25 years after the initial infection. Meningovascular syphilis typically presents with apathy and seizure, and general paresis with dementia and tabes dorsalis. Also, there may be Argyll Robertson pupils, which are bilateral small pupils that constrict when the person focuses on near objects, but do not constrict when exposed to bright light.
Cardiovascular syphilis usually occurs 10–30 years after the initial infection. The most common complication is syphilitic aortitis, which may result in aneurysm formation.
Congenital.
Congenital syphilis is that which is transmitted during pregnancy or during birth. Two-thirds of syphilitic infants are born without symptoms. Common symptoms that develop over the first couple years of life include: enlargement of the liver and spleen (70%), rash (70%), fever (40%), neurosyphilis (20%), and lung inflammation (20%). If untreated, late congenital syphilis may occur in 40%, including: saddle nose deformation, Higoumenakis sign, saber shin, or Clutton's joints among others.
Cause.
Bacteriology.
"Treponema pallidum" subspecies" pallidum" is a spiral-shaped, Gram-negative, highly mobile bacterium. Three other human diseases are caused by related "Treponema pallidum", including yaws (subspecies "pertenue"), pinta (subspecies "carateum") and bejel (subspecies "endemicum"). Unlike subtype "pallidum", they do not cause neurological disease. Humans are the only known natural reservoir for subspecies "pallidum". It is unable to survive without a host for more than a few days. This is due to its small genome (1.14 MDa) failing to encode the metabolic pathways necessary to make most of its macronutrients. It has a slow doubling time of greater than 30 hours.
Transmission.
Syphilis is transmitted primarily by sexual contact or during pregnancy from a mother to her fetus; the spirochaete is able to pass through intact mucous membranes or compromised skin. It is thus transmissible by kissing near a lesion, as well as oral, vaginal, and anal sex. Approximately 30 to 60% of those exposed to primary or secondary syphilis will get the disease. Its infectivity is exemplified by the fact that an individual inoculated with only 57 organisms has a 50% chance of being infected. Most (60%) of new cases in the United States occur in men who have sex with men. It can be transmitted via blood products. However, it is tested for in many countries and thus the risk is low. The risk of transmission from sharing needles appears limited. 
It is not generally possible to contract syphilis through toilet seats, daily activities, hot tubs, or sharing eating utensils or clothing. This is mainly because the bacteria die very quickly outside of the body, making transmission via objects extremely difficult.
Diagnosis.
Syphilis is difficult to diagnose clinically early in its presentation. Confirmation is either via blood tests or direct visual inspection using microscopy. Blood tests are more commonly used, as they are easier to perform. Diagnostic tests are, however, unable to distinguish between the stages of the disease.
Blood tests.
Blood tests are divided into nontreponemal and treponemal tests. Nontreponemal tests are used initially, and include venereal disease research laboratory (VDRL) and rapid plasma reagin tests. However, as these tests are occasionally false positives, confirmation is required with a treponemal test, such as treponemal pallidum particle agglutination (TPHA) or fluorescent treponemal antibody absorption test (FTA-Abs). False positives on the nontreponemal tests can occur with some viral infections such as varicella and measles, as well as with lymphoma, tuberculosis, malaria, endocarditis, connective tissue disease, and pregnancy. Treponemal antibody tests usually become positive two to five weeks after the initial infection. Neurosyphilis is diagnosed by finding high numbers of leukocytes (predominately lymphocytes) and high protein levels in the cerebrospinal fluid in the setting of a known syphilis infection.
Direct testing.
Dark ground microscopy of serous fluid from a chancre may be used to make an immediate diagnosis. However, hospitals do not always have equipment or experienced staff members, whereas testing must be done within 10 minutes of acquiring the sample. Sensitivity has been reported to be nearly 80%, thus can only be used to confirm a diagnosis but not rule one out. Two other tests can be carried out on a sample from the chancre: direct fluorescent antibody testing and nucleic acid amplification tests. Direct fluorescent testing uses antibodies tagged with fluorescein, which attach to specific syphilis proteins, while nucleic acid amplification uses techniques, such as the polymerase chain reaction, to detect the presence of specific syphilis genes. These tests are not as time-sensitive, as they do not require living bacteria to make the diagnosis.
Prevention.
s of 2010[ [update]], there is no vaccine effective for prevention. Abstinence from intimate physical contact with an infected person is effective at reducing the transmission of syphilis, as is the proper use of a latex condom. Condom use, however, does not completely eliminate the risk. Thus, the Centers for Disease Control and Prevention recommends a long-term, mutually monogamous relationship with an uninfected partner and the avoidance of substances such as alcohol and other drugs that increase risky sexual behavior.
Congenital syphilis in the newborn can be prevented by screening mothers during early pregnancy and treating those who are infected. The United States Preventive Services Task Force (USPSTF) strongly recommends universal screening of all pregnant women, while the World Health Organization recommends all women be tested at their first antenatal visit and again in the third trimester. If they are positive, they recommend their partners also be treated. Congenital syphilis is, however, still common in the developing world, as many women do not receive antenatal care at all, and the antenatal care others do receive does not include screening, and it still occasionally occurs in the developed world, as those most likely to acquire syphilis (through drug use, etc.) are least likely to receive care during pregnancy. A number of measures to increase access to testing appear effective at reducing rates of congenital syphilis in low- to middle-income countries.
Syphilis is a notifiable disease in many countries, including Canada the European Union, and the United States. This means health care providers are required to notify public health authorities, which will then ideally provide partner notification to the person's partners. Physicians may also encourage patients to send their partners to seek care. The CDC recommends that sexually active men who have sex with men be tested at least yearly.
Treatment.
Early infections.
The first-choice treatment for uncomplicated syphilis remains a single dose of intramuscular benzathine penicillin G. Doxycycline and tetracycline are alternative choices for those allergic to penicillin; however, due to the risk of birth defects these are not recommended for pregnant women. Resistance to macrolides, rifampin, and clindamycin is often present. Ceftriaxone, a third-generation cephalosporin antibiotic, may be as effective as penicillin-based treatment. It is recommended that a treated person avoid sex until the sores are healed.
Late infections.
For neurosyphilis, due to the poor penetration of penicillin G into the central nervous system, those affected are recommended to be given large doses of intravenous penicillin for a minimum of 10 days. If a person is allergic, ceftriaxone may be used or penicillin desensitization attempted. Other late presentations may be treated with once-weekly intramuscular penicillin G for three weeks. If allergic, as in the case of early disease, doxycycline or tetracycline may be used, albeit for a longer duration. Treatment at this stage limits further progression, but has only slight effect on damage which has already occurred.
Jarisch-Herxheimer reaction.
One of the potential side effects of treatment is the Jarisch-Herxheimer reaction. It frequently starts within one hour and lasts for 24 hours, with symptoms of fever, muscles pains, headache, and a fast heart rate. It is caused by cytokines released by the immune system in response to lipoproteins released from rupturing syphilis bacteria.
Epidemiology.
Syphilis is believed to have infected 12 million additional people in 1999, with greater than 90% of cases in the developing world. It affects between 700,000 and 1.6 million pregnancies a year, resulting in spontaneous abortions, stillbirths, and congenital syphilis. During 2010 it caused about 113,000 deaths down from 202,000 in 1990. In sub-Saharan Africa, syphilis contributes to approximately 20% of perinatal deaths. Rates are proportionally higher among intravenous drug users, those who are infected with HIV, and men who have sex with men. In the United States, rates of syphilis as of 2007 were six times greater in men than women, while they were nearly equal in 1997. African Americans accounted for almost half of all cases in 2010. As of 2014, syphilis infections continue to increase in the United States.
Syphilis was very common in Europe during the 18th and 19th centuries. Flaubert found it universal among nineteenth-century Egyptian prostitutes. In the developed world during the early 20th century, infections declined rapidly with the widespread use of antibiotics, until the 1980s and 1990s. Since the year 2000, rates of syphilis have been increasing in the USA, Canada, the UK, Australia and Europe, primarily among men who have sex with men. Rates of syphilis among American women have, however, remained stable during this time, and rates among UK women have increased, but at a rate less than that of men. Increased rates among heterosexuals have occurred in China and Russia since the 1990s. This has been attributed to unsafe sexual practices, such as sexual promiscuity, prostitution, and decreasing use of barrier protection.
Untreated, it has a mortality of 8% to 58%, with a greater death rate in males. The symptoms of syphilis have become less severe over the 19th and 20th centuries, in part due to widespread availability of effective treatment and partly due to decreasing virulence of the spirochaete. With early treatment, few complications result. Syphilis increases the risk of HIV transmission by two to five times, and coinfection is common (30–60% in a number of urban centers).
History.
The exact origin of syphilis is disputed. Syphilis was indisputably present in the Americas before European contact. The dispute is over whether or not syphilis was also present elsewhere in the world at that time. One of the two primary hypotheses proposes that syphilis was carried from the Americas to Europe by the returning crewmen from Christopher Columbus's voyage to the Americas. The other hypothesis says that syphilis existed in Europe previously, but went unrecognized until shortly after Columbus' return. These are referred to as the "Columbian" and "pre-Columbian" hypotheses, respectively. The Columbian hypothesis is best supported by the available evidence. The first written records of an outbreak of syphilis in Europe occurred in 1494 or 1495 in Naples, Italy, during a French invasion (Italian War of 1494–98). As it was claimed to have been spread by French troops, it was initially known as the "French disease" by the people of Naples. In 1530, the pastoral name "syphilis" (the name of a character) was first used by the Italian physician and poet Girolamo Fracastoro as the title of his Latin poem in dactylic hexameter describing the ravages of the disease in Italy. It was also known historically as the "Great Pox".
The causative organism, "Treponema pallidum", was first identified by Fritz Schaudinn and Erich Hoffmann in 1905. The first effective treatment (Salvarsan) was developed in 1910 by Paul Ehrlich, which was followed by trials of penicillin and confirmation of its effectiveness in 1943. Before the discovery and use of antibiotics in the mid-twentieth century, mercury and isolation were commonly used, with treatments often worse than the disease.
Many famous historical figures (including Franz Schubert and Niccolò Paganini) are believed to have had the disease.
Society and culture.
Arts and literature.
The earliest known depiction of an individual with syphilis is Albrecht Dürer's "Syphilitic Man", a woodcut believed to represent a Landsknecht, a Northern European mercenary. The myth of the "femme fatale" or "poison women" of the 19th century is believed to be partly derived from the devastation of syphilis, with classic examples in literature including John Keats' "La Belle Dame sans Merci".
The artist Jan van der Straet painted a scene of a wealthy man receiving treatment for syphilis with the tropical wood guaiacum sometime around 1580. The title of the work is "Preparation and Use of Guayaco for Treating Syphilis". That the artist chose to include this image in a series of works celebrating the New World indicates how important a treatment, however ineffective, for syphilis was to the European elite at that time. The richly colored and detailed work depicts four servants preparing the concoction while a physician looks on, hiding something behind his back while the hapless patient drinks.
Tuskegee and Guatemala studies.
One of the most infamous United States cases of questionable medical ethics in the 20th century was the Tuskegee syphilis study. The study took place in Tuskegee, Alabama, and was supported by the U.S. Public Health Service (PHS) in partnership with the Tuskegee Institute. The study began in 1932, when syphilis was a widespread problem and there was no safe and effective treatment. The study was designed to measure the progression of untreated syphilis. By 1947, penicillin had been validated as an effective cure for syphilis and was becoming widely used to treat the disease. Study directors, however, continued the study and did not offer the participants treatment with penicillin. This is debated, and some have found that penicillin was given to many of the subjects. 
In the 1960s, Peter Buxtun sent a letter to the CDC, who controlled the study, expressing concern about the ethics of letting hundreds of black men die of a disease that could be cured. The CDC asserted that it needed to continue the study until all of the men had died. In 1972, Buxton went to the mainstream press, causing a public outcry. As a result, the program was terminated, a lawsuit brought those affected nine million dollars, and Congress created a commission empowered to write regulations to deter such abuses from occurring in the future. 
On May 16, 1997, as the thanks to the efforts of the Tuskegee Syphilis Study Legacy Committee formed in 1994, survivors of the study were invited to the White House to be present when President Bill Clinton apologized on behalf of the United States government for the study.
Syphilis experiments were also carried out in Guatemala from 1946 to 1948. They were United States-sponsored human experiments, conducted during the government of Juan José Arévalo with the cooperation of some Guatemalan health ministries and officials. Doctors infected soldiers, prisoners, and mental patients with syphilis and other sexually transmitted diseases, without the informed consent of the subjects, and then treated them with antibiotics. In October 2010, the U.S. formally apologized to Guatemala for conducting these experiments.
Research.
There is no vaccine available for people; however, several vaccines based on treponemal proteins reduce lesion development in an animal model, and research is ongoing.

</doc>
<doc id="28866" url="http://en.wikipedia.org/wiki?curid=28866" title="Saint John, New Brunswick">
Saint John, New Brunswick

Saint John is the largest city in New Brunswick and the second largest city in the maritime provinces. It is known as the Fundy City due to its location on the north shore of the Bay of Fundy at the mouth of the Saint John River, as well as being the only city on the bay. In 1785 Saint John became the first incorporated city in Canada.
Saint John had a population of 70,063 in 2011 over an area of 315.82 sqkm. The Saint John metropolitan area covers a land area of 3,362.95 sqkm across the Caledonia Highlands, with a population (as of 2011) of 127,761 making it the second largest CMA in New Brunswick behind Moncton and marking an increase of 4.4% since 2006.
Geography and climate.
Physical geography.
Situated in the south-central portion of the province, along the north shore of the Bay of Fundy at the mouth of the Saint John River, the city is split by the south-flowing river and the east side is bordered on the north by the Kennebecasis River where it meets the Saint John River at Grand Bay. The harbour is home to a terminal for cruise ships as well as being a fairly busy home for various container ships.
The Saint John River itself flows into the Bay of Fundy through a narrow gorge several hundred feet wide at the centre of the city. It contains a unique phenomenon called the Reversing Falls where the diurnal tides of the bay reverse the water flow of the river for several kilometres. A series of underwater ledges at the narrowest point of this gorge also create a series of rapids.
The topography surrounding Saint John is hilly; a result of the influence of two coastal mountain ranges which run along the Bay of Fundy – the "St. Croix Highlands" and the "Caledonia Highlands". The soil throughout the region is extremely rocky with frequent granite outcrops. The coastal plain hosts numerous freshwater lakes in the eastern, western and northern parts of the city.
In Saint John the height difference from low to high tide is approximately 8 metres (28 ft) due to the funnelling effect of the Bay of Fundy as it narrows. The Reversing Falls in Saint John, actually an area of strong rapids, provides one example of the power of these tides; at every high tide, ocean water is pushed through a narrow gorge in the middle of the city and forces the Saint John River to reverse its flow for several hours.
Neighbourhoods.
Saint John is a city of neighbourhoods, with residents closely identifying with their particular area.
South (End) Central Peninsula—Uptown.
The central peninsula on the east side of the harbour, and the area immediately opposite on the west side, hosts the site of the original city from the merger of Parrtown and Carleton. The western side of the central peninsula subsequently saw increased development and currently includes the central business district (CBD) and the Trinity Royal heritage district, which together are referred to as "Uptown" by residents throughout the city. The term "Uptown" comes from the time when the city was an active port city and people at the slips would go up the hill to the city. As well, most of this area in the central peninsula is situated on a hill, it is rarely called "Downtown." The south end of the central peninsula, south of Duke Street, is appropriately called the South End.
North End (Indiantown/Millidgeville/Mount Pleasant/Portland).
The area north of the Highway #1 from the South Central Peninsula is called the North End; both areas being predominantly urban residential older housing which is undergoing gentrification. Much of the North End is made up of the former city of Portland and comprises another former working class area which is slowly undergoing gentrification at the eastern end of Douglas Avenue; immediately north of Portland and upstream from the Reversing Falls is the former community of Indiantown.
Vessels navigating the Saint John River can only transit the Reversing Falls gorge at slack tide, thus Indiantown became a location during the 19th and 20th centuries where tugboats and paddle wheelers could dock to wait. Being located at the beginning of the navigable part of the Saint John River, Indiantown also became a major terminal for vessels departing to ply their trade upriver.
Further north of the central part of the city, and northeast of the North End and Portland, along the southern bank of the Kennebecasis River is the area of Millidgeville which is generally considered a neighbourhood separate from the North End. The boundary of Millidgeville is typically thought to begin at the "Y" intersection of Somerset Street and Millidge Ave or right after Tartan St. It is a middle to upper-class neighbourhood. Located here is University of New Brunswick, as well as New Brunswick's largest health care centre, the Saint John Regional Hospital, and Saint John's only completely French school and community centre, Centre Scolaire Communautaire Samuel-de-Champlain.
The eastern area of the North End plays host to the city's largest park, and one of Canada's largest urban parks. Rockwood Park encompasses 890 hectares of upland Acadian mixed forest, many hills and several caves, as well as several freshwater lakes, with an extensive trail network, a golf course, and the Cherry Brook Zoo. The park was designed by Calvert Vaux in the mid-to-late 19th century. Mount Pleasant borders the park, and is generally seen as distinct from the traditionally poorer North End.
East Side (Simonds/Loch Lomond).
To the east of the Courtney Bay / Forebay and south of New Brunswick Route 1 is the East Side, where the city has experienced its greatest suburban sprawl in recent decades with commercial retail centres and residential subdivisions. There has been significant and consistent commercial and retail development in the Westmorland Road-McAllister Drive-Consumer's Drive-Major's Brook Drive-Retail Drive corridor since the 1970s, including McAllister Place, the city's largest shopping mall, which opened in 1978, and with active year-to-year development since 1994. The city's current airport is located further east on the coastal plain among several lakes at the far eastern edge of the municipality. Far east side is Loch Lomond, including several urban neighbourhoods are found here, including Forest Hills, Champlain Heights, and Lakewood Heights. The malls were built by filling in Major's Brook (a tributary to Marsh Creek), making the area prone to flooding.
West Side (Carleton/Lancaster/Fairville).
The portion of the city west of the Saint John River is collectively referred to as West Side, although West Saint Johners typically divide this area into several neighbourhoods. As mentioned previously, the Lower West Side is the former working-class neighbourhood that was known as Carleton at the time of the city's formation in 1785. West and north of the Lower West Side is the former city of Lancaster (commonly referred to as Saint John West), which was amalgamated into Saint John in 1967. The dividing line is generally agreed upon to be Martello Tower and not Lancaster Avenue, with the streets east and south of Lancaster Avenue being considered to be the "West Side, and the streets north and west of Lancaster Avenue, having been renamed from Lancaster, NB to Saint John West, NB.
The southern part of Lancaster abutting Saint John Harbour and the Bay of Fundy is Bayshore and the location of Canadian Pacific Railway's Bayshore Yard. The north end of Lancaster, known as Fairville, is home to Moosehead Brewery and older neighbourhoods clustered along Manawagonish Road. North of Fairville are the communities of Milford and Randolph. Randolph, which is home to Dominion Park Beach, includes land on the city's largest island, and is joined by the Canal Bridge over Mosquito Cove on Greenhead Road. The area also contains the Irving Pulp and Paper mill, a highly visible manufacturing plant that sits directly next to the Reversing Falls and is owned and operated by J. D. Irving, Ltd.
West of Lancaster, the city hosts its second largest park, and one of the largest coastal urban parks in the country. The Irving Nature Park, along Saints' Rest Beach sits on an extensive peninsula called Taylor's Island extending into the western part of the harbour into the Bay of Fundy. The park is partially open to vehicles in summer and features ocean views and walking trails through mixed forests.
Suburbs.
Saint John's suburbs, just on the edge of the city limit, are Rothesay, Quispamsis, and Grand Bay-Westfield. Mainly residential, the suburbs have attracted many of Saint John's residents leading to, until the last census of 2011, the city's population to shrink.
Climate.
The climate of Saint John is humid continental (Köppen climate classification ""). The Bay of Fundy never fully freezes, thus moderating the winter temperatures compared with inland locations. Even so, with the prevailing wind blowing from the west (from land to sea), the average January temperature is about -8.2 C. Summers are usually warm to hot, and daytime temperatures often exceed 25 C. The highest temperature recorded in a given year is usually 30 °C (86 °F) or 31 °C (88 °F). The confluence of cold Bay of Fundy air and inland warmer temperatures often creates onshore winds that bring periods of fog and cooler temperatures during the summer months.
Precipitation in Saint John totals about 1390 mm annually and is well distributed throughout the year, although the late autumn and early winter is typically the wettest time of year. Snowfalls can often be heavy, but rain is as common as snow in winter, and it is not unusual for the ground to be snow-free even in mid-winter.
Buildings and structures.
National Historic Sites.
There are 13 National Historic Sites of Canada in Saint John.
Demography.
Population.
The population of the city declined from the 1970 to the early 21st century, but this trend has now reversed itself and has shown its first increase in many years in the 2011 census.
Metropolitan area.
In the year 2011 the population of the Greater Saint John area was 127,761, of whom 49% were male and 51% female. Children under five accounted for approximately 21% of the population. People 65 and over accounted for 27% of the population. In the years between 1996 and 2005, the population of Saint John declined 6.8%. When the census was taken in May 2006 the population of Saint John was 69,684 compared with 68,103 in 2001.
Ethnicity, religion and language.
Canada's 2006 Census found that amongst the Saint John population's reported ethnic origins, 42.1% of the population described their background as Canadian, followed by English (35.6%), Irish (33.6%), Scottish (27.3%), French (22.7%), German (6.0%), Dutch (3.2%), North American Indian (3.2%), Welsh (2.0%), and many others. (Numbers add to more than 100% due to multiple responses: e.g. "English & Scottish".)
With regard to religion, 89.2% identify as Christian (47.6% Protestant, 40.3% Roman Catholic, and 1.3% other Christian, mostly Orthodox and independent churches). 10.1% state no religious affiliation, and other religions including Islam, Judaism, Buddhism, and Hinduism together comprise less than 1%.
While New Brunswick is a bilingual province, the Greater Saint John area is overwhelmingly anglophone: of its 127,761 residents in 2011, only 5,520 were native French speakers, a much lower percentage than that for the province as a whole.
Municipal government (Common Council).
Responsibility.
Saint John is governed by a body of elected officials, referred to as "Common Council" whose responsibilities include
Composition.
The Common Council consists of:
One is elected by the council to serve as Deputy Mayor.
Current Council is
Shelly Rinehart Councillor at Large and Deputy Mayor
Shirley MacAlary, Councillor at Large
Ward 1
Bill Farren
Greg Norton
Ward 2
John MacKenzie
Susan Fullerton
Ward 3
Donna Reardon
Ward 4
Ray Strowbridge
David Meritthew
In the October 9, 2007 Plebiscite, it was decided that as of the May 2008 quadrennial municipal elections, the city will be divided into four wards of approximately equal population, with two councilors to be elected by the voters in that ward, and two councilors to be elected at large.
Economy.
Politically, socially, economically, as well as geographically, the sea has shaped Saint John. The Fundy City, as the city has been called as it is the only city located on the Bay of Fundy, has a long history of shipbuilding at the city's dry dock which is one of the largest in the world. Since 2003 shipbuilding has ended on the scale it once was forcing the city to adopt a new economic strategy. The University of New Brunswick, the New Brunswick Museum and the New Brunswick Community College are important institutions along with Radian6 and Horizon Health Network and many others are a part of Saint John's fast growing Research and Information Technology sectors. As the city moves away from its industrial past it now begins to capitalize on the other new growing economies in Saint John of tourism, having over 1.5 million visitors a year and 200,000 cruise ship visitors a year, creating a renaissance in the city's historic downtown (locally known as uptown) with many small businesses moving in and large scale waterfront developments underway such as the Fundy Quay being condo, hotel, office space along with the Saint John Law Courts and Three Sisters Harbour front condos.
The Arts & Culture sector play a large role in Saint John's economy. The Imperial Theatre is home to the highly acclaimed Saint John Theatre Company, and the Symphony New Brunswick and hosts a large collection of plays, concerts and other stage production year round. Harbour Station entertainment complex is home to the Saint John Sea Dogs of the QMHL and the Saint John Millrats of the NBL.
Art galleries in Saint John cover the uptown, more than any other atlantic Canadian city. Artists like Miller Brittain and Fred Ross have made Uptown Saint John their home and now the torch has been passed to artists like Gerard Collins, Cliff Turner and Peter Salmon and their respective galleries. Uptown art galleries also include the Trinity Galleries, Citadel Gallery, Handworks Gallery and the Saint John Arts Centre (SJAC). The SJAC located in the Carnegie Building, hosts art exhibits, workshops, local song writers circles and other shows too small to be featured at the grand Imperial Theatre.
Saint John still maintains industrial infrastructure in the city's east side such as Canada's largest oil refinery. Wealthy industrialist K.C. Irving and his family built an industrial conglomerate in the city during the 20th century with interests in oil, forestry, shipbuilding, media and transportation. Irving companies remain dominant employers in the region with North America's first deepwater oil terminal, a pulp mill, a paper mill and a tissue paper plant.
Other important economic activity in the city is generated by the Port of Saint John, the Moosehead Brewery (established in 1867, is Canada's only nationally distributed independent brewery [M. Nicholson]), James Ready Brewing Co., the New Brunswick Power Corporation which operates three electrical generating stations in the region including the Point Lepreau Nuclear Generating Station, Bell Aliant which operates out of the former New Brunswick Telephone headquarters, the Horizon Health Network, which operates 5 hospitals in the Saint John area, and numerous information technology companies. There are also a number of call centres which were established in the 1990s under provincial government incentives.
View from Fort Howe of the Saint John skyline prior to Peel Plaza
Maritime activities.
Until the early first decade of the 21st century, Canada's largest shipyard (Irving Shipbuilding) had been an important employer in the city. During the 1980s-early 1990s the shipyard was responsible for building 9 of the 12 Halifax class multi-purpose patrol frigates for the Canadian Navy. However, the shipyard failing to buckle to Union pressure shut down production. The 25 year Union contract with the shipyard is due to end at the end of the 2012 year. This would allow the shipyard to operate under a new contract.
Prior to the opening of the St. Lawrence Seaway in the late 1950s, the Port of Saint John functioned as the winter port for Montreal, Quebec when shipping was unable to traverse the sea ice in the Gulf of St. Lawrence and St. Lawrence River. The Canadian Pacific Railway opened a line to Saint John from Montreal in 1889 across the state of Maine and transferred the majority of its trans-Atlantic passenger and cargo shipping to the port during the winter months. The port fell into decline following the seaway opening and the start of year-round icebreaker services in the 1960s. In 1994 CPR left Saint John when it sold the line to shortline operator New Brunswick Southern Railway. The Canadian National Railway still services Saint John with a secondary mainline from Moncton.
Military.
Besides being the location of several historical forts, such as Fort Howe, Fort Dufferin, Fort Latour, and the Carleton Martello Tower, Saint John is the location of a number of reserve units of the Canadian Forces.
Retailing.
The following malls are located in the city:
East.
"See The East Saint John Mall District"
Energy projects.
Canaport LNG.
Canaport LNG, a partnership between Irving Oil (25%) and Repsol (75%), constructed a state-of-the-art LNG receiving and regasification terminal in Saint John, New Brunswick that began operations in 2009. It is the first LNG regasification plant in Canada, sending out natural gas to both Canadian and American markets. The terminal has a send-out capacity, or the ability to distribute via pipeline, 1 billion cubic feet (28 million cubic metres) of natural gas a day after it has been regasified from its liquid state.
Brunswick Pipeline.
Emera Inc. will invest approximately $350 million, for full ownership of a proposed pipeline which will deliver natural gas from the planned Canaport(TM) Liquefied Natural Gas (LNG) import terminal near Saint John, New Brunswick to markets in Canada and the US Northeast. Brunswick Pipeline will have a diameter of 30 in and will be capable of carrying approximately 850 Mcuft per day of re-gasified LNG. Capacity can be expanded with added compression.
The 145 km pipeline would extend through southwest New Brunswick to an interconnection with the Maritimes and Northeast Pipeline at the Canada/US border near St. Stephen, New Brunswick. The National Energy Board (NEB) has issued its Environmental Assessment Report (EA Report) on the proposed Brunswick Pipeline project. The main finding of the EA Report is that the project is not likely to result in significant adverse environmental effects, provided Brunswick Pipeline meets all of its environmental commitments, and all of the NEB’s recommendations are implemented. The pipeline's construction was completed on January 31, 2009.
Transportation.
Air service into Saint John is provided by the Saint John Airport/Aéroport de Saint-Jean, located near Loch Lomond approximately 25 kilometres by road northeast of the city centre. Flights are offered by Sunwing Airlines (seasonal) and Air Canada. WestJet recently decided to withdraw from the Saint John Airport. Quebec-based PASCAN Aviation announced its expansion into Saint John in late 2012, with direct flights from Saint John to Quebec City, Newfoundland, and other destinations beginning in September 2012.
The main highway in the city is the Saint John Throughway (Route 1). Route 1 extends west to St. Stephen, and northeast towards Moncton. A second major highway, Route 7, connects Saint John with Fredericton. There are two main road crossings over the Saint John River: the Harbour Bridge and the Reversing Falls Bridge, approximately 1 nmi upstream.
The Reversing Falls Railway Bridge carries rail traffic for the New Brunswick Southern Railway on the route from Saint John to Maine. Passenger rail service in Saint John was discontinued in 1994, although the Canadian National Railway and New Brunswick Southern Railway continue to provide freight service.
Bay Ferries operates a ferry service across the Bay of Fundy to Digby, Nova Scotia. The Summerville to Millidgeville Ferry, a free propeller (as opposed to cable) ferry service operated by the New Brunswick Department of Transportation, connects the Millidgeville neighbourhood with Summerville, New Brunswick, across the Kennebecasis River on the Kingston Peninsula.
Acadian Lines used to operate regular inter-city bus services between New Brunswick, Prince Edward Island and Nova Scotia as well as Rivière-du-Loup, and Quebec (connecting with Orléans Express). In 2011, Acadian Lines cancelled bus service on the route between Saint John and Bangor, Maine due to low ticket sales. In November 2012, due to inability to agree a contract, Acadian Lines ceased operations.
Bus service is provided by Saint John Transit (locally) and Maritime Bus (regionally).
Culture.
Saint John shares much of the same cultural roots found in cities like Boston and New York. The presence of Irish heritage is very apparent along with strong maritime traditions. Saint John is a true maritime city with ties to the fisheries and shipbuilding, and is known for the Marco Polo as its flagship vessel. The city has been a traditional hub for creativity, boasting many notable artists, actors and musicians, including Walter Pidgeon, Donald Sutherland, Louis B. Mayer, Fred Ross and Miller Brittain.
Saint John has a long history of brewers, such as Simeon Jones, The Olands, and James Ready. The city is now home to Moosehead Breweries, James Ready Brewing Co., and Big Tide Brewing Co.
Dance, music, and theatre ensembles in the city include:
Saint John has several small private art galleries, as well as concert series hosted by local churches and schools. Cultural festivals and venues include:
The following museums are also located in Saint John:
National Historic Sites of Canada located in Saint John include the following:
Sports.
The following teams are based in Saint John:
The following sporting events have been held here:
Saint John is also home to Exhibition Park Raceway, a Harness Racing facility that has been hosting this form of Horse Racing for over the past 120 years. Prior to 1950 it was known as Moosepath Park.
Education.
In 1964, the University of New Brunswick created UNB Saint John. Initially located in buildings throughout the downtown CBD, in 1968 UNBSJ opened a new campus in the city's Tucker Park neighbourhood. This campus has undergone expansion over the years and is the fastest growing component of the UNB system with many new buildings constructed between the 1970s-first decade of the 21st century. A trend in recent years has been a growth in the number of international students. The city also hosts a New Brunswick Community College campus in the East End of the city. There has also been a satellite campus of Dalhousie Medical School added within the UNBSJ campus in 2010, instructing 30 medical students each year.
In the fall of 2007, a report commissioned by the provincial government recommended that UNBSJ and the NBCC be reformed and consolidated into a new polytechnic post-secondary institute. The proposal immediately came under heavy criticism and led to the organizing of several protests in the uptown area. The diminishment of UNB as a nationally accredited university, the reduction in accessibility to receive degrees, and there are only a couple of the reasons why the community was enraged by the recommendation with support slightly below 90% to keep UNBSJ as it was, and expand the university under its current structure. Seeing that too much political capital would be lost, and that several Saint John are MPs were likely not to support the initiative if the policies recommended by the report were legislated, the government abandoned the commission's report and created an intra-provincial post-secondary commission.
Saint John is served by two school boards; District 8 for Anglophone schools and District 1 (based out of Dieppe, New Brunswick) for the city's only Francophone school, Centre-Scolaire-Communautaire Samuel-de-Champlain. Saint John is also home to Canada's oldest publicly funded school, Saint John High School. The other high schools in the city are Harbour View High School, St. Malachy's High School, and Simonds High School.

</doc>
<doc id="28928" url="http://en.wikipedia.org/wiki?curid=28928" title="Sinope">
Sinope

Sinope may refer to:

</doc>
<doc id="28938" url="http://en.wikipedia.org/wiki?curid=28938" title="Shell script">
Shell script

A shell script is a computer program designed to be run by the Unix shell, a command line interpreter. The various dialects of shell scripts are considered to be scripting languages.
Typical operations performed by shell scripts include file manipulation, program execution, and printing text.
Capabilities.
Shortcuts.
A shell script can provide a convenient variation of a system command where special environment settings, command options, or post-processing apply automatically, but in a way that allows the new script to still act as a fully normal Unix command.
One example would be to create a version of ls, the command to list files, giving it a shorter command name of l, which would be normally saved in a user's bin directory as /home/"username"/bin/l, and a default set of command options pre-supplied.
Here, the first line (Shebang) indicates which interpreter should execute the rest of the script, and the second line makes a listing with options for file format indicators, columns, all files (none omitted), and a size in blocks. The LC_COLLATE=C sets the default collation order to not fold upper and lower case together, not intermix dotfiles with normal filenames as a side effect of ignoring punctuation in the names (dotfiles are usually only shown if an option like -a is used), and the "$@" causes any parameters given to l to pass through as parameters to ls, so that all of the normal options and other syntax known to ls can still be used.
The user could then simply use l for the most commonly used short listing.
Another example of a shell script that could be used as a shortcut would be to print a list of all the files and directories within a given directory.
In this case, the shell script would start with its normal starting line of #!/bin/sh. Following this, the script executes the command clear which clears the terminal of all text before going to the next line. The following line provides the main function of the script. The ls -al command list the files and directories that are in the directory from which the script is being run. The ls command attributes could be changed to reflect the needs of the user.
Note: If an implementation does not have the clear command, try using the clr command instead.
Batch jobs.
Shell scripts allow several commands that would be entered manually at a command-line interface to be executed automatically, and without having to wait for a user to trigger each stage of the sequence. For example, in a directory with three C source code files, rather than manually running the four commands required to build the final program from them, one could instead create a C shell script, here named build and kept in the directory with them, which would compile them automatically:
The script would allow a user to save the file being edited, pause the editor, and then just run ./build to create the updated program, test it, and then return to the editor. Since the 1980s or so, however, scripts of this type have been replaced with utilities like make which are specialized for building programs.
Generalization.
Simple batch jobs are not unusual for isolated tasks, but using shell loops, tests, and variables provides much more flexibility to users. A Bash (Unix shell) script to convert JPEG images to PNG images, where the image names are provided on the command line—possibly via wildcards—instead of each being listed within the script, can be created with this file, typically saved in a file like /home/"username"/bin/jpg2png
The jpg2png command can then be run on an entire directory full of JPEG images with just /home/"username"/bin/jpg2png *.jpg
Verisimilitude.
A key feature of shell scripts is that the invocation of their interpreters is handled as a core operating system feature. So rather than a user's shell only being able to execute scripts in that shell's language, or a script only having its interpreter directive handled correctly if it was run from a shell (both of which were limitations in the early Bourne shell's handling of scripts), shell scripts are set up and executed by the OS itself. A modern shell script is not just on the same footing as system commands, but rather many system commands are actually shell scripts (or more generally, scripts, since some of them are not interpreted by a shell, but instead by Perl, Python, or some other language). This extends to returning exit codes like other system utilities to indicate success or failure, and allows them to be called as components of larger programs regardless of how those larger tools are implemented.
Like standard system commands, shell scripts classically omit any kind of filename extension unless intended to be read into a running shell through a special mechanism for this purpose (such as sh’s “codice_1”, or csh’s source).
Programming.
Many modern shells also supply various features usually found only in more sophisticated general-purpose programming languages, such as control-flow constructs, variables, comments, arrays, subroutine and so on. With these sorts of features available, it is possible to write reasonably sophisticated applications as shell scripts. However, they are still limited by the fact that most shell languages have little or no support for data typing systems, classes, threading, complex math, and other common full language features, and are also generally much slower than compiled code or interpreted languages written with speed as a performance goal.
The standard Unix tools sed and awk provide extra capabilities for shell programming; Perl can also be embedded in shell scripts as can other scripting languages like Tcl. Perl and Tcl come with graphics toolkits as well.
Other scripting languages.
Many powerful scripting languages have been introduced for tasks that are too large or complex to be comfortably handled with ordinary shell scripts, but for which the advantages of a script are desirable and the development overhead of a full-blown, compiled programming language would be disadvantageous. The specifics of what separates scripting languages from high-level programming languages is a frequent source of debate. But generally speaking a scripting language is one which requires an interpreter.
Life cycle.
Shell scripts often serve as an initial stage in software development, and are often subject to conversion later to a different underlying implementation, most commonly being converted to Perl, Python, or C. The interpreter directive allows the implementation detail to be fully hidden inside the script, rather than being exposed as a filename extension, and provides for seamless reimplementation in different languages with no impact on end users.
Advantages and disadvantages.
Perhaps the biggest advantage of writing a shell script is that the commands and syntax are exactly the same as those directly entered at the command line. The programmer does not have to switch to a totally different syntax, as they would if the script were written in a different language, or if a compiled language was used.
Often, writing a shell script is much quicker than writing the equivalent code in other programming languages. The many advantages include easy program or file selection, quick start, and interactive debugging. A shell script can be used to provide a sequencing and decision-making linkage around existing programs, and for moderately sized scripts the absence of a compilation step is an advantage. Interpretive running makes it easy to write debugging code into a script and re-run it to detect and fix bugs. Non-expert users can use scripting to tailor the behavior of programs, and shell scripting provides some limited scope for multiprocessing.
On the other hand, shell scripting is prone to costly errors. Inadvertent typing errors such as rm -rf * / (instead of the intended rm -rf */) are folklore in the Unix community; a single extra space converts the command from one that deletes everything in the sub-directories to one which deletes everything—and also tries to delete everything in the root directory. Similar problems can transform cp and mv into dangerous weapons, and misuse of the > redirect can delete the contents of a file. This is made more problematic by the fact that many UNIX commands differ in name by only one letter: cp, cd, dd, df, etc.
Another significant disadvantage is the slow execution speed and the need to launch a new process for almost every shell command executed. When a script's job can be accomplished by setting up a pipeline in which efficient filter commands perform most of the work, the slowdown is mitigated, but a complex script is typically several orders of magnitude slower than a conventional compiled program that performs an equivalent task.
There are also compatibility problems between different platforms. Larry Wall, creator of Perl, famously wrote that "It is easier to port a shell than a shell script."
Similarly, more complex scripts can run into the limitations of the shell scripting language itself; the limits make it difficult to write quality code, and extensions by various shells to ameliorate problems with the original shell language can make problems worse.
Many disadvantages of using some script languages are caused by design flaws within the language syntax or implementation, and are not necessarily imposed by the use of a text-based command line; there are a number of shells which use other shell programming languages or even full-fledged languages like Scsh (which uses Scheme).

</doc>
