<doc id="359" url="http://en.wikipedia.org/wiki?curid=359" title="List of Atlas Shrugged characters">
List of Atlas Shrugged characters

This is a list of characters in Ayn Rand's novel "Atlas Shrugged."
Major characters.
The following are major characters from the novel.
Protagonists.
Dagny Taggart.
Dagny Taggart is the protagonist of the novel. She is Vice-President in Charge of Operations for Taggart Transcontinental, under her brother, James Taggart. Given James' incompetence, Dagny is responsible for all the workings of the railroad.
Francisco d'Anconia.
Francisco d'Anconia is one of the central characters in "Atlas Shrugged", an owner by inheritance of the world's largest copper mining operation. He is a childhood friend, and the first love, of Dagny Taggart. A child prodigy of exceptional talents, Francisco was dubbed the "climax" of the d'Anconia line, an already prestigious family of skilled industrialists. He was a classmate of John Galt and Ragnar Danneskjöld and student of both Hugh Akston and Robert Stadler. He began working while still in school, proving that he could have made a fortune without the aid of his family's wealth and power. Later, Francisco bankrupts the d'Anconia business to put it out of others' reach. His full name is given as "Francisco Domingo Carlos Andres Sebastián d'Anconia".
John Galt.
John Galt is the primary male hero of "Atlas Shrugged". He initially appears as an unnamed menial worker for Taggart Transcontinental, who often dines with Eddie Willers in the employees' cafeteria, and leads Eddie to reveal important information about Dagny Taggart and Taggart Transcontinental. Only Eddie's side of their conversations is given in the novel. Later in the novel, the reader discovers this worker's true identity.
Before working for Taggart Transcontinental, Galt worked as an engineer for the Twentieth Century Motor Company, where he secretly invented a generator of usable electric energy from ambient static electricity, but abandoned his prototype, and his employment, when dissatisfied by an easily corrupted novel system of payment. This prototype was found by Dagny Taggart and Hank Rearden. Galt himself remains concealed, throughout much of the novel, in a valley concealed by himself, where he unites the most skillful inventors and business leaders under his leadership. Much of the book's third division is given to his broadcast speech, which presents the author's philosophy of Objectivism.
Henry "Hank" Rearden.
Henry (known as "Hank") Rearden is one of the central characters in "Atlas Shrugged". He owns the most important steel company in the United States, and invents Rearden Metal, an alloy stronger than steel (with similar properties to stainless steel). He lives in Philadelphia with his wife Lillian, his brother Philip, and his elderly mother. Rearden represents a type of self-made man or prototypical hero, and illustrates Rand's theory of sex in so far as he accepts the traditional view of sexual congress as a subhuman instinct, but responds sexually to Dagny Taggart.
Eddie Willers.
Edwin "Eddie" Willers is the Special Assistant to the Vice-President in Charge of Operations at Taggart Transcontinental. His father and grandfather worked for the Taggarts, and himself likewise. He is completely loyal to Dagny and to Taggart Transcontinental. Willers does not possess the creative ability of Galt's associates, but matches them in moral courage and is capable of appreciating and making use of their creations. After Dagny shifts her attention and loyalty to saving the captive Galt, Willers maintains the railroad until its collapse.
Ragnar Danneskjöld.
One of Galt's first followers, and world famous as a pirate, who seizes relief ships sent from the United States to the People's States of Europe. He works to ensure that once those espousing Galt's philosophy are restored to their rightful place in society, they have enough capital to rebuild the world. Kept in the background for much of the book, Danneskjöld makes a personal appearance to encourage Rearden to persevere in his increasingly difficult situation, and gives him a bar of gold as compensation for the income taxes he has paid over the last several years. Danneskjöld is married to the actress Kay Ludlow; their relationship is kept hidden from the outside world, which only knows of Ludlow as a retired film star. Considered a misfit by Galt's other adherents, he views his actions as a means to speed the world along in understanding Galt's perspective.
According to Barbara Branden, who was closely associated with Rand at the time the book was written, there were sections written describing Danneskjöld's adventures at sea, cut from the final published text. In a 1974 comment at a lecture, Ayn Rand admitted that Danneskjöld's name was a tribute to Victor Hugo's novel, "Hans of Iceland", wherein the hero becomes the first of the Counts of Danneskjöld. In the published book, Danneskjöld is always seen through the eyes of others (Dagny Taggart or Hank Rearden), except for a brief paragraph in the very last chapter.
Antagonists.
James Taggart.
The President of Taggart Transcontinental and the book's most important antagonist. Taggart is an expert influence peddler but incapable of making operational decisions on his own. He relies on his sister, Dagny Taggart, to actually run the railroad, but nonetheless opposes her in almost every endeavor. In a sense, he is the antithesis of Dagny. This contradiction leads to the recurring absurdity of his life: the desire to overcome those on whom his life depends, and the horror that he will succeed at this. In the final chapters of the novel, he suffers a complete mental breakdown upon realizing that he can no longer deceive himself in this respect.
Lillian Rearden.
The unsupportive wife of Hank Rearden, who dislikes his habits and (secretly at first) seeks to ruin Rearden to prove her own value. Lillian achieves this, when she passes information to James Taggart about her husband's affair with his sister. This information is used to persuade Rearden to sign a Gift Certificate which delivers all the property rights of Rearden Metal to others. Lillian thereafter uses James Taggart for sexual satisfaction, until Hank abandons her.
Dr. Floyd Ferris.
Ferris is a biologist who works as "co-ordinator" at the State Science Institute. He uses his position there to deride reason and productive achievement, and publishes a book entitled "Why Do You Think You Think?" He clashes on several occasions with Hank Rearden, and twice attempts to blackmail Rearden into giving up Rearden Metal. He is also one of the group of looters who tries to get Rearden to agree to the Steel Unification Plan. Ferris hosts the demonstration of the Project X weapon, and is the creator of the Ferris Persuader, a torture machine. When John Galt is captured by the looters, Ferris uses the device on Galt, but it breaks down before extracting the information Ferris wants from Galt. Ferris represents the group which uses brute force on the heroes to achieve the ends of the looters.
Dr. Robert Stadler.
A former professor at Patrick Henry University, and along with colleague Hugh Akston, mentor to Francisco d'Anconia, John Galt and Ragnar Danneskjöld. He has since become a sell-out, one who had great promise but squandered it for social approval, to the detriment of the free. He works at the State Science Institute where all his inventions are perverted for use by the military, including the instrument of his demise: Project X (Xylophone). The character was, in part, modeled on J. Robert Oppenheimer, whom Rand had interviewed for an earlier project, and his part in the creation of nuclear weapons. To his former student Galt, Stadler represents the epitome of human evil, as the "man who knew better" but chose not to act for the good.
Wesley Mouch.
The incompetent and treacherous lobbyist whom Hank Rearden reluctantly employs in Washington, who rises to prominence and authority throughout the novel through trading favours and disloyalty. In return for betraying Hank by helping broker the Equalization of Opportunity Bill (which, by restricting the number of businesses each person may own to one, forces Hank to divest most of his companies), he is given a senior position at the Bureau of Economic Planning and National Resources. Later in the novel he becomes its Top Co-ordinator, a position that eventually becomes Economic Dictator of the country.
Secondary characters.
The following secondary characters also appear in the novel.

</doc>
<doc id="661" url="http://en.wikipedia.org/wiki?curid=661" title="Argument (disambiguation)">
Argument (disambiguation)

In philosophy and logic, an argument is an attempt to persuade someone of something, or give evidence or reasons for accepting a particular conclusion. 
Argument may also refer to: 

</doc>
<doc id="674" url="http://en.wikipedia.org/wiki?curid=674" title="Anatomy">
Anatomy

Anatomy is the branch of biology concerned with the study of the structure of organisms and their parts; with further division into zootomy ("animals") and phytotomy ("plants"). In some of its facets, anatomy is related to embryology and comparative anatomy, which itself is closely related to evolutionary biology and phylogeny. Human anatomy is one of the basic essential sciences of medicine.
The discipline of anatomy is divided into macroscopic and microscopic anatomy. Macroscopic anatomy, or gross anatomy, is the examination of an animal’s body parts using unaided eyesight. Gross anatomy also includes the branch of superficial anatomy. Microscopic anatomy involves the use of optical instruments in the study of the tissues of various structures, known as histology and also in the study of cells.
The history of anatomy is characterized by a progressive understanding of the functions of the organs and structures of the human body. Methods have also improved dramatically, advancing from the examination of animals by dissection of carcasses and cadavers (corpses) to 20th century medical imaging techniques including X-ray, ultrasound, and magnetic resonance imaging.
Definition.
Derived from the Greek ἀνατέμνω "anatemnō" "I cut up, cut open" from ἀνά "ana" "up", and τέμνω "temnō" "I cut", anatomy is the scientific study of the structure of organisms including their systems, organs and tissues. It includes the appearance and position of the various parts, the materials from which they are composed, their locations and their relationships with other parts. Anatomy is quite distinct from physiology and biochemistry, which deal respectively with the functions of those parts and the chemical processes involved. For example, an anatomist is concerned with the shape, size, position, structure, blood supply and innervation of an organ such as the liver; while a physiologist is interested in the production of bile, the role of the liver in nutrition and the regulation of bodily functions.
The discipline of anatomy can be subdivided into a number of branches including gross or macroscopic anatomy and microscopic anatomy. Gross anatomy is the study of structures large enough to be seen with the naked eye, and also includes superficial anatomy or surface anatomy, the study by sight of the external body features. Microscopic anatomy is the study of structures on a microscopic scale, including histology (the study of tissues), and embryology (the study of an organism in its immature condition).
Anatomy can be studied using both invasive and non-invasive methods with the goal of obtaining information about the structure and organization of organs and systems. Methods used include dissection, in which a body is opened and its organs studied, and endoscopy, in which a video camera-equipped instrument is inserted through a small incision in the body wall and used to explore the internal organs and other structures. Angiography using X-rays or magnetic resonance angiography are methods to visualize blood vessels.
The term "anatomy" is commonly taken to refer to human anatomy. However, substantially the same structures and tissues are found throughout the rest of the animal kingdom and the term also includes the anatomy of other animals. The term "zootomy" is also sometimes used to specifically refer to animals. The structure and tissues of plants are of a dissimilar nature and they are studied in plant anatomy.
Animal tissues.
The kingdom Animalia or metazoa, contains multicellular organisms that are heterotrophic and motile (although some have secondarily adopted a sessile lifestyle). Most animals have bodies differentiated into separate tissues and these animals are also known as eumetazoans. They have an internal digestive chamber, with one or two openings; the gametes are produced in multicellular sex organs, and the zygotes include a blastula stage in their embryonic development. Metazoans do not include the sponges, which have undifferentiated cells.
Unlike plant cells, animal cells have neither a cell wall nor chloroplasts. Vacuoles, when present, are more in number and much smaller than those in the plant cell. The body tissues are composed of numerous types of cell, including those found in muscles, nerves and skin. Each typically has a cell membrane formed of phospholipids, cytoplasm and a nucleus. All of the different cells of an animal are derived from the embryonic germ layers. Those simpler invertebrates which are formed from two germ layers of ectoderm and endoderm are called diploblastic and the more developed animals whose structures and organs are formed from three germ layers are called triploblastic. All of a triploblastic animal's tissues and organs are derived from the three germ layers of the embryo, the ectoderm, mesoderm and endoderm.
Animal tissues can be grouped into four basic types: connective, epithelial, muscle and nervous tissue.
Connective tissue.
Connective tissues are fibrous and made up of cells scattered among inorganic material called the extracellular matrix. Connective tissue gives shape to organs and holds them in place. The main types are loose connective tissue, adipose tissue, fibrous connective tissue, cartilage and bone. The extracellular matrix contains proteins, the chief and most abundant of which is collagen. Collagen plays a major part in organizing and maintaining tissues. The matrix can be modified to form a skeleton to support or protect the body. An exoskeleton is a thickened, rigid cuticle which is stiffened by mineralisation, as in crustaceans or by the cross-linking of its proteins as in insects. An endoskeleton is internal and present in all developed animals, as well as in many of those less developed.
Epithelium.
Epithelial tissue is composed of closely packed cells, bound to each other by cell adhesion molecules, with little intercellular space. Epithelial cells can be squamous (flat), cuboidal or columnar and rest on a basal lamina, the upper layer of the basement membrane, the lower layer is the reticular lamina lying next to the connective tissue in the extracellular matrix secreted by the epithelial cells. There are many different types of epithelium, modified to suit a particular function. In the respiratory tract there is a type of ciliated epithelial lining; in the small intestine there are microvilli on the epithelial lining and in the large intestine there are intestinal villi. Skin consists of an outer layer of keratinised stratified squamous epithelium that covers the exterior of the vertebrate body. Keratinocytes make up to 95% of the cells in the skin. The epithelial cells on the external surface of the body typically secrete an extracellular matrix in the form of a cuticle. In simple animals this may just be a coat of glycoproteins. In more advanced animals, many glands are formed of epithelial cells.
Muscle tissue.
Muscle cells (myocytes) form the active contractile tissue of the body. Muscle tissue functions to produce force and cause motion, either locomotion or movement within internal organs. Muscle is formed of contractile filaments and is separated into three types; smooth muscle, skeletal muscle and obliquely striated muscle. Smooth muscle has no striations when examined microscopically. It contracts slowly but maintains contractibility over a wide range of stretch lengths. It is found in such organs as sea anemone tentacles and the body wall of sea cucumbers. Skeletal muscle contracts rapidly but has a limited range of extension. It is found in the movement of appendages and jaws. Obliquely striated muscle is intermediate between the other two. The filaments are staggered and this is the type of muscle found in earthworms that can extend slowly or make rapid contractions. In higher animals striated muscles occur in bundles attached to bone to provide movement and are often arranged in antagonistic sets. Smooth muscle is found in the walls of the uterus, bladder, intestines, stomach, esophagus, respiratory airways, and blood vessels. Cardiac muscle is found only in the heart, allowing it to contract and pump blood round the body.
Nervous tissue.
Nervous tissue is composed of many nerve cells known as neurons which transmit information. In some slow-moving radially symmetrical marine animals such as ctenophores and cnidarians (including sea anemones and jellyfish), the nerves form a nerve net, but in most animals they are organized longitudinally into bundles. In simple animals, receptor neurons in the body wall cause a local reaction to a stimulus. In more complex animals, specialised receptor cells such as chemoreceptors and photoreceptors are found in groups and send messages along neural networks to other parts of the organism. Neurons can be connected together in ganglia. In higher animals, specialized receptors are the basis of sense organs and there is a central nervous system (brain and spinal cord) and a peripheral nervous system. The latter consists of sensory nerves that transmit information from sense organs and motor nerves that influence target organs. The peripheral nervous system is divided into the somatic nervous system which conveys sensation and controls voluntary muscle, and the autonomic nervous system which involuntarily controls smooth muscle, certain glands and internal organs, including the stomach.
Vertebrate anatomy.
All vertebrates have a similar basic body plan and at some point in their lives, (mostly in the embryonic stage), share the major chordate characteristics; a stiffening rod, the notochord; a dorsal hollow tube of nervous material, the neural tube; pharyngeal arches; and a tail posterior to the anus. The spinal cord is protected by the vertebral column and is above the notochord and the gastrointestinal tract is below it. Nervous tissue is derived from the ectoderm, connective tissues are derived from mesoderm, and gut is derived from the endoderm. At the posterior end is a tail which continues the spinal cord and vertebrae but not the gut. The mouth is found at the anterior end of the animal, and the anus at the base of the tail. The defining characteristic of a vertebrate is the vertebral column, formed in the development of the segmented series of vertebrae. In most vertebrates the notochord becomes the nucleus pulposus of the intervertebral discs. However, a few vertebrates, such as the sturgeon and the coelacanth retain the notochord into adulthood. Jawed vertebrates are typified by paired appendages, fins or legs, which may be secondarily lost. The limbs of vertebrates are considered to be homologous because the same underlying skeletal structure was inherited from their last common ancestor. This is one of the arguments put forward by Charles Darwin to support his theory of evolution.
Fish anatomy.
The body of a fish is divided into a head, trunk and tail, although the divisions between the three are not always externally visible. The skeleton, which forms the support structure inside the fish, is either made of cartilage, in cartilaginous fish, or bone in bony fish. The main skeletal element is the vertebral column, composed of articulating vertebrae which are lightweight yet strong. The ribs attach to the spine and there are no limbs or limb girdles. The main external features of the fish, the fins, are composed of either bony or soft spines called rays, which with the exception of the caudal fins, have no direct connection with the spine. They are supported by the muscles which compose the main part of the trunk. The heart has two chambers and pumps the blood through the respiratory surfaces of the gills and on round the body in a single circulatory loop. The eyes are adapted for seeing underwater and have only local vision. There is an inner ear but no external or middle ear. Low frequency vibrations are detected by the lateral line system of sense organs that run along the length of the sides of fish, and these respond to nearby movements and to changes in water pressure.
Sharks and rays are basal fish with numerous primitive anatomical features similar to those of ancient fish, including skeletons composed of cartilage. Their bodies tend to be dorso-ventrally flattened, they usually have five pairs of gill slits and a large mouth set on the underside of the head. The dermis is covered with separate dermal placoid scales. They have a cloaca into which the urinary and genital passages open, but not a swim bladder. Cartilaginous fish produce a small number of large, yolky eggs. Some species are ovoviviparous and the young develop internally but others are oviparous and the larvae develop externally in egg cases.
The bony fish lineage shows more derived anatomical traits, often with major evolutionary changes from the features of ancient fish. They have a bony skeleton, are generally laterally flattened, have five pairs of gills protected by an operculum, and a mouth at or near the tip of the snout. The dermis is covered with overlapping scales. Bony fish have a swim bladder which helps them maintain a constant depth in the water column, but not a cloaca. They mostly spawn a large number of small eggs with little yolk which they broadcast into the water column.
Amphibian anatomy.
Amphibians are a class of animals comprising frogs, salamanders and caecilians. They are tetrapods, but the caecilians and a few species of salamander have either no limbs or their limbs are much reduced in size. Their main bones are hollow and lightweight and are fully ossified and the vertebrae interlock with each other and have articular processes. Their ribs are usually short and may be fused to the vertebrae. Their skulls are mostly broad and short, and are often incompletely ossified. Their skin contains little keratin and lacks scales, but contains many mucous glands and in some species, poison glands. The hearts of amphibians have three chambers, two atria and one ventricle. They have a urinary bladder and nitrogenous waste products are excreted primarily as urea. Amphibians breathe by means of buccal pumping, a pump action in which air is first drawn into the buccopharyngeal region through the nostrils. These are then closed and the air is forced into the lungs by contraction of the throat. They supplement this with gas exchange through the skin which needs to be kept moist.
In frogs the pelvic girdle is robust and the hind legs are much longer and stronger than the forelimbs. The feet have four or five digits and the toes are often webbed for swimming or have suction pads for climbing. Frogs have large eyes and no tail. Salamanders resemble lizards in appearance; their short legs project sideways, the belly is close to or in contact with the ground and they have a long tail. Caecilians superficially resemble earthworms and are limbless. They burrow by means of zones of muscle contractions which move along the body and they swim by undulating their body from side to side.
Reptile anatomy.
Reptiles are a class of animals comprising turtles, tuataras, lizards, snakes and crocodiles. They are tetrapods, but the snakes and a few species of lizard either have no limbs or their limbs are much reduced in size. Their bones are better ossified and their skeletons stronger than those of amphibians. The teeth are conical and mostly uniform in size. The surface cells of the epidermis are modified into horny scales which create a waterproof layer. Reptiles are unable to use their skin for respiration as do amphibians and have a more efficient respiratory system drawing air into their lungs by expanding their chest walls. The heart resembles that of the amphibian but there is a septum which more completely separates the oxygenated and deoxygenated bloodstreams. The reproductive system is designed for internal fertilisation, with a copulatory organ present in most species. The eggs are surrounded by amniotic membranes which prevents them from drying out and are laid on land, or develop internally in some species. The bladder is small as nitrogenous waste is excreted as uric acid.
Turtles are notable for their protective shells. They have an inflexible trunk encased in a horny carapace above and a plastron below. These are formed from bony plates embedded in the dermis which are overlain by horny ones and are partially fused with the ribs and spine. The neck is long and flexible and the head and the legs can be drawn back inside the shell. Turtles are vegetarians and the typical reptile teeth have been replaced by sharp, horny plates. In aquatic species, the front legs are modified into flippers.
Tuataras superficially resemble lizards but the lineages diverged in the Triassic period. There is one living species, "Sphenodon punctatus". The skull has two openings (fenestrae) on either side and the jaw is rigidly attached to the skull. There is one row of teeth in the lower jaw and this fits between the two rows in the upper jaw when the animal chews. The teeth are merely projections of bony material from the jaw and eventually wear down. The brain and heart are more primitive than is the case in other reptiles and the lungs have a single chamber and lack bronchi. The tuatara has a well-developed parietal eye on its forehead.
Lizards have skulls with only one fenestra on each side, the lower bar of bone below the second fenestra having been lost. This results in the jaws being less rigidly attached which allows the mouth to open wider. Lizards are mostly quadrupeds, with the trunk held off the ground by short, sideways-facing legs, but a few species have no limbs and resemble snakes. Lizards have moveable eyelids, eardrums are present and some species have a central parietal eye.
Snakes are closely related to lizards, having branched off from a common ancestral lineage during the Cretaceous period, and they share many of the same features. The skeleton consists of a skull, a hyoid bone, spine and ribs though a few species retain a vestige of the pelvis and rear limbs in the form of pelvic spurs. The bar under the second fenestra has also been lost and the jaws have extreme flexibility allowing the snake to swallow its prey whole. Snakes lack moveable eyelids, the eyes being covered by transparent "spectacle" scales. They do not have eardrums but can detect ground vibrations through the bones of their skull. Their forked tongues are used as organs of taste and smell and some species have sensory pits on their heads enabling them to locate warm-blooded prey.
Crocodilians are large, low-slung aquatic reptiles with long snouts and large numbers of teeth. The head and trunk are dorso-ventrally flattened and the tail is laterally compressed. It undulates from side to side to force the animal through the water when swimming. The tough keratinised scales provide body armour and some are fused to the skull. The nostrils, eyes and ears are elevated above the top of the flat head enabling them to remain above the surface of the water when the animal is floating. Valves seal the nostrils and ears when it is submerged. Unlike other reptiles, crocodilians have hearts with four chambers allowing complete separation of oxygenated and deoxygenated blood.
Bird anatomy.
Birds are tetrapods but though their hind limbs are used for walking or hopping, their front limbs are wings covered with feathers and adapted for flight. Birds are endothermic, have a high metabolic rate, a light skeletal system and powerful muscles. The long bones are thin, hollow and very light. Air sac extensions from the lungs occupy the centre of some bones. The sternum is wide and usually has a keel and the caudal vertebrae are fused. There are no teeth and the narrow jaws are adapted into a horn-covered beak. The eyes are relatively large, particularly in nocturnal species such as owls. They face forwards in predators and sideways in ducks.
The feathers are outgrowths of the epidermis and are found in localized bands from where they fan out over the skin. Large flight feathers are found on the wings and tail, contour feathers cover the bird's surface and fine down occurs on young birds and under the contour feathers of water birds. The only cutaneous gland is the single uropygial gland near the base of the tail. This produces an oily secretion that waterproofs the feathers when the bird preens. There are scales on the legs, feet and claws on the tips of the toes.
Mammal anatomy.
Mammals are a diverse class of animals, mostly terrestrial but some are aquatic and others have evolved flapping or gliding flight. They mostly have four limbs but some aquatic mammals have no limbs or limbs modified into fins and the forelimbs of bats are modified into wings. The legs of most mammals are situated below the trunk, which is held well clear of the ground. The bones of mammals are well ossified and their teeth, which are usually differentiated, are coated in a layer of prismatic enamel. The teeth are shed once (milk teeth) during the animal's lifetime or not at all, as is the case in cetaceans. Mammals have three bones in the middle ear and a cochlea in the inner ear. They are clothed in hair and their skin contains glands which secrete sweat. Some of these glands are specialised as mammary glands, producing milk to feed the young. Mammals breathe with lungs and have a muscular diaphragm separating the thorax from the abdomen which helps them draw air into the lungs. The mammalian heart has four chambers and oxygenated and deoxygenated blood are kept entirely separate. Nitrogenous waste is excreted primarily as urea.
Mammals are amniotes, and most are viviparous, giving birth to live young. The exception to this are the egg-laying monotremes, the platypus and the echidnas of Australia. Most other mammals have a placenta through which the developing foetus obtains nourishment, but in marsupials, the foetal stage is very short and the immature young is born and finds its way to its mother's pouch where it latches on to a nipple and completes its development.
Human anatomy.
Humans have the overall body plan of a mammal. Humans have a head, neck, trunk (which includes the thorax and abdomen), two arms and hands and two legs and feet.
Generally, students of certain biological sciences, paramedics, prosthetists and orthotists, physiotherapists, occupational therapists, nurses, and medical students learn gross anatomy and microscopic anatomy from anatomical models, skeletons, textbooks, diagrams, photographs, lectures and tutorials, and in addition, medical students generally also learn gross anatomy through practical experience of dissection and inspection of cadavers. The study of microscopic anatomy (or histology) can be aided by practical experience examining histological preparations (or slides) under a microscope.
Human anatomy, physiology and biochemistry are complementary basic medical sciences, which are generally taught to medical students in their first year at medical school. Human anatomy can be taught regionally or systemically; that is, respectively, studying anatomy by bodily regions such as the head and chest, or studying by specific systems, such as the nervous or respiratory systems. The major anatomy textbook, Gray's Anatomy, has been reorganized from a systems format to a regional format, in line with modern teaching methods. A thorough working knowledge of anatomy is required by physicians, especially surgeons and doctors working in some diagnostic specialties, such as histopathology and radiology.
Academic human anatomists are usually employed by universities, medical schools or teaching hospitals. They are often involved in teaching anatomy, and research into certain systems, organs, tissues or cells.
Invertebrate anatomy.
Invertebrates constitute a vast array of living organisms ranging from the simplest unicellular eukaryotes such as "Paramecium" to such complex multicellular animals as the octopus, lobster and dragonfly. They constitute about 95% of the animal species. By definition, none of these creatures has a backbone. The cells of single-cell protozoans have the same basic structure as those of multicellular animals but some parts are specialised into the equivalent of tissues and organs. Locomotion is often provided by cilia or flagella or may proceed via the advance of pseudopodia, food may be gathered by phagocytosis, energy needs may be supplied by photosynthesis and the cell may be supported by an endoskeleton or an exoskeleton. Some protozoans can form multicellular colonies.
Metazoans are multicellular organism, different groups of cells of which have separate functions. The most basic types of metazoan tissues are epithelium and connective tissue, both of which are present in nearly all invertebrates. The outer surface of the epidermis is normally formed of epithelial cells and secretes an extracellular matrix which provides support to the organism. An endoskeleton derived from the mesoderm is present in echinoderms, sponges and some cephalopods. Exoskeletons are derived from the epidermis and is composed of chitin in arthropods (insects, spiders, ticks, shrimps, crabs, lobsters). Calcium carbonate constitutes the shells of molluscs, brachiopods and some tube-building polychaete worms and silica forms the exoskeleton of the microscopic diatoms and radiolaria. Other invertebrates may have no rigid structures but the epidermis may secrete a variety of surface coatings such as the pinacoderm of sponges, the gelatinous cuticle of cnidarians (polyps, sea anemones, jellyfish) and the collagenous cuticle of annelids. The outer epithelial layer may include cells of several types including sensory cells, gland cells and stinging cells. There may also be protrusions such as microvilli, cilia, bristles, spines and tubercles.
Arthropod anatomy.
Arthropods comprise the largest phylum in the animal kingdom with over a million known invertebrate species.
Insects possess segmented bodies supported by a hard-jointed outer covering, the exoskeleton, made mostly of chitin. The segments of the body are organized into three distinct parts, a head, a thorax and an abdomen. The head typically bears a pair of sensory antennae, a pair of compound eyes, one to three simple eyes (ocelli) and three sets of modified appendages that form the mouthparts. The thorax has three pairs of segmented legs, one pair each for the three segments that compose the thorax and one or two pairs of wings. The abdomen is composed of eleven segments, some of which may be fused and houses the digestive, respiratory, excretory and reproductive systems. There is considerable variation between species and many adaptations to the body parts, especially wings, legs, antennae and mouthparts.
Spiders a class of arachnids have four pairs of legs; a body of two segments—a cephalothorax and an abdomen. Spiders have no wings and no antennae. They have mouthparts called chelicerae which are often connected to venom glands as most spiders are venomous. They have a second pair of appendages called pedipalps attached to the cephalothorax. These have the same segmentation as the legs and function as taste and smell organs. At the end of each pedipalp is a spoon-shaped cymbium that acts to support the pedipalp.
History.
Ancient.
In 1600 BCE, the Edwin Smith Papyrus, an Ancient Egyptian medical text, described the heart, its vessels, liver, spleen, kidneys, hypothalamus, uterus and bladder, and showed the blood vessels diverging from the heart. The Ebers Papyrus (c. 1550 BCE) features a "treatise on the heart", with vessels carrying all the body's fluids to or from every member of the body.
The anatomy of the muscles and skeleton is described in the "Hippocratic Corpus", an Ancient Greek medical work written by unknown authors. Aristotle described vertebrate anatomy based on animal dissection. Praxagoras identified the difference between arteries and veins. Also in the 4th century BCE, Herophilos and Erasistratus produced more accurate anatomical descriptions based on vivisection of criminals in Alexandria during the Ptolemaic dynasty.
In the 2nd century, Galen of Pergamum, an anatomist, clinician, writer and philosopher, wrote the final and highly influential anatomy treatise of ancient times. He compiled existing knowledge and studied anatomy through dissection of animals. He was one of the first experimental physiologists through his vivisection experiments on animals. Galen's drawings, based mostly on dog anatomy, became effectively the only anatomical textbook for the next thousand years. His work was known to Renaissance doctors only through Islamic Golden Age medicine until it was translated from the Greek some time in the 15th century.
Medieval to early modern.
Anatomy developed little from classical times until the sixteenth century; as the historian Marie Boas writes, "Progress in anatomy before the sixteenth century is as mysteriously slow as its development after 1500 is startlingly rapid".:120–121 Between 1275 and 1326, the anatomists Mondino de Luzzi, Alessandro Achillini and Antonio Benivieni at Bologna carried out the first systematic human dissections since ancient times. Mondino's "Anatomy" of 1316 was the first textbook in the medieval rediscovery of human anatomy. It describes the body in the order followed in Mondino's dissections, starting with the abdomen, then the thorax, then the head and limbs. It was the standard anatomy textbook for the next century.
Leonardo da Vinci (1452–1519) was trained in anatomy by Andrea del Verrocchio. He made use of his anatomical knowledge in his artwork, making many sketches of skeletal structures, muscles and organs of humans and other vertebrates that he dissected.
Andreas Vesalius (1514–1564) (Latinized from Andries van Wezel), professor of anatomy at the University of Padua, is considered the founder of modern human anatomy. Originally from Brabant, Vesalius published the influential book "De humani corporis fabrica" ("the structure of the human body"), a large format book in seven volumes, in 1543. The accurate and intricately detailed illustrations, often in allegorical poses against Italianate landscapes, are thought to have been made by the artist Jan van Calcar, a pupil of Titian.
In England, anatomy was the subject of the first public lectures given in any science; these were given by the Company of Barbers and Surgeons in the 16th century, joined in 1583 by the Lumleian lectures in surgery at the Royal College of Physicians.
Late modern.
In the United States, medical schools began to be set up towards the end of the 18th century. Classes in anatomy needed a continual stream of cadavers for dissection and these were difficult to obtain. Philadelphia, Baltimore and New York were all renowned for body snatching activity as criminals raided graveyards at night, removing newly buried corpses from their coffins. A similar problem existed in Britain where demand for bodies became so great that grave-raiding and even anatomy murder were practised to obtain cadavers. Some graveyards were in consequence protected with watchtowers. The practice was halted in Britain by the Anatomy Act of 1832, while in the United States, similar legislation was enacted after the physician William S. Forbes of Jefferson Medical College was found guilty in 1882 of "complicity with resurrectionists in the despoliation of graves in Lebanon Cemetery".
The teaching of anatomy in Britain was transformed by Sir John Struthers, Regius Professor of Anatomy at the University of Aberdeen from 1863 to 1889. He was responsible for setting up the system of three years of "pre-clinical" academic teaching in the sciences underlying medicine, including especially anatomy. This system lasted until the reform of medical training in 1993 and 2003. As well as teaching, he collected many vertebrate skeletons for his museum of comparative anatomy, published over 70 research papers, and became famous for his public dissection of the Tay Whale. From 1822 the Royal College of Surgeons regulated the teaching of anatomy in medical schools. Medical museums provided examples in comparative anatomy, and were often used in teaching. Ignaz Semmelweis investigated puerperal fever and he discovered how it was caused. He noticed that the frequently fatal fever occurred more often in mothers examined by medical students than by midwives. The students went from the dissecting room to the hospital ward and examined women in childbirth. Semmelweis showed that when the trainees washed their hands in chlorinated lime before each clinical examination, the incidence of puerperal fever among the mothers could be reduced dramatically.
Before the era of modern medical procedures, the main means for studying the internal structure of the body were palpation and dissection. It was the advent of microscopy that opened up an understanding of the building blocks that constituted living tissues. Technical advances in the development of achromatic lenses increased the resolving power of the microscope and around 1839, Matthias Jakob Schleiden and Theodor Schwann identified that cells were the fundamental unit of organization of all living things. Study of small structures involved passing light through them and the microtome was invented to provide sufficiently thin slices of tissue to examine. Staining techniques using artificial dyes were established to help distinguish between different types of tissue. The fields of cytology and histology developed from here in the late 19th century. The invention of the electron microscope brought a great advance in resolution power and allowed research into the ultrastructure of cells and the organelles and other structures within them. About the same time, in the 1950s, the use of X-ray diffraction for studying the crystal structures of proteins, nucleic acids and other biological molecules gave rise to a new field of molecular anatomy.
Short wavelength electromagnetic radiation such as X-rays can be passed through the body and used in medical radiography to view interior structures that have different degrees of opaqueness. Nowadays, modern techniques such as magnetic resonance imaging, computed tomography, fluoroscopy and ultrasound imaging have enabled researchers and practitioners to examine organs, living or dead, in unprecedented detail. They are used for diagnostic and therapeutic purposes and provide information on the internal structures and organs of the body to a degree far beyond the imagination of earlier generations.
Bibliography.
"Main article:" Bibliography of anatomy

</doc>
<doc id="713" url="http://en.wikipedia.org/wiki?curid=713" title="Android (robot)">
Android (robot)

 
An android is a robot or synthetic organism designed to look and act like a human, especially one with a body having a flesh-like resemblance. Until recently, androids have largely remained within the domain of science fiction, frequently seen in film and television. However, advancements in robot technology have allowed the design of functional and realistic humanoid robots.
Etymology.
The word was coined from the Greek root ἀνδρ- 'man' (male, as opposed to anthrop- = human being) and the suffix "" 'having the form or likeness of'.
The "Oxford English Dictionary" traces the earliest use (as "Androides") to Ephraim Chambers' "Cyclopaedia," in reference to an automaton that St. Albertus Magnus allegedly created. The term "android" appears in US patents as early as 1863 in reference to miniature human-like toy automatons. The term "android" was used in a more modern sense by the French author Auguste Villiers de l'Isle-Adam in his work "Tomorrow's Eve" (1886). This story features an artificial humanlike robot named Hadaly. As said by the officer in the story, "In this age of Realien advancement, who knows what goes on in the mind of those responsible for these mechanical dolls." The term made an impact into English pulp science fiction starting from Jack Williamson's "The Cometeers" (1936) and the distinction between mechanical robots and fleshy androids was popularized by Edmond Hamilton's Captain Future (1940–1944).
Although Karel Čapek's robots in "R.U.R. (Rossum's Universal Robots)" (1921)—the play that introduced the word "robot" to the world—were organic artificial humans, the word "robot" has come to primarily refer to mechanical humans, animals, and other beings. The term "android" can mean either one of these, while a cyborg ("cybernetic organism" or "bionic man") would be a creature that is a combination of organic and mechanical parts.
The term "droid", popularized by George Lucas in the original ' film and now used widely within science fiction, originated as an abridgment of "android", but has been used by Lucas and others to mean any robot, including distinctly non-human form machines like R2-D2. The word "android" was used in ' episode "What Are Little Girls Made Of?" The abbreviation "andy", coined as a pejorative by writer Philip K. Dick in his novel "Do Androids Dream of Electric Sheep?", has seen some further usage, such as within the TV series "Total Recall 2070".
Authors have used the term "android" in more diverse ways than "robot" or "cyborg". In some fictional works, the difference between a robot and android is only their appearance, with androids being made to look like humans on the outside but with robot-like internal mechanics. In other stories, authors have used the word "android" to mean a wholly organic, yet artificial, creation. Other fictional depictions of androids fall somewhere in between.
Eric G. Wilson, who defines androids as a "synthetic human being", distinguishes between three types of androids, based on their body's composition:
Although human morphology is not necessarily the ideal form for working robots, the fascination in developing robots that can mimic it can be found historically in the assimilation of two concepts: "simulacra" (devices that exhibit likeness) and "automata" (devices that have independence).
Projects.
Several projects aiming to create androids that look, and, to a certain degree, speak or act like a human being have been launched or are underway.
Japan.
The Intelligent Robotics Lab, directed by Hiroshi Ishiguro at Osaka University, and Kokoro Co., Ltd. have demonstrated the Actroid at Expo 2005 in Aichi Prefecture, Japan and released the Telenoid R1 in 2010. In 2006, Kokoro Co. developed a new "DER 2" android. The height of the human body part of DER2 is 165 cm. There are 47 mobile points. DER2 can not only change its expression but also move its hands and feet and twist its body. The "air servosystem" which Kokoro Co. developed originally is used for the actuator. As a result of having an actuator controlled precisely with air pressure via a servosystem, the movement is very fluid and there is very little noise. DER2 realized a slimmer body than that of the former version by using a smaller cylinder. Outwardly DER2 has a more beautiful proportion. Compared to the previous model, DER2 has thinner arms and a wider repertoire of expressions. Once programmed, it is able to choreograph its motions and gestures with its voice.
The Intelligent Mechatronics Lab, directed by Hiroshi Kobayashi at the Tokyo University of Science, has developed an android head called "Saya", which was exhibited at Robodex 2002 in Yokohama, Japan. There are several other initiatives around the world involving humanoid research and development at this time, which will hopefully introduce a broader spectrum of realized technology in the near future. Now Saya is "working" at the Science University of Tokyo as a guide.
The Waseda University (Japan) and NTT Docomo's manufacturers have succeeded in creating a shape-shifting robot "WD-2". It is capable of changing its face. At first, the creators decided the positions of the necessary points to express the outline, eyes, nose, and so on of a certain person. The robot expresses its face by moving all points to the decided positions, they say. The first version of the robot was first developed back in 2003. After that, a year later, they made a couple of major improvements to the design. The robot features an elastic mask made from the average head dummy. It uses a driving system with a 3DOF unit. The WD-2 robot can change its facial features by activating specific facial points on a mask, with each point possessing three degrees of freedom. This one has 17 facial points, for a total of 56 degrees of freedom. As for the materials they used, the WD-2's mask is fabricated with a highly elastic material called Septom, with bits of steel wool mixed in for added strength. Other technical features reveal a shaft driven behind the mask at the desired facial point, driven by a DC motor with a simple pulley and a slide screw. Apparently, the researchers can also modify the shape of the mask based on actual human faces. To "copy" a face, they need only a 3D scanner to determine the locations of an individual's 17 facial points. After that, they are then driven into position using a laptop and 56 motor control boards. In addition, the researchers also mention that the shifting robot can even display an individual's hair style and skin color if a photo of their face is projected onto the 3D Mask.
Korea.
KITECH researched and developed EveR-1, an android interpersonal communications model capable of emulating human emotional expression via facial "musculature" and capable of rudimentary conversation, having a vocabulary of around 400 words. She is 160 cm tall and weighs 50 kg, matching the average figure of a Korean woman in her twenties. EveR-1's name derives from the Biblical Eve, plus the letter "r" for "robot". EveR-1's advanced computing processing power enables speech recognition and vocal synthesis, at the same time processing lip synchronization and visual recognition by 90-degree micro-CCD cameras with face recognition technology. An independent microchip inside her artificial brain handles gesture expression, body coordination, and emotion expression. Her whole body is made of highly advanced synthetic jelly silicon and with 60 artificial joints in her face, neck, and lower body; she is able to demonstrate realistic facial expressions and sing while simultaneously dancing. In South Korea, the Ministry of Information and Communication has an ambitious plan to put a robot in every household by 2020. Several robot cities have been planned for the country: the first will be built in 2016 at a cost of 500 billion won, of which 50 billion is direct government investment. The new robot city will feature research and development centers for manufacturers and part suppliers, as well as exhibition halls and a stadium for robot competitions. The country's new Robotics Ethics Charter will establish ground rules and laws for human interaction with robots in the future, setting standards for robotics users and manufacturers, as well as guidelines on ethical standards to be programmed into robots to prevent human abuse of robots and vice versa.
United States.
Walt Disney and a staff of Imagineers created Great Moments with Mr. Lincoln that debuted at the 1964 New York World's Fair.
Hanson Robotics, Inc., of Texas and KAIST produced an android portrait of Albert Einstein, using Hanson's facial android technology mounted on KAIST's life-size walking bipedal robot body. This Einstein android, also called "Albert Hubo", thus represents the first full-body walking android in history (see video at ). Hanson Robotics, the FedEx Institute of Technology, and the University of Texas at Arlington also developed the android portrait of sci-fi author Philip K. Dick (creator of "Do Androids Dream of Electric Sheep?", the basis for the film "Blade Runner"), with full conversational capabilities that incorporated thousands of pages of the author's works. In 2005, the PKD android won a first place artificial intelligence award from AAAI.
United Kingdom.
In 2001, Steve Grand OBE, creator of the computer game "Creatures", created an android, or anthropoid; he named it Lucy. The intention was that she would have to learn everything, including how to use her mechanical vocal chords to speak. Her systems were made to be similar to a human's.
Use in fiction.
Androids are a staple of science fiction. Isaac Asimov pioneered the fictionalization of the science of robotics and artificial intelligence, notably in his 1950s series "I, Robot" and "Foundation and Empire". One thing common to most fictional androids is that the real-life technological challenges associated with creating thoroughly human-like robots—such as the creation of strong artificial intelligence—are assumed to have been solved. Fictional androids are often depicted as mentally and physically equal or superior to humans—moving, thinking and speaking as fluidly as them.
The tension between the nonhuman substance and the human appearance—or even human ambitions—of androids is the dramatic impetus behind most of their fictional depictions. Some android heroes seek, like Pinocchio, to become human, as in the films "Bicentennial Man", "Hollywood", "Enthiran" and "A.I. Artificial Intelligence", or Data in "". Others, as in the film "Westworld", rebel against abuse by careless humans. Android hunter Deckard in "Do Androids Dream of Electric Sheep?" and its film adaptation "Blade Runner" discovers that his targets are, in some ways, more human than he is. Android stories, therefore, are not essentially stories "about" androids; they are stories about the human condition and what it means to be human.
One aspect of writing about the meaning of humanity is to use discrimination against androids as a mechanism for exploring racism in society, as in "Blade Runner". Perhaps the clearest example of this is John Brunner's 1968 novel "Into the Slave Nebula", where the blue-skinned android slaves are explicitly shown to be fully human. More recently, the androids Bishop and Annalee Call in the films "Aliens" and "Alien Resurrection" are used as vehicles for exploring how humans deal with the presence of an "Other".
Female androids, or "gynoids", are often seen in science fiction, and can be viewed as a continuation of the long tradition of men attempting to create the stereotypical "perfect woman". Examples include the Greek myth of "Pygmalion" and the female robot Maria in Fritz Lang's "Metropolis". Some gynoids, like Pris in "Blade Runner", are designed as sex-objects, with the intent of "pleasing men's violent sexual desires," or as submissive, servile companions, such as in "The Stepford Wives". Fiction about gynoids has therefore been described as reinforcing "essentialist ideas of femininity", although others have suggested that the treatment of androids is a way of exploring racism and misogyny in society.

</doc>
<doc id="742" url="http://en.wikipedia.org/wiki?curid=742" title="Algorithms (journal)">
Algorithms (journal)

Algorithms is a peer-reviewed open access mathematics journal concerning design, analysis, and experiments on algorithms. It is published by MDPI, which has been accused of being a predatory open access publisher.
The journal was established in 2008. Its editor-in-chief is Kazuo Iwama (Kyoto University). 
Abstracting and indexing.
The journal is abstracted and indexed in Chemical Abstracts Service, Compendex, DBLP Computer Science Bibliography, Inspec, MathSciNet, Scopus, and Zentralblatt MATH.

</doc>
<doc id="771" url="http://en.wikipedia.org/wiki?curid=771" title="American Revolutionary War">
American Revolutionary War

 
The American Revolutionary War (1775–1783), the American War of Independence, or simply the Revolutionary War in the United States, was the armed conflict between the Kingdom of Great Britain and thirteen of its former North American colonies, which had declared themselves the independent United States of America. Early fighting took place primarily on the North American continent. In 1778 France, eager for revenge after its defeat in 1763, signed an alliance with the new nation. The conflict then escalated into a world war with Britain combating France, Spain, and the Netherlands. Contemporaneous fighting also broke out in India between the British East India Company and the French allied Kingdom of Mysore.
The war had its origins in the resistance of many Americans to taxes imposed by the British parliament, which they claimed were unconstitutional. Patriot protests escalated into boycotts and the destruction of a shipment of tea at the Boston Tea Party. The British government punished Massachusetts by closing the port of Boston and taking away self-government. The Patriots responded by setting up a shadow government that took control of the province outside of Boston. Twelve other colonies supported Massachusetts, formed a Continental Congress to coordinate their resistance, and set up committees and conventions which effectively seized power from the royal governments. In April 1775 fighting broke out between Massachusetts militia units and British regulars at Lexington and Concord. The Continental Congress appointed General George Washington to take charge of militia units besieging British forces in Boston, forcing them to evacuate the city in March 1776. Congress supervised the war, giving Washington command of the new Continental Army; he also coordinated state militia units.
In July 1776, the Continental Congress formally declared independence. The British were meanwhile mustering forces to suppress the revolt. Sir William Howe outmaneuvered and defeated Washington, capturing New York City and New Jersey. Washington was able to capture a Hessian detachment at Trenton and drive the British out of most of New Jersey. In 1777 Howe's army launched a campaign against the national capital at Philadelphia, failing to aid Burgoyne's separate invasion force from Canada. Burgoyne's army was trapped and surrendered after the Battles of Saratoga in October 1777. This American victory encouraged France to enter the war in 1778, followed by its ally Spain in 1779.
In 1778, having failed in the northern states, the British shifted strategy toward the southern colonies, where they planned to enlist many Loyalist regiments. British forces had initial success in bringing Georgia and South Carolina under control in 1779 and 1780, but the Loyalist surge was far weaker than expected. In 1781 British forces moved through Virginia, but their escape was blocked by a French naval victory. Washington took control of a Franco-American siege at Yorktown and captured the entire British force of over 7,000 men. The defeat at Yorktown finally turned the British Parliament against the war. The war at sea continued, and the British Navy scored key victories, especially the Battle of the Saintes in 1782. In 1783, the Treaty of Paris ended the war and recognized the sovereignty of the United States over the territory bounded roughly by what is now Canada to the north, Florida to the south, and the Mississippi River to the west. France gained its revenge and little else except a heavy national debt, while Spain picked up Britain's Florida colonies.
Causes.
Taxes.
The close of the Seven Years' War in 1763 (the French and Indian War in North America) saw Britain triumphant in driving the French from North America, but also heavily in debt. Taxes in Britain were already very high and it was thought that the American colonies should pay for the soldiers to be stationed there. Parliament passed the Stamp Act in March 1765, which imposed direct taxes on the colonies for the first time starting November 1. This met with strong condemnation among American spokesmen, who argued that their "Rights as Englishmen" meant that taxes could not be imposed on them because they lacked representation in Parliament. At the same time the colonists rejected the solution of being provided with the representation, claiming that "their local circumstances" made it impossible.
Civil resistance prevented the Act from being enforced, and organized boycotts of British goods were instituted. This resistance was by and large unexpected and "produced a violent and very natural irritation" amongst the British.
A change of government in Britain led to the repeal of the Stamp Act as inexpedient, but also the passage of the Declaratory Act, which stated, "the said colonies and plantations in America have been, are, and of right ought to be, subordinate unto, and dependent upon the imperial crown and parliament of Great Britain."
In their declarations Americans had deemed internal taxes like the Stamp Act as unlawful, but not external taxes like the long-standing custom duties. So in 1767 Parliament passed the Townshend Act, which imposed duties on various British goods exported to the colonies. The Americans quickly denounced this as illegal as well, since the intent of the act was to raise revenue and not regulate trade. For their part, the British could not understand why the Americans deemed Parliamentary trade taxes lawful but not revenue taxes.
In 1768 violence broke out in Boston and 4000 British troops were sent to occupy the city. Parliament threatened to try Massachusetts residents for treason in England. Far from being intimidated, the colonists formed new associations to boycott British goods, albeit with less effectiveness than previously since the Townshend imports were so widely used. In March 1770 five colonists in Boston were killed in the "Boston Massacre", sparking outrage.
In 1773, in an effort to rescue the East India Company from financial difficulties, the British attempted to increase its tea sales by exempting the Company from the tea tax and appointing certain merchants in America to receive and sell the untaxed tea. The landing of this tea was resisted in all the colonies and, when the royal governor of Massachusetts refused to send back the tea ships in Boston, Patriot mobs destroyed the tea chests.
Crisis.
Nobody was punished for the "Boston Tea Party" and in 1774 Parliament ordered Boston harbor closed until the destroyed tea was paid for. It then passed the Massachusetts Government Act to punish the rebellious colony. The upper house of the Massachusetts legislature would be appointed by the Crown, as was already the case in other colonies such as New York and Virginia. The royal governor was able to appoint and remove at will all judges, sheriffs, and other executive officials, and restrict town meetings. Jurors would be selected by the sheriffs and British soldiers would be tried outside the colony for alleged offenses. These were collectively dubbed the "Intolerable Acts" by the Patriots.
Although these actions were not unprecedented (the Massachusetts charter had already been replaced once before in 1691), the people of the colony were outraged. Town meetings resulted in the Suffolk Resolves, a declaration not to cooperate with the royal authorities. In October 1774 an illegal "provincial congress" was established which took over the governance of Massachusetts outside of British-occupied Boston and began training militia for hostilities.
Meanwhile, in September 1774 representatives of the other colonies convened the First Continental Congress in order to respond to the crisis. The Congress rejected a "Plan of Union" to establish an American parliament that could approve or disapprove of the acts of the British parliament. Instead, they endorsed the Suffolk Resolves and demanded the repeal of all Parliamentary acts passed since 1763, not merely the tax on tea and the "Intolerable Acts". They stated that Parliament had no authority over internal matters in America, but that they would "cheerfully consent" to trade regulations, including customs duties for the benefit of the empire. They also required Britain to acknowledge that unilaterally stationing troops in the colonies in a time of peace was "against the law". Although the Congress lacked any legal authority, it ordered the creation of Patriot committees who would enforce a boycott of British goods starting on December 1, 1774.
This time, however, the British would not yield. Edmund Burke introduced a motion to repeal all the Acts of Parliament the Americans objected to and waive any rights of Britain to tax for revenue, but it was defeated 210-105. Parliament voted to restrict all colonial trade to Britain, prevent them from using the Newfoundland fisheries, and to increase the size of the army and navy by 6,000. In February 1775 Prime Minister Lord North proposed not to impose taxes if the colonies themselves made "fixed contributions". This would safeguard the taxing rights of the colonies from future infringement while enabling them to contribute to maintenance of the empire. This proposal was nevertheless rejected by the Congress in July as an "insidious maneuver", by which time hostilities had broken out.
Internal British politics.
During this time the British did not present a united front toward the American Patriots. The Parliament of Great Britain at this time was informally divided between conservative (Tory) and liberal (Whig) factions. The Whigs generally favored lenient treatment of the colonists short of independence while the Tories staunchly upheld the rights of Parliament. The Whigs felt that the Tory policies were pushing Americans to rebel, while the Tories thought Whig leniency (such as repealing the Stamp Act) was doing the same. Many Whigs freely associated themselves with the American Patriot cause, which Tories thought were encouraging the Americans in their resistance. The result was that, although Lord North's Tory government usually had a Parliamentary majority, a large Whig minority opposed it and constantly criticized its policies. Meanwhile, Whig commanders in America such as Sir William Howe and his brother Admiral Howe came under the suspicion of Tories and Loyalists for not vigorously prosecuting the war effort.
First phase, 1775–1778.
Outbreak of the War 1775–76.
Massachusetts.
In February 1775 Parliament declared Massachusetts to be in a state of rebellion. Lieutenant General Thomas Gage, the British North American commander-in chief, commanded four regiments of British regulars (about 4,000 men) from his headquarters in Boston, but the countryside was in the hands of the Revolutionaries. On April 14, he received orders to disarm the rebels and arrest their leaders.
On the night of April 18, 1775, General Gage sent 700 men to seize munitions stored by the colonial militia at Concord, Massachusetts. Riders including Paul Revere alerted the countryside, and when British troops entered Lexington on the morning of April 19, they found 77 Minutemen formed up on the village green. Shots were exchanged, killing several Minutemen. The British moved on to Concord, where a detachment of three companies was engaged and routed at the North Bridge by a force of 500 minutemen. As the British retreated back to Boston, thousands of militiamen attacked them along the roads, inflicting many casualties before timely British reinforcements prevented a total disaster. With the Battles of Lexington and Concord, the war had begun.
The militia converged on Boston, bottling up the British in the city. About 4,500 more British soldiers arrived by sea, and on June 17, 1775, British forces under General William Howe seized the Charlestown peninsula at the Battle of Bunker Hill. Instead of landing behind the Americans, a move that would not only have easily won the battle but also expose the rest of the rebel army to destruction, the British mounted a costly frontal attack. The Americans fell back, but British losses totaled over 1,000 men. The siege was not broken, and Gage was soon replaced by Howe as the British commander-in-chief. General Gage wrote to the Secretary at War in London:
In July 1775, newly appointed General Washington arrived outside Boston to take charge of the colonial forces and to organize the Continental Army. Realizing his army's desperate shortage of gunpowder, Washington asked for new sources. Arsenals were raided and some manufacturing was attempted; 90% of the supply (2 million pounds) was imported by the end of 1776, mostly from France. Patriots in New Hampshire had seized powder, muskets and cannons from Fort William and Mary in Portsmouth Harbor in late 1774. Some of the munitions were used in the Boston campaign.
The standoff continued throughout the fall and winter. During this time Washington was astounded by the failure of Howe to attack his shrinking, poorly armed force. In early March 1776, heavy cannons that the patriots had captured at Fort Ticonderoga were brought to Boston by Colonel Henry Knox, and placed on Dorchester Heights. Since the artillery now overlooked the British positions, Howe's situation was untenable, and the British fled on March 17, 1776, sailing to their naval base at Halifax, Nova Scotia, an event now celebrated in Massachusetts as Evacuation Day. Washington then moved most of the Continental Army to fortify New York City.
Quebec.
Three weeks after the siege of Boston began, the Green Mountain Boys, a group of militia volunteers led by Ethan Allen and Benedict Arnold captured Fort Ticonderoga, a strategically important point on Lake Champlain between New York and the Province of Quebec. After that action they also raided Fort St. John's, not far from Montreal, which alarmed the population and the authorities there. In response, Quebec's governor Guy Carleton began fortifying St. John's, and opened negotiations with the Iroquois and other Native American tribes for their support. These actions, combined with lobbying by both Allen and Arnold and the fear of a British attack from the north, eventually persuaded the Congress to authorize an invasion of Quebec, with the goal of driving the British military from that province. (Quebec was then frequently referred to as "Canada", as most of its territory included the former French Province of Canada.)
Two Quebec-bound expeditions were undertaken. On September 28, 1775, Brigadier General Richard Montgomery marched north from Fort Ticonderoga with about 1,700 militiamen, besieging and capturing Fort St. Jean on November 2 and then Montreal on November 13. General Carleton escaped to Quebec City and began preparing that city for an attack. The second expedition, led by Colonel Arnold, went through the wilderness of what is now northern Maine. Logistics were difficult, with 300 men turning back, and another 200 perishing due to the harsh conditions. By the time Arnold reached Quebec City in early November, he had but 600 of his original 1,100 men. Montgomery's force joined Arnold's, and they attacked Quebec City on December 31, but were defeated by Carleton in a battle that ended with Montgomery dead, Arnold wounded, and over 400 Americans taken prisoner. The remaining Americans held on outside Quebec City until the spring of 1776, suffering from poor camp conditions and smallpox, and then withdrew when a squadron of British ships under Captain Charles Douglas arrived to relieve the siege.
Another attempt was made by the Americans to push back towards Quebec, but they failed at Trois-Rivières on June 8, 1776. Carleton then launched his own invasion and defeated Arnold at the Battle of Valcour Island in October. Arnold fell back to Fort Ticonderoga, where the invasion had begun. While the invasion ended as a disaster for the Americans, Arnold's efforts in 1776 delayed any full-scale British counteroffensive until the Saratoga campaign of 1777.
The invasion cost the Americans their base of support in British public opinion, "So that the violent measures towards America are freely adopted and countenanced by a majority of individuals of all ranks, professions, or occupations, in this country." It gained them at best limited support in the population of Quebec, which, while somewhat supportive early in the invasion, became less so later during the occupation, when American policies against suspected Loyalists became harsher, and the army's hard currency ran out. Two small regiments of Canadiens were recruited during the operation, and they were with the army on its retreat back to Ticonderoga. Even after their retreat, the Patriots continued to view Quebec as a part of their cause and made specific provisions for it to join the U.S. under the 1777 Articles of Confederation.
Expelling the royal officials.
At the onset of war, the British had a significant force only in Boston, though this force would evacuate by the signing of the declaration. Patriots in all 13 colonies were quick to establish new revolutionary governments based around various committees and conventions that they had created in 1774 and early 1775. Royal governors and officials found themselves powerless to stop the rebellion and in many places were forced to flee. In many places the Patriots were energetic and were backed by angry mobs while the Loyalists were too intimidated or poorly organized to be effective without the British army. The term "lynching" originated when Virginia Patriots held informal courts and arrested Loyalists (the term did not suggest execution).
Loyalist Writings.
Loyalist writings throughout the conflict persistently claimed that they were the majority, and influenced London officials to believe that it would be possible to raise many Loyalist regiments. As late as 1780 the Loyalists were deceiving themselves and top London officials about their supposedly strong base of support.
Patriots overwhelmed Loyalists in the Snow Campaign in South Carolina in late 1775. Virginia's governor Lord Dunmore attempted to rally a loyalist force but was decisively beaten in December 1775 at the Battle of Great Bridge. In February 1776 British General Clinton took 2,000 men and a naval squadron to assist Loyalists mustering in North Carolina, only to call it off when he learned they had been crushed at the Battle of Moore's Creek Bridge. In June he tried to seize Charleston, South Carolina, the leading port in the South, but the attack failed as the naval force was repulsed by the Patriot forts.
Apart from thirteen, no other British North American colony joined the rebellion.
British reaction.
King George III issued a Proclamation of Rebellion in August 1775, and addressed Parliament on October 26, 1775. He denounced "the authors and promoters of this desperate conspiracy" who had "labored to inflame my people in America ... and to infuse into their minds a system of opinions repugnant to the true constitution of the Colonies, and to their subordinate relation to Great Britain ..." He detailed measures taken to suppress the revolt, including "friendly offers of foreign assistance". The King's speech was endorsed by both Houses of Parliament, a motion in the House of Commons to oppose coercive measures was defeated 278-108. The British received an Olive Branch Petition written by the Second Continental Congress dated July 8, 1775, imploring the King to reverse the policies of his ministers. The Parliament debated on whether to accept the petition, but after a lengthy debate rejected it by 53 votes, viewing it as insincere. Parliament then voted to impose a blockade against the Thirteen Colonies. The popularity of war in Britain reached a peak in 1777. The king himself took full control as he micromanaged the war effort, despite the opposition of top officials including the prime minister North and the civilian heads of the army and the navy. The king vehemently rejected independence and demanded the use of Indians to distress the Americans.
Separately, the Irish Parliament pledged its loyalty and agreed to the withdrawal of troops from Ireland to suppress the rebellion in America. Most Irish Protestants were against the war and favoured the Americans, but the Catholic establishment supported the king. The American Revolution was the first war in which Irish Catholics were allowed to enlist in the army.
Militarily, the response to the rebellion had been feeble, greatly encouraging the American movement for independence. The peacetime British army had been deliberately kept small since the Glorious Revolution to prevent an abuse of power by the King. To muster a force the British had to launch recruiting campaigns in Britain and Ireland and hire mercenaries from the German states, both immensely time-consuming. In the end, though, the British were able to ship Sir William Howe an army of 32,000 officers and men to open a campaign in late 1776. It was the largest force the British had ever sent outside of Europe at that time.
Campaign of 1776–77.
New York.
Having withdrawn his army from Boston, General Howe now focused on capturing New York City, which then was limited to the southern tip of Manhattan Island. Howe's force arrived off of Staten Island on June 30, 1776, and his army captured it without resistance. To defend the city, General Washington spread his forces along the shores of New York's harbor, concentrated on Long Island and Manhattan. While British and recently hired Hessian troops were assembling, Washington had the newly issued Declaration of American Independence read to his men and the citizens of the city.
Washington's position was extremely dangerous because he had divided his forces between Manhattan and Long Island, exposing both to defeat in detail. The British landed 22,000 men on Long Island in late August and badly defeated the rebel army in the war's largest battle, taking over 1,000 prisoners and driving them back to Brooklyn Heights. Howe then laid siege to the heights, claiming he wanted to spare his men's lives from an immediate assault, although by his own admission such an assault would have succeeded. He had to actively restrain his subordinates from landing the finishing blow. Washington initially reinforced his exposed position, but then personally directed the withdrawal of his entire remaining army and all their supplies across the East River on the night of August 29–30 without significant loss of men and materiel. Howe had failed to conduct adequate scouting to detect the retreat.
A peace conference took place on September 11 to explore the possibility of a negotiated solution. The British advanced Lord North's "fixed contribution" formula of the preceding year and indicated that other laws could be revised or repealed so long as the authority of Britain was acknowledged. But the Patriot side still insisted that full independence was required.
Howe then resumed the attack. On September 15, Howe landed about 12,000 men on lower Manhattan, quickly taking control of New York City. The Americans withdrew north up the island to Harlem Heights, where they skirmished the next day but held their ground. On September 21 a devastating fire broke out in the city which the rebels were widely blamed for, although no conclusive proof exists. On October 12 the British made an attempt to encircle the Americans, which failed because of Howe's decision to land on an island that was easily cut off from the mainland. The Americans evacuated Manhattan, and on October 28 fought the Battle of White Plains against the pursuing British. During the battle Howe declined to attack Washington's highly vulnerable main force, instead attacking a hill that was of no strategic significance.
Washington retreated, and Howe returned to Manhattan and captured Fort Washington in mid November, taking about 3,000 prisoners. Thus began the infamous "prison ships" system the British maintained in New York for the rest of the war, in which more American soldiers and sailors died of neglect and disease than died in every battle of the entire war, combined.
Howe then detached General Clinton with 6,000 men to seize Newport, Rhode Island for the British fleet, which was accomplished without encountering any major resistance. Clinton objected to this move, believing the force would have been better employed up the Delaware River, where they might have inflicted irreparable damage on the retreating Americans.
New Jersey.
General Lord Cornwallis continued to chase Washington's army through New Jersey, but Howe ordered him to halt and Washington escaped across the Delaware River into Pennsylvania on December 7. Howe refused to order a pursuit across the river, even though the outlook of the Continental Army was bleak. "These are the times that try men's souls," wrote Thomas Paine, who was with the army on the retreat. The army had dwindled to fewer than 5,000 men fit for duty, and would be reduced to 1,400 after enlistments expired at the end of the year. Congress moved inland and abandoned Philadelphia in despair, although popular resistance to British occupation was growing in the countryside.
Howe proceeded to divide his forces in New Jersey into small detachments that were vulnerable to defeat in detail, with the weakest forces stationed the closest to Washington's army. Washington decided to take the offensive, stealthily crossing the Delaware on the night of December 25–26, and capturing nearly 1,000 surprised and unfortified Hessians at the Battle of Trenton. Cornwallis marched to retake Trenton but was first repulsed and then outmaneuvered by Washington, who successfully attacked the British rearguard at Princeton on January 3, 1777, taking around 200 prisoners. Howe then conceded most of New Jersey to Washington, in spite of Howe's massive numerical superiority over him. Washington entered winter quarters at Morristown, New Jersey, having given a morale boost to the American cause. Throughout the winter New Jersey militia continued to harass British and Hessian forces near their three remaining posts along the Raritan River. In April 1777 Washington was amazed that Howe made no effort to attack his weak army.
Campaigns of 1777–78.
When the British began to plan operations for 1777, they had two main armies in North America: an army in Quebec (later under the command of John Burgoyne), and Howe's army in New York. In London, Lord George Germain approved a campaign for these armies to converge on Albany, New York and divide the American colonies in two, but did not give any express orders to Howe, who was developing his own plans. In November 1776 Howe requested large reinforcements so he could launch attacks against Philadelphia, New England, and Albany. These reinforcements were not granted so Howe modified his plan to launch an attack against Philadelphia only. Germain gave his approval to this, believing that Philadelphia could be taken in time for Howe to coordinate with the northern army. Howe, on the other hand, opted to send his army to Philadelphia by sea via the Chesapeake Bay instead of taking shorter routes either overland through New Jersey or through the Delaware Bay. This left him completely incapable of assisting Burgoyne.
Upstate New York.
The first of the 1777 campaigns was an expedition from Quebec led by General John Burgoyne. The goal was to seize the Lake Champlain and Hudson River corridor, effectively isolating New England from the rest of the American colonies. Burgoyne's invasion had two components: he would lead about 8,000 men along Lake Champlain towards Albany, New York, while a second column of about 2,000 men, led by Barry St. Leger, would move down the Mohawk River Valley and link up with Burgoyne in Albany.
Burgoyne set off in June, and recaptured Fort Ticonderoga in early July. Thereafter, his march was slowed by the Americans who knocked down trees in his path, and by his army's extensive baggage train. A detachment sent out to seize supplies was decisively defeated in the Battle of Bennington by American militia in August, depriving Burgoyne of nearly 1,000 men.
Meanwhile, St. Leger—more than half of his force Native Americans led by Sayenqueraghta—had laid siege to Fort Stanwix. American militiamen and their Native American allies marched to relieve the siege but were ambushed and scattered at the Battle of Oriskany. When a second relief expedition approached, this time led by Benedict Arnold, St. Leger's Indian support abandoned him, forcing him to break off the siege and return to Quebec.
Burgoyne's army had been reduced to about 6,000 men by the loss at Bennington and the need to garrison Ticonderoga, and he was running short on supplies. Despite these setbacks, he determined to push on towards Albany. An American army of 8,000 men, officially commanded by General Horatio Gates (but effectively being led by his subordinate Benedict Arnold), had entrenched about 10 miles (16 km) south of Saratoga, New York. Burgoyne tried to outflank the Americans but was checked at the first battle of Saratoga in September. Burgoyne's situation was desperate, but he now hoped that help from Howe's army in New York City might be on the way. It was not: Howe had instead sailed away on his expedition to capture Philadelphia. American militiamen flocked to Gates' army, swelling his force to 11,000 by the beginning of October. After being badly beaten at the second battle of Saratoga, Burgoyne surrendered on October 17.
British General Clinton in New York City attempted a diversion in favor of Burgoyne in early October, capturing two key forts but withdrawing after hearing of the surrender.
Saratoga was the turning point of the war. Revolutionary confidence and determination, suffering from Howe's successful occupation of Philadelphia, was renewed. What is more important, the victory encouraged France to make an open alliance with the Americans, after two years of semi-secret support. For the British, the war had now become much more complicated.
The Americans held the British prisoners taken at Saratoga until the end of the war, in direct violation of the agreed surrender terms, which specified they would be repatriated immediately.
Pennsylvania.
Howe began his campaign in June by making a series of maneuvers in New Jersey, which failed to engage Washington's greatly inferior force. He then loaded his troops onto transports and slowly sailed to the northern end of the Chesapeake Bay, landing 15,000 troops on August 25 at the head of the Elk River. Washington positioned his 11,000 men in a strong position along the Brandywine River, between the British and Philadelphia, but Howe outflanked and defeated him on September 11, 1777. French observers noted that Howe failed to follow up on his victory, which could have destroyed Washington's army.
The Continental Congress again abandoned Philadelphia, and on September 26, Howe finally outmaneuvered Washington and marched into the city unopposed. A part of Howe's army was then split off to reduce rebel forts blocking his communications up the Delaware River. Hoping to bring about another Trenton-like victory while the British were divided, on October 4 Washington mounted a surprise assault against the British at Germantown. Howe had failed to alert his troops there, despite being aware of the impending attack the previous day. The British were in danger of a rout, but faulty American decisions resulted in Washington being repulsed with heavy losses.
The armies met at White Marsh in December, where after some skirmishing Howe decided to retire, ignoring the vulnerability of Washington's rear, where an attack could have cut off Washington from his baggage and provisions. Washington and his army encamped at Valley Forge in December 1777, about 20 miles (32 km) from Philadelphia, where they stayed for the next six months. Over the winter, 2,500 men (out of 10,000) died from disease and exposure and the army was reduced to 4,000 effectives. During this time Howe's army, comfortable in Philadelphia, made no effort to exploit the weakness of the American army. The next spring the army emerged from Valley Forge in good order, thanks in part to a training program supervised by Baron von Steuben, who introduced the most modern Prussian methods of organization and tactics.
Historians speculate that the British "forfeited several chances for military victory in 1776–1777 ..." and "if General Howe had violated military tradition by advancing in December on the Continental troops quartered [at Valley Forge], he might have readily overwhelmed them and possibly ended the war."
Howe submitted his resignation in October 1777; until it was accepted he spent his time in Philadelphia preparing his arguments for an expected parliamentary inquiry. Although he had twice as many men as Washington, the bitter memory of Bunker Hill made him highly reluctant to attack entrenched American forces. General Clinton replaced Howe as British commander-in-chief on May 24, 1778.
Foreign intervention.
From 1776, France had informally been involved in the American Revolutionary War, with French admiral Latouche Tréville having provided supplies, ammunition and guns from France to the United States after Thomas Jefferson encouraged a French alliance. Guns such as de Valliere type were used, playing an important role in such battles as the Battle of Saratoga. After learning of the American victory at Saratoga, the French became concerned that the British would reconcile their differences with the colonists and turn on France. In particular, King Louis XVI was influenced by alarmist reports suggesting that Britain was preparing to make huge concessions to the colonies and then, allied with them, strike at French and Spanish possessions in the West Indies. To thwart this, they concluded a Treaty of Alliance with the United States on February 6, 1778, committing the Americans to seek nothing less than absolute independence. Previously France had only been willing to act in conjunction with their Spanish ally but now they were willing to go to war alone if necessary. Britain responded by recalling its ambassador, although Franco-British hostilities did not actually break out until June 17, 1778.
In 1776, the Count of Aranda met in representation of Spain with the first U.S. Commission composed by Benjamin Franklin, Silas Deane and Arthur Lee. The Continental Congress had charged the commissioners to travel to Europe and forge alliances with other European powers that could help break the British naval blockade along the North American coast. Aranda invited the commission to his house in Paris, where he was acting as Spanish ambassador and he became an active supporter of the struggle of the fledgling Colonies, recommending an early and open Spanish commitment to the Colonies. However, he was overruled by José Moñino, 1st Count of Floridablanca who opted for a more discreet approach. The Spanish position was later summarized by the Spanish Ambassador to the French Court, Jerónimo Grimaldi, in a letter to Arthur Lee who was in Madrid trying to persuade the Spanish government to declare an open alliance. Grimaldi told Lee that "You have considered your own situation, and not ours. The moment is not yet come for us. The war with Portugal — France being unprepared, and our treasure ships from South America not being arrived — makes it improper for us to declare immediately." Meanwhile, Grimaldi reassured Lee, stores of clothing and powder were deposited at New Orleans and Havana for the Americans, and further shipments of blankets were being collected at Bilbao.
Spain finally entered officially the war in June 1779, thus implementing the Treaty of Aranjuez, although the Spanish government had been providing assistance to the revolutionaries since the very beginning of the war. So too had the Dutch Republic, which was formally brought into the war at the end of 1780.
Second phase, 1778–1781.
British policies.
Following news of the surrender at Saratoga and concern over French intervention, the British decided to completely accept the original demands made by the American Patriots. Parliament repealed the remaining tax on tea and declared that no taxes would ever be imposed on colonies without their consent (except for custom duties, the revenues of which would be returned to the colonies). A Commission was formed to negotiate directly with the Continental Congress for the first time. The Commission was empowered to suspend all the other objectionable acts by Parliament passed since 1763, issue general pardons, and declare a cessation of hostilities. The Commissioners arrived in America in June 1778 and offered to place the colonies in the condition of 1763 if they would return to the allegiance of the King. Moreover, they agreed that no troops would be placed in the colonies without their consent. The Congress refused to negotiate with the commission unless they first acknowledged American independence or withdrew all troops. On October 3, 1778, the British published a proclamation offering amnesty to any colonies or individuals who accepted their proposals within forty days, implying serious consequences if they still refused. There was no positive reply.
King George III gave up all hope of subduing America by more armies, while Britain had a European war to fight. "It was a joke," he said, "to think of keeping Pennsylvania." There was no hope of recovering New England. But the King was still determined "never to acknowledge the independence of the Americans, and to punish their contumacy by the indefinite prolongation of a war which promised to be eternal". His plan was to keep the 30,000 men garrisoned in New York, Rhode Island, Quebec, and Florida; other forces would attack the French and Spanish in the West Indies. To punish the Americans the King planned to destroy their coasting-trade, bombard their ports; sack and burn towns along the coast and turn loose the Native Americans to attack civilians in frontier settlements. These operations, the King felt, would inspire the Loyalists; would splinter the Congress; and "would keep the rebels harassed, anxious, and poor, until the day when, by a natural and inevitable process, discontent and disappointment were converted into penitence and remorse" and they would beg to return to his authority. The plan meant destruction for the Loyalists and loyal Native Americans, an indefinite prolongation of a costly war, and the risk of disaster as the French and Spanish assembled an armada to invade the British Isles. The King hoped to re-subjugate the rebellious colonies after dealing with the Americans' European allies.
Northern theater after Saratoga, 1778–81.
French entry into the war had changed British strategy, and Clinton abandoned Philadelphia to reinforce New York City, now vulnerable to French naval power. Washington shadowed Clinton on his withdrawal through New Jersey and attacked him at Monmouth on June 28, 1778. The battle was tactically inconclusive but Clinton successfully disengaged and continued his retreat to New York. It was the last major battle in the north. Clinton's army went to New York City in July, arriving just before a French fleet under Admiral d'Estaing arrived off the American coast. Washington's army returned to White Plains, New York, north of New York City. Although both armies were back where they had been two years earlier, the nature of the war had now changed as the British had to withdraw troops from North America to counter the French threats elsewhere.
In August 1778 the Americans attempted to capture British-held Newport, Rhode Island with the assistance of France, but the effort failed when the French withdrew their support. The war in the north then bogged down into a stalemate, with neither side capable of attacking the other in any decisive manner. The British instead attempted to wear out American resolve by launching various raiding expeditions such as Tryon's raid against Connecticut in July 1779. In that year the Americans won two morale-enhancing victories by capturing posts at Stony Point and Paulus Hook, although the British quickly retook them. In October 1779 the British voluntarily abandoned Newport and Stony Point in order to consolidate their forces.
During the winter of 1779–80 the American army suffered worse hardships than they had at Valley Forge previously. The Congress was ineffective, the Continental currency worthless, and the supply system was fundamentally broken. Washington was finding it extremely difficult to keep his army together, even without any major fighting against the British. In 1780 actual mutinies broke out in the American camp. The Continental Army's strength dwindled to such an extent that the British decided to mount two probing attacks against New Jersey in June 1780. The New Jersey militia strongly rallied, however, and the British quickly returned to their bases.
In July 1780 the American cause received a boost when a 5,500 strong French expeditionary force arrived at Newport, Rhode Island. Washington hoped to use this assistance to attack the British at New York and end the war. Events elsewhere, however, would frustrate this. Additional French reinforcements were prevented from arriving by a British blockade of French ports, and the French troops at Newport quickly found themselves blockaded as well. Moreover, the French fleet refused to visit the American coast in 1780, having suffered significant damage in actions in the West Indies.
Benedict Arnold, the American victor of Saratoga, grew increasingly disenchanted with struggle and decided to defect. In September 1780 he attempted to surrender the key American fort at West Point along the Hudson River to the British, but his plot was exposed. He escaped and continued to fight under the British army. He wrote an open letter justifying his actions by claiming he had only fought for a redress of grievances and since Britain had withdrawn those grievances (see above) there was no reason to continue shedding blood, particularly in an alliance with an ancient and tyrannical enemy like France. He led the last British attack in the north, a devastating raid against New London in September 1781.
The British held Staten Island, Manhattan, and Long Island until peace was made in 1783. These areas contained about 2% of the population of the Thirteen Colonies.
Northern and Western frontier.
West of the Appalachian Mountains and along the border with Quebec, the American Revolutionary War was an "Indian War". Most Native Americans supported the British. Like the Iroquois Confederacy, tribes such as the Shawnee split into factions, and the Chickamauga split off from the rest of the Cherokee over differences regarding peace with the Americans. The British supplied their native allies with muskets, gunpowder and advice, while Loyalists led raids against civilian settlements, especially in New York, Kentucky, and Pennsylvania. Joint Iroquois-Loyalist attacks in the Wyoming Valley in Pennsylvania and at Cherry Valley in New York in 1778 provoked Washington to send the Sullivan Expedition into western New York during the summer of 1779. There was little fighting as Sullivan systematically destroyed the Indians' winter food supplies, forcing them to flee permanently to British bases in Quebec and the Niagara Falls area.
In the Ohio Country and the Illinois Country, the Virginia frontiersman George Rogers Clark attempted to neutralize British influence among the Ohio tribes by capturing the outposts of Kaskaskia and Cahokia and Vincennes in the summer of 1778, at which he succeeded. When General Henry Hamilton, the British commander at Detroit, retook Vincennes, Clark returned in a surprise march in February 1779 and captured Hamilton.
In March 1782, Pennsylvania militiamen killed about a hundred neutral Native Americans in the Gnadenhütten massacre. In the last major encounters of the war, a force of 200 Kentucky militia was defeated at the Battle of Blue Licks in August 1782.
Georgia and the Carolinas, 1778–81.
During the first three years of the American Revolutionary War, the primary military encounters were in the north, although some attempts to organize Loyalists were defeated, a British attempt at Charleston, South Carolina failed, and a variety of efforts to attack British forces in East Florida failed. After French entry into the war, the British turned their attention to the southern colonies, where they hoped to regain control by recruiting large numbers of Loyalists. This southern strategy also had the advantage of keeping the Royal Navy closer to the Caribbean, where the British needed to defend economically important possessions against the French and Spanish.
On December 29, 1778, an expeditionary corps from Clinton's army in New York captured Savannah, Georgia. An attempt by French and American forces to retake Savannah failed on October 9, 1779. Clinton then besieged Charleston, capturing it and most of the southern Continental Army on May 12, 1780. With relatively few casualties, Clinton had seized the South's biggest city and seaport, providing a base for further conquest.
The remnants of the southern Continental Army began to withdraw to North Carolina but were pursued by Lt. Colonel Banastre Tarleton, who defeated them at the Waxhaws on May 29, 1780. With these events, organized American military activity in the region collapsed, though the war was carried on by partisans such as Francis Marion. Cornwallis took over British operations, while Horatio Gates arrived to command the American effort. On August 16, 1780, Gates was defeated at the Battle of Camden in South Carolina, setting the stage for Cornwallis to invade North Carolina. Georgia and South Carolina were thus both restored to Britain for the time being.
Cornwallis' efforts to advance into North Carolina were frustrated. A Loyalist wing of his army was utterly defeated at the Battle of Kings Mountain on October 7, 1780, which temporarily aborted his planned advance. He received reinforcements, but his light infantry under Tarleton was decisively defeated by Daniel Morgan at the Battle of Cowpens on January 17, 1781. In spite of this, Cornwallis decided to proceed, gambling that he would receive substantial Loyalist support. General Nathanael Greene, who replaced General Gates, evaded contact with Cornwallis while seeking reinforcements. By March, Greene's army had grown to the point where he felt that he could face Cornwallis directly. In the key Battle of Guilford Court House, Cornwallis drove Greene's much larger army off the battlefield, but in doing so suffered casualties amounting to one-fourth of his army. Compounding this, far fewer Loyalists were joining up than expected because the Patriots put heavy pressure on them and their families, who would become hostages. Cornwallis decided to retreat to coastal Wilmington, North Carolina for resupply and reinforcement, leaving the interior of the Carolinas and Georgia open to Greene. He then proceeded north into Virginia (see below).
American troops in conjunction with Patriot partisans then began the process of reclaiming territory in South Carolina and Georgia. Despite British victories at Hobkirk's Hill and at the Siege of Ninety-Six, by the middle of the year they had been forced to withdraw to the coastal lowlands region of both colonies. The final battle (Battle of Eutaw Springs) in September 1781 was indecisive but by the end of the year the British held only Savannah and Charleston.
Virginia, 1781.
Cornwallis proceeded from Wilmington north into Virginia, on the grounds that Virginia needed to be subdued in order to hold the southern colonies. Earlier, in January 1781, a small British raiding force under Benedict Arnold had landed there, and began moving through the countryside, destroying supply depots, mills, and other economic targets. In February, General Washington dispatched General Lafayette to counter Arnold, later also sending General Anthony Wayne. Arnold was reinforced with additional troops from New York in March, and his army was joined with that of Cornwallis in May. Lafayette skirmished with Cornwallis, avoiding a large-scale battle while gathering reinforcements.
Cornwallis' Virginia campaign was strongly opposed by his superior, General Clinton, who did not believe such a large and disease-ridden area, with a hostile population, could be pacified with the limited forces available. Clinton instead favored conducting operations further north in the Chesapeake region (Maryland, Delaware, and southern Pennsylvania) where he believed there was a strong Loyalist presence. Upon his arrival at Williamsburg in June, Cornwallis received orders from Clinton to establish a fortified naval base and a request to send several thousand troops to New York to counter a possible Franco-American attack. Following these orders, he fortified Yorktown, and, shadowed by Lafayette, awaited the arrival of the Royal Navy.
The northern, southern, and naval theaters of the war converged in 1781 at Yorktown, Virginia. The French fleet became available for operations, which could either move against Yorktown or New York. Washington still favored attacking New York, but the French decided to send the fleet to their preferred target at Yorktown. Learning of the planned movement of the French fleet in August, Washington began moving his army south to cooperate. The British fleet, not realizing that the French had sent their entire fleet to America, dispatched an inadequate force under Admiral Graves.
In early September, French naval forces defeated the British fleet at the Battle of the Chesapeake, cutting off Cornwallis' escape. Cornwallis, still expecting to receive support, failed to break out while he had the chance. When Washington's army arrived outside Yorktown, Cornwallis prematurely abandoned his outer position, hastening his subsequent defeat. The combined Franco-American force of 18,900 men began besieging Cornwallis in early October. For several days, the French and Americans bombarded the British defenses, and then began taking the outer redoubts. The British attempted to cobble together a relief expedition, but encountered numerous delays. Cornwallis decided his position was becoming untenable and he surrendered his entire army of over 7,000 men on October 19, 1781, the same day that the British fleet at New York sailed for his relief.
Downfall of the North Ministry.
News of the surrender at Yorktown arrived in Britain in November 1781. King George III took the news calmly and delivered a defiant address pledging to continue the war, a majority of the House of Commons endorsed it. In the succeeding months news arrived of other reverses, however. The French and Spanish successfully took several West Indian islands and appeared to be on the verge of completely expelling the British there. Minorca also surrendered to a Franco-Spanish force on February 5, 1782 and Gibraltar seemed to be in danger of falling as well. In light of this, Parliament on February 27, 1782 voted to cease all offensive operations in America and seek peace. Threatened with votes of no confidence, on March 20 Lord North resigned and his Tory government was replaced by the Whigs. Ironically, shortly after North resigned the British won the Battle of the Saintes, putting an end to the French threat in the West Indies, and they successfully relieved Gibraltar. Had the North government held out for a few more months they would have been considerably strengthened and could have continued the war in spite of Yorktown.
The new Whig administration accepted American independence as a basis for peace. There were no further major military activities in North America, although the British still had 30,000 garrison troops occupying New York City, Charleston, and Savannah. The war continued elsewhere, including the siege of Gibraltar and naval operations in the East and West Indies, until peace was agreed in 1783.
Naval conflict.
When the war began, the British had overwhelming naval superiority over the American colonists although their fleet was old and in poor condition, a situation that would be blamed on Lord Sandwich, the First Lord of the Admiralty. During the first three years of the war, the Royal Navy was primarily used to transport troops for land operations and to protect commercial shipping. The American colonists had no ships of the line, and relied extensively on privateering to harass British shipping. The privateers caused worry disproportionate to their material success, although those operating out of French channel ports before and after France joined the war caused significant embarrassment to the Royal Navy and inflamed Anglo-French relations. About 55,000 American sailors served aboard the privateers during the war. The American privateers had almost 1,700 ships, and they captured 2,283 enemy ships. The Continental Congress authorized the creation of a small Continental Navy in October 1775, which was primarily used for commerce raiding. John Paul Jones became the first great American naval hero, capturing HMS "Drake" on April 24, 1778, the first victory for any American military vessel in British waters.
Britain vs. France, Spain, Mysore, and Holland 1778–1783.
Europe.
Spain entered the war as a French ally with the goal of recapturing Gibraltar and Minorca, which had been captured by an Anglo-Dutch force in 1704. Gibraltar was besieged for more than three years, but the British garrison stubbornly resisted and was resupplied twice: once after Admiral Rodney's victory over Juan de Lángara in the 1780 "Moonlight Battle", and again by Admiral Richard Howe in 1782. Further Franco-Spanish efforts to capture Gibraltar were unsuccessful. One notable success took place on February 5, 1782, when Spanish and French forces captured Minorca, which Spain retained after the war. Ambitious plans for an invasion of Great Britain in 1779 had to be abandoned.
West Indies and Gulf Coast.
There was much action in the West Indies, especially in the Lesser Antilles. Although France lost St. Lucia early in the war, its navy dominated the West Indies, capturing Dominica, Grenada, Saint Vincent, Montserrat, Tobago, St. Kitts and the Turks and Caicos between 1778 and 1782. Dutch possessions in the West Indies and South America were captured by Britain but later recaptured by France and restored to the Dutch Republic. At the Battle of the Saintes in April 1782, a victory by Rodney's fleet over the French Admiral de Grasse frustrated the hopes of France and Spain to take Jamaica and other colonies from the British.
On the Gulf Coast, Count Bernardo de Gálvez, the Spanish governor of Louisiana, quickly removed the British from their outposts on the lower Mississippi River in 1779 in actions at Manchac and Baton Rouge in British West Florida. Gálvez then captured Mobile in 1780 and stormed and captured the British citadel and capital of Pensacola in 1781. On May 8, 1782, Gálvez captured the British naval base at New Providence in the Bahamas; it was ceded by Spain after the Treaty of Paris and simultaneously recovered by British Loyalists in 1783. Gálvez' actions led to the Spanish acquisition of East and West Florida in the peace settlement, denied the British the opportunity of encircling the American forces from the south, and kept open a vital conduit for supplies to the American frontier. The Continental Congress cited Gálvez in 1785 for his aid during the revolution and George Washington took him to his right during the first parade of July 4.
Central America was also subject to conflict between Britain and Spain, as Britain sought to expand its informal trading influence beyond coastal logging and fishing communities in present-day Belize, Honduras, and Nicaragua. Expeditions against San Fernando de Omoa in 1779 and San Juan in 1780 (the latter famously led by a young Horatio Nelson) met with only temporary success before being abandoned due to disease. The Spanish colonial leaders, in turn, could not completely eliminate British influences along the Mosquito Coast. Except for the French acquisition of Tobago, sovereignty in the West Indies was returned to the "status quo ante bellum" in the peace of 1783.
India.
When word reached India in 1778 that France had entered the war, the British East India Company moved quickly to capture French colonial outposts there, capturing Pondicherry after two months of siege. The capture of the French-controlled port of Mahé on India's west coast motivated Mysore's ruler, Hyder Ali (who was already upset at other British actions, and benefited from trade through the port), to open the Second Anglo-Mysore War in 1780. Ali, and later his son Tipu Sultan, almost drove the British from southern India but was frustrated by weak French support, and the war ended "status quo ante bellum" with the 1784 Treaty of Mangalore. French opposition was led in 1782 and 1783 by Admiral the Baillie de Suffren, who recaptured Trincomalee from the British and fought five celebrated, but largely inconclusive, naval engagements against British Admiral Sir Edward Hughes. France's Indian colonies were returned after the war.
Fourth Anglo-Dutch War.
The Dutch Republic, nominally neutral, had been trading with the Americans, exchanging Dutch arms and munitions for American colonial wares (in contravention of the British "Navigation Acts"), primarily through activity based in St. Eustatius, before the French formally entered the war. The British considered this trade to include contraband military supplies and had attempted to stop it, at first diplomatically by appealing to previous treaty obligations, interpretation of whose terms the two nations disagreed on, and then by searching and seizing Dutch merchant ships. The situation escalated when the British seized a Dutch merchant convoy sailing under Dutch naval escort in December 1779, prompting the Dutch to join the League of Armed Neutrality. Britain responded to this decision by declaring war on the Dutch in December 1780, sparking the Fourth Anglo-Dutch War. The war was a military and economic disaster for the Dutch Republic. Paralyzed by internal political divisions, it could not respond effectively to British blockades of its coast and the capture of many of its colonies. In the 1784 peace treaty between the two nations, the Dutch lost the Indian port of Negapatam and were forced to make trade concessions. The Dutch Republic signed a friendship and trade agreement with the United States in 1782, becoming the third country (after Morocco and later France) to formally recognize the United States.
Treaty of Paris.
In London, as political support for the war plummeted after Yorktown, British Prime Minister Lord North resigned in March 1782. In April 1782, the Commons voted to end the war in America. Preliminary peace articles were signed in Paris at the end of November 1782; the formal end of the war did not occur until the Treaty of Paris (for the U.S.) and the Treaties of Versailles (for the other Allies) were signed on September 3, 1783. The last British troops left New York City on November 25, 1783, and the United States Congress of the Confederation ratified the Paris treaty on January 14, 1784.
Britain negotiated the Paris peace treaty without consulting her Native American allies and ceded all Native American territory between the Appalachian Mountains and the Mississippi River to the United States. Native Americans reluctantly confirmed these land cessions with the United States in a series of treaties, but the fighting would be renewed in conflicts along the frontier in the coming years, the largest being the Northwest Indian War. The British sought to establish a buffer Indian state in the American Midwest, and continued to pursue that goal as late as 1814 in the War of 1812.
The United States gained more than it expected, thanks to the award of western territory. The other Allies had mixed-to-poor results. France made some gains over its nemesis, Great Britain, but its material gains were minimal and its financial losses huge. It was already in financial trouble and its borrowing to pay for the war used up all its credit and created the financial disasters that marked the 1780s. Historians link those disasters to the coming of the French Revolution. The Dutch clearly lost on all points. The Spanish had a mixed result; they did not achieve their primary war goal (recovery of Gibraltar), but they did gain territory. However, in the long run, as the case of Florida shows, the new territory was of little or no value.
Analysis of combatants.
The population of Great Britain and Ireland in 1780 was approximately 12.6 million while the population of the thirteen colonies for the same year has been estimated at 2.8 million including over 500,000 slaves. Theoretically this gave Britain a 4.5:1 manpower advantage, by comparison the Union's manpower advantage over the Confederacy in the American Civil War was only 2.5:1. In practice, the British army never had more than a slight numerical advantage over the Continental Army due to a number of factors, including the need to maintain significant numbers of troops outside of North America. Conscription outside of naval impressment did not exist in Britain back then, and the proportion of Americans willing to serve in their own country's defense was believed to be considerably larger than the proportion of Britons willing to serve overseas. One pre-war estimate claimed that the Patriots could mobilize 100,000 men in a matter of months, but substantial loyalist or neutralist sentiment would keep Patriot forces much smaller than their potential.
Historians continue to debate whether the odds for American victory were long or short. John E. Ferling says the odds were so long that the American victory was "Almost A Miracle." On the other hand, Joseph Ellis says the odds favored the Americans, and asks whether there ever was any realistic chance for the British to win? He argues that this opportunity came only once, in the summer of 1776 and the British failed that test. Admiral Howe and his brother General Howe, "missed several opportunities to destroy the Continental Army...Chance, luck, and even the vagaries of the weather played crucial roles." Ellis's point is that the strategic and tactical decisions of the Howes were fatally flawed because they underestimated the challenges posed by the Patriots. Ellis concludes that once the Howe brothers failed, the opportunity for a British victory "would never come again.":11 The U.S. Army's official textbook argues that while the British difficulties were great, they were hardly insurmountable. "The British forfeited several chances for military victory in 1776–1777, and again in 1780 they might have won had they been able to throw 10,000 fresh troops into the American war."
Patriots.
The Americans began the war with significant disadvantages compared to the British. They had no national government, no national army or navy, no financial system, no banks, no established credit, and no functioning government departments, such as a treasury. The Congress tried to handle administrative affairs through legislative committees, which proved inefficient. The state governments were themselves brand new and officials had no administrative experience. In peacetime the colonies relied heavily on ocean travel and shipping, but that was now shut down by the British blockade and the Americans had to rely on slow overland travel.
However, the Americans had multiple advantages that in the long run outweighed the initial disadvantages they faced. The Americans had a large prosperous population that depended not on imports but on local production for food and most supplies, while the British were mostly shipped in from across the ocean. The British faced a vast territory far larger than Britain or France, located at a far distance from home ports. Most of the Americans lived on farms distant from the seaports—the British could capture any port but that did not give them control over the hinterland. They were on their home ground, had a smoothly functioning, well organized system of local and state governments, newspapers and printers, and internal lines of communications. They had a long-established system of local militia, previously used to combat the French and Native Americans, with companies and an officer corps that could form the basis of local militias, and provide a training ground for the national army created by Congress.
Motivation was a major asset. The Patriots wanted to win; over 200,000 fought in the war; 25,000 died. The British expected the Loyalists to do much of the fighting; they did much less than expected. The British hired German mercenaries to do much of their fighting.
At the onset of the war, the Americans had no major international allies. Battles such as the Battle of Bennington, the Battles of Saratoga and even defeats such as the Battle of Germantown proved decisive in gaining the attention and support of powerful European nations such as France and Spain, who moved from covertly supplying the Americans with weapons and supplies, to overtly supporting them militarily, moving the war to a global stage.
The new Continental Army suffered significantly from a lack of an effective training regime, and largely inexperienced officers and sergeants. The inexperience of its officers was compensated for in part by its senior officers; officers such as George Washington, Horatio Gates, Charles Lee, Richard Montgomery and Francis Marion all had military experience with the British Army during the French and Indian War. The Americans solved their training dilemma during their stint in Winter Quarters at Valley Forge, where they were relentlessly drilled and trained by General Friedrich Wilhelm von Steuben, a veteran of the famed Prussian General Staff. He taught the Continental Army the essentials of military discipline, drills, tactics and strategy, and wrote the Revolutionary War Drill Manual. When the Army emerged from Valley Forge, it proved its ability to equally match the British troops in battle when they fought a successful strategic action at the Battle of Monmouth.
When the war began, the 13 colonies lacked a professional army or navy. Each colony sponsored local militia. Militiamen were lightly armed, had little training, and usually did not have uniforms. Their units served for only a few weeks or months at a time, were reluctant to travel far from home and thus were unavailable for extended operations, and lacked the training and discipline of soldiers with more experience. If properly used, however, their numbers could help the Continental armies overwhelm smaller British forces, as at the battles of Concord, Bennington and Saratoga, and the siege of Boston. Both sides used partisan warfare but the Americans effectively suppressed Loyalist activity when British regulars were not in the area.
Seeking to coordinate military efforts, the Continental Congress established a regular army on June 14, 1775, and appointed George Washington as commander-in-chief. The development of the Continental Army was always a work in progress, and Washington used both his regulars and state militia throughout the war.
The United States Marine Corps traces its institutional roots to the Continental Marines of the war, formed by a resolution of the Continental Congress on November 10, 1775, a date regarded and celebrated as the birthday of the Marine Corps. At the beginning of 1776, Washington's army had 20,000 men, with two-thirds enlisted in the Continental Army and the other third in the various state militias. At the end of the American Revolution in 1783, both the Continental Navy and Continental Marines were disbanded. About 250,000 men served as regulars or as militiamen for the Revolutionary cause in the eight years of the war, but there were never more than 90,000 men under arms at one time.
Armies were small by European standards of the era, largely attributable to limitations such as lack of powder and other logistical capabilities on the American side. It was also difficult for Great Britain to transport troops across the Atlantic and they depended on local supplies that the Patriots tried to cut off. By comparison, Duffy notes that Frederick the Great usually commanded from 23,000 to 50,000 in battle. Both figures pale in comparison to the armies that were fielded in the early 19th century, where troop formations approached or exceeded 100,000 men.
Loyalists.
Historians have estimated that approximately 40 to 45 percent of the colonists supported the rebellion, while 15 to 20 percent remained loyal to the Crown. The rest attempted to remain neutral and kept a low profile.
At least 25,000 Loyalists fought on the side of the British. Thousands served in the Royal Navy. On land, Loyalist forces fought alongside the British in most battles in North America. Many Loyalists fought in partisan units, especially in the Southern theater.
The British military met with many difficulties in maximizing the use of Loyalist factions. British historian Jeremy Black wrote, "In the American war it was clear to both royal generals and revolutionaries that organized and significant Loyalist activity would require the presence of British forces." In the South, the use of Loyalists presented the British with "major problems of strategic choice" since while it was necessary to widely disperse troops in order to defend Loyalist areas, it was also recognized that there was a need for "the maintenance of large concentrated forces able" to counter major attacks from the American forces. In addition, the British were forced to ensure that their military actions would not "offend Loyalist opinion", eliminating such options as attempting to "live off the country", destroying property for intimidation purposes, or coercing payments from colonists ("laying them under contribution").
British.
Britain entered the war with confidence; it had the world's most powerful navy, a well-trained professional army, a sound financial system that could pay the costs, a stable government, and experienced leadership. However they were beset with major challenges. Compared to the Americans, the British had no major allies, and only had troops provided by small German states to bolster the small British Army. At the onset of the war, the British Army was less than 48,000 strong worldwide, and suffered from a lack of effective recruiting. By 1778, the army was pardoning criminals for military service and had extended the age range for service to be from 16 to 50. Although its officer and non-commissioned officer corps were relatively professional and experienced, this professionalism was diluted because wealthy individuals lacking military experience could purchase commissions and promotions. As a consequence, inexperienced officers sometimes found their way into positions of high responsibility.
Distance was also a major problem for the British. Although the Royal Navy was the largest and most experienced in the world at the time, it sometimes took months for troops to reach North America, and orders were often out of date because the military situation on the ground had changed by the time they arrived. Additionally, the British had logistical problems whenever they operated away from the coast; they were vulnerable to guerilla attacks on their supply chains whenever they went far inland. On a logistical note, the flints used in British weapons also put them at a disadvantage on the battlefield. British flints could only fire for 6 rounds before requiring re-sharpening, while American flints could fire 60 rounds before resharpening. A common expression ran among the redcoats; which was that "Yankee flint was as good as a glass of grog." Although discipline was harsh in the army, the redcoats had little self-discipline; gambling, looting, promiscuity and heavy drinking were common problems, among all ranks alike. The army suffered from mediocre organisation in terms of logistics, food supplies were often bad and the sparse land of America offered little in the way of finding reliable substitutes.
Suppressing a rebellion in America also posed other problems. At the onset of the war, the British had around 8,000 men stationed in North America, however these were required to cover an area that stretched from northern Canada to Florida, a distance of almost 2,000 miles (3,200 km). As the colonies had not been united before the war, there was no central area of strategic importance. In European conflicts, the capture of a capital city often meant the end of the war; however in America, when the British seized key cities such as New York, Philadelphia or Boston—or Washington D.C. in the War of 1812 thirty years later—the war continued unabated. Furthermore, despite the fact that at its height, the British fielded some 56,000 men in the colonies exclusive of mercenaries and militia, they lacked the sufficient numbers to both defeat the Americans on the battlefield and simultaneously occupy the captured areas. It was not unusual for the Americans to suffer a string of defeats, only to have the British retreat because they could not occupy the captured land. Despite strong Loyalist support, these troops were often displaced by Patriot militia when British regulars were not in the area, demonstrated at battles such as Kings Mountain. The manpower shortage became critical when France, Spain and the Netherlands entered the war, as the British were spread across several theatres worldwide, when before they were concentrated only in America.
The British also had to contend with several psychological factors during the conflict. The need to maintain Loyalist allegiance provided setbacks, as the British could not use the harsh methods of suppressing rebellion they had used in Ireland and Scotland. Loyalists often came from the same communities as Patriots and as a result, such methods could not be employed for fear of alienating them. Even despite these limitations, neutral colonists were often driven into the ranks of the Revolutionaries due to the conflict, such as the war in the Carolinas, marked by heavy brutality on both sides. As a result of the manpower shortage and Patriot control of the countryside, where the majority of the American population lived, the British often could not simultaneously defeat the Americans on the field and occupy the captured areas, evidenced by withdrawals from Philadelphia and the Carolinas after great initial success. A single American victory could often reverse the impact of a string of British successes, as shown by engagements at Trenton, Bennington, King's Mountain and even defeats such as Germantown, all of which went a long way to galvanizing Patriot support for the war, and of persuading European powers such as France and Spain to support the rebellion.
Early in 1775, the British Army consisted of about 36,000 men worldwide, but wartime recruitment steadily increased this number. Great Britain had a difficult time appointing general officers, however. General Thomas Gage, in command of British forces in North America when the rebellion started, was criticized for being too lenient (perhaps influenced by his American wife). General Jeffrey Amherst, 1st Baron Amherst turned down an appointment as commander in chief due to an unwillingness to take sides in the conflict. Similarly, Admiral Augustus Keppel turned down a command, saying "I cannot draw the sword in such a cause." The Earl of Effingham publicly resigned his commission when his 22nd Regiment of foot was posted to America, and William Howe and John Burgoyne were members of parliament who opposed military solutions to the American rebellion. Howe and Henry Clinton stated that they were unwilling participants in the war and were only following orders.
Over the course of the war, Great Britain signed treaties with various German states, which supplied about 30,000 soldiers. Germans made up about one-third of the British troop strength in North America. The Landgraviate of Hesse-Kassel contributed more soldiers than any other state, and German soldiers became known as "Hessians" to the Americans. Revolutionary speakers called German soldiers "foreign mercenaries", and they are scorned as such in the Declaration of Independence. By 1779, the number of British and German troops stationed in North America was over 60,000, although these were spread from Canada to Florida. Initially, several German principalities offered military support to Great Britain but these offers were rejected. However, as the war dragged on it became clear that Great Britain would need the extra manpower of the German states and led to Great Britain seeking support from German principalities such as Hesse-Kassel and Ansbach-Bayreuth.
The Secretary of State at War Lord Barrington and the Adjutant-General Edward Harvey were both strongly opposed to outright war on land. In 1766 Barrington had recommended withdrawing the army from the 13 Colonies to Canada, Nova Scotia and Florida. At the beginning of the war he urged a naval blockade, which would quickly damage the colonists' trading activities.
African Americans.
African Americans—slave and free—served on both sides during the war. The British recruited slaves belonging to Patriot masters and promised freedom to those who served by act of Lord Dunmore's Proclamation. Because of manpower shortages, George Washington lifted the ban on black enlistment in the Continental Army in January 1776. Small all-black units were formed in Rhode Island and Massachusetts; many slaves were promised freedom for serving. (Some of the men promised freedom were sent back to their masters, after the war was over, out of political convenience. George Washington received and ignored letters from the re-enslaved soldiers.) Another all-black unit came from Saint-Domingue with French colonial forces. At least 5,000 black soldiers fought for the Revolutionary cause.
Tens of thousands of slaves escaped during the war and joined British lines; others simply moved off in the chaos. For instance, in South Carolina, nearly 25,000 slaves (30% of the enslaved population) fled, migrated or died during the disruption of the war. This greatly disrupted plantation production during and after the war. When they withdrew their forces from Savannah and Charleston, the British also evacuated 10,000 slaves belonging to Loyalists. Altogether, the British evacuated nearly 20,000 blacks at the end of the war. More than 3,000 of them were freedmen and most of these were resettled in Nova Scotia; other blacks were sold in the West Indies.
Native Americans.
Most Native Americans east of the Mississippi River were affected by the war, and many communities were divided over the question of how to respond to the conflict. Though a few tribes were on friendly terms with the Americans, most Native Americans opposed the United States as a potential threat to their territory. Approximately 13,000 Native Americans fought on the British side, with the largest group coming from the Iroquois tribes, who fielded around 1,500 men. The powerful Iroquois Confederacy was shattered as a result of the conflict; although the Confederacy did not take sides, the Seneca, Onondaga, and Cayuga nations sided with the British. Members of the Mohawk fought on both sides. Many Tuscarora and Oneida sided with the colonists. The Continental Army sent the Sullivan Expedition on raids throughout New York to cripple the Iroquois tribes that had sided with the British. Both during and after the war friction between the Mohawk leaders Joseph Louis Cook and Joseph Brant, who had sided with the Americans and the British respectively, further exacerbated the split.
Creek and Seminole allies of Britain fought against Americans in Georgia and South Carolina. In 1778, a force of 800 Creeks destroyed American settlements along the Broad River in Georgia. Creek warriors also joined Thomas Brown's raids into South Carolina and assisted Britain during the Siege of Savannah. Many Native Americans were involved in the fighting between Britain and Spain on the Gulf Coast and up the Mississippi River—mostly on the British side. Thousands of Creeks, Chickasaws, and Choctaws fought in or near major battles such as the Battle of Fort Charlotte, the Battle of Mobile, and the Siege of Pensacola.
Sex, race, class.
Pybus (2005) estimates that about 20,000 slaves defected to or were captured by the British, of whom about 8,000 died from disease or wounds or were recaptured by the Patriots, and 12,000 left the country at the end of the war, for freedom in Canada, the Caribbean or London, or some enslaved and transported to the West Indies.
Baller (2006) examines family dynamics and mobilization for the Revolution in central Massachusetts. He reports that warfare and the farming culture were sometimes incompatible. Militiamen found that living and working on the family farm had not prepared them for wartime marches and the rigors of camp life. Rugged individualism conflicted with military discipline and regimentation. A man's birth order often influenced his military recruitment, as younger sons went to war and older sons took charge of the farm. A person's family responsibilities and the prevalent patriarchy could impede mobilization. Harvesting duties and family emergencies pulled men home regardless of the sergeant's orders. Some relatives might be Loyalists, creating internal strains. On the whole, historians conclude the Revolution's effect on patriarchy and inheritance patterns favored egalitarianism.
McDonnell (2006) shows a grave complication in Virginia's mobilization of troops was the conflicting interests of distinct social classes, which tended to undercut a unified commitment to the Patriot cause. The Assembly balanced the competing demands of elite slave-owning planters, the middling yeomen (some owning a few slaves), and landless indentured servants, among other groups. The Assembly used deferments, taxes, military service substitute, and conscription to resolve the tensions. Unresolved class conflict, however, made these laws less effective. There were violent protests, many cases of evasion, and large-scale desertion, so that Virginia's contributions came at embarrassingly low levels. With the British invasion of the state in 1781, Virginia was mired in class division as its native son, George Washington, made desperate appeals for troops.
Costs of the war.
Casualties.
Americans and allies.
The total loss of life throughout the war is largely unknown. As was typical in the wars of the era, disease claimed far more lives than battle. Between 1775 and 1782 a smallpox epidemic swept across North America, killing 40 people in Boston alone. Historian Joseph Ellis suggests that Washington's decision to have his troops inoculated against the smallpox epidemic, including the use of biological warfare by the British, was one of his most important decisions.
At least 25,000 American Patriots died during active military service. About 6,800 of these deaths were in battle; the other 17,000 recorded deaths were from disease, including about 8,000–12,000 who died of starvation or disease brought on by deplorable conditions while prisoners of war, most in rotting British prison ships in New York. Another estimate, however, puts the total death toll at around 70,000, which if true would make the conflict proportionately deadlier than the American Civil War. The uncertainty arises from the number of disease deaths, which were believed to be quite numerous, amounting to an estimated 10,000 in 1776 alone. The number of Patriots seriously wounded or disabled by the war has been estimated from 8,500 to 25,000. Proportionate to the population of the colonies, the Revolutionary War was at least the second-deadliest conflict in American history, ranking ahead of World War II and behind only the Civil War.
British and allies.
In 1784 a British lieutenant compiled a detailed list of 205 British officers killed in action during the war, including deaths in Europe, the Caribbean, and the East Indies. An extrapolation based on this list puts British Army losses at some 4,000 killed and died of wounds. A table from 1781 puts total British Army deaths at 6,046 in North America (from 1775-1779) and 3,326 in the West Indies (from 1778-1780). Approximately 1,800 Germans were killed in combat out of a total of 7,774 deaths. British returns in 1783 listed 43,633 rank and file deaths "in the British service".
About 171,000 sailors served in the Royal Navy during the war; about a quarter had been pressed into service. About 1,240 were killed in battle, while 18,500 died from disease (figures from 1776-1780 only). The greatest killer was scurvy, a disease that had been shown to be preventable by issuing lemon or lime juice to sailors but was not taken seriously. Scurvy would be eradicated in the Royal Navy in 1790s by the chairman of the Navy's Sick and Hurt Board, Gilbert Blane. About 42,000 British sailors deserted during the war.
Financial costs.
The British spent about £80 million and ended with a national debt of £250 million, which it easily financed at about £9.5 million a year in interest. The French spent 1.3 billion livres (about £56 million). Their total national debt was £187 million, which they could not easily finance; over half the French national revenue went to debt service in the 1780s. The debt crisis became a major enabling factor of the French Revolution as the government could not raise taxes without public approval. The United States spent $37 million at the national level plus $114 million by the states. This was mostly covered by loans from France and the Netherlands, loans from Americans, and issuance of an increasing amount of paper money (which became "not worth a continental"). The U.S. finally solved its debt and currency problems in the 1790s when Secretary of the Treasury Alexander Hamilton secured legislation by which the national government assumed all of the state debts, and in addition created a national bank and a funding system based on tariffs and bond issues that paid off the foreign debts.
References.
To avoid duplication, notes for sections with a link to a "Main article" will be found in the linked article.
Further reading.
</dl>
Reference literature.
These are some of the standard works about the war in general that are not listed above; books about specific campaigns, battles, units, and individuals can be found in those articles.
</dl>

</doc>
<doc id="849" url="http://en.wikipedia.org/wiki?curid=849" title="Aircraft">
Aircraft

An aircraft is a machine that is able to fly by gaining support from the air, or, in general, the atmosphere of a planet. It counters the force of gravity by using either static lift or by using the dynamic lift of an airfoil, or in a few cases the downward thrust from jet engines.
The human activity that surrounds aircraft is called "aviation". Crewed aircraft are flown by an onboard pilot, but unmanned aerial vehicles may be remotely controlled or self-controlled by onboard computers. Aircraft may be classified by different criteria, such as lift type, aircraft propulsion, usage and others.
History.
Flying model craft and stories of manned flight go back many centuries, however the first manned ascent – and safe descent – in modern times took place by hot-air balloon in the 18th century. Each of the two World Wars led to great technical advances. Consequently the history of aircraft can be divided into five eras:
Methods of lift.
Lighter than air – aerostats.
Aerostats use buoyancy to float in the air in much the same way that ships float on the water. They are characterized by one or more large gasbags or canopies, filled with a relatively low-density gas such as helium, hydrogen, or hot air, which is less dense than the surrounding air. When the weight of this is added to the weight of the aircraft structure, it adds up to the same weight as the air that the craft displaces.
Small hot-air balloons called sky lanterns date back to the 3rd century BC, and were only the second type of aircraft to fly, the first being kites.
A balloon was originally any aerostat, while the term airship was used for large, powered aircraft designs – usually fixed-wing – though none had yet been built. The advent of powered balloons, called dirigible balloons, and later of rigid hulls allowing a great increase in size, began to change the way these words were used. Huge powered aerostats, characterized by a rigid outer framework and separate aerodynamic skin surrounding the gas bags, were produced, the Zeppelins being the largest and most famous. There were still no fixed-wing aircraft or non-rigid balloons large enough to be called airships, so "airship" came to be synonymous with these aircraft. Then several accidents, such as the Hindenburg disaster in 1937, led to the demise of these airships. Nowadays a "balloon" is an unpowered aerostat and an "airship" is a powered one.
A powered, steerable aerostat is called a "dirigible". Sometimes this term is applied only to non-rigid balloons, and sometimes "dirigible balloon" is regarded as the definition of an airship (which may then be rigid or non-rigid). Non-rigid dirigibles are characterized by a moderately aerodynamic gasbag with stabilizing fins at the back. These soon became known as "blimps". During the Second World War, this shape was widely adopted for tethered balloons; in windy weather, this both reduces the strain on the tether and stabilizes the balloon. The nickname "blimp" was adopted along with the shape. In modern times, any small dirigible or airship is called a blimp, though a blimp may be unpowered as well as powered.
Heavier-than-air – aerodynes.
Heavier-than-air aircraft, such as airplanes, must find some way to push air or gas downwards, so that a reaction occurs (by Newton's laws of motion) to push the aircraft upwards. This dynamic movement through the air is the origin of the term "aerodyne". There are two ways to produce dynamic upthrust: aerodynamic lift, and powered lift in the form of engine thrust.
Aerodynamic lift involving wings is the most common, with fixed-wing aircraft being kept in the air by the forward movement of wings, and rotorcraft by spinning wing-shaped rotors sometimes called rotary wings. A wing is a flat, horizontal surface, usually shaped in cross-section as an aerofoil. To fly, air must flow over the wing and generate lift. A "flexible wing" is a wing made of fabric or thin sheet material, often stretched over a rigid frame. A "kite" is tethered to the ground and relies on the speed of the wind over its wings, which may be flexible or rigid, fixed, or rotary.
With powered lift, the aircraft directs its engine thrust vertically downward. V/STOL aircraft, such as the Harrier Jump Jet and F-35B take off and land vertically using powered lift and transfer to aerodynamic lift in steady flight.
A pure rocket is not usually regarded as an aerodyne, because it does not depend on the air for its lift (and can even fly into space); however, many aerodynamic lift vehicles have been powered or assisted by rocket motors. Rocket-powered missiles that obtain aerodynamic lift at very high speed due to airflow over their bodies are a marginal case.
Fixed-wing.
The forerunner of the fixed-wing aircraft is the kite. Whereas a fixed-wing aircraft relies on its forward speed to create airflow over the wings, a kite is tethered to the ground and relies on the wind blowing over its wings to provide lift. Kites were the first kind of aircraft to fly, and were invented in China around 500 BC. Much aerodynamic research was done with kites before test aircraft, wind tunnels, and computer modelling programs became available.
The first heavier-than-air craft capable of controlled free-flight were gliders. A glider designed by Cayley carried out the first true manned, controlled flight in 1853.
Practical, powered, fixed-wing aircraft (the aeroplane or airplane) were invented by Wilbur and Orville Wright. Besides the method of propulsion, fixed-wing aircraft are in general characterized by their wing configuration. The most important wing characteristics are:
A variable geometry aircraft can change its wing configuration during flight.
A "flying wing" has no fuselage, though it may have small blisters or pods. The opposite of this is a "lifting body", which has no wings, though it may have small stabilizing and control surfaces.
Wing-in-ground-effect vehicles may be considered as fixed-wing aircraft. They "fly" efficiently close to the surface of the ground or water, like conventional aircraft during takeoff. An example is the Russian ekranoplan (nicknamed the "Caspian Sea Monster"). Man-powered aircraft also rely on ground effect to remain airborne with a minimal pilot power, but this is only because they are so underpowered — in fact, the airframe is capable of flying higher.
Rotorcraft.
Rotorcraft, or rotary-wing aircraft, use a spinning rotor with aerofoil section blades (a "rotary wing") to provide lift. Types include helicopters, autogyros, and various hybrids such as gyrodynes and compound rotorcraft.
"Helicopters" have a rotor turned by an engine-driven shaft. The rotor pushes air downward to create lift. By tilting the rotor forward, the downward flow is tilted backward, producing thrust for forward flight. Some helicopters have more than one rotor and a few have rotors turned by gas jets at the tips.
"Autogyros" have unpowered rotors, with a separate power plant to provide thrust. The rotor is tilted backward. As the autogyro moves forward, air blows upward across the rotor, making it spin. This spinning increases the speed of airflow over the rotor, to provide lift. Rotor kites are unpowered autogyros, which are towed to give them forward speed or tethered to a static anchor in high-wind for kited flight.
"Cyclogyros" rotate their wings about a horizontal axis.
"Compound rotorcraft" have wings that provide some or all of the lift in forward flight. They are nowadays classified as "powered lift" types and not as rotorcraft. "Tiltrotor" aircraft (such as the V-22 Osprey), tiltwing, tailsitter, and coleopter aircraft have their rotors/propellers horizontal for vertical flight and vertical for forward flight.
Propulsion.
Unpowered aircraft.
Gliders are heavier-than-air aircraft that do not employ propulsion once airborne. Take-off may be by launching forward and downward from a high location, or by pulling into the air on a tow-line, either by a ground-based winch or vehicle, or by a powered "tug" aircraft. For a glider to maintain its forward air speed and lift, it must descend in relation to the air (but not necessarily in relation to the ground). Many gliders can 'soar' – gain height from updrafts such as thermal currents. The first practical, controllable example was designed and built by the British scientist and pioneer George Cayley, whom many recognise as the first aeronautical engineer. Common examples of gliders are sailplanes, hang gliders and paragliders.
Balloons drift with the wind, though normally the pilot can control the altitude, either by heating the air or by releasing ballast, giving some directional control (since the wind direction changes with altitude). A wing-shaped hybrid balloon can glide directionally when rising or falling; but a spherically shaped balloon does not have such directional control.
Kites are aircraft that are tethered to the ground or other object (fixed or mobile) that maintains tension in the tether or kite line; they rely on virtual or real wind blowing over and under them to generate lift and drag. Kytoons are balloon-kite hybrids that are shaped and tethered to obtain kiting deflections, and can be lighter-than-air, neutrally buoyant, or heavier-than-air.
Powered aircraft.
Powered aircraft have one or more onboard sources of mechanical power, typically aircraft engines although rubber and manpower have also been used. Most aircraft engines are either lightweight piston engines or gas turbines. Engine fuel is stored in tanks, usually in the wings but larger aircraft also have additional fuel tanks in the fuselage.
Propeller aircraft.
Propeller aircraft use one or more propellers (airscrews) to create thrust in a forward direction. The propeller is usually mounted in front of the power source in "tractor configuration" but can be mounted behind in "pusher configuration". Variations of propeller layout include "contra-rotating propellers" and "ducted fans".
Many kinds of power plant have been used to drive propellers. Early airships used man power or steam engines. The more practical internal combustion piston engine was used for virtually all fixed-wing aircraft until World War II and is still used in many smaller aircraft. Some types use turbine engines to drive a propeller in the form of a turboprop or propfan. Human-powered flight has been achieved, but has not become a practical means of transport. Unmanned aircraft and models have also used power sources such as electric motors and rubber bands.
Jet aircraft.
Jet aircraft use airbreathing jet engines, which take in air, burn fuel with it in a combustion chamber, and accelerate the exhaust rearwards to provide thrust.
Turbojet and turbofan engines use a spinning turbine to drive one or more fans, which provide additional thrust. An afterburner may be used to inject extra fuel into the hot exhaust, especially on military "fast jets". Use of a turbine is not absolutely necessary: other designs include the pulse jet and ramjet. These mechanically simple designs cannot work when stationary, so the aircraft must be launched to flying speed by some other method. Other variants have also been used, including the motorjet and hybrids such as the Pratt & Whitney J58, which can convert between turbojet and ramjet operation.
Compared to propellers, jet engines can provide much higher thrust, higher speeds and, above about 40000 ft, greater efficiency. They are also much more fuel-efficient than rockets. As a consequence nearly all large, high-speed or high-altitude aircraft use jet engines.
Rotorcraft.
Some rotorcraft, such as helicopters, have a powered rotary wing or "rotor", where the rotor disc can be angled slightly forward so that a proportion of its lift is directed forwards. The rotor may, like a propeller, be powered by a variety of methods such as a piston engine or turbine. Experiments have also used jet nozzles at the rotor blade tips.
Design and construction.
Aircraft are designed according to many factors such as customer and manufacturer demand, safety protocols and physical and economic constraints. For many types of aircraft the design process is regulated by national airworthiness authorities.
The key parts of an aircraft are generally divided into three categories:
Structure.
The approach to structural design varies widely between different types of aircraft. Some, such as paragliders, comprise only flexible materials that act in tension and rely on aerodynamic pressure to hold their shape. A balloon similarly relies on internal gas pressure but may have a rigid basket or gondola slung below it to carry its payload. Early aircraft, including airships, often employed flexible doped aircraft fabric covering to give a reasonably smooth aeroshell stretched over a rigid frame. Later aircraft employed semi-monocoque techniques, where the skin of the aircraft is stiff enough to share much of the flight loads. In a true monocoque design there is no internal structure left.
The key structural parts of an aircraft depend on what type it is.
Aerostats.
Lighter-than-air types are characterised by one or more gasbags, typically with a supporting structure of flexible cables or a rigid framework called its hull. Other elements such as engines or a gondola may also be attached to the supporting structure.
Aerodynes.
Heavier-than-air types are characterised by one or more wings and a central fuselage. The fuselage typically also carries a tail or empennage for stability and control, and an undercarriage for takeoff and landing. Engines may be located on the fuselage or wings. On a fixed-wing aircraft the wings are rigidly attached to the fuselage, while on a rotorcraft the wings are attached to a rotating vertical shaft. Smaller designs sometimes use flexible materials for part or all of the structure, held in place either by a rigid frame or by air pressure. The fixed parts of the structure comprise the airframe.
Avionics.
The avionics comprise the flight control systems and related equipment, including the cockpit instrumentation, navigation, radar, monitoring, and communication systems.
Flight characteristics.
Flight envelope.
The flight envelope of an aircraft refers to its capabilities in terms of airspeed and load factor or altitude. The term can also refer to other measurements such as maneuverability. When a craft is pushed, for instance by diving it at high speeds, it is said to be flown "outside the envelope", something considered unsafe.
Range.
The range is the distance an aircraft can fly between takeoff and landing, as limited by the time it can remain airborne.
For a powered aircraft the time limit is determined by the fuel load and rate of consumption.
For an unpowered aircraft, the maximum flight time is limited by factors such as weather conditions and pilot endurance. Many aircraft types are restricted to daylight hours, while balloons are limited by their supply of lifting gas. The range can be seen as the average ground speed multiplied by the maximum time in the air.
Flight dynamics.
Flight dynamics is the science of air vehicle orientation and control in three dimensions. The three critical flight dynamics parameters are the angles of rotation around three axes about the vehicle's center of mass, known as "pitch", "roll", and "yaw" (quite different from their use as Tait-Bryan angles).
Flight dynamics is concerned with the stability and control of an aircraft's rotation about each of these axes.
Stability.
An aircraft that is unstable tends to diverge from its current flight path and so is difficult to fly. An aircraft which is very stable tends to stay on its current flight path and is difficult to manoeuvre. So it is important for any design to achieve the desired degree of stability. Since the widespread use of digital computers, it is becoming increasingly common for designs to be inherently unstable and to rely on computerised control systems to provide artificial stability.
A fixed wing is typically unstable in pitch, roll and yaw. Pitch and yaw stabilities of conventional fixed wing designs need horizontal and vertical stabilisers, which act in a similar way to the feathers on an arrow. These stabilizing surfaces allow equilibrium of aerodynamic forces and to stabilise the flight dynamics of pitch and yaw. They are usually mounted on the tail section (empennage), although in the canard layout, the main aft wing replaces the canard foreplane as pitch stabilizer. tandem and Tailless aircraft rely on the same general rule to achieve stability, the aft surface being the stabilising one.
A rotary wing is typically unstable in yaw, requiring a vertical stabiliser.
A balloon is typically very stable in pitch and roll due to the way the payload is hung underneath.
Control.
Flight control surfaces enable the pilot to control an aircraft's flight attitude and are usually part of the wing or mounted on, or integral with, the associated stabilizing surface. Their development was a critical advance in the history of aircraft, which had until that point been uncontrollable in flight.
Aerospace engineers develop control systems for a vehicle's orientation (attitude) about its center of mass. The control systems include actuators, which exert forces in various directions, and generate rotational forces or moments about the aerodynamic center of the aircraft, and thus rotate the aircraft in pitch, roll, or yaw. For example, a pitching moment is a vertical force applied at a distance forward or aft from the aerodynamic center of the aircraft, causing the aircraft to pitch up or down. Control systems are also sometimes used to increase or decrease drag, for example to slow the aircraft to a safe speed for landing.
The two main aerodynamic forces acting on any aircraft are lift supporting it in the air and drag opposing its motion. Control surfaces or other techniques may also be used to affect these forces directly, without inducing any rotation.
Impacts of aircraft use.
Aircraft permit long distance, high speed travel and may be the more fuel efficient mode of transportation in some circumstances. Aircraft have environmental and climate impacts beyond fuel efficiency considerations, however. They are also relatively noisy compared to other forms of travel and high altitude aircraft generate contrails, which experimental evidence suggests may alter weather patterns.
Uses for aircraft.
Aircraft are produced in several different types optimized for various uses; military aircraft, which includes not just combat types but many types of supporting aircraft, and civil aircraft, which include all non-military types, experimental and model.
Military.
A military aircraft is any aircraft that is operated by a legal or insurrectionary armed service of any type. Military aircraft can be either combat or non-combat:
Most military aircraft are powered heavier-than-air types. Other types such as gliders and balloons have also been used as military aircraft; for example, balloons were used for observation during the American Civil War and World War I, and military gliders were used during World War II to land troops.
Civil.
Civil aircraft divide into "commercial" and "general" types, however there are some overlaps.
Commercial aircraft include types designed for scheduled and charter airline flights, carrying passengers, mail and other cargo. The larger passenger-carrying types are the airliners, the largest of which are wide-body aircraft. Some of the smaller types are also used in general aviation, and some of the larger types are used as VIP aircraft.
General aviation is a catch-all covering other kinds of private (where the pilot is not paid for time or expenses) and commercial use, and involving a wide range of aircraft types such as business jets (bizjets), trainers, homebuilt, gliders, warbirds and hot air balloons to name a few. The vast majority of aircraft today are general aviation types.
Experimental.
An experimental aircraft is one that has not been fully proven in flight, or one that carries an FAA airworthiness certificate in the "Experimental" category. Often, this implies that new aerospace technologies are being tested on the aircraft, although the term also refers to amateur- and kit-built aircraft; many of which are based on proven designs.
Model.
A model aircraft is a small unmanned type made to fly for fun, for static display, for aerodynamic research or for other purposes. A scale model is a replica of some larger design.
External links.
History
Information

</doc>
<doc id="869" url="http://en.wikipedia.org/wiki?curid=869" title="American Film Institute">
American Film Institute

The American Film Institute (AFI) is a film organization that educates filmmakers and honors the heritage of the moving picture arts in the U.S. AFI is supported by private funding and public membership.
Membership.
The institute is composed of leaders from the film, entertainment, business and academic communities. A Board of Trustees chaired by Sir Howard Stringer and a Board of Directors chaired by Robert Daly guide the organization, which is led by President and CEO Bob Gazzale (as of October 2014). Prior leaders were founding director George Stevens, Jr. (from 1967 to 1980) and Jean Picker Firstenberg (from 1980 to 2007).
List of programs in brief.
AFI educational and cultural programs include:
History.
The American Film Institute was founded by a 1965 presidential mandate announced in the Rose Garden of the White House by Lyndon B. Johnson – to establish a national arts organization to preserve the legacy of American film heritage, educate the next generation of filmmakers and honor the artists and their work. Two years later, in 1967, AFI was established, supported by the National Endowment for the Arts, the Motion Picture Association of America and the Ford Foundation.
The original 22-member Board of Trustees included Chair Gregory Peck and Vice Chair Sidney Poitier as well as Francis Ford Coppola, Arthur Schlesinger, Jr., Jack Valenti and other representatives from the arts and academia.
The institute established a training program for filmmakers known then as the Center for Advanced Film Studies. Also created in the early years were a repertory film exhibition program at the Kennedy Center for the Performing Arts and the AFI Catalog of Feature Films — a scholarly source for American film history. The institute moved to its current eight-acre Hollywood campus in 1981. The film training program grew into the AFI Conservatory, an accredited graduate school.
AFI moved its presentation of first-run and auteur films from the Kennedy Center to the historic 1938 Art Deco AFI Silver Theatre and Cultural Center, which now hosts two major film festivals – AFI Fest and AFI Docs – making AFI the largest nonprofit film exhibitor in the world. AFI educates audiences and recognizes artistic excellence through its awards programs and 10 Top 10 Lists.
AFI Conservatory.
In 1969, the institute established the AFI Conservatory for Advanced Film Studies at Greystone, the Doheny Mansion in Beverly Hills, California. The first class included filmmakers Terrence Malick, David Lynch, Caleb Deschanel and Paul Schrader. That program grew into the AFI Conservatory, an accredited graduate film school located in the hills above Hollywood, California, providing training in six filmmaking disciplines: cinematography, directing, editing, producing, production design and screenwriting. Mirroring a professional production environment, Fellows collaborate to make more films than any other graduate level program. Admission to AFI Conservatory is highly selective, with a maximum of 140 graduates per year.
In 2013, Emmy and Oscar-winning director, producer and screenwriter James L. Brooks ("As Good as It Gets", "Broadcast News", "Terms of Endearment") joined AFI as Artistic Director of the AFI Conservatory where he provides leadership for the film program. Brooks' artistic role at the AFI Conservatory has a rich legacy that includes Daniel Petrie, Jr., Robert Wise and Frank Pierson. Award-winning director Bob Mandel serves as Dean of the AFI Conservatory.
Notable alumni.
AFI Conservatory's alumni have careers in film, television and on the web. They have been recognized with all of the major industry awards – Academy Award, Emmy Award, guild awards, and the Tony Award.
Among the alumni of AFI are Andrea Arnold, ("Red Road", "Fish Tank"), Darren Aronofsky ("Requiem for a Dream", "Black Swan"), Doug Ellin ("Entourage"), Todd Field ("In the Bedroom", "Little Children"), Jack Fisk ("Badlands", "Days of Heaven","There Will Be Blood"), Carl Franklin ("One False Move", "Devil in a Blue Dress", "House of Cards"), Janusz Kamiński ("Lincoln", "Schindler's List", "Saving Private Ryan"), Matthew Libatique ("Noah", "Black Swan"), David Lynch ("Mulholland Drive", "Blue Velvet"), Terrence Malick ("Days of Heaven", "The Thin Red Line", "The Tree of Life"), Victor Nuñez, ("Ruby in Paradise", "Ulee's Gold"), Wally Pfister ("Memento", "The Dark Knight", "Inception"), Robert Richardson ("Platoon", "JFK", "Django Unchained") and many others.
AFI programs.
AFI Catalog of Feature Films.
The AFI Catalog, started in 1968, is a web-based filmographic database. A research tool for film historians, the catalog consists of entries on more than 60,000 feature films and 17,000 short films produced from 1893–2011, as well as AFI Awards Outstanding Movies of the Year from 2000 through 2010.
AFI Awards.
Each year the AFI Awards honor the ten outstanding films and the ten outstanding television programs. The Awards are announced in December and a private luncheon for award honorees takes place the following January.
The AFI Awards were first announced in 2000.
AFI 100 Years… series.
The AFI 100 Years... series, which ran from 1998 to 2008 and created jury-selected lists of America's best movies in categories such as Musicals, Laughs and Thrills, prompted new generations to experience classic American films. The juries consisted of over 1,500 artists, scholars, critics and historians, with movies selected based on the film's popularity over time, historical significance and cultural impact. "Citizen Kane" was voted the greatest American film twice.
AFI film festivals.
AFI operates two film festivals: AFI Fest in Los Angeles, and AFI Docs (formally known as Silverdocs) in Silver Spring, Maryland, and Washington, D.C..
AFI Fest.
AFI Fest is the American Film Institute's annual celebration of artistic excellence. The festival is a showcase for the best festival films of the year and an opportunity for master filmmakers and emerging artists to come together with audiences in the movie capital of the world. AFI Fest is the only festival of its stature that is free to the public. The Academy of Motion Picture Arts and Sciences recognizes AFI Fest as a qualifying festival for the Short Films category for the annual Academy Awards.
The festival has paid tribute to numerous influential filmmakers and artists over the years, including Agnès Varda, Pedro Almodóvar and David Lynch as Guest Artistic Directors, and has screened scores of films that have produced Oscar nominations and wins. The American Film Market (AFM) is the market partner of AFI Fest. Audi is the festival's presenting sponsor. Additional sponsors include American Airlines and Stella Artois.
AFI Docs.
Held annually in June, AFI Docs (formerly Silverdocs) is a documentary festival in Washington, D.C.. The festival attracts over 27,000 documentary enthusiasts.
AFI Silver Theatre and Cultural Center.
The AFI Silver Theatre and Cultural Center is a moving image exhibition, education and cultural center located in Silver Spring, Maryland. Anchored by the restoration of noted architect John Eberson's historic 1938 Silver Theatre, it features 32,000 square feet of new construction housing two stadium theatres, office and meeting space, and reception and exhibit areas.
The AFI Silver Theatre and Cultural Center presents film and video programming, augmented by filmmaker interviews, panels, discussions,and musical performances.
The AFI Directing Workshop for Women.
The Directing Workshop for Women is a training program committed to educating and mentoring participants in an effort to increase the number of women working professionally in screen directing. In this tuition-free program, each participant is required to complete a short film by the end of the year-long program.
Alumnae of the program include Maya Angelou, Anne Bancroft, Dyan Cannon, Ellen Burstyn, Jennifer Getzinger, Lesli Linka Glatter and Nancy Malone.
AFI Directors Series.
AFI released a set of hour-long programs reviewing the career of acclaimed directors. The Directors Series content was copyrighted in 1997 by Media Entertainment Inc and The American Film Institute, and the VHS and DVDs were released between 1999 and 2001 on Winstar TV and Video.
Directors featured included:

</doc>
<doc id="887" url="http://en.wikipedia.org/wiki?curid=887" title="MessagePad">
MessagePad

The MessagePad is the first series of personal digital assistant devices developed by Apple Computer for the Newton platform in 1993. Some electronic engineering and the manufacture of Apple's MessagePad devices was undertaken in Japan by the Sharp Corporation. The devices were based on the ARM 610 RISC processor and all featured handwriting recognition software and were developed and marketed by Apple. The devices ran the Newton OS.
Details.
Screen and input.
With the MessagePad 120 with Newton OS 2.0, the Newton Keyboard by Apple became available, which can also be used via the dongle on Newton devices with a Newton InterConnect port, most notably the Apple MessagePad 2000/2100 series, as well as the Apple eMate 300.
Newton devices featuring Newton OS 2.1 or higher can be used with the screen turned horizontally ("landscape") as well as vertically ("portrait"). A change of a setting rotates the contents of the display by 90, 180 or 270 degrees. Handwriting recognition still works properly with the display rotated, although display calibration is needed when rotation in any direction is used for the first time or when the Newton device is reset.
Handwriting recognition.
In initial versions (Newton OS 1.x) the handwriting recognition gave extremely mixed results for users and was sometimes inaccurate. The original handwriting recognition engine was called Calligrapher, and was licensed from a Russian company called Paragraph International. Calligrapher's design was quite sophisticated; it attempted to learn the user's natural handwriting, using a database of known words to make guesses as to what the user was writing, and could interpret writing anywhere on the screen, whether hand-printed, in cursive, or a mix of the two. By contrast, Palm Pilot's Graffiti had a less sophisticated design than Calligrapher, but was sometimes found to be more accurate and precise due to its reliance on a fixed, predefined stroke alphabet. The stroke alphabet used letter shapes which resembled standard handwriting, but which were modified to be both simple and very easy to differentiate. Palm Computing also released two versions of Graffiti for Newton devices. Ironically, the Newton version sometimes performed better and could also show strokes as they were being written as input was done on the display itself, rather than on a silkscreen area.
For editing text, Newton had a very intuitive system for handwritten editing, such as scratching out words to be deleted, circling text to be selected, or using written carets to mark inserts.
Later releases of the Newton operating system retained the original recognizer for compatibility, but added a hand-printed-text-only (not cursive) recognizer, called "Rosetta", which was developed by Apple, included in version 2.0 of the Newton operating system, and refined in Newton 2.1. Rosetta is generally considered a significant improvement and many reviewers, testers, and most users consider the Newton 2.1 handwriting recognition software better than any of the alternatives even 10 years after it was introduced. Recognition and computation of handwritten horizontal and vertical formulas such as "1 + 2 =" was also under development but never released. However, users wrote similar programs which could evaluate mathematical formulas using the Newton OS Intelligent Assistant, a unique part of every Newton device.
The handwriting recognition and parts of the user interface for the Newton are best understood in the context of the broad history of Pen computing, which is quite extensive.
A vital feature of the Newton handwriting recognition system is the modeless error correction. That is, correction done in situ without using a separate window or widget, using a minimum of gestures. If a word is recognized improperly, the user could double-tap the word and a list of alternatives would pop up in a menu under the stylus. Most of the time, the correct word will be in the list. If not, a button at the bottom of the list allows the user to edit individual characters in that word. Other pen gestures could do such things as transpose letters (also in situ). The correction popup also allowed the user to revert to the original, un-recognized letter shapes - this would be useful in note-taking scenarios if there was insufficient time to make corrections immediately. To conserve memory and storage space, alternative recognition hypotheses would not be saved indefinitely. If the user returned to a note a week later, for example, they would only see the best match. Error correction in many current handwriting systems provides such functionality but adds more steps to the process, greatly increasing the interruption to a user's workflow that a given correction requires.
User interface.
Text could also be entered by tapping with the stylus on a small on-screen pop-up QWERTY virtual keyboard, although more layouts were developed by users. Newton devices could also accept free-hand "Sketches", "Shapes", and "Ink Text", much like a desktop computer graphics tablet. With "Shapes", Newton could recognize that the user was attempting to draw a circle, a line, a polygon, etc., and it would clean them up into perfect vector representations (with modifiable control points and defined vertices) of what the user was attempting to draw. "Shapes" and "Sketches" could be scaled or deformed once drawn. "Ink text" captured the user's free-hand writing but allowed it to be treated somewhat like recognized text when manipulating for later editing purposes ("ink text" supported word wrap, could be formatted to be bold, italic, etc.). At any time a user could also direct their Newton device to recognize selected "ink text" and turn it into recognized text (deferred recognition). A Newton note (or the notes attached to each contact in Names and each Dates calendar or to-do event) could contain any mix of interleaved text, Ink Text, Shapes, and Sketches.
Connectivity.
The MessagePad 100 series of devices used Macintosh's proprietary serial ports—round Mini-DIN 8 connectors. The MessagePad 2000/2100 models (as well as the eMate 300) have a small, proprietary "Newton InterConnect" port. However, the development of the Newton hardware/software platform was canceled by Steve Jobs on February 27, 1998, so the InterConnect port, while itself very advanced, can only be used to connect a serial dongle. A prototype multi-purpose InterConnect device containing serial, audio in, audio out, and other ports was also discovered. In addition, all Newton devices have infrared connectivity, initially only the Sharp ASK protocol, but later also IrDA, though the Sharp ASK protocol was kept in for compatibility reasons. Unlike the Palm Pilot, all Newton devices are equipped with a standard PC Card expansion slot (two on the 2000/2100). This allows native modem and even Ethernet connectivity; Newton users have also written drivers for 802.11b wireless networking cards and ATA-type flash memory cards (including the popular CompactFlash format), as well as for Bluetooth cards. Newton can also dial a phone number through the built-in speaker of the Newton device by simply holding a telephone handset up to the speaker and transmitting the appropriate tones. Fax and printing support is also built in at the operating system level, although it requires peripherals such as parallel adapters, PCMCIA cards, or serial modems, the most notable of which is the lightweight Newton Fax Modem released by Apple in 1993. It is powered by 2 AA batteries, and can also be used with a power adapter. It provides data transfer at 2400 bit/s, and can also send and receive fax messages at 9600 and 4800 bit/s respectively.
Power options.
The original Apple MessagePad and MessagePad 100 used four AAA batteries. They were eventually replaced by AA batteries with the release of the Apple MessagePad 110.
The use of 4 AA NiCd (MessagePad 110, 120 and 130) and 4x AA NiMH cells (MP2x00 series, eMate 300) give a runtime of up to 30 hours (MP2100 with two 20 MB Linear Flash memory PC Cards, no backlight usage) and up to 24 hours with backlight on. While adding more weight to the handheld Newton devices than AAA batteries or custom battery packs, the choice of an easily replaceable/rechargeable cell format gives the user a still unsurpassed runtime and flexibility of power supply. This, together with the flash memory used as internal storage starting with the Apple MessagePad 120 (if all cells lost their power, no data was lost due to the non-volatility of this storage), gave birth to the slogan "Newton never dies, it only gets new batteries".
Later efforts and improvements.
The Apple MessagePad 2000/2100, with a vastly improved handwriting recognition system, 162 MHz StrongARM SA-110 RISC processor, Newton OS 2.1, and a better, clearer, backlit screen, attracted critical plaudits. 
Cases.
Apple and third parties marketed several "wallets" (cases) for the handheld Newton devices, which would hold them securely along with the owner's credit cards, driver's license, business cards, and cash. Most also protected the LCD screen. 
Market reception.
Fourteen months after Sculley demoed it at the May 1992, Chicago CES, the MessagePad was first offered for sale on August 2, 1993 at the Boston Macworld Expo. The hottest item at the show, it cost $900. 50,000 MessagePads were sold in the device’s first three months on the market.
The original Apple MessagePad and MessagePad 100 were limited by the very short lifetime of their inadequate AAA batteries.
Critics also panned the handwriting recognition that was available in the debut models, which had been trumpeted in the Newton's marketing campaign. It was this problem that was skewered in the Doonesbury comic strips in which a written text entry is (erroneously) translated as "Egg Freckles?", as well as in the animated television series The Simpsons. However, the word 'freckles' was not included in the Newton dictionary, although a user could add it themselves. Difficulties were in part caused by the long time requirements for the Calligrapher handwriting recognition software to "learn" the user's handwriting; this process could take anywhere from two weeks to two months.
Another factor which limited the early Newton devices' appeal was that desktop connectivity was not included in the basic retail package, a problem that was later solved with 2.x Newton devices - these were bundled with a serial cable and the appropriate Newton Connection Utilities software.
Later versions of Newton OS offered improved handwriting recognition, quite possibly a leading reason for the continued popularity of the devices among Newton users. Even given the age of the hardware and software, Newtons still demand a sale price on the used market far greater than that of comparatively aged PDAs produced by other companies. In 2006 CNET compared an Apple MessagePad 2000 to a Samsung Q1, and the Newton was declared better. In 2009, CNET compared an Apple MessagePad 2000 to an iPhone, and the Newton was still declared better.
A chain of dedicated Newton only stores called Newton Source existed from 1994 through 1998. Locations included NYC, Los Angeles, San Francisco, Chicago and Boston. The Westwood Village, California, near U.C.L.A. featured the trademark red and yellow lightbulb Newton logo in neon. The stores provided an informative educational venue to learn about the Newton platform in a hands on relaxed fashion. The stores had no traditional computer retail counters and featured oval desktops where interested users could become intimately involved with the Newton product range. The stores were a model for the later Apple Stores. 
Newton device models.
Notes: The eMate 300 actually has ROM chips silk screened with 2.2 on them. Stephanie Mak on her website discusses this:
If one removes all patches to the eMate 300 (by replacing the ROM chip, and then putting in the original one again, as the eMate and the MessagePad 2000/2100 devices erase their memory completely after replacing the chip), the result will be the Newton OS saying that this is version 2.2.00. Also, the Original MessagePad and the MessagePad 100 share the same model number, as they only differ in the ROM chip version. (The OMP has OS versions 1.0 to 1.05, or 1.10 to 1.11, while the MP100 has 1.3 that can be upgraded with various patches.)
Timeline of Newton models.
 
Other uses.
[[File:PSAINS InteractStation7.jpg|250px|thumb|right|Petrosains uses Newton technology.]]
There were a number of projects that used the Newton as a portable information device in cultural settings such as museums. For example, Visible Interactive created a walking tour in San Francisco's Chinatown but the most significant effort took place in [[Malaysia]] at the [[Petronas]] Discovery Center, known as Petrosains.
In 1995, an exhibit design firm, DMCD Inc., was awarded the contract to design a new 100,000 sqft science museum in the [[Petronas Towers]] in Kuala Lumpur. A major factor in the award was the concept that visitors would use a Newton device to access additional information, find out where they were in the museum, listen to audio, see animations, control robots and other media, and to bookmark information for printout at the end of the exhibit.
The device became known as the ARIF, a Malay word for "wise man" or "seer" and it was also an acronym for A Resourceful Informative Friend. Some 400 ARIFS were installed and over 300 are still in use today. The development of the ARIF system was extremely complex and required a team of hardware and software engineers, designers, and writers. ARIF is an ancestor of the PDA systems used in museums today and it boasted features that have not been attempted since.
The Newton was also used in healthcare applications, for example in collecting data directly from patients. Newtons were used as electronic diaries, with patients entering their symptoms and other information concerning their health status on a daily basis. The compact size of the device and its ease of use made it possible for the electronic diaries to be carried around and used in the patients' everyday life setting. This was an early example of [[electronic patient-reported outcome]]s ([[ePRO]])
External links.
Reviews.
[[Category:Apple Newton]]
[[Category:Products introduced in 1993]]
[[Category:Apple personal digital assistants]]

</doc>
<doc id="904" url="http://en.wikipedia.org/wiki?curid=904" title="Aluminium">
Aluminium

Aluminium (or aluminum; see spelling differences) is a chemical element in the boron group with symbol Al and atomic number 13. It is a silvery-white, soft, nonmagnetic, ductile metal. Aluminium is the third most abundant element (after oxygen and silicon), and the most abundant metal in the Earth's crust. It makes up about 8% by weight of the Earth's solid surface. Aluminium metal is so chemically reactive that native specimens are rare and limited to extreme reducing environments. Instead, it is found combined in over 270 different minerals. The chief ore of aluminium is bauxite.
Aluminium is remarkable for the metal's low density and for its ability to resist corrosion due to the phenomenon of passivation. Structural components made from aluminium and its alloys are vital to the aerospace industry and are important in other areas of transportation and structural materials. The most useful compounds of aluminium, at least on a weight basis, are the oxides and sulfates.
Despite its prevalence in the environment, no known form of life uses aluminium salts metabolically. In keeping with its pervasiveness, aluminium is well tolerated by plants and animals. Owing to their prevalence, potential beneficial (or otherwise) biological roles of aluminium compounds are of continuing interest.
Characteristics.
Physical.
Aluminium is a relatively soft, durable, lightweight, ductile and malleable metal with appearance ranging from silvery to dull gray, depending on the surface roughness. It is nonmagnetic and does not easily ignite. A fresh film of aluminium serves as a good reflector (approximately 92%) of visible light and an excellent reflector (as much as 98%) of medium and far infrared radiation. The yield strength of pure aluminium is 7–11 MPa, while aluminium alloys have yield strengths ranging from 200 MPa to 600 MPa. Aluminium has about one-third the density and stiffness of steel. It is easily machined, cast, drawn and extruded.
Aluminium atoms are arranged in a face-centered cubic (fcc) structure. Aluminium has a stacking-fault energy of approximately 200 mJ/m2.
Aluminium is a good thermal and electrical conductor, having 59% the conductivity of copper, both thermal and electrical, while having only 30% of copper's density. Aluminium is capable of being a superconductor, with a superconducting critical temperature of 1.2 kelvin and a critical magnetic field of about 100 gauss (10 milliteslas).
Chemical.
Corrosion resistance can be excellent due to a thin surface layer of aluminium oxide that forms when the metal is exposed to air, effectively preventing further oxidation. The strongest aluminium alloys are less corrosion resistant due to galvanic reactions with alloyed copper. This corrosion resistance is also often greatly reduced by aqueous salts, particularly in the presence of dissimilar metals.
In highly acidic solutions aluminium reacts with water to form hydrogen, and in highly alkaline ones to form aluminates— protective passivation under these conditions is negligible. Also, chlorides such as common sodium chloride are well-known sources of corrosion of aluminium and are among the chief reasons that household plumbing is never made from this metal.
However, owing to its resistance to corrosion generally, aluminium is one of the few metals that retain silvery reflectance in finely powdered form, making it an important component of silver-colored paints. Aluminium mirror finish has the highest reflectance of any metal in the 200–400 nm (UV) and the 3,000–10,000 nm (far IR) regions; in the 400–700 nm visible range it is slightly outperformed by tin and silver and in the 700–3000 (near IR) by silver, gold, and copper.
Aluminium is oxidized by water at temperatures below 280 °C to produce hydrogen, aluminium hydroxide and heat:
This conversion is of interest for the production of hydrogen. Challenges include circumventing the formed oxide layer, which inhibits the reaction and the expenses associated with the storage of energy by regeneration of the Al metal.
Isotopes.
Aluminium has many known isotopes, whose mass numbers range from 21 to 42; however, only 27Al (stable isotope) and 26Al (radioactive isotope, t1⁄2 = 7.2×105 y) occur naturally. 27Al has a natural abundance above 99.9%. 26Al is produced from argon in the atmosphere by spallation caused by cosmic-ray protons. Aluminium isotopes have found practical application in dating marine sediments, manganese nodules, glacial ice, quartz in rock exposures, and meteorites. The ratio of 26Al to 10Be has been used to study the role of transport, deposition, sediment storage, burial times, and erosion on 105 to 106 year time scales. Cosmogenic 26Al was first applied in studies of the Moon and meteorites. Meteoroid fragments, after departure from their parent bodies, are exposed to intense cosmic-ray bombardment during their travel through space, causing substantial 26Al production. After falling to Earth, atmospheric shielding drastically reduces 26Al production, and its decay can then be used to determine the meteorite's terrestrial age. Meteorite research has also shown that 26Al was relatively abundant at the time of formation of our planetary system. Most meteorite scientists believe that the energy released by the decay of 26Al was responsible for the melting and differentiation of some asteroids after their formation 4.55 billion years ago.
Natural occurrence.
Stable aluminium is created when hydrogen fuses with magnesium either in large stars or in supernovae.
In the Earth's crust, aluminium is the most abundant (8.3% by weight) metallic element and the third most abundant of all elements (after oxygen and silicon). Because of its strong affinity to oxygen, it is almost never found in the elemental state; instead it is found in oxides or silicates. Feldspars, the most common group of minerals in the Earth's crust, are aluminosilicates. Native aluminium metal can only be found as a minor phase in low oxygen fugacity environments, such as the interiors of certain volcanoes. Native aluminium has been reported in cold seeps in the northeastern continental slope of the South China Sea and Chen "et al." (2011) have proposed a theory of its origin as resulting by reduction from tetrahydroxoaluminate Al(OH)4− to metallic aluminium by bacteria.
It also occurs in the minerals beryl, cryolite, garnet, spinel and turquoise. Impurities in Al2O3, such as chromium or iron yield the gemstones ruby and sapphire, respectively.
Although aluminium is an extremely common and widespread element, the common aluminium minerals are not economic sources of the metal. Almost all metallic aluminium is produced from the ore bauxite (AlO"x"(OH)3–2"x"). Bauxite occurs as a weathering product of low iron and silica bedrock in tropical climatic conditions. Large deposits of bauxite occur in Australia, Brazil, Guinea and Jamaica and the primary mining areas for the ore are in Australia, Brazil, China, India, Guinea, Indonesia, Jamaica, Russia and Suriname.
Production and refinement.
Bauxite is converted to aluminium oxide (Al2O3) via the Bayer process. Relevant chemical equations are:
The intermediate sodium aluminate, given the simplified formula NaAlO2, is soluble in strongly alkaline water, and the other components of the ore are not. Depending on the quality of the bauxite ore, twice as much waste ("red mud") is generated compared to the amount of alumina.
The conversion of alumina to aluminium metal is achieved by the Hall-Héroult process. In this energy-intensive process, a solution of alumina in a molten (950 and) mixture of cryolite (Na3AlF6) with calcium fluoride is electrolyzed to give the metal:
At the anode, oxygen is formed:
The aluminium metal then sinks to the bottom of the solution and is tapped off, usually cast into large blocks called aluminium billets for further processing. To some extent, the carbon anode is consumed by subsequent reaction with oxygen to form carbon dioxide. The anodes in a reduction cell must therefore be replaced regularly, since they are consumed in the process. The cathodes do erode, mainly due to electrochemical processes and metal movement. After five to ten years, depending on the current used in the electrolysis, a cell must be rebuilt because of cathode wear.
Aluminium electrolysis with the Hall-Héroult process consumes a lot of energy. The worldwide average specific energy consumption is approximately 15±0.5 kilowatt-hours per kilogram of aluminium produced (52 to 56 MJ/kg). The most modern smelters achieve approximately 12.8 kW·h/kg (46.1 MJ/kg). (Compare this to the heat of reaction, 31 MJ/kg, and the Gibbs free energy of reaction, 29 MJ/kg.) Reduction line currents for older technologies are typically 100 to 200 kiloamperes; state-of-the-art smelters operate at about 350 kA. Trials have been reported with 500 kA cells.
The Hall-Heroult process produces aluminium with a purity of above 99%. Further purification can be done by the Hoopes process. The process involves the electrolysis of molten aluminium with a sodium, barium and aluminium fluoride electrolyte. The resulting aluminium has a purity of 99.99%.
Electric power represents about 20% to 40% of the cost of producing aluminium, depending on the location of the smelter. Aluminium production consumes roughly 5% of electricity generated in the U.S. Aluminium producers tend to locate smelters in places where electric power is both plentiful and inexpensive—such as the United Arab Emirates with its large natural gas supplies, and Iceland and Norway with energy generated from renewable sources. The world's largest smelters of alumina are People's Republic of China, Russia, and Quebec and British Columbia in Canada.
In 2005, the People's Republic of China was the top producer of aluminium with almost a one-fifth world share, followed by Russia, Canada, and the US, reports the British Geological Survey.
Over the last 50 years, Australia has become the world's top producer of bauxite ore and a major producer and exporter of alumina (before being overtaken by China in 2007). Australia produced 77 million tonnes of bauxite in 2013. The Australian deposits have some refining problems, some being high in silica, but have the advantage of being shallow and relatively easy to mine.
Recycling.
Aluminium is theoretically 100% recyclable without any loss of its natural qualities. According to the International Resource Panel's Metal Stocks in Society report, the global per capita stock of aluminium in use in society (i.e. in cars, buildings, electronics etc.) is 80 kg. Much of this is in more-developed countries (350 – per capita) rather than less-developed countries (35 kg per capita). Knowing the per capita stocks and their approximate lifespans is important for planning recycling.
Recovery of the metal via recycling has become an important use of the aluminium industry. Recycling was a low-profile activity until the late 1960s, when the growing use of aluminium beverage cans brought it to the public awareness.
Recycling involves melting the scrap, a process that requires only 5% of the energy used to produce aluminium from ore, though a significant part (up to 15% of the input material) is lost as dross (ash-like oxide). An aluminium stack melter produces significantly less dross, with values reported below 1%. The dross can undergo a further process to extract aluminium.
In Europe aluminium experiences high rates of recycling, ranging from 42% of beverage cans, 85% of construction materials and 95% of transport vehicles.
Recycled aluminium is known as secondary aluminium, but maintains the same physical properties as primary aluminium. Secondary aluminium is produced in a wide range of formats and is employed in 80% of alloy injections. Another important use is for extrusion.
White dross from primary aluminium production and from secondary recycling operations still contains useful quantities of aluminium that can be extracted industrially. The process produces aluminium billets, together with a highly complex waste material. This waste is difficult to manage. It reacts with water, releasing a mixture of gases (including, among others, hydrogen, acetylene, and ammonia), which spontaneously ignites on contact with air; contact with damp air results in the release of copious quantities of ammonia gas. Despite these difficulties, the waste has found use as a filler in asphalt and concrete.
Compounds.
See also: .
Oxidation state +3.
The vast majority of compounds, including all Al-containing minerals and all commercially significant aluminium compounds, feature aluminium in the oxidation state 3+. The coordination number of such compounds varies, but generally Al3+ is six-coordinate or tetracoordinate. Almost all compounds of aluminium(III) are colorless.
Halides.
All four trihalides are well known. Unlike the structures of the three heavier trihalides, aluminium fluoride (AlF3) features six-coordinate Al. The octahedral coordination environment for AlF3 is related to the compactness of fluoride ion, six of which can fit around the small Al3+ center. AlF3 sublimes (with cracking) at 1291 °C. With heavier halides, the coordination numbers are lower. The other trihalides are dimeric or polymeric with tetrahedral Al centers. These materials are prepared by treating aluminium metal with the halogen, although other methods exist. Acidification of the oxides or hydroxides affords hydrates. In aqueous solution, the halides often form mixtures, generally containing six-coordinate Al centers, which are feature both halide and aquo ligands. When aluminium and fluoride are together in aqueous solution, they readily form complex ions such as [AlF(H2O)5]2+, AlF3(H2O)3, and [AlF6]3-. In the case of chloride, polyaluminium clusters are formed such as [Al13O4(OH)24(H2O)12]7+.
Oxide and hydroxides.
Aluminium forms one stable oxide, known by its mineral name corundum. Sapphire and ruby are impure corundum contaminated with trace amounts of other metals. The two oxide-hydroxides, AlO(OH), are boehmite and diaspore. There are three trihydroxides: bayerite, gibbsite, and nordstrandite, which differ in their crystalline structure (polymorphs). Most are produced from ores by a variety of wet processes using acid and base. Heating the hydroxides leads to formation of corundum. These materials are of central importance to the production of aluminium and are themselves extremely useful.
Carbide, nitride, and related materials.
Aluminium carbide (Al4C3) is made by heating a mixture of the elements above 1000 °C. The pale yellow crystals consist of tetrahedral aluminium centers. It reacts with water or dilute acids to give methane. The acetylide, Al2(C2)3, is made by passing acetylene over heated aluminium.
Aluminium nitride (AlN) is the only nitride known for aluminium. Unlike the oxides it features tetrahedral Al centers. It can be made from the elements at 800 °C. It is air-stable material with a usefully high thermal conductivity. Aluminium phosphide (AlP) is made similarly, and hydrolyses to give phosphine:
Organoaluminium compounds and related hydrides.
A variety of compounds of empirical formula AlR3 and AlR1.5Cl1.5 exist. These species usually feature tetrahedral Al centers, e.g. "trimethylaluminium" has the formula Al2(CH3)6 (see figure). With large organic groups, triorganoaluminium exist as three-coordinate monomers, such as triisobutylaluminium. Such compounds are widely used in industrial chemistry, despite the fact that they are often highly pyrophoric. Few analogues exist between organoaluminium and organoboron compounds except for large organic groups.
The important aluminium hydride is lithium aluminium hydride (LiAlH4), which is used in as a reducing agent in organic chemistry. It can be produced from lithium hydride and aluminium trichloride:
Several useful derivatives of LiAlH4 are known, e.g. sodium bis(2-methoxyethoxy)dihydridoaluminate. The simplest hydride, aluminium hydride or alane, remains a laboratory curiosity. It is a polymer with the formula (AlH3)"n", in contrast to the corresponding boron hydride with the formula (BH3)2.
Oxidation states +1 and +2.
Although the great majority of aluminium compounds feature Al3+ centers, compounds with lower oxidation states are known and sometime of significance as precursors to the Al3+ species.
Aluminium(I).
AlF, AlCl and AlBr exist in the gaseous phase when the trihalide is heated with aluminium. The composition AlI is unstable at room temperature with respect to the triiodide:
A stable derivative of aluminium monoiodide is the cyclic adduct formed with triethylamine, Al4I4(NEt3)4. Also of theoretical interest but only of fleeting existence are Al2O and Al2S. Al2O is made by heating the normal oxide, Al2O3, with silicon at 1800 °C in a vacuum. Such materials quickly disproportionates to the starting materials.
Aluminium(II).
Very simple Al(II) compounds are invoked or observed in the reactions of Al metal with oxidants. For example, aluminium monoxide, AlO, has been detected in the gas phase after explosion and in stellar absorption spectra. More thoroughly investigated are compounds of the formula R4Al2 which contain an Al-Al bond and where R is a large organic ligand.
Analysis.
The presence of aluminium can be detected in qualitative analysis using aluminon.
Applications.
General use.
Aluminium is the most widely used non-ferrous metal. Global production of aluminium in 2005 was 31.9 million tonnes. It exceeded that of any other metal except iron (837.5 million tonnes). Forecast for 2012 is 42–45 million tonnes, driven by rising Chinese output.
Aluminium is almost always alloyed, which markedly improves its mechanical properties, especially when tempered. For example, the common aluminium foils and beverage cans are alloys of 92% to 99% aluminium. The main alloying agents are copper, zinc, magnesium, manganese, and silicon (e.g., duralumin) and the levels of these other metals are in the range of a few percent by weight.
Some of the many uses for aluminium metal are in:
Aluminium is usually alloyed – it is used as pure metal only when corrosion resistance and/or workability is more important than strength or hardness. A thin layer of aluminium can be deposited onto a flat surface by physical vapor deposition or (very infrequently) chemical vapor deposition or other chemical means to form optical coatings and mirrors.
Aluminium compounds.
Because aluminium is abundant and most of its derivatives exhibit low toxicity, the compounds of aluminium enjoy wide and sometimes large-scale applications.
Alumina.
Aluminium oxide (Al2O3) and the associated oxy-hydroxides and trihydroxides are produced or extracted from minerals on a large scale. The great majority of this material is converted to metallic aluminium. In 2013 about 10% of the domestic shipments in the United States were used for other applications. A major use is as an absorbent. For example, alumina removes water from hydrocarbons, which enables subsequent processes that are poisoned by moisture. Aluminium oxides are common catalysts for industrial processes, e.g. the Claus process for converting hydrogen sulfide to sulfur in refineries and for the alkylation of amines. Many industrial catalysts are "supported", meaning generally that an expensive catalyst (e.g., platinum) is dispersed over a high surface area material such as alumina. Being a very hard material (Mohs hardness 9), alumina is widely used as an abrasive and the production of applications that exploit its inertness, e.g., in high pressure sodium lamps.
Sulfates.
Several sulfates of aluminium find applications. Aluminium sulfate (Al2(SO4)3·(H2O)18) is produced on the annual scale of several billions of kilograms. About half of the production is consumed in water treatment. The next major application is in the manufacture of paper. It is also used as a mordant, in fire extinguisher, as a food additive, in fireproofing, and in leather tanning. Aluminium ammonium sulfate, which is also called ammonium alum, (NH4)Al(SO4)2·12H2O, is used as a mordant and in leather tanning. Aluminium potassium sulfate ([Al(K)](SO4)2)·(H2O)12 is used similarly. The consumption of both alums is declining.
Chlorides.
Aluminium chloride (AlCl3) is used in petroleum refining and in the production of synthetic rubber and polymers. Although it has a similar name, aluminium chlorohydrate has fewer and very different applications, e.g. as a hardening agent and an antiperspirant. It is an intermediate in the production of aluminium metal.
Niche compounds.
Given the scale of aluminium compounds, a small scale application could still involve thousands of tonnes. One of the many compounds used at this intermediate level include aluminium acetate, a salt used in solution as an astringent. Aluminium borate (Al2O3·B2O3) is used in the production of glass and ceramics. Aluminium fluorosilicate (Al2(SiF6)3) is used in the production of synthetic gemstones, glass and ceramic. Aluminium phosphate (AlPO4) is used in the manufacture: of glass and ceramic, pulp and paper products, cosmetics, paints and varnishes and in making dental cement. Aluminium hydroxide (Al(OH)3) is used as an antacid, as a mordant, in water purification, in the manufacture of glass and ceramic and in the waterproofing of fabrics. Lithium aluminium hydride is a powerful reducing agent used in organic chemistry. Organoaluminiums are used as Lewis acids and cocatalysts. For example, methylaluminoxane is a cocatalyst for Ziegler-Natta olefin polymerization to produce vinyl polymers such as polyethene.
Aluminium alloys in structural applications.
Aluminium alloys with a wide range of properties are used in engineering structures. Alloy systems are classified by a number system (ANSI) or by names indicating their main alloying constituents (DIN and ISO).
The strength and durability of aluminium alloys vary widely, not only as a result of the components of the specific alloy, but also as a result of heat treatments and manufacturing processes. A lack of knowledge of these aspects has from time to time led to improperly designed structures and gained aluminium a bad reputation.
One important structural limitation of aluminium alloys is their fatigue strength. Unlike steels, aluminium alloys have no well-defined fatigue limit, meaning that fatigue failure eventually occurs, under even very small cyclic loadings. This implies that engineers must assess these loads and design for a fixed life rather than an infinite life.
Another important property of aluminium alloys is their sensitivity to heat. Workshop procedures involving heating are complicated by the fact that aluminium, unlike steel, melts without first glowing red. Forming operations where a blow torch is used therefore require some expertise, since no visual signs reveal how close the material is to melting. Aluminium alloys, like all structural alloys, also are subject to internal stresses following heating operations such as welding and casting. The problem with aluminium alloys in this regard is their low melting point, which make them more susceptible to distortions from thermally induced stress relief. Controlled stress relief can be done during manufacturing by heat-treating the parts in an oven, followed by gradual cooling—in effect annealing the stresses.
The low melting point of aluminium alloys has not precluded their use in rocketry; even for use in constructing combustion chambers where gases can reach 3500 K. The Agena upper stage engine used a regeneratively cooled aluminium design for some parts of the nozzle, including the thermally critical throat region.
Another alloy of some value is aluminium bronze (Cu-Al alloy).
History.
Ancient Greeks and Romans used aluminium salts as dyeing mordants and as astringents for dressing wounds; alum is still used as a styptic.
In 1761, Guyton de Morveau suggested calling the base alum "alumine." In 1808, Humphry Davy identified the existence of a metal base of alum, which he at first termed "alumium" and later "aluminum" (see etymology section, below).
The metal was first produced in 1825 in an impure form by Danish physicist and chemist Hans Christian Ørsted. He reacted anhydrous aluminium chloride with potassium amalgam, yielding a lump of metal looking similar to tin. Friedrich Wöhler was aware of these experiments and cited them, but after redoing the experiments of Ørsted he concluded that this metal was pure potassium. He conducted a similar experiment in 1827 by mixing anhydrous aluminium chloride with potassium and yielded aluminium. Wöhler is generally credited with isolating aluminium (Latin "alumen", alum). Further, Pierre Berthier discovered aluminium in bauxite ore. Henri Etienne Sainte-Claire Deville improved Wöhler's method in 1846. As described in his 1859 book, aluminium trichloride could be reduced by sodium, which was more convenient and less expensive than potassium used by Wöhler. 
In the mid-1880s, aluminium metal was exceedingly difficult to produce, which made pure aluminium more valuable than gold. So celebrated was the metal that bars of aluminium were exhibited at the Exposition Universelle of 1855. Napoleon III of France is reputed to held a banquet where the most honored guests were given aluminium utensils, while the others made do with gold.
Aluminium was selected as the material to use for the 100 oz capstone of the Washington Monument in 1884, a time when one ounce (30 grams) cost the daily wage of a common worker on the project (in 1884 about $1 for 10 hours of labor; today, a construction worker in the US working on such a project might earn $25–$35 per hour and therefore around $300 in an equivalent single 10-hour day). The capstone, which was set in place on 6 December 1884 in an elaborate dedication ceremony, was the largest single piece of aluminium cast at the time.
The Cowles companies supplied aluminium alloy in quantity in the United States and England using smelters like the furnace of Carl Wilhelm Siemens by 1886.
Hall-Heroult process: availability of cheap aluminium metal.
Charles Martin Hall of Ohio in the U.S. and Paul Héroult of France independently developed the Hall-Héroult electrolytic process that facilitated large-scale production of metallic aluminium. This process remains in use today. In 1888, with the financial backing of Alfred E. Hunt, the Pittsburgh Reduction Company started; today it is known as Alcoa. Héroult's process was in production by 1889 in Switzerland at Aluminium Industrie, now Alcan, and at British Aluminium, now Luxfer Group and Alcoa, by 1896 in Scotland.
By 1895, the metal was being used as a building material as far away as Sydney, Australia in the dome of the Chief Secretary's Building.
With the explosive expansion of the airplane industry during World War I (1914-1917), major governments demanded large shipments of aluminium for light, strong airframes. They often subsidized factories and the necessary electrical supply systems.
Many navies have used an aluminium superstructure for their vessels; the 1975 fire aboard USS "Belknap" that gutted her aluminium superstructure, as well as observation of battle damage to British ships during the Falklands War, led to many navies switching to all steel superstructures.
Aluminium wire was once widely used for domestic electrical wiring. Owing to corrosion-induced failures, a number of fires resulted.
Etymology.
Two variants of the metal's name are in current use, "aluminium" () and "aluminum" ()—besides the obsolete "alumium". The International Union of Pure and Applied Chemistry (IUPAC) adopted "aluminium" as the standard international name for the element in 1990 but, three years later, recognized "aluminum" as an acceptable variant. Hence their periodic table includes both. IUPAC internal publications use either spelling in nearly the same number.
Most countries use the spelling "aluminium". In the United States and Canada, the spelling "aluminum" predominates. The Canadian Oxford Dictionary prefers "aluminum", whereas the Australian Macquarie Dictionary prefers "aluminium". In 1926, the American Chemical Society officially decided to use "aluminum" in its publications; American dictionaries typically label the spelling "aluminium" as "chiefly British".
The various names all derive from its status as a base of alum. It is borrowed from Old French; its ultimate source, "alumen", in turn is a Latin word that literally means "bitter salt".
The earliest citation given in the Oxford English Dictionary for any word used as a name for this element is "alumium", which British chemist and inventor Humphry Davy employed in 1808 for the metal he was trying to isolate electrolytically from the mineral "alumina". The citation is from the journal "Philosophical Transactions of the Royal Society of London": "Had I been so fortunate as to have obtained more certain evidences on this subject, and to have procured the metallic substances I was in search of, I should have proposed for them the names of silicium, alumium, zirconium, and glucium."
Davy settled on "aluminum" by the time he published his 1812 book "Chemical Philosophy": "This substance appears to contain a peculiar metal, but as yet Aluminum has not been obtained in a perfectly free state, though alloys of it with other metalline substances have been procured sufficiently distinct to indicate the probable nature of alumina." But the same year, an anonymous contributor to the "Quarterly Review," a British political-literary journal, in a review of Davy's book, objected to "aluminum" and proposed the name "aluminium", "for so we shall take the liberty of writing the word, in preference to aluminum, which has a less classical sound."
The "-ium" suffix conformed to the precedent set in other newly discovered elements of the time: potassium, sodium, magnesium, calcium, and strontium (all of which Davy isolated himself). Nevertheless, "-um" spellings for elements were not unknown at the time, as for example platinum, known to Europeans since the 16th century, molybdenum, discovered in 1778, and tantalum, discovered in 1802. The "-um" suffix is consistent with the universal spelling alumina for the oxide (as opposed to aluminia), as lanthana is the oxide of lanthanum, and magnesia, ceria, and thoria are the oxides of magnesium, cerium, and thorium respectively.
The "aluminum" spelling is used in the Webster's Dictionary of 1828. In his advertising handbill for his new electrolytic method of producing the metal in 1892, Charles Martin Hall used the "-um" spelling, despite his constant use of the "-ium" spelling in all the patents he filed between 1886 and 1903. Hall's domination of production of the metal ensured that "aluminum" became the standard English spelling in North America.
Health concerns.
Despite its widespread occurrence in nature, aluminium has no known function in biology. Aluminium salts are remarkably nontoxic, aluminium sulfate having an LD50 of 6207 mg/kg (oral, mouse), which corresponds to 500 grams for an 80 kg person. The extremely low acute toxicity notwithstanding, the health effects of aluminium are of interest in view of the widespread occurrence of the element in the environment and in commerce.
Some toxicity can be traced to deposition in bone and the central nervous system, which is particularly increased in patients with reduced renal function. Because aluminium competes with calcium for absorption, increased amounts of dietary aluminium may contribute to the reduced skeletal mineralization (osteopenia) observed in preterm infants and infants with growth retardation. In very high doses, aluminium is associated with altered function of the blood–brain barrier. A small percentage of people are allergic to aluminium and experience contact dermatitis, digestive disorders, vomiting or other symptoms upon contact or ingestion of products containing aluminium, such as antiperspirants and antacids. In those without allergies, aluminium is not as toxic as heavy metals, but there is evidence of some toxicity if it is consumed in amounts greater than 40 mg/day per kg of body mass. Although the use of aluminium cookware has not been shown to lead to aluminium toxicity in general, excessive consumption of antacids containing aluminium compounds and excessive use of aluminium-containing antiperspirants provide more significant exposure levels. Studies have shown that consumption of acidic foods or liquids with aluminium significantly increases aluminium absorption, and maltol has been shown to increase the accumulation of aluminium in nervous and osseous tissue. Furthermore, aluminium increases estrogen-related gene expression in human breast cancer cells cultured in the laboratory. The estrogen-like effects of these salts have led to their classification as a metalloestrogen.
The effects of aluminium in antiperspirants have been examined over the course of decades with little evidence of skin irritation. Nonetheless, its occurrence in antiperspirants, dyes (such as aluminium lake), and food additives has caused concern. Although there is little evidence that normal exposure to aluminium presents a risk to healthy adults, some studies point to risks associated with increased exposure to the metal. Aluminium in food may be absorbed more than aluminium from water. It is classified as a non-carcinogen by the US Department of Health and Human Services.
In case of suspected sudden intake of a large amount of aluminium, deferoxamine mesylate may be given to help eliminate it from the body by chelation.
Alzheimer's disease.
Aluminium has controversially been implicated as a factor in Alzheimer's disease. The Camelford water pollution incident involved a number of people consuming aluminium sulfate. Investigations of the long-term health effects are still ongoing, but elevated brain aluminium concentrations have been found in post-mortem examinations of victims, and further research to determine if there is a link with cerebral amyloid angiopathy has been commissioned.
According to the Alzheimer's Society, the medical and scientific opinion is that studies have not convincingly demonstrated a causal relationship between aluminium and Alzheimer's disease. Nevertheless, some studies, such as those on the PAQUID cohort, cite aluminium exposure as a risk factor for Alzheimer's disease. Some brain plaques have been found to contain increased levels of the metal. Research in this area has been inconclusive; aluminium accumulation may be a consequence of the disease rather than a causal agent. A new study, dated Feb 5 2015, links the brain translocation of alum particles to a Trojan horse mechanism, that obeys to CCL2, signaling the major inflammatory monocyte chemoattractant.
Effect on plants.
Aluminium is primary among the factors that reduce plant growth on acid soils. Although it is generally harmless to plant growth in pH-neutral soils, the concentration in acid soils of toxic Al3+ cations increases and disturbs root growth and function.
Most acid soils are saturated with aluminium rather than hydrogen ions. The acidity of the soil is therefore a result of hydrolysis of aluminium compounds. This concept of "corrected lime potential" to define the degree of base saturation in soils became the basis for procedures now used in soil testing laboratories to determine the "lime requirement" of soils.
Wheat's adaptation to allow aluminium tolerance is such that the aluminium induces a release of organic compounds that bind to the harmful aluminium cations. Sorghum is believed to have the same tolerance mechanism. The first gene for aluminium tolerance has been identified in wheat. It was shown that sorghum's aluminium tolerance is controlled by a single gene, as for wheat. This is not the case in all plants.
Biodegradation.
A Spanish scientific report from 2001 claimed that the fungus "Geotrichum candidum" consumes the aluminium in compact discs. However, other reports on it always refer back to the 2001 Spanish report and there is no supporting original research since that report. Better documented, the bacterium "Pseudomonas aeruginosa" and the fungus "Cladosporium resinae" are commonly detected in aircraft fuel tanks using kerosene-based fuels (not AV gas), and can degrade aluminium in cultures. However, this is not a matter of the bacteria or fungi directly attacking or consuming the aluminium, but rather a result of the microbes' waste having a corrosive nature.

</doc>
<doc id="1261" url="http://en.wikipedia.org/wiki?curid=1261" title="April 26">
April 26

April 26 is the day of the year in the Gregorian calendar.

</doc>
<doc id="1304" url="http://en.wikipedia.org/wiki?curid=1304" title="Abduction">
Abduction

Abduction may refer to:

</doc>
<doc id="1309" url="http://en.wikipedia.org/wiki?curid=1309" title="Almost all">
Almost all

In mathematics, the phrase "almost all" has a number of specialised uses which extend its intuitive meaning.
Cardinality.
"Almost all" is sometimes used synonymously with "all but [except] finitely many" (formally, a cofinite set) or "all but a countable set" (formally, a cocountable set); see almost.
A simple example is that almost all prime numbers are odd, which is based on the fact that all but one prime number are odd. (The exception is the number 2, which is prime but not odd.)
Measure theory.
When speaking about the reals, sometimes it means "all reals but a set of Lebesgue measure zero" (formally, almost everywhere). In this sense almost all reals are not a member of the Cantor set even though the Cantor set is uncountable.
More generally, "almost all" is sometimes used in the sense of "almost everywhere" in measure theory, or in the closely related sense of "almost surely" in probability theory.
Number theory.
In number theory, if "P"("n") is a property of positive integers, and if "p"("N") denotes the number of positive integers "n" less than "N" for which "P"("n") holds, and if 
(see limit), then we say that ""P"("n") holds for almost all positive integers "n"" (formally, asymptotically almost surely) and write
For example, the prime number theorem states that the number of prime numbers less than or equal to "N" is asymptotically equal to "N"/ln "N". Therefore the proportion of prime integers is roughly 1/ln "N", which tends to 0. Thus, "almost all" positive integers are composite (not prime), however there are still an infinite number of primes.

</doc>
<doc id="1315" url="http://en.wikipedia.org/wiki?curid=1315" title="Abbey">
Abbey

An abbey (from Latin "abbatia," from Latin "abbās", derived from Aramaic "abba," "father") has historically been a Catholic or, more recently, Anglican, monastery or convent, under the authority of an Abbot or an Abbess, who serves as the spiritual father or mother of the community.
The term can also refer to an establishment which has long ceased to function as an abbey, in some cases for centuries (for example, see Westminster Abbey below). There were many orders that had their own styles of abbeys. Among these were the primary orders such as Benedictine, Cistercian, Carthusian. However there were also the minor order such as the Dominicans, Franciscans, and Carmelites.
Origins.
The formation of communities dates from pre-Christian times, as witness the Essenes; but the earliest Christian monastic foundations of which there is definite knowledge were simply groups of huts without any orderly arrangement, erected about the abode of some solitary famous for holiness and asceticism, around whom had gathered a knot of disciples anxious to learn his doctrine and to imitate his way of life.
In the earliest age of Christian monasticism the ascetics were accustomed to live singly, independent of one another, not far from some village church, supporting themselves by the labour of their own hands, and distributing the surplus after the supply of their own scanty wants to the poor. Increasing religious fervour, aided by persecution, drove them farther and farther away from the civilization into mountain solitudes or lonely deserts. The deserts of Egypt swarmed with the "cells" or huts of these anchorites. Anthony the Great, who had retired to the Egyptian Thebaid during the persecution of Maximian, AD 312, was the most celebrated among them for his austerities, his sanctity, and his power as an exorcist. His fame collected round him a host of followers imitating his asceticism in an attempt to imitate his sanctity. The deeper he withdrew into the wilderness, the more numerous his disciples became. They refused to be separated from him, and built their cells round that of their spiritual father. Thus arose the first monastic community, consisting of anchorites living each in his own little dwelling, united together under one superior. Anthony, as Johann August Wilhelm Neander remarks, without any conscious design of his own, had become the founder of a new mode of living in common, Coenobitism.
Pachomius.
At Tabennae on the Nile, in Upper Egypt, however, St. Pachomius laid the foundations of the coenobitical life, arranging everything in an organized manner. He built several monasteries, each containing about 1,600 separate cells laid out in lines as an encampment, where the monks slept and performed some of their manual tasks; but there were large halls for their common needs, as the church, refectory, kitchen, even an infirmary and a guest-house. An enclosure protecting all these buildings gave the settlement the appearance of a walled village. It was this arrangement of monasteries, inaugurated by St. Pachomius, which finally spread throughout Palestine, and received the name of laurae, that is "lanes" or "alleys." In addition to these congregations of solitaries, all living in huts apart, there were caenobia, monasteries wherein the inmates lived a common life, none of them being permitted to retire to the cells of a laurae before they had therein undergone a lengthy period of training. In time this form of common life superseded that of the older laurae.
Palladius, who visited the Egyptian monasteries about the close of the 4th century, found among the 300 members of the coenobium of Panopolis, under the Pachomian rule, 15 tailors, 7 smiths, 4 carpenters, 12 cameldrivers and 15 tanners. Each separate community had its own oeconomus or steward, who was subject to a chief steward stationed at the head establishment. All the produce of the monks' labour was committed to him, and by him shipped to Alexandria. The money raised by the sale was expended in the purchase of stores for the support of the communities, and what was over was devoted to charity. Twice in the year the superiors of the several coenobia met at the chief monastery, under the presidency of an archimandrite ("the chief of the fold," from "miandra", a sheepfold), and at the last meeting gave in reports of their administration for the year. Details concerning the coenobia in the vicinity of Antioch are found in the writings of Chrysostom. The monks lived in separate huts, "kalbbia," forming a religious hamlet on the mountain side. They were subject to an abbot, and observed a common rule.
Great Lavra, Mount Athos.
The necessity for defence from attacks (for monastic houses tended to accumulate rich gifts), economy of space and convenience of access from one part of the community to another, by degrees dictated a more compact and orderly arrangement of the buildings of a monastic coenobium. Large piles of building were erected, with strong outside walls, capable of resisting the assaults of an enemy, within which all the necessary edifices were ranged round one or more open courts, usually surrounded with cloisters. The usual Eastern arrangement is exemplified in the plan of the convent of the Great Lavra, Mount Athos. 
This monastery, like the oriental monasteries generally, is surrounded by a strong and lofty blank stone wall, enclosing an area of between 3 and 4 acres (12,000 and 16,000 m²). The longer side extends to a length of about 500 ft. There is only one main entrance, on the north side (A), defended by three separate iron doors. Near the entrance is a large tower (M), a constant feature in the monasteries of the Levant. There is a small postern gate at L. The enceinte comprises two large open courts, surrounded with buildings connected with cloister galleries of wood or stone. The outer court, which is much the larger, contains the granaries and storehouses (K), and the kitchen (H) and other offices connected with the refectory (G). Immediately adjacent to the gateway is a two-storied guest-house, opening from a cloister (C). The inner court is surrounded by a cloister (EE), from which open the monks' cells (II). In the centre of this court stands the katholikon or conventual church, a square building with an apse of the cruciform domical Byzantine type, approached by a domed narthex. In front of the church stands a marble fountain (F), covered by a dome supported on columns. Opening from the western side of the cloister, but actually standing in the outer court, is the refectory (G), a large cruciform building, about 100 ft each way, decorated within with frescoes of saints. At the upper end is a semicircular recess, recalling the triclinium of the Lateran Palace at Rome, in which is placed the seat of the hegumenos or abbot. This apartment is chiefly used as a hall of meeting, the oriental monks usually taking their meals in their separate cells.
Orders.
Benedictine monasteries.
Monasticism in the West owes its extension and development to Benedict of Nursia (born AD 480). His rule was diffused rapidly from the parent foundation on Monte Cassino, the first abbey (529), through the whole of western Europe, and every country witnessed the erection of monasteries far exceeding anything that had yet been seen in spaciousness and splendour. Few great towns in Italy were without their Benedictine convent, and they quickly rose in all the great centres of population in England, France and Spain. Many monasteries were founded between AD 520 and 700. Before the Council of Constance, AD 1415, no fewer than 15,070 abbeys had been established of this order alone. No special plan was adopted or followed in the building of the first caenobia. The monks simply copied the buildings familiar to them, the Roman house or villa, whose plan, throughout the extent of the Roman Empire, was practically uniform. The founders of monasteries had often merely to install a community in an already existing villa. When they had to build, the natural instinct was to copy old models. If they fixed upon a site with existing buildings in good repair, they simply adapted them to their requirements, as St. Benedict did at Monte Cassino. The spread of the monastic life gradually effected great changes in the model of the Roman villa. The various avocations followed by the monks required suitable buildings, which were at first erected not upon any premeditated plan, but just as the need for them arose. These requirements, however, being practically the same in every country, resulted in practically similar arrangements everywhere. The buildings of a Benedictine abbey were uniformly arranged after one plan, modified where necessary to accommodate the arrangement to local circumstances.
The plan of the great Abbey of Saint Gall, erected about AD 719, indicates the general arrangement of a monastery of the first class towards the early part of the 9th century. According to architect Robert Willis, the general appearance of the convent is that of a town of isolated houses with streets running between them. It was planned in compliance with the Benedictine rule, which enjoined that, if possible, the monastery should contain every necessity of life. It should comprise a mill, a bakehouse, stables, and cow-houses, so that the monks had no need to go outside.
The general distribution of the buildings may be thus described:-The church, with its cloister to the south, occupies the centre of a quadrangular area, about 430 ft square. The buildings, as in all great monasteries, are distributed into groups. The church forms the nucleus, as the centre of the religious life of the community. In closest connection with the church is the group of buildings appropriated to the monastic line and its daily requirements---the refectory for eating, the dormitory for sleeping, the common room for social intercourse, the chapter-house for religious and disciplinary conference. These essential elements of monastic life are ranged about a cloister court, surrounded by a covered arcade, affording communication sheltered from the elements between the various buildings. The infirmary for sick monks, with the physician's house and physic garden, lies to the east. In the same group with the infirmary is the school for the novices. The outer school, with its headmaster's house against the opposite wall of the church, stands outside the convent enclosure, in close proximity to the abbot's house, that he might have a constant eye over them. The buildings devoted to hospitality are divided into three groups,--one for the reception of distinguished guests, another for monks visiting the monastery, a third for poor travellers and pilgrims. The first and third are placed to the right and left of the common entrance of the monastery,---the hospitium for distinguished guests being placed on the north side of the church, not far from the abbot's house; that for the poor on the south side next to the farm buildings. The monks are lodged in a guest-house built against the north wall of the church. The group of buildings connected with the material wants of the establishment is placed to the south and west of the church, and is distinctly separated from the monastic buildings. The kitchen, buttery and offices are reached by a passage from the west end of the refectory, and are connected with the bakehouse and brewhouse, which are placed still farther away. The whole of the southern and western sides is devoted to workshops, stables and farm-buildings. The buildings, with some exceptions, seem to have been of one story only, and all but the church were probably erected of wood. The whole includes thirty-three separate blocks. The church (D) is cruciform, with a nave of nine bays, and a semicircular apse at either extremity. That to the west is surrounded by a semicircular colonnade, leaving an open "paradise" (E) between it and the wall of the church. The whole area is divided by screens into various chapels. The high altar (A) stands immediately to the east of the transept, or ritual choir; the altar of Saint Paul (B) in the eastern, and that of St Peter (C) in the western apse. A cylindrical campanile stands detached from the church on either side of the western apse (FF).
The "cloister court", (G) on the south side of the nave of the
church has on its east side the "pisalis" or "calefactory", (H), the common sitting-room of the brethren, warmed by flues beneath the floor. On this side in later monasteries we invariably find the chapter house. It appears, however, from the inscriptions on the plan itself, that the north walk of the cloisters served for the purposes of a chapter-house, and was fitted up with benches on the long sides. Above the calefactory is the "dormitory" opening into the south transept of the church, to enable the monks to attend the nocturnal services with readiness, via the day-stair which lead to a cloister first or a night-stair which lead directly to the church. A passage at the other end leads to the "necessarium" (I). The southern side is occupied by the "refectory" (K), from the west end of which by a vestibule the kitchen (L) is reached. This is separated from the main buildings of the monastery, and is connected by a long passage with a building containing the bake house and brew house (M), and the sleeping-rooms of the servants. The upper story of the refectory is the "vestiarium," where the ordinary clothes of the brethren were kept. On the western side of the cloister is another two-story building (N). The cellar is below, and the larder and store-room above. Between this building and the church, opening by one door into the cloisters, and by another to the outer part of the monastery area, is the "parlour" for interviews with visitors from the external world (O). On the eastern side of the north transept is the "scriptorium" or writing-room (P1), with the library above.
To the east of the church stands a group of buildings comprising two miniature conventual establishments, each complete in itself. Each has a covered cloister surrounded by the usual buildings, i.e. refectory, dormitory, etc., and a church or chapel on one side, placed back to back. A detached building belonging to each contains a bath and a kitchen. One of these diminutive convents is appropriated to the "oblati" or novices (Q), the other to the sick monks as an "infirmary" (R).
The "residence of the physicians" (S) stands contiguous to the infirmary, and the physic garden (T) at the north-east corner of the monastery. Besides other rooms, it contains a drug store, and a chamber for those who are dangerously ill. The "house for bloodletting and purging" adjoins it on the west (U).
The "outer school," to the north of the convent area, contains a large schoolroom divided across the middle by a screen or partition, and surrounded by fourteen little rooms, termed the dwellings of the scholars. The head-master's house (W) is opposite, built against the side wall of the church. The two "hospitia" or guest-houses for the entertainment of strangers of different degrees (X1 X2) comprise a large common chamber or refectory in the centre, surrounded by sleeping-apartments. Each is provided with its own brewhouse and bakehouse, and that for travelers of a superior order has a kitchen and storeroom, with bedrooms for their servants and stables for their horses. There is also an "hospitium" for strange monks, abutting on the north wall of the church (Y).
Beyond the cloister, at the extreme verge of the convent area to the south, stands the "factory" (Z), containing workshops for shoemakers, saddlers (or shoemakers, sellarii), cutlers and grinders, trencher-makers, tanners, curriers, fullers, smiths and goldsmiths, with their dwellings in the rear. On this side we also find the farm buildings, the large granary and threshing-floor (a), mills (c), malthouse (d). Facing the west are the stables (e), ox-sheds (f), goatstables (gl, piggeries (h), sheep-folds
(i), together with the servants' and labourers' quarters (k). At the south-east corner we find the hen and duck house, and poultry-yard (m), and the dwelling of the keeper (n). Hard by is the kitchen garden (o), the beds bearing the names of the vegetables growing in them, onions, garlic, celery, lettuces, poppy, carrots, cabbages, etc., eighteen in all. In the same way the physic garden presents the names of the medicinal herbs, and the cemetery (p) those of the trees, apple, pear, plum, quince, etc., planted there.
Many of the present grand cathedrals were originally benedictine monasteries or abbeys. These were converted by Henry VIII and contain cloisters, chapter houses, and other abbatial buildings. Some of these are Canterbury, Chester, Durham, Ely, Gloucester, Norwich, Peterborough, Rochester, Winchester, and Worcester.
Every large monastery had depending upon it smaller foundations known as cells or priories. Sometimes these foundations were no more than a single building serving as residence and farm offices, while other examples were miniature monasteries for 5 or 10 monks. The outlying farming establishments belonging to the monastic foundations were known as villae or granges. They were usually staffed by lay-brothers, sometimes under the supervision of a single monk.
Westminster Abbey.
Westminster Abbey was founded in the 10th century by St. Dunstan and it shows hints of French architecture in its designs. It is another example of a great Benedictine abbey, identical in its general arrangements, so far as they can be traced, with those described above. 
The only traces of Dunstan's monastery to be seen today are in the round arches and massive supporting columns of the undercroft and the Pyx Chamber in the cloisters. The cloister and monastic buildings lie to the south side of the church. Parallel to the nave, on the south side of the cloister, was the refectory, with its lavatory at the door.
On the eastern side there are remains of the dormitory, raised on a vaulted substructure and communicating with the south transept. The chapter-house opens out of the same alley of the cloister. The small cloister lay to the south-east of the larger cloister, and still farther to the east we have the remains of the infirmary with the table hall, the refectory of those who were able to leave their chambers. The abbot's house formed a small courtyard at the west entrance, close to the inner gateway.
St. Mary's Abbey, York.
St Mary's Abbey, York, the largest and richest Benedictine establishment in the north of England, was first founded in 1055.
It exhibited the usual Benedictine arrangements. The entrance was by a strong gateway to the north. Close to the entrance was a chapel, where is now the church of St Olaf, in which the new-comers paid their devotions immediately on their arrival. Near the gate to the south was the guest-hall or hospitium. The buildings are completely ruined, but the walls of the nave and the cloisters, are still visible on the grounds of Yorkshire Museum. The precincts were surrounded by a strong fortified wall on three sides, the river Ouse being sufficient protection on the fourth side. The stone walls still exist and are one of the best surviving examples of abbey walls which remain in the country.
Abbey of Cluny.
The Abbey of Cluny was founded by William I, Duke of Aquitaine in 910, and was noted for its strict observance of the Rule of St. Benedict. The Abbey was built in the Romanesque style.
Reforms adopted at Cluny resulted in many departures from precedent, chief among which was a highly centralized form of government entirely foreign to Benedictine tradition. The reform quickly spread beyond the limits of the Abbey of Cluny, partly by the founding of new houses and partly by the incorporation of those already existing. By the twelfth century Cluny was at the head of an order consisting of some 314 monasteries.
The abbey-church of Cluny was on a scale commensurate with the greatness of the congregation, and was regarded as one of the wonders of the Middle Ages. It was no less than 555 feet in length, and was the largest church in Christendom until the erection of St. Peter's at Rome. It consisted of five naves, a narthex, or ante-church, and several towers. Commenced by St. Hugh, the sixth abbot, in 1089, it was finished and consecrated by Pope Innocent II in 1131-32, the narthex being added in 1220. Together with the conventual buildings it covered an area of twenty-five acres. At the suppression in 1790 it was bought by the town and almost entirely destroyed.
English Cluniac houses.
The first English house of the Cluniac order was that of Lewes, founded by the William de Warenne, 1st Earl of Surrey, c. AD 1077. All Cluniac houses in England were French colonies, governed by priors of that nation. All but one of the Cluniac houses in Britain which were larger than cells were known as priories, symbolising their subordination to Cluny. The exception was the priory at Paisley which was raised to the status of an abbey in 1245 answerable only to the Pope. The head of the Order was the Abbot at Cluny. All English and Scottish Cluniacs were bound to cross to France to Cluny to consult or be consulted unless the abbot chose to come to Britain, which happened rarely.
Cistercian abbeys.
The Cistercians, a Benedictine reform, were established at Cîteaux in 1098 by St. Robert, Abbot of Molesme for the purpose of restoring as far as possible the literal observance of the Rule of St. Benedict. La Ferté, Pontigny, Clairvaux, and Morimond were the first four daughters of Cîteaux, which, in their turn, gave birth to many other monasteries. Cîteaux being the mother-abbey of the Cistercian Order, the abbot was recognized as head and superior general of the whole order. The monks of Cîteaux created the vineyards of Clos-Vougeot and Romanée, the most celebrated of Burgundy. 
The rigid self-abnegation, which was the ruling principle of this reformed congregation of the Benedictine order, extended itself to the churches and other buildings erected by them. The defining architectural characteristic of the Cistercian abbeys was the most extreme simplicity and a studied plainness. Only a single, central tower was permitted, and that was to be very low. Unnecessary pinnacles and turrets were prohibited. The triforium was omitted. The windows were to be plain and undivided, and it was forbidden to decorate them with stained glass. All needless ornament was proscribed. The crosses must be of wood; the candlesticks of iron. The renunciation of the world was to be evidenced in all that met the eye.
The same spirit manifested itself in the choice of the sites of their monasteries. The more dismal, the more savage, the more hopeless a spot appeared, the more did it please their rigid mood. But they came not merely as ascetics, but as improvers. The Cistercian monasteries are, as a rule, found placed in deep, well-watered valleys. They always stand on the border of a stream; often with the buildings extending over it, as at Fountains Abbey. These valleys, now so rich and productive, had a very different appearance when the brethren first chose them as their place of retreat. Wide swamps, deep morasses, tangled thickets, and wild, impassable forests were their prevailing features. The "bright valley," Clara Vallis of St Bernard, was known as the "Valley of Wormwood," infamous as a den of robbers. "
See also:
Austin Canons.
The buildings of the Austin canons or Black canons (so called from the colour of their habit) present few distinctive peculiarities. This order had its first seat in England at St. Botolph's Priory, Colchester, Essex, where a house for Austin canons was founded about AD 1105, and it very soon spread widely. As an order of regular clergy, holding a middle position between monks and secular canons, almost resembling a community of parish priests living under rule, they adopted naves of great length to accommodate large congregations. The choir is usually long, and is sometimes, as at Llanthony and Christchurch (Twynham), shut off from the aisles, or, as at Bolton, Kirkham, etc., is destitute of aisles altogether. The nave in the northern houses, not infrequently, had only a north aisle, as at Bolton, Brinkburn and Lanercost. The arrangement of the monastic buildings followed the ordinary type. The prior's lodge was almost invariably attached to the S.W. angle of the nave.
The above plan of the Abbey of St Augustine's at Bristol, now the cathedral church of that city, shows the arrangement of the buildings, which departs very little from the ordinary Benedictine type. The Austin canons' house at Thornton, in Lincolnshire, is remarkable for the size and magnificence of its gate-house, the upper floors of which formed the guest-house of the establishment, and for possessing an octagonal chapter-house of Decorated date.
Premonstratensians.
The Premonstratensian regular canons, or White canons, had as many as 35 houses in England, of which the most perfect remaining are those of Easby, Yorkshire, and Bayham, Kent. The head house of the order in England was Welbeck. This order was a reformed branch of the Augustinian canons, founded, AD 1119, by Norbert of Xanten, on the Lower Rhine, c. 1080) at Prémontré, a secluded marshy valley in the forest of Coucy in the diocese of Laon. The order spread widely. Even in the founder's lifetime it possessed houses in Aleppo and Kingdom of Jerusalem where "The Premonstrntensian abbey of Saint Samuel was a daughter house of Prémontré itself. Its abbot had the status of a suffragan of the patriarch of Jerusalem, with the right to
a cross but not to a mitre nor a ring". It long maintained its rigid austerity, until in the course of years wealth impaired its discipline, and its members sank into indolence and luxury. The Premonstratensians were brought to England shortly after AD 1140, and were first settled at Newhouse, in Lincolnshire, near the Humber. The ground-plan of Easby Abbey, owing to its situation on the edge of the steeply sloping banks of a river, is singularly irregular. The cloister is duly placed on the south side of the church, and the chief buildings occupy their usual positions round it. But the cloister garth, as at Chichester, is not rectangular, and all the surrounding buildings are thus made to sprawl in a very awkward fashion. The church follows the plan adopted by the Austin canons in their northern abbeys, and has only one aisle to the nave—that to the north; while the choir is long, narrow and aisleless. Each transept has an aisle to the east, forming three chapels.
The church at Bayham was destitute of aisles either to nave or choir. The latter terminated in a three-sided apse. This church is remarkable for its exceeding narrowness in proportion to its length. Extending in longitudinal dimensions 257 ft, it is not more than 25 ft broad. Stern Premonstratensian canons wanted no congregations, and cared for no possessions; therefore they built their church like a long room.
The Premonstratension order still exists and a small group of these "Chanones de Premontre" now run the former Benedictine Abbey at Conques in southwest France, which has become well known as a refuge for pilgrims travelling the Way of Saint James, from Le Puy en Velay in Auvergne, to Santiago de Compostela in Galicia, Spain.
Copts.
The plan of a Coptic Orthodox monastery, from Lenoir, shows a church of three aisles, with cellular apses, and two ranges of cells on either side of an oblong gallery.

</doc>
<doc id="1332" url="http://en.wikipedia.org/wiki?curid=1332" title="August 7">
August 7

August 7 is the day of the year in the Gregorian calendar.
This day marks the approximate midpoint of summer in the Northern Hemisphere and of winter in the Southern Hemisphere (starting the season at the June solstice).

</doc>
<doc id="1347" url="http://en.wikipedia.org/wiki?curid=1347" title="Allosaurus">
Allosaurus

Allosaurus is a genus of large theropod dinosaur that lived 155 to 150 million years ago during the late Jurassic period (Kimmeridgian to early Tithonian). The name "Allosaurus" means "different lizard". It is derived from the Greek ἄλλος/"allos" ("different, other") and σαῦρος/"sauros" ("lizard / generic reptile"). The first fossil remains that can definitely be ascribed to this genus were described in 1877 by paleontologist Othniel Charles Marsh, and it became known as "Antrodemus". As one of the first well-known theropod dinosaurs, it has long attracted attention outside of paleontological circles. Indeed, it has been a top feature in several films and documentaries about prehistoric life.
"Allosaurus" was a large bipedal predator. Its skull was large and equipped with dozens of large, sharp teeth. It averaged 8.5 m in length, though fragmentary remains suggest it could have reached over 12 m. Relative to the large and powerful hindlimbs, its three-fingered forelimbs were small, and the body was balanced by a long and heavily muscled tail. It is classified as an allosaurid, a type of carnosaurian theropod dinosaur. The genus has a complicated taxonomy, and includes an uncertain number of valid species, the best known of which is "A. fragilis". The bulk of "Allosaurus" remains have come from North America's Morrison Formation, with material also known from Portugal and possibly Tanzania. It was known for over half of the 20th century as "Antrodemus", but study of the copious remains from the Cleveland-Lloyd Dinosaur Quarry brought the name "Allosaurus" back to prominence, and established it as one of the best-known dinosaurs.
As the most abundant large predator in the Morrison Formation, "Allosaurus" was at the top of the food chain, probably preying on contemporaneous large herbivorous dinosaurs and perhaps even other predators. Potential prey included ornithopods, stegosaurids, and sauropods. Some paleontologists interpret "Allosaurus" as having had cooperative social behavior, and hunting in packs, while others believe individuals may have been aggressive toward each other, and that congregations of this genus are the result of lone individuals feeding on the same carcasses. It may have attacked large prey by ambush, using its upper jaw like a hatchet.
Description.
"Allosaurus" was a typical large theropod, having a massive skull on a short neck, a long tail and reduced forelimbs. "Allosaurus fragilis", the best-known species, had an average length of 8.5 m, with the largest definitive "Allosaurus" specimen (AMNH 680) estimated at 9.7 meters long (32 ft), and an estimated weight of 2.3 metric tons (2.5 short tons). In his 1976 monograph on "Allosaurus", James Madsen mentioned a range of bone sizes which he interpreted to show a maximum length of 12 to. As with dinosaurs in general, weight estimates are debatable, and since 1980 have ranged between 1500 kg, 1000 to, and 1010 kg for modal adult weight (not maximum). John Foster, a specialist on the Morrison Formation, suggests that 1000 kg is reasonable for large adults of "A. fragilis", but that 700 kg is a closer estimate for individuals represented by the average-sized thigh bones he has measured. Using the subadult specimen nicknamed "Big Al", researchers using computer modelling arrived at a best estimate of 1500 kg for the individual, but by varying parameters they found a range from approximately 1400 kg to approximately 2000 kg.
Several gigantic specimens have been attributed to "Allosaurus", but may in fact belong to other genera. The closely related genus "Saurophaganax" (OMNH 1708) reached perhaps 10.9 m in length, and its single species has sometimes been included in the genus "Allosaurus" as "Allosaurus maximus", though recent studies support it as a separate genus. Another potential specimen of "Allosaurus", once assigned to the genus "Epanterias" (AMNH 5767), may have measured 12.1 meters in length (40 ft). A more recent discovery is a partial skeleton from the Peterson Quarry in Morrison rocks of New Mexico; this large allosaurid may be another individual of "Saurophaganax".
Skull.
The skull and teeth of "Allosaurus" were modestly proportioned for a theropod of its size. Paleontologist Gregory S. Paul gives a length of 845 mm for a skull belonging to an individual he estimates at 7.9 m long. Each premaxilla (the bones that formed the tip of the snout), held five teeth with D-shaped cross-sections, and each maxilla (the main tooth-bearing bones in the upper jaw) had between 14 and 17 teeth; the number of teeth does not exactly correspond to the size of the bone. Each dentary (the tooth-bearing bone of the lower jaw) had between 14 and 17 teeth, with an average count of 16. The teeth became shorter, more narrow, and more curved toward the back of the skull. All of the teeth had saw-like edges. They were shed easily, and were replaced continually, making them common fossils.
The skull had a pair of horns above and in front of the eyes. These horns were composed of extensions of the lacrimal bones, and varied in shape and size. There were also lower paired ridges running along the top edges of the nasal bones that led into the horns. The horns were probably covered in a keratin sheath and may have had a variety of functions, including acting as sunshades for the eye, being used for display, and being used in combat against other members of the same species (although they were fragile). There was a ridge along the back of the skull roof for muscle attachment, as is also seen in tyrannosaurids.
Inside the lacrimal bones were depressions that may have held glands, such as salt glands. Within the maxillae were sinuses that were better developed than those of more basal theropods such as "Ceratosaurus" and "Marshosaurus"; they may have been related to the sense of smell, perhaps holding something like Jacobson's organ. The roof of the braincase was thin, perhaps to improve thermoregulation for the brain. The skull and lower jaws had joints that permitted motion within these units. In the lower jaws, the bones of the front and back halves loosely articulated, permitting the jaws to bow outward and increasing the animal's gape. The braincase and frontals may also have had a joint.
Postcranial skeleton.
"Allosaurus" had nine vertebrae in the neck, 14 in the back, and five in the sacrum supporting the hips. The number of tail vertebrae is unknown and varied with individual size; James Madsen estimated about 50, while Gregory S. Paul considered that to be too many and suggested 45 or less. There were hollow spaces in the neck and anterior back vertebrae. Such spaces, which are also found in modern theropods (that is, the birds), are interpreted as having held air sacs used in respiration. The rib cage was broad, giving it a barrel chest, especially in comparison to less derived theropods like "Ceratosaurus". "Allosaurus" had gastralia (belly ribs), but these are not common findings, and they may have ossified poorly. In one published case, the gastralia show evidence of injury during life. A furcula (wishbone) was also present, but has only been recognized since 1996; in some cases furculae were confused with gastralia. The ilium, the main hip bone, was massive, and the pubic bone had a prominent foot that may have been used for both muscle attachment and as a prop for resting the body on the ground. Madsen noted that in about half of the individuals from the Cleveland-Lloyd Dinosaur Quarry, independent of size, the pubes had not fused to each other at their foot ends. He suggested that this was a sexual characteristic, with females lacking fused bones to make egg-laying easier. This proposal has not attracted further attention, however.
The forelimbs of "Allosaurus" were short in comparison to the hindlimbs (only about 35% the length of the hindlimbs in adults) and had three fingers per hand, tipped with large, strongly curved and pointed claws. The arms were powerful, and the forearm was somewhat shorter than the upper arm (1:1.2 ulna/humerus ratio). The wrist had a version of the semilunate carpal also found in more derived theropods like maniraptorans. Of the three fingers, the innermost (or thumb) was the largest, and diverged from the others. The phalangeal formula is 2-3-4-0-0, meaning that the innermost finger (phalange) has two bones, the next has three, and the third finger has four. The legs were not as long or suited for speed as those of tyrannosaurids, and the claws of the toes were less developed and more hoof-like than those of earlier theropods. Each foot had three weight-bearing toes and an inner dewclaw, which Madsen suggested could have been used for grasping in juveniles. There was also what is interpreted as the splint-like remnant of a fifth (outermost) metatarsal, perhaps used as a lever between the Achilles tendon and foot.
Classification.
"Allosaurus" was an allosaurid, a member of a family of large theropods within the larger group Carnosauria. The family name Allosauridae was created for this genus in 1878 by Othniel Charles Marsh, but the term was largely unused until the 1970s in favor of Megalosauridae, another family of large theropods that eventually became a wastebasket taxon. This, along with the use of "Antrodemus" for "Allosaurus" during the same period, is a point that needs to be remembered when searching for information on "Allosaurus" in publications that predate James Madsen's 1976 monograph. Major publications using the name "Megalosauridae" instead of "Allosauridae" include Gilmore, 1920, von Huene, 1926, Romer, 1956 and 1966, Steel, 1970, and Walker, 1964.
Following the publication of Madsen's influential monograph, Allosauridae became the preferred family assignment, but it too was not strongly defined. Semi-technical works used Allosauridae for a variety of large theropods, usually those that were larger and better-known than megalosaurids. Typical theropods that were thought to be related to "Allosaurus" included "Indosaurus", "Piatnitzkysaurus", "Piveteausaurus", "Yangchuanosaurus", "Acrocanthosaurus", "Chilantaisaurus", "Compsosuchus", "Stokesosaurus", and "Szechuanosaurus". Given modern knowledge of theropod diversity and the advent of cladistic study of evolutionary relationships, none of these theropods is now recognized as an allosaurid, although several, like "Acrocanthosaurus" and "Yangchuanosaurus", are members of closely related families.
Below is a cladogram by Benson "et al." in 2010.
Allosauridae is one of four families in Carnosauria; the other three are Neovenatoridae, Carcharodontosauridae and Sinraptoridae. Allosauridae has at times been proposed as ancestral to the Tyrannosauridae (which would make it paraphyletic), one recent example being Gregory S. Paul's "Predatory Dinosaurs of the World", but this has been rejected, with tyrannosaurids identified as members of a separate branch of theropods, the Coelurosauria. Allosauridae is the smallest of the carnosaur families, with only "Saurophaganax" and a currently unnamed French allosauroid accepted as possible valid genera besides "Allosaurus" in the most recent review. Another genus, "Epanterias", is a potential valid member, but it and "Saurophaganax" may turn out to be large examples of "Allosaurus". Recent reviews have kept the genus "Saurophaganax" and included "Epanterias" with "Allosaurus".
Discovery and history.
Early discoveries and research.
The discovery and early study of "Allosaurus" is complicated by the multiplicity of names coined during the Bone Wars of the late 19th century. The first described fossil in this history was a bone obtained secondhand by Ferdinand Vandiveer Hayden in 1869. It came from Middle Park, near Granby, Colorado, probably from Morrison Formation rocks. The locals had identified such bones as "petrified horse hoofs". Hayden sent his specimen to Joseph Leidy, who identified it as half of a tail vertebra, and tentatively assigned it to the European dinosaur genus "Poekilopleuron" as "Poicilopleuron" ["sic"] "valens". He later decided it deserved its own genus, "Antrodemus".
"Allosaurus" itself is based on YPM 1930, a small collection of fragmentary bones including parts of three vertebrae, a rib fragment, a tooth, a toe bone, and, most useful for later discussions, the shaft of the right humerus (upper arm). Othniel Charles Marsh gave these remains the formal name "Allosaurus fragilis" in 1877. "Allosaurus" comes from the Greek "allos/αλλος", meaning "strange" or "different" and "sauros/σαυρος", meaning "lizard" or "reptile". It was named 'different lizard' because its vertebrae were different from those of other dinosaurs known at the time of its discovery. The species epithet "fragilis" is Latin for "fragile", referring to lightening features in the vertebrae. The bones were collected from the Morrison Formation of Garden Park, north of Cañon City. Marsh and Edward Drinker Cope, who were in scientific competition, went on to coin several other genera based on similarly sparse material that would later figure in the taxonomy of "Allosaurus". These include Marsh's "Creosaurus" and "Labrosaurus", and Cope's "Epanterias".
In their haste, Cope and Marsh did not always follow up on their discoveries (or, more commonly, those made by their subordinates). For example, after the discovery by Benjamin Mudge of the type specimen of "Allosaurus" in Colorado, Marsh elected to concentrate work in Wyoming; when work resumed at Garden Park in 1883, M. P. Felch found an almost complete "Allosaurus" and several partial skeletons. In addition, one of Cope's collectors, H. F. Hubbell, found a specimen in the Como Bluff area of Wyoming in 1879, but apparently did not mention its completeness, and Cope never unpacked it. Upon unpacking in 1903 (several years after Cope had died), it was found to be one of the most complete theropod specimens then known, and in 1908 the skeleton, now cataloged as AMNH 5753, was put on public view. This is the well-known mount poised over a partial "Apatosaurus" skeleton as if scavenging it, illustrated as such by Charles R. Knight. Although notable as the first free-standing mount of a theropod dinosaur, and often illustrated and photographed, it has never been scientifically described.
The multiplicity of early names complicated later research, with the situation compounded by the terse descriptions provided by Marsh and Cope. Even at the time, authors such as Samuel Wendell Williston suggested that too many names had been coined. For example, Williston pointed out in 1901 that Marsh had never been able to adequately distinguish "Allosaurus" from "Creosaurus". The most influential early attempt to sort out the convoluted situation was produced by Charles W. Gilmore in 1920. He came to the conclusion that the tail vertebra named "Antrodemus" by Leidy was indistinguishable from those of "Allosaurus", and "Antrodemus" thus should be the preferred name because as the older name it had priority. "Antrodemus" became the accepted name for this familiar genus for over fifty years, until James Madsen published on the Cleveland-Lloyd specimens and concluded that "Allosaurus" should be used because "Antrodemus" was based on material with poor, if any, diagnostic features and locality information (for example, the geological formation that the single bone of "Antrodemus" came from is unknown). "Antrodemus" has been used informally for convenience when distinguishing between the skull Gilmore restored and the composite skull restored by Madsen.
Cleveland-Lloyd discoveries.
Although sporadic work at what became known as the Cleveland-Lloyd Dinosaur Quarry in Emery County, Utah had taken place as early as 1927, and the fossil site itself described by William J. Stokes in 1945, major operations did not begin there until 1960. Under a cooperative effort involving nearly 40 institutions, thousands of bones were recovered between 1960 and 1965. The quarry is notable for the predominance of "Allosaurus" remains, the condition of the specimens, and the lack of scientific resolution on how it came to be. The majority of bones belong to the large theropod "Allosaurus fragilis" (it is estimated that the remains of at least 46 "A. fragilis" have been found there, out of at minimum 73 dinosaurs), and the fossils found there are disarticulated and well-mixed. Nearly a dozen scientific papers have been written on the taphonomy of the site, suggesting numerous mutually exclusive explanations for how it may have formed. Suggestions have ranged from animals getting stuck in a bog, to becoming trapped in deep mud, to falling victim to drought-induced mortality around a waterhole, to getting trapped in a spring-fed pond or seep. Regardless of the actual cause, the great quantity of well-preserved "Allosaurus" remains has allowed this genus to be known in detail, making it among the best-known theropods. Skeletal remains from the quarry pertain to individuals of almost all ages and sizes, from less than 1 m to 12 m long, and the disarticulation is an advantage for describing bones usually found fused.
Recent work: 1980s–present.
The period since Madsen's monograph has been marked by a great expansion in studies dealing with topics concerning "Allosaurus" in life (paleobiological and paleoecological topics). Such studies have covered topics including skeletal variation, growth, skull construction, hunting methods, the brain, and the possibility of gregarious living and parental care. Reanalysis of old material (particularly of large 'allosaur' specimens), new discoveries in Portugal, and several very complete new specimens have also contributed to the growing knowledge base.
"Big Al" and "Big Al Two".
In 1991 "Big Al" (MOR 693), a 95% complete, partially articulated specimen of "Allosaurus" was discovered. It measured about 8 meters (about 26 ft) in length. MOR 693 was excavated near Shell, Wyoming, by a joint Museum of the Rockies and University of Wyoming Geological Museum team. This skeleton was discovered by a Swiss team, led by Kirby Siber. In 1996 the same team discovered a second "Allosaurus", "Big Al Two", which is the best preserved skeleton of its kind to date.
The completeness, preservation, and scientific importance of this skeleton gave "Big Al" its name; the individual itself was below the average size for "Allosaurus fragilis", and was a subadult estimated at only 87% grown. The specimen was described by Breithaupt in 1996. 19 of its bones were broken or showed signs of infection, which may have contributed to "Big Al's" death. Pathologic bones included five ribs, five vertebrae, and four bones of the feet; several damaged bones showed osteomyelitis, a bone infection. A particular problem for the living animal was infection and trauma to the right foot that probably affected movement and may have also predisposed the other foot to injury because of a change in gait. Al had an infection on the first phalanx on the third toe that was afflicted by an involucrum. The infection was long lived, perhaps up to 6 months. Big Al Two is also known to have multiple injuries.
Species and taxonomy.
There are currently five valid and one undescribed species of "Allosaurus" ("A. amplus", "A. europaeus", the type species "A. fragilis", the as-yet not formally described "A. jimmadseni", and "A. lucasi").
"A. fragilis", "A. jimmadseni", and "A. lucasi" are all known from remains discovered in the Kimmeridgian–Tithonian Upper Jurassic-age Morrison Formation of the United States, spread across the states of Colorado, Montana, New Mexico, Oklahoma, South Dakota, Utah, and Wyoming. "A. fragilis" is regarded as the most common, known from the remains of at least sixty individuals. For a while in the late 1980s and early 1990s it was common to recognize "A. fragilis" as the short-snouted species, with the long-snouted taxon being "A. atrox"; however, subsequent analysis of specimens from the Cleveland-Lloyd Quarry, Como Bluff, and Dry Mesa Quarry showed that the differences seen in the Morrison Formation material could be attributed to individual variation. A study of skull elements from the Cleveland-Lloyd site found wide variation between individuals, calling into question previous species-level distinctions based such features as the shape of the lacrimal horns, and the proposed differentiation of "A. jimmadseni" based on the shape of the jugal. "A. europaeus" was found in the Kimmeridgian-age Porto Novo Member of the Lourinhã Formation, but may be the same as "A. fragilis".
"Allosaurus tendagurensis" was found in Kimmeridgian-age rocks of Tendaguru, in Mtwara, Tanzania. It was subsequently classified as a more basal tetanuran, before being finally recognized as a carcharodontosaurid, Although obscure, it was a large theropod, possibly around 10 meters long (33 ft) and 2.5 metric tons (2.8 short tons) in weight.
"Allosaurus" is regarded as a probable synonym of the genera "Creosaurus", "Epanterias", and "Labrosaurus". Most of the species that are regarded as synonyms of "A. fragilis", or that were misassigned to the genus, are obscure and were based on scrappy remains. One exception is "Labrosaurus ferox", named in 1884 by Marsh for an oddly formed partial lower jaw, with a prominent gap in the tooth row at the tip of the jaw, and a rear section greatly expanded and turned down. Later researchers suggested that the bone was pathologic, showing an injury to the living animal, and that part of the unusual form of the rear of the bone was due to plaster reconstruction. It is now regarded as an example of "A. fragilis."
Other remains formerly thought to pertain to "Allosaurus" were described Australia, and Siberia, but these fossils have been reassessed as belonging to other dinosaurs.
The issue of synonyms is complicated by the type specimen of "Allosaurus fragilis" (catalog number YPM 1930) being extremely fragmentary, consisting of a few incomplete vertebrae, limb bone fragments, rib fragments, and a tooth. Because of this, several scientists have interpreted the type specimen as potentially dubious, and thus the genus "Allosaurus" itself or at least the species "A. fragilis" would be a "nomen dubium" ("dubious name", based on a specimen too incomplete to compare to other specimens or to classify). To address this situation, Gregory S. Paul and Kenneth Carpenter (2010) submitted a petition to the ICZN to have the name "A. fragilis" officially transferred to the more complete specimen USNM4734 (as a neotype). This request is currently pending review.
Paleoecology.
"Allosaurus" was the most common large theropod in the vast tract of Western American fossil-bearing rock known as the Morrison Formation, accounting for 70 to 75% of theropod specimens, and as such was at the top trophic level of the Morrison food web. The Morrison Formation is interpreted as a semiarid environment with distinct wet and dry seasons, and flat floodplains. Vegetation varied from river-lining forests of conifers, tree ferns, and ferns (gallery forests), to fern savannas with occasional trees such as the "Araucaria"-like conifer "Brachyphyllum".
The Morrison Formation has been a rich fossil hunting ground. The flora of the period has been revealed by fossils of green algae, fungi, mosses, horsetails, ferns, cycads, ginkgoes, and several families of conifers. Animal fossils discovered include bivalves, snails, ray-finned fishes, frogs, salamanders, turtles, sphenodonts, lizards, terrestrial and aquatic crocodylomorphans, several species of pterosaur, numerous dinosaur species, and early mammals such as docodonts, multituberculates, symmetrodonts, and triconodonts. Dinosaurs known from the Morrison include the theropods "Ceratosaurus", "Ornitholestes", and "Torvosaurus", the sauropods "Apatosaurus", "Brachiosaurus", "Camarasaurus", and "Diplodocus", and the ornithischians "Camptosaurus", "Dryosaurus", and "Stegosaurus". "Allosaurus" is commonly found at the same sites as "Apatosaurus", "Camarasaurus", "Diplodocus", and "Stegosaurus". The Late Jurassic formations of Portugal where "Allosaurus" is present are interpreted as having been similar to the Morrison but with a stronger marine influence. Many of the dinosaurs of the Morrison Formation are the same genera as those seen in Portuguese rocks (mainly "Allosaurus", "Ceratosaurus", "Torvosaurus", and "Apatosaurus"), or have a close counterpart ("Brachiosaurus" and "Lusotitan", "Camptosaurus" and "Draconyx").
"Allosaurus" coexisted with fellow large theropods "Ceratosaurus" and "Torvosaurus" in both the United States and Portugal. The three appear to have had different ecological niches, based on anatomy and the location of fossils. Ceratosaurs and torvosaurs may have preferred to be active around waterways, and had lower, thinner bodies that would have given them an advantage in forest and underbrush terrains, whereas allosaurs were more compact, with longer legs, faster but less maneuverable, and seem to have preferred dry floodplains. "Ceratosaurus", better known than "Torvosaurus", differed noticeably from "Allosaurus" in functional anatomy by having a taller, narrower skull with large, broad teeth. "Allosaurus" was itself a potential food item to other carnivores, as illustrated by an "Allosaurus" pubic foot marked by the teeth of another theropod, probably "Ceratosaurus" or "Torvosaurus". The location of the bone in the body (along the bottom margin of the torso and partially shielded by the legs), and the fact that it was among the most massive in the skeleton, indicates that the "Allosaurus" was being scavenged.
Paleobiology.
Life history.
The wealth of "Allosaurus" fossils, from nearly all ages of individuals, allows scientists to study how the animal grew and how long its lifespan may have been. Remains may reach as far back in the lifespan as eggs—crushed eggs from Colorado have been suggested as those of "Allosaurus". Based on histological analysis of limb bones, bone deposition appears to stop at around 22 to 28 years, which is comparable to that of other large theropods like "Tyrannosaurus". From the same analysis, its maximum growth appears to have been at age 15, with an estimated growth rate of about 150 kilograms (330 lb) per year.
Medullary bone tissue (endosteally derived, ephemeral, mineralization located inside the medulla of the long bones in gravid female birds) has been reported in at least one "Allosaurus" specimen, a shin bone from the Cleveland-Lloyd Quarry. Today, this bone tissue is only formed in female birds that are laying eggs, as it is used to supply calcium to shells. Its presence in the "Allosaurus" individual has been used to establish sex and show it had reached reproductive age. However, other studies have called into question some cases of medullary bone in dinosaurs, including this "Allosaurus" individual. Data from extant birds suggested that the medullary bone in this "Allosaurus" individual may have been the result of a bone pathology instead.
The discovery of a juvenile specimen with a nearly complete hindlimb shows that the legs were relatively longer in juveniles, and the lower segments of the leg (shin and foot) were relatively longer than the thigh. These differences suggest that younger "Allosaurus" were faster and had different hunting strategies than adults, perhaps chasing small prey as juveniles, then becoming ambush hunters of large prey upon adulthood. The thigh bone became thicker and wider during growth, and the cross-section less circular, as muscle attachments shifted, muscles became shorter, and the growth of the leg slowed. These changes imply that juvenile legs has less predictable stresses compared with adults, which would have moved with more regular forward progression. Conversely, the skull bones appear to have generally grown isometrically, increasing in size without changing in proportion.
Feeding.
Paleontologists accept "Allosaurus" as an active predator of large animals. There is dramatic evidence for allosaur attacks on "Stegosaurus", including an "Allosaurus" tail vertebra with a partially healed puncture wound that fits a "Stegosaurus" tail spike, and a "Stegosaurus" neck plate with a U-shaped wound that correlates well with an "Allosaurus" snout. Sauropods seem to be likely candidates as both live prey and as objects of scavenging, based on the presence of scrapings on sauropod bones fitting allosaur teeth well and the presence of shed allosaur teeth with sauropod bones. However, as Gregory Paul noted in 1988, "Allosaurus" was probably not a predator of fully grown sauropods, unless it hunted in packs, as it had a modestly sized skull and relatively small teeth, and was greatly outweighed by contemporaneous sauropods. Another possibility is that it preferred to hunt juveniles instead of fully grown adults. Research in the 1990s and first decade of the 21st century may have found other solutions to this question. Robert T. Bakker, comparing "Allosaurus" to Cenozoic sabre-toothed carnivorous mammals, found similar adaptations, such as a reduction of jaw muscles and increase in neck muscles, and the ability to open the jaws extremely wide. Although "Allosaurus" did not have sabre teeth, Bakker suggested another mode of attack that would have used such neck and jaw adaptations: the short teeth in effect became small serrations on a saw-like cutting edge running the length of the upper jaw, which would have been driven into prey. This type of jaw would permit slashing attacks against much larger prey, with the goal of weakening the victim.
Similar conclusions were drawn by another study using finite element analysis on an "Allosaurus" skull. According to their biomechanical analysis, the skull was very strong but had a relatively small bite force. By using jaw muscles only, it could produce a bite force of 805 to 2,148 N, less than the values for alligators (13,000 N), lions (4,167 N), and leopards (2,268 N), but the skull could withstand nearly 55,500 N of vertical force against the tooth row. The authors suggested that "Allosaurus" used its skull like a hatchet against prey, attacking open-mouthed, slashing flesh with its teeth, and tearing it away without splintering bones, unlike "Tyrannosaurus", which is thought to have been capable of damaging bones. They also suggested that the architecture of the skull could have permitted the use of different strategies against different prey; the skull was light enough to allow attacks on smaller and more agile ornithopods, but strong enough for high-impact ambush attacks against larger prey like stegosaurids and sauropods. Their interpretations were challenged by other researchers, who found no modern analogues to a hatchet attack and considered it more likely that the skull was strong to compensate for its open construction when absorbing the stresses from struggling prey. The original authors noted that "Allosaurus" itself has no modern equivalent, that the tooth row is well-suited to such an attack, and that articulations in the skull cited by their detractors as problematic actually helped protect the palate and lessen stress. Another possibility for handling large prey is that theropods like "Allosaurus" were "flesh grazers" which could take bites of flesh out of living sauropods that were sufficient to sustain the predator so it would not have needed to expend the effort to kill the prey outright. This strategy would also potentially have allowed the prey to recover and be fed upon in a similar way later. An additional suggestion notes that ornithopods were the most common available dinosaurian prey, and that allosaurs may have subdued them by using an attack similar to that of modern big cats: grasping the prey with their forelimbs, and then making multiple bites on the throat to crush the trachea. This is compatible with other evidence that the forelimbs were strong and capable of restraining prey.
A biomechanical study published in 2013 by Eric Snively and colleagues found that "Allosaurus" had an unusually low attachment point on the skull for the longissimus capitis superficialis neck muscle compared to other theropods such as "Tyrannosaurus". This would have allowed the animal to make rapid and forceful vertical movements with the skull. The authors found that vertical strikes as proposed by Bakker and Rayfield are consistent with the animal's capabilities. They also found that the animal probably processed carcasses by vertical movements in a similar manner to falcons, such as kestrels: the animal could have gripped prey with the skull and feet, then pulled back and up to remove flesh. This differs from the prey-handling envisioned for tyrannosaurids, which probably tore flesh with lateral shakes of the skull, similar to crocodilians. In addition, "Allosaurus" was able to "move its head and neck around relatively rapidly and with considerable control", at the cost of power.
Other aspects of feeding include the eyes, arms, and legs. The shape of the skull of "Allosaurus" limited potential binocular vision to 20° of width, slightly less than that of modern crocodilians. As with crocodilians, this may have been enough to judge prey distance and time attacks. The arms, compared with those of other theropods, were suited for both grasping prey at a distance or clutching it close, and the articulation of the claws suggests that they could have been used to hook things. Finally, the top speed of "Allosaurus" has been estimated at 30 to 55 kilometers per hour (19 to 34 miles per hour).
Social behavior.
It has been speculated since the 1970s that "Allosaurus" preyed on sauropods and other large dinosaurs by hunting in groups.
Such a depiction is common in semitechnical and popular dinosaur literature. Robert T. Bakker has extended social behavior to parental care, and has interpreted shed allosaur teeth and chewed bones of large prey animals as evidence that adult allosaurs brought food to lairs for their young to eat until they were grown, and prevented other carnivores from scavenging on the food. However, there is actually little evidence of gregarious behavior in theropods, and social interactions with members of the same species would have included antagonistic encounters, as shown by injuries to gastralia and bite wounds to skulls (the pathologic lower jaw named "Labrosaurus ferox" is one such possible example). Such head-biting may have been a way to establish dominance in a pack or to settle territorial disputes.
Although "Allosaurus" may have hunted in packs, it has been argued that "Allosaurus" and other theropods had largely aggressive interactions instead of cooperative interactions with other members of their own species. The study in question noted that cooperative hunting of prey much larger than an individual predator, as is commonly inferred for theropod dinosaurs, is rare among vertebrates in general, and modern diapsid carnivores (including lizards, crocodiles, and birds) very rarely cooperate to hunt in such a way. Instead, they are typically territorial and will kill and cannibalize intruders of the same species, and will also do the same to smaller individuals that attempt to eat before they do when aggregated at feeding sites. According to this interpretation, the accumulation of remains of multiple "Allosaurus" individuals at the same site, e.g. in the Cleveland–Lloyd Quarry, are not due to pack hunting, but to the fact that "Allosaurus" individuals were drawn together to feed on other disabled or dead allosaurs, and were sometimes killed in the process. This could explain the high proportion of juvenile and subadult allosaurs present, as juveniles and subadults are disproportionally killed at modern group feeding sites of animals like crocodiles and Komodo dragons. The same interpretation applies to Bakker's lair sites. There is some evidence for cannibalism in "Allosaurus", including "Allosaurus" shed teeth found among rib fragments, possible tooth marks on a shoulder blade, and cannibalized allosaur skeletons among the bones at Bakker's lair sites.
Brain and senses.
The brain of "Allosaurus", as interpreted from spiral CT scanning of an endocast, was more consistent with crocodilian brains than those of the other living archosaurs, birds. The structure of the vestibular apparatus indicates that the skull was held nearly horizontal, as opposed to strongly tipped up or down. The structure of the inner ear was like that of a crocodilian, and so "Allosaurus" probably could have heard lower frequencies best, and would have had trouble with subtle sounds. The olfactory bulbs were large and seem to have been well suited for detecting odors, although the area for evaluating smells was relatively small.
Paleopathology.
In 2001, Bruce Rothschild and others published a study examining evidence for stress fractures and tendon avulsions in theropod dinosaurs and the implications for their behavior. Since stress fractures are caused by repeated trauma rather than singular events they are more likely to be caused by the behavior of the animal than other kinds of injury. Stress fractures and tendon avulsions occurring in the forelimb have special behavioral significance since while injuries to the feet could be caused by running or migration, resistant prey items are the most probable source of injuries to the hand. "Allosaurus" was one of only two theropods examined in the study to exhibit a tendon avulsion, and in both cases the avulsion occurred on the forelimb. When the researchers looked for stress fractures, they found that "Allosaurus" had a significantly greater number of stress fractures than "Albertosaurus", "Ornithomimus" or "Archaeornithomimus". Of the 47 hand bones the researchers studied, 3 were found to contain stress fractures. Of the feet, 281 bones were studied and 17 found to have stress fractures. The stress fractures in the foot bones "were distributed to the proximal phalanges" and occurred across all three weight-bearing toes in "statistically indistinguishable" numbers. Since the lower end of the third metatarsal would have contacted the ground first while an allosaur was running it would have borne the most stress. If the allosaurs' stress fractures were caused by damage accumulating while walking or running this bone should have experience more stress fractures than the others. The lack of such a bias in the examined "Allosaurus" fossils indicates an origin for the stress fractures from a source other than running. The authors conclude that these fractures occurred during interaction with prey, like an allosaur trying to hold struggling prey with its feet. The abundance of stress fractures and avulsion injuries in "Allosaurus" provide evidence for "very active" predation-based rather than scavenging diets.
The left scapula and fibula of an "Allosaurus fragilis" specimen catalogued as USNM 4734 are both pathological, both probably due to healed fractures. The specimen USNM 8367 preserved several pathological gastralia which preserve evidence of healed fractures near their middle. Some of the fractures were poorly healed and "formed pseudoarthroses." A specimen with a fractured rib was recovered from the Cleveland-Lloyd Quarry. Another specimen had fractured ribs and fused vertebrae near the end of the tail. An apparent subadult male "Allosaurus fragilis" was reported to have extensive pathologies, with a total of fourteen separate injuries. The specimen MOR 693 had pathologies on five ribs, the sixth neck vertebra the third eighth and thirteenth back vertebrae, the second tail vertebra and its chevron, the gastralia right scapula, manual phalanx I left ilium metatarsals III and V, the first phalanx of the third toe and the third phalanx of the second. The ilium had "a large hole... caused by a blow from above".The near end of the first phalanx of the third toe was afflicted by an involucrum.
Other pathologies reported in "Allosaurus" include: Willow breaks in two ribs. Healed fractures in the humerus and radius. Distortion of joint surfaces in the foot possibly due to osteoarthritis or developmental issues. Osteopetrosis along the endosteal surface of a tibia. Distortions of the joint surfaces of the tail vertebrae possibly due to osetoarthritis or developmental issues. "[E]xtensive 'neoplastic' ankylosis of caudals," possibly due to physical trauma as well as the fusion of chevrons to centra. Coossification of vertebral centra near the end of the tail. Amputation of a chevron and foot bone, both possibly a result of bites. "[E]xtensive exostoses" in the first phalanx of the third toe. Lesions similar to those caused by osteomyelitis in two scapulae. Bone spurs in a premaxilla, ungual, and two metacarpals. Exostosis in a pedal phalanx possibly attributable to an infectious disease. A metacarpal with a round depressed fracture.
In popular culture.
Along with "Tyrannosaurus", "Allosaurus" has come to represent the quintessential large, carnivorous dinosaur in western popular culture. It is a common dinosaur in American museums, due in particular to the excavations at the Cleveland-Lloyd Dinosaur Quarry; by 1976, as a result of cooperative operations, 38 museums in eight countries on three continents had Cleveland-Lloyd allosaur material or casts. "Allosaurus" is the official state fossil of Utah.
"Allosaurus" has been depicted in popular culture since the early years of the 20th century. It is top predator in both Arthur Conan Doyle's 1912 novel, "The Lost World", and its 1925 film adaptation, the first full-length motion picture to feature dinosaurs. "Allosaurus" was used as the starring dinosaur of the 1956 film "The Beast of Hollow Mountain", and the 1969 film "The Valley of Gwangi", two genre combinations of living dinosaurs with Westerns. In "The Valley of Gwangi", Gwangi is billed as an "Allosaurus", although Ray Harryhausen based his model for the creature on Charles R. Knight's depiction of a "Tyrannosaurus". Harryhausen sometimes confuses the two, stating in a DVD interview "They're both meat eaters, they're both tyrants... one was just a bit larger than the other." "Allosaurus" appeared in the second episode of the 1999 BBC television series "Walking with Dinosaurs" and the follow-up special "The Ballad of Big Al", which speculated on the life of the "Big Al" specimen, based on scientific evidence from the numerous injuries and pathologies in its skeleton. "Allosaurus" also made an appearance in the Discovery Channel series "Dinosaur Revolution". Its depiction in this series was based upon a specimen with a smashed lower jaw that was uncovered by paleontologist Thomas Holtz.

</doc>
<doc id="1579" url="http://en.wikipedia.org/wiki?curid=1579" title="Alcmene">
Alcmene

In Greek mythology, Alcmene or Alcmena (Ancient Greek: Ἀλκμήνη or Ἀλκμάνα [Doric]) was the wife of Amphitryon and mother, by Zeus, of Heracles. She was also the mother by Amphitryon of Iphicles and Laonome.
Background.
According to the "Bibliotheca", Alcmene was born to Electryon, the son of Perseus and Andromeda, and king of Tiryns and Mycenae or Medea in Argolis. Her mother was Anaxo, daughter of Alcaeus and Astydamia. Other accounts say her mother was Lysidice, the daughter of Pelops and Hippodameia, or Eurydice the daughter of Pelops. According to Pausanias, the poet Asius made Alcmene the daughter of Amphiaraus and Eriphyle.
Hesiod describes Alcmene as the tallest, most beautiful woman with wisdom surpassed by no person born of mortal parents. It is said that her face and dark eyes were as charming as Aphrodite's, and that she honoured her husband like no woman before her.
Exile to Thebes.
According to the "Bibliotheca", Alcmene went with Amphitryon to Thebes, where he was purified by Creon for accidentally killing Electryon. Alcmene refused to marry Amphitryon until he had avenged the death of her brothers. During Amphitryon's expedition against the Taphians and Teleboans, Zeus visited Alcmene disguised as Amphitryon. Extending one night into three, Zeus slept with Alcmene, his great-granddaughter, thereby conceiving Heracles, while recounting Amphitryon's victories against the Teleboans. When Amphitryon finally returned to Thebes, Alcmene told him that he had come the night before and slept with her; he learned from Tiresias what Zeus had done.
Birth of Heracles.
Homer.
In Homer's "Iliad", when Alcmene was about to give birth to Heracles, Zeus announced to all the gods that on that day a child, descended from Zeus himself, would be born who would rule all those around him. Hera, after requesting Zeus to swear an oath to that effect, descended from Olympus to Argos and made the wife of Sthenelus (a son of Perseus) give birth to Eurystheus after only seven months, while at the same time preventing Alcmene from delivering Heracles. This resulted in the fulfilment of Zeus's oath by Eurystheus rather than Heracles.
Ovid.
According to Ovid's "Metamorphoses", while in labour, Alcmene was having difficulty giving birth to such a large child. After seven days and nights in agony, Alcmene stretched out her arms and called upon Lucina, the goddess of childbirth (the Roman equivalent of Eileithyia). While Lucina did go to Alcmene, she had, however, been previously instructed by Juno (Hera) to stop the delivery. With her hands clasped and legs crossed, Lucina muttered charms, thereby preventing Alcmene from giving birth. Alcmene struggled in pain, cursed the heavens, and became close to death. Galanthis, a maid of Alcmene who was nearby, observed Lucina's actions and quickly deduced Juno's plans. She announced that Alcmene had safely delivered her child, and this surprised Lucina so much that she immediately jumped up and unclenched her hands. As soon as Lucina leapt up, Alcmene was released from her spell and gave birth to Heracles. As punishment for deceiving Lucina, Galanthis was transformed into a weasel; she continued to live with Alcmene.
Pausanias.
In Pausanias' recounting, Hera sent witches (as they were called by the Thebans) to hinder Alcmene's delivery of Heracles. The witches were successful in preventing the birth until Historis, daughter of Tiresias, thought of a trick to deceive the witches. Like Galanthis, Historis announced that Alcmene had delivered her child; having been deceived, the witches went away, allowing Alcmene to give birth.
Plautus.
In contrast to the depictions of difficult labor above, an alternative version is presented in "Amphitryon", a comedic play by Plautus. Here Alcmene calls upon Jupiter, who performs a miracle allowing her to give birth quickly and without pain. After a crash of thunder and light, the baby arrives without anyone's assistance.
Death.
After the death of Amphitryon, Alcmene married Rhadamanthys, son of Zeus, and lived with him in exile at Ocaleae in Boeotia. It is said that after Heracles was apotheosised, Hyllus, having pursued and killed Eurystheus, cut off Eurystheus' head and gave it to Alcmene, who gouged out the eyes with weaving pins. In Metamorphoses, an aging Alcmene recounted the story of the birth of Heracles to Iole.
There are two accounts of Alcmene's death. In the first, according to the Megarians, Alcmene was walking from Argos to Thebes when she died at Megara. The Heracleidae fell into disagreement about where to take Alcmene's body, with some wishing to take her corpse back to Argos, and others wishing to take it to Thebes to be buried with Amphitryon and Heracles' children by Megara. However, the god in Delphi gave the Heracleidae an oracle that it was better to bury Alcmene in Megara. In the second account given by the Thebans, when Alcmene died, she was turned from human form to a stone.
Pausanias indicated that an altar to Alcmene had been built in the Cynosarges in Athens, alongside altars to Heracles, Hebe, and Iolaus. Pausanias also said that Alcmene's tomb is located near the Olympieum at Megara.

</doc>
<doc id="1616" url="http://en.wikipedia.org/wiki?curid=1616" title="Alexios III Angelos">
Alexios III Angelos

Alexios III Angelos (Greek: Αλέξιος Γ' Άγγελος) (c. 1153–1211) was Byzantine Emperor from 1195 to 1203. A member of the extended Imperial family, Alexios came to throne after deposing his brother in 1195. The most significant event of his reign was the attack of the Fourth Crusade on Constantinople in 1203, on behalf of Alexios IV Angelos. Alexios III took over the defence of the city, which he mismanaged, then fled the city at night with one of his three daughters. From Adrianople, and then Mosynopolis, he unsuccessfully attempted to rally his supporters, only to end up a captive of Marquis Boniface of Montferrat. He was ransomed, sent to Asia Minor where he plotted against his son-in-law Theodore Laskaris, but was eventually arrested and spent his last days confined to the Monastery of Hyakinthos in Nicaea, where he died.
Early life.
Alexios III Angelos was the second son of Andronikos Doukas Angelos and Euphrosyne Kastamonitissa. Andronikos was himself a son of Theodora Komnene Angelina, the youngest daughter of Emperor Alexios I Komnenos and Irene Doukaina. Thus Alexios Angelos was a member of the extended imperial family. Together with his father and brothers, Alexios had conspired against Emperor Andronikos I Komnenos (c. 1183), and thus he spent several years in exile in Muslim courts, including that of Saladin.
His younger brother Isaac was threatened with execution under orders of Andronikos I, their first-cousin once-removed, on September 11, 1185. Isaac made a desperate attack on the imperial agents and killed their leader Stephen Hagiochristophorites. He then took refuge in the church of Hagia Sophia and from there appealed to the populace. His actions provoked a riot, which resulted in the deposition of Andronikos I and the proclamation of Isaac as Emperor. Alexios was now closer to the imperial throne than ever before.
Reign.
By 1190 Alexios had returned to the court of his younger brother, from whom he received the elevated title of "sebastokratōr". In 1195, while Isaac II was away hunting in Thrace, Alexios was acclaimed as emperor by the troops with the covert support of Alexios' wife Euphrosyne Doukaina Kamatera. Alexios captured Isaac at Stagira in Macedonia, put out his eyes, and thenceforth kept him a close prisoner, despite having previously been redeemed by Alexios from captivity at Antioch and showered with honours.
To compensate for this crime and to solidify his position as emperor, Alexios had to scatter money so lavishly as to empty his treasury, and to allow such licence to the officers of the army as to leave the Empire practically defenceless. These actions inevitably led to the financial ruin of the state. At Christmas 1196, Holy Roman Emperor Henry VI attempted to force Alexios to pay him a tribute of 5,000 pounds (later negotiated down to 1,600 pounds) of gold or face invasion. Alexios gathered the money by plundering imperial tombs at the church of the Holy Apostles and taxing the people heavily, though Henry's death in September 1197 meant the gold was never dispatched. The able and forceful empress Euphrosyne tried in vain to sustain his credit and his court; Vatatzes, the favourite instrument in her attempts at reform, was assassinated by the emperor's orders.
In the east the Empire was overrun by the Seljuk Turks; from the north, the Kingdom of Hungary and the rebellious Bulgarians and Vlachs descended unchecked to ravage the Balkan provinces of the Empire, sometimes penetrating as far as Greece, while Alexios squandered the public treasure on his palaces and gardens and attempted to deal with the crisis through diplomatic means. The Emperor's attempts to bolster the empire's defences by special concessions to pronoiars (notables) in the frontier zone backfired, as the latter increased their regional autonomy. Byzantine authority survived, but in a much weakened state.
Fourth Crusade.
Soon Alexios was threatened by a new and more formidable danger. In 1202, soldiers assembled at Venice to launch the Fourth Crusade. Alexios IV Angelos, the son of the deposed Isaac II, had recently escaped from Constantinople and now appealed for support to the crusaders, promising to end the schism of East and West, to pay for their transport, and to provide military support if they would help him depose his uncle and ascend to his father's throne.
The crusaders, whose objective had been Egypt, were persuaded to set their course for Constantinople, arriving there in June 1203, proclaiming Alexios IV as Emperor, and inviting the populace of the capital to depose his uncle. Alexios III took no effective measures to resist, and his attempts to bribe the crusaders failed. His son-in-law, Theodore Laskaris, who was the only one to attempt anything significant, was defeated at Scutari, and the siege of Constantinople began. Unfortunately for the city, misgovernment by Alexios III had left the Byzantine navy with only 20 worm-eaten hulks by the time the crusaders arrived.
In July, the crusaders, led by the aged Doge Enrico Dandolo, scaled the walls and took control of a major section of the city. In the ensuing fighting, the crusaders set the city on fire, ultimately leaving 20,000 people homeless. On 17 July Alexios III finally took action and led 17 divisions from the St. Romanus Gate, vastly outnumbering the crusaders. His courage failed, however, and the Byzantine army returned to the city without a fight. His courtiers demanded action, and Alexios III promised to fight. Instead, that night (July 17/18), Alexios III hid in the palace, and finally, with one of his daughters, Eirene, and such treasure (1,000 pounds of gold) as he could collect, got into a boat and escaped to Debeltos in Thrace, leaving his wife and his other daughters behind. Isaac II, drawn from his prison and robed once more in the imperial purple, received his son, Alexios IV, in state.
Life in exile.
Alexios III attempted to organize resistance to the new regime from Adrianople and then Mosynopolis, where he was joined by the later usurper Alexios V Doukas "Mourtzouphlos" in April 1204, after the definitive fall of Constantinople to the crusaders and the establishment of the Latin Empire. At first Alexios III received Alexios V well, even allowing him to marry his daughter Eudokia Angelina. Later Alexios V was blinded and deserted by his father-in-law, who fled from the crusaders into Thessaly. Here Alexios III eventually surrendered, with Euphrosyne, to Marquis Boniface of Montferrat, who was establishing himself as ruler of the Kingdom of Thessalonica.
Alexios III attempted to escape Boniface's "protection" in 1205, seeking shelter with Michael I Komnenos Doukas, the ruler of Epirus. Captured by Boniface, Alexios and his retinue were sent to Montferrat before being brought back to Thessalonica in c. 1209. At that point the deposed emperor was ransomed by Michael I, who sent him to Asia Minor, where Alexios' son-in-law Theodore Laskaris - now emperor of Nicaea - was holding his own against the Latins. Here Alexios conspired against his son-in-law after the latter refused to recognize Alexios' authority, receiving the support of Kaykhusraw I, the sultan of Rûm. In the Battle of Antioch on the Meander in 1211, the sultan was defeated and killed, and Alexios was captured by Theodore Laskaris. Alexios was then confined to a monastery at Nicaea, where he died later in 1211.
Family.
By his marriage to Euphrosyne Doukaina Kamatera, Alexios had three daughters:

</doc>
<doc id="1638" url="http://en.wikipedia.org/wiki?curid=1638" title="The Triumph of Time">
The Triumph of Time

"The Triumph of Time" is a poem by Algernon Charles Swinburne, published in 1866. It is in adapted ottava rima and is full of elaborate use of literary devices, particularly alliteration. The theme, which purports to be autobiographical, is that of rejected love. The (male) speaker deplores the ruin of his life, and in tones at times reminiscent of "Hamlet", craves oblivion, for which the sea serves as a constant metaphor.

</doc>
<doc id="1645" url="http://en.wikipedia.org/wiki?curid=1645" title="Alhazen">
Alhazen

Abū ʿAlī al-Ḥasan ibn al-Ḥasan ibn al-Haytham (Arabic: أبو علي، الحسن بن الحسن بن الهيثم‎), frequently referred to as Ibn al-Haytham (Arabic: ابن الهيثم, Latinized as Alhazen or Alhacen; 965 – 1040), was an Arab, Muslim, polymath and philosopher who made significant contributions to the principles of optics, astronomy, mathematics, meteorology, visual perception and the scientific method.
In medieval Europe, he was honored as "Ptolemaeus Secundus" ("Ptolemy the Second") or simply called "The Physicist". He is also sometimes called al-Basri (Arabic: البصري) after Basra, his birthplace. He spent most of his life close to the court of the Caliphate in Cairo and earned his living authoring various treatises and tutoring members of the nobilities.
Overview.
Biography.
Born c. 965 in Basra, which was then part of the Buyid emirate, to an Arab family.
He arrived in Cairo under the reign of Fatimid Caliph al-Hakim, a patron of the sciences who was particularly interested in astronomy. Ibn al-Haytham has proposed to the Caliph a hydraulic project to improve regulation of the flooding of the Nile, a task requiring an early attempt at building a dam at the present site of the Aswan Dam.
His field work convinced him of the technical impracticality of this scheme. Al-Haytham continued to live in Cairo, in the neighborhood of the famous University of al-Azhar, until his death after 1040. Legend has it that after deciding the scheme was impractical and fearing the caliph's anger, Alhazen feigned madness and was kept under house arrest from 1011 until al-Hakim's death in 1021. During this time, he wrote his influential "Book of Optics" and continued to write further treatises on astronomy, geometry, number theory, optics and natural philosophy.
Among his students were "Sorkhab" ("Sohrab"), a Persian from Iran's Semnan who was his student for over 3 years, and "Abu al-Wafa Mubashir ibn Fatek," an Egyptian prince who learned mathematics from Alhazen.
Legacy.
Alhazen made significant contributions to optics, number theory, geometry, astronomy and natural philosophy. Alhazen's work on optics is credited with contributing a new emphasis on experiment.
His main work, "Kitab al-Manazir" ("Book of Optics") was known in Islamicate societies mainly, but not exclusively, through the thirteenth-century commentary by Kamāl al-Dīn al-Fārisī, the "Tanqīḥ "al-Manāẓir" li-dhawī l-abṣār wa l-baṣā'ir". In al-Andalus, it was used by the eleventh-century prince of the Banu Hud dynasty of Zaragossa and author of an important mathematical text, al-Mu'taman ibn Hūd. A Latin translation of the "Kitab al-Manazir" was made probably in the late twelfth or early thirteenth century. This translation was read by and greatly influenced a number of scholars in Catholic Europe including: Roger Bacon, Robert Grosseteste, Witelo, Giambattista della Porta, Leonardo Da Vinci, Galileo Galilei, Christian Huygens, René Descartes, and Johannes Kepler. His research in catoptrics (the study of optical systems using mirrors) centred on spherical and parabolic mirrors and spherical aberration. He made the observation that the ratio between the angle of incidence and refraction does not remain constant, and investigated the magnifying power of a lens. His work on catoptrics also contains the problem known as "Alhazen's problem". Meanwhile in the Islamic world, Alhazen's work influenced Averroes' writings on optics, and his legacy was further advanced through the 'reforming' of his "Optics" by Persian scientist Kamal al-Din al-Farisi (died ca. 1320) in the latter's "Kitab Tanqih al-Manazir" ("The Revision of" [Ibn al-Haytham's] "Optics"). Alhazen wrote as many as 200 books, although only 55 have survived. Some of his treatises on optics survived only through Latin translation. During the Middle Ages his books on cosmology were translated into Latin, Hebrew and other languages. The crater Alhazen on the Moon is named in his honour, as was the asteroid 59239 Alhazen. In honour of Alhazen, the Aga Khan University (Pakistan) named its Ophthalmology endowed chair as "The Ibn-e-Haitham Associate Professor and Chief of Ophthalmology". Alhazen, by the name Ibn al-Haytham, is featured on the obverse of the Iraqi 10,000-dinar banknote issued in 2003, and on 10-dinar notes from 1982. A research facility that UN weapons inspectors suspected of conducting chemical and biological weapons research in Saddam Hussein's Iraq was also named after him.
One of the major scientific anniversaries that will be celebrated during the 2015 International Year of Light is: the works on optics by Ibn Al-Haytham (1015).
Book of Optics.
Alhazen's most famous work is his seven-volume treatise on optics "Kitab al-Manazir" ("Book of Optics"), written from 1011 to 1021.
"Optics" was translated into Latin by an unknown scholar at the end of the 12th century or the beginning of the 13th century. It was printed by Friedrich Risner in 1572, with the title "Opticae thesaurus: Alhazeni Arabis libri septem, nuncprimum editi; Eiusdem liber De Crepusculis et nubium ascensionibus" (English : Optics treasure: Arab Alhazeni seven books, published for the first time: The book of the Twilight of the clouds and ascensions). Risner is also the author of the name variant "Alhazen"; before Risner he was known in the west as Alhacen, which is the correct transcription of the Arabic name. This work enjoyed a great reputation during the Middle Ages. Works by Alhazen on geometric subjects were discovered in the Bibliothèque nationale in Paris in 1834 by E. A. Sedillot. In all, A. Mark Smith has accounted for 18 full or near-complete manuscripts, and five fragments, which are preserved in 14 locations, including one in the Bodleian Library at Oxford, and one in the library of Bruges.
Theory of vision.
Two major theories on vision prevailed in classical antiquity. The first theory, the emission theory, was supported by such thinkers as Euclid and Ptolemy, who believed that sight worked by the eye emitting rays of light. The second theory, the intromission theory supported by Aristotle and his followers, had physical forms entering the eye from an object. Previous Islamic writers (such as al-Kindi) had argued essentially on Euclidean, Galenist, or Aristotelian lines. The strongest influence on the "Book of Optics" was from Ptolemy's "Optics", while the description of the anatomy and physiology of the eye was based on Galen's account. Alhazen's achievement was to come up with a theory which successfully combined parts of the mathematical ray arguments of Euclid, the medical tradition of Galen, and the intromission theories of Aristotle. Alhazen's intromission theory followed al-Kindi (and broke with Aristotle) in asserting that "from each point of every colored body, illuminated by any light, issue light and color along every straight line that can be drawn from that point". This however left him with the problem of explaining how a coherent image was formed from many independent sources of radiation; in particular, every point of an object would send rays to every point on the eye. What Alhazen needed was for each point on an object to correspond to one point only on the eye. He attempted to resolve this by asserting that only perpendicular rays from the object would be perceived by the eye; for any one point on the eye, only the ray which reached it directly, without being refracted by any other part of the eye, would be perceived. He argued using a physical analogy that perpendicular rays were stronger than oblique rays; in the same way that a ball thrown directly at a board might break the board, whereas a ball thrown obliquely at the board would glance off, perpendicular rays were stronger than refracted rays, and it was only perpendicular rays which were perceived by the eye. As there was only one perpendicular ray that would enter the eye at any one point, and all these rays would converge on the centre of the eye in a cone, this allowed him to resolve the problem of each point on an object sending many rays to the eye; if only the perpendicular ray mattered, then he had a one-to-one correspondence and the confusion could be resolved. He later asserted (in book seven of the "Optics") that other rays would be refracted through the eye and perceived "as if" perpendicular.
His arguments regarding perpendicular rays do not clearly explain why "only" perpendicular rays were perceived; why would the weaker oblique rays not be perceived more weakly? His later argument that refracted rays would be perceived as if perpendicular does not seem persuasive. However, despite its weaknesses, no other theory of the time was so comprehensive, and it was enormously influential, particularly in Western Europe: "Directly or indirectly, his "De Aspectibus" inspired much of the activity in optics which occurred between the 13th and 17th centuries." Kepler's later theory of the retinal image (which resolved the problem of the correspondence of points on an object and points in the eye) built directly on the conceptual framework of Alhazen.
Alhazen showed through experiment that light travels in straight lines, and carried out various experiments with lenses, mirrors, refraction, and reflection. He was the
first to consider separately the vertical and horizontal components of reflected and refracted light rays, which was an important step in understanding optics geometrically.
The camera obscura was known to the ancient Chinese and was described by the Han Chinese polymathic genius Shen Kuo in his scientific book Dream Pool Essays which was printed and published in the year 1088 C.E.. Aristotle had discussed the basic principle behind it in his "Problems", however Alhazen's work also contained the first clear description, outside of China, of camera obscura in the areas of the middle east, Europe, Africa and India. and early analysis of the device.
Alhazen studied the process of sight, the structure of the eye, image formation in the eye, and the visual system. Ian P. Howard argued in a 1996 "Perception" article that Alhazen should be credited with many discoveries and theories which were previously attributed to Western Europeans writing centuries later. For example, he described what became in the 19th century Hering's law of equal innervation; he had a description of vertical horopters which predates Aguilonius by 600 years and is actually closer to the modern definition than Aguilonius's; and his work on binocular disparity was repeated by Panum in 1858. Craig Aaen-Stockdale, while agreeing that Alhazen should be credited with many advances, has expressed some caution, especially when considering Alhazen in isolation from Ptolemy, who Alhazen was extremely familiar with. Alhazen corrected a significant error of Ptolemy regarding binocular vision, but otherwise his account is very similar; Ptolemy also attempted to explain what is now called Hering's law. In general, Alhazen built on and expanded the optics of Ptolemy. In a more detailed account of Ibn al-Haytham's contribution to the study of binocular vision based on Lejeune and Sabra, Raynaud showed that the concepts of correspondence, homonymous and crossed diplopia were in place in Ibn al-Haytham's optics. But contrary to Howard, he explained why Ibn al-Haytham did not give the circular figure of the horopter and why, by reasoning experimentally, he was in fact closer to the discovery of Panum's fusional area than that of the Vieth-Müller circle. In this regard, Ibn al-Haytham's theory of binocular vision faced two main limits: the lack of recognition of the role of the retina, and obviously the lack of an experimental investigation of ocular tracts.
Alhazen's most original contribution was that after describing how he thought the eye was anatomically constructed, he went on to consider how this anatomy would behave functionally as an optical system. His understanding of pinhole projection from his experiments appears to have influenced his consideration of image inversion in the eye, which he sought to avoid. He maintained that the rays that fell perpendicularly on the lens (or glacial humor as he called it) were further refracted outward as they left the glacial humor and the resulting image thus passed upright into the optic nerve at the back of the eye. He followed Galen in believing that the lens was the receptive organ of sight, although some of his work hints that he thought the retina was also involved.
Alhazen's synthesis of light and vision adhered to the Aristotelian scheme, exhaustively describing the process of vision in a logical, complete fashion.
Scientific method.
An aspect associated with Alhazen's optical research is related to systemic and methodological reliance on experimentation ("i'tibar")(Arabic: إعتبار) and controlled testing in his scientific inquiries. Moreover, his experimental directives rested on combining classical physics ("ilm tabi'i") with mathematics ("ta'alim"; geometry in particular). This mathematical-physical approach to experimental science supported most of his propositions in "Kitab al-Manazir" ("The Optics"; "De aspectibus" or "Perspectivae") and grounded his theories of vision, light and colour, as well as his research in catoptrics and dioptrics (the study of the refraction of light). According to Matthias Schramm, Alhazen: was the first to make a systematic use of the method of varying the experimental conditions in a constant and uniform manner, in an experiment showing that the intensity of the light-spot formed by the projection of the moonlight through two small apertures onto a screen diminishes constantly as one of the apertures is gradually blocked up. G. J. Toomer expressed some skepticism regarding Schramm's view, arguing that caution is needed to avoid reading anachronistically particular passages in Alhazen's very large body of work, and while acknowledging Alhazen's importance in developing experimental techniques, argued that he should not be considered in isolation from other Islamic and ancient thinkers.
A. Mark Smith's critical editions (2001, 2006, 2008, 2010) of "De Aspectibus" contain a Latin glossary with page numbers of each occurrence of the words, to illustrate Alhacen's experimental viewpoint. Smith shows that Alhacen was received well in the West because he reinforced the importance of the Hellenic tradition to them.
Alhazen's problem.
His work on catoptrics in Book V of the Book of Optics contains a discussion of what is now known as Alhazen's problem, first formulated by Ptolemy in 150 AD. It comprises drawing lines from two points in the plane of a circle meeting at a point on the circumference and making equal angles with the normal at that point. This is equivalent to finding the point on the edge of a circular billiard table at which a cue ball at a given point must be aimed in order to carom off the edge of the table and hit another ball at a second given point. Thus, its main application in optics is to solve the problem, "Given a light source and a spherical mirror, find the point on the mirror where the light will be reflected to the eye of an observer." This leads to an equation of the fourth degree. This eventually led Alhazen to derive a formula for the sum of fourth powers, where previously only the formulas for the sums of squares and cubes had been stated. His method can be readily generalized to find the formula for the sum of any integral powers, although he did not himself do this (perhaps because he only needed the fourth power to calculate the volume of the paraboloid he was interested in). He used his result on sums of integral powers to perform what would now be called an integration, where the formulas for the sums of integral squares and fourth powers allowed him to calculate the volume of a paraboloid. Alhazen eventually solved the problem using conic sections and a geometric proof. His solution was extremely long and complicated and may not have been understood by mathematicians reading him in Latin translation. Later mathematicians used Descartes' analytical methods to analyse the problem, with a new solution being found in 1997 by the Oxford mathematician Peter M. Neumann. Recently, Mitsubishi Electric Research Laboratories (MERL) researchers Amit Agrawal, Yuichi Taguchi and Srikumar Ramalingam solved the extension of Alhazen's problem to general rotationally symmetric quadric mirrors including hyperbolic, parabolic and elliptical mirrors. They showed that the mirror reflection point can be computed by solving an eighth degree equation in the most general case. If the camera (eye) is placed on the axis of the mirror, the degree of the equation reduces to six. Alhazen's problem can also be extended to multiple refractions from a spherical ball. Given a light source and a spherical ball of certain refractive index, the closest point on the spherical ball where the light is refracted to the eye of the observer can be obtained by solving a tenth degree equation.
Refraction.
 has noted that Alhazen's treatment of refraction describes an experimental setup without publication of data. Ptolemy published his experimental results for refraction, in contrast. One generation before Alhazen, Ibn Sahl discovered his statement of the lengths of the hypotenuse for each incident and refracted right triangle, respectively. This is equivalent to Descartes' formulation for refraction. Alhacen's convention for describing the incident and refracted angles is still in use. His failure to publish his data is an open question.
Other contributions.
The Kitab al-Manazir (Book of Optics) describes several experimental observations that Alhazen made and how he used his results to explain certain optical phenomena using mechanical analogies. He conducted experiments with projectiles, and a description of his conclusions is: "it was only the impact of perpendicular projectiles on surfaces which was forceful enough to enable them to penetrate whereas the oblique ones were deflected. For example, to explain refraction from a rare to a dense medium, he used the mechanical analogy of an iron ball thrown at a thin slate covering a wide hole in a metal sheet. A perpendicular throw would break the slate and pass through, whereas an oblique one with equal force and from an equal distance would not." He also used this result to explain how intense, direct light hurts the eye, using a mechanical analogy: "Alhazen associated 'strong' lights with perpendicular rays and 'weak' lights with oblique ones. The obvious answer to the problem of multiple rays and the eye was in the choice of the perpendicular ray since there could only be one such ray from each point on the surface of the object which could penetrate the eye."
Sudanese psychologist Omar Khaleefa has argued that Alhazen should be considered be the "founder of experimental psychology", for his pioneering work on the psychology of visual perception and optical illusions. Khaleefa has also argued that Alhazen should also be considered the "founder of psychophysics", a sub-discipline and precursor to modern psychology. Although Alhazen made many subjective reports regarding vision, there is no evidence that he used quantitative psychophysical techniques and the claim has been rebuffed.
Alhazen offered an explanation of the Moon illusion, an illusion that played an important role in the scientific tradition of medieval Europe. Many authors repeated explanations that attempted to solve the problem of the Moon appearing larger near the horizon than it does when higher up in the sky, a debate that is still unresolved. Alhazen argued against Ptolemy's refraction theory, and defined the problem in terms of perceived, rather than real, enlargement. He said that judging the distance of an object depends on there being an uninterrupted sequence of intervening bodies between the object and the observer. When the Moon is high in the sky there are no intervening objects, so the Moon appears close. The perceived size of an object of constant angular size varies with its perceived distance. Therefore, the Moon appears closer and smaller high in the sky, and further and larger on the horizon. Through works by Roger Bacon, John Pecham and Witelo based on Alhazen's explanation, the Moon illusion gradually came to be accepted as a psychological phenomenon, with the refraction theory being rejected in the 17th century. Although Alhazen is often credited with the perceived distance explanation, he was not the first author to offer it. Cleomedes ( 2nd century) gave this account (in addition to refraction), and he credited it to Posidonius ( 135-50 BC). Ptolemy may also have offered this explanation in his "Optics", but the text is obscure. Alhazen's writings were more widely available in the Middle Ages than those of these earlier authors, and that probably explains why Alhazen received the credit.
Other works on physics.
Optical treatises.
Besides the "Book of Optics", Alhazen wrote several other treatises on the same subject, including his "Risala fi l-Daw’" ("Treatise on Light"). He investigated the properties of luminance, the rainbow, eclipses, twilight, and moonlight. Experiments with mirrors and magnifying lenses provided the foundation for his theories on catoptrics.
Celestial physics.
Alhazen discussed the physics of the celestial region in his "Epitome of Astronomy", arguing that Ptolemaic models needed to be understood in terms of physical objects rather than abstract hypotheses; in other words that it should be possible to create physical models where (for example) none of the celestial bodies would collide with each other. The suggestion of mechanical models for the Earth centred Ptolemaic model "greatly contributed to the eventual triumph of the Ptolemaic system among the Christians of the West". Alhazen's determination to root astronomy in the realm of physical objects was important however, because it meant astronomical hypotheses "were accountable to the laws of physics", and could be criticised and improved upon in those terms.
He also wrote "Maqala fi daw al-qamar" ("On the Light of the Moon").
Mechanics.
In his work, Alhazen discussed theories on the motion of a body. In his "Treatise on Place", Alhazen disagreed with Aristotle's view that nature abhors a void, and he used geometry in an attempt to demonstrate that place ("al-makan") is the imagined three-dimensional void between the inner surfaces of a containing body.
Astronomical works.
On the Configuration of the World.
In his "On the Configuration of the World" Alhazen presented a detailed description of the physical structure of the earth:The earth as a whole is a round sphere whose center is the center of the world. It is stationary in its [the world's] middle, fixed in it and not moving in any direction nor moving with any of the varieties of motion, but always at rest.
The book is a non-technical explanation of Ptolemy's Almagest, which was eventually translated into Hebrew and Latin in the 13th and 14th centuries and subsequently had an influence on astronomers such as Georg von Peuerbach during the European Middle Ages and Renaissance.
Doubts Concerning Ptolemy.
In his "Al-Shukūk ‛alā Batlamyūs", variously translated as "Doubts Concerning Ptolemy" or "Aporias against Ptolemy", published at some time between 1025 and 1028, Alhazen criticized Ptolemy's "Almagest", "Planetary Hypotheses", and "Optics", pointing out various contradictions he found in these works, particularly in astronomy. Ptolemy's "Almagest" concerned mathematical theories regarding the motion of the planets, whereas the "Hypotheses" concerned what Ptolemy thought was the actual configuration of the planets. Ptolemy himself acknowledged that his theories and configurations did not always agree with each other, arguing that this was not a problem provided it did not result in noticeable error, but Alhazen was particularly scathing in his criticism of the inherent contradictions in Ptolemy's works. He considered that some of the mathematical devices Ptolemy introduced into astronomy, especially the equant, failed to satisfy the physical requirement of uniform circular motion, and noted the absurdity of relating actual physical motions to imaginary mathematical points, lines and circles:
Ptolemy assumed an arrangement ("hay'a") that cannot exist, and the fact that this arrangement produces in his imagination the motions that belong to the planets does not free him from the error he committed in his assumed arrangement, for the existing motions of the planets cannot be the result of an arrangement that is impossible to exist... [F]or a man to imagine a circle in the heavens, and to imagine the planet moving in it does not bring about the planet's motion.
Having pointed out the problems, Alhazen appears to have intended to resolve the contradictions he pointed out in Ptolemy in a later work. Alhazen's belief was that there was a "true configuration" of the planets which Ptolemy had failed to grasp; his intention was to complete and repair Ptolemy's system, not to replace it completely.
In the "Doubts Concerning Ptolemy" Alhazen set out his views on the difficulty of attaining scientific knowledge and the need to question existing authorities and theories:
Truth is sought for itself [but] the truths, [he warns] are immersed in uncertainties [and the scientific authorities (such as Ptolemy, whom he greatly respected) are] not immune from error...
He held that the criticism of existing theories—which dominated this book—holds a special place in the growth of scientific knowledge.
Model of the Motions of Each of the Seven Planets.
Alhazen's "The Model of the Motions of Each of the Seven Planets" was written 1038. Only one damaged manuscript has been found, with only the introduction and the first section, on the theory of planetary motion, surviving. (There was also a second section on astronomical calculation, and a third section, on astronomical instruments.) Following on from his "Doubts on Ptolemy", Alhazen described a new, geometry-based planetary model, describing the motions of the planets in terms of spherical geometry, infinitesimal geometry and trigonometry. He kept a geocentric universe and assumed that celestial motions are uniformly circular, which required the inclusion of epicycles to explain observed motion, but he managed to eliminate Ptolemy's equant. In general, his model made no attempt to provide a causal explanation of the motions, but concentrated on providing a complete, geometric description which could be used to explain observed motions, without the contradictions inherent in Ptolemy's model.
Other astronomical works.
Alhazen wrote a total of twenty-five astronomical works, some concerning technical issues such as "Exact Determination of the Meridian", a second group concerning accurate astronomical observation, a third group concerning various astronomical problems and questions such as the location of the Milky Way; Alhazen argued for a distant location, based on the fact that it does not move in relation to the fixed stars. The fourth group consists of ten works on astronomical theory, including the "Doubts" and "Model of the Motions" discussed above.
Mathematical works.
In mathematics, Alhazen built on the mathematical works of Euclid and Thabit ibn Qurra and worked on "the beginnings of the link between algebra and geometry."
He developed a formula for adding the first 100 natural numbers, using a geometric proof to prove the formula.
Geometry.
Alhazen explored what is now known as the Euclidean parallel postulate, the fifth postulate in Euclid's "Elements", using a proof by contradiction, and in effect introducing the concept of motion into geometry. He formulated the Lambert quadrilateral, which Boris Abramovich Rozenfeld names the "Ibn al-Haytham–Lambert quadrilateral". His theorems on quadrilaterals, including the Lambert quadrilateral, were the first theorems on elliptical geometry and hyperbolic geometry. These theorems, along with his alternative postulates, such as Playfair's axiom, can be seen as marking the beginning of non-Euclidean geometry. His work had a considerable influence on its development among the later Persian geometers Omar Khayyám and Nasīr al-Dīn al-Tūsī, and the European geometers Witelo, Gersonides, and Alfonso.
In elementary geometry, Alhazen attempted to solve the problem of squaring the circle using the area of lunes (crescent shapes), but later gave up on the impossible task. The two lunes formed from a right triangle by erecting a semicircle on each of the triangle's sides, inward for the hypotenuse and outward for the other two sides, are known as the lunes of Alhazen; they have the same total area as the triangle itself.
Number theory.
His contributions to number theory include his work on perfect numbers. In his "Analysis and Synthesis", Alhazen may have been the first to state that every even perfect number is of the form 2"n"−1(2"n" − 1) where 2"n" − 1 is prime, but he was not able to prove this result successfully (Euler later proved it in the 18th century).
Alhazen solved problems involving congruences using what is now called Wilson's theorem. In his "Opuscula", Alhazen considers the solution of a system of congruences, and gives two general methods of solution. His first method, the canonical method, involved Wilson's theorem, while his second method involved a version of the Chinese remainder theorem.
Other works.
Influence of Melodies on the Souls of Animals.
Alhazen also wrote a "Treatise on the Influence of Melodies on the Souls of Animals", although no copies have survived. It appears to have been concerned with the question of whether animals could react to music, for example whether a camel would increase or decrease its pace.
Engineering.
In engineering, one account of his career as a civil engineer has him summoned to Egypt by the Fatimid Caliph, Al-Hakim bi-Amr Allah, to regulate the flooding of the Nile River. He carried out a detailed scientific study of the annual inundation of the Nile River, and he drew plans for building a dam, at the site of the modern-day Aswan Dam. His field work, however, later made him aware of the impracticality of this scheme, and he soon feigned madness so he could avoid punishment from the Caliph.
Philosophy.
In his "Treatise on Place", Alhazen disagreed with Aristotle's view that nature abhors a void, and he used geometry in an attempt to demonstrate that place ("al-makan") is the imagined three-dimensional void between the inner surfaces of a containing body. Abd-el-latif, a supporter of Aristotle's philosophical view of place, later criticized the work in "Fi al-Radd ‘ala Ibn al-Haytham fi al-makan" ("A refutation of Ibn al-Haytham’s place") for its geometrization of place.
Alhazen also discussed space perception and its epistemological implications in his "Book of Optics". In "tying the visual perception of space to prior bodily experience, Alhacen unequivocally rejected the
intuitiveness of spatial perception and, therefore, the autonomy of vision. Without tangible notions of distance and size for
correlation, sight can tell us next to nothing about such things."
Theology.
Alhazen was a devout Muslim, though it is uncertain which branch of Islam he followed. He may have been either a follower of the Ash'ari school of Sunni Islamic theology according to Ziauddin Sardar and Lawrence Bettany (and opposed to the views of the Mu'tazili school), a follower of the Mu'tazili school of Islamic theology according to Peter Edward Hodgson, or a possibly follower of Shia Islam according to A. I. Sabra.
Alhazen wrote a work on Islamic theology in which he discussed prophethood and developed a system of philosophical criteria to discern its false claimants in his time. He also wrote a treatise entitled "Finding the Direction of Qibla by Calculation" in which he discussed finding the Qibla, where Salat prayers are directed towards, mathematically.
He wrote in his "Doubts Concerning Ptolemy":
Truth is sought for its own sake ... Finding the truth is difficult, and the road to it is rough. For the truths are plunged in obscurity. ... God, however, has not preserved the scientist from error and has not safeguarded science from shortcomings and faults. If this had been the case, scientists would not have disagreed upon any point of science...
In "The Winding Motion", Alhazen further wrote:
From the statements made by the noble Shaykh, it is clear that he believes in Ptolemy's words in everything he says, without relying on a demonstration or calling on a proof, but by pure imitation ("taqlid"); that is how experts in the prophetic tradition have faith in Prophets, may the blessing of God be upon them. But it is not the way that mathematicians have faith in specialists in the demonstrative sciences.
Alhazen described his theology:
I constantly sought knowledge and truth, and it became my belief that for gaining access to the and closeness to God, there is no better way than that of searching for truth and knowledge.
List of works.
According to medieval biographers, Alhazen wrote more than 200 works on a wide range of subjects, of which at least 96 of his scientific works are known. Most of his works are now lost, but more than 50 of them have survived to some extent. Nearly half of his surviving works are on mathematics, 23 of them are on astronomy, and 14 of them are on optics, with a few on other subjects. Not all his surviving works have yet been studied, but some of the ones that have are given below.
Commemorations.
Ibn Al-Haytham's work has been commemorated by the naming of the Alhazen crater on the moon after him. The asteroid 59239 Alhazen was also named in his honour.
In 2014, the "Hiding in the Light" episode of "", presented by Neil deGrasse Tyson, focused on the accomplishments of Ibn al-Haytham. He was voiced by Alfred Molina in the episode.
UNESCO has declared 2015 the International Year of Light. Amongst others, this will be celebrating Ibn Al-Haytham's achievements in optics, mathematics and astronomy. An international campaign, created by the 1001 Inventions organisation, titled 1001 Inventions and the World of Ibn Al-Haytham featuring a series of interactive exhibits, workshops and live shows about his work will partner with science centers, science festivals, museums, and educational institutions, as well as digital and social media platforms. 1001 Inventions is a founding partner of the International Year of Light.
UNESCO's website on Ibn al-Haytham copies a part from Jim Al-Khalili's popular history "Pathfinders: The Golden Age of Arabic Science".
Sources.
</dl>

</doc>
<doc id="1825" url="http://en.wikipedia.org/wiki?curid=1825" title="Hermann Kolbe">
Hermann Kolbe

Hermann Kolbe ("Adolph Wilhelm Hermann Kolbe", 27 Sept. 1818–25 Nov. 1884), was a seminal contributor in the birth of modern organic chemistry as Professor at Marburg and Leipzig. Kolbe coined the term synthesis, and contributed to the philosophical demise of vitalism through synthesis of the biologic natural product acetic acid from carbon disulfide, to structural theory via modifications to the idea of "radicals" and accurate prediction of the existence of secondary and tertiary alcohols, and to the emerging array of organic reactions through his Kolbe electrolysis of carboxylate salts, the Kolbe-Schmitt reaction in the preparation of aspirin, and the Kolbe nitrile synthesis. After studies with Wöhler and Bunsen, Kolbe was involved with the early internationalization of chemistry through overseas work in London (with Frankland), and rose through the ranks of his field to edit the , to be elected to the Royal Swedish Academy of Sciences, and to win the Royal Society of London's Davy Medal in the year of his death. Despite these accomplishments and his training a storied next generation of chemists (including Zaitsev, Curtius, Beckmann, Graebe, Markovnikov, etc.), Kolbe is remembered for editing the "Journal" for more than a decade, where his rejection of Kekulé's structure of benzene, van't Hoff's theory on the origin of chirality, and von Baeyer's reforms of nomenclature were personally critical and linguistically violent. Kolbe died of a heart attack in Leipzig at age 68, six years after the death of his wife, Charlotte. He was survived by four children.
Life.
Kolbe was born in Elliehausen, near Göttingen, Kingdom of Hanover (Germany) as the eldest son of a Protestant pastor. At the age of 13 he entered the Göttingen Gymnasium, residing at the home of one of the professors. He obtained the leaving certificate (the Abitur) six years later. He had become passionate about the study of chemistry, matriculating at the University of Göttingen in the spring of 1838 in order to study with the famous chemist Friedrich Wöhler.
In 1842 he became an assistant to Robert Bunsen at the Philipps-Universität Marburg; he took his doctoral degree there in 1843. A new opportunity arose in 1845, when he became assistant to Lyon Playfair at the new "Museum of Economic Geology" in London, where he became a close friend of Edward Frankland. From 1847 he was engaged in editing the "Handwörterbuch der reinen und angewandten Chemie" ("Dictionary of Pure and Applied Chemistry") edited by Justus von Liebig, Wöhler, and Johann Christian Poggendorff, and he also wrote an important textbook. In 1851 Kolbe succeeded Bunsen as professor of chemistry at Marburg, and in 1865 he was called to the Universität Leipzig. In 1864, he was elected a foreign member of the Royal Swedish Academy of Sciences.
In 1853 he married Charlotte, the daughter of General-Major Wilhelm von Bardeleben. His wife died in 1876 after 23 years of happy marriage. They had four children.
Work in chemical research.
As late as the 1840s, and despite Friedrich Wöhler's synthesis of urea in 1828, some chemists still believed in the doctrine of vitalism, according to which a special life-force was necessary to create "organic" (i.e., in its original meaning, biologically derived) compounds. Kolbe promoted the idea that organic compounds could be derived from substances clearly sourced from outside this "organic" context, directly or indirectly, by substitution processes. (Hence, while by modern definitions, he was converting one organic molecule to another, by the parlance of his era, he was converting "inorganic"—"anorganisch"—substances into "organic" ones only thought accessible through vital processes.) He validated his theory by converting carbon disulfide to acetic acid in several steps (1843–45). Kolbe also introduced a modified idea of structural radicals, so contributing to the development of structural theory. A dramatic success came when his theoretical prediction of the existence of secondary and tertiary alcohols was confirmed by the synthesis of the first of these classes of organic molecules. Kolbe was the first person to use the word synthesis in its present day meaning, and contributed a number of new chemical reactions.
In particular, Kolbe developed procedures for the electrolysis of the salts of fatty and other carboxylic acids (Kolbe electrolysis) and prepared salicylic acid, a building block of aspirin in a process called Kolbe synthesis or Kolbe-Schmitt reaction. His method for the synthesis of nitriles is called the Kolbe nitrile synthesis, and with Edward Frankland he found that nitriles can be hydrolyzed to the corresponding acids.
In addition to his own bench research and scholarly and editorial work, Kolbe oversaw student research at Leipzig and especially at Marburg; students spending time under his tutelage included Peter Griess, Aleksandr Mikhailovich Zaitsev (known for Zaitsev's rule predicting the product composition of elimination reactions), Theodor Curtius (discoverer of diazo compounds, hydrazines, and the Curtius rearrangement), Ernst Otto Beckmann (discoverer of the Beckmann rearrangement), Carl Graebe (discoverer of alizarin), Oscar Loew, Constantin Fahlberg, Nikolai Menshutkin, Vladimir Markovnikov (first to describe carbocycles smaller and larger than cyclohexane, and known for Markovnikov's rule describing addition reactions to alkenes), Jacob Volhard, Ludwig Mond, Alexander Crum Brown (first to describe the double bond of ethylene), Maxwell Simpson, and Frederick Guthrie.
Work as journal editor.
Kolbe served for more than a decade as what, in modern terms, would be understood the senior editor of the ("Journal of practical chemistry", from 1870 to 1884), Kolbe was sometimes so severely critical of the work of others, especially after about 1874, that some wondered whether he might have been suffering a mental illness. He was intolerant of what he regarded as loose speculation parading as theory, and sought through his writings to save his beloved science of chemistry from what he regarded as the scourge of modern structural theory.
His rejection of structural chemistry, especially the theories of the structure of benzene by August Kekulé, the theory of the asymmetric carbon atom by J.H. van't Hoff, and the reform of chemical nomenclature by Adolf von Baeyer, resulted in vituperative articles in the "Journal für Praktische Chemie". Some translated quotes illustrate his manner of articulating the deep conflict between his interpretation of chemistry and that of the structural chemists: 
"...Baeyer is an excellent experimentor, but he is only an empiricist, lacking sense and capability, and his interpretations of his experiments show particular deficiency in his familiarity with the principles of true science..."
The violence of his language worked unfairly to limit his posthumous reputation. He died of a heart attack, in Leipzig.

</doc>
<doc id="1844" url="http://en.wikipedia.org/wiki?curid=1844" title="Archimedes">
Archimedes

Archimedes of Syracuse (; Greek: Ἀρχιμήδης;  287 BC –  212 BC) was an Ancient Greek mathematician, physicist, engineer, inventor, and astronomer. Although few details of his life are known, he is regarded as one of the leading scientists in classical antiquity.
Generally considered the greatest mathematician of antiquity and one of the greatest of all time, Archimedes anticipated modern calculus and analysis by applying concepts of infinitesimals and the method of exhaustion to derive and rigorously prove a range of geometrical theorems, including the area of a circle, the surface area and volume of a sphere, and the area under a parabola. Other mathematical achievements include deriving an accurate approximation of pi, defining and investigating the spiral bearing his name, and creating a system using exponentiation for expressing very large numbers. He was also one of the first to apply mathematics to physical phenomena, founding hydrostatics and statics, including an explanation of the principle of the lever. He is credited with designing innovative machines, such as his screw pump, compound pulleys, and defensive war machines to protect his native Syracuse from invasion.
Archimedes died during the Siege of Syracuse when he was killed by a Roman soldier despite orders that he should not be harmed. Cicero describes visiting the tomb of Archimedes, which was surmounted by a sphere and a cylinder, which Archimedes had requested to be placed on his tomb, representing his mathematical discoveries.
Unlike his inventions, the mathematical writings of Archimedes were little known in antiquity. Mathematicians from Alexandria read and quoted him, but the first comprehensive compilation was not made until "c." 530 AD by Isidore of Miletus in Byzantine Constantinople, while commentaries on the works of Archimedes written by Eutocius in the sixth century AD opened them to wider readership for the first time. The relatively few copies of Archimedes' written work that survived through the Middle Ages were an influential source of ideas for scientists during the Renaissance, while the discovery in 1906 of previously unknown works by Archimedes in the Archimedes Palimpsest has provided new insights into how he obtained mathematical results.
Biography.
Archimedes was born "c". 287 BC in the seaport city of Syracuse, Sicily, at that time a self-governing colony in Magna Graecia, located along the coast of Southern Italy. The date of birth is based on a statement by the Byzantine Greek historian John Tzetzes that Archimedes lived for 75 years. In "The Sand Reckoner", Archimedes gives his father's name as Phidias, an astronomer about whom nothing is known. Plutarch wrote in his "Parallel Lives" that Archimedes was related to King Hiero II, the ruler of Syracuse. A biography of Archimedes was written by his friend Heracleides but this work has been lost, leaving the details of his life obscure. It is unknown, for instance, whether he ever married or had children. During his youth, Archimedes may have studied in Alexandria, Egypt, where Conon of Samos and Eratosthenes of Cyrene were contemporaries. He referred to Conon of Samos as his friend, while two of his works ("The Method of Mechanical Theorems" and the "Cattle Problem") have introductions addressed to Eratosthenes.
Archimedes died "c". 212 BC during the Second Punic War, when Roman forces under General Marcus Claudius Marcellus captured the city of Syracuse after a two-year-long siege. According to the popular account given by Plutarch, Archimedes was contemplating a mathematical diagram when the city was captured. A Roman soldier commanded him to come and meet General Marcellus but he declined, saying that he had to finish working on the problem. The soldier was enraged by this, and killed Archimedes with his sword. Plutarch also gives a lesser-known account of the death of Archimedes which suggests that he may have been killed while attempting to surrender to a Roman soldier. According to this story, Archimedes was carrying mathematical instruments, and was killed because the soldier thought that they were valuable items. General Marcellus was reportedly angered by the death of Archimedes, as he considered him a valuable scientific asset and had ordered that he not be harmed. Marcellus called Archimedes "a geometrical Briareus".
The last words attributed to Archimedes are "Do not disturb my circles", a reference to the circles in the mathematical drawing that he was supposedly studying when disturbed by the Roman soldier. This quote is often given in Latin as ""Noli turbare circulos meos"," but there is no reliable evidence that Archimedes uttered these words and they do not appear in the account given by Plutarch. Valerius Maximus, writing in "Memorable Doings and Sayings" in the 1st century AD, gives the phrase as ""...sed protecto manibus puluere 'noli' inquit, 'obsecro, istum disturbare"'" - "... but protecting the dust with his hands, said 'I beg of you, do not disturb this.'" The phrase is also given in Katharevousa Greek as "μὴ μου τοὺς κύκλους τάραττε!" ("Mē mou tous kuklous taratte!").
The tomb of Archimedes carried a sculpture illustrating his favorite mathematical proof, consisting of a sphere and a cylinder of the same height and diameter. Archimedes had proven that the volume and surface area of the sphere are two thirds that of the cylinder including its bases. In 75 BC, 137 years after his death, the Roman orator Cicero was serving as quaestor in Sicily. He had heard stories about the tomb of Archimedes, but none of the locals was able to give him the location. Eventually he found the tomb near the Agrigentine gate in Syracuse, in a neglected condition and overgrown with bushes. Cicero had the tomb cleaned up, and was able to see the carving and read some of the verses that had been added as an inscription. A tomb discovered in a hotel courtyard in Syracuse in the early 1960s was claimed to be that of Archimedes, but its location today is unknown.
The standard versions of the life of Archimedes were written long after his death by the historians of Ancient Rome. The account of the siege of Syracuse given by Polybius in his "Universal History" was written around seventy years after Archimedes' death, and was used subsequently as a source by Plutarch and Livy. It sheds little light on Archimedes as a person, and focuses on the war machines that he is said to have built in order to defend the city.
Discoveries and inventions.
Archimedes' principle.
The most widely known anecdote about Archimedes tells of how he invented a method for determining the volume of an object with an irregular shape. According to Vitruvius, a votive crown for a temple had been made for King Hiero II, who had supplied the pure gold to be used, and Archimedes was asked to determine whether some silver had been substituted by the dishonest goldsmith. Archimedes had to solve the problem without damaging the crown, so he could not melt it down into a regularly shaped body in order to calculate its density.
While taking a bath, he noticed that the level of the water in the tub rose as he got in, and realized that this effect could be used to determine the volume of the crown. For practical purposes water is incompressible, so the submerged crown would displace an amount of water equal to its own volume. By dividing the mass of the crown by the volume of water displaced, the density of the crown could be obtained. This density would be lower than that of gold if cheaper and less dense metals had been added. Archimedes then took to the streets naked, so excited by his discovery that he had forgotten to dress, crying "Eureka!" (Greek: "εὕρηκα,"heúrēka"!", meaning "I have found [it]!"). The test was conducted successfully, proving that silver had indeed been mixed in.
The story of the golden crown does not appear in the known works of Archimedes. Moreover, the practicality of the method it describes has been called into question, due to the extreme accuracy with which one would have to measure the water displacement. Archimedes may have instead sought a solution that applied the principle known in hydrostatics as Archimedes' principle, which he describes in his treatise "On Floating Bodies". This principle states that a body immersed in a fluid experiences a buoyant force equal to the weight of the fluid it displaces. Using this principle, it would have been possible to compare the density of the golden crown to that of solid gold by balancing the crown on a scale with a gold reference sample, then immersing the apparatus in water. The difference in density between the two samples would cause the scale to tip accordingly. Galileo considered it "probable that this method is the same that Archimedes followed, since, besides being very accurate, it is based on demonstrations found by Archimedes himself." In a 12th-century text titled "Mappae clavicula" there are instructions on how to perform the weighings in the water in order to calculate the percentage of silver used, and thus solve the problem. The Latin poem "Carmen de ponderibus et mensuris" of the 4th or 5th century describes the use of a hydrostatic balance to solve the problem of the crown, and attributes the method to Archimedes.
Archimedes' screw.
A large part of Archimedes' work in engineering arose from fulfilling the needs of his home city of Syracuse. The Greek writer Athenaeus of Naucratis described how King Hiero II commissioned Archimedes to design a huge ship, the "Syracusia", which could be used for luxury travel, carrying supplies, and as a naval warship. The "Syracusia" is said to have been the largest ship built in classical antiquity. According to Athenaeus, it was capable of carrying 600 people and included garden decorations, a gymnasium and a temple dedicated to the goddess Aphrodite among its facilities. Since a ship of this size would leak a considerable amount of water through the hull, the Archimedes' screw was purportedly developed in order to remove the bilge water. Archimedes' machine was a device with a revolving screw-shaped blade inside a cylinder. It was turned by hand, and could also be used to transfer water from a low-lying body of water into irrigation canals. The Archimedes' screw is still in use today for pumping liquids and granulated solids such as coal and grain. The Archimedes' screw described in Roman times by Vitruvius may have been an improvement on a screw pump that was used to irrigate the Hanging Gardens of Babylon. The world's first seagoing steamship with a screw propeller was the "SS Archimedes", which was launched in 1839 and named in honor of Archimedes and his work on the screw.
Claw of Archimedes.
The Claw of Archimedes is a weapon that he is said to have designed in order to defend the city of Syracuse. Also known as "the ship shaker," the claw consisted of a crane-like arm from which a large metal grappling hook was suspended. When the claw was dropped onto an attacking ship the arm would swing upwards, lifting the ship out of the water and possibly sinking it. There have been modern experiments to test the feasibility of the claw, and in 2005 a television documentary entitled "Superweapons of the Ancient World" built a version of the claw and concluded that it was a workable device.
Heat ray.
The 2nd century AD author Lucian wrote that during the Siege of Syracuse ("c." 214–212 BC), Archimedes destroyed enemy ships with fire. Centuries later, Anthemius of Tralles mentions burning-glasses as Archimedes' weapon. The device, sometimes called the "Archimedes heat ray", was used to focus sunlight onto approaching ships, causing them to catch fire.
This purported weapon has been the subject of ongoing debate about its credibility since the Renaissance. René Descartes rejected it as false, while modern researchers have attempted to recreate the effect using only the means that would have been available to Archimedes. It has been suggested that a large array of highly polished bronze or copper shields acting as mirrors could have been employed to focus sunlight onto a ship. This would have used the principle of the parabolic reflector in a manner similar to a solar furnace.
A test of the Archimedes heat ray was carried out in 1973 by the Greek scientist Ioannis Sakkas. The experiment took place at the Skaramagas naval base outside Athens. On this occasion 70 mirrors were used, each with a copper coating and a size of around five by three feet (1.5 by 1 m). The mirrors were pointed at a plywood mock-up of a Roman warship at a distance of around 160 feet (50 m). When the mirrors were focused accurately, the ship burst into flames within a few seconds. The plywood ship had a coating of tar paint, which may have aided combustion. A coating of tar would have been commonplace on ships in the classical era.
In October 2005 a group of students from the Massachusetts Institute of Technology carried out an experiment with 127 one-foot (30 cm) square mirror tiles, focused on a mock-up wooden ship at a range of around 100 feet (30 m). Flames broke out on a patch of the ship, but only after the sky had been cloudless and the ship had remained stationary for around ten minutes. It was concluded that the device was a feasible weapon under these conditions. The MIT group repeated the experiment for the television show "MythBusters", using a wooden fishing boat in San Francisco as the target. Again some charring occurred, along with a small amount of flame. In order to catch fire, wood needs to reach its autoignition temperature, which is around 300 °C (570 °F).
When "MythBusters" broadcast the result of the San Francisco experiment in January 2006, the claim was placed in the category of "busted" (or failed) because of the length of time and the ideal weather conditions required for combustion to occur. It was also pointed out that since Syracuse faces the sea towards the east, the Roman fleet would have had to attack during the morning for optimal gathering of light by the mirrors. "MythBusters" also pointed out that conventional weaponry, such as flaming arrows or bolts from a catapult, would have been a far easier way of setting a ship on fire at short distances.
In December 2010, "MythBusters" again looked at the heat ray story in a special edition entitled "President's Challenge". Several experiments were carried out, including a large scale test with 500 schoolchildren aiming mirrors at a mock-up of a Roman sailing ship 400 feet (120 m) away. In all of the experiments, the sail failed to reach the 210 °C (410 °F) required to catch fire, and the verdict was again "busted". The show concluded that a more likely effect of the mirrors would have been blinding, dazzling, or distracting the crew of the ship.
Other discoveries and inventions.
While Archimedes did not invent the lever, he gave an explanation of the principle involved in his work "On the Equilibrium of Planes". Earlier descriptions of the lever are found in the Peripatetic school of the followers of Aristotle, and are sometimes attributed to Archytas. According to Pappus of Alexandria, Archimedes' work on levers caused him to remark: "Give me a place to stand on, and I will move the Earth." (Greek: δῶς μοι πᾶ στῶ καὶ τὰν γᾶν κινάσω) Plutarch describes how Archimedes designed block-and-tackle pulley systems, allowing sailors to use the principle of leverage to lift objects that would otherwise have been too heavy to move. Archimedes has also been credited with improving the power and accuracy of the catapult, and with inventing the odometer during the First Punic War. The odometer was described as a cart with a gear mechanism that dropped a ball into a container after each mile traveled.
Cicero (106–43 BC) mentions Archimedes briefly in his dialogue "De re publica", which portrays a fictional conversation taking place in 129 BC. After the capture of Syracuse "c." 212 BC, General Marcus Claudius Marcellus is said to have taken back to Rome two mechanisms, constructed by Archimedes and used as aids in astronomy, which showed the motion of the Sun, Moon and five planets. Cicero mentions similar mechanisms designed by Thales of Miletus and Eudoxus of Cnidus. The dialogue says that Marcellus kept one of the devices as his only personal loot from Syracuse, and donated the other to the Temple of Virtue in Rome. Marcellus' mechanism was demonstrated, according to Cicero, by Gaius Sulpicius Gallus to Lucius Furius Philus, who described it thus:
Hanc sphaeram Gallus cum moveret, fiebat ut soli luna totidem conversionibus in aere illo quot diebus in ipso caelo succederet, ex quo et in caelo sphaera solis fieret eadem illa defectio, et incideret luna tum in eam metam quae esset umbra terrae, cum sol e regione. — When Gallus moved the globe, it happened that the Moon followed the Sun by as many turns on that bronze contrivance as in the sky itself, from which also in the sky the Sun's globe became to have that same eclipse, and the Moon came then to that position which was its shadow on the Earth, when the Sun was in line.
This is a description of a planetarium or orrery. Pappus of Alexandria stated that Archimedes had written a manuscript (now lost) on the construction of these mechanisms entitled "On Sphere-Making". Modern research in this area has been focused on the Antikythera mechanism, another device built  100 BC that was probably designed for the same purpose. Constructing mechanisms of this kind would have required a sophisticated knowledge of differential gearing. This was once thought to have been beyond the range of the technology available in ancient times, but the discovery of the Antikythera mechanism in 1902 has confirmed that devices of this kind were known to the ancient Greeks.
Mathematics.
While he is often regarded as a designer of mechanical devices, Archimedes also made contributions to the field of mathematics. Plutarch wrote: "He placed his whole affection and ambition in those purer speculations where there can be no reference to the vulgar needs of life."
Archimedes was able to use infinitesimals in a way that is similar to modern integral calculus. Through proof by contradiction (reductio ad absurdum), he could give answers to problems to an arbitrary degree of accuracy, while specifying the limits within which the answer lay. This technique is known as the method of exhaustion, and he employed it to approximate the value of π. In "Measurement of a Circle" he did this by drawing a larger regular hexagon outside a circle and a smaller regular hexagon inside the circle, and progressively doubling the number of sides of each regular polygon, calculating the length of a side of each polygon at each step. As the number of sides increases, it becomes a more accurate approximation of a circle. After four such steps, when the polygons had 96 sides each, he was able to determine that the value of π lay between 31⁄7 (approximately 3.1429) and 310⁄71 (approximately 3.1408), consistent with its actual value of approximately 3.1416. He also proved that the area of a circle was equal to π multiplied by the square of the radius of the circle (πr2). In "On the Sphere and Cylinder", Archimedes postulates that any magnitude when added to itself enough times will exceed any given magnitude. This is the Archimedean property of real numbers.
In "Measurement of a Circle", Archimedes gives the value of the square root of 3 as lying between 265⁄153 (approximately 1.7320261) and 1351⁄780 (approximately 1.7320512). The actual value is approximately 1.7320508, making this a very accurate estimate. He introduced this result without offering any explanation of how he had obtained it. This aspect of the work of Archimedes caused John Wallis to remark that he was: "as it were of set purpose to have covered up the traces of his investigation as if he had grudged posterity the secret of his method of inquiry while he wished to extort from them assent to his results." It is possible that he used an iterative procedure to calculate these values.
In "The Quadrature of the Parabola", Archimedes proved that the area enclosed by a parabola and a straight line is 4⁄3 times the area of a corresponding inscribed triangle as shown in the figure at right. He expressed the solution to the problem as an infinite geometric series with the common ratio 1⁄4:
If the first term in this series is the area of the triangle, then the second is the sum of the areas of two triangles whose bases are the two smaller secant lines, and so on. This proof uses a variation of the series 1/4 + 1/16 + 1/64 + 1/256 + · · · which sums to 1⁄3.
In "The Sand Reckoner", Archimedes set out to calculate the number of grains of sand that the universe could contain. In doing so, he challenged the notion that the number of grains of sand was too large to be counted. He wrote: "There are some, King Gelo (Gelo II, son of Hiero II), who think that the number of the sand is infinite in multitude; and I mean by the sand not only that which exists about Syracuse and the rest of Sicily but also that which is found in every region whether inhabited or uninhabited." To solve the problem, Archimedes devised a system of counting based on the myriad. The word is from the Greek μυριάς "murias", for the number 10,000. He proposed a number system using powers of a myriad of myriads (100 million) and concluded that the number of grains of sand required to fill the universe would be 8 vigintillion, or 8×1063.
Writings.
The works of Archimedes were written in Doric Greek, the dialect of ancient Syracuse. The written work of Archimedes has not survived as well as that of Euclid, and seven of his treatises are known to have existed only through references made to them by other authors. Pappus of Alexandria mentions "On Sphere-Making" and another work on polyhedra, while Theon of Alexandria quotes a remark about refraction from the now-lost "Catoptrica". During his lifetime, Archimedes made his work known through correspondence with the mathematicians in Alexandria. The writings of Archimedes were first collected by the Byzantine Greek architect Isidore of Miletus ("c". 530 AD), while commentaries on the works of Archimedes written by Eutocius in the sixth century AD helped to bring his work a wider audience. Archimedes' work was translated into Arabic by Thābit ibn Qurra (836–901 AD), and Latin by Gerard of Cremona ("c." 1114–1187 AD). During the Renaissance, the "Editio Princeps" (First Edition) was published in Basel in 1544 by Johann Herwagen with the works of Archimedes in Greek and Latin. Around the year 1586 Galileo Galilei invented a hydrostatic balance for weighing metals in air and water after apparently being inspired by the work of Archimedes.
Apocryphal works.
Archimedes' "Book of Lemmas" or "Liber Assumptorum" is a treatise with fifteen propositions on the nature of circles. The earliest known copy of the text is in Arabic. The scholars T. L. Heath and Marshall Clagett argued that it cannot have been written by Archimedes in its current form, since it quotes Archimedes, suggesting modification by another author. The "Lemmas" may be based on an earlier work by Archimedes that is now lost.
It has also been claimed that Heron's formula for calculating the area of a triangle from the length of its sides was known to Archimedes. However, the first reliable reference to the formula is given by Heron of Alexandria in the 1st century AD.
Archimedes Palimpsest.
The foremost document containing the work of Archimedes is the Archimedes Palimpsest. In 1906, the Danish professor Johan Ludvig Heiberg visited Constantinople and examined a 174-page goatskin parchment of prayers written in the 13th century AD. He discovered that it was a palimpsest, a document with text that had been written over an erased older work. Palimpsests were created by scraping the ink from existing works and reusing them, which was a common practice in the Middle Ages as vellum was expensive. The older works in the palimpsest were identified by scholars as 10th century AD copies of previously unknown treatises by Archimedes. The parchment spent hundreds of years in a monastery library in Constantinople before being sold to a private collector in the 1920s. On October 29, 1998 it was sold at auction to an anonymous buyer for $2 million at Christie's in New York. The palimpsest holds seven treatises, including the only surviving copy of "On Floating Bodies" in the original Greek. It is the only known source of "The Method of Mechanical Theorems", referred to by Suidas and thought to have been lost forever. "Stomachion" was also discovered in the palimpsest, with a more complete analysis of the puzzle than had been found in previous texts. The palimpsest is now stored at the Walters Art Museum in Baltimore, Maryland, where it has been subjected to a range of modern tests including the use of ultraviolet and x-ray light to read the overwritten text.
The treatises in the Archimedes Palimpsest are: "On the Equilibrium of Planes, On Spirals, Measurement of a Circle, On the Sphere and the Cylinder, On Floating Bodies, The Method of Mechanical Theorems" and "Stomachion".
Notes.
a. In the preface to "On Spirals" addressed to Dositheus of Pelusium, Archimedes says that "many years have elapsed since Conon's death." Conon of Samos lived "c." 280–220 BC, suggesting that Archimedes may have been an older man when writing some of his works.
b. The treatises by Archimedes known to exist only through references in the works of other authors are: "On Sphere-Making" and a work on polyhedra mentioned by Pappus of Alexandria; "Catoptrica", a work on optics mentioned by Theon of Alexandria; "Principles", addressed to Zeuxippus and explaining the number system used in "The Sand Reckoner"; "On Balances and Levers"; "On Centers of Gravity"; "On the Calendar". Of the surviving works by Archimedes, T. L. Heath offers the following suggestion as to the order in which they were written: "On the Equilibrium of Planes I", "The Quadrature of the Parabola", "On the Equilibrium of Planes II", "On the Sphere and the Cylinder I, II", "On Spirals", "On Conoids and Spheroids", "On Floating Bodies I, II", "On the Measurement of a Circle", "The Sand Reckoner".
c. Boyer, Carl Benjamin "A History of Mathematics" (1991) ISBN 0-471-54397-7 "Arabic scholars inform us that the familiar area formula for a triangle in terms of its three sides, usually known as Heron's formula — "k" = √("s"("s" − "a")("s" − "b")("s" − "c")), where "s" is the semiperimeter — was known to Archimedes several centuries before Heron lived. Arabic scholars also attribute to Archimedes the 'theorem on the broken chord' ... Archimedes is reported by the Arabs to have given several proofs of the theorem."
d. "It was usual to smear the seams or even the whole hull with pitch or with pitch and wax". In Νεκρικοὶ Διάλογοι ("Dialogues of the Dead"), Lucian refers to coating the seams of a skiff with wax, a reference to pitch (tar) or wax.
External links.
Listen to this article ()
This audio file was created from a revision of the "Archimedes" article dated 2009-03-31, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="1919" url="http://en.wikipedia.org/wiki?curid=1919" title="Antwerp (disambiguation)">
Antwerp (disambiguation)

Antwerp is a city in Belgium and capital of the Antwerp province.
Antwerp may also refer to:

</doc>
<doc id="1924" url="http://en.wikipedia.org/wiki?curid=1924" title="Argo Navis">
Argo Navis

Argo Navis (or simply Argo) was a large constellation in the southern sky that has since been divided into three constellations. It represented the "Argo", the ship used by Jason and the Argonauts in Greek mythology. The abbreviation was "Arg" and the genitive was "Argus Navis".
Due to precession, the stars of Argo have been shifted farther south since Classical times, and far fewer of its stars are visible today from the latitude of the Mediterranean.
The original constellation was found low near the southern horizon of the Mediterranean sky. The ship became visible in springtime and sailed westward, skimming along the southern horizon. The ancient Greeks identified it with the ship sailed by Jason and the Argonauts in search of the Golden Fleece.
Argo Navis is the only one of the 48 constellations listed by the 2nd century astronomer Ptolemy that is no longer officially recognized as a constellation. It was unwieldy due to its enormous size: were it still considered a single constellation, it would be the largest of all. In 1752, the French astronomer Nicolas Louis de Lacaille subdivided it into Carina (the keel, or the hull, of the ship), Puppis (the poop deck, or stern), and Vela (the sails). When Argo Navis was split, Lacaille did not retain Bayer's designations (which bore scant relationship to the actual positions of the stars), but like Bayer he did use a single Greek-letter sequence for the three parts: Carina has the α, β and ε, Vela has γ and δ, Puppis has ζ, and so on.:82 (For the dimmer stars, however, Lacaille used a separate Latin-letter sequence for each part.):82
The constellation Pyxis (the mariner's compass) occupies an area which in antiquity was considered part of Argo's mast. Some authors state that Pyxis was part of the Greek conception of Argo Navis, but magnetic compasses were unknown in ancient Greek times. Lacaille considered it a separate constellation, representing one of the modern scientific instruments he placed among the constellations (like Microscopium and Telescopium); he assigned it Bayer designations separate from those of Carina, Puppis and Vela, and his illustration shows an isolated instrument not related to the figure of Argo.:262 In 1844 John Herschel suggested formalizing the mast as a new constellation, Malus, to replace Lacaille's Pyxis, but the idea did not catch on.
The Māori had several names for what was the constellation Argo, including "Te Waka-o-Tamarereti", "Te Kohi-a-Autahi", and "Te Kohi".

</doc>
<doc id="1937" url="http://en.wikipedia.org/wiki?curid=1937" title="Alexander Fleming">
Alexander Fleming

Sir Alexander Fleming, FRSE, FRS, FRCS(Eng) (6 August 1881 – 11 March 1955) was a Scottish biologist, pharmacologist and botanist. He wrote many articles on bacteriology, immunology, and chemotherapy. His best-known discoveries are the enzyme lysozyme in 1923 and the antibiotic substance benzylpenicillin (Penicillin G) from the mould "Penicillium notatum" in 1928, for which he shared the Nobel Prize in Physiology or Medicine in 1945 with Howard Florey and Ernst Boris Chain.
Early life and education.
Fleming was born on 6 August 1881 at Lochfield farm near Darvel, in Ayrshire, Scotland. He was the third of the four children of farmer Hugh Fleming (1816–1888) from his second marriage to Grace Stirling Morton (1848–1928), the daughter of a neighbouring farmer. Hugh Fleming had four surviving children from his first marriage. He was 59 at the time of his second marriage, and died when Alexander (known as Alec) was seven.
Fleming went to Loudoun Moor School and Darvel School, and earned a two-year scholarship to Kilmarnock Academy before moving to London, where he attended the Royal Polytechnic Institution. After working in a shipping office for four years, the twenty-year-old Fleming inherited some money from an uncle, John Fleming. His elder brother, Tom, was already a physician and suggested to his younger sibling that he should follow the same career, and so in 1903, the younger Alexander enrolled at St Mary's Hospital Medical School in Paddington; he qualified with an MBBS degree from the school with distinction in 1906. 
Fleming had been a private in the London Scottish Regiment of the Volunteer Force since 1900, and had been a member of the rifle club at the medical school. The captain of the club, wishing to retain Fleming in the team suggested that he join the research department at St Mary's, where he became assistant bacteriologist to Sir Almroth Wright, a pioneer in vaccine therapy and immunology. In 1908, he gained a BSc degree with Gold Medal in Bacteriology, and became a lecturer at St Mary's until 1914.
Fleming served throughout World War I as a captain in the Royal Army Medical Corps, and was Mentioned in Dispatches. He and many of his colleagues worked in battlefield hospitals at the Western Front in France. In 1918 he returned to St Mary's Hospital, where he was elected Professor of Bacteriology of the University of London in 1928. In 1951 he was elected the Rector of the University of Edinburgh for a term of 3 years.
Research.
Work before penicillin.
Following World War I, Fleming actively searched for anti-bacterial agents, having witnessed the death of many soldiers from sepsis resulting from infected wounds. Antiseptics killed the patients' immunological defences more effectively than they killed the invading bacteria. In an article he submitted for the medical journal "The Lancet" during World War I, Fleming described an ingenious experiment, which he was able to conduct as a result of his own glass blowing skills, in which he explained why antiseptics were killing more soldiers than infection itself during World War I. Antiseptics worked well on the surface, but deep wounds tended to shelter anaerobic bacteria from the antiseptic agent, and antiseptics seemed to remove beneficial agents produced that protected the patients in these cases at least as well as they removed bacteria, and did nothing to remove the bacteria that were out of reach. Sir Almroth Wright strongly supported Fleming's findings, but despite this, most army physicians over the course of the war continued to use antiseptics even in cases where this worsened the condition of the patients.
Accidental discovery.
"When I woke up just after dawn on September 28, 1928, I certainly didn't plan to revolutionise all medicine by discovering the world's first antibiotic, or bacteria killer," Fleming would later say, "But I suppose that was exactly what I did."
By 1927, Fleming had been investigating the properties of staphylococci. He was already well-known from his earlier work, and had developed a reputation as a brilliant researcher, but his laboratory was often untidy. On 3 September 1928, Fleming returned to his laboratory having spent August on holiday with his family. Before leaving, he had stacked all his cultures of staphylococci on a bench in a corner of his laboratory. On returning, Fleming noticed that one culture was contaminated with a fungus, and that the colonies of staphylococci immediately surrounding the fungus had been destroyed, whereas other staphylococci colonies farther away were normal, famously remarking "That's funny". Fleming showed the contaminated culture to his former assistant Merlin Price, who reminded him, "That's how you discovered lysozyme." Fleming grew the mould in a pure culture and found that it produced a substance that killed a number of disease-causing bacteria. He identified the mould as being from the "Penicillium" genus, and, after some months of calling it "mould juice", named the substance it released "penicillin" on 7 March 1929. The laboratory in which Fleming discovered and tested penicillin is preserved as the Alexander Fleming Laboratory Museum in St. Mary's Hospital, Paddington.
He investigated its positive anti-bacterial effect on many organisms, and noticed that it affected bacteria such as staphylococci and many other Gram-positive pathogens that cause scarlet fever, pneumonia, meningitis and diphtheria, but not typhoid fever or paratyphoid fever, which are caused by Gram-negative bacteria, for which he was seeking a cure at the time. It also affected "Neisseria gonorrhoeae," which causes gonorrhoea although this bacterium is Gram-negative.
Fleming published his discovery in 1929, in the British "Journal of Experimental Pathology," but little attention was paid to his article. Fleming continued his investigations, but found that cultivating "penicillium" was quite difficult, and that after having grown the mould, it was even more difficult to isolate the antibiotic agent. Fleming's impression was that because of the problem of producing it in quantity, and because its action appeared to be rather slow, penicillin would not be important in treating infection. Fleming also became convinced that penicillin would not last long enough in the human body ("in vivo") to kill bacteria effectively. Many clinical tests were inconclusive, probably because it had been used as a surface antiseptic. In the 1930s, Fleming’s trials occasionally showed more promise, and he continued, until 1940, to try to interest a chemist skilled enough to further refine usable penicillin. Fleming finally abandoned penicillin, and not long after he did, Howard Florey and Ernst Boris Chain at the Radcliffe Infirmary in Oxford took up researching and mass-producing it, with funds from the U.S. and British governments. They started mass production after the bombing of Pearl Harbor. By D-Day in 1944, enough penicillin had been produced to treat all the wounded with the Allied forces.
Purification and stabilisation.
In Oxford, Ernst Boris Chain and Edward Abraham discovered how to isolate and concentrate penicillin. Abraham was the first to propose the correct structure of penicillin. Shortly after the team published its first results in 1940, Fleming telephoned Howard Florey, Chain's head of department, to say that he would be visiting within the next few days. When Chain heard that Fleming was coming, he remarked "Good God! I thought he was dead."
Norman Heatley suggested transferring the active ingredient of penicillin back into water by changing its acidity. This produced enough of the drug to begin testing on animals. There were many more people involved in the Oxford team, and at one point the entire Dunn School was involved in its production.
After the team had developed a method of purifying penicillin to an effective first stable form in 1940, several clinical trials ensued, and their amazing success inspired the team to develop methods for mass production and mass distribution in 1945.
Fleming was modest about his part in the development of penicillin, describing his fame as the "Fleming Myth" and he praised Florey and Chain for transforming the laboratory curiosity into a practical drug. Fleming was the first to discover the properties of the active substance, giving him the privilege of naming it: penicillin. He also kept, grew, and distributed the original mould for twelve years, and continued until 1940 to try to get help from any chemist who had enough skill to make penicillin. But Sir Henry Harris said in 1998: "Without Fleming, no Chain; without Chain, no Florey; without Florey, no Heatley; without Heatley, no penicillin."
Antibiotics.
Fleming's accidental discovery and isolation of penicillin in September 1928 marks the start of modern antibiotics. Before that, several scientists had published or pointed out that mould or "penicillium sp." were able to inhibit bacterial growth, and even to cure bacterial infections in animals. Ernest Duchesne in 1897 in his thesis "Contribution to the study of vital competition in micro-organisms: antagonism between moulds and microbes", or also Clodomiro Picado Twight whose work at Institut Pasteur in 1923 on the inhibiting action of fungi of the "Penicillin sp" genre in the growth of staphylococci drew little interest from the direction of the Institut at the time. Fleming was the first to push these studies further by isolating the penicillin, and by being motivated enough to promote his discovery at a larger scale. Fleming also discovered very early that bacteria developed antibiotic resistance whenever too little penicillin was used or when it was used for too short a period. Almroth Wright had predicted antibiotic resistance even before it was noticed during experiments. Fleming cautioned about the use of penicillin in his many speeches around the world. He cautioned not to use penicillin unless there was a properly diagnosed reason for it to be used, and that if it were used, never to use too little, or for too short a period, since these are the circumstances under which bacterial resistance to antibiotics develops.
Myths.
The popular story of Winston Churchill's father paying for Fleming's education after Fleming's father saved young Winston from death is false. According to the biography, "Penicillin Man: Alexander Fleming and the Antibiotic Revolution" by Kevin Brown, Alexander Fleming, in a letter to his friend and colleague Andre Gratia, described this as "A wondrous fable." Nor did he save Winston Churchill himself during World War II. Churchill was saved by Lord Moran, using sulphonamides, since he had no experience with penicillin, when Churchill fell ill in Carthage in Tunisia in 1943. The "Daily Telegraph" and the "Morning Post" on 21 December 1943 wrote that he had been saved by penicillin. He was saved by the new sulphonamide drug Sulphapyridine, known at the time under the research code M&B 693, discovered and produced by May & Baker Ltd, Dagenham, Essex – a subsidiary of the French group Rhône-Poulenc. In a subsequent radio broadcast, Churchill referred to the new drug as "This admirable M&B." It is highly probable that the correct information about the sulphonamide did not reach the newspapers because, since the original sulphonamide antibacterial, Prontosil, had been a discovery by the German laboratory Bayer, and as Britain was at war with Germany at the time, it was thought better to raise British morale by associating Churchill's cure with a British discovery, penicillin.
Personal life.
On 24 December 1915, Fleming married a trained nurse, Sarah Marion McElroy of Killala, County Mayo, Ireland. Their only child, Robert Fleming, (b. 1924) became a general medical practitioner. After Sarah's death in 1949, Fleming married Dr. Amalia Koutsouri-Vourekas, a Greek colleague at St. Mary's, on 9 April 1953; she died in 1986.
Death.
On 11 March 1955, Fleming died at his home in London of a heart attack. He was buried in St Paul's Cathedral.
Honours, awards and achievements.
His discovery of penicillin had changed the world of modern medicine by introducing the age of useful antibiotics; penicillin has saved, and is still saving, millions of people around the world.
The laboratory at St Mary's Hospital where Fleming discovered penicillin is home to the Fleming Museum, a popular London attraction. His alma mater, St Mary's Hospital Medical School, merged with Imperial College London in 1988. The Sir Alexander Fleming Building on the South Kensington campus was opened in 1998 and is now one of the main preclinical teaching sites of the Imperial College School of Medicine.
His other alma mater, the Royal Polytechnic Institution (now the University of Westminster) has named one of its student halls of residence "Alexander Fleming House", which is near to Old Street.
It was a discovery that would change the course of history. The active ingredient in that mould, which Fleming named penicillin, turned out to be an infection-fighting agent of enormous potency. When it was finally recognized for what it was, the most efficacious life-saving drug in the world, penicillin would alter forever the treatment of bacterial infections. By the middle of the century, Fleming's discovery had spawned a huge pharmaceutical industry, churning out synthetic penicillins that would conquer some of mankind's most ancient scourges, including syphilis, gangrene and tuberculosis.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="1962" url="http://en.wikipedia.org/wiki?curid=1962" title="Apparent magnitude">
Apparent magnitude

The apparent magnitude (m) of a celestial body is a measure of its brightness as seen by an observer on Earth, adjusted to the value it would have in the absence of the atmosphere. The brighter the object appears, the lower the value of its magnitude. Generally the visible spectrum (vmag) is used as a basis for the apparent magnitude, but other regions of the spectrum, such as the near-infrared J-band, are also used. In the visible spectrum Sirius is the brightest star in the visible sky (excluding the Sun), whereas in the near-infrared J-band, Betelgeuse is the brightest.
History.
The scale used to indicate magnitude originates in the Hellenistic practice of dividing stars visible to the naked eye into six "magnitudes". The brightest stars in the night sky were said to be of first magnitude ("m" = 1), whereas the faintest were of sixth magnitude ("m" = 6), the limit of human visual perception (without the aid of a telescope). Each grade of magnitude was considered twice the brightness of the following grade (a logarithmic scale). although that ratio was subjective as no photodetectors existed. This rather crude scale for the brightness of stars was popularized by Ptolemy in his "Almagest", and is generally believed to originate with Hipparchus.
In 1856, Norman Robert Pogson formalized the system by defining a first magnitude star as a star that is 100 times as bright as a sixth-magnitude star, thereby establishing the logarithmic scale still in use today. This implies that a star of magnitude "m" is 2.512 times as bright as a star of magnitude "m+1". This figure, the fifth root of 100 became known as "Pogson's Ratio". The zero point of Pogson's scale was originally defined by assigning Polaris a magnitude of exactly 2. Astronomers later discovered that Polaris is slightly variable, so they switched to Vega as the standard reference star, assigning the brightness of Vega as the definition of zero magnitude at any specified wavelength.
Apart from small corrections, the brightness of Vega still serves as the definition of zero magnitude for visible and near infrared wavelengths, where its spectral energy distribution (SED) closely approximates that of a black body for a temperature of 11,000 K. However with the advent of infrared astronomy it was revealed that Vega's radiation includes an Infrared excess presumably due to a circumstellar disk consisting of dust at warm temperatures (but much cooler than the star's surface). At shorter (e.g. visible) wavelengths, there is negligible emission from dust at these temperatures. However in order to properly extend the magnitude scale further into the infrared, this peculiarity of Vega shouldn't affect the definition of the magnitude scale. Therefore the magnitude scale was extrapolated to "all" wavelengths on the basis of the black body radiation curve for an ideal stellar surface at 11,000 K uncontaminated by circumstellar radiation. On this basis the spectral irradiance (usually expressed in Janskys) for the zero magnitude point, as a function of wavelength can be computed (see ). Small deviations are specified between systems using measurement appartuses developed independently so that data obtained by different astronomers can be properly compared; of greater practical importance is the definition of magnitude not at a single wavelength but applying to the response of standard spectral filters used in photometry over various wavelength bands.
With the modern magnitude systems, brightness over a very wide range is specified according to the logarithmic definition detailed below, using this zero reference. In practice such apparent magnitudes do not exceed 30 (for detectable measurements). The brightness of Vega is exceeded by four stars in the night sky at visible wavelengths (and more at infrared wavelengths) as well as bright planets such as Venus, Mars, and Jupiter, and these must be described by "negative" magnitudes. For example, Sirius, the brightest star of the celestial sphere, has an apparent magnitude of −1.4 in the visible; negative magnitudes for other very bright astronomical objects can be found in the table below.
Calculations.
As the amount of light received actually depends on the thickness of the Earth's atmosphere in the line of sight to the object, the apparent magnitudes are adjusted to the value they would have in the absence of the atmosphere. The dimmer an object appears, the higher the numerical value given to its apparent magnitude. Note that brightness varies with distance; an extremely bright object may appear quite dim, if it is far away. Brightness varies inversely with the square of the distance. The absolute magnitude, "M", of a celestial body (outside the Solar System) is the apparent magnitude it would have if it were at 10 parsecs (~32.6 light years) and that of a planet (or other Solar System body) is the apparent magnitude it would have if it were 1 astronomical unit from both the Sun and Earth. The absolute magnitude of the Sun is 4.83 in the V band (yellow) and 5.48 in the B band (blue).
The apparent magnitude, "m", in the band, "x", can be defined as,
where formula_2 is the observed flux in the band x, and formula_3 and formula_4 are a reference magnitude, and reference flux in the same band x, such as that of Vega. An increase of 1 in the magnitude scale corresponds to a decrease in brightness by a factor of formula_5. Based on the properties of logarithms, a difference in magnitudes, formula_6, can be converted to a variation in brightness as formula_7.
Example: Sun and Moon.
"What is the ratio in brightness between the Sun and the full moon?"
The apparent magnitude of the Sun is −26.74 (brighter), and the mean apparent magnitude of the full moon is −12.74 (dimmer).
Difference in magnitude : 
Variation in Brightness : 
The Sun appears about 400,000 times brighter than the full moon.
Magnitude addition.
Sometimes, it might be useful to add magnitudes. For example, to determine the combined magnitude of a double star when the magnitudes of the individual components are known. This can be done by setting an equation using the brightness (in linear units) of each magnitude.
Solving for formula_11 yields
where formula_11 is the resulting magnitude after adding formula_14 and formula_15. Note that the negative of each magnitude is used because greater intensities equate to lower magnitudes.
Standard reference values.
It is important to note that the scale is logarithmic: the relative brightness of two objects is determined by the difference of their magnitudes. For example, a difference of 3.2 means that one object is about 19 times as bright as the other, because Pogson's Ratio raised to the power 3.2 is approximately 19.05.
A common misconception is that the logarithmic nature of the scale is because the human eye itself has a logarithmic response. In Pogson's time this was thought to be true (see Weber-Fechner law), but it is now believed that the response is a power law (see Stevens' power law).
Magnitude is complicated by the fact that light is not monochromatic. The sensitivity of a light detector varies according to the wavelength of the light, and the way it varies depends on the type of light detector. For this reason, it is necessary to specify how the magnitude is measured for the value to be meaningful. For this purpose the UBV system is widely used, in which the magnitude is measured in three different wavelength bands: U (centred at about 350 nm, in the near ultraviolet), B (about 435 nm, in the blue region) and V (about 555 nm, in the middle of the human visual range in daylight). The V band was chosen for spectral purposes and gives magnitudes closely corresponding to those seen by the light-adapted human eye, and when an apparent magnitude is given without any further qualification, it is usually the V magnitude that is meant, more or less the same as visual magnitude.
Because cooler stars, such as red giants and red dwarfs, emit little energy in the blue and UV regions of the spectrum their power is often under-represented by the UBV scale. Indeed, some L and T class stars have an estimated magnitude of well over 100, because they emit extremely little visible light, but are strongest in infrared.
Measures of magnitude need cautious treatment and it is extremely important to measure like with like. On early 20th century and older orthochromatic (blue-sensitive) photographic film, the relative brightnesses of the blue supergiant Rigel and the red supergiant Betelgeuse irregular variable star (at maximum) are reversed compared to what human eyes perceive, because this archaic film is more sensitive to blue light than it is to red light. Magnitudes obtained from this method are known as photographic magnitudes, and are now considered obsolete.
For objects within the Milky Way with a given absolute magnitude, 5 is added to the apparent magnitude for every tenfold increase in the distance to the object. This relationship does not apply for objects at very great distances (far beyond the Milky Way), because a correction for general relativity must then be taken into account due to the non-Euclidean nature of space. 
For planets and other Solar System bodies the apparent magnitude is derived from its phase curve and the distances to the Sun and observer.
Table of notable celestial objects.
Some of the above magnitudes are only approximate. Telescope sensitivity also depends on observing time, optical bandpass, and interfering light from scattering and airglow.

</doc>
<doc id="1970" url="http://en.wikipedia.org/wiki?curid=1970" title="Apollo 16">
Apollo 16

Apollo 16 was the tenth manned mission in the United States Apollo space program, the fifth and penultimate to land on the Moon and the first to land in the lunar highlands. The second of the so-called "J missions," it was crewed by Commander John Young, Lunar Module Pilot Charles Duke and Command Module Pilot Ken Mattingly. Launched from the Kennedy Space Center in Florida at 12:54 PM EST on April 16, 1972, the mission lasted 11 days, 1 hour, and 51 minutes, and concluded at 2:45 PM EST on April 27.
John Young and Charles Duke spent 71 hours—just under three days—on the lunar surface, during which they conducted three extra-vehicular activities or moonwalks, totaling 20 hours and 14 minutes. The pair drove the Lunar Roving Vehicle (LRV), the second produced and used on the Moon, 26.7 km. On the surface, Young and Duke collected 95.8 kg of lunar samples for return to Earth, while Command Module Pilot Ken Mattingly orbited in the Command/Service Module (CSM) above to perform observations. Mattingly spent 126 hours and 64 revolutions in lunar orbit. After Young and Duke rejoined Mattingly in lunar orbit, the crew released a subsatellite from the Service Module (SM). During the return trip to Earth, Mattingly performed a one-hour spacewalk to retrieve several film cassettes from the exterior of the Service Module.
Apollo 16's landing spot in the highlands was chosen to allow the astronauts to gather geologically older lunar material than the samples obtained in the first four landings, which were in or near lunar maria. Samples from the Descartes Formation and the Cayley Formation disproved a hypothesis that the formations were volcanic in origin.
Crew.
Mattingly had originally been assigned to the prime crew of Apollo 13, but was exposed to the measles through Duke, at that time on the back-up crew for Apollo 13, who had caught it from one of his children. He never contracted the illness, but was nevertheless removed from the crew and replaced by his backup, Jack Swigert, three days before the launch. Young, a captain in the United States Navy, had flown on three spaceflights prior to Apollo 16: Gemini 3, Gemini 10 and Apollo 10, which orbited the Moon. One of 19 astronauts selected by NASA in April 1966, Duke had never flown in space before Apollo 16. He served on the support crew of Apollo 10 and was a Capsule Communicator (CAPCOM) for Apollo 11.
Backup crew.
Although not officially announced, the original backup crew consisted of Fred W. Haise (CDR), William R. Pogue (CMP) and Gerald P. Carr (LMP), who were targeted for the prime crew assignment on Apollo 19. However, after the cancellations of Apollos 18 and 19 were finalized in September 1970 this crew would not rotate to a lunar mission as planned. Subsequently, Roosa and Mitchell were recycled to serve as members of the backup crew after returning from Apollo 14, while Pogue and Carr were reassigned to the Skylab program where they flew on Skylab 4.
Mission insignia.
The insignia of Apollo 16 is dominated by a rendering of an American eagle and a red, white and blue shield, representing the people of the United States, over a gray background representing the lunar surface. Overlaying the shield is a gold NASA vector, orbiting the Moon. On its gold-outlined blue border, there are 16 stars, representing the mission number, and the names of the crew members: Young, Mattingly, Duke. The insignia was designed from ideas originally submitted by the crew of the mission.
Planning and training.
Landing site selection.
Apollo 16 was the second of the Apollo type J missions, featuring the use of the Lunar Roving Vehicle, increased scientific capability, and lunar surface stays of three days. As Apollo 16 was the penultimate mission in the Apollo program and there was no new hardware or procedures to test on the lunar surface, the last two missions (the other being Apollo 17) presented opportunities for astronauts to clear up some uncertainties in understanding the Moon's properties. Although previous Apollo expeditions, including Apollo 14 and Apollo 15, obtained samples of pre-mare lunar material, before lava began to upwell from the Moon's interior and flood the low areas and basins, none had actually visited the lunar highlands.
Apollo 14 had visited and sampled a ridge of material that had been ejected by the impact that created the Mare Imbrium impact basin. Likewise, Apollo 15 had also sampled material in the region of Imbrium, visiting the basin's edge. There remained the possibility, because the Apollo 14 and Apollo 15 landing sites were closely associated with the Imbrium basin, that different geologic processes were prevalent in areas of the lunar highlands far from Mare Imbrium. Several members of the scientific community remarked that the central lunar highlands resembled regions on Earth that were created by volcanic processes and hypothesized the same might be true on the Moon. They had hoped that scientific output from the Apollo 16 mission would provide an answer.
Two locations on the Moon were given primary consideration for exploration by the Apollo 16 expedition: the Descartes Highlands region west of Mare Nectaris and the crater Alphonsus. At Descartes, the Cayley and Descartes formations were the primary areas of interest in that scientists suspected, based on telescopic and orbital imagery, that the terrain found there was formed by magma more viscous than that which formed the lunar maria. The Cayley Formation's age was approximated to be about the same as Mare Imbrium based on the local frequency of impact craters. The considerable distance between the Descartes site and previous Apollo landing sites would be beneficial for the network of geophysical instruments, portions of which were deployed on each Apollo expedition beginning with Apollo 12.
At the Alphonsus, three scientific objectives were determined to be of primary interest and paramount importance: the possibility of old, pre-Imbrium impact material from within the crater's wall, the composition of the crater's interior and the possibility of past volcanic activity on the floor of the crater at several smaller "dark halo" craters. Geologists feared, however, that samples obtained from the crater might have been contaminated by the Imbrium impact, thus preventing Apollo 16 from obtaining samples of pre-Imbrium material. There also remained the distinct possibility that this objective had already been satisfied by the Apollo 14 and Apollo 15 missions, as the Apollo 14 samples had not yet been completely analyzed and samples from Apollo 15 had not yet been obtained.
It was decided to target the Apollo 16 mission for the Descartes site. Following the decision, the Alphonsus site was considered the most likely candidate for Apollo 17, but was eventually rejected. With the assistance of orbital photography obtained on the Apollo 14 mission, the Descartes site was determined to be safe enough for a manned landing. The specific landing site was between two young impact craters, North Ray and South Ray craters – 1000 and in diameter, respectively – which provided "natural drill holes" which penetrated through the lunar regolith at the site, thus leaving exposed bedrock that could be sampled by the crew.
After selecting the landing site for Apollo 16, sampling the Descartes and Cayley formations, two geologic units of the lunar highlands, was determined by mission planners to be the primary sampling interest of the mission. It was these formations that the scientific community widely suspected were formed by lunar volcanism, but this hypothesis was proven incorrect by the composition of lunar samples from the mission.
Training.
In preparation for their mission, the Apollo 16 astronauts participated in an extensive training program that included several field geology trips to introduce the astronauts to concepts and techniques they would use on the lunar surface. During these trips, the astronauts visited and provided scientific descriptions of geologic features they were likely to encounter. In July 1971, the Apollo 16 astronauts visited Sudbury, Ontario, Canada for geology training exercises, the first time U.S. astronauts did so. Geologists chose the area because of a 60 mi wide crater created about 1.8 billion years ago by a large meteorite. The Sudbury Basin shows evidence of shatter cone geology familiarizing the Apollo crew with geologic evidence of a meteor impact. During the training exercises the astronauts did not wear space suits, but carried radio equipment to converse with each other and a scientist-astronaut, practicing procedures they would use on the lunar surface.
In addition to field geology training, the astronauts also trained to use the space suits, adapt to the reduced lunar gravity, collect samples, maneuver in the Lunar Roving Vehicle, and land and recover after the mission. They also received survival training and preparation for other technical aspects of the mission.
Mission highlights.
Launch and outbound trip.
The launch of Apollo 16 was delayed one month from March 17 to April 16. This was the first launch delay in the Apollo program due to a technical problem. During the delay, the space suits, a spacecraft separation mechanism and batteries in the Lunar Module (LM) were modified and tested. There were concerns that the explosive mechanism designed to separate the docking ring from the Command Module (CM) would not create enough pressure to completely sever the ring. This, along with a dexterity issue in Young's space suit and fluctuations in the capacity of the Lunar Module batteries, required investigation and trouble-shooting. In January 1972, three months before the planned April launch date, a fuel tank in the Command Module was accidentally damaged during a routine test. The rocket was returned to the Vertical Assembly Building (VAB) and the fuel tank replaced, and the rocket returned to the launch pad in February in time for the scheduled launch.
The official mission countdown began on Monday, April 10, 1972, at 8:30 AM, six days before the launch. At this point the Saturn V rocket's three stages were powered up and drinking water was pumped into the spacecraft. As the countdown began, the crew of Apollo 16 was participating in final training exercises in anticipation of a launch on April 16. The astronauts underwent their final preflight physical examination on April 11. On April 15, liquid hydrogen and liquid oxygen propellants were pumped into the spacecraft, while the astronauts rested in anticipation of their launch the next day.
The Apollo 16 mission launched from the Kennedy Space Center in Florida at 12:54 PM EST on April 16, 1972. The launch was nominal; the crew experienced vibration similar to that of previous crews. The first and second stages of the Saturn V rocket performed nominally; the spacecraft entered orbit around Earth just under 12 minutes after lift-off. After reaching orbit, the crew spent time adapting to the zero-gravity environment and preparing the spacecraft for Trans Lunar Injection (TLI), the burn of the third-stage rocket that would propel them to the Moon. In Earth orbit, the crew faced minor technical issues, including a potential problem with the environmental control system and the S-IVB third stage's attitude control system, but eventually resolved or compensated for them as they prepared to depart towards the Moon. After two orbits, the rocket's third stage reignited for just over five minutes, propelling the craft towards the Moon at about 22000 mph. Six minutes after the burn of the S-IVB, the Command/Service Module, containing the crew, separated from the rocket and traveled for 15 m before turning around and retrieving the Lunar Module from inside the expended rocket stage. The maneuver, known as transposition, went smoothly and the LM was extracted from the S-IVB. Following transposition and docking, the crew noticed the exterior surface of the Lunar Module was giving off particles from a spot where the LM's skin appeared torn or shredded; at one point, Duke estimated they were seeing about five to ten particles per second. The crew entered the Lunar Module through the docking tunnel connecting it with the Command Module to inspect its systems, at which time they did not spot any major issues. Once on course towards the Moon, the crew put the spacecraft into a rotisserie "barbecue" mode in which the craft rotated along its long axis three times per hour to ensure even heat distribution about the spacecraft from the Sun. After further preparing the craft for the voyage, the crew began the first sleep period of the mission just under 15 hours after launch.
By the time Mission Control issued the wake-up call to the crew for flight day two, the spacecraft was about 98000 nmi away from the Earth, traveling at about 5322 ft/s. As it was not due to arrive in lunar orbit until flight day four, flight days two and three were largely preparatory days, consisting of spacecraft maintenance and scientific research. On day two, the crew performed an electrophoresis experiment, also performed on Apollo 14, in which they attempted to prove the higher purity of particle migrations in the zero-gravity environment. The remainder of day two included a two-second mid-course correction burn performed by the Command/Service Module's Service Propulsion System engine to tweak the spacecraft's trajectory. Later in the day, the astronauts entered the Lunar Module for the second time in the mission to further inspect the landing craft's systems. The crew reported they had observed additional paint peeling from a portion of the LM's outer aluminum skin. Despite this, the crew discovered that the spacecraft's systems were performing nominally. Following the LM inspection, the crew reviewed checklists and procedures for the following days in anticipation of their arrival and the Lunar Orbit Insertion burn. Command Module Pilot Mattingly reported a "gimbal lock" warning light, indicating the craft was not reporting an attitude. Mattingly alleviated this by realigning the guidance system using the Sun and Moon. At the end of day two, Apollo 16 was about 140000 nmi away from Earth.
At the beginning of day three, the spacecraft was about 157000 nmi away from the Earth. The velocity of the craft steadily decreased, as it had not yet reached the lunar sphere of gravitational influence. The early part of day three was largely housekeeping, spacecraft maintenance and exchanging status reports with Mission Control in Houston. The crew performed the Apollo light flash experiment, or ALFMED, to investigate "light flashes" that were seen by the astronauts when the spacecraft was dark, regardless of whether or not their eyes were open, on Apollo lunar flights. This was thought to be caused by the penetration of the eye by cosmic ray particles. During the second half of the day, Young and Duke again entered the Lunar Module to power it up and check its systems, and perform housekeeping tasks in preparation for lunar landing. The systems were found to be functioning as expected. Following this, the crew donned their space suits and rehearsed procedures that would be used on landing day. Just before the end of flight day three at 59 hours, 19 minutes, 45 seconds after liftoff, while 178673 nmi from the Earth and 33821 nmi from the Moon, the spacecraft's velocity began increasing as it accelerated towards the Moon after entering the lunar sphere of influence.
After waking up on flight day four, the crew began preparations for the maneuver that would brake the spacecraft into orbit around the Moon, or lunar orbit insertion. At a distance of 11142 nmi from the Moon, the Scientific Instrument Module (SIM) bay cover was jettisoned. At just over 74 hours into the mission, the spacecraft passed behind the Moon, losing direct contact with Mission Control. While over the far side of the Moon, the Command/Service Module's Service Propulsion System engine burned for 6 minutes and 15 seconds, braking the spacecraft into an orbit around the Moon with a low point (pericynthion) of 58.3 and a high point (apocynthion) of 170.4 nautical miles (108.0 and 315.6 km, respectively). After entering lunar orbit, the crew began preparations for the Descent Orbit Insertion (DOI) maneuver to further modify the spacecraft's orbital trajectory. The maneuver was successful, decreasing the craft's pericynthion to 10.7 nmi. The remainder of flight day four was spent making observations and preparing for activation of the Lunar Module, undocking, and landing the next day.
Lunar surface.
The crew continued preparing for Lunar Module activation and undocking shortly after waking up to begin flight day five. The boom that extended the mass spectrometer out from the Command/Service Module's Scientific Instruments Bay was stuck in a semi-deployed position. It was decided that Young and Duke would visually inspect the boom after undocking from the CSM in the LM. They entered the LM for activation and checkout of the spacecraft's systems. Despite entering the LM 40 minutes ahead of schedule, they completed preparations only 10 minutes early due to numerous delays in the process. With the preparations finished, they undocked in the LM "Orion" from Mattingly in the Command/Service Module "Casper" 96 hours, 13 minutes, 13 seconds into the mission. For the rest of the two crafts' passes over the near side of the Moon, Mattingly prepared to shift "Casper" to a circular orbit while Young and Duke prepared "Orion" for the descent to the lunar surface. At this point, during tests of the CSM's steerable rocket engine in preparation for the burn to modify the craft's orbit, a malfunction occurred in the engine's backup system. According to mission rules, "Orion" would have then re-docked with "Casper", in case Mission Control decided to abort the landing and use the Lunar Module's engines for the return trip to Earth. After several hours of analysis, however, mission controllers determined that the malfunction could be worked around and Young and Duke could proceed with the landing. As a result of this, powered descent to the lunar surface began about six hours behind schedule. Because of the delay, Young and Duke began their descent to the surface at an altitude higher than that of any previous mission, at 20.1 km. At an altitude of about 4000 m, Young was able to view the landing site in its entirety. Throttle-down of the LM's landing engine occurred on time and the spacecraft tilted forward to its landing orientation at an altitude of 2200 m. The LM landed 270 m north and 60 m west of the planned landing site at 104 hours, 29 minutes, and 35 seconds into the mission, at 2:23:35 UTC on April 21.
After landing, Young and Duke began powering down some of the LM's systems to conserve battery power. Upon completing their initial adjustments, the pair configured "Orion" for their three-day stay on the lunar surface, removed their space suits and took initial geological observations of the immediate landing site. They then settled down for their first meal on the surface. After eating, they configured the cabin for their first sleep period on the Moon. The landing delay caused by the malfunction in the Command/Service Module's main engine necessitated significant modifications to the mission schedule. Apollo 16 would spend one less day in lunar orbit after surface exploration had been completed to afford the crew contingency time to compensate for any further problems and to conserve expendables. In order to improve Young's and Duke's sleep schedule, the third and final moonwalk of the mission was trimmed from seven hours to five.
The next morning, flight day five, Young and Duke ate breakfast and began preparations for the first extra-vehicular activity (EVA), or moonwalk. After the pair donned and pressurized their space suits and depressurized the Lunar Module cabin, Young climbed out onto the "porch" of the LM, a small platform above the ladder. Duke handed Young a jettison bag full of trash to dispose of on the surface. Young then lowered the equipment transfer bag (ETB), containing equipment for use during the EVA, to the surface. Young descended the ladder and, upon setting foot on the lunar surface, became the ninth human to walk on the Moon. Upon stepping onto the surface, Young expressed his sentiments about being there: "There you are: Mysterious and Unknown Descartes. Highland plains. Apollo 16 is gonna change your image. I'm sure glad they got ol' Brer Rabbit, here, back in the briar patch where he belongs." Duke soon descended the ladder and joined Young on the surface, becoming the tenth and youngest human to walk on the Moon, at age 36. After setting foot on the lunar surface, Duke expressed his excitement, commenting: "Fantastic! Oh, that first foot on the lunar surface is super, Tony!" The pair's first task of the moonwalk was to unload the Lunar Roving Vehicle, the Far Ultraviolet Camera/Spectrograph (UVC), and other equipment, from the Lunar Module. This was done without problems. On first driving the lunar rover, Young discovered that the rear steering was not working. He alerted Mission Control to the problem before setting up the television camera and planting the flag of the United States with Duke. The day's next task was to deploy the Apollo Lunar Surface Experiments Package (ALSEP); while they were parking the lunar rover, on which the TV camera was mounted, to observe the deployment, the rear steering began functioning without explanation. While deploying a heat-flow experiment that had burned up with the Lunar Module "Aquarius" on Apollo 13 and had been attempted without success on Apollo 15, a cable was inadvertently snapped after getting caught around Young's foot. After ALSEP deployment, they collected samples in the vicinity. About four hours after the beginning of EVA-1, they mounted the lunar rover and drove to the first geologic stop, Plum Crater, a 36 m crater on the rim of Flag Crater, a crater 290 m across. There, at a distance of 1.4 km from the LM, they sampled material from the vicinity of Flag Crater, which scientists believed penetrated through the upper regolith layer to the underlying Cayley Formation. It was there that Young retrieved, at the request of Mission Control, the largest rock returned by an Apollo mission, a breccia nicknamed Big Muley after mission geology principal investigator William R. Muehlberger. The next stop of the day was Buster Crater, about 1.6 km from the LM. There, Duke took pictures of Stone Mountain and South Ray Crater while Young deployed a magnetic field experiment. At that point, scientists began to reconsider their pre-mission hypothesis that Descartes had been the setting of ancient volcanic activity, as the two astronauts had yet to find any volcanic material. Following their stop at Buster, Young did a demonstration drive of the lunar rover while Duke filmed with a 16 mm movie camera. After completing more tasks at the ALSEP, they returned to the LM to close out the moonwalk. They reentered the LM 7 hours, 6 minutes, and 56 seconds after the start of the EVA. Once inside, they pressurized the LM cabin, went through a half-hour briefing with scientists in Mission Control, and configured the cabin for the sleep period.
Shortly after waking up on the morning of flight day six three and a half minutes early, they discussed with Mission Control in Houston the day's timeline of events. The second lunar excursion's primary objective was to visit Stone Mountain to climb up the slope of about 20 degrees to reach a cluster of five craters known as "Cinco Craters." After preparations for the day's moonwalk were completed, the astronauts climbed out of the Lunar Module. After departing the immediate landing site in the lunar rover, they arrived at the day's first destination, the Cinco Craters, 3.8 km from the LM. At 152 m above the valley floor, the pair were at the highest elevation above the LM of any Apollo mission. After marveling at the view (including South Ray) from the side of Stone Mountain, which Duke described as "spectacular," the astronauts gathered samples in the vicinity. After spending 54 minutes on the slope, they climbed aboard the lunar rover en route to the day's second stop, station five, a crater 20 m across. There, they hoped to find Descartes material that had not been contaminated by ejecta from South Ray Crater, a large crater south of the landing site. The samples they collected there, although their origin is still not certain, are, according to geologist Don Wilhelms, "a reasonable bet to be Descartes." The next stop, station six, was a 10 m blocky crater, where the astronauts believed they could sample the Cayley Formation as evidenced by the firmer soil found there. Bypassing station seven to save time, they arrived at station eight on the lower flank of Stone Mountain, where they sampled material on a ray from South Ray Crater for about an hour. There, they collected black and white breccias and smaller, crystalline rocks rich in plagioclase. At station nine, an area known as the "Vacant Lot," which was believed to be free of ejecta from South Ray, they spent about 40 minutes gathering samples. Twenty-five minutes after departing station nine, they arrived at the final stop of the day, halfway between the ALSEP site and the LM. There, they dug a double core and conducted several penetrometer tests along a line stretching 50 m east of the ALSEP. At the request of Young and Duke, the moonwalk was extended by ten minutes. After returning to the LM to wrap up the second lunar excursion, they climbed back inside the landing craft's cabin, sealing and pressurizing the interior after 7 hours, 23 minutes, and 26 seconds of EVA time, breaking a record that had been set on Apollo 15. After eating a meal and proceeding with a debriefing on the day's activities with Mission Control, they reconfigured the LM cabin and prepared for the sleep period.
Flight day seven was their third and final day on the lunar surface, returning to orbit to rejoin Mattingly in the Command/Service Module following the day's moonwalk. During the third and final lunar excursion, they were to explore North Ray Crater, the largest of any of the craters any Apollo expedition had visited. After exiting "Orion", the pair drove the lunar rover 0.8 km away from the LM before adjusting their heading to travel 1.4 km to North Ray Crater. The drive was smoother than that of the previous day, as the craters were shallower and boulders were less abundant north of the immediate landing site. Boulders gradually became larger and more abundant as they approached North Ray in the lunar rover. Upon arriving at the rim of North Ray crater, they were 4.4 km away from the LM. After their arrival, the duo took photographs of the 1 km wide and 230 m deep crater. They visited a large boulder, taller than a four-story building, which became known as 'House Rock'. Samples obtained from this boulder delivered the final blow to the pre-mission volcanic hypothesis, proving it incorrect. House Rock had numerous bullet hole-like marks where micrometeoroids from space had impacted the rock. About 1 hour and 22 minutes after arriving, they departed for station 13, a large boulder field about 0.5 km from North Ray. On the way, they set a lunar speed record, traveling at an estimated 17.1 km/h downhill. They arrived at a 3 m high boulder, which they called 'Shadow Rock'. Here, they sampled permanently shadowed soil. During this time, Mattingly was preparing the Command/Service Module in anticipation their return approximately six hours later. After three hours and six minutes, they returned to the LM, where they completed several experiments and offloaded the rover. A short distance from the LM, Duke placed a photograph of his family and a United States Air Force commemorative medallion on the surface. Young drove the rover to a point about 90 m east of the LM, known as the 'VIP site,' so its television camera, controlled remotely by Mission Control, could observe Apollo 16's liftoff from the Moon. They then reentered the LM after a 5 hour and 40 minute final excursion. After pressurizing the LM cabin, the crew began preparing to return to lunar orbit.
Return to Earth.
Eight minutes before departing the lunar surface, CAPCOM James Irwin notified Young and Duke from Mission Control that they were go for liftoff. Two minutes before launch, they activated the "Master Arm" switch and then the "Abort Stage" button, after which they awaited ignition of "Orion"’s ascent stage engine. When the ascent stage ignited, small explosive charges severed the ascent stage from the descent stage and cables connecting the two were severed by a guillotine-like mechanism. Six minutes after liftoff, at a speed of about 5000 km/h, Young and Duke reached lunar orbit. Young and Duke successfully rendezvoused and re-docked with Mattingly in the Command/Service Module. To minimize the transfer of lunar dust from the LM cabin into the CSM, Young and Duke cleaned the cabin before opening the hatch separating the two spacecraft. After opening the hatch and reuniting with Mattingly, the crew transferred the samples Young and Duke had collected on the surface into the CSM for transfer to Earth. After transfers were completed, the crew would sleep before jettisoning the empty Lunar Module ascent stage the next day, when it was to be crashed intentionally into the lunar surface.
The next day, after final checks were completed, the expended LM ascent stage was jettisoned. Because of a failure by the crew to activate a certain switch in the LM before sealing it off, it initially tumbled after separation and did not execute the rocket burn necessary for the craft's intentional de-orbit. The ascent stage eventually crashed into the lunar surface nearly a year after the mission. The crew's next task, after jettisoning the Lunar Module ascent stage, was to release a subsatellite into lunar orbit from the CSM's Scientific Instrument Bay. The burn to alter the CSM's orbit to that desired for the subsatellite had been cancelled; as a result, the subsatellite lasted half of its anticipated lifetime. Just under five hours later, on the CSM's 65th orbit around the Moon, its Service Propulsion System main engine was reignited to propel the craft on a trajectory that would return it to Earth. The SPS engine performed the burn flawlessly despite the malfunction that had delayed the lunar landing several days before.
At a distance of about 170000 nmi from Earth, Mattingly performed a "deep-space" extra-vehicular activity, or spacewalk, during which he retrieved several film cassettes from the CSM's SIM bay. While outside the spacecraft, Mattingly set up a biological experiment, the Microbial Ecology Evaluation Device (MEED). The MEED experiment was only performed on Apollo 16. The crew carried out various housekeeping and maintenance tasks aboard the spacecraft and ate a meal before concluding the day.
The penultimate day of the flight was largely spent performing experiments, aside from a twenty-minute press conference during the second half of the day. During the press conference, the astronauts answered questions pertaining to several technical and non-technical aspects of the mission prepared and listed by priority at the Manned Spacecraft Center in Houston by journalists covering the flight. In addition to numerous housekeeping tasks, the astronauts prepared the spacecraft for its atmospheric reentry the next day. At the end of the crew's final full day in space, the spacecraft was approximately 77000 nmi from Earth and closing at a rate of about 7000 ft/s.
When the wake-up call was issued to the crew for their final day in space by CAPCOM Tony England, it was about 45000 nmi out from Earth, traveling just over 9000 ft/s. Just over three hours before splashdown in the Pacific Ocean, the crew performed a final course correction burn, changing their velocity by 1.4 ft/s. Approximately ten minutes before reentry into Earth's atmosphere, the cone-shaped Command Module containing the three crewmembers separated from the Service Module, which would burn up during reentry. At 265 hours and 37 minutes into the mission, at a velocity of about 36000 ft/s, Apollo 16 began atmospheric reentry. At its maximum, the temperature of the heat shield was between 4000 and. After successful parachute deployment and less than 14 minutes after reentry began, the Command Module splashed down in the Pacific Ocean 350 km southeast of the island of Kiritimati (or "Christmas Island"), 290 hours, 37 minutes, 6 seconds after liftoff. The spacecraft and its crew was retrieved by the USS "Ticonderoga". They were safely aboard the "Ticonderoga" 37 minutes after splashdown.
Lunar subsatellite PFS-2.
The Apollo 16 subsatellite (PFS-2) was a small satellite released into lunar orbit from the Service Module. Its principal objective was to measure charged particles and magnetic fields all around the Moon as the Moon orbited Earth, similar to its sister spacecraft, PFS-1, released eight months earlier by Apollo 15. "The low orbits of both subsatellites were to be similar ellipses, ranging from 55 to above the lunar surface."
"Instead, something bizarre happened. The orbit of PFS-2 rapidly changed shape and distance from the Moon. In 2-1/2 weeks the satellite was swooping to within a hair-raising 6 mi of the lunar surface at closest approach. As the orbit kept changing, PFS-2 backed off again, until it seemed to be a safe 30 miles away. But not for long: inexorably, the subsatellite's orbit carried it back toward the Moon. And on May 29, 1972—only 35 days and 425 orbits after its release"—PFS-2 crashed into the Lunar surface.
In later years, through a study of many lunar orbiting satellites, scientists came to discover that most low lunar orbits (LLO) are unstable. PFS-2 had been placed, unknown to mission planners at the time, squarely into one of the most unstable of orbits, at 11 degrees orbital inclination, far from the four "frozen lunar orbits" discovered only later at 27º, 50º, 76º, and 86º inclination.
Spacecraft locations.
The aircraft carrier USS "Ticonderoga" delivered the Apollo 16 Command Module to the North Island Naval Air Station, near San Diego, California, on Friday, May 5, 1972. On Monday, May 8, 1972, ground service equipment being used to empty the residual toxic reaction control system fuel in the Command Module tanks exploded in a Naval Air Station hangar. Forty-six people were sent to the hospital for 24 to 48 hours observation, most suffering from inhalation of toxic fumes. Most seriously injured was a technician who suffered a fractured kneecap when the GSE cart overturned on him. A hole was blown in the hangar roof 250 feet above; about 40 windows in the hangar were shattered. The Command Module suffered a three-inch gash in one panel.
The Apollo 16 Command Module "Casper" is on display at the U.S. Space & Rocket Center in Huntsville, Alabama. The Lunar Module ascent stage separated 24 April 1972 but a loss of attitude control rendered it out of control. It orbited the Moon for about a year. Its impact site on the Moon is unknown.
Duke donated some flown items, including a lunar map, to Kennesaw State University in Kennesaw, Georgia. He left two items on the Moon, both of which he photographed. The most famous is a plastic-encased photo portrait of his family (NASA Photo AS16-117-18841). The reverse of the photo is signed by Duke's family and bears this message: "This is the family of Astronaut Duke from Planet Earth. Landed on the Moon, April 1972." The other item was a commemorative medal issued by the United States Air Force, which was celebrating its 25th anniversary in 1972. He took two medals, leaving one on the Moon and donating the other to the Wright-Patterson Air Force Base museum.
In 2006, shortly after Hurricane Ernesto affected Bath, North Carolina, eleven-year-old Kevin Schanze discovered a piece of metal debris on the ground near his beach home. Schanze and a friend discovered a "stamp" on the 36 in flat metal sheet, which upon further inspection turned out to be a faded copy of the Apollo 16 mission insignia. NASA later confirmed the object to be a piece of the first stage of the Saturn V rocket that launched Apollo 16 into space. In July 2011, after returning the piece of debris at NASA's request, 16-year-old Schanze was given an all-access tour of the Kennedy Space Center and VIP seating for the launch of STS-135, the final mission of the Space Shuttle program.
References.
 This article incorporates  from websites or documents of the .
Bibliography.
</dl>

</doc>
<doc id="2047" url="http://en.wikipedia.org/wiki?curid=2047" title="Alcoholics Anonymous">
Alcoholics Anonymous

Alcoholics Anonymous (AA) is an international mutual aid fellowship founded in 1935 (two years after the end of prohibition in the United States in December 1933) by Bill Wilson and Dr. Bob Smith in Akron, Ohio. AA states that its primary purpose is to help alcoholics "to stay sober and help other alcoholics achieve sobriety". With other early members Bill Wilson and Bob Smith developed AA's Twelve Step program of spiritual and character development. AA's initial Twelve Traditions were introduced in 1946 to help the fellowship be stable and unified while disengaged from "outside issues" and influences. The Traditions recommend that members and groups remain anonymous in public media, altruistically helping other alcoholics and avoiding affiliations with any other organization. The Traditions also recommend that those representing AA avoid dogma and coercive hierarchies. Subsequent fellowships such as Narcotics Anonymous have adopted and adapted the Twelve Steps and the Twelve Traditions to their respective primary purposes.
According to AA's most recent membership survey, from 2011, 27% of members are sober under a year, 24% are sober 1–5 years, 12% 5–10 years, and 36% of members have over ten years sober. The Cochrane Review of eight studies, published in 2006, measuring the effectiveness of AA found no significant difference between the results of AA and twelve-step participation compared to other treatments. To determine the effectiveness of AA, the authors suggested more studies comparing treatment outcomes with control groups were necessary
The first female member, Florence Rankin, joined AA in March 1937, and the first non-Protestant member, a Roman Catholic, joined in 1939. AA membership has since spread "across diverse cultures holding different beliefs and values", including geopolitical areas resistant to grassroots movements. In the Fourth Edition of Alcoholics Anonymous (November 2001), it states "Since the third edition was published in 1976, worldwide membership of AA has just about doubled, to an estimated two million or more..."
AA's name is derived from its first book, informally called "The Big Book", originally titled "Alcoholics Anonymous: The Story of How More Than One Hundred Men Have Recovered From Alcoholism".
Oxford Group origins.
AA sprang from The Oxford Group, a non-denominational movement modeled after first-century Christianity. Some members found the Group to help in maintaining sobriety. One such "Grouper", as they were called, was Ebby Thacher, Wilson's former drinking buddy and his acknowledged sponsor. Following the evangelical bent of the Group, Thacher told Wilson that he had "got religion" and was sober, and that Wilson could do the same if he set aside objections to religion and instead formed a personal idea of God, "another power" or "higher power".
Wilson felt with Thacher a "kinship of common suffering" and—while drunk—attended his first Group gathering. Within days, Wilson admitted himself to the Charles B. Towns Hospital, but not before drinking four beers on the way—the last time Wilson drank alcohol. Under the care of Dr. William Duncan Silkworth (an early benefactor of AA), Wilson's detox included the deliriant belladonna. At the hospital in a state of despair, Wilson experienced a bright flash of light, which he felt to be God revealing himself.
Following his hospital discharge Wilson joined the Oxford Group and recruited other alcoholics to the Group. Wilson's early efforts to help others become sober were ineffective, prompting Dr. Silkworth to suggest that Wilson place less stress on religion and more on "the science" of treating alcoholism. Wilson's first success came during a business trip to Akron, Ohio, where he was introduced to Dr. Robert Smith, a surgeon and Oxford Group member who was unable to stay sober. After thirty days of working with Wilson, Smith drank his last drink on June 10, 1935, the date marked by AA for its anniversaries.
While Wilson and Smith credited their sobriety to working with alcoholics under the auspices of the Oxford Group, a Group associate pastor sermonized against Wilson and his alcoholic Groupers for forming a "secret, ashamed sub-group" engaged in "divergent works". By 1937, Wilson separated from the Oxford Group. AA Historian Ernest Kurtz described the split:
"...more and more, Bill discovered that new adherents could get sober by believing in each other and in the strength of this group. Men [no women were members yet] who had proven over and over again, by extremely painful experience, that they could not get sober on their own had somehow become more powerful when two or three of them worked on their common problem. This, then—whatever it was that occurred among them—was what they could accept as a power greater than themselves. They did not need the Oxford Group."
In 1955, Wilson acknowledged AA's debt, saying "The Oxford Groupers had clearly shown us what to do. And just as importantly, we learned from them "what not to do."" Among the Oxford Group practices that AA retained were informal gatherings, a "changed-life" developed through "stages", and working with others for no material gain. AA's analogs for these are meetings, "the steps", and sponsorship. AA's tradition of anonymity was a reaction to the publicity-seeking practices of the Oxford Group, as well as AA's wish to not promote, Wilson said, "erratic public characters who through broken anonymity might get drunk and destroy confidence in us."
The Big Book, the Twelve Steps and the Twelve Traditions.
To share their method, Wilson and other members wrote the initially-titled book, "Alcoholics Anonymous: The Story of How More Than One Hundred Men Have Recovered from Alcoholism", from which AA drew its name. Informally known as "The Big Book" (with its first 164 pages virtually unchanged since the 1939 edition), it suggests a twelve-step program in which members admit that they are powerless over alcohol and need help from a "higher power". They seek guidance and strength through prayer and meditation from God or a Higher Power of their own understanding; take a moral inventory with care to include resentments; list and become ready to remove character defects; list and make amends to those harmed; continue to take a moral inventory, pray, meditate, and try to help other alcoholics recover. The second half of the book, "Personal Stories" (subject to additions, removal and retitling in subsequent editions), is made of AA members' redemptive autobiographical sketches.
In 1941, interviews on American radio and favorable articles in US magazines, including a piece by Jack Alexander in "The Saturday Evening Post", led to increased book sales and membership. By 1946, as the growing fellowship quarreled over structure, purpose, and authority, as well as finances and publicity, Wilson began to form and promote what became known as AA's "Twelve Traditions," which are guidelines for an altruistic, unaffiliated, non-coercive, and non-hierarchical structure that limited AA's purpose to only helping alcoholics on a non-professional level while shunning publicity. Eventually he gained formal adoption and inclusion of the Twelve Traditions in all future editions of the Big Book. At the 1955 conference in St. Louis, Missouri, Wilson relinquished stewardship of AA to the General Service Conference, as AA grew to millions of members internationally.
Organization and finances.
AA says it is "not organized in the formal or political sense", and Bill Wilson called it a "benign anarchy". In Ireland, Shane Butler said that AA “looks like it couldn’t survive as there’s no leadership or top-level telling local cumanns what to do, but it has worked and proved itself extremely robust.” Butler explained that "AA’s 'inverted pyramid' style of governance has helped it to avoid many of the pitfalls that political and religious institutions have encountered since it was established here in 1946."
In 2006, AA counted 1,867,212 members and 106,202 AA groups worldwide. The Twelve Traditions informally guide how individual AA groups function, and the Twelve Concepts for World Service guide how the organization is structured globally.
A member who accepts a service position or an organizing role is a "trusted servant" with terms rotating and limited, typically lasting three months to two years and determined by group vote and the nature of the position. Each group is a self-governing entity with AA World Services acting only in an advisory capacity. AA is served entirely by alcoholics, except for seven "nonalcoholic friends of the fellowship" of the 21-member AA Board of Trustees.
AA groups are self-supporting, relying on voluntary donations from members to cover expenses. The AA General Service Office (GSO) limits contributions to US$3,000 a year. Above the group level, AA may hire outside professionals for services that require specialized expertise or full-time responsibilities.
AA receives proceeds from books and literature that constitute more than 50% of the income for its General Service Office. Unlike individual groups, the GSO is not self-supporting and maintains a small salaried staff. It also maintains service centers, which coordinate activities such as printing literature, responding to public inquiries, and organizing conferences. They are funded by local members and are responsible to the AA groups they represent. Other International General Service Offices (Australia, Costa Rica, Russia, etc.) are independent of AA World Services in New York.
Program.
The scope of AA's program is much broader than just abstinence from drinking alcohol. Its goal is to effect enough change in the alcoholic's thinking "to bring about recovery from alcoholism" through a spiritual awakening. A spiritual awakening is meant to be achieved by taking the Twelve Steps, and sobriety is furthered by volunteering for AA and regular AA meeting attendance or contact with AA members. Members are encouraged to find an experienced fellow alcoholic, called a sponsor, to help them understand and follow the AA program. The sponsor should preferably have experience of all twelve of the steps, be the same gender as the sponsored person, and refrain from imposing personal views on the sponsored person. Following the helper therapy principle, sponsors in AA may benefit from their relationship with their charges, as "helping behaviors" correlate with increased abstinence and lower probabilities of binge drinking.
AA's program is an inheritor of Counter-Enlightenment philosophy. AA shares the view that acceptance of one's inherent limitations is critical to finding one's proper place among other humans and God. Such ideas are described as "Counter-Enlightenment" because they are contrary to the Enlightenment's ideal that humans have the capacity to make their lives and societies a heaven on earth using their own power and reason. 
After evaluating AA's literature and observing AA meetings for sixteen months, sociologists David R. Rudy and Arthur L. Greil found that for an AA member to remain sober a high level of commitment is necessary. This commitment is facilitated by a change in the member's worldview. To help members stay sober AA must, they argue, provide an all-encompassing worldview while creating and sustaining an atmosphere of transcendence in the organization. To be all-encompassing AA's ideology places an emphasis on tolerance rather than on a narrow religious worldview that could make the organization unpalatable to potential members and thereby limit its effectiveness. AA's emphasis on the spiritual nature of its program, however, is necessary to institutionalize a feeling of transcendence. A tension results from the risk that the necessity of transcendence, if taken too literally, would compromise AA's efforts to maintain a broad appeal. As this tension is an integral part of AA, Rudy and Greil argue that AA is best described as a "quasi-religious organization".
Meetings.
AA meetings are "quasi-ritualized therapeutic sessions run by and for, alcoholics". They are usually informal and often feature discussions. Local AA directories list a variety of weekly meetings. Those listed as "closed" are only for those with "a desire to stop drinking", while "open" meetings are available to anyone (nonalcoholics can attend as observers). At speaker meetings, one or two members tell their stories, while discussion meetings allocate the most time for general discussion. Some meetings are devoted to studying and discussing the AA literature. Except for men's and women's meetings, meetings targeting specific demographics (including newcomers, gay people, and young people), AA meetings do not exclude other alcoholics. While AA has pamphlets that suggest meeting formats, groups have the autonomy to hold and conduct meetings as they wish "except in matters affecting other groups or AA as a whole". Different cultures affect ritual aspects of meetings, but around the world "many particularities of the AA meeting format can be observed at almost any AA gathering".
Confidentiality.
US courts have not extended the status of privileged communication, such as that enjoyed by clergy and lawyers, to AA related communications between members.
Spirituality.
A study found an association between an increase in attendance to AA meetings with increased spirituality and a decrease in the frequency and intensity of alcohol use. The research also found that AA was effective at helping agnostics and atheists become sober. The authors concluded that though spirituality was an important mechanism of behavioral change for some alcoholics, it was not the only effective mechanism. Since the mid-1970s, a number of 'agnostic' or 'no-prayer' AA groups have begun across the U.S., Canada, and other parts of the world, which hold meetings that adhere to a tradition allowing alcoholics to freely express their doubts or disbelief that spirituality will help their recovery, and forgo use of closing prayers.
Disease concept of alcoholism.
More informally than not, AA's membership has helped popularize the disease concept of alcoholism, though AA officially has had no part in the development of such postulates which had appeared as early as the late eighteenth century. Though AA initially avoided the term "disease", in 1973 conference-approved literature categorically stated that "we had the disease of alcoholism." Regardless of official positions, from AA's inception most members have believed alcoholism to be a disease.
Though cautious regarding the medical nature of alcoholism, AA has let others voice opinions. The Big Book states that alcoholism "is an illness which only a spiritual experience will conquer." Ernest Kurtz says this is "The closest the book Alcoholics Anonymous comes to a definition of alcoholism." In his introduction to The Big Book, non-member Dr. William Silkworth said those unable to moderate their drinking have an allergy. Addressing the allergy concept, AA said "The doctor’s theory that we have an allergy to alcohol interests us. As laymen, our opinion as to its soundness may, of course, mean little." AA later acknowledged that "alcoholism is not a true allergy, the experts now inform us." Wilson explained in 1960 why AA had refrained from using the term "disease":
We AAs have never called alcoholism a disease because, technically speaking, it is not a disease entity. For example, there is no such thing as heart disease. Instead there are many separate heart ailments or combinations of them. It is something like that with alcoholism. Therefore, we did not wish to get in wrong with the medical profession by pronouncing alcoholism a disease entity. Hence, we have always called it an illness or a malady—a far safer term for us to use.
Canadian and United States demographics.
AA's New York General Service Office regularly surveys AA members in North America. Its 2011 survey of over 8,000 members in Canada and the United States concluded that, in North America, AA members who responded to the survey were 65% male and 35% female. Average member sobriety is slightly under 10 years with 36% sober more than ten years, 12% sober from five to ten years, 24% sober from one to five years, and 27% sober less than one year. Before coming to AA, 63% of members received some type of treatment or counseling, such as medical, psychological, or spiritual. After coming to AA, 62% received outside treatment or counseling. Of those members, 82% said that outside help played an important part in their recovery. The same survey showed that AA received 12% of its membership from court ordered attendance.
Effectiveness.
Research limitations.
AA tends to polarize observers into believers and non-believers, and discussion of AA often creates controversy rather than objective reflection. Moreover, a randomized study of AA is difficult: AA members are not randomly selected from the population of chronic alcoholics; they are instead self-selected or mandated by courts to attend AA meetings. There are two opposing types of self-selection bias: (1) drinkers may be motivated to stop drinking before they participate in AA; (2) AA may attract the more severe and difficult cases. Controlled experiments with AA versus non-AA subjects are also difficult because AA is so easily accessible. Twelve-step groups like AA are not conducive to probability sampling of members. Research on AA is therefore susceptible to sampling bias.
Studies.
Studies of AA's efficacy have produced inconsistent results. While some studies have suggested an association between AA attendance and increased abstinence or other positive outcomes, other studies have not. A Cochrane Review of eight studies, published between 1967 and 2005, measuring the effectiveness of AA, found "no experimental studies unequivocally demonstrated the effectiveness of AA" in treating alcoholism, based on a meta-analysis of the results of eight trials involving 3,417 individuals.
Retention.
Every third year since 1968, AA has issued a pamphlet summarizing its latest triennial survey of meeting attendants. Additional published comments and analysis for academics and professionals have supplemented the survey results from 1970 through 1990. The 1990 commentary evaluated data of triennial surveys from 1977 through 1989 and found that the distribution of those with one year or less indicated that one quarter (26%) of those who first attend an AA meeting are still attending after one year. Furthermore, nearly one third (31.5%) leave the program after one month, and by the end of the third month, almost half (47.4%) leave. Of those who stay for three months, half (50.0%) will attain one year. After the first year, the rate of attrition slows. Only those in the first year were recorded by month.
Two surveys that sampled the general population produced independent results on AA continuance rates. The 1990 National Longitudinal Alcohol Epidemiologic Survey (NLAES) found that Alcoholics Anonymous has a 31% continuance rate. The 2001-2002 National Epidemiological Survey on Alcoholism and Related Conditions (NESARC) indicates a slightly higher rate, at 35.2%.
Sobriety of members.
Internal AA surveys suggest that about 40% of the members sober for less than a year will remain another year. About 80% of those sober more than one year, but less than five years will remain sober and active in the fellowship another year. About 90% of the members sober five years or more will remain sober and active in the fellowship another year. Those who remained sober outside the fellowship could not be calculated using the survey results.
Health-care costs.
As a volunteer-supported program, AA is free of charge. This contrasts with treatments for alcoholism such as inpatient treatment, drug therapy, psychotherapy, and cognitive-based therapy. One study found that the institutional use of twelve-step-facilitation therapy to encourage participation in AA reduced healthcare expenditures by 45% when compared to another group that was not encouraged to participate in AA.
Relationship with institutions.
Hospitals.
Many AA meetings take place in treatment facilities. Carrying the message of AA into hospitals was how the co-founders of AA first remained sober. They discovered great value of working with alcoholics who are still suffering, and that even if the alcoholic they were working with did not stay sober, they did. Bill Wilson wrote, "Practical experience shows that nothing will so much insure immunity from drinking as intensive work with other alcoholics". Bill Wilson visited Towns Hospital in New York City in an attempt to help the alcoholics who were patients there in 1934. At St. Thomas Hospital in Akron, Ohio, Smith worked with still more alcoholics. In 1939, a New York mental institution, Rockland State Hospital, was one of the first institutions to allow AA hospital groups. Service to corrections and treatment facilities used to be combined until the General Service Conference, in 1977, voted to dissolve its Institutions Committee and form two separate committees, one for treatment facilities, and one for correctional facilities.
Prisons.
In the United States and Canada, AA meetings are held in hundreds of correctional facilities. The AA General Service Office has published a workbook with detailed recommendations for methods of approaching correctional-facility officials with the intent of developing an in-prison AA program. In addition, AA publishes a variety of pamphlets specifically for the incarcerated alcoholic. Additionally, the AA General Service Office provides a pamphlet with guidelines for members working with incarcerated alcoholics.
United States Court rulings.
United States courts have ruled that inmates, parolees, and probationers cannot be ordered to attend AA. Though AA itself was not deemed a religion, it was ruled that it contained "enough" religious components (variously described in "Griffin v. Coughlin" below as, inter alia, "religion", "religious activity", "religious exercise") to make coerced attendance at AA meetings a violation of the Establishment Clause of the First Amendment of the constitution. In 2007, the Ninth Circuit of the U.S. Court of Appeals stated that a parolee who was ordered to attend AA had standing to sue his parole office.
American treatment industry.
In 1949, the Hazelden treatment center was founded and staffed by AA members, and since then many alcoholic rehabilitation clinics have incorporated AA's precepts into their treatment programs. 31% of AA's membership results from treatment centers referrals.
United Kingdom treatment industry.
A cross-sectional survey of substance-misuse treatment providers in the West Midlands found fewer than 10% integrated twelve-step methods in their practice and only a third felt their consumers were suited for Alcoholics Anonymous or Narcotics Anonymous membership. Less than half were likely to recommend self-help groups to their clients. Providers with nursing qualifications were more likely to make such referrals than those without them. A statistically significant correlation was found between providers' self-reported level of spirituality and their likelihood of recommending AA or NA.
Criticism.
Moderation or abstinence.
Stanton Peele argued that some AA groups apply the disease model to all problem drinkers, whether or not they are "full-blown" alcoholics. Along with Nancy Shute, Peele has advocated that besides AA, other options should be available to problem drinkers who can manage their drinking with the right treatment. The Big Book, however, acknowledges "moderate drinkers" and "a certain type of hard drinker" are able to stop or moderate their drinking. The Big Book suggests no program for these drinkers, but instead seeks to help drinkers without "power of choice in drink."
Cultural identity.
One review of AA warned of detrimental iatrogenic effects of twelve-step philosophy and concluded that AA uses many methods that are also used by cults. A subsequent study concluded, however, that AA's program bore little resemblance to religious cults because the techniques used appeared beneficial. Another study found that the AA program's focus on admission of having a problem increases deviant stigma and strips members of their previous cultural identity, replacing it with the deviant identity. A survey of group members, however, found they had a bicultural identity and saw AA's program as a complement to their other national, ethnic, and religious cultures.
Literature.
Alcoholics Anonymous publishes several books, reports, pamphlets, and other media, including a periodical known as the "AA Grapevine". Two books are used primarily: "Alcoholics Anonymous" (the "Big Book") and "Twelve Steps and Twelve Traditions", the latter explaining AA's fundamental principles in depth.
AA in music.
Psychedelic folk musician Red Label Catharsis refers to AA in the song 

</doc>
<doc id="2085" url="http://en.wikipedia.org/wiki?curid=2085" title="Assyria">
Assyria

Assyria was a major Mesopotamian East Semitic kingdom, and empire, of the Ancient Near East, existing as an independent state for a period of approximately nineteen centuries from c. 2500 BC to 605 BC, spanning the Early Bronze Age through to the late Iron Age. For a further thirteen centuries, from the end of the 7th century BC to the mid-7th century AD, it survived as a geo-political entity, for the most part ruled by foreign powers, although a number of small Neo-Assyrian states such as Assur, Adiabene, Osroene and Hatra arose at different times between the 1st century BC and late 3rd century AD.
Centered on the Upper Tigris river, in northern Mesopotamia (modern northern Iraq, northeastern Syria and southeastern Turkey) the Assyrians came to rule powerful empires at several times.
As a substantial part of the greater Mesopotamian "cradle of civilization" which included Sumer, Akkad and much later Babylonia, Assyria was at the height of technological, scientific and cultural achievements for its time. At its peak, the Assyrian empire stretched from Cyprus in the Mediterranean Sea to Persia (Iran), and from what is now Armenia to the Arabian Peninsula and Egypt.
Assyria is named for its original capital, the ancient city of Aššur (a.k.a. Ashur) which dates to c. 2600 BC (located in what is now the Saladin Province of northern Iraq), originally one of a number of Akkadian city states in Mesopotamia. In the 25th and 24th centuries BC, Assyrian kings were pastoral leaders, and from the late 24th century BC became subject to Sargon of Akkad, who united all the Akkadian Semites and Sumerian-speaking peoples of Mesopotamia under the Akkadian Empire, which lasted from c. 2334 BC to 2154 BC. Following the fall of the Akkadian Empire c. 2154 BC, and the short lived succeeding Neo-Sumerian Empire which ruled southern Assyria but not the north, Assyria regained full independence.
The history of Assyria proper is roughly divided into three periods, known as Old Assyrian, Middle Assyrian and Neo-Assyrian. These terms are in wide use in Assyrology and roughly correspond to the early to Middle Bronze Age, Late Bronze Age and Early Iron Age, respectively. In the Old Assyrian period, Assyria established colonies in Asia Minor and the Levant and, under king Ilushuma, it asserted itself over southern Mesopotamia. From the mid 18th century BC, Assyria came into conflict with the newly created state of Babylonia, which eventually eclipsed the far older Sumero-Akkadian states in the south, such as Ur, Isin, Larsa and Kish.
Assyria experienced fluctuating fortunes in the Old Assyrian period. Assyria became a regionally powerful nation with the Old Assyrian Empire from the late 21st century to the mid 18th century BC. Following this, it found itself under short periods of Babylonian and Mitanni-Hurrian rule in the 18th and 15th centuries BC respectively, and another period of great power occurred with the rise of the Middle Assyrian Empire (from 1365 BC to 1056 BC), which included the reigns of great kings, such as Ashur-uballit I, Arik-den-ili, Tukulti-Ninurta I and Tiglath-Pileser I. During this period, Assyria overthrew the Mitanni-Empire and eclipsed the Hittite Empire, Egyptian Empire, Babylonia, Elam and Phrygia in the Near East.
Beginning with the campaigns of Adad-nirari II from 911 BC, it again became a great power over the next three centuries, overthrowing the Twenty-fifth dynasty of Egypt and conquering Egypt, Babylonia, Elam, Urartu, Armenia, Media, Persia, Mannea, Gutium, Phoenicia/Canaan, Aramea (Syria), Arabia, Israel, Judah, Edom, Moab, Ammon, Samarra, Cilicia, Cyprus, Chaldea, Nabatea, Commagene, Dilmun, Libya, the Hurrians, Sutu and Neo-Hittites, driving the Ethiopians, Kushites and Nubians from Egypt, subjugating the Cimmerians and Scythians and exacting tribute from Phrygia, Magan and Punt among others.
After its fall (between 612 BC and 605 BC), Assyria remained a province and geo-political entity under the Babylonian, Median, Achaemenid, Seleucid, Parthian, Roman and Sassanid empires until the Arab Islamic invasion and conquest of Mesopotamia in the mid-7th century AD, when it was finally dissolved, after which the remnants of the Assyrian people (by now almost exclusively Eastern Rite Assyrian Christians) gradually became an ethnic, linguistic, cultural and religious minority in the traditional Assyrian homelands, surviving there to this day. (see Assyrian continuity).
Names.
Assyria was also sometimes known as Subartu and "Azuhinum" prior to the rise of the city state of Ashur after which it was 𒀸𒋗𒁺 𐎹 Aššūrāyu, and after its fall, from 605 BC through to the late 7th century AD variously as Athura and also referenced as Atouria according to Strabo, "Syria" (Greek), "Assyria" (Latin) and Assuristan. After its dissolution in the mid 7th century AD it remained "The Ecclesiastical Province of Ator". The term "Assyria" can also refer to the geographic region or heartland where Assyria, its empires and the Assyrian people were (and still are) centered. The modern Assyrian Christian (AKA Chaldo-Assyrian) ethnic minority in northern Iraq, north east Syria, south east Turkey and north west Iran are the descendants of the ancient Assyrians (see Assyrian continuity).
Pre-history of Assyria.
In prehistoric times, the region that was to become known as Assyria (and Subartu) was home to a Neanderthal culture such as has been found at the Shanidar Cave. The earliest Neolithic sites in Assyria were the Jarmo culture c. 7100 BC and Tell Hassuna, the centre of the "Hassuna culture", c. 6000 BC.
During the 3rd millennium BC, a very intimate cultural symbiosis developed between the Sumerians and the Semitic Akkadians throughout Mesopotamia, which included widespread bilingualism. The influence of Sumerian (a language isolate, i.e. not related to any other language) on Akkadian (and vice versa) is evident in all areas, from lexical borrowing on a massive scale, to syntactic, morphological, and phonological convergence. This has prompted scholars to refer to Sumerian and Akkadian in the 3rd millennium BC as a "sprachbund".
Akkadian gradually replaced Sumerian as the spoken language of Mesopotamia somewhere after the turn of the 3rd and the 2nd millennium BC (the exact dating being a matter of debate), but Sumerian continued to be used as a sacred, ceremonial, literary and scientific language in Mesopotamia until the 1st century AD.
The cities of Assur (also spelled Ashur or Aššur) and Nineveh, together with a number of other towns and cities, existed since at least before the middle of the 3rd millennium BC (c. 2600 BC), although they appear to have been Sumerian-ruled administrative centres at this time, rather than independent states.
According to some Judaeo-Christian writers , the city of Ashur was founded by Ashur the son of Shem, who was deified by later generations as the city's patron god . However, it is not among the cities said to have been founded by him in Genesis 10:11–12, and the far older Assyrian annals make no mention of the much later Judeo-Christian figures of Shem and Ashur.
Assyrian tradition lists an early Assyrian king named Ushpia as having dedicated the first temple to the god Ashur in the city in the 21st century BC. It is highly likely that the city was named in honour of its patron Assyrian god with the same name.
Classical dating.
George Syncellus in his "Chronographia" quotes a fragment from Julius Africanus which dates the founding of Assyria to 2284 BC. The Roman historian Velleius Paterculus citing Aemilius Sura states that Assyria was founded 1995 years before Philip V was defeated in 197 BC (at the Battle of Cynoscephalae) by the Romans. The sum therefore 197 + 1995 = 2192 BC for the foundation of Assyria. Diodorus Siculus recorded another tradition from Ctesias, that dates Assyria 1,306 years before 883 BC (the starting date of the reign of Ashurnasirpal II) and so the sum 883 + 1306 = 2189 BC. The "Chronicle" of Eusebius provides yet another date for the founding of Assyria, with the accession of Ninus, dating to 2057 BC, but the Armenian translation of the "Chronicle" puts this figure back slightly to 2116 BC. Another classical dating tradition found in the "Excerpta Latina Barbari" dates the foundation of Assyria, under Belus, to 2206 BC.
Early Assyria, 2600–2335 BC.
The city of Ashur, together with a number of other Assyrian cities, seem to have been established by 2600 BC, however it is likely that they were initially Sumerian dominated administrative centres. In c. the late 26th century BC, Eannatum of Lagash, then the dominant Sumerian ruler in Mesopotamia, mentions "smiting Subartu" (Subartu being the Sumerian name for Assyria). Similarly, in c. the early 25th century BC, Lugal-Anne-Mundu the king of the Sumerian state of Adab lists Subartu as paying tribute to him.
Of the early history of the kingdom of Assyria, little is positively known. In the Assyrian King List, the earliest king recorded was Tudiya. In archaeological reports from Ebla, it appeared that Tudiya's activities were confirmed with the discovery of a tablet where he concluded a treaty for the operation of a "karum" (trading colony) in Eblaite territory, with "king" Ibrium of Ebla (who is now known to have been the vizier of Ebla for king Ishar-Damu). This entire reading is now questionable, as several scholars have more recently argued that the treaty in question may not have been with king Tudiya of Assyria, but rather with the unnamed king of an uncertain location called "Abarsal".
Tudiya was succeeded on the list by Adamu and then a further thirteen rulers (Yangi, Shuhlamu, Harharu, Mandaru, Imshu, Harshu, Didanu, Hanu, Zuabu, Nuabu, Abazu, Belu and Azarah). Nothing concrete is yet known about these names, although it has been noted that a much later Babylonian tablet listing the ancestral lineage of Hammurabi, the Amorite king of Babylon, seems to have copied the same names from Tudiya through Nuabu, though in a heavily corrupted form.
The earliest kings, such as Tudiya, who are recorded as "kings who lived in tents", were independent semi-nomadic pastoralist rulers. These kings at some point became fully urbanised and founded the "city state" of Ashur.
Assyria in the Akkadian Empire and Neo-Sumerian Empires.
During the Akkadian Empire (2334–2154 BC) the Assyrians, like all the Mesopotamian Semites (and also the Sumerians), became subject to the dynasty of the city state of Akkad, centered in central Mesopotamia. The Akkadian Empire founded by Sargon the Great, claimed to encompass the surrounding "four quarters". The region of Assyria, north of the seat of the empire in central Mesopotamia had also been known as Subartu by the Sumerians, and the name Azuhinum in Akkadian records also seems to refer to Assyria proper.
Assyrian rulers were subject to Sargon and his successors, and the city of Ashur became a regional administrative center of the Empire, implicated by the Nuzi tablets.
During this period, the Akkadian-speaking Semites of Mesopotamia came to rule an empire encompassing not only Mesopotamia itself but large swathes of Asia Minor, ancient Iran, Elam, the Arabian Peninsula, Canaan and Syria.
Assyria seems to have already been firmly involved in trade in Asia Minor by this time; the earliest known reference to Anatolian "karum"s in Hatti, was found on later cuneiform tablets describing the early period of the Akkadian Empire (c. 2350 BC). On those tablets, Assyrian traders in Burushanda implored the help of their ruler, Sargon the Great, and this appellation continued to exist throughout the Assyrian Empire for about 1,700 years. The name "Hatti" itself even appears in later accounts of his grandson, Naram-Sin, campaigning in Anatolia.
Assyrian and Akkadian traders spread the use of writing in the form of the Mesopotamian cuneiform script to Asia Minor and The Levant (modern Syria and Lebanon).
However, towards the end of the reign of Sargon the Great, the Assyrian faction rebelled against him; "the tribes of Assyria of the upper country—in their turn attacked, but they submitted to his arms, and Sargon settled their habitations, and he smote them grievously".
The Akkadian Empire was destroyed by economic decline and internal civil war, followed by attacks from barbarian Gutian people in 2154 BC.
The rulers of Assyria during the period between c. 2154 BC and 2112 BC once again became fully independent, as the Gutians are only known to have administered southern Mesopotamia. However, the king list is the only information from Assyria for this period.
Most of Assyria briefly became part of the Neo-Sumerian Empire (or 3rd dynasty of Ur) founded in c. 2112 BC. Sumerian domination extended as far as the city of Ashur, but appears not to have reached Nineveh and the far north of Assyria. One local ruler ("shakkanakku") named Zāriqum (who does not appear on any Assyrian king list) is listed as paying tribute to Amar-Sin of Ur. Ashur's rulers appear to have remained largely under Sumerian domination until the mid-21st century BC (c. 2050 BC); the king list names Assyrian rulers for this period and several are known from other references to have also borne the title of "shakkanakka" or vassal governors for the neo-Sumerians.
Old Assyrian Kingdom.
The first written inscriptions by 'urbanised' Assyrian kings appear in the mid-21st century BC, after they had shrugged off Sumerian domination. The land of Assyria as a whole then consisted of a number of city states and small Semitic kingdoms, some of which were initially independent of Assyria. The foundation of the first major temple in the city of Ashur was traditionally ascribed to king Ushpia who reigned c. 2050 BC, possibly a contemporary of Ishbi-Erra of Isin and Naplanum of Larsa. He was reputedly succeeded by kings named Apiashal, Sulili, Kikkiya and Akiya (died c. 2026 BC), of whom little is known, apart from much later mentions of Kikkiya conducting fortifications on the city walls, and building work on temples in Ashur.
The main rivals, neighbours or trading partners to early Assyrian kings during the 22nd, 21st and 20th centuries BC would have been the Hattians and Hurrians to the north in Asia Minor, the Gutians, Lullubi and Turukku to the east in the Zagros Mountains of northwest Ancient Iran, the Elamites to the southeast in what is now south central Iran, the Amorites to the west in what is today Syria, and their fellow Sumero-Akkadian city-states of southern Mesopotamia such as Isin, Kish, Ur, Eshnunna and Larsa.
Like many city-states in early Mesopotamian history, Ashur was, to a great extent, originally an oligarchy rather than a monarchy. Authority was considered to lie with "the City", and the polity had three main centres of power—an assembly of elders, a hereditary ruler, and an eponym. The ruler presided over the assembly and carried out its decisions. He was not referred to with the usual Akkadian term for "king", "šarrum"/"Sharru"; that was instead reserved for the city's patron deity Assur, of whom the ruler was the high priest. The ruler himself was only designated as "the steward of Assur" ("iššiak Assur"), where the term for steward is a borrowing from Sumerian "ensi(k)". The third centre of power was the eponym ("limmum"), who gave the year his name, similarly to the later archons and consuls of Classical Antiquity. He was annually elected by lot and was responsible for the economic administration of the city, which included the power to detain people and confiscate property. The institution of the eponym as well as the formula "iššiak Assur" lingered on as ceremonial vestiges of this early system throughout the history of the Assyrian monarchy.
Dynasty of Puzur-Ashur I, 2025–1809 BC, Old Assyrian Empire.
In approximately 2025 BC (long chronology), Puzur-Ashur I (perhaps a contemporary of Shu-ilishu of Larsa and Samium of Isin) is speculated to have overthrown Kikkiya and founded a new dynasty which was to survive for 216 years. His descendants left inscriptions mentioning him regarding the building of temples to gods such as Ashur, Adad and Ishtar in Assyria. The length of his reign is unknown.
Shalim-ahum (died c. 2009 BC) succeeded the throne at a currently unknown date. He left inscriptions in archaic Old Assyrian regarding the construction of a temple dedicated to the god Ashur, and the placement of beer vats within it.
Ilushuma (c. 2008–1975 BC) took the throne in c. 2008 BC, and is known from his inscription (extant in several copies) where he claims to have "washed the copper" and "established liberty" for the Akkadians in Sumerian city-states as far as the Persian Gulf. This has been taken by some scholars to imply that he made military campaigns into Southern Mesopotamia to relieve his fellow Mesopotamians from Amorite and Elamite invasions, however some recent scholars have taken the view that the inscription means he supplied these areas with copper from Hatti, and that the word used for "liberty" ("adduraru") is usually in the context of his exempting the southern Mesopotamian kings from tariffs.
"The freedom of the Akkadians and their children I established. I purified their copper. I established their freedom from the border of the marshes and Ur and Nippur, Awal, and Kish, Der of the goddess Ishtar, as far as the City of (Ashur)."
Assyria had long held extensive contact with Hattian, Hittite and Hurrian cities on the Anatolian plateau in Asia Minor. The Assyrians who had for centuries traded in the region, and possibly ruled small areas bordering Assyria, now established significant colonies in Cappadocia (e.g., at Kanesh (modern Kültepe) from 2008 BC to 1740 BC). These colonies, called "karum", from the Akkadian word for 'port', were attached to Hattian cities in Anatolia, but physically separate, and had special tax status. They must have arisen from a long tradition of trade between Assyria and the Anatolian cities, but no archaeological or written records show this. The trade consisted of metals (copper or tin and perhaps iron; the terminology is not entirely clear) being traded for textiles from Assyria.
Erishum I (c. 1974–1935 BC) vigorously expanded Assyrian colonies in Asia Minor during his long reign, the major ones appearing to be at Kanesh, Ḫattuša (Boğazköy) (the future capital of the Hittite Empire) and Amkuwa (Alisar Höyük), together with a further eighteen smaller colonies. He created some of the earliest examples of Written Law, conducted extensive building work in the form of fortifying the walls of major Assyrian cities and the erection of temples dedicated to Ashur and Ishtar. It is from his reign that the continuous "limmum" lists are known, however there are references to the eponym-books for his predecessors having been destroyed at some point.
Ikunum (c. 1934–1921 BC) built a major temple for the god Ningal. He further strengthened the fortifications of the city of Assur and maintained Assyria's colonies in Asia Minor.
Sargon I (c. 1920–1881 BC) succeeded him in c. 1920 BC, and had an unusually long reign of 39 years. It is likely he was named after his illustrious predecessor Sargon of Akkad. He is known to have refortified the defences of major Assyrian cities, and maintained Assyrian colonies in Asia Minor during his reign. Apart from this, little has yet been unearthed about him. At some point he appears to have withdrawn Assyrian aid to southern Mesopotamia. It was during his reign in Assyria that the initially minor city-state of Babylon was founded in 1894 BC by an Amorite "Malka" (prince) named Sumuabum.
Puzur-Ashur II (c. 1881–1873 BC) came to the throne as an already older man due to his fathers long reign. Little is known about his rule, but it appears to have been uneventful.
Naram-Suen (c. 1872–1818 BC) ascended to the throne in 1872 BC, and is likely named after his predecessor Naram-Sin of the Akkadian Empire. Assyria continued to be wealthy during his 54-year-long reign (one of the longest in the ancient Near East), and he defeated the future usurper king Shamshi-Adad I who attempted to take his throne.
Erishum II (c. 1818–1809 BC) was to be the last king of the dynasty of Puzur-Ashur I, founded c. 2025 BC. After only eight or nine years in power he was overthrown by Shamshi-Adad I, the Amorite usurper who had previously been defeated in an attempt to unseat Naram-Suen, and who claimed legitimacy by asserting descent from the mid 21st century BC Assyrian king, Ushpia.
Amorite Period in Assyria, 1809–1750 BC.
The Amorites were successfully repelled by the Assyrian kings of the 20th and 19th centuries BC.
However, in 1809 BC the native Mesopotamian king of Assyria Erishum II was deposed, and the throne of Assyria was usurped by Shamshi-Adad I (c. 1809 – 1776 BC) in the expansion of Semitic Amorite tribes from the Khabur River delta in the north eastern Levant.
Although regarded as an Amorite by later Assyrian tradition, Shamshi-Adad's descent is suggested to be from the same line as the native Mesopotamian ruler Ushpia in the Assyrian King List. He put his son Ishme-Dagan on the throne of a nearby Assyrian city, Ekallatum, and maintained Assyria's Anatolian colonies. Shamshi-Adad I then went on to conquer the kingdom of Mari (in modern Syria) on the Euphrates putting another of his sons, Yasmah-Adad on the throne there. Shamshi-Adad's Assyria now encompassed the whole of northern Mesopotamia and included territory in central Mesopotamia, Asia Minor and northern Syria. Shamshi-Adad I mentions conducting raids on the Canaanite coasts of the far off Mediterranean, where he erected stelae to commemorate his victories. He himself resided in a new capital city founded in the Khabur valley in northern Mesopotamia, called Shubat-Enlil.
Ishme-Dagan I (1774–1763 BC) inherited Assyria, but Yasmah-Adad was overthrown by a new king called Zimrilim in Mari. The new king of Mari allied himself with the Amorite king Hammurabi of Babylon, who had made the recently created, and originally minor state of Babylon into a major power. It was from the reign of Hammurabi onwards that southern Mesopotamia came to be known as Babylonia.
Assyria now faced the rising power of Babylon in the south. Ishme-Dagan responded by making an alliance with the enemies of Babylon, and the power struggle continued without resolution for decades. Ishme-Dagan, like his father was a great warrior, and in addition to repelling Babylonian attacks, campaigned successfully against the Turukku and Lullubi of the Zagros Mountains (in modern Iran) who had attacked the Assyrian city of Ekallatum, and against Dadusha, king of Eshnunna, and the state of Iamhad (modern Aleppo).
Assyria under Babylonian domination, 1750–1732 BC.
Hammurabi, after first conquering Mari, Larsa, and Eshnunna, eventually prevailed over Ishme-Dagan's successor Mut-Ashkur (1750–1740 BC), and subjected him to Babylon c. 1750 BC. With Hammurabi, the various "karum" colonies in Anatolia ceased trade activity—probably because the goods of Assyria were now being traded with the Babylonians. The Assyrian monarchy survived, however the three Amorite kings succeeding Ishme-Dagan, Mut-Ashkur (who was the son of Ishme-Dagan and married to a Hurrian queen), Rimush (1739–1733 BC) and Asinum (1732 BC), were vassals, dependent on the Babylonians during the reign of Hammurabi, and for a short time, of his successor Samsu-iluna.
Assyrian Adaside dynasty, 1732–1451 BC.
The short lived Babylonian Empire quickly began to unravel upon the death of Hammurabi, and Babylonia lost control over Assyria during the reign of Hammurabi's successor Samsu-iluna (1750–1712 BC). A period of civil war ensued after Asinum (a grandson of Shamshi-Adad I and the last Amorite ruler of Assyria) was deposed in approximately 1732 BC by a powerful native Assyrian vice regent named Puzur-Sin, who regarded Asinum as both a foreigner and a former lackey of Babylon.
A native king named Ashur-dugul seized the throne in 1732 BC, probably with the help of Puzur-Sin. However, he was unable to retain control for long, and was soon deposed by a rival claimant, Ashur-apla-idi. Internal instability ensued with four further kings (Nasir-Sin, Sin-namir, Ipqi-Ishtar and Adad-salulu) all reigning in quick succession over a period of approximately six years between 1732 and 1727 BC. Babylonia seems to have been too powerless to intervene or take advantage of this situation.
Finally, a king named Adasi (1726–1701 BC) came to the fore c. 1726 BC and managed to quell the civil unrest and stabilise the situation in Assyria. Adasi completely drove the Babylonians and Amorites from the Assyrian sphere of influence during his reign, and Babylonian power began to quickly wane in Mesopotamia as a whole, also losing the far south of Mesopotamia (an area roughly corresponding to ancient Sumer) to the native Akkadian-speaking Sealand Dynasty, although the Amorites would retain control over a much reduced and weak Babylonia itself until 1595 BC, when they were overthrown by the Kassites, a people from the Zagros Mountains who spoke a language isolate and were neither Semites nor Indo-Europeans.
Adasi was succeeded by Bel-bani (1700–1691 BC) who is credited in Assyrian annals with inflicting further defeats on the Babylonians and Amorites, and further strengthening and stabilising the kingdom.
Little is currently known of many of the kings that followed such as; Libaya (1690–1674 BC), Sharma-Adad I (1673–1662 BC), Iptar-Sin (1661–1650 BC), Bazaya (1649–1622 BC) (a contemporary of Peshgaldaramesh of the Sealand Dynasty), Lullaya (1621–1618 BC) (who usurperped the throne from Bazaya), Shu-Ninua (1615–1602 BC) and Sharma-Adad II (1601–1599 BC). However, Assyria seems to have been a relatively strong and stable nation, existing undisturbed by its neighbours such as the Hatti, Hittites, Hurrians, Amorites, Babylonians, Elamites or Mitanni during this period.
Assyria remained strong and secure; when Babylon was sacked and its Amorite rulers deposed by the Hittite Empire, and subsequently fell to the Kassites in 1595 BC, both powers were unable to make any inroads into Assyria, and there seems to have been no trouble between the first Kassite ruler of Babylon, Agum II, and Erishum III (1598–1586 BC) of Assyria, and a mutually beneficial treaty was signed between the two rulers.
Shamshi-Adad II (1585–1580 BC), Ishme-Dagan II (1579–1562 BC) and Shamshi-Adad III (1562–1548 BC) seem also to have had peaceful tenures, although few records have thus far been discovered about their reigns. Similarly, Ashur-nirari I (1547–1522 BC) seems not to have been troubled by the newly founded Mitanni Empire in Asia Minor, the Hittite empire, or Babylon during his 25-year reign. He is known to have been an active king, improving the infrastructure, dedicating temples and conducting various building projects throughout the kingdom.
Puzur-Ashur III (1521–1498 BC) proved to be a strong and energetic ruler. He undertook much rebuilding work in Assur, the city was refortified and the southern quarters incorporated into the main city defences. Temples to the moon god Sin (Nanna) and the sun god Shamash were erected during his reign. He signed a treaty with Burna-Buriash I the Kassite king of Babylon, defining the borders of the two nations in the late 16th century BC. He was succeeded by Enlil-nasir I (1497–1483 BC) who appears to have had a peaceful an uneventful reign, as does his successor Nur-ili (1482–1471 BC).
The son of Nur-ili, Ashur-shaduni (1470 BC) was deposed by his uncle Ashur-rabi I (1470–1451 BC) in his first year of rule. Little is known about his nineteen-year reign, but it appears to have been largely uneventful.
Assyria in decline, 1450–1393 BC.
The emergence of the Mitanni Empire in the 16th century BC did eventually lead to a short period of sporadic Mitanni-Hurrian domination in the latter half of the 15th century. The Indo-European-speaking Mitanni are thought to have conquered and formed the ruling class over the indigenous Hurrians of eastern Anatolia. The Hurrians spoke a language isolate, i.e. neither Semitic nor Indo-European.
Ashur-nadin-ahhe I (1450–1431 BC) was courted by the Egyptians, who were rivals of the Mitanni, and attempting to gain a foothold in the Near East. Amenhotep II sent the Assyrian king a tribute of gold to seal an alliance against the Hurri-Mitanni empire. It is likely that this alliance prompted Saushtatar, the Mitanni emperor, to invade Assyria, and sack the city of Ashur, after which Assyria became a sometime vassal state, with Ashur-nadin-ahhe I being forced to pay tribute to Saushtatar. He was deposed by his own brother Enlil-nasir II (1430–1425 BC) in 1430 BC, possibly with the aid of the Mitanni, who received tribute from the new king. Ashur-nirari II (1424–1418 BC) had an uneventful reign, and appears to have also paid tribute to the Mitanni Empire.
The Assyrian monarchy survived, and the Mitanni influence appears to have been sporadic. They appear not to have been always willing or indeed able to interfere in Assyrian internal and international affairs.
Ashur-bel-nisheshu (1417–1409 BC) seems to have been independent of Mitanni influence, as evidenced by his signing a mutually beneficial treaty with Karaindash, the Kassite king of Babylonia in the late 15th century. He also undertook extensive rebuilding work in Ashur itself, and Assyria appears to have redeveloped its former highly sophisticated financial and economic systems during his reign.
Ashur-rim-nisheshu (1408–1401 BC) also undertook building work, strengthening the city walls of the capital.
Ashur-nadin-ahhe II (1400–1393 BC) also received a tribute of gold and diplomatic overtures from Egypt, probably in an attempt to gain Assyrian military support against Egypt's Mitanni and Hittite rivals in the region. However, the Assyrian king appears not to have been in a strong enough position to challenge the Mitanni or Hittites.
Eriba-Adad I (1392–1366 BC), a son of Ashur-bel-nisheshu, ascended the throne in 1392 BC and finally broke the ties to the Mitanni Empire.
There are dozens of Mesopotamian cuneiform texts from this period, with precise observations of solar and lunar eclipses, that have been used as 'anchors' in the various attempts to define the chronology of Babylonia and Assyria for the early 2nd millennium BC (i.e., the "high", "middle", and "low" chronologies.)
Middle Assyrian Empire, 1392–1056 BC.
Scholars variously date the beginning of the "Middle Assyrian period" to either the fall of the Old Assyrian kingdom of Shamshi-Adad I, or to the ascension of Ashur-uballit I to the throne of Assyria.
Assyrian expansion and empire, 1392–1056 BC.
By the reign of Eriba-Adad I (1392–1366 BC) Mitanni influence over Assyria was on the wane. Eriba-Adad I became involved in a dynastic battle between Tushratta and his brother Artatama II and after this his son Shuttarna II, who called himself king of the Hurri while seeking support from the Assyrians. A pro-Assyria faction appeared at the royal Mitanni court. Eriba-Adad I had thus finally broken Mitanni influence over Assyria, and in turn had now made Assyria an influence over Mitanni affairs.
Ashur-uballit I (1365–1330 BC) succeeded the throne of Assyria in 1365 BC, and proved to be a fierce, ambitious and powerful ruler. Assyrian pressure from the southeast and Hittite pressure from the north-west, enabled Ashur-uballit I to break Mitanni power. He met and decisively defeated Shuttarna II, the Mitanni king in battle, making Assyria once more an imperial power at the expense of not only the Mitanni themselves, but also Kassite Babylonia, the Hurrians and the Hittites; and a time came when the Kassite king in Babylon was glad to marry Muballiṭat-Šērūa, the daughter of Ashur-uballit, whose letters to Akhenaten of Egypt form part of the Amarna letters.
This marriage led to disastrous results for Babylonia, as the Kassite faction at court murdered the half Assyrian Babylonian king and placed a pretender on the throne. Assur-uballit I promptly invaded Babylonia to avenge his son-in-law, entering Babylon, deposing the king and installing Kurigalzu II of the royal line king there.
Ashur-uballit I then attacked and defeated Mattiwaza, the Mitanni king, despite attempts by the Hittite king Suppiluliumas, now fearful of growing Assyrian power, to help the Mitanni. The lands of the Mitanni and Hurrians were duly appropriated by Assyria, making it a large and powerful empire.
Enlil-nirari (1329–1308 BC) succeeded Ashur-uballit I. He described himself as a "Great-King" ("Sharru rabû") in letters to the Hittite kings. He was immediately attacked by Kurigalzu II of Babylon who had been installed by his father, but succeeded in defeating him, repelling Babylonian attempts to invade Assyria, counterattacking and appropriating Babylonian territory in the process, thus further expanding Assyria.
The successor of Enlil-nirari, Arik-den-ili (c. 1307–1296 BC), consolidated Assyrian power, and successfully campaigned in the Zagros Mountains to the east, subjugating the Lullubi and Gutians. In Syria, he defeated Semitic tribes of the so-called Ahlamu group, who were possibly predecessors of the Arameans or an Aramean tribe.
He was followed by Adad-nirari I (1295–1275 BC) who made Kalhu (Biblical Calah/Nimrud) his capital, and continued expansion to the northwest, mainly at the expense of the Hittites and Hurrians, conquering Hittite territories such as Carchemish and beyond. He then moved into north eastern Asia Minor, conquering Shupria. Adad-nirari I made further gains to the south, annexing Babylonian territory and forcing the Kassite rulers of Babylon into accepting a new frontier agreement in Assyria's favour.
Adad-nirari's inscriptions are more detailed than any of his predecessors. He declares that the gods of Mesopotamia called him to war, a statement used by most subsequent Assyrian kings. He referred to himself again as "Sharru Rabi" (meaning "The Great King" in the Akkadian language) and conducted extensive building projects in Ashur and the provinces.
In 1274 BC Shalmaneser I (1274–1244 BC) ascended the throne. He proved to be a great warrior king. During his reign he conquered the Hurrian kingdom of Urartu that would have encompassed most of Eastern Anatolia and the Caucasus Mountains in the 9th century BC, and the fierce Gutians of the Zagros. He then attacked the Mitanni-Hurrians, defeating both King Shattuara and his Hittite and Aramaean allies, finally completely destroying the Hurri-Mitanni kingdom in the process.
During the campaign against the Hittites, Shattuara cut off the Assyrian army from their supply of food and water, but the Assyrians broke free in a desperate battle, counterattacked, and conquered and annexed what remained of the Mitanni kingdom. Shalmaneser I installed an Assyrian prince, Ilu-ippada as ruler of Mitanni, with Assyrian governors such as Meli-sah, installed to rule individual cities.
The Hittites, having failed to save Mitanni, allied with Babylon in an unsuccessful economic war against Assyria for many years. Assyria was now a large and powerful empire, and a major threat to Egyptian and Hittite interests in the region, and was perhaps the reason that these two powers, fearful of Assyrian might, made peace with one another. Like his father, Shalmaneser was a great builder and he further expanded the city of Kalhu at the juncture of the Tigris and Zab Rivers.
Shalmaneser's son and successor, Tukulti-Ninurta I (1244–1207 BC), won a major victory against the Hittites and their king Tudhaliya IV at the Battle of Nihriya and took thousands of prisoners. He then conquered Babylonia, taking Kashtiliash IV as a captive and ruled there himself as king for seven years, taking on the old title "King of Sumer and Akkad" first used by Sargon of Akkad. Tukulti-Ninurta I thus became the first Akkadian speaking native Mesopotamian to rule the state of Babylonia, its founders having been foreign Amorites, succeeded by equally foreign Kassites. Tukulti-Ninurta petitioned the god Shamash before beginning his counter offensive. Kashtiliash IV was captured, single-handed by Tukulti-Ninurta according to "his" account, who "trod with my feet upon his lordly neck as though it were a footstool" and deported him ignominiously in chains to Assyria. The victorious Assyrian demolished the walls of Babylon, massacred many of the inhabitants, pillaged and plundered his way across the city to the Esagila temple, where he made off with the statue of Marduk. He then proclaimed himself "king of Karduniash, king of Sumer and Akkad, king of Sippar and Babylon, king of Tilmun and Meluhha." Middle Assyrian texts recovered at ancient Dūr-Katlimmu, include a letter from Tukulti-Ninurta to his "sukkal rabi'u", or grand vizier, Ashur-iddin advising him of the approach of his general Shulman-mushabshu escorting the captive Kashtiliash, his wife, and his retinue which incorporated a large number of women, on his way to exile after his defeat. In the process he defeated the Elamites, who had themselves coveted Babylon. He also wrote an epic poem documenting his wars against Babylon and Elam. After a Babylonian revolt, he raided and plundered the temples in Babylon, regarded as an act of sacrilege. As relations with the priesthood in Ashur began deteriorating, Tukulti-Ninurta built a new capital city; Kar-Tukulti-Ninurta.
A number of historians, including Julian Jaynes, identify Tukulti-Ninurta I and his deeds as the historical origin for the fictional biblical character Nimrod in the Old Testament.
However, Tukulti-Ninurta's sons rebelled and besieged the ageing king in his capital. He was murdered and then succeeded by Ashur-nadin-apli (1206–1203 BC) who left the running of his empire to Assyrian regional governors such as Adad-bēl-gabbe. Another unstable period for Assyria followed, it was riven by periods of internal strife and the new king only made token and unsuccessful attempts to recapture Babylon, whose Kassite kings had taken advantage of the upheavals in Assyria and freed themselves from Assyrian rule. However, Assyria itself was not threatened by foreign powers during the reigns of Ashur-nirari III (1202–1197 BC), Enlil-kudurri-usur (1196–1193 BC) and Ninurta-apal-Ekur (1192–1180 BC), although Ninurta-apal-Ekur usurped the throne from Enlil-kudurri-usur.
Ashur-Dan I (1179–1133 BC) stabilised the internal unrest in Assyria during his unusually long reign, quelling instability. During the twilight years of the Kassite dynasty in Babylonia, he records that he seized northern Babylonia, including the cities of Zaban, Irriya and Ugar-sallu during the reigns of Marduk-apla-iddina I and Zababa-shuma-iddin, plundering them and "taking their vast booty to Assyria." However, the conquest of northern Babylonia brought Assyria into direct conflict with Elam which had taken the remainder of Babylonia. The powerful Elamites, under king Shutruk-Nahhunte, fresh from sacking Babylon, entered into a protracted war with Assyria, they briefly took the Assyrian city of Arrapkha, which Ashur-Dan I then retook, eventually defeating the Elamites and forcing a treaty upon them in the process.
Another very brief period of internal upheaval followed the death of Ashur-Dan I when his son and successor Ninurta-tukulti-Ashur (1133 BC) was deposed in his first year of rule by his own brother Mutakkil-Nusku and forced to flee to Babylonia. Mutakkil-Nusku himself died in the same year (1133 BC).
A third brother, Ashur-resh-ishi I (1133–1116 BC) took the throne. This was to lead to a renewed period of Assyrian expansion and empire. As the Hittite empire collapsed from the onslaught of the Indo-European Phrygians (called Mushki in Assyrian annals), Babylon and Assyria began to vie for Aramaean regions (in modern Syria), formerly under firm Hittite control. When their forces encountered one another in this region, the Assyrian king Ashur-resh-ishi I met and defeated Nebuchadnezzar I of Babylon on a number of occasions. Assyria then invaded and annexed Hittite-controlled lands in Asia Minor, Aram (Syria), and Gutians and Kassite regions in the Zagros, marking an upsurge in imperian expansion.
Tiglath-Pileser I (1115–1077 BC), vies with Shamshi-Adad I and Ashur-uballit I among historians as being regarded as the founder of the first Assyrian empire. The son of Ashur-resh-ishi I, he ascended to the throne upon his father's death, and became one of the greatest of Assyrian conquerors during his 38-year reign.
His first campaign in 1112 BC was against the Phrygians who had attempted to occupy certain Assyrian districts in the Upper Euphrates region of Asia Minor; after defeating and driving out the Phrygians he then overran the Luwian kingdoms of Commagene, Cilicia and Cappadocia in western Asia Minor, and drove the Neo-Hittites from the Assyrian province of Subartu, northeast of Malatia.
In a subsequent campaign, the Assyrian forces penetrated Urartu, into the mountains south of Lake Van and then turned westward to receive the submission of Malatia. In his fifth year, Tiglath-Pileser again attacked Commagene, Cilicia and Cappadocia, and placed a record of his victories engraved on copper plates in a fortress he built to secure his Anatolian conquests.
The Aramaeans of northern and central Syria were the next targets of the Assyrian king, who made his way as far as the sources of the Tigris. The control of the high road to the Mediterranean was secured by the possession of the Hittite town of Pitru at the junction between the Euphrates and Sajur; thence he proceeded to conquer the Canaanite/Phoenician city-states of Byblos, Tyre, Sidon, Simyra, Berytus (Beirut), Aradus and finally Arvad where he embarked onto a ship to sail the Mediterranean, on which he killed a "nahiru" or "sea-horse" (which A. Leo Oppenheim translates as a narwhal) in the sea. He was passionately fond of hunting and was also a great builder. The general view is that the restoration of the temple of the gods Ashur and Hadad at the Assyrian capital of Assur (Ashur) was one of his initiatives.
He also invaded and defeated Babylon twice, assuming the old title "King of Sumer and Akkad", forcing tribute from Babylon, although he did not actually depose the actual king in Babylonia, where the old "Kassite Dynasty" had now succumbed to an Elamite one.
He was succeeded by Asharid-apal-Ekur (1076–1074 BC) who reigned for just two years. His reign marked the elevation of the office of "ummânu" (royal scribe) in importance.
Ashur-bel-kala (1073–1056 BC) kept the vast empire together, campaigning successfully against Urartu and Phrygia to the north and the Arameans to the west. He maintained friendly relations with Marduk-shapik-zeri of Babylon, however upon the death of that king, he invaded Babylonia and deposed the new ruler Kadašman-Buriaš, appointing Adad-apla-iddina as his vassal in Babylon. He built some of the earliest examples of both Zoological Gardens and Botanical Gardens in Ashur, collecting all manner of animals and plants from his empire, and receiving a collection of exotic animals as tributes from Egypt.
He was also a great hunter, describing his exploits "at the city of Araziqu which is before the land of Hatti and at the foot of Mount Lebanon." These locations show that well into his reign Assyria still controlled a vast empire.
Late in his reign, the Middle Assyrian Empire erupted into civil war, when a rebellion was orchestrated by Tukulti-Mer, a pretender to the throne of Assyria. Ashur-bel-kala eventually crushed Tukulti-Mer and his allies, however the civil war in Assyria had allowed hordes of Arameans to take advantage of the situation, and press in on Assyrian controlled territory from the west. Ashur-bel-kala counterattacked them, and conquered as far as Carchemish and the source of the Khabur river, but by the end of his reign many of the areas of Syria and Phoenicia-Canaan to the west of these regions as far as the Mediterranean, previously under firm Assyrian control, were eventually lost to the Assyrian Empire.
Assyria during the Bronze Age Collapse, 1055–936 BC.
The Bronze Age Collapse from 1200 BC to 900 BC was a dark age for the entire Near East, North Africa, Asia Minor, Caucasus, Mediterranean and Balkan regions, with great upheavals and mass movements of people.
Assyria and its empire were not unduly affected by these tumultuous events for some 150 years, perhaps the only ancient power that was not. However, upon the death of Ashur-bel-kala in 1056 BC, Assyria went into a "comparative" decline for the next 100 or so years. The empire shrank significantly, and by 1020 BC Assyria appears to have controlled only areas close to Assyria itself, essential to keeping trade routes open in eastern Aramea, south eastern Asia Minor central Mesopotamia and north western Iran.
New West Semitic peoples such as the Arameans, Chaldeans and Suteans moved into areas to the west and south of Assyria, including overrunning much of Babylonia to the south, Indo-European speaking Iranic peoples such as the Medes, Persians and Parthians moved into the lands to the east of Assyria, displacing the native Gutians and pressuring Elam and Mannea (which were all ancient non Indo-European civilisations of Iran), and to the north the Phrygians overran the Hittites, a new Hurrian state named Urartu arose in the Caucasus, and Cimmerians, Colchians (Georgians) and Scythians around the Black Sea and Caucasus. Egypt was divided and in disarray, and Israelites were battling with other fellow Semitic Canaanite peoples such as the Amalekites, Moabites, Edomites and Ammonites and the non-Semitic Peleset/Philistines (who have been conjectured to be one of the so-called Sea Peoples) for the control of southern Canaan.
Despite the apparent weakness of Assyria in comparison to its former might, at heart it in fact remained a solid, well defended nation whose warriors were the best in the world. Assyria, with its stable monarchy, powerful army and secure borders was in a stronger position during this time than potential rivals such as Egypt, Babylonia, Elam, Phrygia, Urartu, Persia and Media Kings such as Ashur-bel-kala, Eriba-Adad II, Ashur-rabi II, Ashurnasirpal I, Tiglath-Pileser II and Ashur-Dan II successfully defended Assyria's borders and upheld stability during this tumultuous time.
Assyrian kings during this period appear to have adopted a policy of maintaining and defending a compact, secure nation and satellite colonies immediately surrounding it, and interspersed this with sporadic punitive raids and invasions of neighbouring territories when the need arose.
Eriba-Adad II ruled for only two years, and in that time continued to campaign against the Arameans and neo-Hittites before he was deposed by his elderly uncle Shamshi-Adad IV (1053–1050 BC) who appears to have had an uneventful reign. Ashurnasirpal I (1049–1031 BC) succeeded him, and during his reign he continued to campaign endlessly against the Arameans to the west. Assyria was also afflicted by famine during this period. Shalmaneser II (1030–1019 BC) appears to have lost territory in the Levant to the Arameans, who also appear to have also occupied Nairi in southeast Asia Minor, hitherto an Assyrian colony.
Ashur-nirari IV took the throne in 1018 BC, and captured the Babylonian city of Atlila from Simbar-Shipak and continued Assyrian campaigns against the Arameans. He was eventually deposed by his uncle Ashur-rabi II in 1013 BC.
During the reign of Ashur-rabi II (1013–972 BC) Aramaean tribes took the cities of Pitru and Mutkinu (which had been taken and colonized by Tiglath Pileser I.) This event showed how far Assyria could assert itself militarily when the need arose. The Assyrian king attacked the Arameans, forced his way to the far off Mediterranean and constructed a stele in the area of Mount Atalur.
Ashur-resh-ishi II (971–968 BC) in all likelihood a fairly elderly man due to the length of his father's reign, had a largely uneventful period of rule, concerning himself with defending Assyria's borders and conducting various rebuilding projects within Assyria.
Tiglath-Pileser II (967–936 BC) succeeded him, and reigned for 28 years. He maintained the policies of his recent predecessors, but appears to have had an uneventful reign.
Society in the Middle Assyrian period.
Assyria had difficulties with keeping the trade routes open. Unlike the situation in the Old Assyrian period, the Anatolian metal trade was effectively dominated by the Hittites and the Hurrians. These people now controlled the Mediterranean ports, while the Kassites controlled the river route south to the Persian Gulf.
The Middle Assyrian kingdom was well organized, and in the firm control of the king, who also functioned as the High Priest of Ashur, the state god. He had certain obligations to fulfill in the cult, and had to provide resources for the temples. The priesthood became a major power in Assyrian society. Conflicts with the priesthood are thought to have been behind the murder of king Tukulti-Ninurta I.
The main Assyrian cities of the middle period were Ashur, Kalhu (Nimrud) and Nineveh, all situated in the Tigris River valley. At the end of the Bronze Age, Nineveh was much smaller than Babylon, but still one of the world's major cities (population c. 33,000). By the end of the Neo-Assyrian period, it had grown to a population of some 120,000, and was possibly the largest city in the world at that time. All free male citizens were obliged to serve in the army for a time, a system which was called the "ilku"-service. A legal code was produced during the 14th and 13th centuries which, among other things, clearly shows that the social position of women in Assyria was lower than that of neighbouring societies. Men were permitted to divorce their wives with no compensation paid to the latter. If a woman committed adultery, she could be beaten or put to death. It's not certain if these laws were seriously enforced, but they appear to be a backlash against some older documents that granted things like equal compensation to both partners in divorce. The women of the king's harem and their servants were also subject to harsh punishments, such as beatings, mutilation, and death. Assyria, in general, had much harsher laws than most of the region. Executions were not uncommon, nor were whippings followed by forced labour. Some offenses allowed the accused a trial under torture/duress. One tablet that covers property rights has brutal penalties for violators. A creditor could force debtors to work for him, but not sell them.
The Middle Assyrian Period is marked by the long wars fought during this period that helped build Assyria into a warrior society. The king depended on both the citizen class and priests in his capital, and the landed nobility who supplied the horses needed by Assyria's military. Documents and letters illustrate the importance of the latter to Assyrian society. Assyria needed less artificial irrigation than Babylon, and horse-breeding was extensive. Portions of elaborate texts about the care and training of them have been found. Trade was carried out in all directions. The mountain country to the north and west of Assyria was a major source of metal ore, as well as lumber. Economic factors were a common "casus belli".
Assyrian architecture, like that of Babylonia, was influenced by Sumero-Akkadian styles (and to some degree Mitanni), but early on developed its own distinctive style. Palaces sported colourful wall decorations, and seal-cutting (an art learned from Mittani) developed apace. Schools for scribes taught both the Babylonian and Assyrian dialects of Akkadian, and Sumerian and Akkadian literary works were often copied with an Assyrian flavour. The Assyrian dialect of Akkadian was used in legal, official, religious, and practical texts such as medicine or instructions on manufacturing items. During the 13th to 10th centuries, picture tales appeared as a new art form: a continuous series of images carved on square stone steles. Somewhat reminiscent of a comic book, these show events such as warfare or hunting, placed in order from the upper left to the lower right corner of the stele with captions written underneath them. These and the excellent cut seals show that Assyrian art was beginning to surpass that of Babylon. Architecture saw the introduction of a new style of ziggurat, with two towers and colorful enameled tiles.
Neo-Assyrian Empire, 911–612 BC.
Ashur-Dan II (935–912 BC) oversaw a marked economic and organisational upturn in the fortunes of Assyria, laying the platform for it to once again forge an empire. He is recorded as having made successful punitive raids outside the borders of Assyria to clear Aramean and other tribal peoples from the regions surrounding Assyria in all directions. He concentrated on rebuilding Assyria within its natural borders, from Tur Abdin to Arrapha (Kirkuk), he built government offices in all provinces, and created a major economic boost by providing ploughs throughout the land, which yielded record grain production.
The Neo-Assyrian Empire is usually considered to have begun with the accession of Adad-nirari II, in 911 BC, lasting until the fall of Nineveh at the hands of the Babylonians, Chaldeans, Medes/Persians, Scythians and Cimmerians in 612 BC.
Expansion, 911–627 BC.
When the ancient Dark Ages (which for Assyria lasted from 1050 to 936 BC) finally lifted, the world had changed dramatically. Ancient kingdoms such as Assyria, Babylonia, Elam and Egypt still endured, the Hittites did also, in the form of smaller Neo-Hittite states. A number of new states had arisen during the tumultuous time between 1200 and 936 BC, such as; Persia, Media, Parthia, Mannea, Israel, Urartu, Phrygia, Lydia, the Aramean and Phoenician states of the Levant, Doric Greece, Putria (Libya), Colchia, Tabal, Nubia/Kush. In addition, other nations and peoples; such as Chaldea, Judah, Scythia, Cimmeria, Samaria, Ethiopia, Nabatea, Armenia and the Arabs were to emerge in the following centuries.
However, it was the ancient state of Assyria which would once more rise to prominence, and Assyria was to meet and defeat these new peoples, together with old foes, over the coming three centuries.
Beginning with the campaigns of Adad-nirari II (911-892 BC), Assyria once more became a great power, growing to be the greatest empire the world had yet seen. The new king firmly subjugated the areas that were previously only under nominal Assyrian vassalage, conquering and deporting troublesome Aramean, Neo-Hittite and Hurrian populations in the north to far-off places. Adad-nirari II then twice attacked and defeated Shamash-mudammiq of Babylonia, annexing a large area of land north of the Diyala River and the towns of Hīt and Zanqu in mid Mesopotamia. Later in his reign, he made further gains against King Nabu-shuma-ukin I of Babylonia. He then conquered Kadmuh and Nisibin from the Arameans, and secured the Khabur region.
His successor, Tukulti-Ninurta II (891–884 BC) consolidated Assyria's gains and expanded into the Zagros Mountains in modern Iran, subjugating the newly arrived Persians, Parthians and Medes as well as pushing into central Asia Minor.
Ashurnasirpal II (883–859 BC) was a fierce and ruthless ruler who advanced without opposition through Aram and Canaan (modern Syria, Lebannon, Jordan and Israel) and Asia Minor as far as the Mediterranean and conquered and exacted tribute from Aramea, Phrygia and Phoenicia among others. Ashurnasirpal II also repressed revolts among the Medes and Persians in the Zagros Mountains, and moved his capital to the city of Kalhu (Calah/Nimrud). The palaces, temples and other buildings raised by him bear witness to a considerable development of wealth, science, architecture and art. He also built a number of new heavily fortified towns, such as Imgur-Enlil (Balawat), Tushhan, Kar-Ashurnasirpal and Nibarti-Ashur. Ashurnasirpal II also had a keen interest in Botany and Zoology; collecting all manner of plants, seeds and animals to be displayed in Assyria.
Shalmaneser III (858–823 BC) had his authority challenged by a large alliance of a dozen nations, some of which were vassals, including; Babylonia, Egypt, Elam, Persia, Israel, Hamath, Phoenicia, the Arabs, Arameans, Suteans and neo Hittites among others, fighting them to a standstill at the Battle of Qarqar. The failure of this alliance prevented pharaoh Osorkon II from regaining an Egyptian foothold in the Near East.
Subsequent to this, Shalmaneser III attacked and reduced Babylonia to vassalage, including subjugating the Chaldean, Aramean and Sutean tribes settled within it. He then defeated Aramea, Israel, Moab, Edom, Urartu, Phoenicia, the Neo-Hittite states and the desert dwelling Arabs of the Arabian Peninsula, forcing all of these to pay tribute to Assyria.
It is in Assyrian accounts of the 850's BC, recorded during the reign of Shalmaneser III, that the Arabs and Chaldeans first enter the pages of written history.
His armies penetrated to The Caucasus, Lake Van and the Taurus Mountains; the Hittites around Carchemish were compelled to pay tribute, and the kingdoms of Hamath and Aram Damascus were subdued. In 831 BC, he received the submission of the Georgian kingdom of Tabal. He consolidated Assyrian control over the regions conquered by his predecessors and, by the end of his 27-year reign, Assyria was master of Mesopotamia, The Levant, western Iran, Israel, Jordan and much of Asia Minor. Due to old age, in the last 6 years of his reign he passed command of his armies to the "Turtanu" (General) Dayyan-Assur.
However, his successor, Shamshi-Adad V (822-811 BC) (also known as Shamshi-Ramman II), inherited an empire beset by civil war in Assyria itself. The first years of his reign saw a serious struggle for the succession of the aged Shalmaneser III. The revolt, which had broken out by 826 BC, was led by Shamshi-Adad's brother Assur-danin-pal. The rebellious brother, according to Shamshi-Adad's own inscriptions, succeeded in bringing to his side 27 important cities, including Nineveh and Babylon. The rebellion lasted until 820 BC, preventing Assyria expanding its empire further until it was quelled.
Later in his reign, Shamshi-Adad V successfully campaigned against both Babylonia and Elam, and forced a treaty in Assyria's favour on the Babylonian king Marduk-zakir-shumi I. In 814 BCE, he won the battle of Dur-Papsukkal against the new Babylonian king Murduk-balassu-iqbi, and went on to subjugate the immigrant tribes of Chaldeans, Arameans, and Suteans who had recently settled in parts of Babylonia.
He was succeeded by Adad-nirari III (810–782 BC), who was merely a boy. The Empire was thus ruled by his mother, the famed queen Semiramis (Shammuramat), until 806 BC. Semiramis held the empire together, and appears to have campaigned successfully in subjugating the Persians, Parthians and Medes during her regency, leading to the later Iranian and also Greek myths and legends surrounding her.
In 806 BC, Adad-nirari III took the reins of power from Semiramis. He invaded the Levant and subjugated the Arameans, Phoenicians, Philistines, Israelites, Neo-Hittites, Moabites and Edomites. He entered Damascus and forced tribute upon its Aramean king Ben-Hadad III. He next turned eastward to Iran, and subjugated the Persians, Medes and the pre Iranian Manneans, penetrating as far north east as the Caspian Sea. He then turned south, forcing Babylonia to pay tribute. His next targets were the migrant Aramean, Chaldean and Sutu tribes, who had settled in the far south eastern corner of Mesopotamia, whom he conquered and reduced to vassalage. Then the Arabs in the deserts of the Arabian Peninsula to the south of Mesopotamia were invaded, vanquished and forced to pay tribute also.
It is from this general period that the Cilician Indo-Anatolian term "Surai" (Syria) first appears in historical record in what is now called the Çineköy inscription, not in reference to the region of Aramea now encompassing modern Syria in The Levant, but specifically and only to Assyria itself.
Adad-nirari III died prematurely in 782 BC, which led to a temporary period of stagnation within the empire. Assyria continued its military dominance, however Shalmaneser IV (782 - 773 BC) himself seems to have wielded little personal authority, and a victory over Argishti I, king of Urartu at Til Barsip is accredited to an Assyrian General (Turtanu) named Shamshi-ilu, who does not even bother to mention his king. Shamshi-ilu also scored victories over the Arameans, Phrygians, Persians and Neo-Hittites, and again, takes personal credit at the expense of his king.
Ashur-dan III ascended the throne in 772 BC. He proved to be a largely ineffectual ruler who was beset by internal rebellions in the cities of Ashur, Arrapkha and Guzana; and his personal authority was checked by powerful generals, such as Shamshi-ilu. He failed to make any further gains in Babylonia, Canaan and Aram. His reign was also marred by Plague and an ominous Solar Eclipse and, as with his predecessor, military victories were credited to Shamshi-ilu.
Ashur-nirari V became king in 754 BC, the early part of his reign seems to have been one of permanent internal revolution, and he apprears to have barely left his palace in Nineveh. However later in his reign he led a number of successful campaigns in Asia Minor and the Levant. He was deposed by Tiglath-pileser III in 745 BC bringing a resurgence to Assyrian expansion.
Tiglath-Pileser III (745–727 BC), a usurper whose original name was "Pulu", initiated a renewed period of Assyrian expansion; Urartu, Persia, Media, Mannea, Babylonia, Arabia, Phoenicia, Israel, Judah, Samaria, Nabatea, Chaldea, Cyprus, Moab, Edom and the Neo-Hittites were subjugated, Tiglath-Pileser III was declared king in Babylon and the Assyrian empire was now stretched from the Caucasus Mountains to Arabia and from the Caspian Sea to Cyprus.
Tiglath-Pileser III had reorganised the Assyrian army into the first professional fighting force in history, he also incorporated conquered peoples into the imperial army to serve as light infantry, thus expanding the size of the army. He greatly improved the civil administration of his empire, reducing the influence of hitherto powerful nobles, regional governors and viceroys, and deporting troublesome peoples to other parts of his vast empire, setting the template for all future ancient empires
Tiglath-Pileser III also introduced Mesopotamian Eastern Aramaic as the "Lingua Franca" of Assyria and its vast empire, whose Akkadian infused descendant dialects still survive among the modern Assyrian Christian people to this day.
Shalmaneser V (726–723 BC) consolidated Assyrian power during his short reign, and repressed Egyptian attempts to gain a foothold in the near east, defeating and driving out Pharaoh Shoshenq V from the region. He is mentioned in Biblical sources as having conquered Israel the Samaritans, and being responsible for deporting the Ten Lost Tribes of Israel to Assyria.
Sargon II (722–705 BC) maintained the empire, driving the Cimmerians and Scythians from Ancient Iran, where they had invaded and attacked the Persians and Medes, who were vassals of Assyria. Deioces, king of the Medes and Persians was then forced to pay tribute after launching a failed rebellion against Assyria. When in 720 BCE a revolt occurred in Canaan against Sargon II, king Hanno sought the help of Pharaoh Osorkon IV of the 22nd Dynasty of Egypt. The Egyptian king sent a general named Raia as well as troops in order to support the neighboring ally. However, the coalition was defeated in battle at Raphia: Raia fled back to Egypt, Raphia and Gaza were looted and Hanno was burnt alive by the Assyrians.<br>
In 716 BCE Sargon II crossed the Sinai and amassed an army on Egypt's border. Osorkon IV personally met the Assyrian king at the "Brook of Egypt" (most likely el-Arish) and was forced pay tribute to Sargon II to avoid being invaded. Mannea, Cilicia Cappadocia and Commagene were conquered, Urartu was ravaged, and Babylonia, Chaldea, Aram, Phoenicia, Israel, Arabia, Cyprus and the famed Midas (king of Phrygia) were forced to pay tribute. His "stele" has been found as far west as Larnaca in Cyprus. Sargon II conquered Gurgum, Milid, the Georgian state of Tabal, and all of the Neo-Hittite kingdoms of the Taurus Mountains. Egypt, now under a new Nubian dynasty, once again attempted to gain ground in the region by supporting Israel's rebellion against the empire, however Sargon II once again crushed the uprising, and Piye was routed and driven back over the Sinai. Sargon II was killed in 705 BC while on a punitive raid against the Cimmerians, and was succeeded by Sennacherib.
Sennacherib (705-681 BC), a ruthless ruler, defeated the Greeks who were attempting to gain a foothold in Cilicia, and then defeated and drove the Nubian ruled Egyptians from the Near East where the new Nubian Pharaoh Taharqa had once again fomented revolt against Assyria among the Israelites, Judeans and Canaanites.
Sennacherib was forced to contend with a major revolt within his empire, which included a large alliance of subject peoples, including Babylonians, Persians, Medes, Chaldeans, Elamites, Parthians, Manneans and Arameans. The prime movers in this rebellion were Mushezib-Marduk of Babylonia, Achaemenes of Persia, Khumban-umena III of Elam, and Deioces of Media. The Battle of Halule was fought in 691 BC between Sennacherib and his enemies, in which this vast alliance failed to overthrow Sennacherib. The Assyrian king was then able to subjugate these nations individually, Babylon was sacked and largely destroyed by Sennacherib.
He sacked Israel, subjugated the Samaritans and laid siege to Judah, forcing tribute upon it. He installed his own son Ashur-nadin-shumi as king in Babylonia. He maintained Assyrian domination over the Medes, Manneans and Persians to the east, Asia Minor and the southern Caucasus to the north and north west, and the Levant, Phoenicia and Aram in the west. 
Sennacherib's palace and garden at Nineveh have been proposed by some scholars as the true location of the Hanging Gardens of Babylon. It was during the reign of Sennacherib that the previously minor city of Nineveh (extant since approximately 3000 BC) was transformed into the capital of Assyria, growing at its height to be the largest city in the world at the time, with a population of up to 150,000 people.
Sennacherib was murdered by his own sons (according to the Bible the sons were named Adrammelech, Abimelech and Sharezer) in a palace revolt, apparently in revenge for the destruction of Babylon, a city sacred to all Mesopotamians, including the Assyrians.
Esarhaddon (680–669 BC) expanded Assyria still further, campaigning deep into the Caucasus Mountains in the north, defeating king Rusas II and breaking Urartu completely in the process. Esarhaddon campaigned successfully subjugating the Scythian king Ishpakaia, and the Cimmerian king Teushpa in Asia Minor, and in Ancient Iran, the Manneans, Gutians, Persians and Phraortes the king of the Medes were subjugated.
To the west, the kings of Judah, Edom, Moab, Israel, Sidon, Ekron, Byblos, Arvad, Samarra, Ammon, Amalek, and the ten Greek kings of Cyprus, are listed as Assyrian subjects.
Tiring of Egyptian interference in the Assyrian Empire, Esarhaddon decided to conquer Egypt. In 671 BC crossed the Sinai Desert, and invaded and took Egypt with surprising ease and speed, driving its foreign Nubian/Kushite and Ethiopian rulers out and destroying the Kushite Empire in the process. Esarhaddon now declared himself "king of Egypt, Libya, and Kush", and returned with rich booty from the cities of the delta; he erected a victory stele at this time, and paraded the captive Nubian Prince Ushankhuru, the son of Taharqa in Nineveh. Esarhaddon stationed a small army in northern Egypt and describes how; "All Ethiopians (read Nubians/Kushites) I deported from Egypt, leaving not one left to do homage to me".[69] He installed native Egyptian princes throughout the land to rule on his behalf.[70] The conquest by Esarhaddon effectively marked the end of the short lived Kushite Empire.
He expanded the empire as far south as Arabia, Meluhha and Dilmun (modern Bahrain and Qatar).
Esarhaddon also completely rebuilt Babylon during his reign, bringing peace to Mesopotamia as a whole. The Babylonians, Egyptians, Elamites, Cimmerians, Scythians, Persians, Medes, Manneans, Arameans, Chaldeans, Israelites, Phoenicians and Urartians were vanquished and regarded as vassals and Assyria's empire was kept secure.
He imposed a so-called "Vassal Treaty" upon his Persian and Median subjects, forcing Teispes of Persia and Deioces of Media to submit both to himself, and in advance to his chosen successor, Ashurbanipal. Esarhaddon died whilst preparing to leave for Egypt to once more eject the Nubians, who were attempting to encroach on the southern part of the country. This task was successfully completed by his successor, Ashurbanipal.
Under Ashurbanipal (669–627 BC), Assyrian domination spanned from the Caucasus Mountains (modern Armenia, Georgia and Azerbaijan) in the north to Nubia, Egypt, Libya and Arabia in the south, and from the East Mediterranean, Cyprus and Antioch in the west to Persia and the Caspian Sea in the east.
He was an unusually educated man for his time, being able to read and write in Akkadian, Aramaic and Sumerian, and having a proficient understanding of Astronomy and Mathematics, as well as military, civil and political aptitude. He built the famed Library of Ashurbanipal which contained a multitude of ancient texts from all over Mesopotamia, and was the first library in history to classify works in order of genre.
Ashurbanipal began his rule by easily crushing the Nubian/Cushite king Taharqa, who had attempted to invade the southern part of Assyrian-controlled Egypt. Memphis was sacked, and Taharqa was chased back into Nubia (modern Sudan) by a pursuing Assyrian army, and was never again to pose a threat. Ashurbanipal then put down a series of rebellions by the native Egyptians themselves, installing Necho I as a puppet Pharaoh. However in 664 BC, the new Nubian-Kushite king Tantamani once more attempted to invade Egypt, however he was savagely crushed, Thebes was sacked and looted, and he fled to Nubia, bringing to an end, once and for all, Nubian designs on Egypt.
Phraortes, the king of the Medes and Persians, also rebelled against Assyria, and attempted to attack Assyria itself in 653 BC, however he met with defeat at the hands of Ashurbanipal, and was killed. The succeeding Median kings, Madius and then Cyaxares the Great, were both in turn subjugated by Ashurbanipal.
At around this time, Gyges king of Lydia in western Asia Minor, offered his submission to Ashurbanipal.
In 652 BC, just one year after his victory over Phraortes, his own brother Shamash-shum-ukin, the Assyrian king of Babylon who had spent seventeen years peacefully subject to his sibling, became infused with Babylonian nationalism, declaring that Babylon and not Nineveh should be the seat of empire. Shamash-shum-ukin raised a powerful coalition of peoples resentful of being subject to Assyria, including- Babylonians, Chaldeans, Persians, Arameans, Suteans, Arabs, Elamites, Scythians, Cimmerians, and even some Assyrians. War raged between the two brothers for five years, until in 648 BC, Babylon was sacked, and Shamash-shum-ukin slain. Ashurbanipal then wrought savage revenge, Elam was utterly destroyed, the Aramean, Chaldean, Sutean tribes were brutally punished, Arabia was ravaged by the Assyrian army, and its rebellious shiekhs put to death. Cyrus I of Persia (grandfather of Cyrus the Great) was forced into submission, as a part of this defeated alliance.
Late in his reign, Ashurbanipal was forced to contend with renewed attempts on his empire by the Scythians and Cimmerians. The Scythians were able to once more ravage Assyria's Median and Persian colonies in Ancient Iran before being finally subdued, and the last decade or so of his reign seems to have been peaceful.
He built vast libraries and initiated a surge in the building of temples and palaces. After the crushing of the Babylonian revolt, Ashurbanipal appeared master of all he surveyed. To the east, Elam was devastated and prostrate before Assyria, the Manneans and the Iranian Persians and Medes were vassals. To the south, Babylonia was occupied, the Chaldeans, Arabs, Sutu and Nabateans subjugated, the Nubian empire destroyed, and Egypt paid tribute. To the north, the Scythians and Cimmerians had been vanquished and driven from Assyrian territory, Urartu (Armenia), Phrygia, Corduene and the neo Hittites were in vassalage, and Lydia pleading for Assyrian protection. To the west, Aramea (Syria), the Phoenicians, Israel, Judah, Samarra and Cyprus were subjugated, and the Hellenised inhabitants of Caria, Cilicia, Cappadocia and Commagene paid tribute to Assyria.
Assyria conquered the 25th dynasty Egypt (expelling its Nubian/Kushite dynasty) as well as Babylonia, Chaldea, Elam, Media, Persia, Urartu (Armenia), Phoenicia, Aramea/Syria, Phrygia, the Neo-Hittite States, the Hurrian lands, Arabia, Gutium, Israel, Judah, Samarra, Moab, Edom, Corduene, Cilicia, Mannea and parts of Ancient Greece (such as Cyprus), and defeated and/or exacted tribute from Scythia, Cimmeria, Lydia, Nubia, Ethiopia and others.
At its height, the Empire encompassed the whole of the modern nations of Iraq, Syria, Egypt, Lebanon, Israel, Jordan, Kuwait, Bahrain, Palestine and Cyprus, together with large swathes of Iran, Saudi Arabia, Turkey, Sudan, Libya, Armenia, Georgia and Azerbaijan.
Assyria now appeared stronger than ever. However, the long struggles pacifying the Babylonians, Chaldeans, Arameans and Elamites, the exertions undertaken in keeping the Medes, Scythians, Persians, Urartians and Cimmerians subjugated, and the constant campaigning over three centuries to control and expand its vast empire in all directions, had left Assyria exhausted.
It had been drained of wealth and manpower; the devastated provinces could yield nothing to supply the needs of the imperial exchequer, it was difficult to find sufficient troops to garrison and effectively control the huge empire, and after the death of Ashurbanipal severe civil unrest broke out in Assyria itself, and the empire began to unravel.
Downfall, 626–605 BC.
The Assyrian Empire was severely crippled following the death of Ashurbanipal in 627 BC—the nation and its empire descending into a prolonged and brutal series of civil wars involving three rival kings, Ashur-etil-ilani, Sin-shumu-lishir and Sin-shar-ishkun. Egypt's 26th Dynasty, which had been installed by the Assyrians as vassals, quietly detached itself from Assyria, although it was careful to retain friendly relations.
Ashur-etil-ilani came to the throne in 626 BC, and was immediately beset by a series of internal civil wars. He was deposed in 623 BC, after four years of bitter fighting by Sin-shumu-lishir, an Assyrian "Turtanu" (General) who also occupied and claimed the throne of Babylon in that year. In turn, Sin-shumu-lishir was deposed as ruler of Assyria and Babylonia after a year of warfare by Sin-shar-ishkun (622–612 BC)—who was then himself faced with constant violent rebellion in the Assyrian homeland.
This situation led to wholesale revolution in Babylonia, and during the reign of Sin-shar-ishkun many Assyrian colonies to the west, east and north similarly took advantage and ceased to pay tribute to Assyria, most significantly the Medes, Persians, Scythians, Cimmerians, Babylonians, Chaldeans and Arameans.
The Scythians and Cimmerians took advantage of the bitter fighting among the Assyrians to raid Assyrian colonies, with hordes of horse borne marauders ravaging parts of Asia Minor and the Caucasus, where the vassal kings of Urartu and Lydia begged their Assyrian overlord for help in vain. They also raided the Levant, Israel and Judah (where Ashkelon was sacked by the Scythians) and all the way into Egypt whose coasts were ravaged and looted with impunity.
The Iranic peoples (the Medes, Persians and Parthians), aided by the previous Assyrian destruction of the hitherto dominant Elamites of Ancient Iran, also took advantage of the upheavals in Assyria to coalesce into a powerful Median dominated force which destroyed the "pre-Iranic" Assyrian vassal kingdom of Mannea and absorbed the remnants of the pre-Iranic Elamites of southern Iran, and the equally pre-Iranic Gutians, Manneans and Kassites of the Zagros Mountains and the Caspian Sea.
In Aram (modern Syria), Phoenicia and southern Canaan (modern Israel, Jordan, Sinai and Palestine), the various Aramean, Phoenician and Jewish states quietly reasserted their independence, and in western Asia Minor and eastern Mediterranean, the Lydians, Greeks, Cilicians, Carians, Cappadocian and Luwian states did the same. Armenians, Sarmatians and Colchians (Georgians) also began to establish themselves in parts of the Caucasus.
By 620 BC, Nabopolassar, (a previously unknown "Malka" of the Chaldean tribes who had settled the far southeast of Mesopotamia circa 900 BC) had claimed the city of Babylon and swathes of Babylonia in the confusion. Sin-shar-ishkun amassed a large army to eject Nabopolassar from Babylon; however, yet another massive revolt broke out in Assyria proper, forcing the bulk of his army to turn back, where they promptly joined the rebels in Nineveh. Similarly, Nabopolassar was unable to gain control over all of Babylonia, and could not make any inroads into Assyria despite its weakened state, being repelled at every attempt. The next four years saw bitter fighting in the heart of Babylonia itself, as the Assyrians tried to wrest back control.
However, in 615 BC Nabopolassar entered into an alliance with the Median king Cyaxares the Great, a hitherto vassal of Assyria, who had taken advantage of the upheavals in Assyria to free the Iranian peoples from Assyrian vassalage and unite the Iranian Medes, Persians and Parthians, together with the remnants of the pre-Iranian Elamites, Gutians, Kassites and Manneans, into a powerful Median-dominated force.
Mass alliances against Assyria were not a new phenomenon. During the Middle Assyrian Empire (1365-1020 BC), peoples such as the Hittites, Babylonians, Mitanni, Hurrians, Elamites, Phrygians, Kassites, Arameans, Gutians and Canaanites had formed various coalitions at different times in vain attempts to break Assyrian power. During the Neo Assyrian Empire, in the reigns of Shalmaneser III in the 9th century BC, Sargon II in the 8th century BC, and Sennacherib and Ashurbanipal in the earlier part of the 7th century BC, combined attempts to break Assyrian dominance by alliances including at different times; Babylonians, Egyptians, Greeks, Persians, Elamites, Nubians, Medes, Chaldeans, Phoenicians, Canaanites, Lydians, Arameans, Suteans, Israelites, Judeans, Scythians, Cimmerians, Manneans, Urartians, Cilicians, Neo-Hittites and Arabs had all failed, Assyria being strong, well led and united, at the height of its power, and able to deal with any threat.
However the nation at this time was in a severely depleted state, ravaged by over a decade of internal civil war, disunity, instability, economic crisis and battle fatigue, and the forces ranged against it from all sides were to prove too much for the severely weakened Assyrians.
While Sin-shar-ishkun was fighting both the home grown rebels in Assyria and the Chaldeans and Babylonians in southern Mesopotamia, Cyaxares (technically a vassal of Assyria), in an alliance with the Scythians and Cimmerians, launched a surprise attack on a civil war bleaguered Assyria in 615 BC, sacking Kalhu (the Biblical Calah/Nimrud) and taking Arrapkha (modern Kirkuk). Nabopolassar, still pinned down in southern Mesopotamia, was completely uninvolved in this major breakthrough against Assyria.
However, from this point, the alliance of Medes, Persians, Chaldeans, Babylonians, Scythians and Cimmerians fought in unison against Assyria.
Despite the sorely depleted state of Assyria, bitter fighting ensued; throughout 614 BC the alliance of powers continued to gradually make hard fought inroads into Assyria itself, however in 613 BC the Assyrians somehow rallied against the odds and scored a number of counterattacking victories over the Medes-Persians, Babylonians-Chaldeans and Scythians-Cimmerians.
This led to the coalition of forces ranged against it to unite and launch a massive combined attack in 612 BC, finally besieging and entering Nineveh in late 612 BC, with Sin-shar-ishkun being slain in the bitter street by street fighting.
Despite the loss of almost all of its major cities, and in the face of overwhelming odds, Assyrian resistance continued. Ashur-uballit II (612- 605 BC) took the throne amid the street by street fighting in Nineveh, and refused a request to bow in vassalage to Nabopolassar, Cyaxares and their allies. He managed to break out of Nineveh and successfully fight his way to the northern Assyrian city of Harran, he took the city and founded it as a new capital. Ashur-uballit II somehow managed to keep control of a now greatly reduced Assyria for five years or so, repelling attacks by his enemies. However, Harran too was eventually besieged and taken by the Medes, Babylonians and Scythians in 608 BC, with Ashur-uballit II once more managing to break free of the siege.
Egypt, itself a former Assyrian colony whose current dynasty had been installed as puppet rulers by the Assyrians, then came to the aid of its former master, possibly in fear that without Assyrian protection it would be next to succumb, having already been ravaged by the Scythians.
Ashur-uballit II and Necho of Egypt made a failed attempt to recapture Harran in 608 BC. The next three years saw the remnants of the Assyrian army and their Egyptian allies vainly attempting to eject the invaders from Assyria. In 605 BC, the Babylonians, Scythians and Medes-Persians defeated the Assyrians and Egyptians at Carchemish.
Sections of the Assyrian army retreated to the western corner of Assyria after the fall of Harran and Carchemish, and a number of Assyrian imperial records survive between 604 BC and 599 BC in and around the Assyrian city of Dur-Katlimmu in what is today north eastern Syria, and so it is possible that remnants of the Assyrian administration and army still continued to hold out in the region for a few years.
The fate of Ashur-uballit II remains unknown, his Limmu Lists end after the fall of Harran, and it is possible he was either killed at this time, at the battle of Carshemish in 605 BC, or continued to fight on, eventually simply disappearing into obscurity.
Certainly by 599 BC at the very latest, Assyria had been destroyed as an independent political entity, although it was to launch major rebellions against the Achaemenid Empire in 546 BC and 520 BC, and remained a geo-political region, ethnic entity and colonised province until the late 7th century AD, with small Assyrian states emerging in the region between the 2nd century BC and 4th century AD.
Assyria after the empire.
Achaemenid Assyria, Athura, Assuristan, Assyria province, Adiabene, Osroene and Hatra.
Assyria was initially ruled by the short lived Median Empire (605-549 BC) after its fall. In a twist of fate, Nabonidus the last king of Babylon (together with his son and co-regent Belshazzar) was himself an Assyrian from Harran. He had overthrown the short lived Chaldean dynasty in Babylonia, after which the Chaldeans disappeared from history, being fully absorbed into the native population of Babylonia. However, apart from plans to dedicate religious temples in the city of Harran, Nabonidus showed little interest in rebuilding Assyria. Nineveh and Kalhu remained in ruins with only small numbers of Assyrians living within them, conversely a number of towns and cities such as Arrapkha, Guzana, Nohadra and Harran remained intact, and Assur and Arbela (Irbil) were not completely destroyed, as is attested by their later revival. However, Assyria spent much of this short period in a degree of devastation following its fall.
Achaemenid Assyria (549–330 BC).
After the Medes were overthrown by the Persians as the dominant force in Ancient Iran, Assyria was ruled by the Persian Achaemenid Empire (as Athura) from 549 BC to 330 BC (see Achaemenid Assyria). Between 546 and 545 BC, Assyria rebelled against the new Persian Dynasty, which had usurped the previous Median dynasty. The rebellion centered around Tyareh was eventually quashed by Cyrus the Great.
Assyria seems to have recovered dramatically, and flourished during this period. It became a major agricultural and administrative centre of the Achaemenid Empire, and its soldiers were a mainstay of the Persian Army. In fact, Assyria even became powerful enough to raise another full-scale revolt against the Persian empire in 520–519 BC.
The Persians had spent centuries under Assyrian domination (their first ruler Achaemenes, having been a vassal of Assyria), and Assyrian influence can be seen in Achaemenid art, infrastructure and administration. Early Persian rulers saw themselves as successors to Ashurbanipal, and Mesopotamian Aramaic was retained as the "lingua franca" of the empire for over two hundred years. Nineveh was never rebuilt however, and 200 years after it was sacked Xenophon reported only small numbers of Assyrians living amongst its ruins. Conversely the ancient city of Assur once more became a rich and prosperous entity.
Macedonian and Seleucid Assyria.
In 332 BC, Assyria fell to Alexander the Great, the Macedonian Emperor from Greece, who called the inhabitants "Assyrioi". The Macedonian Empire (332-312) was partitioned in 312 BC. It thereafter became part of the Seleucid Empire (312 BC) and was renamed Syria, a Hurrian, Luwian and Greek corruption of Assyria, a term which for many centuries until the Seleucid era had only and specifically meant and referred to Assyria and the Assyrians, and not to The Levant and its largely Aramean, Phoenician and Neo-Hittite inhabitants. It is from this period that the later "Syria" Vs "Assyria" naming controversy arises, the Seleucids applied the name not only to Assyria itself, but also to the Levantine lands to the west (historically known as Aram modern Syria/Eber Nari), which had been part of the Assyrian empire but never a part of Assyria.
When they lost control of Assyria, the name "Syria" survived but was erroneously applied only to the land of Aramea (also known as Eber Nari) to the west that had once been part of the Assyrian empire, but apart from the north eastern corner, had never been a part of Assyria itself, nor inhabited by Assyrians. This was to lead to both the Assyrians from Northern Mesopotamia and the Arameans and Phoenicians from the Levant being collectively dubbed Syrians (and later also Syriacs) in Greco-Roman culture, regardless of ethnicity, history or geography.
During Seleucid rule, Assyrians ceased to hold the senior military and civil positions they had enjoyed under the Achaemenids, being largely replaced by Greeks. The Greek language also replaced Mesopotamian East Aramaic as the lingua franca of the empire, although this did not affect the Assyrian population themselves, who were not Hellenised during the Seleucid era.
During the Seleucid period in southern Mesopotamia, Babylon was gradually abandoned in favour of a new city named Dura Seleucus, effectively bringing an end to "Babylonia".
Parthian Assyria (150 BC – 116 AD); Adiabene (69 BC – 117 AD).
By 150 BC, Assyria was largely under the control of the Parthian Empire, once more as Athura (the Mesopotamian East Aramaic word for Assyria). The Parthians seem to have exercised only loose control over Assyria. Temples to the native gods of Assyria were resurrected in many towns and cities. A number of independent Neo-Assyrian states arose, the most notable being Adiabene (69 BC - 117 AD). Adiabene was described by historian Georges Roux as a virtual resurrection of Assyria.
The Assyrians began to convert to Christianity from Mesopotamian religion (Ashurism) during the period between the early 1st and 3rd centuries AD, with the founding of the Assyrian Church of the East.
Roman Assyria (116 AD – 118 AD).
However, in 116 AD, under Trajan, Assyria and its independent states were taken over by Rome as the Roman Province of Assyria. The Assyrian kingdom of Adiabene was destroyed as an independent state during this period. Roman rule lasted only a few years, and the Parthians once more regained control with the help of the Assyrians, who were incited to overthrow the Roman garrisons by the Parthian king.
However, a number of Assyrians were conscripted into the Roman Army, with many serving in the region of Hadrians Wall in Ancient Britain, and inscriptions in Aramaic made by Assyrian and Aramean soldiers have been discovered in northern England dating from the 2nd century AD.
Parthian Assyria restored (119 AD – 225 AD), Osroene, Hatra.
Romans and Parthians fought over Assyria and the rest of Mesopotamia for the next century, allowing a number of other Neo-Assyrian states to arise, namely Osroene (132 BC to AD 244) and Hatra (155–241 AD). Osroene became the first Christian state in history, and a major center of Syriac literature and Syriac Christianity.
In addition, the ancient capital city of Ashur again flourished, and appears to have gained independence during the 2nd and 3rd centuries AD. Temples to the Assyrian national gods Ashur, Sin, Hadad, Ishtar and Shamash were once more dedicated throughout Assyria during this period. The noted Assyriologist Simo Parpola has speculated that Assyria may well have once again been fully independent for a while.
Sassanid Assyria (Assuristan (226 AD – circa 650 AD).
In 226 AD, Assyria was largely taken over by the Sassanid Empire. After driving out the Romans and Parthians, the Sassanid rulers set about destroying the independent states within Assyria; Hatra was dissolved in 241 AD, Osroene in 244 AD and Assur was sacked by Shapur I in 256 AD.
It was known as Asuristan (the Sassanid name for Assyria) during this period, and became the birthplace of the Church of the East (now split into the Assyrian Church of the East and Chaldean Catholic Church), with a flourishing Syriac (Assyrian) Christian culture which exists there to this day. Temples were still being dedicated to the national god Ashur in his home city and in Harran during the 4th century, indicating an Assyrian identity was still strong.
During the Sassanid period, much of what had once been Babylonia in southern Mesopotamia was incorporated into Assyria.
Parts of Assyria appear to have been semi independent as late as the latter part of the 4th century AD, with a king named Sennacherib II ruling the northern reaches in 370s AD.
Assyrians after Assyria.
Centuries of constant warfare between the Byzantine Empire and Sassanid Empire left both empires exhausted, depleted and battle-fatigued, allowing the Muslim Arabs to break from the Arabian peninsula and invade territories hitherto held by these empires. After the Arab Islamic conquest in the 7th century, Assyria was dissolved as an entity, although the native Assyrian population still regarded the region as Athura, an appellation which survived until the 19th century AD. Under Arab rule, Mesopotamia as a whole underwent a gradual process of Arabisation and Islamification, and the region saw a gradual large influx of "non indigenous" Arabs, Kurds, Iranian, and Turkic peoples. However, the indigenous Assyrian population of northern Mesopotamia (known as Ashuriyun by the Arabs) resisted this process, retaining their language, religion, culture and identity.
The previously basic civilisation of the desert dwelling Arabs was greatly enhanced and enriched by the influence and knowledge of native Mesopotamian scientists, physicians, mathematicians, theologians, astronomers, architects, agriculturalists, artists and astrologers.
However, despite this, indigenous Assyrians became second class citizens in a greater Arab Islamic state, and those who resisted Arabisation and conversion to Islam were subject to religious, ethnic and cultural discrimination, and had certain restrictions imposed upon them. They were excluded from specific duties and occupations reserved for Muslims, did not enjoy the same political rights as Muslims, their word was not equal to that of a Muslim in legal and civil matters, as Christians they were subject to payment of a special tax (jizyah), they were banned from spreading their religion further in Muslim ruled lands, men were banned from marrying Muslim women, but at the same time they were also expected to adhere to the same laws of property, contract and obligation as the Muslim Arabs.
Although predominantly Christian, a minority of Assyrians still held onto their ancient Mesopotamian religion until as late as the 10th century AD.
Assyrian people, still retaining Akkadian infused and influenced Eastern Aramaic and Assyrian Church of the East Christianity, remained dominant in the north of Mesopotamia (what had been Assyria) as late as the 14th century AD and the city of Assur was still occupied by Assyrians during the Islamic period until the mid-14th century when the Muslim Turco-Mongol ruler Tamurlane conducted a religiously motivated massacre of indigenous Assyrian Christians. After that, there are no traces of a settlement at Ashur in the archaeological and numismatic record, and from this point the Assyrian population was dramatically reduced in their homeland.
A religious schism among the Assyrians of northern Mesopotamia emerged in the 16th and 17th centuries AD, when a large number of hitherto Assyrian Church of the East Assyrians from the far north of Mesopotamia entered communion with the Roman Catholic Church after becoming dissatisfied with the Abuna family's leadership of the Assyrian Church, and after at first failing to gain acceptance within the Syriac Orthodox Church. Rome named this new church "The Church of Assyria and Mosul" and its first leader "Patriarch of the East Assyrians" in 1553 AD. However in 1683 AD, Rome altered the name to the Chaldean Catholic Church, this group of Assyrians eventually became known as Chaldean Catholics or Chaldo-Assyrians despite having no ethnic, historical, linguistic or geographic connections whatsoever to the long extinct Chaldean tribe of south east Mesopotamia.
The Assyrians suffered a number of religiously and ethnically motivated massacres throughout the 17th, 18th and 19th centuries AD, culminating in the large scale Hamidian massacres of unarmed men, women and children by Muslim Turks and Kurds in the late 19th century, which further greatly reduced numbers, particularly in southeastern Turkey.
The Assyrians suffered a further catastrophic series of massacres known as the Assyrian Genocide, at the hands of the Ottomans and their Kurdish and Arab allies from 1915–1918. The genocide (committed in conjunction with the Armenian Genocide and Greek Genocide) accounted for up to 300,000 unarmed Assyrian civilians, and the forced deportations of many more. The sizeable Assyrian presence in south eastern Asia Minor which had endured for over four millennia was reduced to a few thousand. As a consequence, the surviving Assyrians took up arms, and an Assyrian war of independence was fought during World War I, For a time, the Assyrians fought successfully against overwhelming numbers, scoring a number of victories over the Ottomans and Kurds, and also hostile Arab and Iranian groups; then their Russian allies left the war following the Russian Revolution, and Armenian resistance broke. The Assyrians were left cut off, surrounded, and without supplies, forcing those in Asia Minor and Northwest Iran to fight their way, with civilians in tow, to the safety of British lines and their fellow Assyrians in northern Iraq.
The Assyrian Levies were founded by the British in 1928, with ancient Assyrian military rankings, such as Rab-shakeh, Rab-talia and Tartan, being revived for the first time in millennia for this force. The Assyrians were prized by the British rulers for their fighting qualities, loyalty, bravery and discipline, and were used to help the British put down insurrections among the Arabs and Kurds, guard the borders with Iran and Turkey and protect British military installations.
After Iraq was granted independence by the British in 1933, the Assyrians suffered the Simele Massacre, where thousands of unarmed villagers (men, women and children) were slaughtered by joint Arab-Kurdish forces of the Iraqi Army. These massacres followed a clash between Assyrian tribesmen and the Iraqi army, where the Iraqi forces suffered a defeat after trying to disarm the Assyrians, whom they feared would attempt to secede from Iraq. Armed Assyrian Levies were prevented by the British from going to the aid of these civilians.
The Assyrians were allied with the British during World War II, with eleven Assyrian companies seeing action in Palestine/Israel and another four serving in Cyprus. The Parachute Company was attached to the Royal Marine Commando and was involved in fighting in Albania, Italy and Greece. Assyrians played a major role in the victory over Arab-Iraqi forces at the Battle of Habbaniya in 1941, when the Iraqi government decided to join WW2 on the side of Nazi Germany. The British presence in Iraq lasted until 1955, and Assyrian Levies remained attached to British forces until this time.
The period from the 1940s through to 1963 saw a period of respite for the Assyrians. The regime of President Kassim in particular saw the Assyrians accepted into mainstream society. Many urban Assyrians became successful businessmen, others were well represented in politics and the military, their towns and villages flourished undisturbed, and Assyrians came to excel, and be over represented in sports such as Boxing, Football, Athletics, Wrestling and Swimming.
However in 1963, the Ba'ath Party took power by force in Iraq. The Baathists, though secular, were Arab Nationalists, and set about attempting to "Arabize" the many non Arab peoples of Iraq, including the Assyrians. Other ethnic groups targeted for forced "Arabization" included Kurds, Armenians, Turcomans, Mandeans, Yezidi, Shabaki, Kawliya, Persians and Circassians. This policy included refusing to acknowledge the Assyrians as an ethnic group, banning the publication of written material in Eastern Aramaic, and banning its teaching in schools, banning parents giving Assyrian names to their children, banning Assyrian political parties, taking control of Assyrian churches, attempting to divide Assyrians on denominational lines (e.g. Assyrian Church of the East vs Chaldean Catholic Church vs Syriac Orthodox) and forced relocations of Assyrians from their traditional homelands to major cities.
In response to Baathist persecution, the Assyrians of the Zowaa movement within the Assyrian Democratic Movement took up armed struggle against the Iraqi regime in 1982 under the leadership of Yonadam Kanna, and then joined up with the IKF in early 1990s. Yonadam Kanna in particular was a target of the Saddam Hussein Ba'ath regime for many years.
The policies of the Bathists have also long been mirrored in Turkey, whose governments have refused to acknowledge the Assyrians as an ethnic group since the 1920s, and have attempted to "Turkify" the Assyrians by calling them "Semitic Turks" and forcing them to adopt Turkic names. In Syria too, the Assyrian/Syriac Christians have faced pressure to identify as "Arab Christians".
Many persecutions have befallen the Assyrians since, such as the Anfal campaign and Baathist, Arab and Kurdish nationalist and Islamist persecutions.
In recent years, the Assyrians in northern Iraq and north east Syria have become the target of extreme unprovoked Islamic terrorism. As a result, Assyrians have taken up arms, alongside other groups (such as the Kurds, Turcomans and Armenians) in response to unprovoked attacks by Al Qaeda, ISIS/ISIL, Nusra Front and other terrorist Islamic Fundamentalist groups. In 2014 Islamic terrorists of ISIS attacked Assyrian towns and villages in the Assyrian Homeland of northern Iraq, together with cities such as Mosul and Kirkuk which have large Assyrian populations. There have been reports of atrocities committed by ISIS terrorists since, including; beheadings, crucifixions, child murders, rape, forced conversions, Ethnic Cleansing, robbery, and extortion in the form of illegal taxes levied upon non Muslims. Assyrians in Iraq have responded by forming armed militias to defend their territories.
Thus far, the only people who have been attested with a high level of genetic, historical, linguistic and cultural research to be the descendants of the ancient Mesopotamians are the Assyrian Christians of Iraq and its surrounding areas in north west Iran, north east Syria and south eastern Turkey (see Assyrian continuity), although others have made unsubstantiated claims of continuity. Assyria continued to exist as a geopolitical entity until the Arab-Islamic conquest in the mid-7th century, and Assyrian identity, personal, family and tribal names, and both spoken and written evolutions of Mesopotamian Aramaic (which still contain many Akkadian loan words and an Akkadian grammatical structure) have survived among the Assyrian people from ancient times to this day. (see Assyrian people).
Assyrian religion.
The Assyrians, like the rest of the Mesopotamian peoples, followed the Sumero-Akkadian Mesopotamian Religion, with the national god Ashur having pride of place at the head of the pantheon. This religion survived in Assyria from c. 3500 BC through to its gradual decline in the face of Christianity between the 1st and 10th centuries AD.
Other major gods within the pantheon were Anu, Baal, Ea, Enlil, Ishtar (Astarte), Shamash, Tammuz, Adad/Hadad, Sin (Nanna), Dagan, Ninurta, Nisroch, Nergal, Tiamat, Ninlil, Mullissu, Zababa and El.
Native religion was still strongly followed at least until the 4th century AD, and survived in pockets until at least the 10th century AD, although Assyrians had begun to adopt Eastern Rite Christianity (as well as for a time Manicheanism and Gnosticism) which, like Syriac literature, had its birthplace in Assyria between the 1st and 3rd centuries AD. Assyrians today are exclusively Christian, with most following the Assyrian Church of the East, Chaldean Catholic Church, Ancient Church of the East and Syriac Orthodox churches.
Language.
During the 3rd millennium BC, a very intimate cultural symbiosis developed between the Sumerians and the Akkadians, which included widespread bilingualism. The influence of Sumerian on Akkadian (and vice versa) is evident in all areas, from lexical borrowing on a massive scale, to syntactic, morphological, and phonological convergence. This has prompted scholars to refer to Sumerian and Akkadian in the 3rd millennium BC as a "sprachbund".
Akkadian gradually replaced Sumerian as the spoken language of Mesopotamia somewhere around the turn of the 3rd and the 2nd millennium BC (the exact dating being a matter of debate), but Sumerian continued to be used as a sacred, ceremonial, literary and scientific language in Mesopotamia until the 1st century AD.
In ancient times, Assyrians spoke a dialect of the Akkadian language, an eastern branch of the Semitic languages. The first inscriptions, called Old Assyrian (OA), were made in the Old Assyrian period. In the Neo-Assyrian period the Aramaic language became increasingly common, more so than Akkadian—this was thought to be largely due to the mass deportations undertaken by Assyrian kings, in which large Aramaic-speaking populations, conquered by the Assyrians, were relocated to Assyria and interbred with the Assyrians. The ancient Assyrians also used the Sumerian language in their literature and liturgy, although to a more limited extent in the Middle- and Neo-Assyrian periods, when Akkadian became the main literary language.
The destruction of the Assyrian capitals of Nineveh and Assur by the Babylonians, Medes and their allies, ensured that much of the bilingual elite (but not all) were wiped out. By the 7th century BC, much of the Assyrian population used Akkadian influenced Eastern Aramaic and not Akkadian itself. The last Akkadian inscriptions in Mesopotamia date from the 1st century AD. However, Eastern Aramaic dialects, as well as Akkadian and Mesopotamian Aramaic personal and family names, still survive to this day among Assyrians in the regions of northern Iraq, southeast Turkey, northwest Iran and northeast Syria that constituted old Assyria.
After 90 years of effort, the University of Chicago has published an Assyrian Dictionary, whose form is more encyclopedia in style than dictionary.
Arts and sciences.
Assyrian art preserved to the present day predominantly dates to the Neo-Assyrian period. Art depicting battle scenes, and occasionally the impaling of whole villages in gory detail, was intended to show the power of the emperor, and was generally made for propaganda purposes. These stone reliefs lined the walls in the royal palaces where foreigners were received by the king. Other stone reliefs depict the king with different deities and conducting religious ceremonies. Many stone reliefs were discovered in the royal palaces at Nimrud (Kalhu) and Khorsabad (Dur-Sharrukin). A rare discovery of metal plates belonging to wooden doors was made at Balawat (Imgur-Enlil).
Assyrian sculpture reached a high level of refinement in the Neo-Assyrian period. One prominent example is the winged bull "Lamassu", or shedu that guard the entrances to the king's court. These were apotropaic meaning they were intended to ward off evil. C. W. Ceram states in "The March of Archaeology" that "lamassi" were typically sculpted with five legs so that four legs were always visible, whether the image were viewed frontally or in profile.
Although works of precious gems and metals usually do not survive the ravages of time, some fine pieces of Assyrian jewelry were found in royal tombs at Nimrud.
There is ongoing discussion among academics over the nature of the Nimrud lens, a piece of quartz unearthed by Austen Henry Layard in 1850, in the Nimrud palace complex in northern Iraq. A small minority believe that it is evidence for the existence of ancient Assyrian telescopes, which could explain the great accuracy of Assyrian astronomy. Other suggestions include its use as a magnifying glass for jewellers, or as a decorative furniture inlay. The Nimrud Lens is held in the British Museum.
The Assyrians were also innovative in military technology, with the use of heavy cavalry, sappers, siege engines etc.
Legacy.
Achaemenid Assyria (539–330 BC) retained a separate identity (Athura), official correspondence being in Imperial Aramaic, and there was even a determined revolt of the two Assyrian provinces of Mada and Athura in 520 BC. Under Seleucid rule (330 BC – approximately 150 BC), however, Aramaic gave way to Greek as the official administrative language. Aramaic was marginalised as an official language, but remained spoken in both Assyria and Babylonia by the general populace. It also remained the spoken tongue of the indigenous Assyrian/Babylonian citizens of all Mesopotamia under Persian, Greek and Roman rule, and indeed well into the Arab period it was still the language of the majority, particularly in the north of Mesopotamia, surviving to this day among the Assyrian Christians.
Between 150 BC and 226 AD, Assyria changed hands between the Parthian Iranians and Romans (Roman Province of Assyria) until coming under the rule of Sassanid Persia in 226–651 AD, where it was known as Asuristan.
A number of at least partly neo-Assyrian kingdoms existed in the area between in the late classical and early Christian period also; Adiabene, Hatra and Osroene.
Classical historiographers and Biblical writers had only retained a fragmented, very dim and often inaccurate picture of Assyria. It was remembered that there had been an Assyrian empire predating the Persian one, but all particulars were lost. Thus Jerome's "Chronicon" lists 36 kings of the Assyrians, beginning with Ninus, son of Belus, down to Sardanapalus, the last king of the Assyrians before the empire fell to Arbaces the Median. Almost none of these have been substantiated as historical, with the exception of the Neo-Assyrian and Babylonian rulers listed in Ptolemy's Canon, beginning with Nabonassar.
Mesopotamian empires such as the Akkadian Empire, Babylonian Empire, Middle Assyrian Empire, Neo Assyrian Empire and Neo Babylonian Empire asserted Mesopotamian dominance from the Caucasus Mountains to Arabia and Egypt, and from Cyprus and the east Mediterranean to Persia. Thus the influence exerted by the Babylonian-Assyrian religion was particularly profound on other Semites, including the Hebrews, Chaldeans, Canaanites, Arameans, Phoenicians and Arabs, while their astral theology affected the ancient world in general, including the Persians, Greeks, Armenians and the later Romans. The impetus to the purification of the old Semitic polytheistic religions to which the Hebrews for a long time clung in common with their fellows—the various branches of nomadic Arameans and Arabs—was largely furnished by the remarkable civilization unfolded in the Euphrates valley and in many of the traditions, myths and legends embodied in the Old Testament; traces of direct adaptation from and responses to Babylonia and Assyria may be discerned, while the indirect influences in the domain of the prophetical books, as also in the Psalms and in the so-called "wisdom literature", are even more noteworthy. Stories in the Tanakh, Old Testament and Quran such as; the Genesis creation narrative, Tower of Babel, The Great Flood and the book of Esther, as well as various biblical characters such as Noah, Nimrod, Lilith and Asnapper bear clear influence from Assyria and Babylonia.
Even when we reach the New Testament period, we have not passed entirely beyond the sphere of Babylonian-Assyrian influences. In such a movement as early Christian gnosticism, Assyrio-Babylonian elements—modified, to be sure, and transformed—are largely present, while the growth of an apocalyptic literature is ascribed with apparent justice by many scholars to the recrudescence of views, the ultimate source of which is to be found in the astral-theology of the Babylonian and Assyrian Priests.
The Assyrians began to form and adopt a distinct Eastern Rite Christianity, with its accompanying Syriac literature, between the 1st and 3rd centuries AD, however native religion was still alive and well into the 4th century AD, and pockets survived into the 10th century AD and possibly as late as the 17th century in Mardin. However, the religion is now dead, and the indigenous Assyrian (a.k.a. Chaldo-Assyrian) people, though still retaining Eastern Aramaic dialects as a mother tongue, are now wholly Christian.
The modern discovery of Babylonia and Assyria begins with excavations in Nineveh in 1845, which revealed the Library of Ashurbanipal. Decipherment of cuneiform was a formidable task that took more than a decade; but, by 1857, the Royal Asiatic Society was convinced that reliable reading of cuneiform texts was possible. Assyriology has since pieced together the formerly largely forgotten history of Mesopotamia. In the wake of the archaeological and philological rediscovery of ancient Assyria, Assyrian nationalism became increasingly popular among the surviving remnants of the Assyrian people, and has come to strongly identify with ancient Assyria.

</doc>
<doc id="2193" url="http://en.wikipedia.org/wiki?curid=2193" title="Arcology">
Arcology

Arcology, a portmanteau of "architecture" and "ecology", is a vision of architectural design principles for very densely populated habitats. The concept has been primarily popularized, and the term itself coined, by architect Paolo Soleri. It appears also in science fiction: These structures have been largely hypothetical insofar as no 'arcology' envisioned by Soleri himself has yet been completed, but he posited that a completed arcology would provide space for a variety of residential, commercial, and agricultural facilities while minimizing individual human environmental impact. Arcologies are often portrayed in sci-fi as self-contained or economically self-sufficient.
Development.
An arcology is distinguished from a merely large building in that it is designed to lessen the impact of human habitation on any given ecosystem. It could be self-sustainable, employing all or most of its own available resources for a comfortable life: power; climate control; food production; air and water conservation and purification; sewage treatment; etc. An arcology is designed to make it possible to supply those items for a large population. An arcology would supply and maintain its own municipal or urban infrastructures in order to operate and connect with other urban environments apart from its own.
Arcology was proposed to reduce human impact on natural resources. Arcology designs might apply conventional building and civil engineering techniques in very large but practical projects in order to achieve pedestrian economies of scale that have proven, post-automobile, to be difficult to achieve in other ways.
Frank Lloyd Wright proposed an early version called Broadacre City although, in contrast to an arcology, Wright's idea is comparatively two-dimensional and depends on a road network. Wright's plan described transportation, agriculture, and commerce systems that would support an economy. Critics said that Wright's solution failed to account for population growth, and assumed a more rigid democracy than the U.S.A. actually has.
Buckminster Fuller proposed the Old Man River's City project, a domed city with a capacity of 125,000, as a solution to the housing problems in East St. Louis, Illinois.
Paolo Soleri proposed later solutions, and coined the term 'arcology'. Soleri describes ways of compacting city structures in three dimensions to combat two-dimensional urban sprawl, to economize on transportation and other energy uses. Like Wright, Soleri proposed changes in transportation, agriculture, and commerce. Soleri explored reductions in resource consumption and duplication, land reclamation; he also proposed to eliminate most private transportation. He advocated for greater "frugality" and favored greater use of shared social resources, including public transit (and public libraries).
Similar real-world projects.
The largest arcology-style project under current development is Masdar City near Abu Dhabi, in the United Arab Emirates. It is projected to house between 45,000 and 50,000 inhabitants on 6 square kilometers, and to have a sustainable, zero-waste, ecology.
Arcosanti is an experimental "arcology prototype" - a demonstration project under construction in central Arizona. Designed by Paolo Soleri, its primary purpose is to demonstrate Soleri's personal designs, his application of principles of arcology to create a pedestrian-friendly urban form.
Many cities in the world have proposed projects adhering to the design principles of the arcology concept, like Tokyo, and Dongtan near Shanghai. The Dongtan project may have collapsed, and it failed to open for the Shanghai World Expo in 2010.
Certain urban projects reflect arcology principles. Pedestrian connection systems often provide a wide range of goods and services in a single structure. Some examples include the +15 system in downtown Calgary, Montréal’s RÉSO or the Minneapolis Skyway System and The Windscreen in Fermont, Quebec. They include supermarkets, malls and entertainment complexes. The +15 is the world's most extensive skywalk, at 16 km in total length. Minneapolis has the longest single path, at 8 mi. Seward's Success, Alaska was never built, but would have been a small city just outside of Anchorage. Chicago has a sizeable tunnel system known as the Chicago Pedway connecting a portion of the buildings in the Chicago Loop.
The Las Vegas Strip has many arcology features to protect people from the 45 C heat. Many major casinos are connected by tunnels, footbridges, and monorails. It is possible to travel from Mandalay Bay at the south end of the Strip to the Las Vegas Convention Center, three miles (5 km) to the north, without using streets. In many cases, it is possible to travel between several different casinos without ever going outdoors. It is possible to live in this complex without need to venture outside, except the Strip has not generally been considered self-sustainable. Soleri did not advocate for enclosed cities, although he did sketch a design and build a model of an 'arcology' for outer space.
The Toronto downtown area features an underground pedestrian network, PATH. Multiple high-rises are connected by a series of underground tunnels. It is possible to live in this complex without needing to venture outside, but the PATH network is not self-sustaining, nor is it presently self-sustainable. The total network spans 28-kilometres (17 mi).
McMurdo Station of the United States Antarctic Program and other scientific research stations on the continent of Antarctica resemble the popular conception of an arcology as a technologically advanced, relatively self-sufficient human community. The Antarctic research base provides living and entertainment amenities for roughly 3,000 staff who visit each year. Its remoteness and the measures needed to protect its population from the harsh environment give it an insular character. The station is not self-sufficient — the U.S. military delivers 8 e6USgal of fuel and 11 e6lb of supplies and equipment yearly through its Operation Deep Freeze resupply effort — but it is isolated from conventional support networks. The base generates electricity with its own power plant, and grows fruits and vegetables in a hydroponic green house when resupply is non-existent. Under international treaty, it must avoid damage to the surrounding ecosystem.
Crystal Island is a proposed arcology in Moscow, Russia. In 2009, construction was postponed indefinitely due to the global economic crisis.
The Begich Towers operates like a small-scale arcology encompassing nearly all of the population of Whittier, Alaska. The pair of buildings contains residential housing as well as a school, grocery, and municipal offices 
In popular culture.
Most proposals to build real arcologies have failed due to financial, structural or conceptual shortcomings. Arcologies are therefore found primarily in fictional works. One significant example is the novel Oath of Fealty by Larry Niven and Jerry Pournelle. In the novel, a segment of the Los Angeles population has moved into an arcology. The plot examines the social changes that result, both inside and outside the arcology. Thus the arcology is presented not just as a plot device but as a subject of critique.
Films and television.
Video games
Role-playing and table-top games
References.
Notes
External links.
Usage of "arcology" vs. "hyperstructure"

</doc>
<doc id="2198" url="http://en.wikipedia.org/wiki?curid=2198" title="Abdulaziz al-Omari">
Abdulaziz al-Omari

Abdulaziz al-Omari (Arabic: عبد العزيز العمري‎, "ʿAbd al-ʿAzīz al-ʿUmarī", also transliterated as Alomari or al-Umari; May 28, 1979 – September 11, 2001) was a Saudi airport security guard and Imam, best known for being one of five hijackers of American Airlines Flight 11 as part of the September 11 attacks.
Omari arrived in the United States in June 2001, on a tourist visa, obtained through the Visa Express program. On September 11, 2001, Omari boarded American Airlines Flight 11 and assisted in the hijacking of the plane, which was crashed into the North Tower of the World Trade Center, as part of the coordinated attacks.
Early life and education.
Little is known about Omari's life, and it is unclear whether some information refers to Omari or another person by that name. He has used birth dates of December 24, 1972 and May 28, 1979.
Omari came from 'Asir Province, a poor region in southwestern Saudi Arabia that borders Yemen, and graduated with honours from high school, attained a degree from the Imam Muhammad Ibn Saud Islamic University, was married, and had a daughter.
Career.
He is alleged to have often served as an Imam at his mosque in Saudi Arabia and is believed by American Authorities to have been a student of a Saudi cleric named Sulaiman Al-Alwan, whose mosque is located in Al-Qassim Province.
According to Tawfiq bin Attash, Omari was one of a group of future hijackers who provided security at Kandahar airport after their basic training at an al-Qaeda camp. During the 2000 Al Qaeda Summit in Kuala Lumpur, American authorities claim that immigration records show that a person named Abdulaziz al-Omari was visiting the country, although they say they are not sure that this was the same person.
In the autumn of 2001, after the September 11 attacks, al Jazeera television broadcast a tape they claim was made by Omari. The speaker made a farewell suicide video. In it, he reads "I am writing this with my full conscience and I am writing this in expectation of the end, which is near. . . God praise everybody who trained and helped me, namely the leader Sheikh Osama bin Laden."
According to FBI director Robert Mueller and the 9/11 Commission, Omari entered the United States through a Dubai flight on June 29, 2001, with Salem al-Hazmi, landing in New York. He had used the controversial Visa Express program to gain entry. He apparently stayed with several other hijackers in Paterson, New Jersey, before moving to his own place at 4032 57th Terrace, Vero Beach, Florida. On his rental agreement form for that house, Omari gave two license-plates authorized to park in his space, one of which was registered to Atta. Omari occasionally trained on simulators at the FlightSafety Academy in Vero Beach, Florida together with Mohand al-Shehri and Saeed al-Ghamdi.
Omari obtained a fake United States ID card from All Services Plus in Passaic County, New Jersey, which was in the business of selling fake documents, including another to Khalid al-Mihdhar.
Attacks.
On September 10, 2001, Mohamed Atta picked up Omari from the Milner Hotel in Boston, Massachusetts, and the two drove their rented Nissan to a Comfort Inn in South Portland, Maine, where they spent the night in room 232. It was initially reported that Adnan and Ameer Bukhari were the two hijackers who had rented and driven the car.
In the early hours of September 11, they boarded a commuter flight back to Boston to connect to American Airlines Flight 11. American 11 was hijacked 15 minutes after the flight departed by Omari and four other hijackers, which allowed trained pilot Mohamed Atta to crash the Boeing 767 into the North Tower of the World Trade Center as part of an attack that killed thousands of people.
Legacy.
Mistaken identity.
Controversy over Omari's identity erupted shortly after the attacks. At first, the FBI had named Abdul Rahman al-Omari, a pilot for Saudi Arabian Airlines, as the pilot of Flight 11. It was quickly shown that this person was still alive, and the FBI issued an apology. It was also quickly determined that Mohamed Atta was the pilot among the hijackers. The FBI then named Abdulaziz al-Omari as a hijacker.
A man with the same name as those given by the FBI turned up alive in Saudi Arabia, saying that he had studied at the University of Denver and his passport was stolen there in 1995. The name, origin, birth date, and occupation were released by the FBI, but the picture was not of him. "I couldn't believe it when the FBI put me on their list", he said. "They gave my name and my date of birth, but I am not a suicide bomber. I am here. I am alive. I have no idea how to fly a plane. I had nothing to do with this."

</doc>
<doc id="2224" url="http://en.wikipedia.org/wiki?curid=2224" title="April 8">
April 8

April 8 is the day of the year in the Gregorian calendar.

</doc>
<doc id="2238" url="http://en.wikipedia.org/wiki?curid=2238" title="Antipope John XXIII">
Antipope John XXIII

Baldassarre Cossa (c. 1370 – 22 December 1419) was antipope John XXIII (1410–1415) during the Western Schism. The Catholic Church regards him as an antipope, as he opposed the Pope whom the Catholic Church now recognizes as the rightful successor of Saint Peter. He was eventually deposed and tried for various crimes, though later accounts question the veracity of those accusations.
Early life.
Baldassarre Cossa was born on the island of Procida or Ischia in the Kingdom of Naples into a noble but impoverished family. Initially he followed a military career, taking part in the Angevin-Neapolitan war. His two brothers were sentenced to death for piracy by Ladislaus of Naples.
He studied law at the University of Bologna and obtained a doctorate. In 1392 he entered the service of Pope Boniface IX, first working in Bologna and then in Rome. (The Western Schism had begun in 1378 and there were two competing popes at the time, one in Avignon supported by France and Spain, and one in Rome supported by most of Italy, Germany and England.) Still a member of the laity, he became Cardinal deacon in 1402 and Papal legate in Forlì in 1403. At this time Cossa also had some links with local robber bands, which were often used to intimidate his rivals and attack carriages. These connections added to his influence and power in the region.
Role in the Western Schism.
The Council of Pisa.
He was one of the seven cardinals who, in May 1408, deserted Pope Gregory XII, and, with those following Antipope Benedict XIII from Avignon, convened the Council of Pisa, of which Cossa became the leader. The aim of the council was to end the schism; to this end they deposed Gregory XII and Benedict XIII and elected the new pope Alexander V in 1409. Gregory and Benedict ignored this decision however, so that there were now three simultaneous claimants to the Papacy.
Election to the Papacy.
Alexander V died soon after, and on 25 May 1410 Cossa was consecrated pope, taking the name John XXIII. He had been ordained priest only one day earlier. John XXIII was acknowledged as pope by France, England, Bohemia, Prussia, Portugal, parts of the Holy Roman Empire, and numerous Northern Italian city states, including Florence and Venice; however, the Avignon Pope Benedict XIII was regarded as pope by the Kingdoms of Aragon, Castile, Sicily and Scotland and Gregory XII was still favored by Ladislaus of Naples, Carlo I Malatesta, the princes of Bavaria, Louis III, Elector Palatine, and parts of Germany and Poland.
The Medici had supported Cossa in his campaign to become cardinal and pope. Once in office, John XXIII made the Medici Bank the bank of the papacy, contributing considerably to the family's wealth and prestige.
John had his officials sell indulgences, a controversial practice that was protested in various parts of Europe, for instance by the followers of Jan Hus in Prague.
The main enemy of John was Ladislaus of Naples, who protected Gregory XII in Rome. Following his election as pope, John spent a year in Bologna and then joined forces with Louis II of Anjou to march against Ladislaus. An initial victory proved short-lived and Ladislaus retook Rome in May 1413, forcing John to flee to Florence.
In Florence he met Sigismund, King of the Romans. Sigismund wanted to end the schism and urged John to call a general council. John did so with hesitation, at first trying to have the council held in Italy (rather than in a German Imperial City, as Sigismund wanted). The Council of Constance was convened on 30 October 1413. During the third session, rival Pope Gregory XII authorized the council as well. The council resolved that all three popes should abdicate and a new pope be elected.
Flight from the Council of Constance.
In March, John escaped from Constance disguised as a postman. According to the Klingenberger Chronicle, written by a noble client of Frederick IV, Duke of Austria, John XXIII travelled down the Rhine to Schaffhausen in a boat, while Frederick accompanied him with a small band of men on horseback. There was a huge outcry in Constance when it was discovered that John had fled, and Sigismund was furious about this setback to his plans for ending the Schism. The King of the Romans issued orders to all the powers on the Upper Rhine and in Swabia stating that he had declared Frederick to be an outlaw and that his lands and possessions were forfeit. In due course this led to a great deal of political upheaval and many Austrian losses in the region, notably in Aargau to the Swiss Confederation.
In the meantime, Pope John XXIII and Frederick fled further downriver along the Rhine to the town of Freiburg im Breisgau, which recognised the duke of Austria as its lord. There Sigismund's lieutenant Ludwig III, Elector Palatine caught up with them. He convinced Frederick that he stood to lose too much by harbouring the fugitive pope, and the Austrian duke agreed to give himself and John up and return to Constance.
Deposition.
During his absence John was deposed by the council, and upon his return he was tried for heresy, simony, schism and immorality, and found guilty on all counts. Gibbon wrote, "The more scandalous charges were suppressed; the vicar of Christ was accused only of piracy, rape, sodomy, murder and incest." John was given over to Ludwig III, Elector Palatine, who imprisoned him for several months in Heidelberg and Mannheim.
The last remaining claimant in Avignon, Benedict XIII, refused to resign and was excommunicated. Martin V was elected as new pope in 1417.
Death and burial.
Cossa, as he was again, was imprisoned in Germany. He was freed in 1418 after a heavy ransom was paid by the Medici. He went to Florence where he submitted to Martin V who made him Cardinal Bishop of Frascati. Cossa died only a few months later.
The Medici oversaw the construction of his magnificent tomb by Donatello and Michelozzo in the Battistero di San Giovanni in Florence. Pope Martin V protested in vain against the inscription on the sarcophagus: "John the former pope".
The 1910 "Catholic Encyclopedia" remarks that "Undeniably secular and ambitious, his moral life was not above reproach, and his unscrupulous methods in no wise accorded with the requirements of his high office ... the heinous crimes of which his opponents in the council accused him were certainly gravely exaggerated." One of his secretaries concluded that John was "a great man in temporal things, but a complete failure and worthless in spiritual things".
Numbering issues.
He should not be confused with Pope John XXIII of the twentieth century. When Angelo Roncalli was elected pope in 1958, there was some confusion as to whether he would be "John XXIII" or "John XXIV"; he then declared that he was John XXIII to put this question to rest. There was no John XX; this is why Gibbon refers to the antipope John as John XXII.
Notes and references.
In 1983 political satirist/novelist Richard Condon ("The Manchurian Candidate") wrote "A Trembling Upon Rome," a novel of historical fiction about the life of Baldassare Cossa.

</doc>
<doc id="2251" url="http://en.wikipedia.org/wiki?curid=2251" title="Accusative case">
Accusative case

The accusative case (abbreviated acc) of a noun is the grammatical case used to mark the direct object of a transitive verb. The same case is used in many languages for the objects of (some or all) prepositions. It is a noun that is having something done to it, usually joined (such as in Latin) with the nominative case. The syntactic functions of the accusative consist of designating the immediate object of an action, the intended result, the goal of a motion, and the extent of an action.
The accusative case existed in Proto-Indo-European and is present in some Indo-European languages (including Latin, Sanskrit, Greek, German, Polish, Romanian, Russian, Ukrainian), in the Uralic languages, in Altaic languages, and in Semitic languages (such as Hebrew and Classical Arabic). Finnic languages, such as Finnish and Estonian, have two cases to mark objects, the accusative and the partitive case. In morphosyntactic alignment terms, both perform the accusative function, but the accusative object is telic, while the partitive is not.
Modern English, which almost entirely lacks declension in its nouns, does not have an explicitly marked accusative case even in the pronouns. Such forms as "whom", "them", and "her" derive rather from the old Germanic dative forms, of which the -m and -r endings are characteristic. This conflation of the old accusative, dative, instrumental, and (after prepositions) genitive cases is the "oblique case". Most modern English grammarians no longer use the Latin accusative/dative model, though they tend to use the terms "objective" for oblique, "subjective" for nominative, and "possessive" for genitive "(see Declension in English)." "Hine", a true accusative masculine third person singular pronoun, is attested in some northern English dialects as late as the 19th century.
Etymology.
The English name "accusative (case)" is an Anglicisation of the Latin "accūsātīvus" ("cāsus"), which was translated from Ancient Greek αἰτιατικὴ (πτῶσις), "aitiatikē (ptôsis)". The Greek term can mean either "(inflection) for something caused" or "for an accusation". The intended meaning was likely the first, which would be translated as Latin "causātīvus" or "effectīvus", but the Latin term was a translation of the second. Compare Russian вини́тельный "vinítel’nyj", from винить "vinít’" "to blame".
Description.
In the sentence He sees the woman", "he" is the subject of the sentence, while in "The woman sees him, "him" is the object. In English we distinguish the two uses by different forms of the pronoun: he/him. If, however, instead of a pronoun, we use a noun, we make no such distinction in the form of the word. Thus, we use the same word "man" in both The man sees the woman" and "The woman sees the man. In many languages, however, different forms of the word are used not only for pronouns, but for nouns too. For example, in Latin "The man sees the woman" = "Vir feminam videt", while "The woman sees the man" = "Femina virum videt". For "man", Latin uses "vir" for the subject, and "virum" for the object. Likewise, in the same pair of sentences, we have "femina" for a subject and "feminam" for object. The form used for the direct object ("him", "virum", "feminam") is known as the "accusative case", while the form used for the subject ("he", "vir", "femina") is known as the nominative case.
Just as with pronouns and nouns, many inflected languages also make distinctions between cases in their adjectives and (for languages that have them) articles. Thus in German, "the giant" as the subject of a sentence may be expressed as "der Riese": nominative case. As the object of a verb, this becomes "den Riesen", the accusative.
Examples.
Indo-European languages.
Latin.
In Latin, nouns, adjectives, or pronouns in the accusative case ("accusativus") can be used
For the accusative endings, see Latin declension.
Latin prepositions.
Some Latin prepositions take a noun in the accusative. A few prepositions may take either an accusative or an ablative, in which case the accusative indicates motion, and the ablative indicates no motion. E.g. "in casā", "in the cottage"; "in casam", "into the cottage".
This "aide-memoire" was taught in schools when Latin was on the curriculum:
"Ante, apud, ad, adversus,"
"Circum, circa, citra, cis,"
"Contra, inter, erga, extra,"
"Infra, intra, iuxta, ob,"
"Penes, pone, post," and "praeter,"
"Prope, propter, per, secundum,"
"Supra, versus, ultra, trans:"
When 'motion' 'tis, not 'state' they mean.**
Or try: ** "And unto these if motion be intended,"
"Let In, Sub, Super, Subter be appended ' '
German.
German uses the accusative to mark direct objects and objects of certain prepositions, or adverbs relating to time. The accusative is only marked for masculine articles, pronouns, adjectives, and weak nouns.
German articles.
The masculine forms for German articles, e.g., "der" ('the'), "ein" ('a/an'), "mein" ('my'), etc., change in the accusative case: they always end in "-en". The article of feminine, neutral and plural forms do not change.
For example, "Hund" (dog) is a masculine ("ein/der") word, so the article changes when used in the accusative case:
German pronouns.
Some German pronouns also change in the accusative case.
German prepositions.
The accusative case is also used after particular German prepositions. These include "bis, durch, entlang, für, gegen, ohne, um", after which the accusative case is always used, and "an, auf, hinter, in, neben, über, unter, vor, zwischen" which can govern either the accusative or the dative. The latter prepositions take the accusative when motion or action is specified (being done into/onto the space), but take the dative when location is specified (being done in/on that space). These prepositions are also used in conjunction with certain verbs, in which case it is the verb in question which governs whether the accusative or dative should be used.
German adjectives.
Adjective endings also change in the accusative case. Another factor that determines the endings of adjectives is whether the adjective is being used after a definite article (the), after an indefinite article (a/an) or without any article before the adjective ("many" green apples).
German adverbial use.
In German, the accusative case is also used for some adverbial expressions, mostly temporal ones, as in ""Diesen Abend bleibe ich daheim" (This evening I'm staying at home), where "diesen Abend" is marked as accusative, although not a direct object.
Russian.
In Russian, accusative is used not only to display the direct object of an action, but also to indicate the destination or goal of motion. It is also used with some prepositions. The prepositions "в" and "на" can both take accusative in situations where they are indicating the goal of a motion.
In the masculine, Russian also distinguishes between animate and inanimate nouns with regard to the accusative; only the animates carry a marker in this case.
In fact Russian almost lost the real PIE accusative case, since only singular feminine nouns ending in 'a' have a distinct form. Other words use the genitive case or the nominative case in place of the accusative, depending on their animacy.
Armenian.
While the Armenian dialects both have a de facto accusative case, Eastern Armenian uses an accusative marker for transitive verbs
Example:
գիրք - girkh - book (Nominative)
ուսուցիչ - usuchičh - teacher (Nominative)
Արամը վերցրեց գիրքը: 
Aramë verchrech girkhë
Aram took the book.
Արամը սիրում է իր ուսուցչին: 
Aramë sirum ē ir usuchčhin
Aram loves his teacher.
Greek.
In both Ancient and Modern Greek, nouns, adjectives, verb participles, articles and pronouns are used in the accusative case, when they indicate a direct object or if they are preceded by a preposition. There is a wide variety of accusative markers depending on gender, number and declension. Like in Latin, all neuter names yield the same form in both the nominative and the accusative case in Ancient Greek. In its modern successor, this rule also extends to most feminine nouns, except these ending to -ος.
Example: "He was also calumniating Socrates.""
Constructed languages.
Esperanto.
Esperanto grammar involves only two cases, a nominative and an accusative. The accusative is formed by the addition of "-n" to the nominative form, and is the case used for direct objects. Other case functions, including dative functions, are achieved with prepositions, all of which normally take the nominative case. Direction of motion can be expressed either by the accusative case, or by the preposition "al" (to) with the nominative.
Ido.
In Ido the "-n" suffix is optional, as subject–verb–object order is assumed when it is not present. Note that this is sometimes done in Esperanto, especially by beginners, but it is considered incorrect while in Ido it is the norm.
Uralic languages.
Finnish.
According to traditional Finnish grammars, the accusative is the case of a total object, while the case of a partial object is the partitive. The negative forms of verbs always take the partial object, whereas in positive sentences it depends on the nature of the action, the main rule being that incomplete or indefinite action requires a partial object.
The accusative singular is identical either to the nominative (often called nominative-accusative) or the genitive (genitive-accusative). In plural, only nominative-accusative exists. The active verb forms usually require the total object in the genitive-accusative and passive forms take the nominative-accusative. The only exceptions to this rule are imperative first and second persons, and the rarely used third infinitive in instructive, which take the total object in nominative-accusative.
The personal pronouns and the personal interrogative pronoun "kuka/ken" have a special accusative form ending in "-t" which is used in place of both nominative-accusative and genitive-accusative. For example, the accusative form of "hän" (he/she) is "hänet", and the accusative form of "kuka" (or "ken") is "kenet".
The major new Finnish grammar, "Iso suomen kielioppi", breaks with the traditional classification by limiting the accusative case to the special case of the personal pronouns and "kuka/ken". The new grammar considers other total objects as being in the nominative or genitive case.
Hungarian.
The accusative case in Hungarian applies to nouns, pronouns; even to adjectives and numerals when either of them stands alone in the sense of direct object.
Accusative is formed by the suffix -t. In many cases, "-t" is preceded by a suffix-initial vowel, primarily based on specific vowel harmony, resulting in "-et", "-ot", or "-öt". The rules are complex, also involve consonants, and have exceptions. Thus: k"e"rt"et" "garden", k"é"k"et" "blue"; h"a"t"o"t "six"; p"o"lc"ot" "shelf"; k"ö"d"öt" "fog".
In some words, a low vowel "a" or "e" appears instead of the expected harmonic vowel: e.g. fal"at" (ˣfal"ot") "wall"; nyolc"at" (ˣnyolc"ot") "eight"; könyv"et" (ˣkönyv"öt") "book".
In fewer cases, the root of the word is also affected. Word endings "-a" or "-e" will (even if they are the endings of a preceding suffix) change to "-á" and "-é", respectively, before "-t". E.g.: f"a" (tree) -> fát. The long vowel of a one-syllable word may get shortened. E.g.: úr (lord) -> "u"rat. But: b"ú"r (Boer) -> b"ú"rt. If a word has more than one syllable and the last syllable ends in a consonant, the vowel of the last syllable may drop. E.g.: köröm (fingernail) -> "körmöt. But: kör"öm" ("my" circle) -> körömet". Notably, the first-person and second-person personal pronouns have quite unique accusative forms (indeed, as indicated in the table, in the singular case the ending "-et" is rather optional, even considered archaic).
Semitic languages.
An ending for accusative case existed in Proto-Semitic, Akkadian, and Ugaritic, but today it is preserved only in literary Arabic and Ge'ez.
Classical Arabic.
In Arabic, the accusative case (also the subjunctive mood) is called النصب "an-naṣb", and a word in the accusative case (also a verb in the subjunctive) is called المنصوب "al-manṣūb", both from the verb نصب "naṣaba" "set up". The accusative is used to mark the object of a verb and to form adverbs.

</doc>
<doc id="2284" url="http://en.wikipedia.org/wiki?curid=2284" title="Assault gun">
Assault gun

An assault gun is a gun or howitzer mounted on a motor vehicle or armored chassis, designed for use in the direct fire role in support of infantry when attacking other infantry or fortified positions. The term is a literal translation of the German word "Sturmgeschütz". Germany introduced the first purpose-built assault gun, the StuG III, in the late 1930s thus establishing this category of armoured vehicles.
Historically, the custom-built fully armored assault guns usually mounted the gun or howitzer in a fully enclosed casemate on a tank chassis. The use of a casemate instead of a turret limited these weapons' field of fire, but allowed a larger gun to be fitted relative to the chassis, more armor to be fitted for the same weight, and provided a cheaper construction. In most cases, these turretless vehicles also presented a lower profile as a target for the enemy.
The assault gun looks and works in the same way as the similar tank destroyer, the only difference in most cases being the gun. Assault guns generally used larger calibre, lower velocity guns, with their primary ammunition being that of high-explosive shells; these were meant for taking out soft targets as outlined in its infantry support role. This was contrasted with the tank destroyer, which utilized higher velocity, and therefore oftentimes smaller calibre guns, firing armour-piercing shells as their primary ammunition. These vehicles thus oftentimes sacrificed being able to fire a good high explosive shell in exchange for maximal armour penetration characteristics. Towards the beginning of the war, a single vehicle could generally be used in both roles, but that changed as the classes became increasingly specialized as the war progressed.
History.
World War II.
Assault guns were primarily used during World War II by the forces of Nazi Germany and the Soviet Union. Early in the war, the Germans began to create makeshift assault guns by mounting their infantry support weapons on the bed of a truck or on obsolete tanks with the turret removed. Later in the war, both the Germans and the Soviets introduced fully armored purpose-built assault guns into their arsenals.
Early on, the Soviets built the KV-2, a variant of the KV-1 heavy tank with a short-barreled 152 mm howitzer mounted in an oversized turret. This was not a success in battle, and was replaced with a very successful series of increasingly powerful turretless assault guns: the SU-76, SU-122, and the heavy SU-152, which were followed by the ISU-122 and ISU-152 on the new IS heavy tank chassis.
The primary German assault gun was the Sturmgeschütz III (StuG III). Late production StuG III variants, armed with a high-velocity dual-purpose 75mm gun blurred the line between assault guns and tank destroyers and was the Wehrmacht's most-produced armored fighting vehicle, at some 9,400 examples. The Germans also built a number of other fully armored turretless assault guns, including the StuG IV, Brummbär and Sturmtiger. The latter two were very heavy vehicles and were built only in small quantities.
Battalions of assault guns, usually StuG IIIs, commonly replaced the intended panzer battalion in the German panzergrenadier divisions due to the chronic shortage of tanks, and were sometimes used as makeshifts even in the panzer divisions. Independent battalions were also deployed as 'stiffeners' for infantry divisions, and the StuG III's anti-tank capabilities bolstered dwindling tank numbers on the Eastern and Western fronts.
American and British forces also deployed vehicles designed for a close support role, but these were conventional tanks whose only significant modification was the replacement of the main gun with a howitzer. Two versions of the American Sherman tank were armed with the M4 105 mm howitzer, the M4(105) and the M4A3(105). The Churchill, Centaur and Cromwell tanks were all produced in versions armed with 95 mm howitzers: the Churchill Mark V and Mark VIII, the Centaur Mark IV and the Cromwell Mark VI. Earlier British tanks, such as the Crusader cruiser tank and the Matilda II Infantry tank were produced in versions armed with the 3-inch howitzer, the first versions of the Churchill tank also had this gun in a hull mounting. As the amount of German armour encountered by the Allies decreased, especially in Italy, a number of American tank destroyer units were used in the assault gun role for infantry support.
The AVRE version of the Churchill Tank was armed with a Spigot mortar that fired a 40 lb HE-filled projectile (nicknamed the "Flying Dustbin") 150 yd. Its task was to attack fortified positions such as bunkers at close range (see Hobart's Funnies).
Postwar use.
In the post-WWII era, vehicles fitting into an "assault gun" category were developed as a light-weight, air-deployable, direct fire weapon for use with airborne troops. Current weapons were either based on jeeps or small tracked vehicles and the airborne troops thus always fought at a distinct disadvantage in terms of heavy weapons. The Soviet Union and the United States were the most attracted to the idea of providing this capability to traditionally light airborne forces. Their answers to the problem were similar, with the United States developing the M56 Scorpion and the Soviet Union developing the ASU-57, both essentially air-droppable light anti-tank guns.
The Soviets went on to develop an improved air-droppable assault gun, the ASU-85, which served through the 1980s, while their SU-100 remained in service with Communist countries, including Vietnam and Cuba, years after WW2. The US M56 and another armored vehicle, the M50 Ontos, were to be the last of the more traditional assault guns in US service. Improvised arrangements such as M113 personnel carriers with recoilless rifles were quickly replaced by missile carrier vehicles in the anti-tank role.
The only vehicle with the qualities of an assault gun to be fielded after the removal of the M50 and M56 from service within the US military was the M551 Sheridan. The Sheridan's gun was a low-velocity weapon suitable in the assault role, but with the addition of the Shillelagh missile could double in the anti-tank role as well. The Sheridan, however, was not developed as an assault gun but as a light reconnaissance vehicle.
Currently there appears to be a move toward wheeled vehicles fitting a "tank destroyer" or "assault gun" role, such as the M1128 Mobile Gun System of the US Army, the Centauro Wheeled Tank Destroyer of the Italian and Spanish Armies, the Chinese anti-tank gun PTL-02 and the French AMX 10 RC heavy armored car. While these vehicles might be useful in a direct fire role, none were developed with this specifically in mind, reminiscent of the use of tank destroyers by the US military in the assault gun role during WWII.

</doc>
<doc id="2288" url="http://en.wikipedia.org/wiki?curid=2288" title="Self-propelled anti-aircraft weapon">
Self-propelled anti-aircraft weapon

An anti-aircraft vehicle, also known as a self-propelled anti-aircraft gun (SPAAG) or self-propelled air defense system (SPAD), is a mobile vehicle with a dedicated anti-aircraft capability. The Russian equivalent of "SPAAG" is "ZSU", for "zenitnaya samokhodnaya ustanovka", ("anti-aircraft self-propelled mount").
Specific weapon systems used include machine guns, autocannons, larger guns, or missiles, and some mount both guns and longer-ranged missiles (Pantsir-S1). Platforms used include both trucks and heavier combat vehicles such as APCs and tanks, which add protection from aircraft, artillery, and small arms fire for front line deployment.
Anti-aircraft guns are usually mounted in a quickly-traversing turret with a high rate of elevation, for tracking fast-moving aircraft. They are often in dual or quadruple mounts, allowing a high rate of fire. Today, missiles (generally mounted on similar turrets) have largely supplanted anti-aircraft guns.
History.
World War I.
Anti-aircraft machine guns have long been mounted on trucks, and these were quite common during World War I. A predecessor of the WW2 German "88" anti-aircraft gun, the WWI German 77 mm anti-aircraft gun, was truck-mounted and used to great effect against British tanks.
The British QF 3 inch 20 cwt was mounted on trucks for use on the Western Front.
Inter-war period.
Between the two World Wars the United Kingdom developed the Birch gun, a general purpose artillery piece on an armoured tracked chassis capable of maintaining formation with their current tanks. The gun could be elevated for anti-aircraft use.
Vickers Armstrong also developed a SPAAG based on the chassis of the Mk.E 6-ton light tank/Dragon Medium Mark IV tractor, mounting a Vickers QF-1 "Pom-Pom" gun of 40 mm. About 26 were sold to Siam and saw action as Infantry support guns and AA guns during the Franco-Thai war (1940-1941) along with 30 Vickers Mk.E Type B 6-ton tanks. This was probably the first tracked SPAAG manufactured in series. Later the British also developed a version of the Mk.VI light tank armed with 4 machine guns that was known as Light Tank AA Mk.I. And also a twin 15 mm version based on the Light Tank Mk.V was built.
Among early pre-war pioneers of self-propelled AA guns were the Germans. By the time of the war, they fielded the SdKfz 10/4 and 6/2, cargo halftracks mounting single 20 mm or 37 mm AA guns (respectively). Later in the war similar German halftracks mounted quadruple 20 mm weapons.
World War II.
Larger guns followed on larger trucks, but these mountings generally required off-truck setup in order to unlimber the stabilizing legs these guns needed. One exception to this rule was the Italian Cannone da 90/53 which was highly effective when mounted on trucks, a fit known as the "autocannoni da 90/53". The 90/53 was a feared weapon, notably in the anti-tank role, but only a few hundred had been produced by the time of the armistice in 1943.
Other nations tended to work on truck chassis. Starting in 1941, the British developed the "en portee" method of mounting an anti-tank gun (initially a 2 pounder) on a truck. This was to prevent the weapon from being damaged by long-distance towing across rough, stony deserts, and it was intended only to be a carrying method, with the gun unloaded for firing. However, crews tended to fire their weapons from their vehicles for the mobility this method provided, with consequent casualties. 
This undoubtedly inspired their Morris C9/B (officially the "Carrier, SP, 4x4, 40 mm AA"), a Bofors 40 mm AA gun mounted on a chassis derived from the Morris "Quad" Field Artillery Tractor truck. Similar types, based on 3-ton lorries, were produced in Britain, Canada and Australia, and together formed the most numerous self-propelled AA guns in British service.
The U.S. Army brought truck-towed Bofors 40 mm AA guns along with truck-mounted units fitted with mechanized turrets when they sailed, first for Great Britain and then onto France. The turrets carried four .50 inch (12.7 mm) machine guns, which were designed to be adjusted to converge at the single point where enemy aircraft were expected to appear at low altitude in conduction of strafing runs directed at large infantry and field artillery units.
Interest in mobile AA turned to heavier vehicles with the mass and stability needed to easily train weapons of all sizes. Probably the desire, particularly in German service, for anti-aircraft vehicles to be armoured for their own protection also assisted this trend.
The concept of an armored SPAAG was pioneered by Hungary during World War II Hungary by producing the 40M Nimrod based on the Luftvärnskanonvagn L-62 Anti II license acquired from Sweden. Germany followed later with their "Flakpanzer" series. German World War II SPAAGs include the Möbelwagen, Wirbelwind, Ostwind and Kugelblitz. Other forces followed with designs of their own, notably the American M16 created by mounting quadruple M2 machine guns on a M3 Half-track.
The British developed their own SPAAGs throughout the war mounting multiple machine guns and light cannon on various tank and armoured car chassis and by 1943, the Crusader AA tanks, which mounted the Bofors 40 mm gun or two-three Oerlikon 20 mm cannon. Although used during the Normandy landings, by that point German aircraft were contained by the Allies own air forces and they were largely unneeded.
Cold War and later.
The introduction of jet engines and the subsequent rough doubling of aircraft speeds greatly reduced the effectiveness of the SPAAG against attack aircraft. A typical SPAAG round might have a muzzle velocity on the order of 1000 m/s and might take as long as two to three seconds to reach a target at its maximum range. An aircraft flying at 1000 km/h is moving at a rate of about 280 m/s. This means the aircraft will have moved hundreds of meters during the flight time of the shells, greatly complicating the aiming problem to the point where close passes were essentially impossible to aim using manual gunsights. This speed also allowed the aircraft to rapidly fly out of range of the guns; even if the aircraft passes directly over the SPAAG, it would be within its firing radius for under 30 seconds.
SPAAG development continued through the early 1950s with ever-larger guns, improving the range and allowing the engagement to take place at longer distances where the crossing angle was smaller and aiming was easier. Examples including the 40 mm U.S. M42 Duster and the 57 mm Soviet ZSU-57-2. However, both were essentially obsolete before they entered service, and found employment solely in the ground-support role. The M42 was introduced to the Vietnam War to counter an expected North Vietnamese air offensive, but when this failed to materialize it was used as an effective direct-fire weapon. The ZSU-57 found similar use in the Yugoslav Wars, where its high-angle fire was useful in the mountainous terrain.
By the late 1950s the US Army had given up on the SPAAG concept, considering all gun-based weapons to be useless against modern aircraft. This belief was generally held by many forces, and the anti-aircraft role turned almost exclusively to missile systems. The Soviet Union remained an outlier, beginning development of a new SPAAG in 1957, which emerged as the ZSU-23-4 in 1965. This system included search-and-track radars, fire control, and automatic gun-laying, greatly increasing its effectiveness against modern targets. The ZSU-23 proved very effective when used in concert with SAMs; the presence of SAMs forced aircraft to fly low to avoid their radars, placing them within range of the ZSUs.
The success of the ZSU-23 led to a resurgence of SPAAG development. This was also prompted by the introduction of attack helicopters in the 1970s, which could hide behind terrain and then "pop up" for an attack lasting only a few tens of seconds; missiles were ineffective at low altitudes, while the helicopters would often be within range of the guns for a rapid counterattack. Notable among these later systems is the German Gepard, the first western SPAAG to offer performance equal to or better than the ZSU. This system was widely copied in various NATO forces. US attempts to introduce a new SPAAG were doomed to become a series of half-measures and dismal failures; the failure of the missile-based MIM-46 Mauler led to the introduction of the optically-aimed M163 VADS of very short range, and the long-delayed and finally cancelled M247 Sergeant York which offered almost laughable performance in testing, unable to hit even stationary targets. 
SPAAG development continues, with many modern examples often combining both guns and short-range missiles. Examples include the Soviet/Russian Tunguska-M1, which supplanted the ZSU-23 in service, newer versions of the Gepard, and the British Marksman turret, which can be used on a wide variety of platforms. Other single-type examples include the South Korean K30 Biho and K263A1 radar-guided Vulcan, Chinese Type 95 SPAAA, Swedish CV9040 AAV, Polish PZA Loara, American M6 Bradley Linebacker and M1097 Humvee Avenger, Yugoslavian BOV-3, Canadian ADATS, Japanese Type 87 SPAAG, Italian SIDAM 25 and Otomatic, and versions of the French AMX-13, Turkish ACV-15 with Zıpkın PMADS.

</doc>
<doc id="2303" url="http://en.wikipedia.org/wiki?curid=2303" title="Aramaic language">
Aramaic language

Aramaic (Classical Syriac: ܐܪܡܝܐ "Arāmāyā") is a family of languages or dialects, belonging to the Semitic family. More specifically, it is a part of the Northwest Semitic subfamily, which also includes Canaanite languages such as Hebrew and Phoenician. The Aramaic script was widely adopted for other languages and is ancestral to both the Arabic and modern Hebrew alphabets.
During its approximately 3,100 years of written history, Aramaic has served variously as a language of administration of empires and as a language of divine worship. It was the lingua franca of the Neo-Assyrian Empire (911-605 BC), Neo-Babylonian Empire (605-539 BC) and Achaemenid Empire (539-323 BC), of the Neo-Assyrian states of Assur, Adiabene, Osroene and Hatra, the Aramean state of Palmyra, and the day-to-day language of Yehud Medinata and of Judaea (539 BC – 70 AD), the language that Jesus probably used the most, the language of large sections of the biblical books of Daniel and Ezra, and is the main language of the Talmud and Syriac Christianity, in particular the Assyrian Church of the East, the Nestorian Church, the Chaldean Catholic Church, the Ancient Church of the East, the Saint Thomas Christian Churches in India, the Syriac Orthodox Church, the Assyrian Pentecostal Church, and the Maronite Church. It is also the language of the Mandeans and their Gnostic religion, Mandeanism, as well as the language of the once widespread but now extinct Manichaean religion.
However, Jewish Aramaic was different from the other forms both in lettering and grammar. Parts of the Dead Sea Scrolls are in Jewish Aramaic showing the Jewish lettering, related to the Hebrew script. Aramaic was also the original language of the Bahrani people of Eastern Arabia.
Aramaic's long history and diverse and widespread use has led to the development of many divergent varieties, which are sometimes considered dialects, though they are distinct enough that they are sometimes considered languages. Therefore, there is not one singular, static Aramaic language; each time and place rather has had its own variation. Aramaic is retained as a liturgical language by certain Eastern Christian churches, in the form of Syriac, the Aramaic variety by which Eastern Christianity was diffused, whether or not those communities once spoke it or another form of Aramaic as their vernacular, but have since shifted to another language as their primary community language.
Modern Aramaic is spoken today as a first language by many scattered, predominantly small, and largely isolated communities of differing Christian, Jewish, and Mandean ethnic groups of West Asia—most numerously by the Assyrians in the form of Assyrian Neo-Aramaic and Chaldean Neo-Aramaic—that have all retained use of the once dominant lingua franca despite subsequent language shifts experienced throughout the Middle East. The Aramaic languages are now considered endangered.
Etymology.
The term "Aramaic", meaning the language of Arameans settling in the region of ancient , ארם or ܐܪܡ (ʾArām), derives from the Hebrew/Aramaic root verb רום (rum) meaning "to rise, be high, piled up, or tall".
"Aram" is used as a proper name of several people in the Torah (Hebrew Bible) including descendants of Shem (Genesis 10:22), Abraham (Genesis 22:21) and Jacob (1 Chronicles 7:34). "Ram" is another Biblical name of one of King David's ancestors (Ruth 4:19) also in use meaning "high, exalted or mighty." "Ram" also occurs as parts of names such as the second syllable of "Avram" (Founder of Judiasm and later called "Avraham" or "Abraham"), "Av" meaning "father" and "ram" meaning exalted.
Ancient , bordering Northern Israel and now called Syria, is considered the linguistic epicenter of Aramaic, the language of the Arameans who settled the area during the Bronze Age circa 3500 BC. There is some confusion about the origin of the language, often mistaken to have originated within Assyria (Iraq). In fact, Arameans carried their language and writing into Mesopotamia by voluntary migration, by forced exile of conquering armies, and by nomadic Chaldean invasions of Babylonia in 1200 BC to 1000 BC. Interestingly, the Christian primary text written in Koine Greek, New Testament, translates the word "Hebrew" as "Aramaic". Part of this confusion is attributed to the Greek naming "Aram" "Syria" (Συρια; Acts 15:41, Galatians 1:21), and at the same time calling "Assyria" (Iraq) "Syria".
Geographic distribution.
During the Neo-Assyrian and the Neo-Babylonian period, Aramaeans, the native speakers of Aramaic, began to settle in greater numbers, at first in Babylonia, and later in Assyria (Upper Mesopotamia, modern-day northern Iraq, northeast Syria, northwest Iran, and south eastern Turkey). The influx eventually resulted in the Neo-Assyrian Empire (911-605 BC) adopting an Akkadian-influenced Imperial Aramaic as the "lingua franca" of its empire. This policy was continued by the short-lived Neo-Babylonian Empire and Median Empire, and all three empires became operationally bilingual in written sources, with Aramaic used alongside Akkadian. The Persian Empire (539-323 BC) continued this tradition, and the extensive influence of these empires led to Aramaic gradually becoming the "lingua franca" of most of western Asia, the Arabian Peninsula, Asia Minor, the Caucasus, and Egypt. Aramaic writing has been found as far north as Hadrian's Wall in Ancient Britain, in the form of inscriptions in Aramaic, made by Assyrian and Aramean soldiers serving in the Roman Legions in northern England during the 2nd century AD.
From the late 7th century AD to the 14th century AD, Aramaic was gradually replaced as the "lingua franca" of the Middle East by Arabic. However, Aramaic remains a spoken, literary, and liturgical language among indigenous Assyrians, and also some Jews. It is spoken by the Assyrians of Iraq, northeastern Syria, southeastern Turkey, and northwest Iran, with diaspora communities in Armenia, Georgia, Azerbaijan and southern Russia. Mandaeans also continue to use Aramaic as a liturgical language, as most are now Arabic-speakers. There are still also a small number of native speakers of Western Aramaic in isolated villages in western Syria. The turbulence of the last two centuries (particularly the Assyrian Genocide) has seen speakers of first-language and literary Aramaic dispersed throughout the world. However, there are a number of sizable Assyrian towns in northern Iraq such as Alqosh, Bakhdida, Bartella, Tel Esqof, and Tel Keppe, and numerous small villages, where Aramaic is still the main spoken language, and many large cities in this region also have Assyrian Aramaic-speaking communities, particularly Mosul, Irbil, Kirkuk, Dohuk, and Hasakah.
Aramaic languages and dialects.
Aramaic is often spoken of as a single language. However, it is in reality a group of related languages, rather than a single monolithic language—something which it has never been. Some Aramaic languages differ more from each other than the Romance languages do among themselves. Its long history, extensive literature, and use by different religious communities are all factors in the diversification of the language. Some Aramaic dialects are mutually intelligible, whereas others are not, not unlike the situation with modern Varieties of Arabic. Some Aramaic languages are known under different names; for example, Syriac is particularly used to describe the Eastern Aramaic of indigenous Christian ethnic communities of Assyrians (a.k.a. Chaldo-Assyrians) in Iraq, southeastern Turkey, northeastern Syria, and northwestern Iran, and Saint Thomas Christians in India. Most dialects can be described as either "Eastern" or "Western", the dividing line being roughly the Euphrates, or slightly west of it. It is also helpful to draw a distinction between those Aramaic languages that are modern living languages (often called "Neo-Aramaic"), those that are still in use as literary languages, and those that are extinct and are only of interest to scholars. Although there are some exceptions to this rule, this classification gives "Modern", "Middle", and "Old" periods, alongside "Eastern" and "Western" areas, to distinguish between the various languages and dialects that are Aramaic.
Writing system.
The earliest Aramaic alphabet was based on the Phoenician alphabet. In time, Aramaic developed its distinctive "square" style. The ancient Israelites and other peoples of Canaan adopted this alphabet for writing their own languages. Thus, it is better known as the Hebrew alphabet today. This is the writing system used in Biblical Aramaic and other Jewish writing in Aramaic. The other main writing system used for Aramaic was developed by Christian communities: a cursive form known as the Syriac alphabet. A highly modified form of the Aramaic alphabet, the Mandaic alphabet, is used by the Mandaeans.
In addition to these writing systems, certain derivatives of the Aramaic alphabet were used in ancient times by particular groups: Nabataean in Petra, for instance and Palmyrenean in Palmyra. In modern times, Turoyo (see below) has sometimes been written in a Latin alphabet.
History.
The history of Aramaic is broken down into three broad periods:
This classification is based on that used by Klaus Beyer*.
Old Aramaic.
The term "Old Aramaic" is used to describe the varieties of the language from its first known use until the point roughly marked by the rise of the Sasanian Empire (224 CE), dominating the influential, eastern dialect region. As such, the term covers over thirteen centuries of the development of Aramaic. This vast time span includes all Aramaic that is now effectively extinct.
The central phase in the development of Old Aramaic was its official use by the Achaemenid Empire (500–330 BCE). The period before this, dubbed "Ancient Aramaic", saw the development of the language from being spoken in Aramaean city-states to become a major means of communication in diplomacy and trade throughout Mesopotamia, the Levant and Egypt. After the fall of the Achaemenid Empire, local vernaculars became increasingly prominent, fanning the divergence of an Aramaic dialect continuum and the development of differing written standards.
Ancient Aramaic.
"Ancient Aramaic" refers to the earliest known period of the language, from its origin until it becomes the "lingua franca" of the Fertile Crescent. It was the language of the Aramaean city-states of Damascus, Hamath and Arpad.
There are inscriptions that evidence the earliest use of the language, dating from the 10th century BC. These inscriptions are mostly diplomatic documents between Aramaean city-states. The alphabet of Aramaic at this early period seems to be based on Phoenician, and there is a unity in the written language. It seems that, in time, a more refined alphabet, suited to the needs of the language, began to develop from this in the eastern regions of Aram. Oddly, the dominance of the Neo-Assyrian Empire under Tiglath-Pileser III over Aram in the middle of the 8th century led to the establishment of Aramaic as a lingua franca of the empire, rather than it being eclipsed by Akkadian.
From 700 BC, the language began to spread in all directions, but lost much of its homogeneity. Different dialects emerged in Assyria, Babylonia, the Levant and Egypt. However, the Akkadian-influenced Aramaic of Assyria, and then Babylon, started to come to the fore. As described in 2 Kings 18:26, Hezekiah, king of Judah, negotiates with Assyrian ambassadors in Aramaic, the author claiming this was so that the common people would not understand. Around 600 BC, Adon, a Canaanite king, used Aramaic to write to an Egyptian Pharaoh.
"Chaldee" or "Chaldean Aramaic" used to be common terms for the Aramaic of the Chaldean dynasty of Babylonia. It was used to describe Biblical Aramaic, which was, however, written in a later style. It is not to be confused with the modern language Chaldean Neo-Aramaic.
Imperial Aramaic.
Around 500 BC, following the Achaemenid conquest of Mesopotamia under Darius I, Aramaic (as had been used in that region) was adopted by the conquerors as the "vehicle for written communication between the different regions of the vast empire with its different peoples and languages. The use of a single official language, which modern scholarship has dubbed Official Aramaic or Imperial Aramaic, can be assumed to have greatly contributed to the astonishing success of the Achaemenids in holding their far-flung empire together for as long as they did". In 1955, Richard Frye questioned the classification of Imperial Aramaic as an "official language", noting that no surviving edict expressly and unambiguously accorded that status to any particular language. Frye reclassifies Imperial Aramaic as the "lingua franca" of the Achaemenid territories, suggesting then that the Achaemenid-era use of Aramaic was more pervasive than generally thought.
Imperial Aramaic was highly standardised; its orthography was based more on historical roots than any spoken dialect, and the inevitable influence of Persian gave the language a new clarity and robust flexibility. For centuries after the fall of the Achaemenid Empire (in 331 BC), Imperial Aramaic – or near enough for it to be recognisable – would remain an influence on the various native Iranian languages. Aramaic script and – as ideograms – Aramaic vocabulary would survive as the essential characteristics of the Pahlavi writing system.
One of the largest collections of Imperial Aramaic texts is that of the Persepolis fortification tablets, which number about five hundred. Many of the extant documents witnessing to this form of Aramaic come from Egypt, and Elephantine in particular (see Elephantine papyri). Of them, the best known is the "Wisdom of Ahiqar", a book of instructive aphorisms quite similar in style to the biblical book of Proverbs. Achaemenid Aramaic is sufficiently uniform that it is often difficult to know where any particular example of the language was written. Only careful examination reveals the occasional loan word from a local language.
A group of thirty Aramaic documents from Bactria have been discovered, and an analysis was published in November 2006. The texts, which were rendered on leather, reflect the use of Aramaic in the 4th century BC Achaemenid administration of Bactria and Sogdiana.
Post-Achaemenid Aramaic.
The conquest by Alexander the Great did not destroy the unity of Aramaic language and literature immediately. Aramaic that bears a relatively close resemblance to that of the 5th century BC can be found right up to the early 2nd century BCE. The Seleucids imposed Greek in the administration of Syria and Mesopotamia from the start of their rule. In the 3rd century BCE, Greek overtook Aramaic as the common language in Egypt and Syria. However, a post-Achaemenid Aramaic continued to flourish from Judaea, Assyria, Mesopotamia, through the Syrian Desert and into northern Arabia and Parthia.
Biblical Aramaic is the Aramaic found in four discrete sections of the Hebrew Bible:
Biblical Aramaic is a somewhat hybrid dialect. It is theorized that some Biblical Aramaic material originated in both Babylonia and Judaea before the fall of the Achaemenid dynasty. According to historical criticism, defiant Jewish propaganda shaped Aramaic Daniel during Seleucid rule. These stories might have existed as oral traditions at their earliest stage. This might be one factor that led to differing collections of Daniel in the Greek Septuagint and the Masoretic Text, which presents a lightly Hebrew-influenced Aramaic.
Under the category of post-Achaemenid is Hasmonaean Aramaic, the official language of Hasmonaean Judaea (142–37 BC). It influenced the Biblical Aramaic of the Qumran texts, and was the main language of non-biblical theological texts of that community. The major Targums, translations of the Hebrew Bible into Aramaic, were originally composed in Hasmonaean. Hasmonaean also appears in quotations in the Mishnah and Tosefta, although smoothed into its later context. It is written quite differently from Achaemenid Aramaic; there is an emphasis on writing as words are pronounced rather than using etymological forms.
Babylonian Targumic is the later post-Achaemenid dialect found in the Targum Onqelos and Targum Jonathan, the "official" targums. The original, Hasmonaean targums had reached Babylon sometime in the 2nd or 3rd century AD. They were then reworked according to the contemporary dialect of Babylon to create the language of the standard targums. This combination formed the basis of Babylonian Jewish literature for centuries to follow.
Galilean Targumic is similar to Babylonian Targumic. It is the mixing of literary Hasmonaean with the dialect of Galilee. The Hasmonaean targums reached Galilee in the 2nd century AD, and were reworked into this Galilean dialect for local use. The Galilean Targum was not considered an authoritative work by other communities, and documentary evidence shows that its text was amended. From the 11th century CE onwards, once the Babylonian Targum had become normative, the Galilean version became heavily influenced by it.
Babylonian Documentary Aramaic is a dialect in use from the 3rd century CE onwards. It is the dialect of Babylonian private documents, and, from the 12th century, all Jewish private documents are in Aramaic. It is based on Hasmonaean with very few changes. This was perhaps because many of the documents in BDA are legal documents, the language in them had to be sensible throughout the Jewish community from the start, and Hasmonaean was the old standard.
Nabataean Aramaic is the language of the Arameo-Arab kingdom of Petra. The kingdom ("c." 200 BC–106 AD) covered the east bank of the Jordan River, the Sinai Peninsula and northern Arabia. Perhaps because of the importance of the caravan trade, the Nabataeans began to use Aramaic in preference to Old North Arabic. The dialect is based on Achaemenid with a little influence from Arabic: "l" is often turned into "n", and there are a few Arabic loanwords. Some Nabataean Aramaic inscriptions exist from the early days of the kingdom, but most are from the first four centuries AD The language is written in a cursive script that is the precursor to the modern Arabic alphabet. The number of Arabic loanwords increases through the centuries, until, in the 4th century, Nabataean merges seamlessly with Arabic.
Palmyrene Aramaic is the dialect that was in use in the Syriac city state of Palmyra in the Syrian Desert from 44 BC to 274 AD. It was written in a rounded script, which later gave way to cursive Estrangela. Like Nabataean, Palmyrene was influenced by Arabic, but to a much lesser degree.
Arsacid Aramaic, that in use during the Arsacid empire (247 BC – 224 AD), represents a continuation of Achaemenid Aramaic, widely spoken throughout the west of the empire. Aramaic continued as the scribal basis for Pahlavi as it developed for the needs of Parthian: using an Aramaic-derived script and incorporating many heterograms, or Aramaic words meant to be read as Parthian ones. The Arsacids saw themselves as a continuation of Achaemenid rule, and so Arsacid Aramaic, more than any other post-Achaemenid dialect, continued the tradition of the chancery of Darius I. Over time, however, it came under the influence of contemporary, spoken Aramaic, Georgian and Persian. After the conquest of the Parthians by the Persian-speaking Sassanids, Arsacid Pahlavi and Aramaic were influential on Sasanian language use.
Late Old Eastern Aramaic.
The dialects mentioned in the last section were all descended from Achaemenid Imperial Aramaic. However, the diverse regional dialects of Late Ancient Aramaic continued alongside these, often as simple, spoken languages. Early evidence for these spoken dialects is known only through their influence on words and names in a more standard dialect. However, these regional dialects became written languages in the 2nd century BC. These dialects reflect a stream of Aramaic that is not dependent on Imperial Aramaic, and shows a clear division between the regions of Mesopotamia, Babylon and the east, and Judah, Syria, and the west.
In the East, the dialects of Palmyrene and Arsacid Aramaic merged with the regional languages to create languages with a foot in Imperial and a foot in regional Aramaic. The written form of Mandaic, the language of the Mandaean religion, was descended from the Arsacid chancery script.
In the kingdom of Osroene, centred on Edessa and founded in 132 BCE, the regional dialect became the official language: Old Syriac. On the upper reaches of the Tigris, East Mesopotamian Aramaic flourished, with evidence from Hatra, Assur and the Tur Abdin. Tatian, the author of the gospel harmony the Diatessaron came from Assyria, and perhaps wrote his work (172 CE) in East Mesopotamian rather than Syriac or Greek. In Babylonia, the regional dialect was used by the Jewish community, Jewish Old Babylonian (from "c." 70 CE). This everyday language increasingly came under the influence of Biblical Aramaic and Babylonian Targumic.
Late Old Western Aramaic.
The western regional dialects of Aramaic followed a similar course to those of the east. They are quite distinct from the eastern dialects and Imperial Aramaic. Aramaic came to coexist with Canaanite dialects, eventually completely displacing Phoenician in the 1st century BCE and Hebrew around the turn of the 4th century CE.
The form of Late Old Western Aramaic used by the Jewish community is best attested, and is usually referred to as Jewish Old Palestinian. Its oldest form is Old East Jordanian, which probably comes from the region of Caesarea Philippi. This is the dialect of the oldest manuscript of Enoch ("c." 170 BCE). The next distinct phase of the language is called Old Judaean into the 2nd century CE. Old Judaean literature can be found in various inscriptions and personal letters, preserved quotations in the Talmud and receipts from Qumran. Josephus' first, non-extant edition of his "Jewish War" was written in Old Judaean.
The Old East Jordanian dialect continued to be used into the 1st century AD by pagan communities living to the east of the Jordan. Their dialect is often then called Pagan Old Palestinian, and it was written in a cursive script somewhat similar to that used for Old Syriac. A Christian Old Palestinian dialect may have arisen from the pagan one, and this dialect may be behind some of the Western Aramaic tendencies found in the otherwise eastern Old Syriac gospels (see Peshitta).
Languages during Jesus' lifetime.
It is generally believed by Christian scholars that in the 1st century CE, Jews in Judaea primarily spoke Aramaic with a decreasing number using Hebrew as a native language. Many learned Hebrew as a liturgical language. Additionally, Koine Greek was the lingua franca or international language of the Middle East in trade, among the Hellenized classes (much like French in the 18th,19th and 20th centuries in Europe), and in the Roman administration. Latin, the language of the Roman army and higher levels of administration, had almost no impact on the linguistic landscape.
In addition to the formal, literary dialects of Aramaic based on Hasmonaean and Babylonian there were a number of colloquial Aramaic dialects. Seven dialects of Western Aramaic were spoken in the vicinity of Judaea in Jesus' time. They were probably distinctive yet mutually intelligible. Old Judaean was the prominent dialect of Jerusalem and Judaea. The region of Engedi had the South-east Judaean dialect. Samaria had its distinctive Samaritan Aramaic, where the consonants "he", "heth" and "‘ayin" all became pronounced as "aleph". Galilean Aramaic, the dialect of Jesus' home region, is only known from a few place names, the influences on Galilean Targumic, some rabbinic literature and a few private letters. It seems to have a number of distinctive features: diphthongs are never simplified into monophthongs. East of the Jordan, the various dialects of East Jordanian were spoken. In the region of Damascus and the Anti-Lebanon mountains, Damascene Aramaic was spoken (deduced mostly from Modern Western Aramaic). Finally, as far north as Aleppo, the western dialect of Orontes Aramaic was spoken.
The three languages influenced one another, especially Hebrew and Aramaic. Hebrew words entered Jewish Aramaic (mostly technical religious words but also everyday words like "‘ēṣ" "wood"). Vice versa, Aramaic words entered Hebrew (not only Aramaic words like "māmmôn" "wealth" but Aramaic ways of using words like making Hebrew "rā’ûi", "seen" mean "worthy" in the sense of "seemly", which is a loan translation of Aramaic "ḥāzê" meaning "seen" and "worthy").
The Greek of the New Testament often preserves non-Greek "semiticisms", including transliterations of Semitic words:
The 2004 film "The Passion of the Christ" used Aramaic for much of its dialogue, specially reconstructed by a scholar, William Fulco, S.J. Where the appropriate words (in 1st century Aramaic) were no longer known, he used the Aramaic of Daniel, 4th-century Syriac and Hebrew as the basis for his work.
Middle Aramaic.
The 3rd century CE is taken as the threshold between Old and Middle Aramaic. During that century, the nature of the various Aramaic languages and dialects begins to change. The descendants of Imperial Aramaic ceased to be living languages, and the eastern and western regional languages began to form vital, new literatures. Unlike many of the dialects of Old Aramaic, much is known about the vocabulary and grammar of Middle Aramaic.
Eastern Middle Aramaic.
Only two of the Old Eastern Aramaic languages continued into this period. In the north of the region, Old Syriac moved into Middle Syriac. In the south, Jewish Old Babylonian became Jewish Middle Babylonian. The post-Achaemenid, Arsacid dialect became the background of the new Mandaic language.
Syriac.
Syriac (also "Middle Syriac") is the classical, literary, liturgical and often spoken language of Syriac Christians to this day, particularly the Assyrian church of the East, Chaldean Catholic Church, Ancient Church of the East, Syriac Orthodox and Saint Thomas Christian churches. It originated in Northern Mesopotamia. Its golden age was the 4th to 6th centuries. This period began with the translation of the Bible into the language: the Peshitta and the masterful prose and poetry of Ephrem the Syrian. Middle Syriac, unlike its forebear, is a thoroughly Christian language, although in time it became the language of those opposed to the Byzantine leadership of the Church of the East. Missionary activity by Assyrian and Nestorian Christians led to the spread of Syriac from Mesopotamia through Persia and into Central Asia, India and China.
Jewish Middle Babylonian Aramaic.
Jewish Middle Babylonian is the language employed by Jewish writers in Babylonia between the 4th century and the 11th century AD. It is most commonly identified with the language of the Babylonian Talmud (which was completed in the 7th century) and of post-Talmudic (Geonic) literature, which are the most important cultural products of Babylonian Jewry. The most important epigraphic sources for the dialect are the hundreds of Aramaic magic bowls written in the Jewish script.
Mandaic.
Mandaic, spoken by the Mandeans of Iraq, is a sister dialect to Jewish Babylonian Aramaic, though it is both linguistically and culturally distinct. Classical Mandaic is the language in which the Mandaean's Gnostic religious literature was composed. It is characterized by a highly phonetic orthography.
Western Middle Aramaic.
The dialects of Old Western Aramaic continued with Jewish Middle Palestinian (in Hebrew "square script"), Samaritan Aramaic (in the old Hebrew script) and Christian Palestinian (in cursive Syriac script). Of these three, only Jewish Middle Palestinian continued as a written language.
Jewish Middle Palestinian Aramaic.
In 135, after the Bar Kokhba revolt, many Jewish leaders, expelled from Jerusalem, moved to Galilee. The Galilean dialect thus rose from obscurity to become the standard among Jews in the west. This dialect was spoken not only in Galilee, but also in the surrounding parts. It is the linguistic setting for the Jerusalem Talmud (completed in the 5th century), Palestinian targumim (Jewish Aramaic versions of scripture), and midrashim (biblical commentaries and teaching). The standard vowel pointing for the Hebrew Bible, the Tiberian system (7th century), was developed by speakers of the Galilean dialect of Jewish Middle Palestinian. Classical Hebrew vocalisation, therefore, in representing the Hebrew of this period, probably reflects the contemporary pronunciation of this Aramaic dialect.
Middle Judaean, the descendant of Old Judaean, is no longer the dominant dialect, and was used only in southern Judaea (the variant Engedi dialect continued throughout this period). Likewise, Middle East Jordanian continues as a minor dialect from Old East Jordanian. The inscriptions in the synagogue at Dura-Europos are either in Middle East Jordanian or Middle Judaean.
Samaritan Aramaic.
The Aramaic dialect of the Samaritan community is earliest attested by a documentary tradition that can be dated back to the 4th century. Its modern pronunciation is based on the form used in the 10th century.
Israelite Christian Aramaic.
The language of Western-Aramaic-speaking Christians is evidenced from the 6th century, but probably existed two centuries earlier. The language itself comes from Christian Old Palestinian, but its writing conventions were based on early Middle Syriac, and it was heavily influenced by Greek. For example, the name Jesus, although "Išo" in Aramaic, is written "Yesûs" in Christian Israelite.
Modern Aramaic.
Over 400,000 people of various communities from across the Middle East, and recent emigrants who have moved out of these communities, speak one of several varieties of Modern Aramaic (also called "Neo-Aramaic") natively, including Christians, Jews, Mandaeans and Muslims. Having lived in remote areas as insulated communities, the remaining modern speakers of Aramaic dialects escaped the linguistic pressures experienced by others during the large-scale language shifts that saw the proliferation of other tongues among those who previously did not speak them, most recently the Arabization of the Middle East and North Africa by Muslim Arabians, during their spread of Islam. Most of the people of that region who converted to Islam, and many from the remaining unconverted population, also adopted Arabic as their first language. The Aramaic-speaking peoples such as Assyrians have preserved their traditions with schools, printing presses and now with electronic media.
The Neo-Aramaic languages are now farther apart in their mutual intelligibility than perhaps they have ever been. Instability throughout the Middle East over the past century has led to a worldwide diaspora of Aramaic-speakers. For Aramaic-speaking Jews, 1950 is a watershed year: the founding of the state of Israel (1948) and consequent Jewish exodus from Arab lands, including Iraq, led most Iraqi Jews, both Aramaic-speaking and Arabic-speaking Iraqi Jews, to emigrate to Israel. However, immigration to Israel has led to the Jewish Neo-Aramaic (and Jewish Iraqi Arabic) being replaced by Modern Hebrew (Ivrit) among children of the migrants. The practical extinction of many Jewish dialects seems imminent.
Modern Eastern Aramaic.
Modern Eastern Aramaic exists in a wide variety of dialects and languages. There is significant difference between the Aramaic spoken by Jews, Chaldo-Assyrian Christians, and Mandaeans.
The Christian languages are often called Modern Syriac (or Neo-Syriac, particularly when referring to their literature), being deeply influenced by the literary and liturgical language of Middle Syriac. However, they also have roots in numerous, previously unwritten, local Aramaic varieties, and are not purely the direct descendants of the language of Ephrem the Syrian. The varieties are not all mutually intelligible. The principal Christian varieties are Assyrian Neo-Aramaic and Chaldean Neo-Aramaic used by the ethnic Assyrians of Iraq, south east Turkey, Iran and north east Syria.
The Judeo-Aramaic languages are now mostly spoken in Israel, and most are facing extinction. The Jewish varieties that have come from communities that once lived between Lake Urmia and Mosul are not all mutually intelligible. In some places, for example Urmia, Christians and Jews speak mutually unintelligible varieties of Modern Eastern Aramaic in the same place. In others, the Nineveh Plains around Mosul for example, the varieties of the two faith communities are similar enough to allow conversation.
Modern Western Syriac (also called Central Neo-Aramaic, being in between Western Neo-Aramaic and Eastern Neo-Syriac) is generally represented by Turoyo, the language of the Tur Abdin. A related language, Mlahsô, has recently become extinct.
Mandaeans, living in the Khūzestān Province of Iran and scattered throughout Iraq, speak Modern Mandaic. It is quite distinct from any other Aramaic variety.
Modern Central Aramaic.
Central Neo-Aramaic consists of Turoyo and the recently extinct Mlahsô.
Modern Western Aramaic.
Very little remains of Western Aramaic. It is still spoken in the villages of Ma'loula, Bakh`a and Jubb`adin on Syria's side of the Anti-Lebanon mountains, as well as by some people who migrated from these villages, to Damascus and other larger towns of Syria. All these speakers of Modern Western Aramaic are fluent in Arabic, which has now become the main language in these villages.
Sounds.
Each dialect of Aramaic has its own distinctive pronunciation, and it would not be feasible here to go into all these properties. Aramaic has a phonological palette of 25 to 40 distinct phonemes. Some modern Aramaic pronunciations lack the series of "emphatic" consonants, and some have borrowed from the inventories of surrounding languages, particularly Arabic, Azerbaijani, Kurdish, Persian and Turkish.
Vowels.
As with most Semitic languages, Aramaic can be thought of as having three basic sets of vowels:
These vowel groups are relatively stable, but the exact articulation of any individual is most dependent on its consonantal setting.
The open vowel is an open near-front unrounded vowel ("short" "a", somewhat like the first vowel in the English "batter", [a]). It usually has a back counterpart ("long" "a", like the "a" in "father", [ɑ], or even tending to the vowel in "caught", [ɔ]), and a front counterpart ("short" "e", like the vowel in "head", [ɛ]). There is much correspondence between these vowels between dialects. There is some evidence that Middle Babylonian dialects did not distinguish between the short "a" and short "e". In West Syriac dialects, and possibly Middle Galilean, the long "a" became the "o" sound. The open "e" and back "a" are often indicated in writing by the use of the letters "alaph" (a glottal stop) or "he" (like the English "h").
The close front vowel is the "long" "i" (like the vowel in "need", [i]). It has a slightly more open counterpart, the "long" "e", as in the final vowel of "café" ([e]). Both of these have shorter counterparts, which tend to be pronounced slightly more open. Thus, the short close "e" corresponds with the open "e" in some dialects. The close front vowels usually use the consonant "y" as a mater lectionis.
The close back vowel is the "long" "u" (like the vowel in "school", [u]). It has a more open counterpart, the "long" "o", like the vowel in "low" ([o]). There are shorter, and thus more open, counterparts to each of these, with the short close "o" sometimes corresponding with the long open "a". The close back vowels often use the consonant "w" to indicate their quality.
Two basic diphthongs exist: an open vowel followed by "y" ("ay"), and an open vowel followed by "w" ("aw"). These were originally full diphthongs, but many dialects have converted them to "e" and "o" respectively.
The so-called "emphatic" consonants (see the next section) cause all vowels to become mid-centralised.
Consonants.
The various alphabets used for writing Aramaic languages have twenty-two letters (all of which are consonants). Some of these letters, though, can stand for two or three different sounds (usually a plosive and a fricative at the same point of articulation). Aramaic classically uses a series of lightly contrasted plosives and fricatives:
Each member of a certain pair is written with the same letter of the alphabet in most writing systems (that is, "p" and "f" are written with the same letter), and are near allophones.
A distinguishing feature of Aramaic phonology (and that of Semitic languages in general) is the presence of "emphatic" consonants. These are consonants that are pronounced with the root of the tongue retracted, with varying degrees of pharyngealization and velarization. Using their alphabetic names, these emphatics are:
Ancient Aramaic may have had a larger series of emphatics, and some Neo-Aramaic languages definitely do. Not all dialects of Aramaic give these consonants their historic values.
Overlapping with the set of emphatics are the "guttural" consonants. They include Ḥêṯ and ʽAyn from the emphatic set, and add ʼĀlap̄ (a glottal stop) and Hê (as the English "h").
Aramaic classically has a set of four sibilants (Ancient Aramaic may have had six):
In addition to these sets, Aramaic has the nasal consonants "m" and "n", and the approximants "r" (usually an alveolar trill), "l", "y" and "w".
Historical sound changes.
Six broad features of sound change can be seen as dialect differentials:
Grammar.
As with other Semitic languages, Aramaic morphology (the way words are formed) is based on the triliteral root. The root consists of three consonants and has a basic meaning, for example, "k-t-b" has the meaning of 'writing'. This is then modified by the addition of vowels and other consonants to create different nuances of the basic meaning:
Nouns and adjectives.
Aramaic nouns and adjectives are inflected to show gender, number and state. The latter somewhat akin to case in Indo-European languages.
Aramaic has two grammatical genders, masculine and feminine. The feminine absolute singular is usually marked by the ending "-â", which is usually written with an aleph. Jewish varieties, however, often use he instead, following Hebrew orthography.
Nouns can be either singular or plural, but an additional "dual" number exists for nouns that usually come in pairs. The dual number gradually disappeared from Aramaic over time and has little influence in Middle and Modern Aramaic.
Aramaic nouns and adjectives can exist in one of three states; these states correspond in part to the role of cases in other languages.
Whereas other Northwest Semitic languages, like Hebrew, have the absolute and construct states, the emphatic/determined state is a unique feature to Aramaic. Case endings, as in Ugaritic, probably existed in a very early stage of the language, and glimpses of them can be seen in a few compounded proper names. However, as most were short final vowels, they were never written, and the few characteristic long vowels of the masculine plural accusative and genitive are not clearly evidenced in inscriptions. Often, the direct object is marked by a prefixed "l-" (the preposition "to") if it is definite.
Adjectives agree with their nouns in number and gender but agree in state only if attributive. Predicative adjectives are in the absolute state regardless of the state of their noun (a copula may or may not be written). Thus, an attributive adjective to an emphatic noun, as in the phrase "the good king", is written also in the emphatic state "malkâ ṭāḇâ"—king[emph.] good[emph.]. In comparison, the predicative adjective, as in the phrase "the king is good", is written in the absolute state "ṭāḇ malkâ"—good[abs.] king[emph.].
The final "-â" in a number of these suffixes is written with the letter aleph. However, some Jewish Aramaic texts employ the letter he for the feminine absolute singular. Likewise, some Jewish Aramaic texts employ the Hebrew masculine absolute singular suffix "-îm" instead of "-în". The masculine determined plural suffix, "-ayyâ", has an alternative version, "-ê". The alternative is sometimes called the "gentilic plural" for its prominent use in ethnonyms ("yəhûḏāyê", 'the Jews', for example). This alternative plural is written with the letter aleph, and came to be the only plural for nouns and adjectives of this type in Syriac and some other varieties of Aramaic. The masculine construct plural, "-ê", is written with yodh. In Syriac and some other variants this ending is diphthongized to "-ai".
Possessive phrases in Aramaic can either be made with the construct state or by linking two nouns with the relative particle "d[î]-". As use of the construct state almost disappears from the Middle Aramaic period on, the latter method became the main way of making possessive phrases.
For example, the various forms of possessive phrases (for "the handwriting of the queen") are:
In Modern Aramaic, the last form is by far the most common. In Biblical Aramaic, the last form is virtually absent.
Verbs.
The Aramaic verb has gradually evolved in time and place, varying between varieties of the language. Verb forms are marked for person (first, second or third), number (singular or plural), gender (masculine or feminine), tense (perfect or imperfect), mood (indicative, imperative, jussive or infinitive) and voice (active, reflexive or passive). Aramaic also employs a system of conjugations, or verbal stems, to mark intensive and extensive developments in the lexical meaning of verbs.
Aspectual tense.
Aramaic has two proper tenses: perfect and imperfect. These were originally aspectual, but developed into something more like a preterite and future. The perfect is unmarked, while the imperfect uses various preformatives that vary according to person, number and gender. In both tenses the third-person singular masculine is the unmarked form from which others are derived by addition of afformatives (and preformatives in the imperfect). In the chart below (on the root K-T-B, meaning "to write"), the first form given is the usual form in Imperial Aramaic, while the second is Classical Syriac.
Conjugations or verbal stems.
Like other Semitic languages, Aramaic employs a number of conjugations, or verbal stems, to extend the lexical coverage of verbs. The basic conjugation of the verb is called the "ground stem", or "G-stem". Following the tradition of mediaeval Arabic grammarians, it is more often called the Pə‘al (also written Pe‘al), using the form of the triliteral root P-‘-L, meaning "to do". This stem carries the basic lexical meaning of the verb.
By doubling of the second radical, or root letter, the D-stem or Pa‘‘el is formed. This is often an intensive development of the basic lexical meaning. For example, "qəṭal" means "he killed", whereas "qaṭṭel" means "he slew". The precise relationship in meaning between the two stems differs for every verb.
A preformative, which can be "ha-", "a-" or "ša-", creates the C-stem or variously the Hap̄‘el, Ap̄‘el or Šap̄‘el (also spelt Haph‘el, Aph‘el and Shaph‘el). This is often an extensive or causative development of the basic lexical meaning. For example, "ṭə‘â" means "he went astray", whereas "aṭ‘î" means "he deceived". The Šap̄‘el is the least common variant of the C-stem. Because this variant is standard in Akkadian, it is possible that its use in Aramaic represents loanwords from that language. The difference between the variants Hap̄‘el and Ap̄‘el appears to be the gradual dropping of the initial "h" sound in later Old Aramaic. This is noted by the respelling of the older he preformative with aleph.
These three conjugations are supplemented with three derived conjugations, produced by the preformative "hiṯ-" or "eṯ-". The loss of the initial "h" sound occurs similarly to that in the form above. These three derived stems are the Gt-stem, Hiṯpə‘el or Eṯpə‘el (also written Hithpe‘el or Ethpe‘el), the Dt-stem, Hiṯpa‘‘al or Eṯpa‘‘al (also written Hithpa‘‘al or Ethpa‘‘al), and the Ct-stem, Hiṯhap̄‘al, Ettap̄‘al, Hištap̄‘al or Eštap̄‘al (also written Hithhaph‘al, Ettaph‘al, Hishtaph‘al or Eshtaph‘al). Their meaning is usually reflexive, but later became passive. However, as with other conjugations, actual meaning differs from verb to verb.
Not all verbs utilise all of these conjugations, and, in some, the G-stem is not used. In the chart below (on the root K-T-B, meaning "to write"), the first form given is the usual form in Imperial Aramaic, while the second is Classical Syriac.
Aramaic also has two proper tenses: the perfect and the imperfect. In Imperial Aramaic, the participle began to be used for a historical present. Perhaps under influence from other languages, Middle Aramaic developed a system of composite tenses (combinations of forms of the verb with pronouns or an auxiliary verb), allowing for narrative that is more vivid. The syntax of Aramaic (the way sentences are put together) usually follows the order verb–subject–object (VSO). Imperial (Persian) Aramaic, however, tended to follow a S-O-V pattern (similar to Akkadian), which was the result of Persian syntactic influence.
Aramaic word processors.
The World's first Aramaic language word processing software was developed in 1986–1987 in Kuwait by information technology professional Sunil Sivanand (1953– ), who is now Managing Director and Chief Technology Architect at Acette. Sunil Sivanand did most of the character generation and programming work on a first generation, twin disk drive IBM Personal Computer. The project was sponsored by Daniel Benjamin, who was a patron of a group of individuals working worldwide to preserve and revive the Aramaic language.

</doc>
<doc id="2414" url="http://en.wikipedia.org/wiki?curid=2414" title="Arrangement">
Arrangement

In music, an arrangement is a musical reconceptualization of a previously composed work. It may differ from the original work by means of reharmonization, melodic paraphrasing, orchestration, or development of the formal structure. Arranging differs from orchestration in that the latter process is limited to the assignment of notes to instruments for performance by an orchestra, concert band, or other musical ensemble. Arranging "involves adding compositional techniques, such as new thematic material for introductions, transitions, or modulations, and endings... Arranging is the art of giving an existing melody musical variety".
Classical music.
Arrangements and transcriptions of classical and serious music go back to the early history of this genre. In particular, music written for the piano has frequently undergone this treatment. The suite of ten piano pieces "Pictures at an Exhibition", by Modest Mussorgsky, has been arranged over twenty times, notably by Maurice Ravel.
Due to his lack of expertise in orchestration, the American composer George Gershwin had his "Rhapsody in Blue" orchestrated and arranged by Ferde Grofé.
Popular music.
Popular music recordings often include parts for brass, string, and other instruments which were added by arrangers and not composed by the original songwriters. Popular music arrangements may also be considered to include new releases of existing songs with a new musical treatment. These changes can include alterations to tempo, meter, key, instrumentation, and other musical elements.
Well-known examples include Joe Cocker's version of The Beatles' With a Little Help from My Friends and Ike And Tina Turner's version of Creedence Clearwater Revival's Proud Mary. The American group Vanilla Fudge and British group Yes based their early careers on radical re-arrangements of contemporary hits. Bonnie Pointer performed disco and Motown-themed versions of "Heaven Must Have Sent You." Remixes, such as in dance music, can also be considered arrangements.
Though arrangers may contribute substantially to finished musical products, for copyright and royalty purposes, they usually hold no legal claim to their work.
Jazz.
Arrangements for small jazz combos are usually informal, minimal, and uncredited. Larger ensembles generally have had greater requirements for notated arrangements, though the early Count Basie big band is known for its many "head" arrangements, so called because they were worked out by the players themselves, memorized (in the player's "head"), and never written down. Most arrangements for big bands, however, were written down and credited to a specific arranger, as with arrangements by Sammy Nestico and Neal Hefti for Count Basie's later big bands.
Don Redman made innovations in jazz arranging as a part of Fletcher Henderson's orchestra in the 1920s. Redman's arrangements introduced a more intricate melodic presentation and "soli" performances for various sections of the big band. Benny Carter became Henderson's primary arranger in the early 30's, becoming known for his arranging abilities in addition to his previous recognition as a performer. Beginning in 1938, Billy Strayhorn became an arranger of great renown for the Duke Ellington orchestra. Jelly Roll Morton is sometimes considered the earliest jazz arranger. While he toured around the years 1912 to 1915, he wrote down parts to enable "pick-up" bands to perform his compositions.
Big band arrangements are informally called "charts". In the swing era they were usually either arrangements of popular songs or they were entirely new compositions. Duke Ellington's and Billy Strayhorn's arrangements for the Duke Ellington big band were usually new compositions, and some of Eddie Sauter's arrangements for the Benny Goodman band and Artie Shaw's arrangements for his own band were new compositions as well. It became more common to arrange sketchy jazz combo compositions for big band after the bop era.
After 1950, the big bands declined in number. However, several bands continued and arrangers provided renowned arrangements. Gil Evans wrote a number of large-ensemble arrangements in the late fifties and early sixties intended for recording sessions only. Other arrangers of note include Vic Schoen, Pete Rugolo, Oliver Nelson, Johnny Richards, Billy May, Thad Jones, Maria Schneider, Bob Brookmeyer, Lou Marini, Nelson Riddle, Ralph Burns, Billy Byers, Gordon Jenkins, Ray Conniff, Henry Mancini, Ray Reach, and Claus Ogerman.
In the 21st century, the Big Band arrangement has made a modest comeback. Gordon Goodwin, Roy Hargrove, and Christian McBride have all rolled out New Big Bands with both original compositions and new arrangements of standard tunes.
Arranging for instrumental groups.
Strings.
The string section is a body of instruments composed of various stringed instruments. By the 19th century orchestral music in Europe had standardized the string section into the following homogeneous instrumental groups: first violins, second violins, violas, cellos, and double basses. The string section in a multi-sectioned orchestra is referred sometimes to as the "string choir."
The harp is also a stringed instrument, but is not a member of or homogeneous with the violin family and is not considered part of the string choir. Samuel Adler classifies the harp as a plucked string instrument in the same category as the guitar (acoustic or electric), mandolin, banjo, or zither. Like the harp these instruments do not belong to the violin family and are not homogeneous with the string choir. In modern arranging these instruments are considered part of the rhythm section. The electric string bass and upright string bass—depending on the circumstance—can be treated by the arranger as either string section or rhythm section instruments.
A group of instruments in which each member plays a unique part—rather than playing in unison with other like instruments—is referred to as a chamber ensemble. A chamber ensemble made up entirely of strings of the violin family is referred to by its size. A string trio consists of three players, a string quartet four, a string quintet five, and so on.
In most circumstances the string section is treated by the arranger as one homogeneous unit and its members are required to play preconceived material rather than improvise.
A string section can be utilized on its own (this is referred to as a string orchestra) or in conjunction with any of the other instrumental sections. More than one string orchestra can be utilized.
A standard string section (vln., vln 2., vla., vcl, cb.) with each section playing unison allows the arranger to create a five-part texture. Often an arranger will divide each violin section in half or thirds to achieve a denser texture. It is possible to carry this division to its logical extreme in which each member of the string section plays his or her own unique part.
Size of the string section.
Artistic, budgetary and logistical concerns will determine the size and instrumentation of a string section. The Broadway musical West Side Story, in 1957, was booked into the Winter Garden theater; composer Leonard Bernstein disliked the playing of "house" viola players he would have to use there, and so he chose to leave them out of the show's instrumentation; a benefit was the creation of more space in the pit for an expanded percussion section.
George Martin, producer and arranger for The Beatles, warns arrangers about the intonation issues when only two like instruments play in unison. "After a string quartet," Martin explains, "I do not think there is a satisfactory sound for strings until one has at least three players on each line...as a rule two stringed instruments together create a slight "beat" which does not give a smooth sound."
While any combination and number of string instruments is possible in a section, a traditional string section sound is achieved with a violin-heavy balance of instruments.

</doc>
<doc id="2528" url="http://en.wikipedia.org/wiki?curid=2528" title="Adenylate cyclase">
Adenylate cyclase

Adenylate cyclase (EC , also commonly known as adenylyl cyclase, abbreviated AC) is an enzyme with key regulatory roles in essentially all cells. It is the most polyphyletic known enzyme: six distinct classes have been described, all catalyzing the same reaction but representing unrelated gene families with no known sequence or structural homology. The best known AC class is class III or AC-III (Roman numerals are used for classes). AC-III occurs widely in eukaryotes and has important roles in many human tissues.
All classes of AC catalyze the conversion of adenosine triphosphate (ATP) to 3',5'-cyclic AMP (cAMP) and pyrophosphate. Divalent cations, usually magnesium (Mg), are generally required and appear to be closely involved in the enzymatic mechanism. The cAMP produced by AC then serves as a regulatory signal via specific cAMP-binding proteins, either transcription factors or other enzymes (e.g., cAMP-dependent kinases).
Class I AC.
Class I AC's occur in many bacteria including "E. coli". This was the first class of AC to be characterized. It was observed that "E. coli" deprived of glucose produce cAMP that serves as an internal signal to activate expression of genes for importing and metabolizing other sugars. cAMP exerts this effect by binding the transcription factor CRP, also known as CAP. Class I AC's are large cytosolic enzymes (~100 kDa) with a large regulatory domain (~50 kDa) that indirectly senses glucose levels. s of 2012[ [update]], no crystal structure is available for class I AC.
Class II AC.
These AC's are toxins secreted by pathogenic bacteria such as "Bacillus anthracis" and "Bordetella pertussis" during infection. These bacteria also secrete proteins that enable the AC-II to enter host cells, where the exogenous AC activity undermines normal cellular processes. The genes for Class II AC's are known as cyaA. Several crystal structures are known for AC-II enzymes.
Class III AC.
These AC's are the most familiar based on extensive study due to their important roles in human health. They are also found in some bacteria, notably "Mycobacterium tuberculosis" where they appear to have a key role in pathogenesis. Most AC-III's are integral membrane proteins involved in transducing extracellular signals into intracellular responses. A Nobel Prize was awarded to Earl Sutherland in 1971 for discovering the key role of AC-III in human liver, where adrenaline indirectly stimulates AC to mobilize stored energy in the "fight or flight" response. The effect of adrenaline is via a G protein signaling cascade, which transmits chemical signals from outside the cell across the membrane to the inside of the cell (cytoplasm). The outside signal (in this case, adrenaline) binds to a receptor, which transmits a signal to the G protein, which transmits a signal to adenylate cyclase, which transmits a signal by converting adenosine triphosphate to cyclic adenosine monophosphate (cAMP). cAMP is known as a second messenger.
cAMP (cyclic adenosine monophosphate) is an important molecule in eukaryotic signal transduction, a so-called second messenger. Adenylate cyclases are often activated or inhibited by G proteins, which are coupled to membrane receptors and thus can respond to hormonal or other stimuli. Following activation of adenylate cyclase, the resulting cAMP acts as a second messenger by interacting with and regulating other proteins such as protein kinase A and cyclic nucleotide-gated ion channels.
Photoactivatable adenylate cyclase (PAC) was discovered in "E. gracilis" and can be expressed in other organisms through genetic manipulation. Shining blue light on a cell containing PAC activates it and abruptly increases the rate of conversion of ATP to cAMP. This is a useful technique for researchers in neuroscience because it allows them to quickly increase the intracellular cAMP levels in particular neurons, and to study the effect of that increase in neural activity on the behavior of the organism. For example, PAC expression in certain neurons has been shown to alter the grooming behavior in fruit flies exposed to blue light. Channelrhodopsin-2 is also used in a similar fashion.
AC-III structure.
Most class III adenylyl cyclases are transmembrane proteins with 12 transmembrane segments. The protein is organized with 6 transmembrane segments, then the C1 cytoplasmic domain, then another 6 membrane segments, and then a second cytoplasmic domain called C2. The important parts for function are the N-terminus and the C1 and C2 regions. The C1a and C2a subdomains are homologous and form an intramolecular 'dimer' that forms the active site. In "Mycobacterium tuberculosis", the AC-III polypeptide is only half as long, comprising one 6-transmembrane domain followed by a cytoplasmic domain, but two of these form a functional homodimer that resembles the mammalian architecture.
Types of AC-III.
There are ten known isoforms of adenylate cyclases in mammals:
</dl>
These are also sometimes called simply AC1, AC2, etc., and, somewhat confusingly, sometimes Roman numerals are used for these isoforms that all belong to the overall AC class III. They differ mainly in how they are regulated, and are differentially expressed in various tissues throughout mammalian development.
AC-III regulation.
Adenylate cyclase is dually regulated by G proteins (Gs stimulating activity and Gi inhibiting it), and by forskolin, as well as other isoform-specific effectors:
In neurons, calcium-sensitive adenylate cyclases are located next to calcium ion channels for faster reaction to Ca2+ influx; they are suspected of playing an important role in learning processes. This is supported by the fact that adenylate cyclases are "coincidence detectors", meaning that they are activated only by several different signals occurring together. In peripheral cells and tissues adenylate cyclases appear to form molecular complexes with specific receptors and other signaling proteins in an isoform-specific manner.
Class IV.
AC-IV was first reported in the bacterium "Aeromonas hydrophila", and the structure of the AC-IV from "Yersinia pestis" has been reported. These are the smallest of the AC enzyme classes; the AC-IV from "Yersinia" is a dimer of 19 kDa subunits with no known regulatory components.
Class V and VI.
These forms of AC have been reported in specific bacteria ("Prevotella ruminicola" and "Rhizobium etti", respectively) and have not been extensively characterized.
Further reading.
</dl>

</doc>
<doc id="2560" url="http://en.wikipedia.org/wiki?curid=2560" title="Administrative law">
Administrative law

Administrative law is the body of law that governs the activities of administrative agencies of government. Government agency action can include rulemaking, adjudication, or the enforcement of a specific regulatory agenda. Administrative law is considered a branch of public law. As a body of law, administrative law deals with the decision-making of administrative units of government (for example, tribunals, boards or commissions) that are part of a national regulatory scheme in such areas as police law, international trade, manufacturing, the environment, taxation, broadcasting, immigration and transport. Administrative law expanded greatly during the twentieth century, as legislative bodies worldwide created more government agencies to regulate the increasingly complex social, economic and political spheres of human interaction.
Civil law countries often have specialized courts, administrative courts, that review these decisions.
Administrative law in common law countries.
Generally speaking, most countries that follow the principles of common law have developed procedures for judicial review that limit the reviewability of decisions made by administrative law bodies. Often these procedures are coupled with legislation or other common law doctrines that establish standards for proper rulemaking. Administrative law may also apply to review of decisions of so-called semi-public bodies, such as non-profit corporations, disciplinary boards, and other decision-making bodies that affect the legal rights of members of a particular group or entity.
While administrative decision-making bodies are often controlled by larger governmental units, their decisions could be reviewed by a court of general jurisdiction under some principle of judicial review based upon due process (United States) or fundamental justice (Canada). Judicial review of administrative decisions is different from an administrative appeal. When sitting in review of a decision, the Court will only look at the method in which the decision was arrived at, whereas in an administrative appeal the correctness of the decision itself will be examined, usually by a higher body in the agency. This difference is vital in appreciating administrative law in common law countries.
The scope of judicial review may be limited to certain questions of fairness, or whether the administrative action is "ultra vires". In terms of ultra vires actions in the broad sense, a reviewing court may set aside an administrative decision if it is unreasonable (under Canadian law, following the rejection of the "Patently Unreasonable" standard by the Supreme Court in Dunsmuir v. New Brunswick), "Wednesbury" unreasonable (under British law), or arbitrary and capricious (under U.S. Administrative Procedure Act and New York State law). Administrative law, as laid down by the Supreme Court of India, has also recognized two more grounds of judicial review which were recognized but not applied by English Courts viz. legitimate expectation and proportionality.
The powers to review administrative decisions are usually established by statute, but were originally developed from the royal prerogative writs of English law, such as the writ of mandamus and the writ of certiorari. In certain Common Law jurisdictions, such as India or Pakistan, the power to pass such writs is a Constitutionally guaranteed power. This power is seen as fundamental to the power of judicial review and an aspect of the independent judiciary.
United States.
In the United States, many government agencies are organized under the executive branch of government, although a few are part of the judicial or legislative branches.
In the federal government, the executive branch, led by the president, controls the federal executive departments, which are led by secretaries who are members of the United States Cabinet. The many important independent agencies of the United States government created by statutes enacted by Congress exist outside of the federal executive departments but are still part of the executive branch.
Congress has also created some special judicial bodies known as Article I tribunals to handle some areas of administrative law.
The actions of executive agencies and independent agencies are the main focus of American administrative law. In response to the rapid creation of new independent agencies in the early twentieth century (see discussion below), Congress enacted the Administrative Procedure Act (APA) in 1946. Many of the independent agencies operate as miniature versions of the tripartite federal government, with the authority to "legislate" (through rulemaking; see Federal Register and Code of Federal Regulations), "adjudicate" (through administrative hearings), and to "execute" administrative goals (through agency enforcement personnel). Because the United States Constitution sets no limits on this tripartite authority of administrative agencies, Congress enacted the APA to establish fair administrative law procedures to comply with the constitutional requirements of due process. Agency procedures are drawn from four sources of authority: the APA, organic statutes, agency rules, and informal agency practice.
The American Bar Association's official journal concerning administrative law is the "Administrative Law Review", a quarterly publication that is managed and edited by students at the Washington College of Law.
Historical development.
Stephen Breyer, a U.S. Supreme Court Justice since 1994, divides the history of administrative law in the United States into six discrete periods, according to his book, "Administrative Law & Regulatory Policy" (3d Ed., 1992):
Agriculture.
The agricultural sector is one of the most heavily regulated sectors in the U.S. economy, as it is regulated in various ways at the international, federal, state, and local levels. Consequently, administrative law is a significant component of the discipline of Agricultural Law. The United States Department of Agriculture and its myriad agencies such as the are the primary sources of regulatory activity, although other administrative bodies such as the Environmental Protection Agency play a significant regulatory role as well.
Administrative law in civil law countries.
Unlike most Common-law jurisdictions, the majority of civil law jurisdictions have specialized courts or sections to deal with administrative cases which, as a rule, will apply procedural rules specifically designed for such cases and different from that applied in private-law proceedings, such as contract or tort claims.
France.
In France, most claims against the national or local governments are handled by administrative courts, which use the "Conseil d'État" (Council of State) as a court of last resort. The main administrative courts are the "tribunaux administratifs" and appeal courts are the "cours administratives d'appel". The French body of administrative law is called "droit administratif".
French administrative law which is the founder of Continental administrative law has a huge effect on other administrative laws of several countries such as Belgium, Greece, Turkey and Tunisia.
Germany.
Administrative law in Germany, called “Verwaltungsrecht”, generally rules the relationship between authorities and the citizens and therefore, it establishes citizens’ rights and obligations against the
authorities. It is a part of the public law, which deals with the organization, the tasks and the acting of the public administration. It also contains rules, regulations, orders and decisions created by and related to administrative agencies, such as federal agencies, federal state authorities, urban administrations, but also admission offices and fiscal authorities etc. Administrative law in Germany follows three basic principles.
Administrative law in Germany can be divided into general administrative law and special administrative law.
General administrative law.
The general administration law is basically ruled in the Administrative Procedures Law (Verwaltungsverfahrensgesetz [VwVfG]). Other legal sources are the Rules of the Administrative Courts (Verwaltungsgerichtsordnung [VwGO]), the social security code (Sozialgesetzbuch [SGB]) and the general fiscal law (Abgabenordnung [AO]).
Administrative Procedures Law.
The Verwaltungsverfahrensgesetz (VwVfG), which was enacted in 1977, regulates the main administrative procedures of the federal government. It serves the purpose to ensure a treatment in accordance with the rule of law by the public authority. Furthermore, it contains the regulations for mass processes and expands the legal protection against the authorities. The VwVfG basically applies for the entire public administrative activities of federal agencies as well as federal state authorities, in case of making federal law. One of the central clause is § 35 VwVfG. It defines the administrative act, the most common form of action in which the public administration occurs against a citizen. The definition in § 35 says, that an administration act is characterized by the following features:
It is an official act of an authority in the field of public law to resolve an individual case with effect to the outside.
§§ 36 – 39, §§ 58 – 59 and § 80 VwV––fG rule the structure and the necessary elements of the
administrative act. § 48 and § 49 VwVfG have a high relevance in practice, as well. In these
paragraphs, the prerequisites for redemption of an unlawful administration act (§ 48 VwVfG ) and
withdrawal of a lawful administration act (§ 49 VwVfG ), are listed.
Other legal sources.
Administration procedural law (Verwaltungsgerichtsordnung [VwGO]), which was enacted in 1960, rules the court procedures at the administrative court. The VwGO is divided into five parts, which are the constitution of the courts, action, remedies and retrial, costs and enforcement15 and final clauses and temporary arrangements.
In absence of a rule, the VwGO is supplemented by the code of civil procedure (Zivilprozessordnung [ZPO]) and the judicature act (Gerichtsverfassungsgesetz [GVG]). In addition to the regulation of the administrative procedure, the VwVfG also constitutes the legal protection in administrative law beyond the court procedure. § 68 VwVGO rules the preliminary proceeding, called “Vorverfahren” or “Widerspruchsverfahren”, which is a stringent prerequisite for the administrative procedure, if an action for rescission or a writ of mandamus against an authority is aimed. The preliminary proceeding gives each citizen, feeling unlawfully mistreated by an authority, the possibility to object and to force a review of an administrative act without going to court. The prerequisites to open the public law remedy are listed in § 40 I VwGO. Therefore, it is necessary to have the existence of a conflict in public law without any constitutional aspects and no assignment to another jurisdiction.
The social security code (Sozialgesetzbuch [SGB]) and the general fiscal law are less important for the administrative law. They supplement the VwVfG and the VwGO in the fields of taxation and social legislation, such as social welfare or financial support for students (BaFÖG) etc.
Special administrative law.
The special administrative law consists of various laws. Each special sector has its own law. The most important ones are the
In Germany, the highest administrative court for most matters is the federal administrative court Bundesverwaltungsgericht. There are federal courts with special jurisdiction in the fields of social security law (Bundessozialgericht) and tax law (Bundesfinanzhof).
Italy.
Administrative law in Italy, known as “Diritto amministrativo”, is a branch of public law, whose rules govern the organization of the public administration and the activities of the pursuit of the public interest of the public administration and the relationship between this and the citizens.
Its genesis is related to the principle of division of powers of the State. The administrative power, originally called "executive", is to organize resources and people whose function is devolved to achieve the public interest objectives as defined by the law.
The Netherlands.
In The Netherlands, administrative law provisions are usually contained in separate laws. There is however a single General Administrative Law Act ("Algemene wet bestuursrecht" or Awb) that applies both to the making of administrative decisions and the judicial review of these decisions in courts. On the basis of the Awb, citizens can oppose a decision ('besluit') made by an administrative agency ('bestuursorgaan') within the administration and apply for judicial review in courts if unsuccessful.
Unlike France or Germany, there are no special administrative courts of first instance in the Netherlands, but regular courts have an administrative "chamber" which specializes in administrative appeals. The courts of appeal in administrative cases however are specialized depending on the case, but most administrative appeals end up in the judicial section of the Council of State (Raad van State).
Before going to court, citizens must usually first object to the decision with the administrative body who made it. This is called "bezwaar". This procedure allows for the administrative body to correct possible mistakes themselves and is used to filter cases before going to court. Sometimes, instead of bezwaar, a different system is used called "administratief beroep" (administrative appeal). The difference with bezwaar is that administratief beroep is filed with a different administrative body, usually a higher ranking one, than the administrative body that made the primary decision. Administratief beroep is available only if the law on which the primary decision is based specifically provides for it. An example involves objecting to a traffic ticket with the district attorney ("officier van justitie"), after which the decision can be appealed in court.
In addition, Netherlands General Administrative Law Act (GALA) is a rather good sample of procedural laws in Europe
Turkey.
In Turkey, the lawsuits against the acts and actions of the national or local governments and public bodies are handled by administrative courts which are the main administrative courts. The decisions of the administrative courts are checked by the Regional Administrative Courts and Council of State. Council of State as a court of last resort is exactly similar to Conseil d'État in France.
Sweden.
In Sweden, there is a system of administrative courts that considers only administrative law cases, and is completely separate from the system of general courts. This system has three tiers, with 12 county administrative courts ("förvaltningsrätt") as the first tier, four administrative courts of appeal ("kammarrätt") as the second tier, and the Supreme Administrative Court of Sweden ("Högsta Förvaltningsdomstolen") as the third tier.
Migration cases are handled in a two-tier system, effectively within the system general administrative courts. Three of the administrative courts serve as migration courts ("migrationsdomstol") with the Administrative Court of Appeal in Stockholm serving as the Migration Court of Appeal ("Migrationsöverdomstolen").
Brazil.
In Brazil, unlike most Civil-law jurisdictions, there is no specialized court or section to deal with administrative cases. In 1998, a constitutional reform, led by the government of the President Fernando Henrique Cardoso, introduced regulatory agencies as a part of the executive branch. Since 1988, Brazilian administrative law has been strongly influenced by the judicial interpretations of the constitutional principles of public administration (art. 37 of Federal Constitution): legality, impersonality, publicity of administrative acts, morality and efficiency.
Chile.
The President of the Republic exercises the administrative function, in collaboration with several Ministries or other authorities with "ministerial rank". Each Ministry has one or more under-secretary that performs through public services the actual satisfaction of public needs. There is not a single specialized court to deal with actions against the Administrative entities, but instead there are several specialized courts and procedures of review.
People's Republic of China.
Administrative law in the People's Republic of China was virtually non-existent before the economic reform era initiated by Deng Xiaoping. Since the 1980s, the People's Republic of China has constructed a new legal framework for administrative law, establishing control mechanisms for overseeing the bureaucracy and disciplinary committees for the Communist Party of China. However, many have argued that the usefulness of these laws is vastly inadequate in terms of controlling government actions, largely because of institutional and systemic obstacles like a weak judiciary, poorly trained judges and lawyers, and corruption.
In 1990, the Administrative Supervision Regulations (行政检查条例) and the Administrative Reconsideration Regulations (行政复议条例) were passed. Both regulations have since been amended and upgraded into laws. The 1993 State Civil Servant Provisional Regulations (国家公务员暂行条例) changed the way government officials were selected and promoted, requiring that they pass exams and yearly appraisals, and introduced a rotation system. In 1994, the State Compensation Law (国家赔偿法) was passed, followed by the Administrative Penalties Law (行政处罚法) in 1996.
Ukraine.
As a homogeneous legal substance isolated in a system of jurisprudence, the administrative law of Ukraine is characterized as: (1) a branch of law; (2) a science; (3) a discipline.

</doc>
<doc id="2577" url="http://en.wikipedia.org/wiki?curid=2577" title="Adrastea (moon)">
Adrastea (moon)

"Not to be confused with the asteroid called 5 Astraea"
Adrastea ( ; Greek: Αδράστεια), also known as Jupiter XV, is the second by distance, and the smallest of the four inner moons of Jupiter. It was discovered in "Voyager 2" probe photographs taken in 1979, making it the first natural satellite to be discovered from images taken by an interplanetary spacecraft, rather than through a telescope. It was officially named after the mythological Adrasteia, foster mother of the Greek god Zeus—the equivalent of the Roman god Jupiter.
Adrastea is one of the few moons in the Solar System known to orbit its planet in less than the length of that planet's day. It orbits at the edge of Jupiter's Main Ring and is thought to be the main contributor of material to the Rings of Jupiter. Despite observations made in the 1990s by the Galileo spacecraft, very little is known about the moon's physical characteristics other than its size and the fact that it is tidally locked to Jupiter.
Discovery and observations.
Adrastea was discovered by David C. Jewitt and G. Edward Danielson in "Voyager 2" probe photographs taken on July 8, 1979, and received the designation S/1979 J 1. Although it appeared only as a dot, it was the first moon to be discovered by an interplanetary spacecraft. Soon after its discovery, two other of the inner moons of Jupiter (Thebe and Metis) were observed in the images taken a few weeks earlier by "Voyager 1". The "Galileo" spacecraft was able to determine the moon's shape in 1998, but the images remain poor. In 1983, Adrastea was officially named after the Greek nymph Adrastea, the daughter of Zeus and his lover Ananke.
Physical characteristics.
Adrastea has an irregular shape and measures 20×16×14 km across. A surface area estimate would be between 840 and 1,600 (~1,200) km2. This makes it the smallest of the four inner moons. The bulk, composition and mass of Adrastea are not known, but assuming that its mean density is like that of Amalthea, around 0.86 g/cm³, its mass can be estimated at about 2 × 1015 kg. Amalthea's density implies that the moon is composed of water ice with a porosity of 10–15%, and Adrastea may be similar.
No surface details of Adrastea are known, due to the low resolution of available images.
Orbit.
Adrastea is the smallest and second closest member of the inner Jovian satellite family. It orbits Jupiter at a radius of about 129,000 km (1.806 Jupiter radii) at the exterior edge of the planet's Main Ring. Adrastea is one of only three moons in the Solar System known to orbit its planet in less than the length of that planet's day—the other two being Jupiter's innermost moon Metis, and Mars' moon Phobos. The orbit has very small eccentricity and inclination—around 0.0015 and 0.03°, respectively. Inclination is relative to the equator of Jupiter.
Due to tidal locking, Adrastea rotates synchronously with its orbital period, keeping one face always looking toward the planet. Its long axis is aligned towards Jupiter, this being the lowest energy configuration.
The orbit of Adrastea lies inside Jupiter's synchronous orbit radius (as does Metis’s), and as a result, tidal forces are slowly causing its orbit to decay so that it will one day impact Jupiter. If its density is similar to Amalthea's then its orbit would actually lie within the fluid Roche limit. However, since it is not breaking up, it must still lie outside its rigid Roche limit.
Adrastea is the second-fastest moving of Jupiter's moons, with an orbital speed of 31.378 km/s.
Relationship with Jupiter's rings.
Adrastea is the largest contributor to material in Jupiter's rings. This appears to consist primarily of material that is ejected from the surfaces of Jupiter's four small inner satellites by meteorite impacts. It is easy for the impact ejecta to be lost from these satellites into space. This is due to the satellites' low density and their surfaces lying close to the edge of their Roche spheres.
It seems that Adrastea is the most copious source of this ring material, as evidenced by the densest ring (the Main Ring) being located at and within Adrastea's orbit. More precisely, the orbit of Adrastea lies near the outer edge of Jupiter's Main Ring. The exact extent of visible ring material depends on the phase angle of the images: in forward-scattered light Adrastea is firmly outside the Main Ring, but in back-scattered light (which reveals much bigger particles) there appears to also be a narrow ringlet outside Adrastea's orbit.
References.
Cited sources

</doc>
<doc id="2681" url="http://en.wikipedia.org/wiki?curid=2681" title="Abdülaziz of the Ottoman Empire">
Abdülaziz of the Ottoman Empire

Abdülaziz of the Ottoman Empire or Abdülaziz I (Ottoman Turkish: عبد العزيز / "`Abdü’l-`Azīz", Turkish: "I. Abdülaziz"; 9/18 February 1830 – 4 June 1876) was the 32nd Sultan of the Ottoman Empire and reigned between 25 June 1861 and 30 May 1876. He was the son of Sultan Mahmud II and succeeded his brother Abdülmecid I in 1861.
Born at the Eyüp Palace, Constantinople (present-day Istanbul), on 9/18 February 1830, Abdülaziz received an Ottoman education but was nevertheless an ardent admirer of the material progress that was made in the West. He was the first Ottoman Sultan who travelled to Western Europe, visiting a number of important European capitals including Paris, London and Vienna in the summer of 1867. The Sultan took an interest in documenting the Sultanate. He was also interested in literature and was also a classical music composer. Some of his compositions have been collected in the album "European Music at the Ottoman Court" by the London Academy of Ottoman Court Music.
Family.
His parents were Mahmud II and Valide Sultan Pertevniyal ("Partav-Nihal"). (1812–1883), originally named Bezime, a Vlach. He was a quarter French. In 1868 Pertevniyal was living in the Dolmabahçe Palace. That year Abdülaziz led the visiting Eugénie de Montijo, Empress of France, to see his mother. Pertevniyal perceived the presence of a foreign woman within her quarters of the seraglio as an insult. She reportedly slapped Eugénie across the face, almost resulting in an international incident. According to another account, Pertevniyal became outraged by the forwardness of Eugénie taking the arm of one of her sons while he gave a tour of the palace garden, and she gave the Empress a slap on the stomach as a possibly more subtly intended than often represented reminder that they were not in France. The Pertevniyal Valide Sultan Mosque was built under the patronage of his mother. The construction work began in November 1869 and the mosque was finished in 1871.
His paternal grandparents were Sultan Abdul Hamid I and Sultana Naksh-i-Dil Haseki. Several accounts identify his paternal grandmother with Aimée du Buc de Rivéry, a cousin of Joséphine de Beauharnais. Pertevniyal was a sister of Hoshiar (Khushiyar), third wife of Ibrahim Pasha of Egypt. Hoshiar and Ibrahim were the parents of Isma'il Pasha.
Reign.
Between 1861 and 1871, the Tanzimat reforms which began during the reign of his brother Abdülmecid I were continued under the leadership of his chief ministers, Mehmed Fuad Pasha and Mehmed Emin Âli Pasha. New administrative districts ("vilayets") were set up in 1864 and a Council of State was established in 1868. Public education was organized on the French model and Istanbul University was reorganised as a modern institution in 1861. He was also integral in establishing the first Ottoman civil code.
Abdülaziz cultivated good relations with the Second French Empire and the British Empire. In 1867 he was the first Ottoman sultan to visit Western Europe; his trip included a visit to the United Kingdom, where he was made a Knight of the Garter by Queen Victoria and shown a Royal Navy Fleet Review with Ismail Pasha. He travelled by a private rail car, which today can be found in the Rahmi M. Koç Museum in Istanbul. His fellow Knights of the Garter created in 1867 were Charles Gordon-Lennox, 6th Duke of Richmond, Charles Manners, 6th Duke of Rutland, Henry Somerset, 8th Duke of Beaufort, Prince Arthur, Duke of Connaught and Strathearn (a son of Queen Victoria), Franz Joseph I of Austria and Alexander II of Russia.
Also in 1867, Abdülaziz became the first Ottoman Sultan to formally recognize the title of Khedive (Viceroy) to be used by the Vali (Governor) of the Ottoman Eyalet of Egypt and Sudan (1517–1867), which thus became the autonomous Ottoman Khedivate of Egypt and Sudan (1867–1914). Muhammad Ali Pasha and his descendants had been the governors (Vali) of Ottoman Egypt and Sudan since 1805, but were willing to use the higher title of Khedive, which was unrecognized by the Ottoman government until 1867. In return, the first Khedive, Ismail Pasha, had agreed a year earlier (in 1866) to increase the annual tax revenues which Egypt and Sudan would provide for the Ottoman treasury. Between 1854 and 1894, the revenues from Egypt and Sudan were often declared as a surety by the Ottoman government for borrowing loans from British and French banks. After the Ottoman government declared a sovereign default on its foreign debt repayments on 30 October 1875, which triggered the Great Eastern Crisis (1875–78) in the empire's Balkan provinces that led to the devastating Russo-Turkish War (1877–78) and the establishment of the Ottoman Public Debt Administration in 1881, the importance for Britain of the sureties regarding the Ottoman revenues from Egypt and Sudan increased. Combined with the much more important Suez Canal which was opened in 1869, these sureties were influential in the British government's decision to occupy Egypt and Sudan in 1882, with the pretext of helping the Ottoman-Egyptian government to put down the Urabi Revolt (1879–1882). Egypt and Sudan (together with Cyprus) nominally remained Ottoman territories until 5 November 1914, when the British Empire declared war against the Ottoman Empire during World War I.
In 1869, Abdülaziz received visits from Eugénie de Montijo, Empress consort of Napoleon III of France and other foreign monarchs on their way to the opening of the Suez Canal. The Prince of Wales, the future Edward VII, twice visited Constantinople.
By 1871 both Mehmed Fuad Pasha and Mehmed Emin Âli Pasha were dead. The Second French Empire, his Western European model, had been defeated in the Franco-Prussian War by the North German Confederation under the leadership of the Kingdom of Prussia. Abdülaziz turned to the Russian Empire for friendship, as unrest in the Balkan provinces continued. In 1875, the Herzegovinian rebellion was the beginning of further unrest in the Balkan provinces. In 1876, the April Uprising saw insurrection spreading among the Bulgarians. Ill feeling mounted against Russia for its encouragement of the rebellions.
While no one event led to his being deposed, the crop failure of 1873 and his lavish expenditures on the Ottoman Navy and on new palaces which he had built, along with mounting public debt, helped to create an atmosphere conducive to his being overthrown. Abdülaziz was deposed by his ministers on 30 May 1876; his death at Feriye Palace in Constantinople a few days later was documented as a suicide at the time, although in Sultan Abdulhamid II's recently surfaced memoirs, the event is described as an assassination by the order of Hüseyin Avni Pasha and Midhat Pasha. When Sultan Murad V began to show signs of paranoia, madness and continuous fainting and vomiting even on the day of his coronation and threw himself into a pool yelling at his guards to protect his life, they were afraid the public would become outraged and revolt to bring the former Sultan back. Within a few days, on 4 June 1876, they arranged for Sultan Abdülaziz to kill himself with scissors, cutting his two wrists at the same time. It was unclear how the Sultan got hold of scissors in his tower prison cell and how he managed to cut two wrists at once, since no autopsy was allowed afterwards. The event was recorded as suicide officially and he was buried in Constantinople.
Family life.
First marriage and issue.
He married firstly at Dolmabahçe Palace, Constantinople on 20 May 1856 to Georgian HH Dürrinev Kadınefendi (Batumi, 15 March 1835 – Constantinople, Üsküdar, Çamlıca Palace, 3 December 1892), and they had three children, including Yusuf Izzettin Efendi. His non-spear great-grandson through him is the current crown prince of Kuwait.
Second marriage and issue.
HH Edadil Kadınefendi (1845 – Dolmabahçe Palace, 12 December 1875) at the Dolmabahçe Palace in 1861 and they had one child.
Third marriage and issue.
Circassian HH Gevheri Kadınefendi (Caucasus, 8 July 1856 – Ortaköy Palace, Ortaköy, Constantinople, 20 September 1894) in 1872 to and they had two children.
Fourth marriage and issue.
Georgian HH Hayranidil Kadınefendi (Kars, 2 November 1846 – Ortaköy Palace, Constantinople, 26 November 1898) at the Dolmabahçe Palace, Constantinople, on 21 September 1866 and they had two children.
Fifth marriage and issue.
Georgian HH Neşerek Kadınefendi (Tbilisi, 1848 – 11 June 1876 - Ortaköy Palace, Constantinople) at the Dolmabahçe Palace, in 1868 and they had three children.
External links.
 Media related to at Wikimedia Commons
 Works written by or about at Wikisource

</doc>
<doc id="2709" url="http://en.wikipedia.org/wiki?curid=2709" title="Aberdeen, South Dakota">
Aberdeen, South Dakota

Aberdeen (Lakota: "Ablíla") is a city in and the county seat of Brown County, South Dakota, United States, about 125 mi northeast of Pierre. The city population was 26,091 at the 2010 census, making it the third largest city in the state. Aberdeen is the principal city of the Aberdeen Micropolitan Statistical Area, which includes all of Brown and Edmunds counties and has a population of 40,602 in 2010. Aberdeen is the home of Northern State University and Presentation College.
History.
Settlement.
Before Aberdeen or Brown County was inhabited by European settlers, it was inhabited by the Sioux Indians from approximately 1700 to 1879. Europeans entered the region for business, founding fur trading posts during the 1820s; these trading posts operated until the mid-1830s. The first "settlers" of this region were the Arikara Indians, but they would later be joined by others.
The first group of Euro-American settlers to reach the area that is now Brown County was a party of four people, three horses, two mules, fifteen cattle, and two wagons. This group of settlers was later joined by another group the following spring, and eventually more ettlers migrated toward this general area, currently known as Columbia, South Dakota. This town was established on June 15, 1879. The town was settled in 1880, and incorporated in 1882.
Creation of the town.
Aberdeen, like many towns of the Midwest, was built around the newly developing railroad systems. Aberdeen was first officially plotted as a town site on January 3, 1881, by Charles Prior, the superintendent of the Minneapolis office of the Chicago, Milwaukee, and St. Paul Railroad, or the Milwaukee Road for short, which was presided over by Alexander Mitchell. Mitchell, Charles Prior's boss, was responsible for the choice of town names, was born in Aberdeen, Scotland, after which the town of Aberdeen, South Dakota, was named. Aberdeen was officially founded on July 6, 1881, the date of the first arrival of a Milwaukee Railroad train. Aberdeen then operated under a city charter granted by the Territorial Legislature in March 1883.
As Aberdeen grew, many businesses and buildings were constructed along Aberdeen's Main Street. However, this soon became a problem due to Aberdeen's periodic flooding, which led to it being referred to as "The Town in the Frog Pond". At first, this unique condition presented no problem to the newly constructed buildings because it had not rained very much but, when heavy rains fell, the Pond reappeared and flooded the basements of every building on Main Street, causing many business owners and home owners much turmoil. When this flooding happened, the city had one steam-powered pump that had to be used to dry out the entire area that had been flooded, which would take days, if not weeks – and more often than not, it would have rained again in this time period and caused even more flooding, even in the basements that had already been emptied of the water. When the water was gone from the basements, the city still had to deal with the mud that also resulted from the heavy rains.
The city decided in 1882 to build an artesian ditch to control the "Frog Pond" effects; the plan was later upgraded and developed into an artesian well in 1884 to combat the heavy rains and keep the basements from flooding. The artesian well was designed by the city engineers to prevent flooding and develop a water system. However, during the digging of the well, the water stream that was found underground was too powerful to be contained. The water came blasting out with violent force and had the entire Main Street submerged in up to four feet of water. The engineers realized the previous flaws of the artesian well plan and soon added a gate valve to the well to control the flow of water, giving Aberdeen its first working water supply.
Aberdeen had four different railroad companies with depots built in the newly developing town. With these four railroads intersecting here, Aberdeen soon became known as the "Hub City of the Dakotas". When looking down on Aberdeen from above, the railroad tracks converging in Aberdeen resembled the spokes of a wheel converging at a hub, hence the name "Hub City of the Dakotas". These four railroad companies are the reason why Aberdeen was able to grow and flourish as it did. The only railroad still running through Aberdeen is the Burlington Northern Santa Fe.
Geography.
Aberdeen is located in northeastern South Dakota, in the James River valley, approximately 11 mi west of the river. The James River enters northeastern South Dakota in Brown County, where it is dammed to form two reservoirs northeast of Aberdeen. The city is bisected by "Moccasin Creek", a slow-moving waterway which flows south and then northeast to the James River.
According to the United States Census Bureau, the city has a total area of 15.60 sqmi, of which 15.50 sqmi is land and 0.10 sqmi is water.
Aberdeen has been assigned the ZIP code range 57401−57402.
Climate.
Aberdeen experiences a humid continental climate (Köppen "Dfb/Dfa") influenced by its position far from moderating bodies of water. This brings four distinct seasons, a phenomenon that is characterized by hot, relatively humid summers and cold, dry winters, and it lies in USDA Hardiness Zone 4. The monthly daily average temperature ranges from 12.0 °F in January to 71.3 °F in July, while there are 13 days of 90 °F+ highs and 37 days with sub-0 °F lows annually. Snowfall occurs mostly in light to moderate amounts during the winter, totaling 38 in. Precipitation, at 21.7 in annually, is concentrated in the warmer months. Extreme temperatures have ranged from −46 °F on January 12, 1912 and February 8, 1895 to 115 °F on July 6 and 15, 1936, although a −42 °F reading occurred as recently as January 15, 2009.
The National Oceanographic and Atmospheric Administration maintains a National Weather Service office in Aberdeen. Their area of responsibility includes northern and eastern South Dakota and two counties in west-central Minnesota.
Aberdeen is the county seat of Brown County. The original county seat was, however, Columbia. During the days of the railroad construction, plans were laid to bring the railroad through Columbia, then the county seat. When word of this spread, land in and around Columbia soared in price due to speculation. When time came for the railroads to purchase land, the increase in land prices led them to change their decision and instead to route the rail lines through Aberdeen. However, once Aberdeen became a town in 1881, there was a long-running controversy concerning which town would be the county seat, which continued until 1890, when it was declared by the newly formed South Dakota state constitution in 1889 that a majority vote could move the county seat if the county seat in question had originally been established by less than a majority vote. The result of the vote declared that Aberdeen would be the county seat once and for all, so all of the records were once again transferred to Aberdeen's courthouse; during the battle for county seat, the records had been moved from Columbia's courthouse to Aberdeen's courthouse (which was built from 1886 to 1887), and back again to Columbia's in what seemed to be a never-ending cycle of the transferring of records. This was typically done in the form of nighttime raids from the two towns.
Demographics.
2010 census.
As of the census of 2010, there were 26,091 people, 11,418 households, and 6,354 families residing in the city. The population density was 1683.3 PD/sqmi. There were 12,158 housing units at an average density of 784.4 /sqmi. The racial makeup of the city was 91.8% White, 0.7% African American, 3.6% Native American, 1.3% Asian, 0.2% Pacific Islander, 0.5% from other races, and 2.0% from two or more races. Hispanic or Latino of any race were 1.6% of the population.
There were 11,418 households of which 27.1% had children under the age of 18 living with them, 42.1% were married couples living together, 9.5% had a female householder with no husband present, 4.0% had a male householder with no wife present, and 44.4% were non-families. 36.9% of all households were made up of individuals and 13.1% had someone living alone who was 65 years of age or older. The average household size was 2.18 and the average family size was 2.86.
The median age in the city was 36.4 years. 22.2% of residents were under the age of 18; 12.8% were between the ages of 18 and 24; 24.1% were from 25 to 44; 24.4% were from 45 to 64; and 16.4% were 65 years of age or older. The gender makeup of the city was 47.6% male and 52.4% female.
2000 census.
As of the census of 2000, there were 24,658 people, 10,553 households and 6,184 families residing in the city. The population density was 1,902.1 per square mile (734.4/km²). There were 11,259 housing units at an average density of 868.5 per square mile (335.3/km²). The racial makeup of the city was 94.61% White, 0.37% Black or African American, 3.17% Native American, 0.54% Asian, 0.13% Pacific Islander, 0.19% from other races, and 0.99% from two or more races. 0.79% of the population were Hispanic or Latino of any race. 53.7% were of German, 15% Norwegian and 8.5% Irish ancestry.
There were 10,553 households out of which 27.3% had children under the age of 18 living with them, 47.0% were married couples living together, 8.9% had a female householder with no husband present, and 41.4% were non-families. 34.9% of all households were made up of individuals and 13.6% had someone living alone who was 65 years of age or older. The average household size was 2.21 and the average family size was 2.86.
Age spread: 21.8% under the age of 18, 14.1% from 18 to 24, 26.4% from 25 to 44, 20.4% from 45 to 64, and 17.2% who were 65 years of age or older. The median age was 36 years. For every 100 females there were 89.2 males. For every 100 females age 18 and over, there were 85.3 males.
As of 2000 the median income for a household in the city was $33,276, and the median income for a family was $43,882. Males had a median income of $30,355 versus $20,092 for females. The per capita income for the city was $17,923. About 7.6% of families and 10.5% of the population were below the poverty line, including 10.6% of those under age 18 and 10.1% of those age 65 or over.
Economy.
Super 8 Motels.
Super 8 Motels was founded in 1972 by Dennis Brown and Ron Rivett as a motel referral system, which was replaced with a franchise operation in 1973. The first Super 8, with 60 rooms, was opened in 1974 in Aberdeen and still operates today as the Super 8 Aberdeen East.
Arts and culture.
The Aberdeen area has several cultural organizations.
The Aberdeen Area Arts Council publishes a small monthly newspaper, "ARTiFACTS", with information on area events.
The Aberdeen Community Theatre was created in 1979 and performs at the Capitol Theatre in downtown Aberdeen. The Capitol Theatre was originally built in 1926 and donated to the Aberdeen Community Theatre in 1991; since then more than $963,000 has been spent on renovating and preserving the historical aspect of the Capitol Theatre. Today, the Aberdeen Community Theatre performs five mainstage productions and three youth productions per year.
The South Dakota Film Festival established in 2007 is held annually in the fall. The festival has been host to Kevin Costner, Graham Greene, Adam Greenberg, CSA and many more stars of film and television. The festival's first feature film screened was Into The Wild, shot partially in SD. The festival is held at the historic Capitol Theatre. 
The NSU Theater Department puts on plays during the school year.
The ArtWorks Cooperative is a partnership of artists who work to market their artwork in a gallery setting. The ArtWorks Cooperative sells artists' work and provides an environment that will benefit the artist in terms of artist-to-artist communication, and public interest.
There are four galleries in Aberdeen: Presentation College’s Wein Gallery, Northern State University's Lincoln Gallery, the Aberdeen Recreation & Cultural Center (ARCC) Gallery and the ArtWorks Cooperative Gallery located in the Lakewood Mall.
Sports.
Minor League Baseball.
Aberdeen has been home to three minor league baseball teams since 1920. The Aberdeen Boosters, a class D league team, played in 1920, the Aberdeen Grays, also a class D team, played from 1921 to 1923. The class C Aberdeen Pheasants from 1946 to 1971, and 1995 to 1997. The Pheasants were the affiliate of the former St. Louis Browns (current Baltimore Orioles). Aberdeen was a stop to the majors for such notable players as Don Larsen (perfect game in the World Series), Lou Piniella (AL rookie of the year with Kansas City Royals in 1969), and Jim Palmer, Baseball Hall of Fame pitcher for the Baltimore Orioles.
Tennis.
Aberdeen is presently home to 24 public tennis courts throughout the city – Melgaard Park (4), Northern State University (12), and Holgate Middle School (8).
Golf.
Aberdeen has three golf courses. These are Lee Park Municipal Golf Course, Moccasin Creek Country Club and Rolling Hills Country Club. Lee Park and Moccasin Creek are both 18 hole courses. Rolling Hills is a combined nine hole course and housing development which opened in 2005.
Hockey / Ice Skating.
Aberdeen has multiple outdoor skating rinks and hockey rinks open to the public during winter months.
Aberdeen is also home to the NAHL team, Aberdeen Wings.
Skateboarding/rollerblading.
Aberdeen has a skate park located between East Melgaard Road and 17th Ave SE at Melgaard Park. The equipment installed includes a quarter pipe, penalty box with half pyramid, bank ramp, spine, kinked rail and a ground rail.
Disc golf.
Aberdeen has two disc golf courses, Melgaard Park and the Richmond Lake Disc Golf Course.
Parks and recreation.
Family Aquatic Center.
Completed in the summer of 2007, this complex includes a zero entry pool, competition lap pool, lazy river, numerous water slides, play sand area, and a concession area.
Wylie Park Recreation Area.
Wylie Park Recreation Area features go kart racing, sand volleyball courts, access to Wylie Lake, camping area, picnic areas, and is connected to Storybook Land. Wylie Lake is a small man-made lake, open in the summer months for swimming, lying on the beach, and paddleboating. 
Storybook Land.
Storybook Land is a park with attractions from several different children's storybooks. The park contains a castle, as well as a train that takes visitors through the park. There are two barns which contain petting zoos. Newly added is the Land of Oz, that features characters and attractions from L. Frank Baum's "The Wonderful Wizard of Oz". Baum was a resident of Aberdeen in the 1880s, but left after the failure of the newspaper he was editing.
Richmond Lake Recreation Area.
The Richmond Lake Recreation Area is used by all types of outdoors enthusiasts. Three separate areas in this park cater to the needs of campers, swimmers, naturalists, boaters and anglers. Campers stay in the South Unit, while the 200 acre Forest Drive Unit is a great place for wildlife viewing. The Boat Ramp Unit provides access to the more than 1000 acre lake.
Richmond Lake Recreation Area's small campground offers a quiet camping experience. The park also features a wheelchair accessible camping cabin.
The park's extensive trail system features over 10 mi of trails, including both accessible and interpretive trails. Hikers, bikers and horseback riders can observe the abundance of prairie plants and wildlife of the area up-close.
The park has multiple private and public boat ramps as well as an accessible fishing dock. Richmond Lake has a population of walleye, northern pike, bass, perch, crappie, bluegill, catfish, and bullheads within its waters. An entrance fee is required to gain access to the water and park itself.
Government.
Aberdeen is the center of government for Brown County. City government is overseen by a mayor/city manager and eight council members. The city council is composed of Mayor Mike Levson, City Manager Lynn Lander and council members Todd Campbell, Jennifer Slaight-Hansen, Mark Remily, Rob Ronayne, Alan Johnson, David Bunsness, Clint Rux and Laure Swanson. Each council member serves a five-year term. 
County government is overseen by five commissioners. Each county commissioner serves a five-year term. The county commissioners include Duane Sutton, Tom Fischbach, Nancy Hansen, Burt Elliot, and Doug Fjeldheim. Aberdeen is home to Brown County offices including clerk-magistrate, county auditor, landfill office, register of deeds, county treasurer, coroner, emergency management, highway superintendent, public welfare, state's attorney, and a few others. 
The state senators from Brown County include Brock Greenfield and David Novstrup, and the state representatives included Lana Greefield, Burt Tulson, Dan Kaiser and Al Novstrup. They are all in office until December 2016 
In 2008, Gov Mike Rounds named Aberdeen as the South Dakota Community of the Year.
Education.
Public schools.
Aberdeen Public Schools are part of the Aberdeen School District. The school district has five elementary schools, two middle schools and one high school.
The elementary schools are C.C. Lee Elementary School, Lincoln Elementary School, May Overby Elementary School, O.M. Tiffany Elementary School and Simmons Elementary School. The two middle schools are Holgate Middle School, which serves the north side of Aberdeen, and Simmons Middle School, which serves the south side of the city. Students in the district attend Central High School. The is located in the district. Aberdeen also has an alternative middle and high school.
The Aberdeen School District's enrollment for the year 2011 – 2012 was approximately 3,945 students, and the average class size was in the low to mid-twenties. Due to a projected increase in enrollment and the modernization of facilities, Simmons Middle School was completely remodeled with the demolition of the original 1929 building and the addition of a new classroom and cafeteria building which was completed in August 2008. The public school in Aberdeen is AA under the SDHSAA.
Parochial schools.
Aberdeen has several parochial schools, including the Catholic-affiliated Roncalli High School, the nondenominational and the .
Special programs.
The South Dakota School for the Blind and Visually Impaired is a state special school under the direction of the South Dakota Board of Regents.
Higher education.
Northern State University.
Northern State University is a public university that was founded in 1901 and today occupies a 72 acre campus. 2,528 students, ranging from first year to graduate students, attended NSU for the 2006–2007 school year. The student to teacher ratio is 19:1.
NSU was originally called the Institute of South Dakota before changing its name to Northern Normal and Industrial School in 1901. It changed its name again in 1939 when it became the Northern State Teachers College, and again in 1964, becoming Northern State College before finalizing at Northern State University in 1989.
NSU offers thirty-eight majors and forty-two minors as well as other degrees, and also has nine graduate degree areas for students wishing to further their education after achieving their first degree.
The mascot of NSU is the wolf.
Presentation College.
Presentation College is a Catholic college on a 100 acre campus, and was founded in 1951. PC had approximately 800 students in the 2006 spring semester. PC offers 26 programs between the main Aberdeen campus and the other campuses located throughout the state. Most of the degrees offered are in the health-care field. The student to teacher ratio is 12:1. Presentation's mascot is the Saint, giving it the nickname the Presentation College Saints.
Tom Gallagher
Media.
"The American News" was founded as a weekly in 1885, by C.W. Starling and Paul Ware. It is a daily newspaper.
Infrastructure.
Transportation.
Air.
The Aberdeen Regional Airport is currently served by Delta Connection. It offers flights to Minneapolis-St. Paul International Airport is served by the Bombardier CRJ200 aircraft.
Roads.
There are two major US highways that serve Aberdeen. One is US Highway 281 that runs north-south from the North Dakota border to the border with Nebraska. The second highway is US Highway 12 that runs east-west across northern South Dakota from the Minnesota border before curving northwest into the southwestern corner of North Dakota. US Highway 12 is the major thoroughfare in Aberdeen. US Highway 12 is signed in the city of Aberdeen as 6th Avenue South. US Highway 281 was recently realigned onto a new bypass that was constructed around the western area of the city.
Transit.
Aberdeen Taxi service provides general taxi service in Aberdeen. Aberdeen Shuttle provides shuttle service to and from the airport along with general taxi services.
Jefferson Lines is a bus service from Aberdeen that connects to Sioux Falls, South Dakota, Fargo, North Dakota, and Minneapolis, Minnesota.
There are five car rental services in Aberdeen: Hertz, Avis, Dollar-Thrifty, Toyota Rent-a-Car and Nissan Rental Car. Hertz and Avis Car rental are located in the airport terminal. Dollar-Thrifty is located in Aberdeen Flying Service. Toyota Rent-a-Car and Nissan Rental Car are located at Harr Motors across from the airport.
The BNSF Railway conveys freight and grain through Aberdeen.
Healthcare.
Aberdeen is currently home to two hospitals, , and Sanford Aberdeen Medical Center.
There are several nursing homes in the area, including Avera Mother Joseph Manor, Manor Care, Bethesda Home of Aberdeen, Aberdeen Health and Rehab, Angelhaus and Gellhaus Carehaus.
Religion.
There are several Roman Catholic, Baptist, Lutheran, Methodist, and the Church of Jesus Christ of Latter-day Saints churches in the area, as well as one synagogue.

</doc>
<doc id="2742" url="http://en.wikipedia.org/wiki?curid=2742" title="Abila">
Abila

Abila is the name of several places:

</doc>
<doc id="2745" url="http://en.wikipedia.org/wiki?curid=2745" title="Azad Kashmir">
Azad Kashmir

Azad Jammu and Kashmir (Urdu: آزاد جموں و کشمیر‎ "Azad Jammu o Kashmir") abbreviated as AJK or Azad Kashmir ("free Kashmir"), is an administrative territory of Pakistan. The territory lies west of the Indian-administered state of Jammu and Kashmir, and was previously part of the former princely state of Jammu and Kashmir, which ceased to exist as a result of the first Kashmir war fought between India and Pakistan in 1947.
Azad Kashmir is part of the greater Kashmir region, which is the subject of a long-running conflict between India and Pakistan. The territory shares a border with Gilgit–Baltistan, together with which it is referred to by the United Nations and other international organizations as "Pakistan-administered Kashmir".
The territory also borders Pakistan's Punjab province to the south and Khyber Pakhtunkhwa province to the west. To the east, Azad Kashmir is separated from the Indian-administered state of Jammu and Kashmir by the Line of Control, the "de facto" border between India and Pakistan. Azad Kashmir has a total area of 13297 km2, with an estimated population of around 4.6 million people.
The territory has a parliamentary form of government, with its capital located at Muzaffarabad. The President of Azad Jammu and Kashmir is the constitutional head of the state, while the prime minister, supported by a Council of Ministers, is the chief executive. The unicameral Azad Jammu & Kashmir Legislative Assembly elects both the prime minister and president. The state has its own Supreme Court and a High Court, while the Government of Pakistan's Ministry of Kashmir Affairs serves as a link between it and Azad Kashmir's government. Neither Azad Kashmir nor Gilgit-Baltistan elect members to Pakistan's National Assembly.
A 2005 earthquake killed 100,000 people and left another three million people displaced, with widespread devastation. Since then, with help from the Government of Pakistan and foreign donors, reconstruction of infrastructure is underway. Azad Kashmir's economy largely depends on agriculture, services, tourism, and remittances sent by members of the Kashmiri diaspora. The territory's official language is Urdu, although Pahari,Hindko, Gojri, Punjabi, and Pashto are also spoken. It has a literacy rate of approximately 64%.
History.
At the time of the Partition of India in 1947, the British abandoned their suzerainty over the princely states, which were left with the options of joining India or Pakistan or remaining independent. Hari Singh, the maharaja of Jammu and Kashmir, wanted his state to remain independent. He signed a stand-still agreement with Pakistan. He delayed his decision in an effort to remain independent. 
In the spring of 1947, an uprising against the Maharaja had broken out in Poonch, an area bordering the Rawalpindi division of the West Punjab. Maharaja's administration is said to have started levying punitive taxes on the peasantry which provoked a local revolt and the administration resorted to brutal suppression. The area's population, full of recently demobolised soldiers from the Second World War, rebelled against the Maharaja's forces and gained control of almost the entire district. Following this victory, the pro-Pakistan chieftains of the western Jammu districts of Muzaffarabad, Poonch and Mirpur proclaimed a provisional Azad Jammu and Kahmir government in Rawalpindi on 3 October, 1947.
On 21 October, several thousand Pashtun tribesmen from the North-West Frontier Province poured into Jammu and Kashmir in order to liberate it from the Maharaja's rule. They were led by experienced military leaders and were equipped with modern arms. The Maharaja's crumbling forces were unable to withstand the onslaught. The raiders captured the towns of Muzaffarabad and Baramulla, the latter just twenty miles northwest of the State's capital Srinagar. On 24 October, the Maharaja requested the military assistance of India, which responded that it was unable to help him unless he acceded to India. Accordingly, on 26 October 1947, Maharaja Hari Singh signed an Instrument of Accession, handing over control of defense, external affairs and communications to the Government of India. Indian troops were immediately airlifted into Srinagar. Pakistan intervened subsequently. Fighting ensued between the Indian and Pakistani armies, with the two areas of control stabilized, more or less, around what is now known as the "Line of Control".
Later, India approached the United Nations, asking it to solve the dispute, and resolutions were passed in favor of the holding of a plebiscite with regard to Kashmir's future. However, no such plebiscite has ever been held on either side, since there was a precondition which required the withdrawal of the Pakistani Army along with the non-state elements and the subsequent partial withdrawal of the Indian Army. from the parts of Kashmir under their respective control – a withdrawal that never took place. In 1949, a cease-fire line separating the Indian- and Pakistani-controlled parts of Kashmir was formally put into effect.
Following the 1949 cease-fire agreement, the government of Pakistan divided the northern and western parts of Kashmir that it occupied at the time of cease-fire into the following two separately-controlled political entities:
An area of Kashmir that was once under Pakistani control is the Shaksgam tract, a small region along the northeastern border of Gilgit–Baltistan that was provisionally ceded by Pakistan to the People's Republic of China in 1963 and which now forms part of China's Xinjiang Uygur Autonomous Region.
In 1972, the then-current border between the Indian- and Pakistani-controlled parts of Kashmir was designated as the "Line of Control". The Line of Control has remained unchanged since the 1972 Simla Agreement, which bound the two countries "to settle their differences by peaceful means through bilateral negotiations". Some political experts claim that, in view of that pact, the only solution to the issue is mutual negotiation between the two countries without involving a third party such as the United Nations.
Government.
Azad Jammu and Kashmir (AJK) is a self-governing state under Pakistani control, but under Pakistan's constitution the state is not formally a part of the country as the dispute on Azad Kashmir has not yet been resolved. Pakistan is administering the region as a self-governing territory rather than incorporating it in the federation since the UN mandated ceasefire. Azad Kashmir has its own elected President, Prime Minister, Legislative Assembly, High Court, with Khawaja Shahad Ahmad as its present chief justice, and official flag. The government of Pakistan has not yet allowed Azad Kashmir to issue its own postage stamps, meaning that those of Pakistan are used instead. Brad Adams the Asia director at Human Rights Watch has said in 2006 Although ‘azad’ means ‘free,’ the residents of Azad Kashmir are anything but, The Pakistani authorities govern Azad Kashmir with strict controls on basic freedoms. The Government of Azad Kashmir has very little control over its territory, with its politicians mainly spending their time in Islamabad.
Azad Kashmir's financial matters, i.e., budget and tax affairs, are dealt with by the Azad Jammu and Kashmir Council rather than by Pakistan's Central Board of Revenue. The Azad Jammu and Kashmir Council is a supreme body consisting of 11 members, six from the government of Azad Jammu and Kashmir and five from the government of Pakistan. Its chairman/chief executive is the president of Pakistan. Other members of the council are the president and the prime minister of Azad Kashmir and a few other AJK ministers. Azad Kashmir Day is celebrated in Azad Jammu and Kashmir on October 24, which is the day that the Azad Jammu and Kashmir government was created in 1947. Pakistan has celebrated Kashmir Solidarity Day on February 5 of each year since 1990 as a day of protest against India's "de facto" sovereignty over its State of Jammu and Kashmir. That day is a national holiday in Pakistan. Kashmiris in Azad Kashmir observe the Kashmir Black Day on October 27 of each year since 1947 as day of protest against military occupation in Indian controlled Jammu and Kashmir.
Administrative divisions.
The state is administratively divided into three divisions which, in turn, are divided into ten districts.
Geography and climate.
The northern part of Azad Jammu and Kashmir encompasses the lower part of the Himalayas, including Jamgarh Peak (15,531 feet [4,734 meters]). However, Hari Parbat peak in the Neelum Valley is the highest peak in the state. Fertile, green, mountainous valleys are characteristic of Azad Kashmir's geography, making it one of the most beautiful regions on the subcontinent.
The southern parts of Azad Kashmir including Bhimber, Mirpur and Kotli districts has extremely hot weather in summers and moderate cold weather in winters. It receives rains mostly in monsoon weather.
In the central and northern parts of state weather remains moderate hot in summers and very cold and chilly in winter. Snow fall also occurs there in December and January.
This region receives rainfall in both winters and summers. Muzaffarabad and Pattan are among the wettest areas of the state. Throughout most of the region, the average rainfall exceeds 1400 mm, with the highest average rainfall occurring near Muzaffarabad (around 1800 mm). During summer, monsoon floods of the Jhelum and Leepa rivers are common, due to high rainfall and melting snow.
Culture.
The culture of Azad Kashmir has many similarities to that of northern Punjabi (Potohar) culture in Punjab province. The natives of Azad Kashmir speak Urdu, Potwari, and the Pahari languages. The traditional dress of Kashmiri women is the shalwar kameez in Pahari style. The shalwar kameez is commonly worn by both men and women. Women use the shawl or Kashmir shawl to cover their head and upper body.
The popular and traditional cuisines of Azad Kashmir are Kashmiri Raan (Fried leg of lamb in Kashmiri style), Rogan Josh, Balti Gosht, Kashmiri Dal Chawal (A mixture of split peas, split red lentils, and boiled rice), and Dam Aloo (Fried Potatoes in Kashmiri style).
The traditional drink of Kashmir is Kashmiri tea. Kashmiris are very fond of drinking tea.
Ethnic groups.
Azad Jammu and Kashmir is predominantly Muslim. The majority of the population is culturally, linguistically, and ethnically related to the people of northern Punjab. The principal languages spoken are Pahari, Gojri, Dogri, Potohari, Urdu, Kashmiri, Pashto, and Punjabi.
Many residents of this area have relatives who live in England. Mirpur, in particular, retains strong links with the UK.
Languages.
Urdu is the official language of Azad Jammu and Kashmir. However, due to the area's diverse cultural blend, many languages are spoken by different populations, including: 
Economy.
Historically the economy of these areas now called ‘Azad’ Kashmir has been agricultural which meant that land was the main source or mean of production. This means that all food for immediate and long term consumption was produced from land. The produce included various crops, Fruits, Vegetables etc. Land was also the source of other livelihood necessities such as wood, fuel, grazing for animals which then turned into dairy products. Because of this land was also the main source of revenue for the governments whose primary purpose for centuries was to accumulate revenue.
Agriculture is a major part of Azad Kashmir's economy. Low-lying areas that have high populations grow crops like barley, mangoes, millet, corn (maize), and wheat, and also raise cattle. In the elevated areas that are less populated and more spread-out, forestry, corn, and livestock are the main sources of income. There are mineral and marble resources in Azad Kashmir close to Mirpur and Muzaffarabad. There are also graphite deposits at Mohriwali. There are also reservoirs of low-grade coal, chalk, bauxite, and zircon. Local household industries produce carved wooden objects, textiles, and dhurrie carpets. There is also an arts and crafts industry that produces such cultural goods as namdas, shawls, pashmina, pherans, Papier-mâché, basketry copper, rugs, wood carving, silk and woolen clothing, patto, carpets, namda gubba, and silverware. Agricultural goods produced in the region include mushrooms, honey, walnuts, apples, cherries, medicinal herbs and plants, resin, deodar, kail, chir, fir, maple, and ash timber.
The migration to UK was accelerated and by the completion of Mangla Dam in 1967 the process of ‘chain migration’ became in full flow. Today, remittances from kashmiri diaspora make a critical role in AJK's economy. In the mid-1950s various economic and social development processes were launched in Azad Kashmir. In the 1960s, with the construction of the Mangla Dam in Mirpur District, the Azad Jammu and Kashmir Government began to receive royalties from the Pakistani government for the electricity that the dam provided to Pakistan. During the mid-2000s, a multi-billion dollar reconstruction began in the aftermath of the 2005 Kashmir earthquake.
In addition to agriculture, textiles, and arts and crafts, remittances have played a major role in the economy of Azad Kashmir. One analyst estimated that the figure for Azad Kashmir was 25.1% in 2001. With regard to annual household income, people living in the higher areas are more dependent on remittances than are those living in the lower areas. In the latter part of 2006, billions of dollars for development were mooted by international aid agencies for the reconstruction and rehabilitation of earthquake-hit zones in Azad Kashmir, though much of that amount was subsequently lost in bureaucratic channels, leading to considerable delays in help getting to the most needy. Hundreds of people continued to live in tents long after the earthquake. A land-use plan for the city of Muzaffarabad was prepared by the Japan International Cooperation Agency.
Kashmir as a whole is the one of the most beautiful regions in the world. Some well-known and popular tourist destinations are the following.
Education.
The literacy rate in Azad Kashmir was 62% in 2004, higher than in any region in Pakistan. However, only 2.2% were graduates, compared to the average of 2.9% for Pakistan.
Universities.
The following is a list of universities recognized by Higher Education Commission of Pakistan (HEC):
Medical colleges.
The following is a list of undergraduate medical institutions recognized by Pakistan Medical and Dental Council (PMDC) as of 2013.
Sports.
In terms of sports, Azad Kashmir is very popular in Football, Cricket and Volleyball. Many of tournaments are also held throughout the year ans in the holy month of Ramazan night floodlight tournaments are also organized.
Mirpur has a cricket stadium Quaid-e-Azam Stadium which has been taken over by the Pakistan Cricket Board for renovation for International standard. There is also a cricket stadium in Muzaffarabad with the capacity of 8,000 person. The stadium hosted 8 matches of Inter-District Under 19 Tournament 2013.
There are also many registered sports clubs in mainly South Asia Cricket Club, , and Kashmir National FC. Pilot FC is the current champion of the District Football Association Cup (DFA Cup). Mirpur also take part in the All AJK Football Championship, last year Mirpur was the winner after beating Rawalakot in the final.
Prominent Kashmiris.
Nawaz Sharif,
Hassan Javaid Khan,leader, Pakistan Young Leaders Federation
Ishaq Dar,Finance ministr of Pakistan
Khawaja Ghulam Nabi Gilkar,First president of Ajk
Abdul Ahad Azad,Kashmiri politician

</doc>
<doc id="2908" url="http://en.wikipedia.org/wiki?curid=2908" title="Ahuitzotl">
Ahuitzotl

Ahuitzotl (Nahuatl: "", ) was the eighth Aztec ruler, the "Hueyi Tlatoani" of the city of Tenochtitlan, son of princess Atotoztli II. He was responsible for much of the expansion of the Mexica domain, and consolidated the empire's power after emulating his predecessor. He took power as tlatoani in the year 7 Rabbit (1486), after the death of his predecessor and brother, Tizoc.
His sons were kings Chimalpilli II and Cuauhtémoc and he also had one daughter.
Biography.
Perhaps the greatest known military leader of Pre-Columbian Mesoamerica, Ahuizotl began his reign by suppressing a Huastec rebellion, and then swiftly more than doubled the size of lands under Aztec dominance. He conquered the Mixtec, Zapotec, and other peoples from Mexico's Pacific coast down to the western part of Guatemala. Ahuizotl also supervised a major rebuilding of Tenochtitlan on a grander scale including the expansion of the Great Pyramid or Templo Mayor in the year 8 Reed (1487).
He presided over the introduction of the Great-tailed Grackle into the Valley of Mexico, the earliest documented case of human-mediated bird introduction in the Western Hemisphere.
Ahuizotl died in the year 10 Rabbit (1502) and was succeeded by his nephew, Moctezuma II.
Ahuizotl took his name from the animal Ahuizotl, which the Aztecs considered to be a legendary creature in its own right rather than a mere mythical representation of the king.
Tomb.
On 3 August 2007, Mexican archaeologists announced discovery of what is believed to be the tomb of Ahuizotl beneath a sculpture of Tlaltecuhtli near the Zócalo in Mexico City.

</doc>
<doc id="2926" url="http://en.wikipedia.org/wiki?curid=2926" title="Antarctic">
Antarctic

The Antarctic ( or ) is a polar region, specifically the region around the Earth's South Pole, opposite the Arctic region around the North Pole. The Antarctic comprises the continent of Antarctica and the ice shelves, waters, and island territories in the Southern Ocean situated south of the Antarctic Convergence. The region covers some 20% of the Southern Hemisphere, of which 5.5% (14 million km2) is the surface area of the continent itself.
Geography.
The maritime part of the region constitutes the area of application of the international Convention for the Conservation of Antarctic Marine Living Resources (CCAMLR), where for technical reasons the Convention uses an approximation of the Convergence line by means of a line joining specified points along parallels of latitude and meridians of longitude. The implementation of the Convention is managed through an international Commission headquartered in Hobart, Australia, by an efficient system of annual fishing quotas, licenses and international inspectors on the fishing vessels, as well as satellite surveillance.
Most of the Antarctic region is situated south of 60°S latitude parallel, and is governed in accordance with the international legal regime of the Antarctic Treaty System. The Treaty area covers the continent itself and its immediately adjacent islands, as well as the archipelagos of the South Orkney Islands, South Shetland Islands, Peter I Island, Scott Island and Balleny Islands.
The islands situated between 60°S latitude parallel to the south and the Antarctic Convergence to the north, and their respective 200 nmi Exclusive Economic Zones fall under the national jurisdiction of the countries that possess them: South Georgia and the South Sandwich Islands (United Kingdom; also an EU Overseas territory), Bouvet Island (Norway), and Heard and McDonald Islands (Australia).
Kerguelen Islands (France; also an EU Overseas territory) are situated in the Antarctic Convergence area, while the Falkland Islands, Isla de los Estados, Hornos Island with Cape Horn, Diego Ramírez Islands, Campbell Island, Macquarie Island, Amsterdam and Saint Paul Islands, Crozet Islands, Prince Edward Islands, and Gough Island and Tristan da Cunha group remain north of the Convergence and thus outside the Antarctic region.
Wildlife.
A variety of animals live in Antarctica for at least some of the year, including:
Most of the Antarctic continent is permanently covered by ice and snow; less than 1% of the land is exposed. There are only two species of flowering plant, Antarctic hair grass and Antarctic pearlwort, but a range of mosses, liverworts, lichens and macrofungi.
Society.
The first Antarctic land discovered was the island of South Georgia, visited by the English merchant Anthony de la Roché in 1675. Although myths and speculation about a "Terra Australis" ("Southern Land") date back to antiquity, the first confirmed sighting of the continent of Antarctica is commonly accepted to have occurred in 1820 by the Russian expedition of Fabian Gottlieb von Bellingshausen and Mikhail Lazarev on "Vostok" and "Mirny". The first human born in the Antarctic was Solveig Gunbjørg Jacobsen born on 8 October 1913 in Grytviken, South Georgia.
The Antarctic region had no indigenous population when first discovered, and its present inhabitants comprise a few thousand transient scientific and other personnel working on tours of duty at the several dozen research stations maintained by various countries. However, the region is visited by more than 40,000 tourists annually, the most popular destinations being the Antarctic Peninsula area (especially the South Shetland Islands) and South Georgia Island.
In December 2009, the growth of tourism, with consequences for both the ecology and the safety of the travellers in its great and remote wilderness, was noted at a conference in New Zealand by experts from signatories to the Antarctic Treaty. The definitive results of the conference would be presented at the Antarctic Treaty states' meeting in Uruguay in May 2010.
Conservation.
The Antarctic hosts the world largest protected area comprising 1.07 million km2, the South Georgia and the South Sandwich Islands Marine Protection Area created in 2012.
Time zones.
Because Antarctica surrounds the South Pole, it is theoretically located in all time zones. For practical purposes, time zones are usually based on territorial claims or the time zone of a station's owner country or supply base.
See also.
Islands:
Further reading.
Cruise ship at Petermann Island, with the Kiev Peninsula of Graham Land in the background.

</doc>
<doc id="3014" url="http://en.wikipedia.org/wiki?curid=3014" title="Analcime">
Analcime

Analcime or analcite (from the Greek "analkimos" - "weak") is a white, grey, or colourless tectosilicate mineral. Analcime consists of hydrated sodium aluminium silicate in cubic crystalline form. Its chemical formula is NaAlSi2O6·H2O. Minor amounts of potassium and calcium substitute for sodium. A silver-bearing synthetic variety also exists (Ag-analcite).
Analcime is usually classified as a zeolite mineral, but structurally and chemically it is more similar to the feldspathoids. Analcime occurs as a primary mineral in analcime basalt and other alkaline igneous rocks. It also occurs as cavity and vesicle fillings associated with prehnite, calcite, and zeolites. 
Locations include the Cyclopean Islands east off Sicily and near Trentino in northern Italy; Victoria in Australia; Kerguelen Island in the Indian Ocean; in the Lake Superior copper district of Michigan, Bergen Hill, New Jersey, Golden, Colorado, and at Searles Lake, California in the United States; and at Cape Blomidon, Nova Scotia and Mont Saint-Hilaire, Quebec in Canada; and in Iceland.

</doc>
<doc id="3022" url="http://en.wikipedia.org/wiki?curid=3022" title="Autonomous building">
Autonomous building

An autonomous building is a building designed to be operated independently from infrastructural support services such as the electric power grid, gas grid, municipal water systems, sewage treatment systems, storm drains, communication services, and in some cases, public roads.
Advocates of autonomous building describe advantages that include reduced environmental impacts, increased security, and lower costs of ownership. Some cited advantages satisfy tenets of green building, not independence per se (see below). Off-grid buildings often rely very little on civil services and are therefore safer and more comfortable during civil disaster or military attacks. (Off-grid buildings would not lose power or water if public supplies were compromised for some reason.)
Most of the research and published articles concerning autonomous building focus on residential homes.
British architects Brenda and Robert Vale have said that, as of 2002, "It is quite possible in all parts of Australia to construct a 'house with no bills', which would be comfortable without heating and cooling, which would make its own electricity, collect its own water and deal with its own waste...These houses can be built now, using off-the-shelf techniques. It is possible to build a "house with no bills" for the same price as a conventional house, but it would be (25%) smaller."
History.
In the 1970s, a group of activists and engineers calling themselves the New Alchemists believed the warnings of imminent resource depletion and starvation. The New Alchemists were famous for the depth of research effort placed in their projects. Using conventional construction techniques, they designed a series of "bioshelter" projects, the most famous of which was the Ark Bioshelter community for Prince Edward Island. They published the plans for all of these, with detailed design calculations and blueprints. The Ark used wind based water pumping and electricity, and was self-contained in food production. It had living quarters for people, fish tanks raising tilapia for protein, a greenhouse watered with fish water and a closed loop sewage reclamation system that recycled human waste into sanitized fertilizer for the fish tanks. As of January 2010, the successor organization to the New Alchemists has a web page up as the "New Alchemy Institute". The PEI Ark has been abandoned and partially renovated several times.
The 1990s saw the development of Earthships, similar in intent to the Ark project, but organized as a for-profit venture, with construction details published in a series of 3 books by Mike Reynolds. The building material is tires filled with earth. This makes a wall that has large amounts of thermal mass (see earth sheltering). Berms are placed on exposed surfaces to further increase the house's temperature stability. The water system starts with rain water, processed for drinking, then washing, then plant watering, then toilet flushing, and finally black water is recycled again for more plant watering. The cisterns are placed and used as thermal masses. Power, including electricity, heat and water heating, is from solar power.
1990s architects such as William McDonough and Ken Yeang applied environmentally responsible building design to large commercial buildings, such as office buildings, making them largely self-sufficient in energy production. One major bank building (ING's Amsterdam headquarters) in the Netherlands was constructed to be autonomous and artistic as well.
Advantages.
As an architect or engineer becomes more concerned with the disadvantages of transportation networks, and dependence on distant resources, their designs tend to include more autonomous elements. The historic path to autonomy was a concern for secure sources of heat, power, water and food. A nearly parallel path toward autonomy has been to start with a concern for environmental impacts, which cause disadvantages.
Autonomous buildings can increase security and reduce environmental impacts by using on-site resources (such as sunlight and rain) that would otherwise be wasted. Autonomy often dramatically reduces the costs and impacts of networks that serve the building, because autonomy short-circuits the multiplying inefficiencies of collecting and transporting resources. Other impacted resources, such as oil reserves and the retention of the local watershed, can often be cheaply conserved by thoughtful designs.
Autonomous buildings are usually energy-efficient in operation, and therefore cost-efficient, for the obvious reason that smaller energy needs are easier to satisfy off-grid. But they may substitute energy production or other techniques to avoid diminishing returns in extreme conservation.
An autonomous structure is not always environmentally friendly. The goal of independence from support systems is associated with, but not identical to, other goals of environmentally responsible green building. However, autonomous buildings also usually include some degree of sustainability through the use of renewable energy and other renewable resources, producing no more greenhouse gases than they consume, and other measures.
Disadvantages.
First and fundamentally, independence is a matter of degree. Complete independence is very hard or impossible to attain. For example, eliminating dependence on the electrical grid is relatively simple but growing all necessary food is a more demanding and time-consuming proposition.
Living in an autonomous shelter can require one to make sacrifices in one's lifestyle choices, personal behavior, and social expectations. Even the most comfortable and technologically advanced autonomous houses may require some differences in behavior. Some people adjust easily. Others describe the experience as inconvenient, irritating, isolating, or even as an unwanted full-time job. A well-designed building can reduce this issue, but usually at the expense of reduced autonomy.
An autonomous house must be custom-built (or extensively retrofitted) to suit the climate and location. Passive solar techniques, alternative toilet and sewage systems, thermal massing designs, basement battery systems, efficient windowing, and the array of other design tactics require some degree of non-standard construction, added expense, ongoing experimentation and maintenance, and also have an effect on the psychology of the space.
The Vales, among others, have shown that living off-grid can be a practical, logical lifestyle choice—under certain conditions.
Systems.
This section includes some minimal descriptions of methods, to give some feel for such a building's practicality, provide indexes to further information, and give a sense of modern trends.
Water.
There are many methods of collecting and conserving water. Use reduction is cost-effective.
Greywater systems reuse drained wash water to flush toilets or to water lawns and gardens. Greywater systems can halve the water use of most residential buildings; however, they require the purchase of a sump, greywater pressurization pump, and secondary plumbing. Some builders are installing waterless urinals and even composting toilets that completely eliminate water usage in sewage disposal.
The classic solution with minimal life-style changes is using a well. Once drilled, a well-foot requires substantial power. However, advanced well-foots can reduce power usage by twofold or more from older models. Well water can be contaminated in some areas. The sono arsenic filter eliminates unhealthy arsenic in well water.
However drilling a well is an uncertain activity, with aquifers depleted in some areas. It can also be expensive.
In regions with sufficient rainfall, it is often more economical to design a building to use rain, with supplementary water deliveries in a drought. Rain water makes excellent soft washwater, but needs antibacterial treatment. If used for drinking, mineral supplements or mineralization is necessary.
Most desert and temperate climates get at least 250 mm of rain per year. This means that a typical one-story house with a greywater system can supply its year-round water needs from its roof alone. In the driest areas, it might require a cistern of 30 m3. Many areas average 13 mm of rain per week, and these can use a cistern as small as 10 m3.
In many areas, it is difficult to keep a roof clean enough for drinking. To reduce dirt and bad tastes, systems use a metal collecting-roof and a "roof cleaner" tank that diverts the first 40 liters. Cistern water is usually chlorinated, though reverse osmosis systems provide even better quality drinking water.
Modern cisterns are usually large plastic tanks. Gravity tanks on short towers are reliable, so pump repairs are less urgent. The least expensive bulk cistern is a fenced pond or pool at ground level.
Reducing autonomy reduces the size and expense of cisterns. Many autonomous homes can reduce water use below 10 USgal per person per day, so that in a drought a month of water can be delivered inexpensively via truck. Self-delivery is often possible by installing fabric water tanks that fit the bed of a pick-up truck.
It can be convenient to use the cistern as a heat sink or trap for a heat pump or air conditioning system; however this can make cold drinking water warm, and in drier years may decrease the efficiency of the HVAC system.
Solar stills can efficiently produce drinking water from ditch water or cistern water, especially high-efficiency multiple effect humidification designs, which separate the evaporator(s) and condenser(s).
New technologies, like reverse osmosis can create unlimited amounts of pure water from polluted water, ocean water, and even from humid air. Water makers are available for yachts that convert seawater and electricity into potable water and brine. Atmospheric water generators extract moisture from dry desert air and filter it to pure water.
Sewage.
Resource.
The approaches below treat human excrement as a waste rather than a resource. Composting toilets use bacteria to decompose human feces into useful, odourless, sanitary compost. The process is sanitary because soil bacteria eat the human pathogens as well as most of the mass of the waste. Nevertheless, most health authorities forbid direct use of "humanure" for growing food. The risk is microbial and viral contamination. In a dry composting toilet, the waste is evaporated or digested to gas (mostly carbon dioxide) and vented, so a toilet produces only a few pounds of compost every six months. To control the odor, modern toilets use a small fan to keep the toilet under negative pressure, and exhaust the gasses to a vent pipe.
Some home sewage treatment systems use biological treatment, usually beds of plants and aquaria, that absorb nutrients and bacteria and convert greywater and sewage to clear water. This odor- and color-free reclaimed water can be used to flush toilets and water outside plants. When tested, it approaches standards for potable water. In climates that freeze, the plants and aquaria need to be kept in a small greenhouse space. Good systems need about as much care as a large aquarium.
Electric incinerating toilets turn excrement into a small amount of ash. They are cool to the touch, have no water and no pipes, and require an air vent in a wall. They are used in remote areas where use of septic tanks is limited, usually to reduce nutrient loads in lakes.
NASA's bioreactor is an extremely advanced biological sewage system. It can turn sewage into air and water through microbial action. NASA plans to use it in the manned Mars mission.
A big disadvantage of complex biological sewage treatment systems is that if the house is empty, the sewage system biota may starve to death.
Another method is NASA's urine-to-water distillation system.
Waste.
Sewage handling is essential for public health. Many diseases are transmitted by poorly functioning sewage systems.
The standard system is a tiled leach field combined with a septic tank. The basic idea is to provide a small system with primary sewage treatment. Sludge settles to the bottom of the septic tank, is partially reduced by anaerobic digestion, and fluid is dispersed in the leach field. The leach field is usually under a yard growing grass. Septic tanks can operate entirely by gravity, and if well managed, are reasonably safe.
Septic tanks have to be pumped periodically by a honey wagon to eliminate non reducing solids. Failure to pump a septic tank can cause overflow that damages the leach field, and contaminates ground water. Septic tanks may also require some lifestyle changes, such as not using garbage disposals, minimizing fluids flushed into the tank, and minimizing nondigestible solids flushed into the tank. For example, septic safe toilet paper is recommended.
However, septic tanks remain popular because they permit standard plumbing fixtures, and require few or no lifestyle sacrifices.
Composting or packaging toilets make it economical and sanitary to throw away sewage as part of the normal garbage collection service. They also reduce water use by half, and eliminate the difficulty and expense of septic tanks. However, they require the local landfill to use sanitary practices.
Incinerator systems are quite practical. The ashes are biologically safe, and less than 1/10 the volume of the original waste, but like all incinerator waste, are usually classified as hazardous waste.
Some of the oldest pre-system sewage types are pit toilets, latrines, and outhouses. These are still used in many developing countries.
Storm drains.
Drainage systems are a crucial compromise between human habitability and a secure, sustainable watershed. Paved areas and lawns or turf do not allow much precipitation to filter through the ground to recharge aquifers. They can cause flooding and damage in neighbourhoods, as the water flows over the surface towards a low point.
Typically, elaborate, capital-intensive storm sewer networks are engineered to deal with stormwater. In some cities, such as the Victorian era London sewers or much of the old City of Toronto, the storm water system is combined with the sanitary sewer system. In the event of heavy precipitation, the load on the sewage treatment plant at the end of the pipe becomes too great to handle and raw sewage is dumped into holding tanks, and sometimes into surface water.
Autonomous buildings can address precipitation in a number of ways:
If a water absorbing swale for each yard is combined with permeable concrete streets, storm drains can be omitted from the neighbourhood. This can save more than $800 per house (1970s) by eliminating storm drains. One way to use the savings is to purchase larger lots, which permits more amenities at the same cost. Permeable concrete is an established product in warm climates, and in development for freezing climates. In freezing climates, the elimination of storm drains can often still pay for enough land to construct swales (shallow water collecting ditches) or water impeding berms instead. This plan provides more land for homeowners and can offer more interesting topography for landscaping.
A green roof captures precipitation and uses the water to grow plants. It can be built into a new building or used to replace an existing roof.
Electricity.
Since electricity is an expensive utility, the first step towards conservation is to design a house and lifestyle to reduce demand. Fluorescent lights, laptop computers and gas-powered refrigerators save electricity, although gas-powered refrigerators are not very efficient. There are also superefficient electric refrigerators, such as those produced by the Sun Frost company, some of which use only about half as much electricity as a mass-market energy star-rated refrigerator.
Using a solar roof, solar cells can provide electric power. Solar roofs have the potential to be more cost-effective than retrofitted solar power, because buildings need roofs anyway. Modern solar cells last about 40 years, which makes them a reasonable investment in some areas. At a sufficient angle, solar cells are cleaned by run-off rain water and therefore have almost no life-style impact.
A number of areas that lack sun have wind. To generate power, the average autonomous house needs only one small wind generator, 5 metres or less in diameter. On a 30 metre high tower, this turbine can provide enough power to supplement solar power on cloudy days. Commercially available wind turbines use sealed, one-moving-part AC generators and passive, self-feathering blades for years of operation without service.
The largest advantage of wind power is that larger wind turbines have a lower per-watt cost than solar cells, provided there is wind. However, location is critical. Just as some locations lack sun for solar cells, some locations lack sufficient wind for an economical turbine installation. In the Great Plains of the United States a 10 metre turbine can supply enough energy to heat and cool a well-built all-electric house. Economic use in other areas requires research, and possibly a site-survey.
During times of low demand, excess power can be stored in batteries for future use. However, batteries need to be replaced every few years. In many areas, battery expenses can be eliminated by attaching the building to the electric power grid and operating the power system with net metering. Utility permission is required, but such cooperative generation is legally mandated in some areas (for example, California).
A grid-based building is less autonomous, but more economical and sustainable with fewer lifestyle sacrifices. In rural areas the grid's cost and impacts can be reduced by using single-wire earth return systems (for example, the MALT-system).
In areas that lack access to the grid, battery size can be reduced by including a generator to recharge the batteries during extended fogs or other low-power conditions. Auxiliary generators are usually run from propane, natural gas, or sometimes diesel. An hour of charging usually provides a day of operation. Modern residential chargers permit the user to set the charging times, so the generator is quiet at night. Some generators automatically test themselves once per week.
Recent advances in passively stable magnetic bearings may someday permit inexpensive storage of power in a flywheel in a vacuum. Well-funded groups like Canada's Ballard Power Systems are also working to develop a "regenerative fuel cell", a device that can generate hydrogen and oxygen when power is available, and combine these efficiently when power is needed.
Earth batteries tap electric currents in the earth called telluric current. They can be installed anywhere in the ground. They provide only low voltages and current. They were used to power telegraphs in the 19th century. As appliance efficiencies increase, they may become practical.
Microbial fuel cells finally allow the generation of electricity from biomass. The plant can be chopped and converted as a whole, or it can be left alive so that waste saps from the plant can be converted by bacteria.
Heating.
Most autonomous buildings are designed to use insulation, thermal mass and passive solar heating and cooling. Examples of these are trombe walls and other technologies as skylights.
Passive solar heating can heat most buildings in even the coldest climates. In colder climates, extra construction costs can be as little as 15% more than new, conventional buildings. In warm climates, those having less than two weeks of frosty nights per year, there is no cost impact.
The basic requirement for passive solar heating is that the solar collectors must face the prevailing sunlight (south in the northern hemisphere, north in the southern hemisphere), and the building must incorporate thermal mass to keep it warm in the night.
A recent, somewhat experimental solar heating system "Annualized geo solar heating" is practical even in regions that get little or no sunlight in winter.
It uses the ground beneath a building for thermal mass. Precipitation can carry away the heat, so the ground is shielded with 6 m skirts of plastic insulation. The thermal mass of this system is sufficiently inexpensive and large that it can store enough summer heat to warm a building for the whole winter, and enough winter cold to cool the building in summer.
In annualized geo solar systems, the solar collector is often separate from (and hotter or colder than) the living space. The building may actually be constructed from insulation, for example, straw-bale construction. Some buildings have been aerodynamically designed so that convection via ducts and interior spaces eliminates any need for electric fans.
A more modest "daily solar" design is very practical. For example, for about a 15% premium in building costs, the Passivhaus building codes in Europe use high performance insulating windows, R-30 insulation, HRV ventilation, and a small thermal mass. With modest changes in the building's position, modern krypton- or argon-insulated windows permit normal-looking windows to provide passive solar heat without compromising insulation or structural strength. If a small heater is available for the coldest nights, a slab or basement cistern can inexpensively provide the required thermal mass. Passivhaus building codes in particular bring unusually good interior air quality, because the buildings change the air several times per hour, passing it though a heat exchanger to keep heat inside.
In all systems, a small supplementary heater increases personal security and reduces lifestyle impacts for a small reduction of autonomy. The two most popular heaters for ultra-high-efficiency houses are a small heat pump, which also provides air-conditioning, or a central hydronic (radiator) air heater with water recirculating from the water heater. Passivhaus designs usually integrate the heater with the ventilation system.
Earth sheltering and windbreaks can also reduce the absolute amount of heat needed by a building. Several feet below the earth, temperature ranges from 4 C in North Dakota to 26 C, in Southern Florida. Wind breaks reduce the amount of heat carried away from a building.
Rounded, aerodynamic buildings also lose less heat.
An increasing number of commercial buildings use a combined cycle with cogeneration to provide heating, often water heating, from the output of a natural gas reciprocating engine, gas turbine or stirling electric generator.
Houses designed to cope with interruptions in civil services generally incorporate a wood stove, or heat and power from diesel fuel or bottled gas, regardless of their other heating mechanisms.
Electric heaters and electric stoves may provide pollution-free heat (depending on the power source), but use large amounts of electricity. If enough electricity is provided by solar panels, wind turbines, or other means, then electric heaters and stoves become a practical autonomous design.
Water heating.
hot water heat recycling units recover heat from water drain lines. They increase a building's autonomy by decreasing the heat or fuel used to heat water. They are attractive because they have no lifestyle changes.
Current practical, comfortable domestic water-heating systems combine a solar preheating system with a thermostatic gas-powered flow-through heater, so that the temperature of the water is consistent, and the amount is unlimited. This reduces life-style impacts at some cost in autonomy. 
Solar water heaters can save large amounts of fuel. Also, small changes in lifestyle, such as doing laundry, dishes and bathing on sunny days, can greatly increase their efficiency. Pure solar heaters are especially useful for laundries, swimming pools and external baths, because these can be scheduled for use on sunny days.
The basic trick in a solar water heating system is to use a well-insulated holding tank. Some systems are vacuum- insulated, acting something like large thermos bottles. The tank is filled with hot water on sunny days, and made available at all times. Unlike a conventional tank water heater, the tank is filled only when there is sunlight. Good storage makes a smaller, higher-technology collector feasible. Such collectors can use relatively exotic technologies, such as vacuum insulation, and reflective concentration of sunlight.
cogeneration systems produce hot water from waste heat. They usually get the heat from the exhaust of a generator or fuel cell.
Heat recycling, cogeneration and solar pre-heating can save 50-75% of the gas otherwise used. Also, some combinations provide redundant reliability by having several sources of heat.
Some authorities advocate replacing bottled gas or natural gas with biogas. However, this is usually impractical unless live-stock are on-site. The wastes of a single family are usually insufficient to produce enough methane for anything more than small amounts of cooking.
Cooling.
Annualized geo solar buildings often have buried, sloped water-tight skirts of insulation that extend 6 m from the foundations, to prevent heat leakage between the earth used as thermal mass, and the surface.
Less dramatic improvements are possible. Windows can be shaded in summer. Eaves can be overhung to provide the necessary shade. These also shade the walls of the house, reducing cooling costs.
Another trick is to cool the building's thermal mass at night, and then cool the building from the thermal mass during the day. It helps to be able to route cold air from a sky-facing radiator (perhaps an air heating solar collector with an alternate purpose) or evaporative cooler directly through the thermal mass. On clear nights, even in tropical areas, sky facing radiators can cool below freezing.
If a circular building is aerodynamically smooth, and cooler than the ground, it can be passively cooled by the "dome effect." Many installations have reported that a reflective or light colored dome induces a local vertical heat driven vortex that sucks cooler overhead air downward into a dome if the dome is vented properly (a single overhead vent, and peripheral vents). Some people have reported a temperature differential as high as 8 °C (15 °F) between the inside of the dome and the outside. Buckminster Fuller discovered this effect with a simple house design adapted from a grain silo, and adapted his Dymaxion house and geodesic domes to use it.
Refrigerators and air conditioners operating from the waste heat of a diesel engine exhaust, heater flue or solar collector are entering use. These use the same principles as a gas refrigerator. Normally, the heat from a flue powers an "absorptive chiller". The cold water or brine from the chiller is used to cool air or a refrigerated space.
Cogeneration is popular in new commercial buildings. In current cogeneration systems small gas turbines or stirling engines powered from natural gas produce electricity and their exhaust drives an absorptive chiller.
A truck trailer refrigerator operating from the waste heat of a tractor's diesel exhaust was demonstrated by NRG Solutions, Inc. NRG developed a hydronic ammonia gas heat exchanger and vaporizer, the two essential new, not commercially available components of a waste heat driven refrigerator.
A similar scheme (multiphase cooling) can be by a multistage evaporative cooler. The air is passed through a spray of salt solution to dehumidify it, then through a spray of water solution to cool it, then another salt solution to dehumidify it again. The brine has to be regenerated, and that can be done economically with a low temperature solar still. Multiphase evaporative coolers can lower the air's temperature by 50°F (28°C), and still control humidity. If the brine regenerator uses high heat, they also partially sterilise the air.
If enough electric power is available, cooling can be provided by conventional air conditioning using a heat pump.
Food production.
Food production has often been included in historic autonomous projects to provide security.
Skilled, intensive gardening can support an adult from as little as 100 square meters of land per person,
possibly requiring the use of organic farming and aeroponics. Some proven intensive, low-effort food-production systems include urban gardening (indoors and outdoors). Indoor cultivation may be set up using hydroponics, while outdoor cultivation may be done using permaculture, forest gardening, no-till farming, and do nothing farming.
Greenhouses are also sometimes included. Sometimes they are also outfitted with irrigation systems or heat sink-systems which can respectively irrigate the plants or help to store energy from the sun and redistribute it at night (when the greenhouses starts to cool down).

</doc>
<doc id="3038" url="http://en.wikipedia.org/wiki?curid=3038" title="Acid–base reaction">
Acid–base reaction

An acid–base reaction is a chemical reaction that occurs between an acid and a base. Several theoretical frameworks provide alternative conceptions of the reaction mechanisms and their application in solving related problems. Their importance becomes apparent in analyzing acid–base reactions for gaseous or liquid species, or when acid or base character may be somewhat less apparent. The first of these concepts was provided by the French chemist Antoine Lavoisier, circa 1776.
Acid–base definitions.
Historic development.
Lavoisier's oxygen theory of acids.
The first scientific concept of acids and bases was provided by Lavoisier circa 1776. Since Lavoisier's knowledge of strong acids was mainly restricted to oxoacids, such as HNO3 (nitric acid) and H2SO4 (sulfuric acid), which tend to contain central atoms in high oxidation states surrounded by oxygen, and since he was not aware of the true composition of the hydrohalic acids (HF, HCl, HBr, and HI), he defined acids in terms of their containing "oxygen", which in fact he named from Greek words meaning "acid-former" (from the Greek οξυς ("oxys") meaning "acid" or "sharp" and γεινομαι ("geinomai") meaning "engender"). The Lavoisier definition was held as absolute truth for over 30 years, until the 1810 article and subsequent lectures by Sir Humphry Davy in which he proved the lack of oxygen in H2S, H2Te, and the hydrohalic acids. However, Davy failed to develop a new theory, concluding that "acidity does not depend upon any particular elementary substance, but upon peculiar arrangement of various substances". One notable modification of oxygen theory was provided by Berzelius, who stated that acids are oxides of nonmetals while bases are oxides of metals.
Liebig's hydrogen theory of acids.
Circa 1838 Justus von Liebig proposed that an acid is a hydrogen-containing substance in which the hydrogen could be replaced by a metal. This redefinition was based on his extensive work on the chemical composition of organic acids, finishing the doctrinal shift from oxygen-based acids to hydrogen-based acids started by Davy. Liebig's definition, while completely empirical, remained in use for almost 50 years until the adoption of the Arrhenius definition.
Arrhenius theory.
The first modern definition of acids and bases was devised by Svante Arrhenius. A hydrogen theory of acids, it followed from his 1884 work with Friedrich Wilhelm Ostwald in establishing the presence of ions in aqueous solution and led to Arrhenius receiving the Nobel Prize in Chemistry in 1903.
As defined by Arrhenius:
This causes the protonation of water, or the creation of the hydronium (H3O+) ion. Thus, in modern times, the symbol H+ is interpreted as a shorthand for H3O+, because it is now known that a bare proton does not exist as a free species in aqueous solution.
The Arrhenius definitions of acidity and alkalinity are restricted to aqueous solutions, and refer to the concentration of the solvent ions. Under this definition, pure H2SO4 and HCl dissolved in toluene are not acidic, and molten NaOH and solutions of calcium amide in liquid ammonia are not alkaline.
Overall, to qualify as an Arrhenius acid, upon the introduction to water, the chemical must either cause, directly or otherwise:
Conversely, to qualify as an Arrhenius base, upon the introduction to water, the chemical must either cause, directly or otherwise:
The "universal aqueous acid–base definition" of the Arrhenius concept is described as the formation of a water molecule from a proton and hydroxide ion. This leads to the definition that in Arrhenius acid–base reactions, a salt and water are formed from the reaction between an acid and a base. This is a neutralization reaction - the acid and base properties of H+ and OH− are neutralized, for they combine to form H2O, the water molecule. The acid-base neutralization reaction can be put into a word equation:
The positive ion from a base and the negative ion from an acid form a salt together - in other words, an acid-base neutralization reaction is a double-replacement reaction. For example, when a neutralization reaction takes place between hydrochloric acid (HCl) and sodium hydroxide (NaOH), the products are sodium chloride (common table salt) and water.
Notice how the cations and the anions merely switched places: the Na+ from the NaOH combined with the Cl− from the HCl to form NaCl, while the OH− from the NaOH combined with the H+ from the HCl to form H2O.
Brønsted–Lowry definition.
The Brønsted–Lowry definition, formulated in 1923, independently by Johannes Nicolaus Brønsted in Denmark and Martin Lowry in England, is based upon the idea of protonation of bases through the de-protonation of acids – that is, the ability of acids to "donate" hydrogen ions (H+)—otherwise known as protons—to bases, which "accept" them.
An acid–base reaction is, thus, the removal of a hydrogen ion from the acid and its addition to the base. The removal of a hydrogen ion from an acid produces its "conjugate base", which is the acid with a hydrogen ion removed. The reception of a proton by a base produces its "conjugate acid", which is the base with a hydrogen ion added.
Unlike the previous definitions, the Brønsted–Lowry definition does not refer to the formation of salt and solvent, but instead to the formation of "conjugate acids" and "conjugate bases", produced by the transfer of a proton from the acid to the base. In this approach, acids and bases are fundamentally different in behavior from salts, which are seen as electrolytes, subject to the theories of Debye, Onsager, and others. An acid and a base react not to produce a salt and a solvent, but to form a new acid and a new base. The concept of neutralization is thus absent. Brønsted–Lowry acid–base behavior is formally independent of any solvent, making it more all-encompassing than the Arrhenius model.
The general formula for acid–base reactions according to the Brønsted–Lowry definition is:
where HA represents the acid, B represents the base, BH+ represents the conjugate acid of B, and A− represents the conjugate base of HA.
For example, a Brønsted-Lowry model for the dissociation of hydrochloric acid (HCl) in aqueous solution would be the following:
The removal of H+ from the HCl produces the chloride ion, Cl−, the conjugate base of the acid. The addition of H+ to the H2O (acting as a base) forms the hydronium ion, H3O+, the conjugate acid of the base.
Water is amphoteric—that is, it can act as both an acid and a base. The Brønsted-Lowry model explains this, showing the dissociation of water into low concentrations of hydronium and hydroxide ions:
This equation is demonstrated in the image below:
Here, one molecule of water acts as an acid, donating an H+ and forming the conjugate base, OH−, and a second molecule of water acts as a base, accepting the H+ ion and forming the conjugate acid, H3O+.
As an example of water acting as an acid, consider an aqueous solution of pyridine, C5H5N.
In this example, a water molecule is split into a hydrogen ion, which is donated to a pyridine molecule, and an hydroxide ion.
In the Brønsted-Lowry model, the solvent does not necessarily have to be water. For example, consider what happens when acetic acid, CH3COOH, dissolves in liquid ammonia.
An H+ ion is removed from acetic acid, forming its conjugate base, the acetate ion, CH3COO−. The addition of an H+ ion to an ammonia molecule of the solvent creates its conjugate acid, the ammonium ion, NH4+.
The Brønsted–Lowry model calls hydrogen-containing substances (like HCl) acids. Thus, some substances, which many chemists considered to be acids, such as SO3 or BCl3, are excluded from this classification due to lack of hydrogen. Gilbert N. Lewis wrote in 1938, "To restrict the group of acids to those substances that contain hydrogen interferes as seriously with the systematic understanding of chemistry as would the restriction of the term oxidizing agent to substances containing oxygen." Furthermore, KOH and KNH2 are not considered Brønsted bases, but rather salts containing the bases OH− and NH2−.
Lewis definition.
The hydrogen requirement of Arrhenius and Brønsted–Lowry was removed by the Lewis definition of acid–base reactions, devised by Gilbert N. Lewis in 1923, in the same year as Brønsted–Lowry, but it was not elaborated by him until 1938. Instead of defining acid–base reactions in terms of protons or other bonded substances, the Lewis definition defines a base (referred to as a "Lewis base") to be a compound that can donate an "electron pair", and an acid (a "Lewis acid") to be a compound that can receive this electron pair.
For example boron trifluoride, BF3 is a typical Lewis acid. It can accept a pair of electrons as it has a vacancy in its octet. The fluoride ion has a full octet and can donate a pair of electrons. Thus
is a typical Lewis acid, Lewis base reaction. All compounds of group 13 elements with a formula AX3 can behave as Lewis acids. Similarly, compounds of group 15 elements with a formula DY3, such as amines, NR3, and phosphines, PR3, can behave as Lewis bases. Adducts between them have the formula X3A←DY3 with a dative covalent bond, shown symbolically as ←, between the atoms A (acceptor) and D (donor). Compounds of group 16 with a formula DX2 may also act as Lewis bases; in this way, a compound like an ether, R2O, or a thioether, R2S, can act as a Lewis base. The Lewis definition is not limited to these examples. For instance, carbon monoxide acts as a Lewis base when it forms an adduct with boron trifluoride, of formula F3B←CO
Adducts involving metal ions are referred to as co-ordination compounds; each ligand donates a pair of electrons to the metal ion. The reaction
can be seen as an acid-base reaction in which a stronger base (ammonia) replaces a weaker one (water)
The Lewis and Brønsted–Lowry definitions are consistent with each other since the reaction
is an acid-base reaction in both theories.
Solvent system definition.
One of the limitations of the Arrhenius definition is its reliance on water solutions. Edward Curtis Franklin studied the acid–base reactions in liquid ammonia in 1905 and pointed out the similarities to the water-based Arrhenius theory. Albert F. O. Germann, working with liquid phosgene, COCl2, formulated the solvent-based theory in 1925, thereby generalizing the Arrhenius definition to cover aprotic solvents.
Germann pointed out that in many solutions, there are ions in equilibrium with the neutral solvent molecules:
For example, water and ammonia undergo such dissociation into hydronium and hydroxide, and ammonium and amide, respectively:
Some aprotic systems also undergo such dissociation, such as dinitrogen tetroxide into nitrosonium and nitrate, antimony trichloride into dichloroantimonium and tetrachloroantimonate, and phosgene into chlorocarboxonium and chloride:
A solute that causes an increase in the concentration of the solvonium ions and a decrease in the concentration of solvate ions is defined as an "acid". A solute that causes an increase in the concentration of the solvate ions and a decrease in the concentration of the solvonium ions is defined as a "base".
Thus, in liquid ammonia, KNH2 (supplying NH2−) is a strong base, and NH4NO3 (supplying NH4+) is a strong acid. In liquid sulfur dioxide (SO2), thionyl compounds (supplying SO2+) behave as acids, and sulfites (supplying SO32−) behave as bases.
The non-aqueous acid–base reactions in liquid ammonia are similar to the reactions in water:
Nitric acid can be a base in liquid sulfuric acid:
The unique strength of this definition shows in describing the reactions in aprotic solvents; for example, in liquid N2O4:
Because the solvent system definition depends on the solute as well as on the solvent itself, a particular solute can be either an acid or a base depending on the choice of the solvent: HClO4 is a strong acid in water, a weak acid in acetic acid, and a weak base in fluorosulfonic acid; this characteristic of the theory has been seen as both a strength and a weakness, because some substances (such as SO3 and NH3) have been seen to be acidic or basic on their own right. On the other hand, solvent system theory has been criticized as being too general to be useful. Also, it has been thought that there is something intrinsically acidic about hydrogen compounds, a property not shared by non-hydrogenic solvonium salts.
Lux–Flood definition.
This acid–base theory was a revival of oxygen theory of acids and bases, proposed by German chemist Hermann Lux in 1939, further improved by Håkon Flood circa 1947 and is still used in modern geochemistry and electrochemistry of molten salts. This definition describes an acid as an oxide ion (O2−) acceptor and a base as an oxide ion donor. For example:
Usanovich definition.
Mikhail Usanovich developed a general theory that does not restrict acidity to hydrogen-containing compounds, but his approach, published in 1938, was even more general than Lewis theory. Usanovich's theory can be summarized as defining an acid as anything that accepts negative species or donates positive ones, and a base as the reverse. This defined the concept of redox (oxidation-reduction) as a special case of acid-base reactions
Some examples of Usanovich acid-base reactions include:
Acid-base equilibrium.
The reaction of a strong acid with a strong base is essentially a quantitative reaction. For example
In this reaction both the sodium and chloride ions are spectators as the neutralization reaction,
does not involve them. With weak bases addition of acid is not quantitative because a solution of a weak base is a buffer solution. A solution of a weak acid is also a buffer solution. When a weak acid reacts with a weak base an equilibrium mixture is produced. For example, adenine, written as AH can react with a hydrogen phosphate ion, HPO42− 
The equilibrium constant for this reaction can be derived from the acid dissociation constants of adenine and the hydrogen phosphate ion.
The notation [x] signifies "concentration of x". When these two equations are combined by eliminating the hydrogen ion concentration, an expression for the equilibrium constant, K. is obtained.
In 1963, Ralph Pearson proposed a qualitative concept known as Hard Soft Acid Base principle. later made quantitative with help of Robert Parr in 1984. 'Hard' applies to species that are small, have high charge states, and are weakly polarizable. 'Soft' applies to species that are large, have low charge states and are strongly polarizable. Acids and bases interact, and the most stable interactions are hard–hard and soft–soft. This theory has found use in organic and inorganic chemistry.
Acid–alkali reaction.
An acid–alkali reaction is a special case of an acid–base reaction, where the base used is also an alkali. When an acid reacts with an alkali it forms a metal salt and water. Acid–alkali reactions are also a type of neutralization reaction.
In general, acid–alkali reactions can be simplified to
by omitting spectator ions.
Acids are in general pure substances that contain hydrogen ions (H+) or cause them to be produced in solutions. Hydrochloric acid (HCl) and sulfuric acid (H2SO4) are common examples. In water, these break apart into ions:
To produce hydroxide ions in water, the alkali breaks apart into ions as below:

</doc>
