<doc id="569" url="http://en.wikipedia.org/wiki?curid=569" title="Anthropology">
Anthropology

Anthropology is the study of humans. Its main subdivisions are cultural anthropology, which describes the workings of societies around the world, and biological anthropology, which concerns long-term development of the human organism. A related discipline, archaeology, studies past human cultures through investigation of physical evidence.
The Oxford Dictionaries define it as "The study of humankind", to include "cultural or social anthropology" and "physical anthropology." The Encyclopedia Britannica has a synonymous statement, "the science of humanity," but lists a slightly different catalogue: "biology and evolutionary history," and "society and culture." The American Anthropological Association offers: "the study of humans, past and present," which "draws and builds upon ... the social and biological sciences as well as the humanities and physical sciences." Eric R. Wolf states "Ideas about race, culture and peoplehood or ethnicity have long served to orient anthropology's inquiries ..."
Origin and development of the term.
The term "anthropology" is a compound of Greek ἄνθρωπος "anthrōpos", "human being" (understood to mean "humankind" or "humanity"), and -λογία "-logia", "study." Unknown in ancient Greek or Latin, it first appears in the scholarly Latin "anthropologia" of Renaissance France, where it spawns the French word "anthropologie", transferred into English as anthropology. It belongs to a class of words produced with the –logy suffix, such as archeo-logy, bio-logy, etc., "the study (or science) of".
Introduction to the 19th century.
Anthropologie has been in use since as early as the 17th century as translation of "anthropologia". The Bartholins, founders of the University of Copenhagen, defined it as follows:
Anthropology, that is to say the science that treats of man, is divided ordinarily and with reason into Anatomy, which considers the body and the parts, and Psychology, which speaks of the soul.
Sporadic use of the term for some of the subject matter occurred subsequently, such as the use by Étienne Serres in 1838 to describe the natural history, or paleontology, of man, based on comparative anatomy, and the creation of a chair in anthropology and ethnography in 1850 at the National Museum of Natural History (France) by Jean Louis Armand de Quatrefages de Bréau. Various short-lived organizations of anthropologists had already been formed. The Société Ethnologique de Paris, the first to use Ethnology, was formed in 1839. Its members were primarily anti-slavery activists. When slavery was abolished in France in 1848 the Société was abandoned.
Meanwhile the Ethnological Society of New York, currently the American Ethnological Society, was founded on its model in 1842, as well as the Ethnological Society of London in 1843, a break-away group of the Aborigines' Protection Society. These anthropologists of the times were liberal, anti-slavery, and pro-human-rights activists. They maintained international connections.
Anthropology and many other current fields are the intellectual results of the comparative methods developed in the earlier 19th century. Theorists in such diverse fields as anatomy, linguistics, and Ethnology, making feature-by-feature comparisons of their subject matters, were beginning to suspect that similarities between animals, languages, and folkways were the result of processes or laws unknown to them then. For them, the publication of Charles Darwin’s On the Origin of Species was the epiphany of everything they had begun to suspect. Darwin himself arrived at his conclusions through comparison of species he had seen in agronomy and in the wild.
Darwin and Wallace unveiled evolution in the late 1850’s. There was an immediate rush to bring it into the social sciences. Paul Broca in Paris was in the process of breaking away from the Société de biologie to form the first of the explicitly anthropological societies, the Société d’Anthropologie de Paris, meeting for the first time in Paris in 1859. When he read Darwin he became an immediate convert to "Transformisme", as the French called evolutionism. His definition now became "the study of the human group, considered as a whole, in its details, and in relation to the rest of nature".
Broca, being what today would be called a neurosurgeon, had taken an interest in the pathology of speech. He wanted to localize the difference between man and the other animals, which appeared to reside in speech. He discovered the speech center of the human brain, today called Broca's area after him. His interest was mainly in Biological anthropology, but a German philosopher specializing in psychology, Theodor Waitz, took up the theme of general and social anthropology in his six-volume work, entitled "Die Anthropologie der Naturvölker", 1859-1864. The title was soon translated as "The Anthropology of Primitive Peoples". The last two volumes were published posthumously.
Waitz defined anthropology as "the science of the nature of man". By nature he meant matter animated by "the Divine breath"; i.e., he was an animist. Following Broca’s lead, Waitz points out that anthropology is a new field, which would gather material from other fields, but would differ from them in the use of comparative anatomy, physiology, and psychology to differentiate man from "the animals nearest to him". He stresses that the data of comparison must be empirical, gathered by experimentation. The history of civilization as well as ethnology are to be brought into the comparison. It is to be presumed fundamentally that the species, man, is a unity, and that "the same laws of thought are applicable to all men".
Waitz was influential among the British ethnologists. In 1863 the explorer, Richard Francis Burton and the speech therapist, James Hunt broke away from the Ethnological Society of London to form the Anthropological Society of London, which henceforward would follow the path of the new anthropology rather than just ethnology. It was the 2nd society dedicated to general anthropology in existence. Representatives from the French "Société" were present, though not Broca. In his keynote address, printed in the first volume of its new publication, "The Anthropological Review", Hunt stressed the work of Waitz, adopting his definitions as a standard. Among the first associates were the young Edward Burnett Tylor, inventor of cultural anthropology, and his brother Alfred Tylor, a geologist. Previously Edward had referred to himself as an ethnologist; subsequently, an anthropologist.
Similar organizations in other countries followed: The American Anthropological Association in 1902, the Anthropological Society of Madrid (1865), the Anthropological Society of Vienna (1870), the Italian Society of Anthropology and Ethnology (1871), and many others subsequently. The majority of these were evolutionist. One notable exception was the (1869) founded by Rudolph Virchow, known for his vituperative attacks on the evolutionists. Not religious himself, he insisted that Darwin’s conclusions lacked empirical foundation.
During the last three decades of the 19th century a proliferation of anthropological societies and associations occurred, most independent, most publishing their own journals, and all international in membership and association. The major theorists belonged to these organizations. They supported the gradual osmosis of anthropology curricula into the major institutions of higher learning. By 1898 the American Association for the Advancement of Science was able to report that 48 educational institutions in 13 countries had some curriculum in anthropology. None of the 75 faculty members were under a department named anthropology.
Introduction to the 20th century.
This meagre statistic expanded in the 20th century to comprise anthropology departments in the majority of the world’s higher educational institutions, many thousands in number. Anthropology has diversified from a few major subdivisions to dozens more. Practical anthropology, the use of anthropological knowledge and technique to solve specific problems, has arrived; for example, the presence of buried victims might stimulate the use of a forensic archaeologist to recreate the final scene. Organization has reached global level. For example, the World Council of Anthropological Associations (WCAA), "a network of national, regional and international associations that aims to promote worldwide communication and cooperation in anthropology", currently contains members from about three dozen nations.
Since the work of Franz Boas and Bronisław Malinowski in the late 19th and early 20th centuries, "social" anthropology in Great Britain and "cultural" anthropology in the US have been distinguished from other social sciences by its emphasis on cross-cultural comparisons, long-term in-depth examination of context, and the importance it places on participant-observation or experiential immersion in the area of research. Cultural anthropology in particular has emphasized cultural relativism, holism, and the use of findings to frame cultural critiques. This has been particularly prominent in the United States, from Boas' arguments against 19th-century racial ideology, through Margaret Mead's advocacy for gender equality and sexual liberation, to current criticisms of post-colonial oppression and promotion of multiculturalism. Ethnography is one of its primary research designs as well as the text that is generated from anthropological fieldwork.
In Great Britain and the Commonwealth countries, the British tradition of social anthropology tends to dominate. In the United States, anthropology has traditionally been divided into the four field approach developed by Franz Boas in the early 20th century: "biological" or "physical" anthropology; "social", "cultural", or "sociocultural" anthropology; and archaeology; plus anthropological linguistics. These fields frequently overlap, but tend to use different methodologies and techniques.
European countries with overseas colonies tended to practice more ethnology (a term coined and defined by Adam F. Kollár in 1783). In non-colonial European countries, social anthropology is now defined as the study of social organization in non-state societies. It is sometimes referred to as sociocultural anthropology in the parts of the world that were influenced by the European tradition.
Fields.
Anthropology is a global discipline where humanities, social, and natural sciences are forced to confront one another. Anthropology builds upon knowledge from natural sciences, including the discoveries about the origin and evolution of "Homo sapiens", human physical traits, human behavior, the variations among different groups of humans, how the evolutionary past of "Homo sapiens" has influenced its social organization and culture, and from social sciences, including the organization of human social and cultural relations, institutions, social conflicts, etc. Early anthropology originated in Classical Greece and Persia and studied and tried to understand observable cultural diversity. As such, anthropology has been central in the development of several new (late 20th century) interdisciplinary fields such as cognitive science, global studies, and various ethnic studies.
According to Clifford Geertz, 
 "anthropology is perhaps the last of the great nineteenth-century conglomerate disciplines still for the most part organizationally intact. Long after natural history, moral philosophy, philology, and political economy have dissolved into their specialized successors, it has remained a diffuse assemblage of ethnology, human biology, comparative linguistics, and prehistory, held together mainly by the vested interests, sunk costs, and administrative habits of academia, and by a romantic image of comprehensive scholarship."
Sociocultural anthropology has been heavily influenced by structuralist and postmodern theories, as well as a shift toward the analysis of modern societies. During the 1970s and 1990s, there was an epistemological shift away from the positivist traditions that had largely informed the discipline. During this shift, enduring questions about the nature and production of knowledge came to occupy a central place in cultural and social anthropology. In contrast, archaeology and biological anthropology remained largely positivist. Due to this difference in epistemology, the four sub-fields of anthropology have lacked cohesion over the last several decades.
Sociocultural.
Sociocultural anthropology draws together the principle axes of cultural anthropology and social anthropology. Cultural anthropology is the comparative study of the manifold ways in which people "make sense" of the world around them, while social anthropology is the study of the "relationships" among persons and groups. Cultural anthropology is more related to philosophy, literature and the arts (how one's culture affects experience for self and group, contributing to more complete understanding of the people's knowledge, customs, and institutions), while social anthropology is more related to sociology and history. in that it helps develop understanding of social structures, typically of others and other populations (such as minorities, subgroups, dissidents, etc.). There is no hard-and-fast distinction between them, and these categories overlap to a considerable degree.
Inquiry in sociocultural anthropology is guided in part by cultural relativism, the attempt to understand other societies in terms of their own cultural symbols and values. Accepting other cultures in their own terms moderates reductionism in cross-cultural comparison. This project is often accommodated in the field of ethnography. Ethnography can refer to both a methodology and the product of ethnographic research, i.e. an ethnographic monograph. As methodology, ethnography is based upon long-term fieldwork within a community or other research site. Participant observation is one of the foundational methods of social and cultural anthropology. Ethnology involves the systematic comparison of different cultures. The process of participant-observation can be especially helpful to understanding a culture from an emic (conceptual, vs. etic, or technical) point of view.
The study of kinship and social organization is a central focus of sociocultural anthropology, as kinship is a human universal. Sociocultural anthropology also covers economic and political organization, law and conflict resolution, patterns of consumption and exchange, material culture, technology, infrastructure, gender relations, ethnicity, childrearing and socialization, religion, myth, symbols, values, etiquette, worldview, sports, music, nutrition, recreation, games, food, festivals, and language (which is also the object of study in linguistic anthropology).
Comparison across cultures is a key element of method in sociocultural anthropology, including the industrialized (and de-industrialized) West. Cultures in the Standard Cross-Cultural Sample (SCCS) of world societies are:
Biological.
Biological Anthropology and Physical Anthropology are synonymous terms to describe anthropological research focused on the study of humans and non-human primates in their biological, evolutionary, and demographic dimensions. It examines the biological and social factors that have affected the evolution of humans and other primates, and that generate, maintain or change contemporary genetic and physiological variation.
Archaeological.
Archaeology is the study of the human past through its material remains. Artifacts, faunal remains, and human altered landscapes are evidence of the cultural and material lives of past societies. Archaeologists examine these material remains in order to deduce patterns of past human behavior and cultural practices. Ethnoarchaeology is a type of archaeology that studies the practices and material remains of living human groups in order to gain a better understanding of the evidence left behind by past human groups, who are presumed to have lived in similar ways.
Linguistic.
Linguistic anthropology (also called anthropological linguistics) seeks to understand the processes of human communications, verbal and non-verbal, variation in language across time and space, the social uses of language, and the relationship between language and culture. It is the branch of anthropology that brings linguistic methods to bear on anthropological problems, linking the analysis of linguistic forms and processes to the interpretation of sociocultural processes. Linguistic anthropologists often draw on related fields including sociolinguistics, pragmatics, cognitive linguistics, semiotics, discourse analysis, and narrative analysis.
Key topics by field: sociocultural.
Art, media, music, dance and film.
Art.
One of the central problems in the anthropology of art concerns the universality of 'art' as a cultural phenomenon. Several anthropologists have noted that the Western categories of 'painting', 'sculpture', or 'literature', conceived as independent artistic activities, do not exist, or exist in a significantly different form, in most non-Western contexts. To surmount this difficulty, anthropologists of art have focused on formal features in objects which, without exclusively being 'artistic', have certain evident 'aesthetic' qualities. Boas' "Primitive Art", Claude Lévi-Strauss' "The Way of the Masks" (1982) or Geertz's 'Art as Cultural System' (1983) are some examples in this trend to transform the anthropology of 'art' into an anthropology of culturally specific 'aesthetics'.
Media.
Anthropology of media (also anthropology of mass media, media anthropology) emphasizes ethnographic studies as a means of understanding producers, audiences, and other cultural and social aspects of mass media. The types of ethnographic contexts explored range from contexts of media production (e.g., ethnographies of newsrooms in newspapers, journalists in the field, film production) to contexts of media reception, following audiences in their everyday responses to media. Other types include cyber anthropology, a relatively new area of internet research, as well as ethnographies of other areas of research which happen to involve media, such as development work, social movements, or health education. This is in addition to many classic ethnographic contexts, where media such as radio, the press, new media and television have started to make their presences felt since the early 1990s.
Music.
Ethnomusicology is an academic field encompassing various approaches to the study of music (broadly defined), that emphasize its cultural, social, material, cognitive, biological, and other dimensions or contexts instead of or in addition to its isolated sound component or any particular repertoire.
Visual.
Visual anthropology is concerned, in part, with the study and production of ethnographic photography, film and, since the mid-1990s, new media. While the term is sometimes used interchangeably with ethnographic film, visual anthropology also encompasses the anthropological study of visual representation, including areas such as performance, museums, art, and the production and reception of mass media. Visual representations from all cultures, such as sandpaintings, tattoos, sculptures and reliefs, cave paintings, scrimshaw, jewelry, hieroglyphics, paintings and photographs are included in the focus of visual anthropology.
Economic, political economic, applied and development.
Economic.
Economic anthropology attempts to explain human economic behavior in its widest historic, geographic and cultural scope. It has a complex relationship with the discipline of economics, of which it is highly critical. Its origins as a sub-field of anthropology begin with the Polish-British founder of Anthropology, Bronislaw Malinowski, and his French compatriot, Marcel Mauss, on the nature of gift-giving exchange (or reciprocity) as an alternative to market exchange. Economic Anthropology remains, for the most part, focused upon exchange. The school of thought derived from Marx and known as Political Economy focuses on production, in contrast. Economic Anthropologists have abandoned the primitivist niche they were relegated to by economists, and have now turned to examine corporations, banks, and the global financial system from an anthropological perspective.
Political economy.
Political economy in anthropology is the application of the theories and methods of Historical Materialism to the traditional concerns of anthropology, including, but not limited to, non-capitalist societies. Political Economy introduced questions of history and colonialism to ahistorical anthropological theories of social structure and culture. Three main areas of interest rapidly developed. The first of these areas was concerned with the "pre-capitalist" societies that were subject to evolutionary "tribal" stereotypes. Sahlins work on Hunter-gatherers as the 'original affluent society' did much to dissipate that image. The second area was concerned with the vast majority of the world's population at the time, the peasantry, many of whom were involved in complex revolutionary wars such as in Vietnam. The third area was on colonialism, imperialism, and the creation of the capitalist world-system. More recently, these Political Economists have more directly addressed issues of industrial (and post-industrial) capitalism around the world.
Applied.
Applied Anthropology refers to the application of the method and theory of anthropology to the analysis and solution of practical problems. It is a, "complex of related, research-based, instrumental methods which produce change or stability in specific cultural systems through the provision of data, initiation of direct action, and/or the formulation of policy". More simply, applied anthropology is the practical side of anthropological research; it includes researcher involvement and activism within the participating community. It is closely related to Development anthropology (distinct from the more critical Anthropology of development).
Development.
Anthropology of development tends to view development from a "critical" perspective. The kind of issues addressed and implications for the approach simply involve pondering why, if a key development goal is to alleviate poverty, is poverty increasing? Why is there such a gap between plans and outcomes? Why are those working in development so willing to disregard history and the lessons it might offer? Why is development so externally driven rather than having an internal basis? In short why does so much planned development fail?
Kinship, feminism, gender and sexuality.
Kinship.
"Kinship" can refer both to "the study of" the patterns of social relationships in one or more human cultures, or it can refer to "the patterns of social relationships" themselves. Over its history, anthropology has developed a number of related concepts and terms, such as descent, descent groups, lineages, affines, cognates and even fictive kinship. Broadly, kinship patterns may be considered to include people related both by descent (one's social relations during development), and also relatives by marriage.
Feminist.
Feminist anthropology is a four field approach to anthropology (archeological, biological, cultural, linguistic) that seeks to reduce male bias in research findings, anthropological hiring practices, and the scholarly production of knowledge. Anthropology engages often with feminists from non-Western traditions, whose perspectives and experiences can differ from those of white European and American feminists. Historically, such 'peripheral' perspectives have sometimes been marginalized and regarded as less valid or important than knowledge from the western world. Feminist anthropologists have claimed that their research helps to correct this systematic bias in mainstream feminist theory. Feminist anthropologists are centrally concerned with the construction of gender across societies. Feminist anthropology is inclusive of as a specialization.
Medical, nutritional, psychological, cognitive and transpersonal.
Medical.
Medical anthropology is an interdisciplinary field which studies "human health and disease, health care systems, and biocultural adaptation". Currently, research in medical anthropology is one of the main growth areas in the field of anthropology as a whole. It focuses on the following six basic fields:
Other subjects that have become central to medical anthropology worldwide are violence and social suffering (Farmer, 1999, 2003; Beneduce, 2010) as well as other issues that involve physical and psychological harm and suffering that are not a result of illness. On the other hand, there are fields that intersect with medical anthropology in terms of research methodology and theoretical production, such as "cultural psychiatry" and "transcultural psychiatry" or "ethnopsychiatry".
Nutritional.
Nutritional anthropology is a synthetic concept that deals with the interplay between economic systems, nutritional status and food security, and how changes in the former affect the latter. If economic and environmental changes in a community affect access to food, food security, and dietary health, then this interplay between culture and biology is in turn connected to broader historical and economic trends associated with globalization. Nutritional status affects overall health status, work performance potential, and the overall potential for economic development (either in terms of human development or traditional western models) for any given group of people.
Psychological.
Psychological anthropology is an interdisciplinary subfield of anthropology that studies the interaction of cultural and mental processes. This subfield tends to focus on ways in which humans' development and enculturation within a particular cultural group—with its own history, language, practices, and conceptual categories—shape processes of human cognition, emotion, perception, motivation, and mental health. It also examines how the understanding of cognition, emotion, motivation, and similar psychological processes inform or constrain our models of cultural and social processes.
Cognitive.
Cognitive anthropology seeks to explain patterns of shared knowledge, cultural innovation, and transmission over time and space using the methods and theories of the cognitive sciences (especially experimental psychology and evolutionary biology) often through close collaboration with historians, ethnographers, archaeologists, linguists, musicologists and other specialists engaged in the description and interpretation of cultural forms. Cognitive anthropology is concerned with what people from different groups know and how that implicit knowledge changes the way people perceive and relate to the world around them.
Transpersonal.
Transpersonal anthropology studies the relationship between altered states of consciousness and culture. As with transpersonal psychology, the field is much concerned with altered states of consciousness (ASC) and transpersonal experience. However, the field differs from mainstream transpersonal psychology in taking more cognizance of cross-cultural issues—for instance, the roles of myth, ritual, diet, and texts in evoking and interpreting extraordinary experiences (Young and Goulet 1994).
Political and legal.
Political.
Political anthropology concerns the structure of political systems, looked at from the basis of the structure of societies. Political anthropology developed as a discipline concerned primarily with politics in stateless societies, a new development started from the 1960s, and is still unfolding: anthropologists started increasingly to study more "complex" social settings in which the presence of states, bureaucracies and markets entered both ethnographic accounts and analysis of local phenomena. The turn towards complex societies meant that political themes were taken up at two main levels. First of all, anthropologists continued to study political organization and political phenomena that lay outside the state-regulated sphere (as in patron-client relations or tribal political organization). Second of all, anthropologists slowly started to develop a disciplinary concern with states and their institutions (and of course on the relationship between formal and informal political institutions). An anthropology of the state developed, and it is a most thriving field today. Geertz' comparative work on "Negara", the Balinese state is an early, famous example.
Legal.
Legal anthropology, also known as Anthropology of Law specializes in "the cross-cultural study of social ordering". Earlier legal anthropological research often focused more narrowly on conflict management, crime, sanctions, or formal regulation. More recent applications include issues such as human rights, legal pluralism, and political uprisings.
Public.
Public Anthropology was created by Robert Borofsky, a professor at Hawaii Pacific University, to "demonstrate the ability of anthropology and anthropologists to effectively address problems beyond the discipline - illuminating larger social issues of our times as well as encouraging broad, public conversations about them with the explicit goal of fostering social change" ().
Nature, science and technology.
Cyborg.
Cyborg anthropology originated as a sub-focus group within the American Anthropological Association's annual meeting in 1993. The sub-group was very closely related to STS and the Society for the Social Studies of Science. Donna Haraway's 1985 "Cyborg Manifesto" could be considered the founding document of cyborg anthropology by first exploring the philosophical and sociological ramifications of the term. Cyborg anthropology studies humankind and its relations with the technological systems it has built, specifically modern technological systems that have reflexively shaped notions of what it means to be human beings.
Digital.
Digital anthropology is the study of the relationship between humans and digital-era technology, and extends to various areas where anthropology and technology intersect. It is sometimes grouped with sociocultural anthropology, and sometimes considered part of material culture. The field is new, and thus has a variety of names with a variety of emphases. These include techno-anthropology, digital ethnography, cyberanthropology, and virtual anthropology.
Ecological.
Ecological anthropology is defined as the "study of cultural adaptations to environments". The sub-field is also defined as, "the study of relationships between a population of humans and their biophysical environment". The focus of its research concerns "how cultural beliefs and practices helped human populations adapt to their environments, and how people used elements of their culture to maintain their ecosystems."
Environmental.
Environmental anthropology is a sub-specialty within the field of anthropology that takes an active role in examining the relationships between humans and their environment across space and time. The contemporary perspective of environmental anthropology, and arguably at least the backdrop, if not the focus of most of the ethnographies and cultural fieldworks of today, is political ecology. Many characterize this new perspective as more informed with culture, politics and power, globalization, localized issues, and more. The focus and data interpretation is often used for arguments for/against or creation of policy, and to prevent corporate exploitation and damage of land. Often, the observer has become an active part of the struggle either directly (organizing, participation) or indirectly (articles, documentaries, books, ethnographies). Such is the case with environmental justice advocate Melissa Checker and her relationship with the people of Hyde Park.
Historical.
Ethnohistory is the study of ethnographic cultures and indigenous customs by examining historical records. It is also the study of the history of various ethnic groups that may or may not exist today. Ethnohistory uses both historical and ethnographic data as its foundation. Its historical methods and materials go beyond the standard use of documents and manuscripts. Practitioners recognize the utility of such source material as maps, music, paintings, photography, folklore, oral tradition, site exploration, archaeological materials, museum collections, enduring customs, language, and place names.
Religion.
The anthropology of religion involves the study of religious institutions in relation to other social institutions, and the comparison of religious beliefs and practices across cultures. Modern anthropology assumes that there is complete continuity between magical thinking and religion, and that every religion is a cultural product, created by the human community that worships it.
Urban.
Urban anthropology is concerned with issues of urbanization, poverty, and neoliberalism. Ulf Hannerz quotes a 1960s remark that traditional anthropologists were "a notoriously agoraphobic lot, anti-urban by definition". Various social processes in the Western World as well as in the "Third World" (the latter being the habitual focus of attention of anthropologists) brought the attention of "specialists in 'other cultures'" closer to their homes. There are two principle approaches in urban anthropology: by examining the types of cities or examining the social issues within the cities. These two methods are overlapping and dependent of each other. By defining different types of cities, one would use social factors as well as economic and political factors to categorize the cities. By directly looking at the different social issues, one would also be studying how they affect the dynamic of the city.
Key topics by field: archaeological and biological.
Anthrozoology.
Anthrozoology (also called human–animal studies or HAS) is the study of interaction between living things. It is a modern interdisciplinary and burgeoning field that overlaps with a number of other disciplines, including anthropology, ethology, medicine, psychology, veterinary medicine and zoology. A major focus of anthrozoologic research is the quantifying of the positive effects of human-animal relationships on either party and the study of their interactions. It includes scholars from a diverse range of fields, including anthropology, sociology, biology, and philosophy.
Biocultural.
Biocultural anthropology is the scientific exploration of the relationships between human biology and culture. Physical anthropologists throughout the first half of the 20th century viewed this relationship from a racial perspective; that is, from the assumption that typological human biological differences lead to cultural differences. After World War II the emphasis began to shift toward an effort to explore the role culture plays in shaping human biology.
Evolutionary.
Evolutionary anthropology is the interdisciplinary study of the evolution of human physiology and human behaviour and the relation between hominins and non-hominin primates. Evolutionary anthropology is based in natural science and social science, combining the human development with socioeconomic factors. Evolutionary anthropology is concerned with both biological and cultural evolution of humans, past and present. It is based on a scientific approach, and brings together fields such as archaeology, behavioral ecology, psychology, primatology, and genetics. It is a dynamic and interdisciplinary field, drawing on many lines of evidence to understand the human experience, past and present.
Forensic.
Forensic anthropology is the application of the science of physical anthropology and human osteology in a legal setting, most often in criminal cases where the victim's remains are in the advanced stages of decomposition. A forensic anthropologist can assist in the identification of deceased individuals whose remains are decomposed, burned, mutilated or otherwise unrecognizable. The adjective "forensic" refers to the application of this subfield of science to a court of law.
Palaeoanthropology.
Paleoanthropology combines the disciplines of paleontology and physical anthropology. It is the study of ancient humans, as found in fossil hominid evidence such as petrifacted bones and footprints.
Organizations.
Contemporary anthropology is an established science with academic departments at most universities and colleges. The single largest organization of Anthropologists is the American Anthropological Association (AAA), which was founded in 1903. Membership is made up of anthropologists from around the globe.
In 1989, a group of European and American scholars in the field of anthropology established the European Association of Social Anthropologists (EASA) which serves as a major professional organization for anthropologists working in Europe. The EASA seeks to advance the status of anthropology in Europe and to increase visibility of marginalized anthropological traditions and thereby contribute to the project of a global anthropology or world anthropology.
Hundreds of other organizations exist in the various sub-fields of anthropology, sometimes divided up by nation or region, and many anthropologists work with collaborators in other disciplines, such as geology, physics, zoology, paleontology, anatomy, music theory, art history, sociology and so on, belonging to professional societies in those disciplines as well.
Controversial ethical stances.
Anthropologists, like other researchers (especially historians and scientists engaged in field research), have over time assisted state policies and projects, especially colonialism.
Some commentators have contended:
Ethics of cultural relativism.
At the same time, anthropologists urge, as part of their quest for scientific objectivity, cultural relativism, which has an influence on all the sub-fields of anthropology. This is the notion that particular cultures should not be judged by one culture's values or viewpoints, but that all cultures should be viewed as relative to each other. There should be no notions, in good anthropology, of one culture being better or worse than another culture.
Ethical commitments in anthropology include noticing and documenting genocide, infanticide, racism, mutilation including circumcision and subincision, and torture. Topics like racism, slavery or human sacrifice, therefore, attract anthropological attention and theories ranging from nutritional deficiencies to genes to acculturation have been proposed, not to mention theories of colonialism and many others as root causes of Man's inhumanity to man. To illustrate the depth of an anthropological approach, one can take just one of these topics, such as "racism" and find thousands of anthropological references, stretching across all the major and minor sub-fields.
Ethical stance to military involvement.
Anthropologists' involvement with the U.S. government, in particular, has caused bitter controversy within the discipline. Franz Boas publicly objected to US participation in World War I, and after the war he published a brief expose and condemnation of the participation of several American archaeologists in espionage in Mexico under their cover as scientists.
But by the 1940s, many of Boas' anthropologist contemporaries were active in the allied war effort against the "Axis" (Nazi Germany, Fascist Italy, and Imperial Japan). Many served in the armed forces, while others worked in intelligence (for example, Office of Strategic Services and the Office of War Information). At the same time, David H. Price's work on American anthropology during the Cold War provides detailed accounts of the pursuit and dismissal of several anthropologists from their jobs for communist sympathies.
Attempts to accuse anthropologists of complicity with the CIA and government intelligence activities during the Vietnam War years have turned up surprisingly little (although anthropologist Hugo Nutini was active in the stillborn Project Camelot). Many anthropologists (students and teachers) were active in the antiwar movement. Numerous resolutions condemning the war in all its aspects were passed overwhelmingly at the annual meetings of the American Anthropological Association (AAA).
Professional anthropological bodies often object to the use of anthropology for the benefit of the state. Their codes of ethics or statements may proscribe anthropologists from giving secret briefings. The Association of Social Anthropologists of the UK and Commonwealth (ASA) has called certain scholarship ethically dangerous. The AAA's current 'Statement of Professional Responsibility' clearly states that "in relation with their own government and with host governments ... no secret research, no secret reports or debriefings of any kind should be agreed to or given."
Anthropologists, along with other social scientists, are working with the US military as part of the US Army's strategy in Afghanistan. The Christian Science Monitor reports that "Counterinsurgency efforts focus on better grasping and meeting local needs" in Afghanistan, under the "Human Terrain System" (HTS) program; in addition, HTS teams are working with the US military in Iraq. In 2009, the American Anthropological Association's Commission on the Engagement of Anthropology with the US Security and Intelligence Communities released its final report concluding, in part, that, "When ethnographic investigation is determined by military missions, not subject to external review, where data collection occurs in the context of war, integrated into the goals of counterinsurgency, and in a potentially coercive environment – all characteristic factors of the HTS concept and its application – it can no longer be considered a legitimate professional exercise of anthropology. In summary, while we stress that constructive engagement between anthropology and the military is possible, CEAUSSIC suggests that the AAA emphasize the incompatibility of HTS with disciplinary ethics and practice for job seekers and that it further recognize the problem of allowing HTS to define the meaning of "anthropology" within DoD."
Post–World War II developments.
Before WWII British 'social anthropology' and American 'cultural anthropology' were still distinct traditions. After the war, enough British and American anthropologists borrowed ideas and methodological approaches from one another that some began to speak of them collectively as 'sociocultural' anthropology.
Basic trends.
There are several characteristics that tend to unite anthropological work. One of the central characteristics is that anthropology tends to provide a comparatively more holistic account of phenomena and tends to be highly empirical. The quest for holism leads most anthropologists to study a particular place, problem or phenomenon in detail, using a variety of methods, over a more extensive period than normal in many parts of academia.
In the 1990s and 2000s (decade), calls for clarification of what constitutes a culture, of how an observer knows where his or her own culture ends and another begins, and other crucial topics in writing anthropology were heard. These dynamic relationships, between what can be observed on the ground, as opposed to what can be observed by compiling many local observations remain fundamental in any kind of anthropology, whether cultural, biological, linguistic or archaeological.
Biological anthropologists are interested in both human variation and in the possibility of human universals (behaviors, ideas or concepts shared by virtually all human cultures). They use many different methods of study, but modern population genetics, participant observation and other techniques often take anthropologists "into the field," which means traveling to a community in its own setting, to do something called "fieldwork." On the biological or physical side, human measurements, genetic samples, nutritional data may be gathered and published as articles or monographs.
Along with dividing up their project by theoretical emphasis, anthropologists typically divide the world up into relevant time periods and geographic regions. Human time on Earth is divided up into relevant cultural traditions based on material, such as the Paleolithic and the Neolithic, of particular use in archaeology. Further cultural subdivisions according to tool types, such as Olduwan or Mousterian or Levalloisian help archaeologists and other anthropologists in understanding major trends in the human past. Anthropologists and geographers share approaches to Culture regions as well, since mapping cultures is central to both sciences. By making comparisons across cultural traditions (time-based) and cultural regions (space-based), anthropologists have developed various kinds of comparative method, a central part of their science.
Commonalities between fields.
Because anthropology developed from so many different enterprises (see History of Anthropology), including but not limited to fossil-hunting, exploring, documentary film-making, paleontology, primatology, antiquity dealings and curatorship, philology, etymology, genetics, regional analysis, ethnology, history, philosophy, and religious studies, it is difficult to characterize the entire field in a brief article, although attempts to write histories of the entire field have been made.
Some authors argue that anthropology originated and developed as the study of "other cultures", both in terms of time (past societies) and space (non-European/non-Western societies). For example, the classic of urban anthropology, Ulf Hannerz in the introduction to his seminal "Exploring the City: Inquiries Toward an Urban Anthropology" mentions that the "Third World" had habitually received most of attention; anthropologists who traditionally specialized in "other cultures" looked for them far away and started to look "across the tracks" only in late 1960s.
Now there exist many works focusing on peoples and topics very close to the author's "home". It is also argued that other fields of study, like History and Sociology, on the contrary focus disproportionately on the West.
In France, the study of Western societies has been traditionally left to sociologists, but this is increasingly changing, starting in the 1970s from scholars like Isac Chiva and journals like "Terrain" ("fieldwork"), and developing with the center founded by Marc Augé ("Le Centre d'anthropologie des mondes contemporains", the Anthropological Research Center of Contemporary Societies).
Since the 1980s it has become common for social and cultural anthropologists to set ethnographic research in the North Atlantic region, frequently examining the connections between locations rather than limiting research to a single locale. There has also been a related shift toward broadening the focus beyond the daily life of ordinary people; increasingly, research is set in settings such as scientific laboratories, social movements, governmental and nongovernmental organizations and businesses.
See also.
</dl>
Further reading.
Dictionaries and encyclopedias.
</dl>
Fieldnotes and memoirs.
</dl>
Histories.
</dl>
Textbooks and key theoretical works.
</dl>

</doc>
<doc id="737" url="http://en.wikipedia.org/wiki?curid=737" title="Afghanistan">
Afghanistan

Afghanistan (Pashto/Dari: افغانستان, "Afġānistān"), officially the Islamic Republic of Afghanistan, is a landlocked country located within South Asia and Central Asia. It has a population of approximately 31 million people, making it the 42nd most populous country in the world. It is bordered by Pakistan in the south and east; Iran in the west; Turkmenistan, Uzbekistan, and Tajikistan in the north; and China in the far northeast. Its territory covers 652,000 km2, making it the 41st largest country in the world.
Human habitation in Afghanistan dates back to the Middle Paleolithic Era, and the country's strategic location along the Silk Road connected it to the cultures of the Middle East and other parts of Asia. Through the ages the land has been home to various peoples and witnessed numerous military campaigns, notably by Alexander the Great, Muslim Arabs, Mongols, British, Soviet Russians, and in the modern-era by Western powers. The land also served as the source from which the Kushans, Hephthalites, Samanids, Saffarids, Ghaznavids, Ghorids, Khiljis, Mughals, Hotaks, Durranis, and others have risen to form major empires.
The political history of the modern state of Afghanistan began with the Hotak and Durrani dynasties in the 18th century. In the late 19th century, Afghanistan became a buffer state in the "Great Game" between British India and the Russian Empire. Following the 1919 Anglo-Afghan War, King Amanullah and King Mohammed Zahir Shah attempted modernization of the country. A series of coups in the 1970s was followed by a Soviet invasion and a series of civil wars that devastated much of the country.
Etymology.
The name "Afghānistān" (Persian: افغانستان‎, ]) is believed to be as old as the ethnonym "Afghan", which is documented in the 10th-century geography book "Hudud ul-'alam". The word Afghan comes from the Sanskrit word अवगाण (Avagāṇa); probably deriving from Aśvaka. The root name "Afghan" was used historically as a reference to the Pashtun people, and the suffix "-stan" means "place of" in Persian language. Therefore, Afghanistan translates to "land of the Afghans" that is "Land of the Pashtuns". The Constitution of Afghanistan states that "[t]he word Afghan shall apply to every citizen of Afghanistan."
History.
Excavations of prehistoric sites by Louis Dupree and others suggest that humans were living in what is now Afghanistan at least 50,000 years ago, and that farming communities in the area were among the earliest in the world. An important site of early historical activities, many believe that Afghanistan compares to Egypt in terms of the historical value of its archaeological sites.
The country sits at a unique nexus point where numerous civilizations have interacted and often fought. It has been home to various peoples through the ages, among them the ancient Iranian peoples who established the dominant role of Indo-Iranian languages in the region. At multiple points, the land has been incorporated within large regional empires, among them the Achaemenid Empire, the Macedonian Empire, the Indian Maurya Empire, and the Islamic Empire.
Many kingdoms have also risen to power in Afghanistan, such as the Greco-Bactrians, Kushans, Hephthalites, Kabul Shahis, Saffarids, Samanids, Ghaznavids, Ghurids, Khiljis, Kartids, Timurids, Mughals, and finally the Hotak and Durrani dynasties that marked the political origins of the modern state.
Pre-Islamic period.
Archaeological exploration done in the 20th century suggests that the geographical area of Afghanistan has been closely connected by culture and trade with its neighbors to the east, west, and north. Artifacts typical of the Paleolithic, Mesolithic, Neolithic, Bronze, and Iron ages have been found in Afghanistan. Urban civilization is believed to have begun as early as 3000 BCE, and the early city of Mundigak (near Kandahar in the south of the country) may have been a colony of the nearby Indus Valley Civilization.
After 2000 BCE, successive waves of semi-nomadic people from Central Asia began moving south into Afghanistan; among them were many Indo-European-speaking Indo-Iranians. These tribes later migrated further south to India, west to what is now Iran, and towards Europe via the area north of the Caspian Sea. The region as a whole was called Ariana.
The people shared similar culture with other Indo-Iranians. The ancient religion of Kafiristan survived here until the 19th century. Another religion, Zoroastrianism is believed by some to have originated in what is now Afghanistan between 1800 and 800 BCE, as its founder Zoroaster is thought to have lived and died in Balkh. Ancient Eastern Iranian languages may have been spoken in the region around the time of the rise of Zoroastrianism. By the middle of the 6th century BCE, the Achaemenid Persians overthrew the Medes and incorporated Arachosia, Aria, and Bactria within its eastern boundaries. An inscription on the tombstone of King Darius I of Persia mentions the Kabul Valley in a list of the 29 countries that he had conquered.
Alexander the Great and his Macedonian forces arrived to Afghanistan in 330 BCE after defeating Darius III of Persia a year earlier in the Battle of Gaugamela. Following Alexander's brief occupation, the successor state of the Seleucid Empire controlled the region as one of their easternmost territories until 305 BCE, when they gave much of it to the Indian Maurya Empire as part of an alliance treaty. The Mauryans introduced Buddhism and controlled the area south of the Hindu Kush until they were overthrown about 185 BCE. Their decline began 60 years after Ashoka's rule ended, leading to the Hellenistic reconquest of the region by the Greco-Bactrians. Much of it soon broke away from the Greco-Bactrians and became part of the Indo-Greek Kingdom. The Indo-Greeks were defeated and expelled by the Indo-Scythians in the late 2nd century BCE.
During the first century BCE, the Parthian Empire subjugated the region, but lost it to their Indo-Parthian vassals. In the mid-to-late first century CE the vast Kushan Empire, centered in modern Afghanistan, became great patrons of Buddhist culture, making Buddhism flourish throughout the region. The Kushans were defeated by the Sassanids in the 3rd century CE. Although the Indo-Sassanids continued to rule at least parts of the region. They were followed by the Kidarite Huns who, in turn, were replaced by the Hephthalites. By the 6th century CE, the successors to the Kushans and Hepthalites established a small dynasty called Kabul Shahi.
Islamization and Mongol invasion.
Before the 19th century, the northwestern area of Afghanistan was referred to by the regional name Khorasan. Two of the four capitals of Khorasan (Herat and Balkh) are now located in Afghanistan, while the regions of Kandahar, Zabulistan, Ghazni, Kabulistan, and Afghanistan formed the frontier between Khorasan and Hindustan.
Arab Muslims brought Islam to Herat and Zaranj in 642 CE and began spreading eastward; some of the native inhabitants they encountered accepted it while others revolted. The land was collectively recognized by the Arabs as al-Hind due to its cultural connection with Greater India. Before Islam was introduced, people of the region were multi-religious, including Zoroastrians, Buddhists, Surya and Nana worshipers, Jews, and others. The Zunbils and Kabul Shahi were first conquered in 870 CE by the Saffarid Muslims of Zaranj. Later, the Samanids extended their Islamic influence south of the Hindu Kush. It is reported that Muslims and non-Muslims still lived side by side in Kabul before the Ghaznavids rose to power in the 10th century.
Afghanistan became one of the main centers in the Muslim world during the Islamic Golden Age. By the 11th century, Mahmud of Ghazni defeated the remaining Hindu rulers and effectively Islamized the wider region, with the exception of Kafiristan. The Ghaznavid dynasty was defeated and replaced by the Ghurids, who expanded and advanced the already powerful Islamic empire. Some speculate that today's Nasher clan is a remnant of the Ghaznavid dynasty.
In 1219 AD, Genghis Khan and his Mongol army overran the region. His troops are said to have annihilated the Khorasanian cities of Herat and Balkh as well as Bamyan. The destruction caused by the Mongols forced many locals to return to an agrarian rural society. Mongol rule continued with the Ilkhanate in the northwest while the Khilji dynasty administered the Afghan tribal areas south of the Hindu Kush until the invasion of Timur, who established the Timurid dynasty in 1370. During the Ghaznavid, Ghurid, and Timurid eras, the region produced many fine Islamic architectural monuments and numerous scientific and literary works.
In the early 16th century, Babur arrived from Fergana and captured Kabul from the Arghun dynasty. From there he began dominating control of the central and eastern territories of Afghanistan. He remained in Kabulistan until 1526 when he invaded Delhi in India to replace the Lodi dynasty with the Mughal Empire. Between the 16th and 18th century, the Khanate of Bukhara, Safavids, and Mughals ruled parts of the territory.
Hotak dynasty and Durrani Empire.
In 1709, Mirwais Hotak, a Pashtun from Kandahar, successfully rebelled against the Persian Safavids. He overthrew and killed Gurgin Khan, and made Afghanistan independent. Mirwais died of a natural cause in 1715 and was succeeded by his brother Abdul Aziz, who was soon killed by Mirwais' son Mahmud for treason. Mahmud led the Afghan army in 1722 to the Persian capital of Isfahan, captured the city after the Battle of Gulnabad and proclaimed himself King of Persia. The Persians rejected Mahmud, and after the massacre of thousands of religious scholars, nobles, and members of the Safavid family, the Hotak dynasty was ousted from Persia by Nader Shah Afshar after the 1729 Battle of Damghan.
In 1738, Nader Shah and his forces captured Kandahar, the last Hotak stronghold, from Shah Hussain Hotak, at which point the incarcerated 16-year-old Ahmad Shah Durrani was freed and made the commander of an Afghan regiment. Soon after the Persian and Afghan forces invaded India. By 1747, the Afghans chose Durrani as their head of state. Durrani and his Afghan army conquered much of present-day Afghanistan, Pakistan, the Khorasan and Kohistan provinces of Iran, and Delhi in India. He defeated the Indian Maratha Empire, and one of his biggest victories was the 1761 Battle of Panipat.
In October 1772, Durrani died of a natural cause and was buried at a site now adjacent to the Shrine of the Cloak in Kandahar. He was succeeded by his son, Timur Shah, who transferred the capital of Afghanistan from Kandahar to Kabul in 1776. After Timur's death in 1793, the Durrani throne passed down to his son Zaman Shah, followed by Mahmud Shah, Shuja Shah and others.
The Afghan Empire was under threat in the early 19th century by the Persians in the west and the British-backed Sikhs in the east. Fateh Khan, leader of the Barakzai tribe, had installed 21 of his brothers in positions of power throughout the empire. After his death, they rebelled and divided up the provinces of the empire between themselves. During this turbulent period, Afghanistan had many temporary rulers until Dost Mohammad Khan declared himself emir in 1826. The Punjab region was lost to Ranjit Singh, who invaded Khyber Pakhtunkhwa and in 1834 captured the city of Peshawar. In 1837, during the Battle of Jamrud near the Khyber Pass, Akbar Khan and the Afghan army killed Sikh Commander Hari Singh Nalwa. By this time the British were advancing from the east and the first major conflict during the "Great Game" was initiated.
Western influence.
Following the 1842 defeat of the British-Indian forces and victory of the Afghans, the British established diplomatic relations with the Afghan government and withdrew all forces from the country. They returned during the Second Anglo-Afghan War in the late 1870s for about two years to assist Abdur Rahman Khan defeat Ayub Khan. The United Kingdom began to exercise a great deal of influence after this and even controlled the state's foreign policy. In 1893, Mortimer Durand made Amir Abdur Rahman Khan sign a controversial agreement in which the ethnic Pashtun and Baloch territories were divided by the Durand Line. This was a standard divide and rule policy of the British and would lead to strained relations, especially with the later new state of Pakistan.
After the Third Anglo-Afghan War and the signing of the Treaty of Rawalpindi in 1919, King Amanullah Khan declared Afghanistan a sovereign and fully independent state. He moved to end his country's traditional isolation by establishing diplomatic relations with the international community and, following a 1927–28 tour of Europe and Turkey, introduced several reforms intended to modernize his nation. A key force behind these reforms was Mahmud Tarzi, an ardent supporter of the education of women. He fought for Article 68 of Afghanistan's 1923 constitution, which made elementary education compulsory. The institution of slavery was abolished in 1923.
Some of the reforms that were actually put in place, such as the abolition of the traditional burqa for women and the opening of a number of co-educational schools, quickly alienated many tribal and religious leaders. Faced with overwhelming armed opposition, Amanullah Khan was forced to abdicate in January 1929 after Kabul fell to rebel forces led by Habibullah Kalakani. Prince Mohammed Nadir Shah, Amanullah's cousin, in turn defeated and killed Kalakani in November 1929, and was declared King Nadir Shah. He abandoned the reforms of Amanullah Khan in favor of a more gradual approach to modernisation but was assassinated in 1933 by Abdul Khaliq, a Hazara school student.
Mohammed Zahir Shah, Nadir Shah's 19-year-old son, succeeded to the throne and reigned from 1933 to 1973. Until 1946, Zahir Shah ruled with the assistance of his uncle, who held the post of Prime Minister and continued the policies of Nadir Shah. Another of Zahir Shah's uncles, Shah Mahmud Khan, became Prime Minister in 1946 and began an experiment allowing greater political freedom, but reversed the policy when it went further than he expected. He was replaced in 1953 by Mohammed Daoud Khan, the king's cousin and brother-in-law. Daoud Khan sought a closer relationship with the Soviet Union and a more distant one towards Pakistan. Afghanistan remained neutral and was neither a participant in World War II nor aligned with either power bloc in the Cold War. However, it was a beneficiary of the latter rivalry as both the Soviet Union and the United States vied for influence by building Afghanistan's main highways, airports, and other vital infrastructure. In 1973, while King Zahir Shah was on an official overseas visit, Daoud Khan launched a bloodless coup and became the first President of Afghanistan. In the meantime, Zulfikar Ali Bhutto got neighboring Pakistan involved in Afghanistan. Some experts suggest that Bhutto paved the way for the April 1978 Saur Revolution.
Derek Gregory argued in his book "The Colonial Present" that the makings of a failed state in Afghanistan had its roots in Western imperialism. The great game between the European powers over what was then the British possession of India, lead England and Russia to require a buffer zone between their imperial interests. A state was literally carved out of nothing, much the same way as it was all throughout Africa. (Stephen Howe, p. 13) Different ethnic groups, different languages and different ways of life were enmeshed together into a single state with little consideration of the effects of such policies. In this context, the creation of Afghanistan (like many other small states created by the European powers) had little to do with self-determination as it was claimed, but over geopolitics. Isah Bowman, a renowned, American geographer, is said to have championed the notion of many small states within Eastern Europe, Latin America and Africa to increase imperial competition, thus weakening their respective power in relation to the United States. (Painter and Jeffrey Ch 9)
Marxist revolution and Soviet war.
In April 1978, the communist People's Democratic Party of Afghanistan (PDPA) seized power in Afghanistan in the Saur Revolution. Within months, opponents of the communist government launched an uprising in eastern Afghanistan that quickly expanded into a civil war waged by guerrilla mujahideen against government forces countrywide. The Pakistani government provided these rebels with covert training centers, while the Soviet Union sent thousands of military advisers to support the PDPA government. Meanwhile, increasing friction between the competing factions of the PDPA — the dominant Khalq and the more moderate Parcham — resulted in the dismissal of Parchami cabinet members and the arrest of Parchami military officers under the pretext of a Parchami coup.
In September 1979, Nur Muhammad Taraki was assassinated in a coup within the PDPA orchestrated by fellow Khalq member Hafizullah Amin, who assumed the presidency. Distrusted by the Soviets, Amin was assassinated by Soviet special forces in December 1979. A Soviet-organized government, led by Parcham's Babrak Karmal but inclusive of both factions, filled the vacuum. Soviet troops were deployed to stabilize Afghanistan under Karmal in more substantial numbers, although the Soviet government did not expect to do most of the fighting in Afghanistan. As a result, however, the Soviets were now directly involved in what had been a domestic war in Afghanistan. The PDPA prohibited usury, declared equality of the sexes, and introducing women to political life.
The United States has been supporting anti-Soviet forces (mujahideen) as early as mid-1979. Billions in cash and weapons, which included over two thousand FIM-92 Stinger surface-to-air missiles, were provided by the United States and Saudi Arabia to Pakistan.
The Soviet war in Afghanistan resulted in the deaths of over 1 million Afghans, mostly civilians, and the creation of about 6million refugees who fled Afghanistan, mainly to Pakistan and Iran. Faced with mounting international pressure and numerous casualties, the Soviets withdrew in 1989 but continued to support Afghan President Mohammad Najibullah until 1992.
Civil war.
From 1989 until 1992, Najibullah's government tried to solve the ongoing civil war with economic and military aid, but without Soviet troops on the ground. Najibullah tried to build support for his government by portraying his government as Islamic, and in the 1990 constitution the country officially became an Islamic state and all references of communism were removed. Nevertheless, Najibullah did not win any significant support, and with the dissolution of the Soviet Union in December 1991, he was left without foreign aid. This, coupled with the internal collapse of his government, led to his ousting from power in April 1992. After the fall of Najibullah's government in 1992, the post-communist Islamic State of Afghanistan was established by the Peshawar Accord, a peace and power-sharing agreement under which all the Afghan parties were united in April 1992, except for the Pakistani supported Hezb-e Islami of Gulbuddin Hekmatyar. Hekmatyar started a bombardment campaign against the capital city Kabul, which marked the beginning of a new phase in the war.
Saudi Arabia and Iran supported different Afghan militias and instability quickly developed. The conflict between the two militias soon escalated into a full-scale war.
Due to the sudden initiation of the war, working government departments, police units, and a system of justice and accountability for the newly created Islamic State of Afghanistan did not have time to form. Atrocities were committed by individuals of the different armed factions while Kabul descended into lawlessness and chaos. Because of the chaos, some leaders increasingly had only nominal control over their (sub-)commanders. For civilians there was little security from murder, rape, and extortion. An estimated 25,000 people died during the most intense period of bombardment by Hekmatyar's Hezb-i Islami and the Junbish-i Milli forces of Abdul Rashid Dostum, who had created an alliance with Hekmatyar in 1994. Half a million people fled Afghanistan.
Southern and eastern Afghanistan were under the control of local commanders such as Gul Agha Sherzai and others. In 1994, the Taliban (a movement originating from Jamiat Ulema-e-Islam-run religious schools for Afghan refugees in Pakistan) also developed in Afghanistan as a political-religious force. The Taliban took control of Kabul and several provinces in southern and central Afghanistan in 1994 and forced the surrender of dozens of local Pashtun leaders.
In late 1994, forces of Ahmad Shah Massoud held on to Kabul and bombardment of the city came to a halt. The Islamic State government took steps to open courts, restore law and order, and initiate a nationwide political process with the goal of national consolidation and democratic elections. Massoud invited Taliban leaders to join the process but they refused.
Taliban Emirate and Northern Alliance.
The Taliban's early victories in 1994 were followed by a series of defeats that resulted in heavy losses that led analysts to believe the Taliban movement had run its course. The Taliban started shelling Kabul in early 1995 but were repelled by forces under Massoud.
On 26 September 1996, as the Taliban, with military support from Pakistan and financial support from Saudi Arabia, prepared for another major offensive, Massoud ordered a full retreat from Kabul. The Taliban seized Kabul on 27 September 1996, and established the Islamic Emirate of Afghanistan. They imposed on the parts of Afghanistan under their control their political and judicial interpretation of Islam, issuing edicts especially targeting women. According to Physicians for Human Rights (PHR), "no other regime in the world has methodically and violently forced half of its population into virtual house arrest, prohibiting them on pain of physical punishment."
After the fall of Kabul to the Taliban, Massoud and Dostum created the United Front (Northern Alliance). The United Front included Massoud's predominantly Tajik forces, Dostum's Uzbek forces, and Hazara and Pashtun factions under leaders such as Haji Mohammad Mohaqiq, Abdul Haq, and Abdul Qadir. The Taliban defeated Dostum's forces during the Battles of Mazar-i-Sharif (1997–98). The Taliban committed systematic massacres against civilians in northern and western Afghanistan
Pakistan's Chief of Army Staff, Pervez Musharraf, was responsible for sending tens of thousands of Pakistanis to fight alongside the Taliban and bin Laden against Northern Alliance forces. In 2001 alone, there were believed to be 28,000 Pakistani nationals fighting inside Afghanistan.<ref name="Ahmed Rashid/The Telegraph"></ref> From 1996 to 2001, the al-Qaeda network of Osama bin Laden and Ayman al-Zawahiri was harbored by the Taliban in Afghanistan, and bin Laden sent thousands of Arab recruits to fight against the United Front.
Massoud remained the only leader of the United Front in Afghanistan. In the areas under his control, Massoud set up democratic institutions and signed the Women's Rights Declaration. The fighting also caused around 1 million people to flee Taliban controlled areas. From 1990 to September 2001, 400,000 Afghan civilians have reportedly died in the wars.
On 9 September 2001, Massoud was assassinated by two French-speaking Arab suicide attackers inside Afghanistan, and two days later the September 11 attacks were carried out in the United States. The US government identified Osama bin Laden and al-Qaeda as the perpetrators of the attacks, and demanded that the Taliban hand over bin Laden. After refusing to comply, the October 2001 Operation Enduring Freedom was launched. During the initial invasion, US and UK forces bombed parts of Afghanistan and worked with ground forces of the Northern Alliance to remove the Taliban from power and destroy al-Qaeda training camps.
Recent history (2002–present).
In December 2001, after the Taliban government was toppled and the new Afghan government under Hamid Karzai was formed, the International Security Assistance Force (ISAF) was established by the UN Security Council to help assist the Karzai administration and provide basic security. Taliban forces also began regrouping inside Pakistan, while more coalition troops entered Afghanistan and began rebuilding the war-torn country.
Shortly after their fall from power, the Taliban began an insurgency to regain control of Afghanistan. Over the next decade, ISAF and Afghan troops led many offensives against the Taliban but failed to fully defeat them. Afghanistan remained one of the poorest countries in the world due to a lack of foreign investment, government corruption, and the Taliban insurgency.
Meanwhile, the Afghan government was able to build some democratic structures, and, on December 7, 2004, the country changed its name to the Islamic Republic of Afghanistan. Attempts were made, often with the support of foreign donor countries, to improve the country's economy, healthcare, education, transport, and agriculture. ISAF forces also began to train the Afghan armed forces and police. In the decade following 2002, over five million Afghan refugees were repatriated to the country, including many who were forcefully deported from Western countries.
By 2009, a Taliban-led shadow government began to form in many parts of the country. US President Barack Obama announced that the U.S. would deploy another 30,000 U.S. soldiers to the country in 2010 for a period of two years. In 2010, Karzai attempted to hold peace negotiations with the Taliban and other groups, but these groups refused to attend and bombings, assassinations, and ambushes intensified.
After the May 2011 death of Osama bin Laden in Pakistan, many prominent Afghan figures were assassinated, Afghanistan–Pakistan border skirmishes intensified, and many large scale attacks by the Pakistani-based Haqqani Network took place across Afghanistan. The United States warned the Pakistani government of possible military action within Pakistan if the government refused to attack these forces in the Federally Administered Tribal Areas, as the United States blamed rogue elements within the Pakistani government for the increased attacks. The Pakistani Army began to intensify their attacks against these groups as part of the War in North-West Pakistan.
Following the 2014 presidential election President Hamid Karzai left power and Ashraf Ghani became President on 29 September 2014. The US war in Afghanistan (America's longest war) officially ended on December 28, 2014. However, thousands of US-led NATO troops have remained in the country to train and advise Afghan government forces.
Geography.
A landlocked mountainous country with plains in the north and southwest, Afghanistan is located within South Asia and Central Asia. It is part of the US coined Greater Middle East Muslim world, which lies between latitudes 29° N and 39° N, and longitudes 60° E and 75° E. The country's highest point is Noshaq, at 7492 m above sea level. It has a continental climate with harsh winters in the central highlands, the glaciated northeast (around Nuristan), and the Wakhan Corridor, where the average temperature in January is below -15 C, and hot summers in the low-lying areas of the Sistan Basin of the southwest, the Jalalabad basin in the east, and the Turkestan plains along the Amu River in the north, where temperatures average over 35 C in July.
Despite having numerous rivers and reservoirs, large parts of the country are dry. The endorheic Sistan Basin is one of the driest regions in the world. Aside from the usual rainfall, Afghanistan receives snow during the winter in the Hindu Kush and Pamir Mountains, and the melting snow in the spring season enters the rivers, lakes, and streams. However, two-thirds of the country's water flows into the neighboring countries of Iran, Pakistan, and Turkmenistan. The state needs more than US$ to rehabilitate its irrigation systems so that the water is properly managed.
The northeastern Hindu Kush mountain range, in and around the Badakhshan Province of Afghanistan, is in a geologically active area where earthquakes may occur almost every year. They can be deadly and destructive sometimes, causing landslides in some parts or avalanches during the winter. The last strong earthquakes were in 1998, which killed about 6,000 people in Badakhshan near Tajikistan. This was followed by the 2002 Hindu Kush earthquakes in which over 150 people were killed and over 1,000 injured. A 2010 earthquake left 11 Afghans dead, over 70 injured, and more than 2,000 houses destroyed.
The country's natural resources include: coal, copper, iron ore, lithium, uranium, rare earth elements, chromite, gold, zinc, talc, barites, sulfur, lead, marble, precious and semi-precious stones, natural gas, and petroleum, among other things. In 2010, US and Afghan government officials estimated that untapped mineral deposits located in 2007 by the US Geological Survey are worth between $900 bn and $3 trillion.
At 652230 km2, Afghanistan is the world's 41st largest country, slightly bigger than France and smaller than Burma, about the size of Texas in the United States. It borders Pakistan in the south and east; Iran in the west; Turkmenistan, Uzbekistan, and Tajikistan in the north; and China in the far east.
Demographics.
s of 2012[ [update]], the population of Afghanistan is around 31,108,077, which includes the roughly 2.7 million Afghan refugees still living in Pakistan and Iran. In 1979, the population was reported to be about 15.5 million. The only city with over a million residents is its capital, Kabul. Other large cities in the country are, in order of population size, Kandahar, Herat, Mazar-i-Sharif, Jalalabad, Lashkar Gah, Taloqan, Khost, Sheberghan, and Ghazni. Urban areas are experiencing rapid population growth following the return of over 5 million expats. According to the Population Reference Bureau, the Afghan population is estimated to increase to 82 million by 2050.
Ethnic groups.
Afghanistan is a multiethnic society, and its historical status as a crossroads has contributed significantly to its diverse ethnic makeup. The population of the country is divided into a wide variety of ethnolinguistic groups. Because a systematic census has not been held in the nation in decades, exact figures about the size and composition of the various ethnic groups are unavailable. An approximate distribution of the ethnic groups is shown in the chart below:
Languages.
Pashto and Dari are the official languages of Afghanistan; bilingualism is very common. Both are Indo-European languages from the Iranian languages sub-family. Dari (Afghan Persian) has long been the prestige language and a lingua franca for inter-ethnic communication. It is the native tongue of the Tajiks, Hazaras, Aimaks, and Kizilbash. Pashto is the native tongue of the Pashtuns, although many Pashtuns often use Dari and some non-Pashtuns are fluent in Pashto.
Other languages, including Uzbek, Arabic, Turkmen, Balochi, Pashayi, and Nuristani languages (Ashkunu, Kamkata-viri, Vasi-vari, Tregami, and Kalasha-ala), are the native tongues of minority groups across the country and have official status in the regions where they are widely spoken. Minor languages also include Pamiri (Shughni, Munji, Ishkashimi, and Wakhi), Brahui, Hindko, and Kyrgyz. A small percentage of Afghans are also fluent in Urdu, English, and other languages.
Religions.
Over 99% of the Afghan population is Muslim; approximately 80–85% are from the Sunni branch, 15–19% are Shia. Until the 1890s, the region around Nuristan was known as Kafiristan (land of the kafirs (unbelievers)) because of its non-Muslim inhabitants, the Nuristanis, an ethnically distinct people whose religious practices included animism, polytheism, and shamanism. Thousands of Afghan Sikhs and Hindus are also found in the major cities. There was a small Jewish community in Afghanistan who had emigrated to Israel and the United States by the end of the twentieth century; only one Jew, Zablon Simintov, remained by 2005.
Governance.
Afghanistan is an Islamic republic consisting of three branches, the executive, legislative, and judicial. The nation was led by Hamid Karzai as the President and leader since late 2001 till 2014. Currently the new president is Ashraf Ghani with Abdul Rashid Dostum and Sarwar Danish as vise presidents. Abdullah Abdullah serves as the chief executive officer (CEO). The National Assembly is the legislature, a bicameral body having two chambers, the House of the People and the House of Elders.
The Supreme Court is led by Chief Justice Abdul Salam Azimi, a former university professor who had been a legal advisor to the president. The current court is seen as more moderate and led by more technocrats than the previous one, which was dominated by fundamentalist religious figures such as Chief Justice Faisal Ahmad Shinwari, who issued several controversial rulings, including seeking to place a limit on the rights of women.
According to Transparency International's 2010 corruption perceptions index results, Afghanistan was ranked as the third most corrupt country in the world. A January 2010 report published by the United Nations Office on Drugs and Crime revealed that bribery consumed an amount equal to 23% of the GDP of the nation. A number of government ministries are believed to be rife with corruption, and while President Karzai vowed to tackle the problem in late 2009 by stating that "individuals who are involved in corruption will have no place in the government", top government officials were stealing and misusing hundreds of millions of dollars through the Kabul Bank. Although the nation's institutions are newly formed and steps have been taken to arrest some, the United States warned that aid to Afghanistan would be greatly reduced if the corruption is not stopped.
Elections and parties.
The 2004 Afghan presidential election was relatively peaceful, in which Hamid Karzai won in the first round with 55.4% of the votes. However, the 2009 presidential election was characterized by lack of security, low voter turnout, and widespread electoral fraud. The vote, along with elections for 420 provincial council seats, took place in August 2009, but remained unresolved during a lengthy period of vote counting and fraud investigation.
Two months later, under international pressure, a second round run-off vote between Karzai and remaining challenger Abdullah was announced, but a few days later Abdullah announced that he would not participate in the 7 November run-off because his demands for changes in the electoral commission had not been met. The next day, officials of the election commission cancelled the run-off and declared Hamid Karzai as President for another five-year term.
In the 2005 parliamentary election, among the elected officials were former mujahideen, Islamic fundamentalists, warlords, communists, reformists, and several Taliban associates. In the same period, Afghanistan reached to the 30th highest nation in terms of female representation in parliament. The last parliamentary election was held in September 2010, but due to disputes and investigation of fraud, the swearing-in ceremony took place in late January 2011. The 2014 presidential election ended with Ashraf Ghani winning by 56.44% votes.
Administrative divisions.
Afghanistan is administratively divided into 34 provinces ("wilayats"), with each province having its own capital and a provincial administration. The provinces are further divided into about 398 smaller provincial districts, each of which normally covers a city or a number of villages. Each district is represented by a district governor.
The provincial governors are appointed by the President of Afghanistan and the district governors are selected by the provincial governors. The provincial governors are representatives of the central government in Kabul and are responsible for all administrative and formal issues within their provinces. There are also provincial councils that are elected through direct and general elections for a period of four years. The functions of provincial councils are to take part in provincial development planning and to participate in the monitoring and appraisal of other provincial governance institutions.
According to article 140 of the constitution and the presidential decree on electoral law, mayors of cities should be elected through free and direct elections for a four-year term. However, due to huge election costs, mayoral and municipal elections have never been held. Instead, mayors have been appointed by the government. In the capital city of Kabul, the mayor is appointed by the President of Afghanistan.
The following is a list of all the 34 provinces in alphabetical order:
Foreign relations and military.
The Afghan Ministry of Foreign Affairs is in charge of maintaining the foreign relations of Afghanistan. The state has been a member of the United Nations since 1946. It enjoys strong economic relations with a number of NATO and allied states, particularly the United States, United Kingdom, Germany and Turkey. In 2012, the United States designated Afghanistan as a major non-NATO ally and created the U.S.–Afghanistan Strategic Partnership Agreement. Afghanistan also has friendly diplomatic relations with neighboring Pakistan, Iran, Turkmenistan, Uzbekistan, Tajikistan, and China, and with regional states such as India, Bangladesh, Kazakhstan, Russia, the UAE, Saudi Arabia, Iraq, Egypt, Japan, and South Korea. It continues to develop diplomatic relations with other countries around the world.
The United Nations Assistance Mission in Afghanistan (UNAMA) was established in 2002 under United Nations Security Council Resolution 1401 in order to help the country recover from decades of war. Today, a number of NATO member states deploy about 38,000 troops in Afghanistan as part of the International Security Assistance Force (ISAF). Its main purpose is to train the Afghan National Security Forces (ANSF). The Afghan Armed Forces are under the Ministry of Defense, which includes the Afghan National Army (ANA) and the Afghan Air Force (AAF). The ANA is divided into 7 major Corps, with the 201st Selab ("Flood") in Kabul followed by the 203rd in Gardez, 205th Atul ("Hero") in Kandahar, 207th in Herat, 209th in Mazar-i-Sharif, and the 215th in Lashkar Gah. The ANA also has a commando brigade, which was established in 2007. The Afghan Defense University (ADU) houses various educational establishments for the Afghan Armed Forces, including the National Military Academy of Afghanistan.
Sharia law.
The National Directorate of Security (NDS) is the nation's domestic intelligence agency, which operates similar to that of the United States Department of Homeland Security (DHS) and has between 15,000 to 30,000 employees. The nation also has about 126,000 national police officers, with plans to recruit more so that the total number can reach 160,000. The Afghan National Police (ANP) is under the Ministry of the Interior and serves as a single law enforcement agency all across the country. The Afghan National Civil Order Police is the main branch of the ANP, which is divided into five Brigades, each commanded by a Brigadier General. These brigades are stationed in Kabul, Gardez, Kandahar, Herat, and Mazar-i-Sharif. Every province has an appointed provincial Chief of Police who is responsible for law enforcement throughout the province.
The police receive most of their training from Western forces under the NATO Training Mission-Afghanistan. According to a 2009 news report, a large proportion of police officers were illiterate and accused of demanding bribes. Jack Kem, deputy to the commander of NATO Training Mission Afghanistan and Combined Security Transition Command Afghanistan, stated that the literacy rate in the ANP would rise to over 50% by January 2012. What began as a voluntary literacy program became mandatory for basic police training in early 2011. Approximately 17% of them tested positive for illegal drug use. In 2009, President Karzai created two anti-corruption units within the Interior Ministry. Former Interior Minister Hanif Atmar said that security officials from the US (FBI), Britain (Scotland Yard), and the European Union will train prosecutors in the unit.
The southern and eastern parts of Afghanistan are the most dangerous due to militant activities and the flourishing drug trade. These particular areas are sometimes patrolled by Taliban insurgents, who often plant improvised explosive devices (IEDs) on roads and carry out suicide bombings. Kidnapping and robberies are also reported. Every year many Afghan police officers are killed in the line of duty in these areas. The Afghan Border Police (ABP) are responsible for protecting the nation's airports and borders, especially the disputed Durand Line border, which is often used by members of criminal organizations and terrorists for their illegal activities. A report in 2011 suggested that up to 3 million people were involved in the illegal drug business in Afghanistan. Many of the attacks on government employees may be ordered by powerful mafia groups. Drugs from Afghanistan are exported to neighboring countries and worldwide. The Afghan Ministry of Counter Narcotics is tasked to deal with these issues by bringing to justice major drug traffickers.
Economy.
Afghanistan is an impoverished and least developed country, one of the world's poorest because of decades of war and lack of foreign investment. s of 2013[ [update]], the nation's GDP stands at about $45.3 billion with an exchange rate of $20.65 billion, and the GDP per capita is $1,100. The country's exports totaled $2.6 billion in 2010. Its unemployment rate is about 35% and roughly the same percentage of its citizens live below the poverty line. According to a 2009 report, about 42% of the population lives on less than $1 a day. The nation has less than $1.5 billion in external debt and is recovering with the assistance of the world community.
The Afghan economy has been growing at about 10% per year in the last decade, which is due to the infusion of over $50 billion in international aid and remittances from Afghan expats. It is also due to improvements made to the transportation system and agricultural production, which is the backbone of the nation's economy. The country is known for producing some of the finest pomegranates, grapes, apricots, melons, and several other fresh and dry fruits, including nuts. Many sources indicate that as much as 11% or more of Afghanistan's economy is derived from the cultivation and sale of opium, and Afghanistan is widely considered the world's largest producer of opium despite Afghan government and international efforts to eradicate the crop.
While the nation's current account deficit is largely financed with donor money, only a small portion is provided directly to the government budget. The rest is provided to non-budgetary expenditure and donor-designated projects through the United Nations system and non-governmental organizations. The Afghan Ministry of Finance is focusing on improved revenue collection and public sector expenditure discipline. For example, government revenues increased 31% to $1.7 billion from March 2010 to March 2011.
Da Afghanistan Bank serves as the central bank of the nation and the "Afghani" (AFN) is the national currency, with an exchange rate of about 47 Afghanis to 1 US dollar. Since 2003, over 16 new banks have opened in the country, including Afghanistan International Bank, Kabul Bank, Azizi Bank, Pashtany Bank, Standard Chartered Bank, and First Micro Finance Bank.
One of the main drivers for the current economic recovery is the return of over 5 million expatriates, who brought with them fresh energy, entrepreneurship and wealth-creating skills as well as much needed funds to start up businesses. For the first time since the 1970s, Afghans have involved themselves in construction, one of the largest industries in the country. Some of the major national construction projects include the $35 billion "New Kabul City" next to the capital, the "Ghazi Amanullah Khan City" near Jalalabad, and the "Aino Mena" in Kandahar. Similar development projects have also begun in Herat, Mazar-e-Sharif, and other cities.
In addition, a number of companies and small factories began operating in different parts of the country, which not only provide revenues to the government but also create new jobs. Improvements to the business environment have resulted in more than $1.5 billion in telecom investment and created more than 100,000 jobs since 2003. Afghan rugs are becoming popular again, allowing many carpet dealers around the country to hire more workers.
Afghanistan is a member of SAARC, ECO, and OIC. It holds an observer status in SCO. Foreign Minister Zalmai Rassoul told the media in 2011 that his nation's "goal is to achieve an Afghan economy whose growth is based on trade, private enterprise and investment". Experts believe that this will revolutionize the economy of the region. Opium production in Afghanistan soared to a record in 2007 with about 3 million people reported to be involved in the business, but then declined significantly in the years following. The government started programs to help reduce poppy cultivation, and by 2010 it was reported that 24 out of the 34 provinces were free from poppy growing.
In June 2012, India advocated for private investments in the resource rich country and the creation of a suitable environment therefor.
Mining.
Michael E. O'Hanlon of the Brookings Institution estimated that if Afghanistan generates about $10 bn per year from its mineral deposits, its gross national product would double and provide long-term funding for Afghan security forces and other critical needs. The United States Geological Survey (USGS) estimated in 2006 that northern Afghanistan has an average 2.9 billion (bn) barrels (bbl) of crude oil, 15.7 trillion cubic feet (15700 ft3 bn m3) of natural gas, and 562 million bbl of natural gas liquids. In December 2011, Afghanistan signed an oil exploration contract with China National Petroleum Corporation (CNPC) for the development of three oil fields along the Amu Darya river in the north.
Other reports show that the country has huge amounts of lithium, copper, gold, coal, iron ore, and other minerals. The Khanashin carbonatite in Helmand Province contains 1000000 MT of rare earth elements. In 2007, a 30-year lease was granted for the Aynak copper mine to the China Metallurgical Group for $3 billion, making it the biggest foreign investment and private business venture in Afghanistan's history. The state-run Steel Authority of India won the mining rights to develop the huge Hajigak iron ore deposit in central Afghanistan. Government officials estimate that 30% of the country's untapped mineral deposits are worth between $900 bn and $3 trillion. One official asserted that "this will become the backbone of the Afghan economy" and a Pentagon memo stated that Afghanistan could become the "Saudi Arabia of lithium". In a 2011 news story, the "CSM" reported, "The United States and other Western nations that have borne the brunt of the cost of the Afghan war have been conspicuously absent from the bidding process on Afghanistan's mineral deposits, leaving it mostly to regional powers."
Transportation.
Air.
Air transport in Afghanistan is provided by the national carrier, Ariana Afghan Airlines (AAA), and by private companies such as Afghan Jet International, East Horizon Airlines, Kam Air, Pamir Airways, and Safi Airways. Airlines from a number of countries also provide flights in and out of the country. These include Air India, Emirates, Gulf Air, Iran Aseman Airlines, Pakistan International Airlines, and Turkish Airlines.
The country has four international airports: Herat International Airport, Hamid Karzai International Airport (formerly Kabul International Airport), Kandahar International Airport, and Mazar-e Sharif International Airport. There are also around a dozen domestic airports with flights to Kabul or Herat.
Rail.
As of 2014, the country has only two rail links, one a 75 km line from Kheyrabad to the Uzbekistan border and the other a 10 km long line from Toraghundi to the Turkmenistan border. Both lines are used for freight only and there is no passenger service as of yet. There are various proposals for the construction of additional rail lines in the country. In 2013, the presidents of Afghanistan, Turkmenistan, and Uzbekistan attended the groundbreaking ceremony for a 225 km line between Turkmenistan-Andkhvoy-Mazar-i-Sharif-Kheyrabad. The line will link at Kheyrabad with the existing line to the Uzbekistan border. Plans exist for a rail line from Kabul to the eastern border town of Torkham, where it will connect with Pakistan Railways. There are also plans to finish a rail line between Khaf, Iran and Herat, Afghanistan.
Roads.
Traveling by bus in Afghanistan remains dangerous due to careless and intoxicated bus drivers as well as militant activities. The buses are usually older model Mercedes-Benz and owned by private companies. Serious traffic accidents are common on Afghan roads and highways, particularly on the Kabul–Kandahar and the Kabul–Jalalabad Road.
Newer automobiles have recently become more widely available after the rebuilding of roads and highways. They are imported from the United Arab Emirates through Pakistan and Iran. s of 2012[ [update]], vehicles more than 10 years old are banned from being imported into the country. The development of the nation's road network is a major boost for the economy due to trade with neighboring countries. Postal services in Afghanistan are provided by the publicly owned Afghan Post and private companies such as FedEx, DHL, and others.
Communication.
Telecommunication services in the country are provided by Afghan Wireless, Etisalat, Roshan, MTN Group, and Afghan Telecom. In 2006, the Afghan Ministry of Communications signed a $64.5 million agreement with ZTE for the establishment of a countrywide optical fiber cable network. s of 2011[ [update]], Afghanistan had around 17 million GSM phone subscribers and over 1 million internet users, but only had about 75,000 fixed telephone lines and a little over 190,000 CDMA subscribers. 3G services are provided by Etisalat and MTN Group. In 2014, Afghanistan leased a space satellite from Eutelsat, called AFGHANSAT 1.
Health.
According to the Human Development Index, Afghanistan is the 15th least developed country in the world. The average life expectancy is estimated to be around 60 years for both sexes. The country has the ninth highest total fertility rate in the world, at 5.64 children born/woman (according to 2012 estimates). It has one of the highest maternal mortality rate in the world, estimated in 2010 at 460 deaths/100,000 live births, and the highest infant mortality rate in the world (deaths of babies under one year), estimated in 2012 to be 119.41 deaths/1,000 live births. Data from 2010 suggest that one in ten children die before they are five years old. The Ministry of Public Health plans to cut the infant mortality rate to 400 for every 100,000 live births before 2020. The country currently has more than 3,000 midwives, with an additional 300 to 400 being trained each year.
A number of hospitals and clinics have been built over the last decade, with the most advanced treatments being available in Kabul. The French Medical Institute for Children and Indira Gandhi Childrens Hospital in Kabul are the leading children's hospitals in the country. Some of the other main hospitals in Kabul include the 350-bed Jamhuriat Hospital and the Jinnah Hospital, which is still under construction. There are also a number of well-equipped military-controlled hospitals in different regions of the country.
It was reported in 2006 that nearly 60% of the population lives within a two-hour walk of the nearest health facility, up from 9% in 2002. The latest surveys show that 57% of Afghans say they have good or very good access to clinics or hospitals. The nation has one of the highest incidences of people with disabilities, with around a million people affected. About 80,000 people are missing limbs; most of these were injured by landmines. Non-governmental charities such as Save the Children and Mahboba's Promise assist orphans in association with governmental structures. Demographic and Health Surveys is working with the Indian Institute of Health Management Research and others to conduct a survey in Afghanistan focusing on maternal death, among other things.
Education.
Education in the country includes K–12 and higher education, which is supervised by the Ministry of Education and the Ministry of Higher Education. The nation's education system was destroyed due to the decades of war, but it began reviving after the Karzai administration came to power in late 2001. More than 5,000 schools were built or renovated in the last decade, with more than 100,000 teachers being trained and recruited. More than seven million male and female students are enrolled in schools, with about 100,000 being enrolled in different universities around the country; at least 35% of these students are female. s of 2013[ [update]], there are 16,000 schools across Afghanistan. Education Minister Ghulam Farooq Wardak stated that another 8,000 schools are required to be constructed for the remaining 3 million children who are deprived of education.
Kabul University reopened in 2002 to both male and female students. In 2006, the American University of Afghanistan was established in Kabul, with the aim of providing a world-class, English-language, co-educational learning environment in Afghanistan. The capital of Kabul serves as the learning center of Afghanistan, with many of the best educational institutions being based there. Major universities outside of Kabul include Kandahar University in the south, Herat University in the northwest, Balkh University in the north, Nangarhar University and Khost University in the east. The National Military Academy of Afghanistan, modeled after the United States Military Academy at West Point, is a four-year military development institution dedicated to graduating officers for the Afghan Armed Forces. The $200 million Afghan Defense University is under construction near Qargha in Kabul. The United States is building six faculties of education and five provincial teacher training colleges around the country, two large secondary schools in Kabul, and one school in Jalalabad.
The literacy rate of the entire population has been very low but is now rising because more students go to schools. In 2010, the United States began establishing a number of Lincoln learning centers in Afghanistan. They are set up to serve as programming platforms offering English language classes, library facilities, programming venues, Internet connectivity, and educational and other counseling services. A goal of the program is to reach at least 4,000 Afghan citizens per month per location. The Afghan National Security Forces are provided with mandatory literacy courses. In addition to this, Baghch-e-Simsim (based on the American Sesame Street) was launched in late 2011 to help young Afghan children learn.
In 2009 and 2010, a 5,000 OLPC - One Laptop Per Child schools deployment took place in Kandahar with funding from an anonymous foundation. The OLPC team seeks local support to undertake larger deployment.
Culture.
The Afghan culture has been around for over two millennia, tracing back to at least the time of the Achaemenid Empire in 500 BCE. It is mostly a nomadic and tribal society, with different regions of the country having their own traditions, reflecting the multi-cultural and multi-lingual character of the nation. In the southern and eastern region the people live according to the Pashtun culture by following Pashtunwali, which is an ancient way of life that is still preserved. The remainder of the country is culturally Persian. Some non-Pashtuns who live in proximity with Pashtuns have adopted Pashtunwali in a process called Pashtunization (or "Afghanization"), while some Pashtuns have been Persianized. Millions of Afghans who have been living in Pakistan and Iran over the last 30 years have been influenced by the cultures of those neighboring nations.
Afghans display pride in their culture, nation, ancestry, and above all, their religion and independence. Like other highlanders, they are regarded with mingled apprehension and condescension, for their high regard for personal honor, for their tribe loyalty and for their readiness to use force to settle disputes. As tribal warfare and internecine feuding has been one of their chief occupations since time immemorial, this individualistic trait has made it difficult for foreigners to conquer them. Tony Heathcote considers the tribal system to be the best way of organizing large groups of people in a country that is geographically difficult, and in a society that, from a materialistic point of view, has an uncomplicated lifestyle. There are an estimated 60 major Pashtun tribes, and the Afghan nomads are estimated at about 2–3 million.
The nation has a complex history that has survived either in its current cultures or in the form of various languages and monuments. However, many of its historic monuments have been damaged in recent wars. The two famous Buddhas of Bamiyan were destroyed by the Taliban, who regarded them as idolatrous. Despite that, archaeologists are still finding Buddhist relics in different parts of the country, some of them dating back to the 2nd century. This indicates that Buddhism was widespread in Afghanistan. Other historical places include the cities of Herat, Kandahar, Ghazni, Mazar-i-Sharif, and Zarang. The Minaret of Jam in the Hari River valley is a UNESCO World Heritage site. A cloak reputedly worn by Islam's prophet Muhammad is kept inside the Shrine of the Cloak in Kandahar, a city founded by Alexander and the first capital of Afghanistan. The citadel of Alexander in the western city of Herat has been renovated in recent years and is a popular attraction for tourists. In the north of the country is the Shrine of Hazrat Ali, believed by many to be the location where Ali was buried. The Afghan Ministry of Information and Culture is renovating 42 historic sites in Ghazni until 2013, when the province will be declared as the capital of Islamic civilization. The National Museum of Afghanistan is located in Kabul.
Although literacy is low, classic Persian and Pashto poetry plays an important role in the Afghan culture. Poetry has always been one of the major educational pillars in the region, to the level that it has integrated itself into culture. Some notable poets include Rumi, Rabi'a Balkhi, Sanai, Jami, Khushal Khan Khattak, Rahman Baba, Khalilullah Khalili, and Parween Pazhwak.
Media and entertainment.
The Afghan mass media began in the early 20th century, with the first newspaper published in 1906. By the 1920s, Radio Kabul was broadcasting local radio services. Afghanistan National Television was launched in 1974 but was closed in 1996 when the media was tightly controlled by the Taliban. Since 2002, press restrictions have been gradually relaxed and private media diversified. Freedom of expression and the press is promoted in the 2004 constitution and censorship is banned, although defaming individuals or producing material contrary to the principles of Islam is prohibited. In 2008, Reporters Without Borders ranked the media environment as 156 out of 173 countries, with the 1st being the most free. Around 400 publications were registered, at least 15 local Afghan television channels, and 60 radio stations. Foreign radio stations, such as Voice of America, BBC World Service, and Radio Free Europe/Radio Liberty (RFE/RL) broadcast into the country.
The city of Kabul has been home to many musicians who were masters of both traditional and modern Afghan music. Traditional music is especially popular during the Nowruz (New Year) and National Independence Day celebrations. Ahmad Zahir, Nashenas, Ustad Sarahang, Sarban, Ubaidullah Jan, Farhad Darya, and Naghma are some of the notable Afghan musicians, but there are many others. Most Afghans are accustomed to watching Bollywood films from India and listening to its filmi hit songs. Many major Bollywood film stars have roots in Afghanistan, including Salman Khan, Saif Ali Khan, Shah Rukh Khan (SRK), Aamir Khan, Feroz Khan, Kader Khan, Naseeruddin Shah, and Celina Jaitley. In addition, several Bollywood films, such as "Dharmatma", "Khuda Gawah", "Escape from Taliban", and "Kabul Express" have been shot inside Afghanistan.
Sports.
The Afghanistan national football team has been competing in international football since 1941. The national team plays its home games at the Ghazi Stadium in Kabul, while football in Afghanistan is governed by the Afghanistan Football Federation. The national team has never competed or qualified for the FIFA World Cup, but has recently won an international football trophy in the SAFF Championship. The country also has a national team in the sport of futsal, a 5-a-side variation of football.
The other most popular sport in Afghanistan is cricket. The Afghan national cricket team, which was formed in the last decade, participated in the 2009 ICC World Cup Qualifier, 2010 ICC World Cricket League Division One and the 2010 ICC World Twenty20. It won the ACC Twenty20 Cup in 2007, 2009, 2011 and 2013. The team eventually made it to play in the 2015 Cricket World Cup. The Afghanistan Cricket Board (ACB) is the official governing body of the sport and is headquartered in Kabul. The Ghazi Amanullah Khan International Cricket Stadium serves as the nation's main cricket stadium, followed by the Kabul National Cricket Stadium. Several other stadiums are under construction. Domestically, cricket is played between teams from different provinces.
Other popular sports in Afghanistan include basketball, volleyball, taekwondo, and bodybuilding. Buzkashi is a traditional sport, mainly among the northern Afghans. It is similar to polo, played by horsemen in two teams, each trying to grab and hold a goat carcass. The Afghan Hound (a type of running dog) originated in Afghanistan and was originally used in hunting.
Further reading.
</dl>
</dl>

</doc>
<doc id="909" url="http://en.wikipedia.org/wiki?curid=909" title="Anglican Communion">
Anglican Communion

The Anglican Communion is an international association of churches consisting of the Church of England and of national and regional Anglican churches in full communion with it. The status of full communion means, ideally, that there is mutual agreement on essential doctrines and that full participation in the sacramental life of each church is available to all communicant Anglicans.
The Archbishop of Canterbury, Primate of All England, has a precedence of honour over the other archbishops of the Anglican Communion. He is recognized as "primus inter pares", or first among equals. The archbishop does not exercise direct authority in the provinces outside England, but instead acts as a focus of unity.
The Anglican Communion considers itself to be part of the One, Holy, Catholic and Apostolic Church and to be both Catholic and Reformed. For some adherents it represents a non-papal Catholicism, for others a form of Protestantism though without a dominant guiding figure such as Luther, Knox, Calvin, Zwingli or Wesley. For others, their self-identity represents some combination of the two. The communion encompasses a wide spectrum of belief and practice including evangelical, liberal, and Catholic.
With a membership currently estimated at around 80 million members worldwide, the Anglican Communion is the third largest Christian communion in the world, after the Roman Catholic Church and the Eastern Orthodox Church. Some of these churches are known as Anglican, such as the Anglican Church of Canada, due to their historical link to England ("Ecclesia Anglicana" means "English Church"). Some, for example the Church of Ireland, the Scottish and American Episcopal churches, and some other associated churches have a separate name. Each church has its own doctrine and liturgy, based in most cases on that of the Church of England; and each church has its own legislative process and overall episcopal polity, under the leadership of a local primate.
Ecclesiology, polity and ethos.
The Anglican Communion has no official legal existence nor any governing structure which might exercise authority over the member churches. There is an Anglican Communion Office in London, under the aegis of the Archbishop of Canterbury, but it only serves in a supporting and organisational role. The Communion is held together by a shared history, expressed in its ecclesiology, polity and ethos and also by participation in international consultative bodies.
Three elements have been important in holding the Communion together: first, the shared ecclesial structure of the component churches, manifested in an episcopal polity maintained through the apostolic succession of bishops and synodical government; second, the principle of belief expressed in worship, investing importance in approved prayer books and their rubrics; and third, the historical documents and the writings of early Anglican divines that have influenced the ethos of the Communion.
Originally, the Church of England was self-contained and relied for its unity and identity on its own history, its traditional legal and episcopal structure and its status as an established church of the state. As such Anglicanism was, from the outset, a movement with an explicitly episcopal polity, a characteristic which has been vital in maintaining the unity of the Communion by conveying the episcopate's role in manifesting visible catholicity and ecumenism.
Early in its development, Anglicanism developed a vernacular prayer book, called the Book of Common Prayer. Unlike other traditions, Anglicanism has never been governed by a magisterium nor by appeal to one founding theologian, nor by an extra-credal summary of doctrine (such as the Westminster Confession of the Presbyterian Church). Instead, Anglicans have typically appealed to the Book of Common Prayer (1662) and its offshoots as a guide to Anglican theology and practice. This had the effect of inculcating the principle of "Lex orandi, lex credendi" (Latin loosely translated as "the law of praying [is] the law of believing") as the foundation of Anglican identity and confession.
Protracted conflict through the seventeenth century with more radical Protestants on the one hand and Roman Catholics who still recognised the primacy of the Pope on the other, resulted in an association of churches that were both deliberately vague about doctrinal principles, yet bold in developing parameters of acceptable deviation. These parameters were most clearly articulated in the various rubrics of the successive prayer books, as well as the Thirty-Nine Articles of Religion. These Articles have historically shaped and continue to direct the ethos of the Communion, an ethos reinforced by their interpretation and expansion by such influential early theologians as Richard Hooker, Lancelot Andrewes, John Cosin, and others.
With the expansion of the British Empire, and hence the growth of Anglicanism outside Great Britain and Ireland, the Communion sought to establish new vehicles of unity. The first major expression of this were the Lambeth Conferences of the communion's bishops, first convened by Archbishop of Canterbury Charles Longley in 1869. From the beginning, these were not intended to displace the autonomy of the emerging provinces of the Communion, but to "discuss matters of practical interest, and pronounce what we deem expedient in resolutions which may serve as safe guides to future action."
Chicago Lambeth Quadrilateral.
One of the enduringly influential early resolutions of the conference was the so-called Chicago-Lambeth Quadrilateral of 1888. Its intent was to provide the basis for discussions of reunion with the Roman Catholic and Orthodox Churches, but it had the ancillary effect of establishing parameters of Anglican identity. It establishes four principles with these words:
Instruments of communion.
As mentioned above, the Anglican Communion has no international juridical organisation. The Archbishop of Canterbury's role is strictly symbolic and unifying and the Communion's three international bodies are consultative and collaborative, their resolutions having no legal effect on the autonomous provinces of the Communion. Taken together, however, the four do function as "instruments of communion", since all churches of the communion participate in them. In order of antiquity, they are:
Since there is no binding authority in the Communion, these international bodies are a vehicle for consultation and persuasion. In recent years, persuasion has tipped over into debates over conformity in certain areas of doctrine, discipline, worship, and ethics. The most notable example has been the objection of many provinces of the Communion (particularly in Africa and Asia) to the changing role of homosexuals in the North American churches (e.g., by blessing same-sex unions and ordaining and consecrating gays and lesbians in same-sex relationships), and to the process by which changes were undertaken.
Those who objected condemned these actions as unscriptural, unilateral, and without the agreement of the Communion prior to these steps being taken. In response, the American Episcopal Church and the Anglican Church of Canada answered that the actions had been undertaken after lengthy scriptural and theological reflection, legally in accordance with their own canons and constitutions and after extensive consultation with the provinces of the Communion.
The Primates' Meeting voted to request the two churches to withdraw their delegates from the 2005 meeting of the Anglican Consultative Council. Canada and the United States decided to attend the meeting but without exercising their right to vote. They have not been expelled or suspended, since there is no mechanism in this voluntary association to suspend or expel an independent province of the Communion. Since membership is based on a province's communion with Canterbury, expulsion would require the Archbishop of Canterbury's refusal to be in communion with the affected jurisdiction(s). In line with the suggestion of the Windsor Report, Rowan Williams (the previous Archbishop of Canterbury) recently established a working group to examine the feasibility of an Anglican covenant which would articulate the conditions for communion in some fashion.
Provinces.
All 38 provinces of the Anglican Communion are autonomous, each with its own primate and governing structure. These provinces may take the form of national churches (such as in Canada, Uganda, or Japan) or a collection of nations (such as the West Indies, Central Africa, or Southeast Asia). They are, in alphabetical order:
In addition, there are six extraprovincial churches, five of which are under the metropolitical authority of the Archbishop of Canterbury.
In addition to other member churches, the churches of the Anglican Communion are in full communion with the Old Catholic churches of the Union of Utrecht and the Scandinavian Lutheran churches of the Porvoo Communion in Europe, the India-based Malankara Mar Thoma Syrian and Malabar Independent Syrian churches and the Philippine Independent Church, also known as the Aglipayan Church.
History.
The Anglican Communion traces much of its growth to the older mission organisations of the Church of England such as the Society for Promoting Christian Knowledge (founded 1698), the Society for the Propagation of the Gospel in Foreign Parts (founded 1701) and the Church Missionary Society (founded 1799). The Church of England (which until the 20th century included the Church in Wales) initially separated from the Roman Catholic Church in 1538 in the reign of King Henry VIII, reunited in 1555 under Queen Mary I and then separated again in 1570 under Queen Elizabeth I (the Roman Catholic Church excommunicated Elizabeth I in 1570 in response to the Act of Supremacy 1559).
The Church of England has always thought of itself not as a new foundation but rather as a reformed continuation of the ancient "English Church" ("Ecclesia Anglicana") and a reassertion of that church's rights. As such it was a distinctly national phenomenon. The Church of Scotland was formed as a separate church from the Roman Catholic Church as a result of the Scottish Reformation in 1560 and the later formation of the Scottish Episcopal Church began in 1582 in the reign of James VI of Scotland over disagreements about the role of bishops.
The oldest-surviving Anglican church building outside of the British Isles (Britain and Ireland) is St Peter's Church in St. George's, Bermuda, established in 1612 (though the actual building had to be rebuilt several times over the following century). This is also the oldest surviving non-Roman Catholic church in the New World. It remained part of the Church of England until 1978 when the Anglican Church of Bermuda separated. The Church of England was the established church not only in England, but in its trans-Oceanic colonies.
Thus the only member churches of the present Anglican Communion existing by the mid-18th century were the Church of England, its closely linked sister church the Church of Ireland (which also separated from Roman Catholicism under Henry VIII) and the Scottish Episcopal Church which for parts of the 17th and 18th centuries was partially underground (it was suspected of Jacobite sympathies).
Global spread of Anglicanism.
The enormous expansion in the 18th and 19th centuries of the British Empire brought Anglicanism along with it. At first all these colonial churches were under the jurisdiction of the Bishop of London. After the American Revolution, the parishes in the newly independent country found it necessary to break formally from a church whose supreme governor was (and remains) the British monarch. Thus they formed their own dioceses and national church, the Episcopal Church in the United States of America, in a mostly amicable separation.
At about the same time, in the colonies which remained linked to the crown, the Church of England began to appoint colonial bishops. In 1787 a bishop of Nova Scotia was appointed with a jurisdiction over all of British North America; in time several more colleagues were appointed to other cities in present-day Canada. In 1814 a bishop of Calcutta was made; in 1824 the first bishop was sent to the West Indies and in 1836 to Australia. By 1840 there were still only ten colonial bishops for the Church of England; but even this small beginning greatly facilitated the growth of Anglicanism around the world. In 1841 a "Colonial Bishoprics Council" was set up and soon many more dioceses were created.
In time, it became natural to group these into provinces and a metropolitan was appointed for each province. Although it had at first been somewhat established in many colonies, in 1861 it was ruled that, except where specifically established, the Church of England had just the same legal position as any other church. Thus a colonial bishop and colonial diocese was by nature quite a different thing from their counterparts back home. In time bishops came to be appointed locally rather than from England and eventually national synods began to pass ecclesiastical legislation independent of England.
A crucial step in the development of the modern communion was the idea of the Lambeth Conferences (discussed above). These conferences demonstrated that the bishops of disparate churches could manifest the unity of the church in their episcopal collegiality despite the absence of universal legal ties. Some bishops were initially reluctant to attend, fearing that the meeting would declare itself a council with power to legislate for the church; but it agreed to pass only advisory resolutions. These Lambeth Conferences have been held roughly every 10 years since 1878 (the second such conference) and remain the most visible coming-together of the whole Communion.
Lambeth 1998.
The Lambeth Conference of 1998 included what has been seen by Philip Jenkins and others as a "watershed in global Christianity". The 1998 Lambeth Conference considered the issue of the theology of same-sex attraction in relation to human sexuality. At this 1998 conference for the first time in centuries the Christians of developing regions, especially, Africa, Asia, and Latin America, prevailed over the bishops of more prosperous countries (many from the USA, Canada, and the UK) who supported a redefinition of Anglican doctrine. Seen in this light 1998 is a date that marks the shift from a West-dominated Christianity to one wherein the growing churches of the two-thirds world are predominant.
Historic episcopate.
The churches of the Anglican Communion have traditionally held that ordination in the historic episcopate is a core element in the validity of clerical ordinations. The Roman Catholic Church does not recognise most Anglican orders (see "Apostolicae curae"). Some Eastern Orthodox Churches have issued statements to the effect that Anglican orders could be accepted, yet have still reordained former Anglican clergy; other Orthodox churches have rejected Anglican orders altogether. Orthodox bishop Kallistos Ware explains this apparent discrepancy as follows:
"Anglican clergy who join the Orthodox Church are reordained; but [some Orthodox Churches hold that] if Anglicanism and Orthodoxy were to reach full unity in the faith, perhaps such reordination might not be found necessary. It should be added, however, that a number of individual Orthodox theologians hold that under no circumstances would it be possible to recognise the validity of Anglican Orders."
Controversies.
One effect of the Communion's dispersed authority has been that conflict and controversy can arise over the effect divergent practices and doctrines in one part of the Communion have on others. Disputes that had been confined to the Church of England could be dealt with legislatively in that realm, but as the Communion spread out into new nations and disparate cultures, such controversies multiplied and intensified. These controversies have generally been of two types: liturgical and social.
The first such controversy of note concerned that of the growing influence of the Catholic Revival manifested in the tractarian and so-called ritualism controversies of the late nineteenth and early twentieth centuries. This controversy produced the Free Church of England and, in the United States and Canada, the Reformed Episcopal Church.
Later, rapid social change and the dissipation of British cultural hegemony over its former colonies contributed to disputes over the role of women, the parameters of marriage and divorce, and the practices of contraception and abortion. In the late 1970s, the Continuing Anglican movement produced a number of new church bodies in opposition to women's ordination, prayer book changes, and the new understandings concerning marriage.
More recently, disagreements over homosexuality have strained the unity of the Communion as well as its relationships with other Christian denominations, leading to another round of withdrawals from the Anglican Communion. Some churches founded outside the Anglican Communion in the late 20th and early 21st centuries, largely in opposition to the ordination of openly homosexual bishops and other clergy are usually referred to as belonging to the Anglican realignment movement, or else as "orthodox" Anglicans.
In some ways they represent a stronger opposition because they have the backing of many member provinces of the Anglican Communion and, in some cases, are or have been missionary jurisdictions of such provinces of the Communion as the Churches of Nigeria, Kenya, Rwanda, and the Southern Cone of America. Such debates about social theology and ethics, have occurred at the same time as debates on prayer book revision and the acceptable grounds for achieving full communion with non-Anglican churches.
References.
</dl>
</dl>

</doc>
<doc id="954" url="http://en.wikipedia.org/wiki?curid=954" title="Albert Speer">
Albert Speer

Berthold Konrad Hermann Albert Speer (]; March 19, 1905 – September 1, 1981) was a German architect who was, for a part of World War II, Minister of Armaments and War Production for the Third Reich. Speer was Adolf Hitler's chief architect before assuming ministerial office. As "the Nazi who said sorry", he accepted moral responsibility at the Nuremberg trials and in his memoirs for complicity in crimes of the Nazi regime, while stating he was ignorant of the Holocaust.
Speer joined the Nazi Party in 1931, launching himself on a political and governmental career which lasted fourteen years. His architectural skills made him increasingly prominent within the Party and he became a member of Hitler's inner circle. Hitler instructed him to design and construct structures including the Reich Chancellery and the "Zeppelinfeld" stadium in Nuremberg where Party rallies were held. Speer also made plans to reconstruct Berlin on a grand scale, with huge buildings, wide boulevards, and a reorganized transportation system.
In February 1942, Hitler appointed Speer Minister of Armaments and War Production. Under his leadership, Germany's war production continued to increase despite considerable Allied bombing. After the war, he was tried at Nuremberg and sentenced to 20 years in prison for his role in the Nazi regime, principally for the use of forced labor. Despite repeated attempts to gain early release, he served his full sentence, most of it at Spandau Prison in West Berlin.
Following his release from Spandau in 1966, Speer published two bestselling autobiographical works, "Inside the Third Reich" and "", detailing his often close personal relationship with Hitler, and providing readers and historians with a unique perspective on the workings of the Nazi regime. He later wrote a third book, "Infiltration", about the SS. Speer died of natural causes in 1981 while on a visit to London.
Early years.
Speer was born in Mannheim, into an upper-middle-class family. He was the second of three sons of Albert and Luise Speer. In 1918, the family moved permanently to their summer home Villa Speer on Schloss-Wolfsbrunnenweg, Heidelberg. According to Henry T. King, deputy prosecutor at Nuremberg who later wrote a book about Speer, "Love and warmth were lacking in the household of Speer's youth." Speer was active in sports, taking up skiing and mountaineering. Speer's Heidelberg school offered rugby football, unusual for Germany, and Speer was a participant. He wanted to become a mathematician, but his father said if Speer chose this occupation he would "lead a life without money, without a position and without a future". Instead, Speer followed in the footsteps of his father and grandfather and studied architecture.
Speer began his architectural studies at the University of Karlsruhe instead of a more highly acclaimed institution because the hyperinflation crisis of 1923 limited his parents' income. In 1924 when the crisis had abated, he transferred to the "much more reputable" Technical University of Munich. In 1925 he transferred again, this time to the Technical University of Berlin where he studied under Heinrich Tessenow, whom Speer greatly admired. After passing his exams in 1927, Speer became Tessenow's assistant, a high honor for a man of 22. As such, Speer taught some of Tessenow's classes while continuing his own postgraduate studies. In Munich, and continuing in Berlin, Speer began a close friendship, ultimately spanning over 50 years, with Rudolf Wolters, who also studied under Tessenow.
In mid-1922, Speer began courting Margarete (Margret) Weber (1905–1987), the daughter of a successful craftsman who employed 50 workers. The relationship was frowned upon by Speer's class-conscious mother, who felt that the Webers were socially inferior. Despite this opposition, the two married in Berlin on 28 August 1928; seven years were to elapse before Margarete Speer was invited to stay at her in-laws' home.
Nazi architect.
Joining the Nazis (1930–1934).
Speer stated he was apolitical when he was a young man, and that he attended a Berlin Nazi rally in December 1930 at the urging of some of his students. On March 1, 1931, he applied to join the Nazi Party and became member number 474,481.
In 1931, Speer surrendered his position as Tessenow's assistant and moved to Mannheim. His father gave him a job as manager of the elder Speer's properties. In July 1932, the Speers visited Berlin to help out the Party prior to the "Reichstag" elections. While they were there, Hanke recommended the young architect to Goebbels to help renovate the Party's Berlin headquarters. Speer agreed to do the work. When the commission was completed, Speer returned to Mannheim and remained there as Hitler took office in January 1933.
The organizers of the 1933 Nuremberg Rally asked Speer to submit designs for the rally, bringing him into contact with Hitler for the first time. Neither the organizers nor Rudolf Hess were willing to decide whether to approve the plans, and Hess sent Speer to Hitler's Munich apartment to seek his approval. This work won Speer his first national post, as Nazi Party "Commissioner for the Artistic and Technical Presentation of Party Rallies and Demonstrations".
Speer's next major assignment was as liaison to the Berlin building trades for Paul Troost's renovation of the Chancellery. As Chancellor, Hitler had a residence in the building and came by every day to be briefed by Speer and the building supervisor on the progress of the renovations. After one of these briefings, Hitler invited Speer to lunch, to the architect's great excitement. Hitler evinced considerable interest in Speer during the luncheon, and later told Speer that he had been looking for a young architect capable of carrying out his architectural dreams for the new Germany. Speer quickly became part of Hitler's inner circle; he was expected to call on Hitler in the morning for a walk or chat, to provide consultation on architectural matters, and to discuss Hitler's ideas. Most days he was invited to dinner.
The two men found much in common: Hitler spoke of Speer as a "kindred spirit" for whom he had always maintained "the warmest human feelings". The young, ambitious architect was dazzled by his rapid rise and close proximity to Hitler, which guaranteed him a flood of commissions from the government and from the highest ranks of the Party. Speer testified at Nuremberg, "I belonged to a circle which consisted of other artists and his personal staff. If Hitler had had any friends at all, I certainly would have been one of his close friends."
First Architect of the Third Reich (1934–1939).
When Troost died on January 21, 1934, Speer effectively replaced him as the Party's chief architect. Hitler appointed Speer as head of the Chief Office for Construction, which placed him nominally on Hess's staff.
One of Speer's first commissions after Troost's death was the "Zeppelinfeld" stadium—the Nürnberg parade grounds seen in Leni Riefenstahl's propaganda masterpiece "Triumph of the Will". This huge work was able to hold 340,000 people. The tribune was influenced by the Pergamon Altar in Anatolia, but was magnified to an enormous scale. Speer insisted that as many events as possible be held at night, both to give greater prominence to his lighting effects and to hide the individual Nazis, many of whom were overweight. Speer surrounded the site with 130 anti-aircraft searchlights. Speer described this as his most beautiful work, and as the only one that stood the test of time.
Nürnberg was to be the site of many more official Nazi buildings, most of which were never built; for example, the German Stadium would have accommodated 400,000 spectators, while an even larger rally ground would have held half a million people. While planning these structures, Speer conceived the concept of "ruin value": that major buildings should be constructed in such a way they would leave aesthetically pleasing ruins for thousands of years into the future. Such ruins would be a testament to the greatness of the Third Reich, just as ancient Greek or Roman ruins were symbols of the greatness of those civilizations.
When Hitler deprecated Werner March's design for the for the 1936 Summer Olympics as too modern, Speer modified the plans by adding a stone exterior. Speer designed the German Pavilion for the 1937 international exposition in Paris. The German and Soviet pavilion sites were opposite each other. On learning (through a clandestine look at the Soviet plans) that the Soviet design included two colossal figures seemingly about to overrun the German site, Speer modified his design to include a cubic mass which would check their advance, with a huge eagle on top looking down on the Soviet figures. Speer received, from Hitler Youth Leader and later fellow Spandau prisoner Baldur von Schirach, the Golden Hitler Youth Honor Badge with oak leaves.
In 1937, Hitler appointed Speer as with the rank of undersecretary of state in the Reich government. The position carried with it extraordinary powers over the Berlin city government and made Speer answerable to Hitler alone. It also made Speer a member of the "Reichstag", though the body by then had little effective power. Hitler ordered Speer to develop plans to rebuild Berlin. The plans centered on a three-mile long grand boulevard running from north to south, which Speer called the "Prachtstrasse", or Street of Magnificence; he also referred to it as the "North-South Axis". At the northern end of the boulevard, Speer planned to build the "Volkshalle", a huge assembly hall with a dome which would have been over 700 ft high, with floor space for 180,000 people. At the southern end of the avenue a great triumphal arch would rise; it would be almost 400 ft high, and able to fit the Arc de Triomphe inside its opening. The outbreak of World War II in 1939 led to the postponement, and later the abandonment, of these plans. Part of the land for the boulevard was to be obtained by consolidating Berlin's railway system. Speer hired Wolters as part of his design team, with special responsibility for the "Prachtstrasse". When Speer's father saw the model for the new Berlin, he said to his son, "You've all gone completely insane."
In January 1938, Hitler asked Speer to build a new Reich Chancellery on the same site as the existing structure, and said he needed it for urgent foreign policy reasons no later than his next New Year's reception for diplomats on January 10, 1939. This was a huge undertaking, especially as the existing Chancellery was in full operation. After consultation with his assistants, Speer agreed. Although the site could not be cleared until April, Speer was successful in building the large, impressive structure in nine months. The structure included a "Marble Gallery" 146 metres long, almost twice the length of the Hall of Mirrors in the Palace of Versailles. Speer employed thousands of workers in two shifts. Hitler, who had remained away from the project, was overwhelmed when Speer presented it, fully furnished, two days early. In appreciation for the architect's work on the Chancellery, Hitler awarded Speer the Nazi Golden Party Badge. Tessenow was less impressed, suggesting to Speer that he should have taken nine years over the project. The second Chancellery was damaged in the Battle of Berlin in 1945 and was eventually dismantled by the Soviets, its stone used for a war memorial.
During the Chancellery project, the pogrom of Kristallnacht took place. Speer made no mention of it in the first draft of "Inside the Third Reich", and it was only on the urgent advice of his publisher that he added a mention of seeing the ruins of the Central Synagogue in Berlin from his car.
Speer was under significant psychological pressure during this period of his life. He would later remember:
Soon after Hitler had given me the first large architectural commissions, I began to suffer from anxiety in long tunnels, in airplanes, or in small rooms. My heart would begin to race, I would become breathless, the diaphragm would seem to grow heavy, and I would get the impression that my blood pressure was rising tremendously ... Anxiety amidst all my freedom and power!
Wartime architect (1939–1942).
Speer supported the German invasion of Poland and subsequent war, though he recognized that it would lead to the postponement, at the least, of his architectural dreams. In his later years, Speer, talking with his biographer-to-be Gitta Sereny, explained how he felt in 1939: "Of course I was perfectly aware that [Hitler] sought world domination ... [A]t that time I asked for nothing better. That was the whole point of my buildings. They would have looked grotesque if Hitler had sat still in Germany. All I "wanted" was for this great man to dominate the globe."
Speer placed his department at the disposal of the "Wehrmacht". When Hitler remonstrated, and said it was not for Speer to decide how his workers should be used, Speer simply ignored him. Among Speer's innovations were quick-reaction squads to construct roads or clear away debris; before long, these units would be used to clear bomb sites. As the war progressed, initially to great German success, Speer continued preliminary work on the Berlin and Nürnberg plans. Speer also oversaw the construction of buildings for the "Wehrmacht" and "Luftwaffe".
In 1940, Joseph Stalin proposed that Speer pay a visit to Moscow. Stalin had been particularly impressed by Speer's work in Paris, and wished to meet the "Architect of the Reich". Hitler, alternating between amusement and anger, did not allow Speer to go, fearing that Stalin would put Speer in a "rat hole" until a new Moscow arose. When Germany invaded the Soviet Union in 1941, Speer came to doubt, despite Hitler's reassurances, that his projects for Berlin would ever be completed.
Minister of Armaments.
Appointment and increasing power.
On February 8, 1942, Minister of Armaments Fritz Todt died in a plane crash shortly after taking off from Hitler's eastern headquarters at Rastenburg. Speer, who had arrived in Rastenburg the previous evening, had accepted Todt's offer to fly with him to Berlin, but had canceled some hours before takeoff (Speer stated in his memoirs that the cancellation was because of exhaustion from travel and a late-night meeting with Hitler). Later that day, Hitler appointed Speer as Todt's successor to all of his posts. In "Inside the Third Reich", Speer recounts his meeting with Hitler and his reluctance to take ministerial office, only doing so because Hitler commanded it. Speer also states that Hermann Göring raced to Hitler's headquarters on hearing of Todt's death, hoping to claim Todt's powers. Hitler instead presented Göring with the "fait accompli" of Speer's appointment.
At the time of Speer's accession to the office, the German economy, unlike the British one, was not fully geared for war production. Consumer goods were still being produced at nearly as high a level as during peacetime. No fewer than five "Supreme Authorities" had jurisdiction over armament production—one of which, the Ministry of Economic Affairs, had declared in November 1941 that conditions did not permit an increase in armament production. Few women were employed in the factories, which were running only one shift. One evening soon after his appointment, Speer went to visit a Berlin armament factory; he found no one on the premises.
Speer overcame these difficulties by centralizing power over the war economy in himself. Factories were given autonomy, or as Speer put it, "self-responsibility", and each factory concentrated on a single product. Backed by Hitler's strong support (the dictator stated, "Speer, I'll sign anything that comes from you"), he divided the armament field according to weapon system, with experts rather than civil servants overseeing each department. No department head could be older than 55—anyone older being susceptible to "routine and arrogance"—and no deputy older than 40. Over these departments was a central planning committee headed by Speer, which took increasing responsibility for war production, and as time went by, for the German economy itself. According to the minutes of a conference at "Wehrmacht" High Command in March 1942, "It is only Speer's word that counts nowadays. He can interfere in all departments. Already he overrides all departments ... On the whole, Speer's attitude is to the point." Goebbels would note in his diary in June 1943, "Speer is still tops with the "Führer". He is truly a genius with organization." Speer was so successful in his position that by late 1943, he was widely regarded among the Nazi elite as a possible successor to Hitler.
While Speer had tremendous power, he was of course subordinate to Hitler. Nazi officials sometimes went around Speer by seeking direct orders from the dictator. When Speer ordered peacetime building work suspended, the "Gauleiters" (Nazi Party district leaders) obtained an exemption for their pet projects. When Speer sought the appointment of Hanke as a labor czar to optimize the use of German labor, Hitler, under the influence of Martin Bormann, instead appointed Fritz Sauckel. Rather than increasing female labor and taking other steps to better organize German labor, as Speer favored, Sauckel advocated importing labor from the occupied nations – and did so, obtaining workers for (among other things) Speer's armament factories, using the most brutal methods.
On December 10, 1943, Speer visited the underground Mittelwerk V-2 rocket factory that used concentration camp labor. Speer later claimed to have been shocked by the conditions there (5.7 percent of the work force died that month).
By 1943, the Allies had gained air superiority over Germany, and bombings of German cities and industry had become commonplace. However, the Allies in their strategic bombing campaign did not concentrate on industry, and Speer, with his improvisational skill, was able to overcome bombing losses. In spite of these losses, German production of tanks more than doubled in 1943, production of planes increased by 80 percent, and production time for "Kriegsmarine"‍‍ '​‍s submarines was reduced from one year to two months. Production would continue to increase until the second half of 1944, by which time enough equipment to supply 270 army divisions was being produced—although the "Wehrmacht" had only 150 divisions in the field.
In January 1944, Speer fell ill with complications from an inflamed knee, and was away from the office for three months. During his absence, his political rivals (mainly Göring, and Martin Bormann), attempted to have some of his powers permanently transferred to them. According to Speer, SS chief Heinrich Himmler tried to have him physically isolated by having Himmler's personal physician Karl Gebhardt treat him, though his "care" did not improve his health. Speer's wife and friends managed to have his case transferred to his friend Dr. Karl Brandt, and he slowly recovered. In April, Speer's rivals for power succeeded in having him deprived of responsibility for construction, and Speer promptly sent Hitler a bitter letter, concluding with an offer of his resignation. Judging Speer indispensable to the war effort, Field Marshal Erhard Milch persuaded Hitler to try to get his minister to reconsider. Hitler sent Milch to Speer with a message not addressing the dispute but instead stating that he still regarded Speer as highly as ever. According to Milch, upon hearing the message, Speer burst out, "The "Führer" can kiss my ass!" After a lengthy argument, Milch persuaded Speer to withdraw his offer of resignation, on the condition his powers were restored. On April 23, 1944, Speer went to see Hitler who agreed that "everything [will] stay as it was, [Speer will] remain the head of all German construction". According to Speer, while he was successful in this debate, Hitler had also won, "because he wanted and needed me back in his corner, and he got me".
Fall of the Reich.
Speer's name was included on the list of members of a post-Hitler government drawn up by the conspirators behind the July 1944 assassination plot to kill Hitler. The list had a question mark and the annotation "to be won over" by his name, which likely saved him from the extensive purges that followed the scheme's failure.
By February 1945, Speer, who had long concluded that the war was lost, was working to supply areas about to be occupied with food and materials to get them through the hard times ahead. On March 19, 1945, Hitler issued his Nero Decree, ordering a scorched earth policy in both Germany and the occupied territories. Hitler's order, by its terms, deprived Speer of any power to interfere with the decree, and Speer went to confront Hitler, telling him the war was lost. Hitler gave Speer 24 hours to reconsider his position, and when the two met the following day, Speer answered, "I stand unconditionally behind you." However, he demanded the exclusive power to implement the Nero Decree, and Hitler signed an order to that effect. Using this order, Speer worked to persuade generals and "Gauleiters" to circumvent the Nero Decree and avoid needless sacrifice of personnel and destruction of industry that would be needed after the war.
Speer managed to reach a relatively safe area near Hamburg as the Nazi regime finally collapsed, but decided on a final, risky visit to Berlin to see Hitler one more time. Speer stated at Nuremberg, "I felt that it was my duty not to run away like a coward, but to stand up to him again." Speer visited the "Führerbunker" on April 22. Hitler seemed calm and somewhat distracted, and the two had a long, disjointed conversation in which the dictator defended his actions and informed Speer of his intent to commit suicide and have his body burned. In the published edition of "Inside the Third Reich", Speer relates that he confessed to Hitler that he had defied the Nero Decree, but then assured Hitler of his personal loyalty, bringing tears to the dictator's eyes. Speer biographer Gitta Sereny argued, "Psychologically, it is possible that this is the way he remembered the occasion, because it was how he would have liked to behave, and the way he would have liked Hitler to react. But the fact is that none of it happened; our witness to this is Speer himself." Sereny notes that Speer's original draft of his memoirs lacks the confession and Hitler's tearful reaction, and contains an explicit denial that any confession or emotional exchange took place, as had been alleged in a French magazine article.
The following morning, Speer left the "Führerbunker"; Hitler curtly bade him farewell. Speer toured the damaged Chancellery one last time before leaving Berlin to return to Hamburg. On April 29, the day before committing suicide, Hitler dictated a final political testament which dropped Speer from the successor government. Speer was to be replaced by his own subordinate, Karl-Otto Saur.
Nuremberg Trial.
After Hitler's death, Speer offered his services to the so-called Flensburg Government, headed by Hitler's successor, Karl Dönitz, and took a significant role in that short-lived regime. On May 15, the Americans arrived and asked Speer if he would be willing to provide information on the effects of the air war. Speer agreed, and over the next several days, provided information on a broad range of subjects. On May 23, two weeks after the surrender of German troops, the British arrested the members of the Flensburg Government and brought Nazi Germany to a formal end.
Speer was taken to several internment centres for Nazi officials and interrogated. In September 1945, he was told that he would be tried for war crimes, and several days later, he was taken to Nuremberg and incarcerated there. Speer was indicted on all four possible counts: first, participating in a common plan or conspiracy for the accomplishment of crime against peace, second, planning, initiating and waging wars of aggression and other crimes against peace, third, war crimes, and lastly, crimes against humanity.
U.S. Supreme Court Justice Robert Jackson, the chief U.S. prosecutor at Nuremberg, alleged, "Speer joined in planning and executing the program to dragoon prisoners of war and foreign workers into German war industries, which waxed in output while the workers waned in starvation." Speer's attorney, Dr. Hans Flächsner, presented Speer as an artist thrust into political life, who had always remained a non-ideologue and who had been promised by Hitler that he could return to architecture after the war. During his testimony, Speer accepted responsibility for the Nazi regime's actions.
An observer at the trial, journalist and author William L. Shirer, wrote that, compared to his codefendants, Speer "made the most straightforward impression of all and ... during the long trial spoke honestly and with no attempt to shirk his responsibility and his guilt". Speer also testified that he had planned to kill Hitler in early 1945 by introducing tabun poison gas into the "Führerbunker" ventilation shaft. He said his efforts were frustrated by the impracticability of tabun and his lack of ready access to a replacement nerve agent, and also by the unexpected construction of a tall chimney that put the air intake out of reach. Speer stated his motive was despair at realising that Hitler intended to take the German people down with him. Speer's supposed assassination plan subsequently met with some skepticism, with Speer's architectural rival Hermann Giesler sneering, "the second most powerful man in the state did not have a ladder."
Speer was found guilty of war crimes and crimes against humanity, though he was acquitted on the other two counts. His claim that he was unaware of Nazi extermination plans, which probably saved him from hanging, was finally revealed to be false in a private correspondence written in 1971 and publicly disclosed in 2007. On 1 October 1946, he was sentenced to 20 years' imprisonment. While three of the eight judges (two Soviet and one American) initially advocated the death penalty for Speer, the other judges did not, and a compromise sentence was reached "after two days' discussion and some rather bitter horse-trading".
The court's judgment stated that:
... in the closing stages of the war [Speer] was one of the few men who had the courage to tell Hitler that the war was lost and to take steps to prevent the senseless destruction of production facilities, both in occupied territories and in Germany. He carried out his opposition to Hitler's scorched earth programme ... by deliberately sabotaging it at considerable personal risk.
Twelve of the defendants were sentenced to death (including Bormann, in absentia) and three acquitted; only seven of the defendants were sentenced to imprisonment. They remained in the cells at Nuremberg as the Allies debated where, and under what conditions, they should be incarcerated.
Imprisonment.
On July 18, 1947, Speer and his six fellow prisoners, all former high officials of the Nazi regime, were flown from Nuremberg to Berlin under heavy guard. The prisoners were taken to Spandau Prison in the British Sector of what would become West Berlin, where they would be designated by number, with Speer given Number Five. Initially, the prisoners were kept in solitary confinement for all but half an hour a day, and were not permitted to address each other or their guards. As time passed, the strict regimen was relaxed, especially during the three months in four that the three Western powers were in control; the four occupying powers took overall control on a monthly rotation. Speer considered himself an outcast among his fellow prisoners for his acceptance of responsibility at Nuremberg.
Speer made a deliberate effort to make as productive a use of his time as possible. He wrote, "I am obsessed with the idea of using this time of confinement for writing a book of major importance ... That could mean transforming prison cell into scholar's den." The prisoners were forbidden to write memoirs, and mail was severely limited and censored. However, as a result of an offer from a sympathetic orderly, Speer was able to have his writings, which eventually amounted to 20,000 sheets, sent to Wolters. By 1954, Speer had completed his memoirs, which became the basis of "Inside the Third Reich", and which Wolters arranged to have transcribed onto 1,100 typewritten pages. He was also able to send letters and financial instructions, and to obtain writing paper and letters from the outside. His many letters to his children, all secretly transmitted, eventually formed the basis for "Spandau: The Secret Diaries".
With the draft memoir complete and clandestinely transmitted, Speer sought a new project. He found one while taking his daily exercise, walking in circles around the prison yard. Measuring the path's distance carefully, Speer set out to walk the distance from Berlin to Heidelberg. He then expanded his idea into a worldwide journey, visualizing the places he was "traveling" through while walking the path around the prison yard. Speer ordered guidebooks and other materials about the nations through which he imagined he was passing, so as to envisage as accurate a picture as possible. Meticulously calculating every meter traveled, and mapping distances to the real-world geography, he began in northern Germany, passed through Asia by a southern route before entering Siberia, then crossed the Bering Strait and continued southwards, finally ending his sentence 35 km south of Guadalajara, Mexico.
Speer devoted much of his time and energy to reading. Though the prisoners brought some books with them in their personal property, Spandau Prison had no library so books were sent from Spandau's municipal library. From 1952 the prisoners were also able to order books from the Berlin central library in Wilmersdorf. Speer was a voracious reader and he completed well over 500 books in the first three years at Spandau alone. He read classic novels, travelogues, books on ancient Egypt, and biographies of such figures as Lucas Cranach, Édouard Manet, and Genghis Khan. Speer took to the prison garden for enjoyment and work, at first to do something constructive while afflicted with writer's block. He was allowed to build an ambitious garden, transforming what he initially described as a "wilderness" into what the American commander at Spandau described as "Speer's Garden of Eden".
Speer's supporters maintained a continual call for his release. Among those who pledged support for Speer's sentence to be commuted were Charles de Gaulle, U.S. diplomat George Ball, former U.S. High Commissioner John J. McCloy, and former Nuremberg prosecutor Hartley Shawcross. Willy Brandt was a strong advocate of Speer's, supporting his release, sending flowers to his daughter on the day of his release, and putting an end to the de-Nazification proceedings against Speer, which could have caused his property to be confiscated. A reduced sentence required the consent of all four of the occupying powers, and the Soviets adamantly opposed any such proposal. Speer served his full sentence, and was released on the stroke of midnight as October 1, 1966 began.
Release and later life.
Speer's release from prison was a worldwide media event, as reporters and photographers crowded both the street outside Spandau and the lobby of the Berlin hotel where Speer spent his first hours of freedom in over 20 years. He said little, reserving most comments for a major interview published in "Der Spiegel" in November 1966, in which he again took personal responsibility for crimes of the Nazi regime. Abandoning plans to return to architecture (two proposed partners died shortly before his release), he revised his Spandau writings into two autobiographical books, and later researched and published a third work, about Himmler and the SS. His books, most notably "Inside the Third Reich" (in German, "Erinnerungen", or "Reminiscences") and "Spandau: The Secret Diaries", provide a unique and personal look into the personalities of the Nazi era, and have become much valued by historians. Speer was aided in shaping the works by Joachim Fest and Wolf Jobst Siedler from the publishing house Ullstein. Speer found himself unable to re-establish his relationship with his children, even with his son Albert, who had also become an architect. According to Speer's daughter Hilde, "One by one my sister and brothers gave up. There was no communication."
Following the publication of his bestselling books, Speer donated a considerable amount of money to Jewish charities. According to Siedler, these donations were as high as 80% of his royalties. Speer kept the donations anonymous, both for fear of rejection, and for fear of being called a hypocrite.
As early as 1953, when Wolters strongly objected to Speer referring to Hitler in the memoirs draft as a criminal, Speer had predicted that were the writings to be published, he would lose a "good many friends". This came to pass, as following the publication of "Inside the Third Reich", close friends, such as Wolters and sculptor Arno Breker, distanced themselves from him. Hans Baur, Hitler's personal pilot, suggested, "Speer must have taken leave of his senses." Wolters wondered that Speer did not now "walk through life in a hair shirt, distributing his fortune among the victims of National Socialism, forswear all the vanities and pleasures of life and live on locusts and wild honey".
Speer made himself widely available to historians and other enquirers. He did an extensive, in-depth interview for the June 1971 issue of "Playboy" magazine, in which he stated, "If I didn't see it, then it was because I didn't want to see it." In October 1973, Speer made his first trip to Britain, flying to London under an assumed name to be interviewed on the BBC "Midweek" programme by Ludovic Kennedy. Upon arrival, he was detained for almost eight hours at Heathrow Airport when British immigration authorities discovered his true identity. The Home Secretary, Robert Carr, allowed Speer into the country for 48 hours. In the same year he appeared in the "The World at War" television programme. While in London eight years later to participate in the BBC "Newsnight" programme, Speer suffered a stroke and died on September 1, 1981. Speer had formed a relationship with a German-born Englishwoman, and was with her at the time of his death.
Even to the end of his life, Speer continued to question his actions under Hitler. In his final book, "Infiltration", he asks, "What would have happened if Hitler had asked me to make decisions that required the utmost hardness? ... How far would I have gone? ... If I had occupied a different position, to what extent would I have ordered atrocities if Hitler had told me to do so?" Speer leaves the questions unanswered.
Legacy and controversy.
The view of Speer as an unpolitical "miracle man" is challenged by Yale historian Adam Tooze. In his 2006 book, "The Wages of Destruction", Tooze, following Gitta Sereny, argues that Speer's ideological commitment to the Nazi cause was greater than he claimed. Tooze further contends that an insufficiently challenged Speer "mythology" (partly fostered by Speer himself through politically motivated, tendentious use of statistics and other propaganda) had led many historians to assign Speer far more credit for the increases in armaments production than was warranted and give insufficient consideration to the "highly political" function of the so-called armaments miracle.
Architectural legacy.
Little remains of Speer's personal architectural works, other than the plans and photographs. No buildings designed by Speer in the Nazi era remain in Berlin; a double row of lampposts along the Strasse des 17. Juni designed by Speer still stands. The tribune of the "Zeppelinfeld" stadium in Nuremberg, though partly demolished, may also be seen. Speer's work may also be seen in London, where he redesigned the interior of the German Embassy to the United Kingdom, then located at 7–9 Carlton House Terrace. Since 1967, it has served as the offices of the Royal Society. His work there, stripped of its Nazi fixtures and partially covered by carpets, survives in part.
Another legacy was the "Arbeitsstab Wiederaufbau zerstörter Städte" (Working group on Reconstruction of destroyed cities), authorized by Speer in 1943 to rebuild bombed German cities to make them more livable in the age of the automobile. Headed by Wolters, the working group took a possible military defeat into their calculations. The "Arbeitsstab"'s recommendations served as the basis of the postwar redevelopment plans in many cities, and "Arbeitsstab" members became prominent in the rebuilding.
Actions regarding the Jews.
As General Building Inspector, Speer was responsible for the Central Department for Resettlement. From 1939 onward, the Department used the Nuremberg Laws to evict Jewish tenants of non-Jewish landlords in Berlin, to make way for non-Jewish tenants displaced by redevelopment or bombing. Eventually, 75,000 Jews were displaced by these measures. Speer was aware of these activities, and inquired as to their progress. At least one original memo from Speer so inquiring still exists, as does the "Chronicle" of the Department's activities, kept by Wolters.
Following his release from Spandau, Speer presented to the German Federal Archives an edited version of the "Chronicle", stripped by Wolters of any mention of the Jews. When David Irving discovered discrepancies between the edited "Chronicle" and other documents, Wolters explained the situation to Speer, who responded by suggesting to Wolters that the relevant pages of the original "Chronicle" should "cease to exist". Wolters did not destroy the "Chronicle", and, as his friendship with Speer deteriorated, allowed access to the original "Chronicle" to doctoral student (who, after obtaining his doctorate, developed his thesis into a book, "Albert Speer: The End of a Myth"). Speer considered Wolters' actions to be a "betrayal" and a "stab in the back". The original "Chronicle" reached the Archives in 1983, after both Speer and Wolters had died.
Knowledge of the Holocaust.
Speer maintained at Nuremberg and in his memoirs that he had no knowledge of the Holocaust. In "Inside the Third Reich", he wrote that in mid-1944, he was told by Hanke (by then "Gauleiter" of Lower Silesia) that the minister should never accept an invitation to inspect a concentration camp in neighbouring Upper Silesia, as "he had seen something there which he was not permitted to describe and moreover could not describe". Speer later concluded that Hanke must have been speaking of Auschwitz, and blamed himself for not inquiring further of Hanke or seeking information from Himmler or Hitler:
These seconds [when Hanke told Speer this, and Speer did not inquire] were uppermost in my mind when I stated to the international court at the Nuremberg Trial that, as an important member of the leadership of the Reich, I had to share the total responsibility for all that had happened. For from that moment on I was inescapably contaminated morally; from fear of discovering something which might have made me turn from my course, I had closed my eyes ... Because I failed at that time, I still feel, to this day, responsible for Auschwitz in a wholly personal sense.
Much of the controversy over Speer's knowledge of the Holocaust has centered on his presence at the Posen Conference on 6 October 1943, at which Himmler gave a speech detailing the ongoing Holocaust to Nazi leaders. Himmler said, "The grave decision had to be taken to cause this people to vanish from the earth ... In the lands we occupy, the Jewish question will be dealt with by the end of the year." Speer is mentioned several times in the speech, and Himmler seems to address him directly. In "Inside the Third Reich", Speer mentions his own address to the officials (which took place earlier in the day) but does not mention Himmler's speech.
In 1971, American historian Erich Goldhagen published an article arguing that Speer was present for Himmler's speech. According to Fest in his biography of Speer, "Goldhagen's accusation certainly would have been more convincing" had he not placed supposed incriminating statements linking Speer with the Holocaust in quotation marks, attributed to Himmler, which were in fact invented by Goldhagen. In response, after considerable research in the German Federal Archives in Koblenz, Speer said he had left Posen around noon (long before Himmler's speech) to journey to Hitler's headquarters at Rastenburg. In "Inside the Third Reich", published before the Goldhagen article, Speer recalled that on the evening after the conference, many Nazi officials were so drunk that they needed help boarding the special train which was to take them to a meeting with Hitler. One of his biographers, Dan van der Vat, suggests this necessarily implies he must have still been present at Posen then, and must have heard Himmler's speech. In response to Goldhagen's article, Speer had alleged that in writing "Inside the Third Reich", he erred in reporting an incident that happened at another conference at Posen a year later, as happening in 1943. In 2007, "the Guardian" reported that a letter from Speer dated December 23, 1971, had been found in Britain in a collection of his correspondence to Hélène Jeanty, widow of a Belgian resistance fighter. In the letter, Speer states that he had been present for Himmler's presentation in Posen. Speer wrote: "There is no doubt – I was present as Himmler announced on October 6, 1943 that all Jews would be killed."
In 2005, the "Daily Telegraph" reported that documents had surfaced indicating that Speer had approved the allocation of materials for the expansion of Auschwitz after two of his assistants toured the facility on a day when almost a thousand Jews were murdered. The documents supposedly bore annotations in Speer's own handwriting. Speer biographer Gitta Sereny stated that, due to his workload, Speer would not have been personally aware of such activities.
The debate over Speer's knowledge of, or complicity in, the Holocaust made him a symbol for people who were involved with the Nazi regime yet did not have (or claimed not to have had) an active part in the regime's atrocities. As film director Heinrich Breloer remarked, "[Speer created] a market for people who said, 'Believe me, I didn't know anything about [the Holocaust]. Just look at the "Führer's" friend, he didn't know about it either.'"
Career summary.
Joined NSDAP: March 1, 1931
Party Number: 474,481
Nazi Party positions.
From 1934 to 1939, Speer was often referred to as "First Architect of the Reich", however this was mainly a title given to him by Hitler and not an actual political position within the Nazi Party or German government.
Government positions.
In 1943, under his authority as Reich Minister of Armaments, Speer also became the Director of "Organisation Todt". The standard uniform Speer wore during the later half of World War II was an insignia-less Nazi Party brown jacket, with an "Org Todt" armband.
Notes.
Explanatory notes
Citations
References.
Online sources

</doc>
<doc id="1144" url="http://en.wikipedia.org/wiki?curid=1144" title="Ardipithecus">
Ardipithecus

Ardipithecus is a genus of an extinct hominine that lived during Late Miocene and Early Pliocene in Afar Depression, Ethiopia. Originally described as one of the earliest ancestors of humans after they diverged from the main ape lineage, the relation of this genus to human ancestors and whether it is a hominin is now a matter of debate. Two fossil species are described in the literature: "A. ramidus", which lived about 4.4 million years ago during the early Pliocene, and "A. kadabba", dated to approximately 5.6 million years ago (late Miocene). Behavioral analysis showed that "Ardipithecus" could be very similar to those of chimpanzees, indicating that the early human ancestors were very chimpanzee-like in behaviour.
"Ardipithecus ramidus".
"A. ramidus" was named in September 1994. The first fossil found was dated to 4.4 million years ago on the basis of its stratigraphic position between two volcanic strata: the basal Gaala Tuff Complex (GATC) and the Daam Aatu Basaltic Tuff (DABT). The name "Ardipithecus ramidus" stems mostly from the Afar language, in which "Ardi" means "ground/floor" (borrowed from the Semitic root in either Amharic or Arabic) and "ramid" means "root". The "pithecus" portion of the name is from the Greek word for "ape".
Like most hominids, but unlike all previously recognized hominins, it had a grasping hallux or big toe adapted for locomotion in the trees. It is not confirmed how much other features of its skeleton reflect adaptation to bipedalism on the ground as well. Like later hominins, "Ardipithecus" had reduced canine teeth.
In 1992–1993 a research team headed by Tim White discovered the first "A. ramidus" fossils—seventeen fragments including skull, mandible, teeth and arm bones—from the Afar Depression in the Middle Awash river valley of Ethiopia. More fragments were recovered in 1994, amounting to 45% of the total skeleton. This fossil was originally described as a species of "Australopithecus", but White and his colleagues later published a note in the same journal renaming the fossil under a new genus, "Ardipithecus". Between 1999 and 2003, a multidisciplinary team led by Sileshi Semaw discovered bones and teeth of nine "A. ramidus" individuals at As Duma in the Gona Western Margin of Ethiopia's Afar Region. The fossils were dated to between 4.35 and 4.45 million years old.
"Ardipithecus ramidus" had a small brain, measuring between 300 and 350 cm3. This is slightly smaller than a modern bonobo or female common chimpanzee brain, but much smaller than the brain of australopithecines like Lucy (~400 to 550 cm3) and roughly 20% the size of the modern "Homo sapiens" brain. Like common chimpanzees, "A. ramidus" was much more prognathic than modern humans.
The teeth of "A. ramidus" lacked the specialization of other apes, and suggest that it was a generalized omnivore and frugivore (fruit eater) with a diet that did not depend heavily on foliage, fibrous plant material (roots, tubers, etc.), or hard and or abrasive food. The size of the upper canine tooth in "A. ramidus" males was not distinctly different from that of females. Their upper canines were less sharp than those of modern common chimpanzees in part because of this decreased upper canine size, as larger upper canines can be honed through wear against teeth in the lower mouth. The features of the upper canine in "A. ramidus" contrast with the sexual dimorphism observed in common chimpanzees, where males have significantly larger and sharper upper canine teeth than females.
The less pronounced nature of the upper canine teeth in "A. ramidus" has been used to infer aspects of the social behavior of the species and more ancestral hominids. In particular, it has been used to suggest that the last common ancestor of hominids and African apes was characterized by relatively little aggression between males and between groups. This is markedly different from social patterns in common chimpanzees, among which intermale and intergroup aggression are typically high. Researchers in a 2009 study said that this condition "compromises the living chimpanzee as a behavioral model for the ancestral hominid condition."
"A. ramidus" existed more recently than the most recent common ancestor of humans and chimpanzees (CLCA or "Pan"-"Homo" LCA) and thus is not fully representative of that common ancestor. Nevertheless, it is in some ways unlike chimpanzees, suggesting that the common ancestor differs from the modern chimpanzee. After the chimpanzee and human lineages diverged, both underwent substantial evolutionary change. Chimp feet are specialized for grasping trees; "A. ramidus" feet are better suited for walking. The canine teeth of "A. ramidus" are smaller, and equal in size between males and females, which suggests reduced male-to-male conflict, increased pair-bonding, and increased parental investment. "Thus, fundamental reproductive and social behavioral changes probably occurred in hominids long before they had enlarged brains and began to use stone tools," the research team concluded.
Ardi.
On October 1, 2009, paleontologists formally announced the discovery of the relatively complete "A. ramidus" fossil skeleton first unearthed in 1994. The fossil is the remains of a small-brained 50-kilogram (110 lb) female, nicknamed "Ardi", and includes most of the skull and teeth, as well as the pelvis, hands, and feet. It was discovered in Ethiopia's harsh Afar desert at a site called Aramis in the Middle Awash region. Radiometric dating of the layers of volcanic ash encasing the deposits suggest that Ardi lived about 4.4 million years ago. This date, however, has been questioned by others. Fleagle and Kappelman suggest that the region in which Ardi was found is difficult to date radiometrically, and they argue that Ardi should be dated at 3.9 million years.<ref name="10.1038/nature09709"></ref>
The fossil is regarded by its describers as shedding light on a stage of human evolution about which little was known, more than a million years before Lucy ("Australopithecus afarensis"), the iconic early human ancestor candidate who lived 3.2 million years ago, and was discovered in 1974 just 74 km (46 mi) away from Ardi's discovery site. However, because the "Ardi" skeleton is no more than 200,000 years older than the earliest fossils of "Australopithecus", and may in fact be younger than they are, some researchers doubt that it can represent a direct ancestor of "Australopithecus".
Some researchers infer from the form of her pelvis and limbs and the presence of her abductable hallux, that "Ardi" was a facultative biped: bipedal when moving on the ground, but quadrupedal when moving about in tree branches. "A. ramidus" had a more primitive walking ability than later hominids, and could not walk or run for long distances. The teeth suggest omnivory, and are more generalised than those of modern apes.
"Ardipithecus kadabba".
"Ardipithecus kadabba" is "known only from teeth and bits and pieces of skeletal bones", and is dated to approximately 5.6 million years ago. It has been described as a "probable chronospecies" (i.e. ancestor) of "A. ramidus". Although originally considered a subspecies of "A. ramidus", in 2004 anthropologists Yohannes Haile-Selassie, Gen Suwa, and Tim D. White published an article elevating "A. kadabba" to species level on the basis of newly discovered teeth from Ethiopia. These teeth show "primitive morphology and wear pattern" which demonstrate that "A. kadabba" is a distinct species from "A. ramidus".
The specific name comes from the Afar word for "basal family ancestor".
Lifestyle.
The toe and pelvic structure of "A. ramidus" suggest to some researchers that the organism walked upright.
According to Scott Simpson, the Gona Project's physical anthropologist, the fossil evidence from the Middle Awash indicates that both "A. kadabba" and "A. ramidus" lived in "a mosaic of woodland and grasslands with lakes, swamps and springs nearby," but further research is needed to determine which habitat "Ardipithecus" at Gona preferred.
Alternative views and further studies.
Due to several shared-characters with chimpanzees, its closeness to ape divergence period, and due to its fossil incompletenes, the exact position of "Ardipithecus" in the fossil record is a subject of controversy. Independent researcher such as Esteban E. Sarmiento of the Human Evolution Foundation in New Jersey, had systematically compared in 2010 the identifying characters of apes and human ancestral fossils in relation to "Ardipithecus", and concluded that the comparison data is not sufficient to support an exclusive human lineage. Sarmiento noted that "Ardipithecus" does not share any characters exclusive to humans and some of its characters (those in the wrist and basicranium) suggest it diverged from the common human/African ape stock prior to the human, chimpanzee and gorilla divergence His comparative (narrow allometry) study in 2011 on the molar and body segment lengths (which included living primates of similar body size) noted that some dimensions including short upper limbs, and metacarpals are reminiscent of humans, but other dimensions such as long toes and relative molar surface area are great ape-like. Sarmiento concluded that such length measures can change back and forth during evolution and are not very good indicators of relatedness. The "Ardipithecus" length measures, however, are good indicators of function and together with dental isotope data and the fauna and flora from the fossil site indicate "Ardipithecus" was mainly a terrestrial quadruped collecting a large portion of its food on the ground. Its arboreal behaviors would have been limited and suspension from branches solely from the upper limbs rare.
However, some later studies still argue for its classification in the human lineage. Comparative study in 2013 on carbon and oxygen stable isotopes within modern and fossil tooth enamel revealed that "Ardipithecus" fed both arboreally (on trees) and on the ground in a more open habitat, unlike chimpanzees and extinct ape "Sivapithecus", thereby differentiating them from apes. In 2014 it was reported that the hand bones of "Ardipithecus", "Australopithecus sediba" and "A. afarensis" consist of distinct human-lineage feature (which is the presence of third metacarpal styloid process, that is absent in apes). Unique brain organisations (such as lateral shift of the carotid foramina, mediolateral abbreviation of the lateral tympanic, and a shortened, trapezoidal basioccipital element) in "Ardipithecus" are also found only "Australopithecus" and "Homo" clade. Comparison of the tooth root morphology with those of "Sahelanthropus tchadensis" also indicated strong resemblance, implying its correct inclusion in human lineage.

</doc>
<doc id="1187" url="http://en.wikipedia.org/wiki?curid=1187" title="Alloy">
Alloy

An alloy is a material composed of two or more metals or a metal and a nonmetal. An alloy may be a solid solution of the elements (a single phase), a mixture of metallic phases (two or more solutions) or an intermetallic compound with no distinct boundary between the phases. Solid solution alloys give a single solid phase microstructure, while partial solutions exhibit two or more phases that may or may not be homogeneous in distribution, depending on the thermal (heat treatment) history of the material. An intermetallic compound has one other alloy or pure metal embedded within another pure metal.
Alloys are used in some applications, where their properties are superior to those of the pure component elements for a given application. Examples of alloys are steel, solder, brass, pewter, Duralumin, phosphor bronze and amalgams.
The alloy constituents are usually measured by mass. Alloys are usually classified as substitutional or interstitial alloys, depending on the atomic arrangement that forms the alloy. They can be further classified as homogeneous (consisting of a single phase), or heterogeneous (consisting of two or more phases) or intermetallic (where there is no distinct boundary between phases).
Introduction.
An alloy is a mixture of either pure or fairly pure chemical elements, which forms an impure substance (admixture) that retains the characteristics of a metal. An alloy is distinct from an impure metal, such as wrought iron, in that, with an alloy, the added impurities are usually desirable and will typically have some useful benefit. Alloys are made by mixing two or more elements; at least one of which being a metal. This is usually called the primary metal or the base metal, and the name of this metal may also be the name of the alloy. The other constituents may or may not be metals but, when mixed with the molten base, they will be soluble, dissolving into the mixture.
When the alloy cools and solidifies (crystallizes), its mechanical properties will often be quite different from those of its individual constituents. A metal that is normally very soft and malleable, such as aluminium, can be altered by alloying it with another soft metal, like copper. Although both metals are very soft and ductile, the resulting aluminium alloy will be much harder and stronger. Adding a small amount of non-metallic carbon to iron produces an alloy called steel. Due to its very-high strength and toughness (which is much higher than pure iron), and its ability to be greatly altered by heat treatment, steel is one of the most common alloys in modern use. By adding chromium to steel, its resistance to corrosion can be enhanced, creating stainless steel, while adding silicon will alter its electrical characteristics, producing silicon steel.
Although the elements usually must be soluble in the liquid state, they may not always be soluble in the solid state. If the metals remain soluble when solid, the alloy forms a solid solution, becoming a homogeneous structure consisting of identical crystals, called a phase. If the mixture cools and the constituents become insoluble, they may separate to form two or more different types of crystals, creating a heterogeneous microstructure of different phases. However, in other alloys, the insoluble elements may not separate until after crystallization occurs. These alloys are called intermetallic alloys because, if cooled very quickly, they first crystallize as a homogeneous phase, but they are supersaturated with the secondary constituents. As time passes, the atoms of these supersaturated alloys separate within the crystals, forming intermetallic phases that serve to reinforce the crystals internally.
Some alloys occur naturally, such as electrum, which is an alloy that is native to Earth, consisting of silver and gold. Meteorites are sometimes made of naturally occurring alloys of iron and nickel, but are not native to the Earth. One of the first alloys made by humans was bronze, which is made by mixing the metals tin and copper. Bronze was an extremely useful alloy to the ancients, because it is much stronger and harder than either of its components. Steel was another common alloy. However, in ancient times, it could only be created as an accidental byproduct from the heating of iron ore in fires (smelting) during the manufacture of iron. Other ancient alloys include pewter, brass and pig iron. In the modern age, steel can be created in many forms. Carbon steel can be made by varying only the carbon content, producing soft alloys like mild steel or hard alloys like spring steel. Alloy steels can be made by adding other elements, such as molybdenum, vanadium or nickel, resulting in alloys such as high-speed steel or tool steel. Small amounts of manganese are usually alloyed with most modern-steels because of its ability to remove unwanted impurities, like phosphorus, sulfur and oxygen, which can have detrimental effects on the alloy. However, most alloys were not created until the 1900s, such as various aluminium, titanium, nickel, and magnesium alloys. Some modern superalloys, such as incoloy, inconel, and hastelloy, may consist of a multitude of different components.
Terminology.
The term alloy is used to describe a mixture of atoms in which the primary constituent is a metal. The primary metal is called the "base", the "matrix", or the "solvent". The secondary constituents are often called "solutes". If there is a mixture of only two types of atoms, not counting impurities, such as a copper-nickel alloy, then it is called a "binary alloy." If there are three types of atoms forming the mixture, such as iron, nickel and chromium, then it is called a "ternary alloy." An alloy with four constituents is a "quaternary alloy," while a five-part alloy is termed a "quinary alloy." Because the percentage of each constituent can be varied, with any mixture the entire range of possible variations is called a "system". In this respect, all of the various forms of an alloy containing only two constituents, like iron and carbon, is called a "binary system," while all of the alloy combinations possible with a ternary alloy, such as alloys of iron, carbon and chromium, is called a "ternary system".
Although an alloy is technically an impure metal, when referring to alloys, the term "impurities" usually denotes those elements which are not desired. These impurities are often found in the base metals or the solutes, but they may also be introduced during the alloying process. For instance, sulfur is a common impurity in steel. Sulfur combines readily with iron to form iron sulfide, which is very brittle, creating weak spots in the steel. Lithium, sodium and calcium are common impurities in aluminium alloys, which can have adverse effects on the structural integrity of castings. Conversely, otherwise pure-metals that simply contain unwanted impurities are often called "impure metals" and are not usually referred to as alloys. Oxygen, present in the air, readily combines with most metals to form metal oxides; especially at higher temperatures encountered during alloying. Great care is often taken during the alloying process to remove excess impurities, using fluxes, chemical additives, or other methods of extractive metallurgy.
In practice, some alloys are used so predominantly with respect to their base metals that the name of the primary constituent is also used as the name of the alloy. For example, 14 karat gold is an alloy of gold with other elements. Similarly, the silver used in jewelry and the aluminium used as a structural building material are also alloys.
The term "alloy" is sometimes used in everyday speech as a synonym for a particular alloy. For example, automobile wheels made of an aluminium alloy are commonly referred to as simply "alloy wheels", although in point of fact steels and most other metals in practical use are also alloys. Steel is such a common alloy that many items made from it, like wheels, barrels, or girders, are simply referred to by the name of the item, assuming it is made of steel. When made from other materials, they are typically specified as such, (i.e.: "bronze wheel," "plastic barrel," or "wood girder").
Theory.
Alloying a metal is done by combining it with one or more other metals or non-metals that often enhance its properties. For example, steel is stronger than iron, its primary element. The electrical and thermal conductivity of alloys is usually lower than that of the pure metals. The physical properties, such as density, reactivity, Young's modulus of an alloy may not differ greatly from those of its elements, but engineering properties such as tensile strength and shear strength may be substantially different from those of the constituent materials. This is sometimes a result of the sizes of the atoms in the alloy, because larger atoms exert a compressive force on neighboring atoms, and smaller atoms exert a tensile force on their neighbors, helping the alloy resist deformation. Sometimes alloys may exhibit marked differences in behavior even when small amounts of one element are present. For example, impurities in semiconducting ferromagnetic alloys lead to different properties, as first predicted by White, Hogan, Suhl, Tian Abrie and Nakamura.
Some alloys are made by melting and mixing two or more metals. Bronze, an alloy of copper and tin, was the first alloy discovered, during the prehistoric period now known as the bronze age; it was harder than pure copper and originally used to make tools and weapons, but was later superseded by metals and alloys with better properties. In later times bronze has been used for ornaments, bells, statues, and bearings. Brass is an alloy made from copper and zinc.
Unlike pure metals, most alloys do not have a single melting point, but a melting range in which the material is a mixture of solid and liquid phases. The temperature at which melting begins is called the solidus, and the temperature when melting is just complete is called the liquidus. However, for many alloys there is a particular proportion of constituents (in some cases more than one)—either a eutectic mixture or a peritectic composition—which gives the alloy a unique melting point.
Heat-treatable alloys.
Alloys are often made to alter the mechanical properties of the base metal, to induce hardness, toughness, ductility, or other desired properties. Most metals and alloys can be work hardened by creating defects in their crystal structure. These defects are created during plastic deformation, such as hammering or bending, and are permanent unless the metal is recrystallized. However, some alloys can also have their properties altered by heat treatment. Nearly all metals can be softened by annealing, which recrystallizes the alloy and repairs the defects, but not as many can be hardened by controlled heating and cooling. Many alloys of aluminium, copper, magnesium, titanium, and nickel can be strengthened to some degree by some method of heat treatment, but few respond to this to the same degree that steel does.
At a certain temperature, (usually between 1500 F and 1600 F, depending on carbon content), the base metal of steel undergoes a change in the arrangement of the atoms in its crystal matrix, called allotropy. This allows the small carbon atoms to enter the interstices of the iron crystal, diffusing into the iron matrix. When this happens, the carbon atoms are said to be in "solution," or mixed with the iron, forming a single, homogeneous, crystalline phase called austenite. If the steel is cooled slowly, the iron will gradually change into its low temperature allotrope. When this happens the carbon atoms will no longer be soluble with the iron, and will be forced to precipitate out of solution, nucleating into the spaces between the crystals. The steel then becomes heterogeneous, being formed of two phases; the carbon (carbide) phase cementite, and ferrite. This type of heat treatment produces steel that is rather soft and bendable. However, if the steel is cooled quickly the carbon atoms will not have time to precipitate. When rapidly cooled, a diffusionless (martensite) transformation occurs, in which the carbon atoms become trapped in solution. This causes the iron crystals to deform intrinsically when the crystal structure tries to change to its low temperature state, making it very hard and brittle.
Conversely, most heat-treatable alloys are precipitation hardening alloys, which produce the opposite effects that steel does. When heated to form a solution and then cooled quickly, these alloys become much softer than normal, during the diffusionless transformation, and then harden as they age. The solutes in these alloys will precipitate over time, forming intermetallic phases, which are difficult to discern from the base metal. Unlike steel, in which the solid solution separates to form different crystal phases, precipitation hardening alloys separate to form different phases within the same crystal. These intermetallic alloys appear homogeneous in crystal structure, but tend to behave heterogeneous, becoming hard and somewhat brittle.
Substitutional and interstitial alloys.
When a molten metal is mixed with another substance, there are two mechanisms that can cause an alloy to form, called "atom exchange" and the "interstitial mechanism". The relative size of each element in the mix plays a primary role in determining which mechanism will occur. When the atoms are relatively similar in size, the atom exchange method usually happens, where some of the atoms composing the metallic crystals are substituted with atoms of the other constituent. This is called a "substitutional alloy". Examples of substitutional alloys include bronze and brass, in which some of the copper atoms are substituted with either tin or zinc atoms. With the interstitial mechanism, one atom is usually much smaller than the other, so cannot successfully replace an atom in the crystals of the base metal. The smaller atoms become trapped in the spaces between the atoms in the crystal matrix, called the "interstices". This is referred to as an "interstitial alloy". Steel is an example of an interstitial alloy, because the very small carbon atoms fit into interstices of the iron matrix. Stainless steel is an example of a combination of interstitial and substitutional alloys, because the carbon atoms fit into the interstices, but some of the iron atoms are replaced with nickel and chromium atoms.
History and examples.
Meteoric iron.
The use of alloys by humans started with the use of meteoric iron, a naturally occurring alloy of nickel and iron. It is the main constituent of iron meteorites which occasionally fall down on Earth from outer space. As no metallurgic processes were used to separate iron from nickel, the alloy was used as it was. Meteoric iron could be forged from a red heat to make objects such as tools, weapons, and nails. In many cultures it was shaped by cold hammering into knives and arrowheads. They were often used as anvils. Meteoric iron was very rare and valuable, and difficult for ancient people to work.
Bronze and brass.
Iron is usually found as iron ore on Earth, except for one deposit of native iron in Greenland, which was used by the Inuit people. Native copper, however, was found worldwide, along with silver, gold and platinum, which were also used to make tools, jewelry, and other objects since Neolithic times. Copper was the hardest of these metals, and the most widely distributed. It became one of the most important metals to the ancients. Eventually, humans learned to smelt metals such as copper and tin from ore, and, around 2500 BC, began alloying the two metals to form bronze, which is much harder than its ingredients. Tin was rare, however, being found mostly in Great Britain. In the Middle East, people began alloying copper with zinc to form brass. Ancient civilizations took into account the mixture and the various properties it produced, such as hardness, toughness and melting point, under various conditions of temperature and work hardening, developing much of the information contained in modern alloy phase diagrams. Arrowheads from the Chinese Qin dynasty (around 200 BC) were often constructed with a hard bronze-head, but a softer bronze-tang, combining the alloys to prevent both dulling and breaking during use.
Amalgams.
Mercury has been smelted from cinnabar for thousands of years. Mercury dissolves many metals, such as gold, silver, and tin, to form amalgams (an alloy in a soft paste, or liquid form at ambient temperature). Amalgams have been used since 200 BC in China for plating objects with precious metals, called gilding, such as armor and mirrors. The ancient Romans often used mercury-tin amalgams for gilding their armor. The amalgam was applied as a paste and then heated until the mercury vaporized, leaving the gold, silver, or tin behind. Mercury was often used in mining, to extract precious metals like gold and silver from their ores.
Precious-metal alloys.
Many ancient civilizations alloyed metals for purely aesthetic purposes. In ancient Egypt and Mycenae, gold was often alloyed with copper to produce red-gold, or iron to produce a bright burgundy-gold. Gold was often found alloyed with silver or other metals to produce various types of colored gold. These metals were also used to strengthen each other, for more practical purposes. Copper was often added to silver to make sterling silver, increasing its strength for use in dishes, silverware, and other practical items. Quite often, precious metals were alloyed with less valuable substances as a means to deceive buyers. Around 250 BC, Archimedes was commissioned by the king to find a way to check the purity of the gold in a crown, leading to the famous bath-house shouting of "Eureka!" upon the discovery of Archimedes' principle.
Pewter.
The term pewter covers a variety of alloys consisting primarily of tin. As a pure metal, tin was much too soft to be used for any practical purpose. However, in the Bronze age, tin was a rare metal and, in many parts of Europe and the Mediterranean, was often valued higher than gold. To make jewelry, forks and spoons, or other objects from tin, it was usually alloyed with other metals to increase its strength and hardness. These metals were typically lead, antimony, bismuth or copper. These solutes sometimes were added individually in varying amounts, or added together, making a wide variety of things, ranging from practical items, like dishes, surgical tools, candlesticks or funnels, to decorative items such as ear rings and hair clips.
The earliest examples of pewter come from ancient Egypt, around 1450 BC. The use of pewter was widespread across Europe, from France to Norway and Britain (where most of the ancient tin was mined) to the Near East. The alloy was also used in China and the Far East, arriving in Japan around 800 AD, where it was used for making objects like ceremonial vessels, tea canisters, or chalices used in shinto shrines.
Steel and pig iron.
The first known smelting of iron began in Anatolia, around 1800 BC. Called the bloomery process, it produced very soft but ductile wrought iron and, by 800 BC, the technology had spread to Europe. Pig iron, a very hard but brittle alloy of iron and carbon, was being produced in China as early as 1200 BC, but did not arrive in Europe until the Middle Ages. Pig iron has a lower melting point than iron, and was used for making cast-iron. However, these metals found little practical use until the introduction of crucible steel around 300 BC. These steels were of poor quality, and the introduction of pattern welding, around the 1st century AD, sought to balance the extreme properties of the alloys by laminating them, to create a tougher metal. Around 700 AD, the Japanese began folding bloomery-steel and cast-iron in alternating layers to increase the strength of their swords, using clay fluxes to remove slag and impurities. This method of Japanese swordsmithing produced one of the purest steel-alloys of ancient times.
While the use of iron started to become more widespread around 1200 BC, mainly because of interruptions in the trade routes for tin, the metal is much softer than bronze. However, very small amounts of steel, (an alloy of iron and around 1% carbon), was always a byproduct of the bloomery process. The ability to modify the hardness of steel by heat treatment had been known since 1100 BC, and the rare material was valued for use in tool and weapon making. Because the ancients could not produce temperatures high enough to melt iron fully, the production of steel in decent quantities did not occur until the introduction of blister steel during the Middle Ages. This method introduced carbon by heating wrought iron in charcoal for long periods of time, but the penetration of carbon was not very deep, so the alloy was not homogeneous. In 1740, Benjamin Huntsman began melting blister steel in a crucible to even out the carbon content, creating the first process for the mass production of tool steel. Huntsman's process was used for manufacturing tool steel until the early 1900s.
With the introduction of the blast furnace to Europe in the Middle Ages, pig iron was able to be produced in much higher volumes than wrought iron. Because pig iron could be melted, people began to develop processes of reducing the carbon in the liquid pig iron to create steel. Puddling was introduced during the 1700s, where molten pig iron was stirred while exposed to the air, to remove the carbon by oxidation. In 1858, Sir Henry Bessemer developed a process of steel-making by blowing hot air through liquid pig iron to reduce the carbon content. The Bessemer process was able to produce the first large scale manufacture of steel. Once the Bessemer process began to gain widespread use, other alloys of steel began to follow. Mangalloy, an alloy of steel and manganese exhibiting extreme hardness and toughness, was one of the first alloy steels, and was created by Robert Hadfield in 1882.
Precipitation-hardening alloys.
In 1906, precipitation hardening alloys were discovered by Alfred Wilm. Precipitation hardening alloys, such as certain alloys of aluminium, titanium, and copper, are heat-treatable alloys that soften when quenched (cooled quickly), and then harden over time. After quenching a ternary alloy of aluminium, copper, and magnesium, Wilm discovered that the alloy increased in hardness when left to age at room temperature. Although an explanation for the phenomenon was not provided until 1919, duralumin was one of the first "age hardening" alloys to be used, and was soon followed by many others. Because they often exhibit a combination of high strength and low weight, these alloys became widely used in many forms of industry, including the construction of modern aircraft.

</doc>
<doc id="1198" url="http://en.wikipedia.org/wiki?curid=1198" title="Acoustics">
Acoustics

Acoustics is the interdisciplinary science that deals with the study of all mechanical waves in gases, liquids, and solids including topics such as vibration, sound, ultrasound and infrasound. A scientist who works in the field of acoustics is an acoustician while someone working in the field of acoustics technology may be called an acoustical engineer. The application of acoustics is present in almost all aspects of modern society with the most obvious being the audio and noise control industries.
Hearing is one of the most crucial means of survival in the animal world, and speech is one of the most distinctive characteristics of human development and culture. Accordingly, the science of acoustics spreads across many facets of human society—music, medicine, architecture, industrial production, warfare and more. Likewise, animal species such as songbirds and frogs use sound and hearing as a key element of mating rituals or marking territories. Art, craft, science and technology have provoked one another to advance the whole, as in many other fields of knowledge. Robert Bruce Lindsay's 'Wheel of Acoustics' is a well accepted overview of the various fields in acoustics.
The word "acoustic" is derived from the Greek word ἀκουστικός ("akoustikos"), meaning "of or for hearing, ready to hear" and that from ἀκουστός ("akoustos"), "heard, audible", which in turn derives from the verb ("akouo"), "I hear".
The Latin synonym is "sonic", after which the term sonics used to be a synonym for acoustics and later a branch of acoustics. Frequencies above and below the audible range are called "ultrasonic" and "infrasonic", respectively.
History.
Early research in acoustics.
In the 6th century BC, the ancient Greek philosopher Pythagoras wanted to know why some combinations of musical sounds seemed more beautiful than others, and he found answers in terms of numerical ratios representing the harmonic overtone series on a string. He is reputed to have observed that when the lengths of vibrating strings are expressible as ratios of integers (e.g. 2 to 3, 3 to 4), the tones produced will be harmonious, and the smaller the integers the more harmonious the sounds. If, for example, a string of a certain length would sound particularly harmonious with a string of twice the length (other factors being equal). In modern parlance, if a string sounds the note C when plucked, a string twice as long will sound a C an octave lower. In one system of musical tuning, the tones in between are then given by 16:9 for D, 8:5 for E, 3:2 for F, 4:3 for G, 6:5 for A, and 16:15 for B, in ascending order.
Aristotle (384-322 BC) understood that sound consisted of contractions and expansions of the air "falling upon and striking the air which is next to it...", a very good expression of the nature of wave motion.
In about 20 BC, the Roman architect and engineer Vitruvius wrote a treatise on the acoustic properties of theaters including discussion of interference, echoes, and reverberation—the beginnings of architectural acoustics. In Book V of his "De architectura" ("The Ten Books of Architecture") Vitruvius describes sound as a wave comparable to a water wave extended to three dimensions, which, when interrupted by obstructions, would flow back and break up following waves. He described the ascending seats in ancient theaters as designed to prevent this deterioration of sound and also recommended bronze vessels of appropriate sizes be placed in theaters to resonate with the fourth, fifth and so on, up to the double octave, in order to resonate with the more desirable, harmonious notes.
The physical understanding of acoustical processes advanced rapidly during and after the Scientific Revolution. Mainly Galileo Galilei (1564–1642) but also Marin Mersenne (1588–1648), independently, discovered the complete laws of vibrating strings (completing what Pythagoras and Pythagoreans had started 2000 years earlier). Galileo wrote "Waves are produced by the vibrations of a sonorous body, which spread through the air, bringing to the tympanum of the ear a stimulus which the mind interprets as sound", a remarkable statement that points to the beginnings of physiological and psychological acoustics. Experimental measurements of the speed of sound in air were carried out successfully between 1630 and 1680 by a number of investigators, prominently Mersenne. Meanwhile Newton (1642–1727) derived the relationship for wave velocity in solids, a cornerstone of physical acoustics (Principia, 1687).
Age of Enlightenment and onward.
The eighteenth century saw major advances in acoustics as mathematicians applied the new techniques of calculus to elaborate theories of sound wave propagation. In the nineteenth century the major figures of mathematical acoustics were Helmholtz in Germany, who consolidated the field of physiological acoustics, and Lord Rayleigh in England, who combined the previous knowledge with his own copious contributions to the field in his monumental work "The Theory of Sound" (1877). Also in the 19th century, Wheatstone, Ohm, and Henry developed the analogy between electricity and acoustics.
The twentieth century saw a burgeoning of technological applications of the large body of scientific knowledge that was by then in place. The first such application was Sabine’s groundbreaking work in architectural acoustics, and many others followed. Underwater acoustics was used for detecting submarines in the first World War. Sound recording and the telephone played important roles in a global transformation of society. Sound measurement and analysis reached new levels of accuracy and sophistication through the use of electronics and computing. The ultrasonic frequency range enabled wholly new kinds of application in medicine and industry. New kinds of transducers (generators and receivers of acoustic energy) were invented and put to use.
Fundamental concepts of acoustics.
Definition.
Acoustics is defined by ANSI/ASA S1.1-2013 as "(a) Science of sound, including its production, transmission, and effects, including biological and psychological effects. (b) Those qualities of a room that, together, determine its character with respect to auditory effects."
The study of acoustics revolves around the generation, propagation and reception of mechanical waves and vibrations.
The steps shown in the above diagram can be found in any acoustical event or process. There are many kinds of cause, both natural and volitional. There are many kinds of transduction process that convert energy from some other form into sonic energy, producing a sound wave. There is one fundamental equation that describes sound wave propagation, the acoustic wave equation, but the phenomena that emerge from it are varied and often complex. The wave carries energy throughout the propagating medium. Eventually this energy is transduced again into other forms, in ways that again may be natural and/or volitionally contrived. The final effect may be purely physical or it may reach far into the biological or volitional domains. The five basic steps are found equally well whether we are talking about an earthquake, a submarine using sonar to locate its foe, or a band playing in a rock concert.
The central stage in the acoustical process is wave propagation. This falls within the domain of physical acoustics. In fluids, sound propagates primarily as a pressure wave. In solids, mechanical waves can take many forms including longitudinal waves, transverse waves and surface waves.
Acoustics looks first at the pressure levels and frequencies in the sound wave and how the wave interact with the environment. This interaction can be described as either a diffraction, interference or a reflection or a mix of the three. If several media is present, a refraction can also occur. Transduction processes are also of special importance to acoustics.
Wave propagation: pressure levels.
In fluids such as air and water, sound waves propagate as disturbances in the ambient pressure level. While this disturbance is usually small, it is still noticeable to the human ear. The smallest sound that a person can hear, known as the threshold of hearing, is nine orders of magnitude smaller than the ambient pressure. The loudness of these disturbances is called the sound pressure level (SPL), and is measured on a logarithmic scale in decibels.
Wave propagation: frequency.
Physicists and acoustic engineers tend to discuss sound pressure levels in terms of frequencies, partly because this is how our ears interpret sound. What we experience as "higher pitched" or "lower pitched" sounds are pressure vibrations having a higher or lower number of cycles per second. In a common technique of acoustic measurement, acoustic signals are sampled in time, and then presented in more meaningful forms such as octave bands or time frequency plots. Both these popular methods are used to analyze sound and better understand the acoustic phenomenon.
The entire spectrum can be divided into three sections: audio, ultrasonic, and infrasonic. The audio range falls between 20 Hz and 20,000 Hz. This range is important because its frequencies can be detected by the human ear. This range has a number of applications, including speech communication and music. The ultrasonic range refers to the very high frequencies: 20,000 Hz and higher. This range has shorter wavelengths which allow better resolution in imaging technologies. Medical applications such as ultrasonography and elastography rely on the ultrasonic frequency range. On the other end of the spectrum, the lowest frequencies are known as the infrasonic range. These frequencies can be used to study geological phenomena such as earthquakes.
Analytic instruments such as the spectrum analyzer facilitate visualization and measurement of acoustic signals and their properties. The spectrogram produced by such an instrument is a graphical display of the time varying pressure level and frequency profiles which give a specific acoustic signal its defining character.
Transduction in acoustics.
A transducer is a device for converting one form of energy into another. In an electroacoustic context, this means converting sound energy into electrical energy (or vice versa). Electroacoustic transducers include loudspeakers, microphones, hydrophones and sonar projectors. These devices convert a sound pressure wave to or from an electric signal. The most widely used transduction principles are electromagnetism, electrostatics and piezoelectricity.
The transducers in most common loudspeakers (e.g. woofers and tweeters), are electromagnetic devices that generate waves using a suspended diaphragm driven by an electromagnetic voice coil, sending off pressure waves. Electret microphones and condenser microphones employ electrostatics—as the sound wave strikes the microphone's diaphragm, it moves and induces a voltage change. The ultrasonic systems used in medical ultrasonography employ piezoelectric transducers. These are made from special ceramics in which mechanical vibrations and electrical fields are interlinked through a property of the material itself.
Acoustician.
An acoustician is an expert in the science of sound.
Education.
There are many types of acoustician, but they usually have a Bachelor's degree or higher qualification. Some possess a degree in acoustics, while others enter the discipline via studies in fields such as physics or engineering. Much work in acoustics requires a good grounding in Mathematics and science. Many acoustic scientists work in research and development. Some conduct basic research to advance our knowledge of the perception (e.g. hearing, psychoacoustics or neurophysiology) of speech, music and noise. Other acoustic scientists advance understanding of how sound is affected as it moves through environments, e.g. Underwater acoustics, Architectural acoustics or Structural acoustics. Others areas of work are listed under subdisciplines below. Acoustic scientists work in government, university and private industry laboratories. Many go on to work in Acoustical Engineering. Some positions, such as Faculty (academic staff) require a Doctor of Philosophy.
Subdisciplines.
These subdisciplines are a slightly modified list from the PACS (Physics and Astronomy Classification Scheme) coding used by the Acoustical Society of America.
Archaeoacoustics.
Archaeoacoustics is the study of sound within archaeology. This typically involves studying the acoustics of archaeological sites and artefacts.
Aeroacoustics.
Aeroacoustics is the study of noise generated by air movement, for instance via turbulence, and the movement of sound through the fluid air. This knowledge is applied in acoustical engineering to study how to quieten aircraft. Aeroacoustics is important to understanding how wind musical instruments work.
Acoustic signal processing.
Acoustic signal processing is the electronic manipulation of acoustic signals. Applications include: active noise control; design for hearing aids or cochlear implants; echo cancellation; music information retrieval, and perceptual coding (e.g. MP3 or Opus).
Architectural acoustics.
Architectural acoustics (also known as building acoustics) involves the scientific understanding of how to achieve a good sound within a building. It typically involves the study of speech intelligibility, speech privacy and music quality in the built environment.
Bioacoustics.
Bioacoustics is the scientific study of the hearing and calls of animal calls, as well as how animals are affected by the acoustic and sounds of their habitat.
Electroacoustics.
This subdiscipline is concerned with the recording, manipulation and reproduction of audio using electronics. This might include products such as mobile phones, large scale public address systems or virtual reality systems in research laboratories.
Environmental noise and soundscapes.
Environmental acoustics is concerned with noise and vibration caused by railways, road traffic, aircraft, industrial equipment and recreational activities. The main aim of these studies is to reduce levels of environmental noise and vibration. Research work now also has a focus on the positive use of sound in urban environments: soundscapes and tranquility.
Musical acoustics.
Musical acoustics is the study of the physics of acoustic instruments; the audio signal processing used in electronic music; the computer analysis of music and composition, and the perception and cognitive neuroscience of music.
Psychoacoustics.
Psychoacoustics explains how humans respond to sounds.
Speech.
Acousticians study the production, processing and perception of speech. Speech recognition and Speech synthesis are two important areas of speech processing using computers. The subject also overlaps with the disciplines of physics, physiology, psychology, and linguistics.
Ultrasonics.
Ultrasonics deals with sounds at frequencies too high to be heard by humans. Specialisms include medical ultrasonics (including medical ultrasonography), sonochemistry, material characterisation and underwater acoustics (Sonar).
Underwater acoustics.
Underwater acoustics is the scientific study of natural and man-made sounds underwater. Applications include sonar to locate submarines, underwater communication by whales, climate change monitoring by measuring sea temperatures acoustically, sonic weapons, and marine bioacoustics.
Vibration and dynamics.
This is the study of how mechanical systems vibrate and interact with their surroundings. Applications might include: ground vibrations from railways; vibration isolation to reduce vibration in operating theatres; studying how vibration can damage health (vibration white finger); vibration control to protect a building from earthquakes, or measuring how structure-borne sound moves through buildings.

</doc>
<doc id="1202" url="http://en.wikipedia.org/wiki?curid=1202" title="Applet">
Applet

In computing, an applet is any small application that performs one specific task that runs within the scope of a dedicated widget engine or a larger program, often as a plug-in. The term is frequently used to refer to a Java applet, a program written in the Java programming language that is designed to be placed on a web page. Applets are typical examples of transient and auxiliary applications that don't monopolize the user's attention. Applets are not full-featured application programs, and are intended to be easily accessible.
The word "applet" was first used in 1990 in PC Magazine.
Applet as an extension of other software.
In some cases, an applet does not run independently. These applets must run either in a container provided by a host program, through a plugin, or a variety of other applications including mobile devices that support the applet programming model.
Web-based Applets.
Applets are used to provide interactive features to web applications that cannot be provided by HTML alone. They can capture mouse input and also have controls like buttons or check boxes. In response to the user action an applet can change the provided graphic content. This makes applets well suitable for demonstration, visualization, and teaching. There are online applet collections for studying various subjects, from physics to heart physiology. Applets are also used to create online game collections that allow players to compete against live opponents in real-time.
An applet can also be a text area only, providing, for instance, a cross platform command-line interface to some remote system. If needed, an applet can leave the dedicated area and run as a separate window. However, applets have very little control over web page content outside the applet dedicated area, so they are less useful for improving the site appearance in general (while applets like news tickers or WYSIWYG editors are also known). Applets can also play media in formats that are not natively supported by the browser
HTML pages may embed parameters that are passed to the applet. Hence the same applet may appear differently depending on the parameters that were passed.
Examples of Web-based Applets include:
Applet Vs. Subroutine.
A larger application distinguishes its applets through several features:
Java Applet.
Java Applets is a java program that is launched from HTML and run in a web browser. Java applet can provide web applications with interactive features that cannot be provided by HTML. Since Java's bytecode is platform-independent, Java applets can be executed by browsers running under many platforms, including Windows, Unix, Mac OS, and Linux. When a Java technology-enabled web browser processes a page that contains an applet, the applet's code is transferred to the client's system and executed by the browser's Java Virtual Machine (JVM). An HTML page references an applet either via the deprecated <applet> tag or via its replacement, the <object> tag.
Security.
Recent developments in the coding of applications including mobile and embedded systems have led to the awareness of the security of applets.
Open Platform Applets.
Applets in an open platform environment should provide secure interactions between different applications. A compositional approach can be used to provide security for open platform applets. Advanced compositional verification methods have been developed for secure applet interactions.
Java Applets.
A Java applet contains different security models: unsigned Java applet security, signed Java applet security, and self signed Java applet security.
Web-based Applets.
In an applet-enabled web browser, many methods can be used to provide applet security for malicious applets. A malicious applet can infect a computer system in many ways, including denial of service, invasion of privacy, and annoyance. A typical solution for malicious applets is to make the web browser to monitor applets' activities. This will result in a web browser that will enable the manual or automatic stopping of malicious applets. To illustrate this method, AppletGuard was used to observe and control any applet in a browser successfully.

</doc>
<doc id="1206" url="http://en.wikipedia.org/wiki?curid=1206" title="Atomic orbital">
Atomic orbital

An atomic orbital is a mathematical function that describes the wave-like behavior of either one electron or a pair of electrons in an atom. This function can be used to calculate the probability of finding any electron of an atom in any specific region around the atom's nucleus. The term may also refer to the physical region or space where the electron can be calculated to be present, as defined by the particular mathematical form of the orbital.
Each orbital in an atom is characterized by a unique set of values of the three quantum numbers n, ℓ, and m, which respectively correspond to the electron's energy, angular momentum, and an angular momentum vector component (the magnetic quantum number). Any orbital can be occupied by a maximum of two electrons, each with its own spin quantum number. The simple names s orbital, p orbital, d orbital and f orbital refer to orbitals with angular momentum quantum number "ℓ" = 0, 1, 2 and 3 respectively. These names, together with the value of n, are used to describe the electron configurations of atoms. They are derived from the description by early spectroscopists of certain series of alkali metal spectroscopic lines as sharp, principal, diffuse, and fundamental. Orbitals for ℓ > 3 continue alphabetically, omitting j (g, h, i, k, …).
Atomic orbitals are the basic building blocks of the atomic orbital model (alternatively known as the electron cloud or wave mechanics model), a modern framework for visualizing the submicroscopic behavior of electrons in matter. In this model the electron cloud of a multi-electron atom may be seen as being built up (in approximation) in an electron configuration that is a product of simpler hydrogen-like atomic orbitals. The repeating "periodicity" of the blocks of 2, 6, 10, and 14 elements within sections of the periodic table arises naturally from the total number of electrons that occupy a complete set of s, p, d and f atomic orbitals, respectively.
Electron properties.
With the development of quantum mechanics and experimental findings (such as the two slits diffraction of electrons), it was found that the orbiting electrons around a nucleus could not be fully described as particles, but needed to be explained by the wave-particle duality. In this sense, the electrons have the following properties:
Wave-like properties:
Particle-like properties:
Thus, despite the obvious analogy to planets revolving around the Sun, electrons cannot be described simply as solid particles. In addition, atomic orbitals do not closely resemble a planet's elliptical path in ordinary atoms. A more accurate analogy might be that of a large and often oddly shaped "atmosphere" (the electron), distributed around a relatively tiny planet (the atomic nucleus). Atomic orbitals exactly describe the shape of this "atmosphere" only when a single electron is present in an atom. When more electrons are added to a single atom, the additional electrons tend to more evenly fill in a volume of space around the nucleus so that the resulting collection (sometimes termed the atom’s “electron cloud”) tends toward a generally spherical zone of probability describing where the atom’s electrons will be found.
Formal quantum mechanical definition.
Atomic orbitals may be defined more precisely in formal quantum mechanical language. Specifically, in quantum mechanics, the state of an atom, i.e. an eigenstate of the atomic Hamiltonian, is approximated by an expansion (see configuration interaction expansion and basis set) into linear combinations of anti-symmetrized products (Slater determinants) of one-electron functions. The spatial components of these one-electron functions are called atomic orbitals. (When one considers also their spin component, one speaks of atomic spin orbitals.) A state is actually a function of the coordinates of all the electrons, so that their motion is correlated, but this is often approximated by this independent-particle model of products of single electron wave functions. (The London dispersion force, for example, depends on the correlations of the motion of the electrons.)
In atomic physics, the atomic spectral lines correspond to transitions (quantum leaps) between quantum states of an atom. These states are labeled by a set of quantum numbers summarized in the term symbol and usually associated with particular electron configurations, i.e., by occupation schemes of atomic orbitals (for example, 1s2 2s2 2p6 for the ground state of neon—term symbol: 1S0).
This notation means that the corresponding Slater determinants have a clear higher weight in the configuration interaction expansion. The atomic orbital concept is therefore a key concept for visualizing the excitation process associated with a given transition. For example, one can say for a given transition that it corresponds to the excitation of an electron from an occupied orbital to a given unoccupied orbital. Nevertheless, one has to keep in mind that electrons are fermions ruled by the Pauli exclusion principle and cannot be distinguished from the other electrons in the atom. Moreover, it sometimes happens that the configuration interaction expansion converges very slowly and that one cannot speak about simple one-determinant wave function at all. This is the case when electron correlation is large.
Fundamentally, an atomic orbital is a one-electron wave function, even though most electrons do not exist in one-electron atoms, and so the one-electron view is an approximation. When thinking about orbitals, we are often given an orbital vision which (even if it is not spelled out) is heavily influenced by this Hartree–Fock approximation, which is one way to reduce the complexities of molecular orbital theory.
Types of orbitals.
Atomic orbitals can be the hydrogen-like "orbitals" which are exact solutions to the Schrödinger equation for a hydrogen-like "atom" (i.e., an atom with one electron). Alternatively, atomic orbitals refer to functions that depend on the coordinates of one electron (i.e. orbitals) but are used as starting points for approximating wave functions that depend on the simultaneous coordinates of all the electrons in an atom or molecule. The coordinate systems chosen for atomic orbitals are usually spherical coordinates ("r", θ, φ) in atoms and cartesians (x, y, z) in polyatomic molecules. The advantage of spherical coordinates (for atoms) is that an orbital wave function is a product of three factors each dependent on a single coordinate: ψ("r", θ, φ) = "R"("r") Θ(θ) Φ(φ).
The angular factors of atomic orbitals Θ(θ) Φ(φ) generate s, p, d, etc. functions as real combinations of spherical harmonics "Y""ℓm"(θ, φ) (where ℓ and m are quantum numbers). There are typically three mathematical forms for the radial functions "R"("r") which can be chosen as a starting point for the calculation of the properties of atoms and molecules with many electrons.
Although hydrogen-like orbitals are still used as pedagogical tools, the advent of computers has made STOs preferable for atoms and diatomic molecules since combinations of STOs can replace the nodes in hydrogen-like atomic orbital. Gaussians are typically used in molecules with three or more atoms. Although not as accurate by themselves as STOs, combinations of many Gaussians can attain the accuracy of hydrogen-like orbitals.
History.
The term "orbital" was coined by Robert Mulliken in 1932 as an abbreviation for "one-electron orbital wave function". However, the idea that electrons might revolve around a compact nucleus with definite angular momentum was convincingly argued at least 19 years earlier by Niels Bohr, and the Japanese physicist Hantaro Nagaoka published an orbit-based hypothesis for electronic behavior as early as 1904.
Explaining the behavior of these electron "orbits" was one of the driving forces behind the development of quantum mechanics.
Early models.
With J.J. Thomson's discovery of the electron in 1897, it became clear that atoms were not the smallest building blocks of nature, but were rather composite particles. The newly discovered structure within atoms tempted many to imagine how the atom's constituent parts might interact with each other. Thomson theorized that multiple electrons revolved in orbit-like rings within a positively charged jelly-like substance, and between the electron's discovery and 1909, this "plum pudding model" was the most widely accepted explanation of atomic structure.
Shortly after Thomson's discovery, Hantaro Nagaoka, a Japanese physicist, predicted a different model for electronic structure. Unlike the plum pudding model, the positive charge in Nagaoka's "Saturnian Model" was concentrated into a central core, pulling the electrons into circular orbits reminiscent of Saturn's rings. Few people took notice of Nagaoka's work at the time,
and Nagaoka himself recognized a fundamental defect in the theory even at its conception, namely that a classical charged object cannot sustain orbital motion because it is accelerating and therefore loses energy due to electromagnetic radiation. Nevertheless, the Saturnian model turned out to have more in common with modern theory than any of its contemporaries.
Bohr atom.
In 1909, Ernest Rutherford discovered that bulk of the atomic mass was tightly condensed into a nucleus, which was also found to be positively charged. It became clear from his analysis in 1911 that the plum pudding model could not explain atomic structure. Shortly after, in 1913, Rutherford's post-doctoral student Niels Bohr proposed a new model of the atom, wherein electrons orbited the nucleus with classical periods, but were only permitted to have discrete values of angular momentum, quantized in units "h"/2π. This constraint automatically permitted only certain values of electron energies. The Bohr model of the atom fixed the problem of energy loss from radiation from a ground state (by declaring that there was no state below this), and more importantly explained the origin of spectral lines.
After Bohr's use of Einstein's explanation of the photoelectric effect to relate energy levels in atoms with the wavelength of emitted light, the connection between the structure of electrons in atoms and the emission and absorption spectra of atoms became an increasingly useful tool in the understanding of electrons in atoms. The most prominent feature of emission and absorption spectra (known experimentally since the middle of the 19th century), was that these atomic spectra contained discrete lines. The significance of the Bohr model was that it related the lines in emission and absorption spectra to the energy differences between the orbits that electrons could take around an atom. This was, however, "not" achieved by Bohr through giving the electrons some kind of wave-like properties, since the idea that electrons could behave as matter waves was not suggested until eleven years later. Still, the Bohr model's use of quantized angular momenta and therefore quantized energy levels was a significant step towards the understanding of electrons in atoms, and also a significant step towards the development of quantum mechanics in suggesting that quantized restraints must account for all discontinuous energy levels and spectra in atoms.
With de Broglie's suggestion of the existence of electron matter waves in 1924, and for a short time before the full 1926 Schrödinger equation treatment of hydrogen-like atom, a Bohr electron "wavelength" could be seen to be a function of its momentum, and thus a Bohr orbiting electron was seen to orbit in a circle at a multiple of its half-wavelength (this physically incorrect Bohr model is still often taught to beginning students). The Bohr model for a short time could be seen as a classical model with an additional constraint provided by the 'wavelength' argument. However, this period was immediately superseded by the full three-dimensional wave mechanics of 1926. In our current understanding of physics, the Bohr model is called a semi-classical model because of its quantization of angular momentum, not primarily because of its relationship with electron wavelength, which appeared in hindsight a dozen years after the Bohr model was proposed.
The Bohr model was able to explain the emission and absorption spectra of hydrogen. The energies of electrons in the "n" = 1, 2, 3, etc. states in the Bohr model match those of current physics. However, this did not explain similarities between different atoms, as expressed by the periodic table, such as the fact that helium (two electrons), neon (10 electrons), and argon (18 electrons) exhibit similar chemical inertness. Modern quantum mechanics explains this in terms of electron shells and subshells which can each hold a number of electrons determined by the Pauli exclusion principle. Thus the "n" = 1 state can hold one or two electrons, while the "n" = 2 state can hold up to eight electrons in 2s and 2p subshells. In helium, all "n" = 1 states are fully occupied; the same for "n" = 1 and "n" = 2 in neon. In argon the 3s and 3p subshells are similarly fully occupied by eight electrons; quantum mechanics also allows a 3d subshell but this is at higher energy than the 3s and 3p in argon (contrary to the situation in the hydrogen atom) and remains empty.
Modern conceptions and connections to the Heisenberg Uncertainty Principle.
Immediately after Heisenberg discovered his uncertainty relation,
it was noted by Bohr that the existence of any sort of wave packet implies uncertainty in the wave frequency and wavelength, since a spread of frequencies is needed to create the packet itself.
In quantum mechanics, where all particle momenta are associated with waves, it is the formation of such a wave packet which localizes the wave, and thus the particle, in space. In states where a quantum mechanical particle is bound, it must be localized as a wave packet, and the existence of the packet and its minimum size implies a spread and minimal value in particle wavelength, and thus also momentum and energy. In quantum mechanics, as a particle is localized to a smaller region in space, the associated compressed wave packet requires a larger and larger range of momenta, and thus larger kinetic energy. Thus, the binding energy to contain or trap a particle in a smaller region of space, increases without bound, as the region of space grows smaller. Particles cannot be restricted to a geometric point in space, since this would require an infinite particle momentum.
In chemistry, Schrödinger, Pauling, Mulliken and others noted that the consequence of Heisenberg's relation was that the electron, as a wave packet, could not be considered to have an exact location in its orbital. Max Born suggested that the electron's position needed to be described by a probability distribution which was connected with finding the electron at some point in the wave-function which described its associated wave packet. The new quantum mechanics did not give exact results, but only the probabilities for the occurrence of a variety of possible such results. Heisenberg held that the path of a moving particle has no meaning if we cannot observe it, as we cannot with electrons in an atom.
In the quantum picture of Heisenberg, Schrödinger and others, the Bohr atom number "n" for each orbital became known as an "n-sphere" in a three dimensional atom and was pictured as the mean energy of the probability cloud of the electron's wave packet which surrounded the atom.
Orbital names.
Orbitals are given names in the form:
where "X" is the energy level corresponding to the principal quantum number n, type is a lower-case letter denoting the shape or subshell of the orbital and it corresponds to the angular quantum number ℓ, and y is the number of electrons in that orbital.
For example, the orbital 1s2 (pronounced "one ess two") has two electrons and is the lowest energy level ("n" = 1) and has an angular quantum number of "ℓ" = 0. In X-ray notation, the principal quantum number is given a letter associated with it. For "n" = 1, 2, 3, 4, 5, …, the letters associated with those numbers are K, L, M, N, O, … respectively.
Hydrogen-like orbitals.
The simplest atomic orbitals are those that are calculated for systems with a single electron, such as the hydrogen atom. An atom of any other element ionized down to a single electron is very similar to hydrogen, and the orbitals take the same form. In the Schrödinger equation for this system of one negative and one positive particle, the atomic orbitals are the eigenstates of the Hamiltonian operator for the energy. They can be obtained analytically, meaning that the resulting orbitals are products of a polynomial series, and exponential and trigonometric functions. (see hydrogen atom).
For atoms with two or more electrons, the governing equations can only be solved with the use of methods of iterative approximation. Orbitals of multi-electron atoms are "qualitatively" similar to those of hydrogen, and in the simplest models, they are taken to have the same form. For more rigorous and precise analysis, the numerical approximations must be used.
A given (hydrogen-like) atomic orbital is identified by unique values of three quantum numbers: n, ℓ, and mℓ. The rules restricting the values of the quantum numbers, and their energies (see below), explain the electron configuration of the atoms and the periodic table.
The stationary states (quantum states) of the hydrogen-like atoms are its atomic orbitals. However, in general, an electron's behavior is not fully described by a single orbital. Electron states are best represented by time-depending "mixtures" (linear combinations) of multiple orbitals. See Linear combination of atomic orbitals molecular orbital method.
The quantum number n first appeared in the Bohr model where it determines the radius of each circular electron orbit. In modern quantum mechanics however, n determines the mean distance of the electron from the nucleus; all electrons with the same value of "n" lie at the same average distance. For this reason, orbitals with the same value of "n" are said to comprise a "shell". Orbitals with the same value of "n" and also the same value of ℓ are even more closely related, and are said to comprise a "subshell".
Quantum numbers.
Because of the quantum mechanical nature of the electrons around a nucleus, atomic orbitals can be uniquely defined by a set of integers known as quantum numbers. These quantum numbers only occur in certain combinations of values, and their physical interpretation changes depending on whether real or complex versions of the atomic orbitals are employed.
Complex orbitals.
In physics, the most common orbital descriptions are based on the solutions to the hydrogen atom, where orbitals are given by the product between a radial function and a pure spherical harmonic. The quantum numbers, together with the rules governing their possible values, are as follows:
The principal quantum number n describes the energy of the electron and is always a positive integer. In fact, it can be any positive integer, but for reasons discussed below, large numbers are seldom encountered. Each atom has, in general, many orbitals associated with each value of "n"; these orbitals together are sometimes called "electron shells".
The azimuthal quantum number ℓ describes the orbital angular momentum of each electron and is a non-negative integer. Within a shell where n is some integer "n"0, ℓ ranges across all (integer) values satisfying the relation formula_2. For instance, the "n" = 1 shell has only orbitals with formula_3, and the "n" = 2 shell has only orbitals with formula_3, and formula_5. The set of orbitals associated with a particular value of ℓ are sometimes collectively called a "subshell".
The magnetic quantum number, formula_6, describes the magnetic moment of an electron in an arbitrary direction, and is also always an integer. Within a subshell where formula_7 is some integer formula_8, formula_6 ranges thus: formula_10.
The above results may be summarized in the following table. Each cell represents a subshell, and lists the values of formula_6 available in that subshell. Empty cells represent subshells that do not exist.
Subshells are usually identified by their formula_12- and formula_7-values. formula_12 is represented by its numerical value, but formula_7 is represented by a letter as follows: 0 is represented by 's', 1 by 'p', 2 by 'd', 3 by 'f', and 4 by 'g'. For instance, one may speak of the subshell with formula_16 and formula_3 as a '2s subshell'.
Each electron also has a spin quantum number, s, which describes the spin of each electron (spin up or spin down). The number s can be +1⁄2 or −1⁄2.
The Pauli exclusion principle states that no two electrons can occupy the same quantum state: every electron in an atom must have a unique combination of quantum numbers.
The above conventions imply a preferred axis (for example, the "z" direction in Cartesian coordinates), and they also imply a preferred direction along this preferred axis. Otherwise there would be no sense in distinguishing "m" = +1 from "m" = −1. As such, the model is most useful when applied to physical systems that share these symmetries. The Stern–Gerlach experiment — where an atom is exposed to a magnetic field — provides one such example.
Real orbitals.
An atom that is embedded in a crystalline solid feels multiple preferred axes, but no preferred direction. Instead of building atomic orbitals out of the product of radial functions and a single spherical harmonic, linear combinations of spherical harmonics are typically used, designed so that the imaginary part of the spherical harmonics cancel out. These real orbitals are the building blocks most commonly shown in orbital visualizations.
In the real hydrogen-like orbitals, for example, n and ℓ have the same interpretation and significance as their complex counterparts, but m is no longer a good quantum number (though its absolute value is). The orbitals are given new names based on their shape with respect to a standardized Cartesian basis. The real hydrogen-like p orbitals are given by the following
where "p"0 = "R""n" 1 "Y"1 0, "p"1 = "R""n" 1 "Y"1 1, and "p"−1 = "R""n" 1 "Y"1 −1, are the complex orbitals corresponding to "ℓ" = 1.
Shapes of orbitals.
Simple pictures showing orbital shapes are intended to describe the angular forms of regions in space where the electrons occupying the orbital are likely to be found. The diagrams cannot, however, show the entire region where an electron can be found, since according to quantum mechanics there is a non-zero probability of finding the electron (almost) anywhere in space. Instead the diagrams are approximate representations of boundary or contour surfaces where the probability density | ψ("r", θ, φ) |2 has a constant value, chosen so that there is a certain probability (for example 90%) of finding the electron within the contour. Although | ψ |2 as the square of an absolute value is everywhere non-negative, the sign of the wave function ψ("r", θ, φ) is often indicated in each subregion of the orbital picture.
Sometimes the ψ function will be graphed to show its phases, rather than the | ψ("r", θ, φ) |2 which shows probability density but has no phases (which have been lost in the process of taking the absolute value, since ψ("r", θ, φ) is a complex number). | ψ("r", θ, φ) |2 orbital graphs tend to have less spherical, thinner lobes than ψ("r", θ, φ) graphs, but have the same number of lobes in the same places, and otherwise are recognizable. This article, in order to show wave function phases, shows mostly ψ("r", θ, φ) graphs.
The lobes can be viewed as interference patterns between the two counter rotating "m" and "−"m"" modes, with the projection of the orbital onto the xy plane having a resonant "m" wavelengths around the circumference. For each m there are two of these ⟨"m"⟩+⟨−"m"⟩ and ⟨"m"⟩−⟨−"m"⟩. For the case where "m" = 0 the orbital is vertical, counter rotating information is unknown, and the orbital is z-axis symmetric. For the case where "ℓ" = 0 there are no counter rotating modes. There are only radial modes and the shape is spherically symmetric. For any given n, the smaller ℓ is, the more radial nodes there are. Loosely speaking "n" is energy, ℓ is analogous to eccentricity, and m is orientation.
Generally speaking, the number n determines the size and energy of the orbital for a given nucleus: as n increases, the size of the orbital increases. However, in comparing different elements, the higher nuclear charge Z of heavier elements causes their orbitals to contract by comparison to lighter ones, so that the overall size of the whole atom remains very roughly constant, even as the number of electrons in heavier elements (higher Z) increases.
Also in general terms, ℓ determines an orbital's shape, and mℓ its orientation. However, since some orbitals are described by equations in complex numbers, the shape sometimes depends on mℓ also. Together, the whole set of orbitals for a given ℓ and n fill space as symmetrically as possible, though with increasingly complex sets of lobes and nodes.
The single s-orbitals (formula_3) are shaped like spheres. For "n" = 1 it is roughly a solid ball (it is most dense at the center and fades exponentially outwardly), but for "n" = 2 or more, each single s-orbital is composed of spherically symmetric surfaces which are nested shells (i.e., the "wave-structure" is radial, following a sinusoidal radial component as well). See illustration of a cross-section of these nested shells, at right. The s-orbitals for all n numbers are the only orbitals with an anti-node (a region of high wave function density) at the center of the nucleus. All other orbitals (p, d, f, etc.) have angular momentum, and thus avoid the nucleus (having a wave node "at" the nucleus).
The three p-orbitals for "n" = 2 have the form of two ellipsoids with a point of tangency at the nucleus (the two-lobed shape is sometimes referred to as a "dumbbell"—there are two lobes pointing in opposite directions from each other). The three p-orbitals in each shell are oriented at right angles to each other, as determined by their respective linear combination of values of mℓ. The overall result is a lobe pointing along each direction of the primary axes.
Four of the five d-orbitals for "n" = 3 look similar, each with four pear-shaped lobes, each lobe tangent at right angles to two others, and the centers of all four lying in one plane. Three of these planes are the xy-, xz-, and yz-planes—the lobes are between the pairs of primary axes—and the fourth has the centres along the x and y axes themselves. The fifth and final d-orbital consists of three regions of high probability density: a torus with two pear-shaped regions placed symmetrically on its z axis. The overall total of 18 directional lobes point in every primary axis direction and between every pair.
There are seven f-orbitals, each with shapes more complex than those of the d-orbitals.
Additionally, as is the case with the s orbitals, individual p, d, f and g orbitals with n values higher than the lowest possible value, exhibit an additional radial node structure which is reminiscent of harmonic waves of the same type, as compared with the lowest (or fundamental) mode of the wave. As with s orbitals, this phenomenon provides p, d, f, and g orbitals at the next higher possible value of n (for example, 3p orbitals vs. the fundamental 2p), an additional node in each lobe. Still higher values of n further increase the number of radial nodes, for each type of orbital.
The shapes of atomic orbitals in one-electron atom are related to 3-dimensional spherical harmonics. These shapes are not unique, and any linear combination is valid, like a transformation to cubic harmonics, in fact it is possible to generate sets where all the d's are the same shape, just like the "p""x", "p""y", and "p""z" are the same shape.
Orbitals table.
This table shows all orbital configurations for the real hydrogen-like wave functions up to 7s, and therefore covers the simple electronic configuration for all elements in the periodic table up to radium. "ψ" graphs are shown with − and + wave function phases shown in two different colors (arbitrarily red and blue). The "p""z" orbital is the same as the "p""0" orbital, but the "p""x" and "p""y" are formed by taking linear
combinations of the "p""+1" and "p""−1" orbitals (which is why they are listed under the "m" = ±1 label). Also, the "p""+1" and "p""−1" are not
the same shape as the "p""0", since they are pure spherical harmonics.
Qualitative understanding of shapes.
The shapes of atomic orbitals can be understood qualitatively by considering the analogous case of standing waves on a circular drum. To see the analogy, the mean vibrational displacement of each bit of drum membrane from the equilibrium point over many cycles (a measure of average drum membrane velocity and momentum at that point) must be considered relative to that point's distance from the center of the drum head. If this displacement is taken as being analogous to the probability of finding an electron at a given distance from the nucleus, then it will be seen that the many modes of the vibrating disk form patterns that trace the various shapes of atomic orbitals. The basic reason for this correspondence lies in the fact that the distribution of kinetic energy and momentum in a matter-wave is predictive of where the particle associated with the wave will be. That is, the probability of finding an electron at a given place is also a function of the electron's average momentum at that point, since high electron momentum at a given position tends to "localize" the electron in that position, via the properties of electron wave-packets (see the Heisenberg uncertainty principle for details of the mechanism).
This relationship means that certain key features can be observed in both drum membrane modes and atomic orbitals. For example, in all of the modes analogous to s orbitals (the top row in the animated illustration below), it can be seen that the very center of the drum membrane vibrates most strongly, corresponding to the antinode in all s orbitals in an atom. This antinode means the electron is most likely to be at the physical position of the nucleus (which it passes straight through without scattering or striking it), since it is moving (on average) most rapidly at that point, giving it maximal momentum.
A mental "planetary orbit" picture closest to the behavior of electrons in s orbitals, all of which have no angular momentum, might perhaps be that of a Keplerian orbit with the orbital eccentricity of 1 but a finite major axis, not physically possible (because particles were to collide), but can be imagined as a limit of orbits with equal major axes but increasing eccentricity.
Below, a number of drum membrane vibration modes are shown. The analogous wave functions of the hydrogen atom are indicated. A correspondence can be considered where the wave functions of a vibrating drum head are for a two-coordinate system ψ("r", θ) and the wave functions for a vibrating sphere are three-coordinate ψ("r", θ, φ).
s-type modes
None of the other sets of modes in a drum membrane have a central antinode, and in all of them the center of the drum does not move. These correspond to a node at the nucleus for all non-s orbitals in an atom. These orbitals all have some angular momentum, and in the planetary model, they correspond to particles in orbit with eccentricity less than 1.0, so that they do not pass straight through the center of the primary body, but keep somewhat away from it.
In addition, the drum modes analogous to p and d modes in an atom show spatial irregularity along the different radial directions from the center of the drum, whereas all of the modes analogous to s modes are perfectly symmetrical in radial direction. The non radial-symmetry properties of non-s orbitals are necessary to localize a particle with angular momentum and a wave nature in an orbital where it must tend to stay away from the central attraction force, since any particle localized at the point of central attraction could have no angular momentum. For these modes, waves in the drum head tend to avoid the central point. Such features again emphasize that the shapes of atomic orbitals are a direct consequence of the wave nature of electrons.
p-type modes
d-type modes
Orbital energy.
In atoms with a single electron (hydrogen-like atoms), the energy of an orbital (and, consequently, of any electrons in the orbital) is determined exclusively by formula_12. The formula_23 orbital has the lowest possible energy in the atom. Each successively higher value of formula_12 has a higher level of energy, but the difference decreases as formula_12 increases. For high formula_12, the level of energy becomes so high that the electron can easily escape from the atom. In single electron atoms, all levels with different formula_7 within a given formula_12 are (to a good approximation) degenerate, and have the same energy. This approximation is broken to a slight extent by the effect of the magnetic field of the nucleus, and by quantum electrodynamics effects. The latter induce tiny binding energy differences especially for s electrons that go nearer the nucleus, since these feel a very slightly different nuclear charge, even in one-electron atoms; see Lamb shift.
In atoms with multiple electrons, the energy of an electron depends not only on the intrinsic properties of its orbital, but also on its interactions with the other electrons. These interactions depend on the detail of its spatial probability distribution, and so the energy levels of orbitals depend not only on formula_12 but also on formula_7. Higher values of formula_7 are associated with higher values of energy; for instance, the 2p state is higher than the 2s state. When formula_32, the increase in energy of the orbital becomes so large as to push the energy of orbital above the energy of the s-orbital in the next higher shell; when formula_33 the energy is pushed into the shell two steps higher. The filling of the 3d orbitals does not occur until the 4s orbitals have been filled.
The increase in energy for subshells of increasing angular momentum in larger atoms is due to electron–electron interaction effects, and it is specifically related to the ability of low angular momentum electrons to penetrate more effectively toward the nucleus, where they are subject to less screening from the charge of intervening electrons. Thus, in atoms of higher atomic number, the formula_7 of electrons becomes more and more of a determining factor in their energy, and the principal quantum numbers formula_12 of electrons becomes less and less important in their energy placement.
The energy sequence of the first 24 subshells (e.g., 1s, 2p, 3d, etc.) is given in the following table. Each cell represents a subshell with formula_12 and formula_7 given by its row and column indices, respectively. The number in the cell is the subshell's position in the sequence. For a linear listing of the subshells in terms of increasing energies in multielectron atoms, see the section below.
"Note: empty cells indicate non-existent sublevels, while numbers in italics indicate sublevels that could exist, but which do not hold electrons in any element currently known."
Electron placement and the periodic table.
Several rules govern the placement of electrons in orbitals ("electron configuration"). The first dictates that no two electrons in an atom may have the same set of values of quantum numbers (this is the Pauli exclusion principle). These quantum numbers include the three that define orbitals, as well as s, or spin quantum number. Thus, two electrons may occupy a single orbital, so long as they have different values of s. However, "only" two electrons, because of their spin, can be associated with each orbital.
Additionally, an electron always tends to fall to the lowest possible energy state. It is possible for it to occupy any orbital so long as it does not violate the Pauli exclusion principle, but if lower-energy orbitals are available, this condition is unstable. The electron will eventually lose energy (by releasing a photon) and drop into the lower orbital. Thus, electrons fill orbitals in the order specified by the energy sequence given above.
This behavior is responsible for the structure of the periodic table. The table may be divided into several rows (called 'periods'), numbered starting with 1 at the top. The presently known elements occupy seven periods. If a certain period has number "i", it consists of elements whose outermost electrons fall in the "i"th shell. Niels Bohr was the first to propose (1923) that the periodicity in the properties of the elements might be explained by the periodic filling of the electron energy levels, resulting in the electronic structure of the atom.
The periodic table may also be divided into several numbered rectangular 'blocks'. The elements belonging to a given block have this common feature: their highest-energy electrons all belong to the same ℓ-state (but the n associated with that ℓ-state depends upon the period). For instance, the leftmost two columns constitute the 's-block'. The outermost electrons of Li and Be respectively belong to the 2s subshell, and those of Na and Mg to the 3s subshell.
The following is the order for filling the "subshell" orbitals, which also gives the order of the "blocks" in the periodic table:
The "periodic" nature of the filling of orbitals, as well as emergence of the s, p, d and f "blocks", is more obvious if this order of filling is given in matrix form, with increasing principal quantum numbers starting the new rows ("periods") in the matrix. Then, each subshell (composed of the first two quantum numbers) is repeated as many times as required for each pair of electrons it may contain. The result is a compressed periodic table, with each entry representing two successive elements:
Although this is the general order of orbital filling according to the Madelung rule, there are exceptions, and the actual electronic energies of each element are also dependent upon additional details of the atoms (see ).
The number of electrons in an electrically neutral atom increases with the atomic number. The electrons in the outermost shell, or "valence electrons", tend to be responsible for an element's chemical behavior. Elements that contain the same number of valence electrons can be grouped together and display similar chemical properties.
Relativistic effects.
For elements with high atomic number Z, the effects of relativity become more pronounced, and especially so for s electrons, which move at relativistic velocities as they penetrate the screening electrons near the core of high-Z atoms. This relativistic increase in momentum for high speed electrons causes a corresponding decrease in wavelength and contraction of 6s orbitals relative to 5d orbitals (by comparison to corresponding s and d electrons in lighter elements in the same column of the periodic table); this results in 6s valence electrons becoming lowered in energy.
Examples of significant physical outcomes of this effect include the lowered melting temperature of mercury (which results from 6s electrons not being available for metal bonding) and the golden color of gold and caesium (which results from narrowing of 6s to 5d transition energy to the point that visible light begins to be absorbed).
In the Bohr Model, an  electron has a velocity given by formula_38, where Z is the atomic number, formula_39 is the fine-structure constant, and "c" is the speed of light. In non-relativistic quantum mechanics, therefore, any atom with an atomic number greater than 137 would require its 1s electrons to be traveling faster than the speed of light. Even in the Dirac equation, which accounts for relativistic effects, the wavefunction of the electron for atoms with "Z" > 137 is oscillatory and unbounded. The significance of element 137, also known as untriseptium, was first pointed out by the physicist Richard Feynman. Element 137 is sometimes informally called feynmanium (symbol Fy) . However, Feynman's approximation fails to predict the exact critical value of Z due to the non-point-charge nature of the nucleus and very small orbital radius of inner electrons, resulting in a potential seen by inner electrons which is effectively less than Z. The critical Z value which makes the atom unstable with regard to high-field breakdown of the vacuum and production of electron-positron pairs, does not occur until Z is about 173. These conditions are not seen except transiently in collisions of very heavy nuclei such as lead or uranium in accelerators, where such electron-positron production from these effects has been claimed to be observed. See Extension of the periodic table beyond the seventh period.
There are no nodes in relativistic orbital densities, although individual components of the wavefunction will have nodes.
Transitions between orbitals.
Bound quantum states have discrete energy levels. When applied to atomic orbitals, this means that the energy differences between states are also discrete. A transition between these states (i.e. an electron absorbing or emitting a photon) can thus only happen if the photon has an energy corresponding with the exact energy difference between said states.
Consider two states of the hydrogen atom:
State 1) "n" = 1, "ℓ" = 0, "m""ℓ" = 0 and "s" = +1⁄2
State 2) "n" = 2, "ℓ" = 0, "m""ℓ" = 0 and "s" = +1⁄2
By quantum theory, state 1 has a fixed energy of "E"1, and state 2 has a fixed energy of "E"2. Now, what would happen if an electron in state 1 were to move to state 2? For this to happen, the electron would need to gain an energy of exactly "E"2 − "E"1. If the electron receives energy that is less than or greater than this value, it cannot jump from state 1 to state 2. Now, suppose we irradiate the atom with a broad-spectrum of light. Photons that reach the atom that have an energy of exactly "E"2 − "E"1 will be absorbed by the electron in state 1, and that electron will jump to state 2. However, photons that are greater or lower in energy cannot be absorbed by the electron, because the electron can only jump to one of the orbitals, it cannot jump to a state between orbitals. The result is that only photons of a specific frequency will be absorbed by the atom. This creates a line in the spectrum, known as an absorption line, which corresponds to the energy difference between states 1 and 2.
The atomic orbital model thus predicts line spectra, which are observed experimentally. This is one of the main validations of the atomic orbital model.
The atomic orbital model is nevertheless an approximation to the full quantum theory, which only recognizes many electron states. The predictions of line spectra are qualitatively useful but are not quantitatively accurate for atoms and ions other than those containing only one electron.

</doc>
<doc id="1227" url="http://en.wikipedia.org/wiki?curid=1227" title="Ashmore and Cartier Islands">
Ashmore and Cartier Islands

The Territory of Ashmore and Cartier Islands is an uninhabited external territory of Australia consisting of four low-lying tropical islands in two separate reefs, and the 12 nautical mile territorial sea generated by the islands. The territory is located in the Indian Ocean situated on the edge of the continental shelf, about 320 km off the northwest coast of Australia and 144 km south of the Indonesian island of Rote.
Geography.
The territory includes 155.4 km2 Ashmore Reef (including West, Middle, and East Islands, and two lagoons within the reef) and 44 km2 Cartier Reef (including Cartier Island). They have a total of 74.1 km of shoreline, measured along the outer edge of the reef. There are no ports or harbours, only offshore anchorage.
West, Middle, and East Islands have a combined land area variously reported as 54 ha, 93 ha, and 112 ha (1 hectare is 0.01 km2, or about 2.5 acres). Cartier Island is an unvegetated sand island, with a reported land area of 0.4 ha.
Ashmore Reef is called "Pulau Pasir" by Indonesians. In the Rote Island language, it is called "Nusa Solokaek". Both names have the meaning "Sand Island".
Nearby Hibernia Reef, 42 km northeast of Ashmore Reef, is not part of the territory, but rather belongs to Western Australia. It has no permanently dry land area, although large parts of the reef become exposed during low tide.
Government.
The territory is administered from Canberra by the Department of Regional Australia, Local Government, Arts and Sport, which is also responsible for the administration of the territories of Christmas Island, Cocos (Keeling) Islands, the Coral Sea Islands, Jervis Bay Territory and Norfolk Island. As part of the Machinery of Government Changes following the 2010 Federal Election, administrative responsibility for Territories was transferred from the Attorney General's Department to the Department of Regional Australia, Local Government, Arts and Sport. Defence of Ashmore and Cartier Islands is the responsibility of Australia, with periodic visits by the Royal Australian Navy, Royal Australian Air Force and Australian Customs and Border Protection Service. The vessel ACV "Ashmore Guardian" is stationed more-or-less permanently off the reef. The islands are also visited by seasonal caretakers and occasional scientific researchers.
On 21 October 2002 the nature reserve was recognised as a wetland of international importance when it was designated Ramsar Site 1220 under the Ramsar Convention on Wetlands.
Due to its proximity to Indonesia, and the area being traditional fishing grounds of Indonesian fishermen for centuries, some Indonesian groups claims Ashmore Reef to be part of Rote Ndao Regency of East Nusa Tenggara province. However, the Indonesian government does not appear to actively contest Australia's possession of the territory. Australia's sovereignty is backed up by the fact that the territory was not administered by the Netherlands (Indonesia's former colonial power), but by the British before it was transferred to Australia.
A memorandum of understanding between the Australian and Indonesian governments allows Indonesian fishermen access to their traditional fishing grounds within the region without any formal visa arrangements, subject to limits.
Ecology and environment.
Ashmore Reef Commonwealth Marine Reserve.
The Ashmore Reef Commonwealth Marine Reserve (formerly Ashmore Reef National Nature Reserve), established in August 1983, comprises an area of approximately 583 km2. It is of significant biodiversity value as it is in the flow of the Indonesian Throughflow ocean current from the Pacific Ocean through Maritime Southeast Asia to the Indian Ocean. It is also in a surface current west from the Arafura Sea and Timor Sea.
The Reserve comprises several marine habitats, including seagrass meadows, intertidal sand flats, coral reef flats, and lagoons, and supports an important and diverse range of species, including 14 species of sea snakes, a population of dugong that may be genetically distinct, a diverse marine invertebrate fauna, and many endemic species, especially of sea snakes and molluscs. There are feeding and nesting sites for loggerhead, hawksbill and green turtles. It is classified as an Important Bird Area and has 50,000 breeding pairs of various kinds of seabirds. A high abundance and diversity of sea cucumbers, over-exploited on other reefs in the region, is present, with 45 species recorded.
Cartier Island Commonwealth Marine Reserve.
The Cartier Island Commonwealth Marine Reserve (formerly Cartier Island Marine Reserve), established in June 2000, comprises an area of approximately 172 km2, within a 4 nautical mile radius from the center of Cartier Island, and extends to a depth of 1 km below the sea floor. It includes the reef around Cartier island, a small submerged pinaccle called Wave Governor Bank, and two shallow pools to the island's northeast.
Economy and migration.
There is no economic activity in the Territory. As Ashmore Reef is the closest point of Australian territory to Indonesia, it was a popular target for people smugglers transporting asylum seekers to Australia despite its only wells being infected with cholera or contaminated and undrinkable. Once they had landed on Ashmore, asylum seekers could claim to have entered Australian territory and request to be processed as refugees. The use of Ashmore for this purpose created great notoriety during late 2001, when refugee arrivals became a major political issue in Australia. As Australia was not the country of first asylum for these "boat people", the Australian Government did not consider that it had a responsibility to accept them.
A number of things were done to discourage the practice such as attempting to have the people smugglers arrested in Indonesia; the so-called Pacific Solution of processing them in third countries; the boarding and forced turnaround of the boats by Australian military forces, and finally excising Ashmore and many other small islands from the Australian migration zone. Two boatloads of asylum seekers were each detained for several days in the lagoon at Ashmore after failed attempts by the Royal Australian Navy to turn them back to Indonesia in October 2001.

</doc>
<doc id="1242" url="http://en.wikipedia.org/wiki?curid=1242" title="Ada (programming language)">
Ada (programming language)

Ada is a structured, statically typed, imperative, wide-spectrum, and object-oriented high-level computer programming language, extended from Pascal and other languages. It has built-in language support for design-by-contract, extremely strong typing, explicit concurrency, offering tasks, synchronous message passing, protected objects, and non-determinism. Ada aims to improve the safety and maintainability by leveraging the compiler to find compile-time errors in favor of runtime errors. Ada is an international standard; the current version (known as Ada 2012) is defined by ISO/IEC 8652:2012.
Ada was originally designed by a team led by Jean Ichbiah of CII Honeywell Bull under contract to the United States Department of Defense (DoD) from 1977 to 1983 to supersede the hundreds of programming languages then used by the DoD. Ada was named after Ada Lovelace (1815–1852), who is credited as being the first computer programmer.
Features.
Ada was originally targeted at embedded and real-time systems. The Ada 95 revision, designed by S. Tucker Taft of Intermetrics between 1992 and 1995, improved support for systems, numerical, financial, and object-oriented programming (OOP).
Notable features of Ada include: strong typing, modularity mechanisms (packages), run-time checking, parallel processing (tasks, synchronous message passing, protected objects, and nondeterministic select statements), exception handling, and generics. Ada 95 added support for object-oriented programming, including dynamic dispatch.
The syntax of Ada minimizes choices of ways to perform basic operations, and prefers English keywords (such as "or else" and "and then") to symbols (such as "||" and "&&"). Ada uses the basic arithmetical operators "+", "-", "*", and "/", but avoids using other symbols. Code blocks are delimited by words such as "declare", "begin", and "end", whereas the "end" (in most cases) is followed by the identifier of the block it closes (e.g., "if … end if", "loop … end loop"). In the case of conditional blocks this avoids a "dangling else" that could pair with the wrong nested if-expression in other languages like C or Java.
Ada is designed for development of very large software systems. Ada packages can be compiled separately. Ada package specifications (the package interface) can also be compiled separately without the implementation to check for consistency. This makes it possible to detect problems early during the design phase, before implementation starts.
A large number of compile-time checks are supported to help avoid bugs that would not be detectable until run-time in some other languages or would require explicit checks to be added to the source code. For example, the syntax requires explicitly named closing of blocks to prevent errors due to mismatched end tokens. The adherence to strong typing allows detection of many common software errors (wrong parameters, range violations, invalid references, mismatched types, etc.) either during compile-time, or otherwise during run-time. As concurrency is part of the language specification, the compiler can in some cases detect potential deadlocks. Compilers also commonly check for misspelled identifiers, visibility of packages, redundant declarations, etc. and can provide warnings and useful suggestions on how to fix the error.
Ada also supports run-time checks to protect against access to unallocated memory, buffer overflow errors, range violations, off-by-one errors, array access errors, and other detectable bugs. These checks can be disabled in the interest of runtime efficiency, but can often be compiled efficiently. It also includes facilities to help program verification. For these reasons, Ada is widely used in critical systems, where any anomaly might lead to very serious consequences, e.g., accidental death, injury or severe financial loss. Examples of systems where Ada is used include avionics, ATC, railways, banking, military and space technology.
Ada's dynamic memory management is high-level and type-safe. Ada does not have generic or untyped pointers; nor does it implicitly declare any pointer type. Instead, all dynamic memory allocation and deallocation must take place through explicitly declared "access types".
Each access type has an associated "storage pool" that handles the low-level details of memory management; the programmer can either use the default storage pool or define new ones (this is particularly relevant for Non-Uniform Memory Access). It is even possible to declare several different access types that all designate the same type but use different storage pools.
Also, the language provides for "accessibility checks", both at compile time and at run time, that ensures that an "access value" cannot outlive the type of the object it points to.
Though the semantics of the language allow automatic garbage collection of inaccessible objects, most implementations do not support it by default, as it would cause unpredictable behaviour in real-time systems. Ada does support a limited form of region-based memory management; also, creative use of storage pools can provide for a limited form of automatic garbage collection, since destroying a storage pool also destroys all the objects in the pool.
Ada was designed to resemble the English language in its syntax for comments: a double-dash ("--"), resembling an em dash, denotes comment text. Comments stop at end of line, so there is no danger of unclosed comments accidentally voiding whole sections of source code. Prefixing each line (or column) with "--" will skip all that code, while being clearly denoted as a column of repeated "--" down the page. There is no limit to the nesting of comments, thereby allowing prior code, with commented-out sections, to be commented-out as even larger sections. All Unicode characters are allowed in comments, such as for symbolic formulas (E[0]=m×c²). To the compiler, the double-dash is treated as end-of-line, allowing continued parsing of the language as a context-free grammar.
The semicolon (";") is a statement terminator, and the null or no-operation statement is codice_1. A single codice_2 without a statement to terminate is not allowed.
Unlike most ISO standards, the Ada language definition (known as the "Ada Reference Manual" or "ARM", or sometimes the "Language Reference Manual" or "LRM") is free content. Thus, it is a common reference for Ada programmers and not just programmers implementing Ada compilers. Apart from the reference manual, there is also an extensive rationale document which explains the language design and the use of various language constructs. This document is also widely used by programmers. When the language was revised, a new rationale document was written.
One notable free software tool that is used by many Ada programmers to aid them in writing Ada source code is the GNAT Programming Studio.
History.
In the 1970s, the US Department of Defense (DoD) was concerned by the number of different programming languages being used for its embedded computer system projects, many of which were obsolete or hardware-dependent, and none of which supported safe modular programming. In 1975, a working group, the High Order Language Working Group (HOLWG), was formed with the intent to reduce this number by finding or creating a programming language generally suitable for the department's and UK Ministry of Defence requirements. After many iterations beginning with an original Straw man proposal the eventual programming language was named Ada. The total number of high-level programming languages in use for such projects fell from over 450 in 1983 to 37 by 1996.
 The HOLWG working group crafted the Steelman language requirements, a series of documents stating the requirements they felt a programming language should satisfy. Many existing languages were formally reviewed, but the team concluded in 1977 that no existing language met the specifications.
Requests for proposals for a new programming language were issued and four contractors were hired to develop their proposals under the names of Red (Intermetrics led by Benjamin Brosgol), Green (CII Honeywell Bull, led by Jean Ichbiah), Blue (SofTech, led by John Goodenough) and Yellow (SRI International, led by Jay Spitzen). In April 1978, after public scrutiny, the Red and Green proposals passed to the next phase. In May 1979, the Green proposal, designed by Jean Ichbiah at CII Honeywell Bull, was chosen and given the name Ada—after Augusta Ada, Countess of Lovelace. This proposal was influenced by the programming language LIS that Ichbiah and his group had developed in the 1970s. The preliminary Ada reference manual
was published in ACM SIGPLAN Notices in June 1979. The Military Standard reference manual was approved on December 10, 1980 (Ada Lovelace's birthday), and
given the number MIL-STD-1815 in honor of Ada Lovelace's birth year. In 1981, C. A. R. Hoare took advantage of his Turing Award speech to criticize Ada for being overly complex and hence unreliable, but subsequently seemed to recant in the foreword he wrote for an Ada textbook.
Ada attracted much attention from the programming community as a whole during its early days. Its backers and others predicted that it might become a dominant language for general purpose programming and not just defense-related work. Ichbiah publicly stated that within ten years, only two programming languages would remain, Ada and Lisp. Early Ada compilers struggled to implement the large, complex language, and both compile-time and run-time performance tended to be slow and tools primitive. Compiler vendors expended most of their efforts in passing the massive, language-conformance-testing, government-required "ACVC" validation suite that was required in another novel feature of the Ada language effort.
The first validated Ada implementation was the NYU Ada/Ed translator, certified on April 11, 1983. NYU Ada/Ed is implemented in the high-level set language SETL. A number of commercial companies began offering Ada compilers and associated development tools, including Alsys, Telesoft, DDC-I, Advanced Computer Techniques, Tartan Laboratories, TLD Systems, and others.
In 1987, the US Department of Defense began to require the use of Ada (the "Ada mandate") for every software project where new code was more than 30% of result, though exceptions to this rule were often granted.
By the late 1980s and early 1990s, Ada compilers had improved in performance, but there were still barriers to full exploitation of Ada's abilities, including a tasking model that was different from what most real-time programmers were used to.
The Department of Defense Ada mandate was effectively removed in 1997, as the DoD began to embrace COTS (commercial off-the-shelf) technology. Similar requirements existed in other NATO countries.
Because of Ada's safety-critical support features, it is now used not only for military applications, but also in commercial projects where a software bug can have severe consequences, e.g., avionics and air traffic control, commercial rockets (e.g., Ariane 4 and 5), satellites and other space systems, railway transport and banking.
For example, the fly-by-wire system software in the Boeing 777 was written in Ada. The Canadian Automated Air Traffic System was written in 1 million lines of Ada (SLOC count). It featured advanced distributed processing, a distributed Ada database, and object-oriented design. Ada is also used in other air traffic systems, e.g., the UK’s next-generation Interim Future Area Control Tools Support (iFACTS) air traffic control system is designed and implemented using SPARK Ada.
It is also used in the French TVM in-cab signalling system on the TGV high-speed rail system, and the metro suburban trains in Paris, London, Hong Kong and New York City.
Standardization.
The language became an ANSI standard in 1983 (), and without any further changes became
an ISO standard in 1987 (ISO-8652:1987). This version of the language is commonly known as Ada 83, from the date of its adoption by ANSI, but is sometimes referred to also as Ada 87, from the date of its adoption by ISO.
Ada 95, the joint ISO/ANSI standard () was published in February 1995, making Ada 95 the first ISO standard object-oriented programming language. To help with the standard revision and future acceptance, the US Air Force funded the development of the GNAT Compiler. Presently, the GNAT Compiler is part of the GNU Compiler Collection.
Work has continued on improving and updating the technical content of the Ada programming language. A Technical Corrigendum to Ada 95 was published in October 2001, and a major Amendment, was published on March 9, 2007. At the Ada-Europe 2012 conference in Stockholm, the Ada Resource Association (ARA) and Ada-Europe announced the completion of the design of the latest version of the Ada programming language and the submission of the reference manual to the International Organization for Standardization (ISO) for approval. ISO/IEC 8652:2012 was published in December 2012.
Other related standards include ISO 8651-3:1988 "Information processing systems—Computer graphics—Graphical Kernel System (GKS) language bindings—Part 3: Ada".
Language constructs.
Ada is an ALGOL-like programming language featuring control structures with reserved words such as "if", "then", "else", "while", "for", and so on. However, Ada also has many data structuring facilities and other abstractions which were not included in the original ALGOL 60, such as type definitions, records, pointers, enumerations. Such constructs were in part inherited or inspired from Pascal.
"Hello, world!" in Ada.
A common example of a language's syntax is the Hello world program:
with Ada.Text_IO; use Ada.Text_IO;
procedure Hello is
begin
 Put_Line ("Hello, world!");
end Hello;
This program can be compiled by using the freely available open source compiler GNAT, by executing
gnatmake hello.adb
Data types.
Ada's type system is not based on a set of predefined primitive types but allows users to declare their own types. This declaration in turn is not based on the internal representation of the type but on describing the goal which should be achieved. This allows the compiler to determine a suitable memory size for the type, and to check for violations of the type definition at compile time and run time (i.e., range violations, buffer overruns, type consistency, etc.). Ada supports numerical types defined by a range, modulo types, aggregate types (records and arrays), and enumeration types. Access types define a reference to an instance of a specified type; untyped pointers are not permitted.
Special types provided by the language are task types and protected types.
For example a date might be represented as:
type Day_type is range 1 .. 31;
type Month_type is range 1 .. 12;
type Year_type is range 1800 .. 2100;
type Hours is mod 24;
type Weekday is (Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday);
type Date is
 record
 Day : Day_type;
 Month : Month_type;
 Year : Year_type;
 end record;
Types can be refined by declaring subtypes:
subtype Working_Hours is Hours range 0 .. 12; -- at most 12 Hours to work a day
subtype Working_Day is Weekday range Monday .. Friday; -- Days to work
Work_Load: constant array(Working_Day) of Working_Hours -- implicit type declaration
 := (Friday => 6, Monday => 4, others => 10); -- lookup table for working hours with initialization
Types can have modifiers such as "limited, abstract, private" etc. Private types can only be accessed and limited types can only be modified or copied within the scope of the package that defines them.
Ada 95 adds additional features for object-oriented extension of types.
Control structures.
Ada is a structured programming language, meaning that the flow of control is structured into standard statements. All standard constructs and deep level early exit are supported so the use of the also supported 'go to' commands is seldom needed.
-- while a is not equal to b, loop.
while a /= b loop
 Ada.Text_IO.Put_Line ("Waiting");
end loop;
if a > b then
 Ada.Text_IO.Put_Line ("Condition met");
else
 Ada.Text_IO.Put_Line ("Condition not met");
end if;
for i in 1 .. 10 loop
 Ada.Text_IO.Put ("Iteration: ");
 Ada.Text_IO.Put (i);
 Ada.Text_IO.Put_Line;
end loop;
loop
 a := a + 1;
 exit when a = 10;
end loop;
case i is
 when 0 => Ada.Text_IO.Put ("zero");
 when 1 => Ada.Text_IO.Put ("one");
 when 2 => Ada.Text_IO.Put ("two");
 -- case statements have to cover all possible cases:
 when others => Ada.Text_IO.Put ("none of the above");
end case;
for aWeekday in Weekday'Range loop -- loop over an enumeration
 Put_Line ( Weekday'Image(aWeekday) ); -- output string representation of an enumeration
 if aWeekday in Working_Day then -- check of a subtype of an enumeration
 Put_Line ( " to work for " &
 Working_Hours'Image (Work_Load(aWeekday)) ); -- access into a lookup table
 end if;
end loop;
Packages, procedures and functions.
Among the parts of an Ada program are packages, procedures and functions.
Example:
Package specification (example.ads)
package Example is
 type Number is range 1 .. 11;
 procedure Print_and_Increment (j: in out Number);
end Example;
Package body (example.adb)
with Ada.Text_IO;
package body Example is
 i : Number := Number'First;
 procedure Print_and_Increment (j: in out Number) is
 function Next (k: in Number) return Number is
 begin
 return k + 1;
 end Next;
 begin
 Ada.Text_IO.Put_Line ( "The total is: " & Number'Image(j) );
 j := Next (j);
 end Print_and_Increment;
-- package initialization executed when the package is elaborated
begin
 while i < Number'Last loop
 Print_and_Increment (i);
 end loop;
end Example;
This program can be compiled, e.g., by using the freely available open source compiler GNAT, by executing
gnatmake -z example.adb
Packages, procedures and functions can nest to any depth and each can also be the logical outermost block.
Each package, procedure or function can have its own declarations of constants, types, variables, and other procedures, functions and packages, which can be declared in any order.
Concurrency.
Ada has language support for task-based concurrency. The fundamental concurrent unit in Ada is a "task" which is a built-in limited type. Tasks are specified in two parts – the task declaration defines the task interface (similar to a type declaration), the task body specifies the implementation of the task.
Depending on the implementation, Ada tasks are either mapped to operating system tasks or processes, or are scheduled internally by the Ada runtime.
Tasks can have entries for synchronisation (a form of synchronous message passing). Task entries are declared in the task specification. Each task entry can have one or more "accept" statements within the task body. If the control flow of the task reaches an accept statement, the task is blocked until the corresponding entry is called by another task (similarly, a calling task is blocked until the called task reaches the corresponding accept statement). Task entries can have parameters similar to procedures, allowing tasks to synchronously exchange data. In conjunction with "select" statements it is possible to define "guards" on accept statements (similar to Dijkstra's guarded commands).
Ada also offers "protected objects" for mutual exclusion. Protected objects are a monitor-like construct, but use guards instead of conditional variables for signaling (similar to conditional critical regions). Protected objects combine the data encapsulation and safe mutual exclusion from monitors, and entry guards from conditional critical regions. The main advantage over classical monitors is that conditional variables are not required for signaling, avoiding potential deadlocks due to incorrect locking semantics. Like tasks, the protected object is a built-in limited type, and it also has a declaration part and a body.
A protected object consists of encapsulated private data (which can only be accessed from within the protected object), and procedures, functions and entries which are guaranteed to be mutually exclusive (with the only exception of functions, which are required to be side effect free and can therefore run concurrently with other functions). A task calling a protected object is blocked if another task is currently executing inside the same protected object, and released when this other task leaves the protected object. Blocked tasks are queued on the protected object ordered by time of arrival.
Protected object entries are similar to procedures, but additionally have "guards". If a guard evaluates to false, a calling task is blocked and added to the queue of that entry; now another task can be admitted to the protected object, as no task is currently executing inside the protected object. Guards are re-evaluated whenever a task leaves the protected object, as this is the only time when the evaluation of guards can have changed.
Calls to entries can be "requeued" to other entries with the same signature. A task that is requeued is blocked and added to the queue of the target entry; this means that the protected object is released and allows admission of another task.
The "select" statement in Ada can be used to implement non-blocking entry calls and accepts, non-deterministic selection of entries (also with guards), time-outs and aborts.
The following example illustrates some concepts of concurrent programming in Ada.
with Ada.Text_IO; use Ada.Text_IO;
procedure Traffic is
 type Airplane_ID is range 1..10; -- 10 airplanes
 task type Airplane (ID: Airplane_ID); -- task representing airplanes, with ID as initialisation parameter
 type Airplane_Access is access Airplane; -- reference type to Airplane
 protected type Runway is -- the shared runway (protected to allow concurrent access)
 entry Assign_Aircraft (ID: Airplane_ID); -- all entries are guaranteed mutually exclusive
 entry Cleared_Runway (ID: Airplane_ID);
 entry Wait_For_Clear;
 private
 Clear: Boolean := True; -- protected private data - generally more than just a flag...
 end Runway;
 type Runway_Access is access all Runway;
 -- the air traffic controller task takes requests for takeoff and landing
 task type Controller (My_Runway: Runway_Access) is
 -- task entries for synchronous message passing
 entry Request_Takeoff (ID: in Airplane_ID; Takeoff: out Runway_Access);
 entry Request_Approach(ID: in Airplane_ID; Approach: out Runway_Access);
 end Controller;
 -- allocation of instances
 Runway1 : aliased Runway; -- instantiate a runway
 Controller1: Controller (Runway1'Access); -- and a controller to manage it
 ------ the implementations of the above types ------
 protected body Runway is
 entry Assign_Aircraft (ID: Airplane_ID)
 when Clear is -- the entry guard - calling tasks are blocked until the condition is true
 begin
 Clear := False;
 Put_Line (Airplane_ID'Image (ID) & " on runway ");
 end;
 entry Cleared_Runway (ID: Airplane_ID)
 when not Clear is
 begin
 Clear := True;
 Put_Line (Airplane_ID'Image (ID) & " cleared runway ");
 end;
 entry Wait_For_Clear
 when Clear is
 begin
 null; -- no need to do anything here - a task can only enter if "Clear" is true
 end;
 end Runway;
 task body Controller is
 begin
 loop
 My_Runway.Wait_For_Clear; -- wait until runway is available (blocking call)
 select -- wait for two types of requests (whichever is runnable first)
 when Request_Approach'count = 0 => -- guard statement - only accept if there are no tasks queuing on Request_Approach
 accept Request_Takeoff (ID: in Airplane_ID; Takeoff: out Runway_Access)
 do -- start of synchronized part
 My_Runway.Assign_Aircraft (ID); -- reserve runway (potentially blocking call if protected object busy or entry guard false)
 Takeoff := My_Runway; -- assign "out" parameter value to tell airplane which runway
 end Request_Takeoff; -- end of the synchronised part
 or
 accept Request_Approach (ID: in Airplane_ID; Approach: out Runway_Access) do
 My_Runway.Assign_Aircraft (ID);
 Approach := My_Runway;
 end Request_Approach;
 or -- terminate if no tasks left who could call
 terminate;
 end select;
 end loop;
 end;
 task body Airplane is
 Rwy : Runway_Access;
 begin
 Controller1.Request_Takeoff (ID, Rwy); -- This call blocks until Controller task accepts and completes the accept block
 Put_Line (Airplane_ID'Image (ID) & " taking off...");
 delay 2.0;
 Rwy.Cleared_Runway (ID); -- call will not block as "Clear" in Rwy is now false and no other tasks should be inside protected object
 delay 5.0; -- fly around a bit...
 loop
 select -- try to request a runway
 Controller1.Request_Approach (ID, Rwy); -- this is a blocking call - will run on controller reaching accept block and return on completion
 exit; -- if call returned we're clear for landing - leave select block and proceed...
 or
 delay 3.0; -- timeout - if no answer in 3 seconds, do something else (everything in following block)
 Put_Line (Airplane_ID'Image (ID) & " in holding pattern"); -- simply print a message
 end select;
 end loop;
 delay 4.0; -- do landing approach...
 Put_Line (Airplane_ID'Image (ID) & " touched down!");
 Rwy.Cleared_Runway (ID); -- notify runway that we're done here.
 end;
 New_Airplane: Airplane_Access;
begin
 for I in Airplane_ID'Range loop -- create a few airplane tasks
 New_Airplane := new Airplane (I); -- will start running directly after creation
 delay 4.0;
 end loop;
end Traffic;
Pragmas.
A pragma is a compiler directive that conveys information to the compiler to allow specific manipulation of compiled output. Certain pragmas are built into the language while other are implementation-specific.
Examples of common usage of compiler pragmas would be to disable certain features, such as run-time type checking or array subscript boundary checking, or to instruct the compiler to insert object code in lieu of a function call (as C/C++ does with inline functions).
References.
Books.
</dl>

</doc>
<doc id="1279" url="http://en.wikipedia.org/wiki?curid=1279" title="Transport in Antarctica">
Transport in Antarctica

Transport in Antarctica has transformed from explorers crossing the isolated remote area of Antarctica by foot to a more open area due to human technologies enabling more convenient and faster transport, predominantly by air and water, as well as land. 
Transportation technologies on a remote area like Antarctica need to be able to deal with extremely low temperatures and continuous winds to ensure the travelers' safety. Due to the fragility of the Antarctic environment, only a limited amount of transport movements can take place and sustainable transportation technologies have to be used to reduce the ecological footprint.
The infrastructure of land, water and air transport needs to be safe and sustainable. 
Currently thousands of tourists and hundreds of scientists a year rely on the Antarctic transportation system.
Land transport.
Mawson Station started using classic Volkswagen Beetles, the first production cars to be used in Antarctica. The first of these was named 'Antarctica 1'. However, the scarcity and poor quality of road infrastructure limits land transportation by conventional vehicles. Winds continuously blow snow on the roads. The South Pole Traverse (also called McMurdo – South Pole Highway) is a 900-mile (1450 km) road in Antarctica linking the United States McMurdo Station on the coast to the Amundsen–Scott South Pole Station.
In 2006 a team of six people took part in the Ice Challenger Expedition. Travelling in a specially designed six wheel drive vehicle, the team completed the journey from the Antarctic coast at Patriot Hills to the geographic South Pole in 69 hours. In doing so they easily beat the previous record of 24 days. They arrived at the South Pole on the 12th of December 2005.
The team members on that expedition were Andrew Regan, Jason De Carteret, Andrew Moon, Richard Griffiths, Gunnar Egilsson and Andrew Miles. The expedition successfully showed that wheeled transport on the continent is not only possible but also often more practical. The expedition also hoped to raise awareness about global warming and climate change.
A second expedition led by Andrew Regan and Andrew Moon departed in November 2010. The Moon-Regan Trans Antarctic Expedition this time traversed the entire continent twice, using two six-wheel-drive vehicles and a Concept Ice Vehicle designed by Lotus. This time the team used the expedition to raise awareness about the global environmental importance of the Antarctic region and to show that biofuel can be a viable and environmentally friendly option.
Water transport.
Antarctica's only harbour is at McMurdo Station. Most coastal stations have offshore anchorages, and supplies are transferred from ship to shore by small boats, barges, and helicopters. A few stations have a basic wharf facility. All ships at port are subject to inspection in accordance with Article 7, Antarctic Treaty. Offshore anchorage is sparse and intermittent, but poses no problem to sailboats designed for the ice, typically with lifting keels and long shorelines.
McMurdo Station (), Palmer Station (); government use only except by permit (see Permit Office under "Legal System"). A number of tour boats, ranging from large motorized vessels to small sailing yachts, visit the Antarctic Peninsula during the summer months (January–March). Most are based in Ushuaia, Argentina.
Air transport.
Transport in Antarctica takes place by air, using aeroplanes and helicopters.
Aeroplane runways and helicopter pads have to be kept snow free to ensure safe take off and landing conditions.
Antarctica has 20 airports, but there are no developed public-access airports or landing facilities. Thirty stations, operated by 16 national governments party to the Antarctic Treaty, have landing facilities for either helicopters and/or fixed-wing aircraft; commercial enterprises operate two additional air facilities.
Helicopter pads are available at 27 stations; runways at 15 locations are gravel, sea-ice, blue-ice, or compacted snow suitable for landing wheeled, fixed-wing aircraft; of these, 1 is greater than 3 km in length, 6 are between 2 km and 3 km in length, 3 are between 1 km and 2 km in length, 3 are less than 1 km in length, and 2 are of unknown length; snow surface skiways, limited to use by ski-equipped, fixed-wing aircraft, are available at another 15 locations; of these, 4 are greater than 3 km in length, 3 are between 2 km and 3 km in length, 2 are between 1 km and 2 km in length, 2 are less than 1 km in length, and data is unavailable for the remaining 4. 
Antarctic airports are subject to severe restrictions and limitations resulting from extreme seasonal and geographic conditions; they do not meet ICAO standards, and advance approval from the respective governmental or nongovernmental operating organization is required for landing (1999 est.) Flights to the continent in the permanent darkness of the winter are normally only undertaken in an emergency, with burning barrels of fuel to outline a runway. On September 11, 2008, a United States Air Force C-17 Globemaster III successfully completed the first landing in Antarctica using night-vision goggles at Pegasus Field.
In April 2001 an emergency evacuation of Dr. Ronald Shemenski was needed from Amundsen–Scott South Pole Station when he contracted pancreatitis. Three C-130 Hercules were called back before their final leg because of weather. Organizers then called on Kenn Borek Air based in Calgary. Two de Havilland Twin Otters were dispatched out of Calgary with one being back-up. Twin Otters are specifically designed for the Canadian north and Kenn Borek Air's motto is "Anywhere, Anytime, World-Wide". The mission was a success but not without difficulties and drawbacks. Ground crews needed to create a 2 km runway with tracked equipment not designed to operate in the low temperatures at that time of year, the aircraft controls had to be "jerry-rigged" when the flaps were frozen in position after landing, and instruments were not reliable because of the cold. When they saw a "faint pink line on the horizon" they knew they were going in the right direction. This was the first rescue from the South Pole during polar winter. Canada honoured the Otter crew for bravery.

</doc>
<doc id="1291" url="http://en.wikipedia.org/wiki?curid=1291" title="Antarctic Treaty System">
Antarctic Treaty System

The Antarctic Treaty and related agreements, collectively known as the Antarctic Treaty System (ATS), regulate international relations with respect to Antarctica, Earth's only continent without a native human population. For the purposes of the treaty system, Antarctica is defined as all of the land and ice shelves south of 60°S latitude. The treaty, entering into force in 1961 and as of 2015 having 52 parties, sets aside Antarctica as a scientific preserve, establishes freedom of scientific investigation and bans military activity on that continent. The treaty was the first arms control agreement established during the Cold War. The Antarctic Treaty Secretariat headquarters have been located in Buenos Aires, Argentina, since September 2004.
The main treaty was opened for signature on December 1, 1959, and officially entered into force on June 23, 1961. The original signatories were the 12 countries active in Antarctica during the International Geophysical Year (IGY) of 1957–58. The twelve countries that had significant interests in Antarctica at the time were: Argentina, Australia, Belgium, Chile, France, Japan, New Zealand, Norway, South Africa, the Soviet Union, the United Kingdom and the United States. These countries had established over 50 Antarctic stations for the IGY. The treaty was a diplomatic expression of the operational and scientific cooperation that had been achieved "on the ice".
Articles of the Antarctic Treaty.
The main objective of the ATS is to ensure in the interests of all humankind that Antarctica shall continue forever to be used exclusively for peaceful purposes and shall not become the scene or object of international discord. Pursuant to Article 1, the treaty forbids any measures of a military nature, but not the presence of military personnel or equipment for the purposes of scientific research.
Other agreements.
Other agreements — some 200 recommendations adopted at treaty consultative meetings and ratified by governments — include:
Meetings.
The Antarctic Treaty System's yearly "Antarctic Treaty Consultative Meetings (ATCM)" are the international forum for the administration and management of the region. Only 29 of the 52 parties to the agreements have the right to participate in decision-making at these meetings, though the other 21 are still allowed to attend. The decision-making participants are the "Consultative Parties" and, in addition to the 12 original signatories, include 17 countries that have demonstrated their interest in Antarctica by carrying out substantial scientific activity there.
Parties.
As of 2015, there are 52 states party to the treaty, 29 of which, including all 12 original signatories to the treaty, have consultative (voting) status. Consultative members include the seven nations that claim portions of Antarctica as national territory. The 45 non-claimant nations either do not recognize the claims of others, or have not stated their positions.
Note: The table can be sorted alphabetically or chronologically using the icon.
Antarctic Treaty Secretariat.
The "Antarctic Treaty Secretariat" was established in Buenos Aires, Argentina in September 2004 by the Antarctic Treaty Consultative Meeting (ATCM). Jan Huber (Netherlands) served as the first Executive Secretary for five years until August 31, 2009. He was succeeded on September 1, 2009 by Manfred Reinke (Germany).
The tasks of the Antarctic Treaty Secretariat can be divided into the following areas:
Legal system.
Antarctica has no permanent population and therefore it has no citizenship nor government. All personnel present on Antarctica at any time are citizens or nationals of some sovereignty outside Antarctica, as there is no Antarctic sovereignty. The majority of Antarctica is claimed by one or more countries, but most countries do not explicitly recognize those claims. The area on the mainland between 90 degrees west and 150 degrees west, combined with the interior of the Norwegian Sector (the extent of which has never been officially defined), is the only major land on Earth not claimed by any country. The Treaty prohibits any nation from claiming this area.
Governments that are party to the Antarctic Treaty and its Protocol on Environmental Protection implement the articles of these agreements, and decisions taken under them, through national laws. These laws generally apply only to their own citizens, wherever they are in Antarctica, and serve to enforce the consensus decisions of the consultative parties: about which activities are acceptable, which areas require permits to enter, what processes of environmental impact assessment must precede activities, and so on. The Antarctic Treaty is often considered to represent an example of the Common heritage of mankind principle.
Argentina.
According to Argentine regulations, any crime committed within 50 kilometers of any Argentine base is to be judged in Ushuaia (as capital of Tierra del Fuego, Antarctica, and South Atlantic Islands). In the part of Argentine Antarctica that is also claimed by Chile and UK, the person to be judged can ask to be transferred there.
Australia.
Since the designation of the Australian Antarctic Territory pre-dated the signing of the Antarctic Treaty, Australian laws that relate to Antarctica date from more than two decades before the Antarctic Treaty era. In terms of criminal law, the laws that apply to the Jervis Bay Territory (which follows the laws of the Australian Capital Territory) apply to the Australian Antarctic Territory. Key Australian legislation applying Antarctic Treaty System decisions include the "Antarctic Treaty Act 1960", the "Antarctic Treaty (Environment Protection) Act 1980" and the "Antarctic Marine Living Resources Conservation Act 1981".
United States.
The law of the United States, including certain criminal offenses by or against U.S. nationals, such as murder, may apply to areas not under jurisdiction of other countries. To this end, the United States now stations special deputy U.S. Marshals in Antarctica to provide a law enforcement presence.
Some U.S. laws directly apply to Antarctica. For example, the Antarctic Conservation Act, Public Law 95-541,   "et seq.", provides civil and criminal penalties for the following activities, unless authorized by regulation or statute:
Violation of the Antarctic Conservation Act carries penalties of up to US$10,000 in fines and one year in prison. The Departments of the Treasury, Commerce, Transportation, and the Interior share enforcement responsibilities. The Act requires expeditions from the U.S. to Antarctica to notify, in advance, the Office of Oceans and Polar Affairs of the State Department, which reports such plans to other nations as required by the Antarctic Treaty. Further information is provided by the Office of Polar Programs of the National Science Foundation.
New Zealand.
In 2006, the New Zealand police reported that jurisdictional issues prevented them issuing warrants for potential American witnesses who were reluctant to testify during the Christchurch Coroner's investigation into the death by poisoning of Australian astrophysicist Rodney Marks at the South Pole base in May 2000. Dr. Marks died while wintering over at the United States' Amundsen–Scott South Pole Station located at the geographic South Pole. Prior to autopsy, the death was attributed to natural causes by the National Science Foundation and the contractor administering the base. However, an autopsy in New Zealand revealed that Dr. Marks died from methanol poisoning. The New Zealand Police launched an investigation. In 2006, frustrated by lack of progress, the Christchurch Coroner said that it was unlikely that Dr. Marks ingested the methanol knowingly, although there is no certainty that he died as the direct result of the act of another person. During media interviews, the police detective in charge of the investigation criticized the National Science Foundation and contractor Raytheon for failing to co-operate with the investigation.
South Africa.
South African law applies to all South African citizens in Antarctica, and they are subject to the jurisdiction of the magistrate's court in Cape Town. In regard to violations of the Antarctic Treaty and related agreements, South Africa also asserts jurisdiction over South African residents and members of expeditions organised in South Africa.

</doc>
<doc id="1395" url="http://en.wikipedia.org/wiki?curid=1395" title="Amazing Grace">
Amazing Grace

"Amazing Grace" is a Christian hymn published in 1779, with words written by the English poet and clergyman John Newton (1725–1807).
Newton wrote the words from personal experience. He grew up without any particular religious conviction, but his life's path was formed by a variety of twists and coincidences that were often put into motion by his recalcitrant insubordination. He was pressed (involuntarily forced) into service in the Royal Navy, and after leaving the service, he became involved in the Atlantic slave trade. In 1748, a violent storm battered his vessel off the coast of County Donegal, Ireland so severely that he called out to God for mercy, a moment that marked his spiritual conversion. Whilst his boat was being repaired in Lough Swilly, he wrote the first verse of his world famous song. He did however, continue his slave trading career until 1754 or 1755, when he ended his seafaring altogether and began studying Christian theology.
Ordained in the Church of England in 1764, Newton became curate of Olney, Buckinghamshire, where he began to write hymns with poet William Cowper. "Amazing Grace" was written to illustrate a sermon on New Year's Day of 1773. It is unknown if there was any music accompanying the verses; it may have simply been chanted by the congregation. It debuted in print in 1779 in Newton and Cowper's "Olney Hymns" but settled into relative obscurity in England. In the United States however, "Amazing Grace" was used extensively during the Second Great Awakening in the early 19th century. It has been associated with more than 20 melodies, but in 1835 it was joined to a tune named "New Britain" to which it is most frequently sung today.
With the message that forgiveness and redemption are possible regardless of sins committed and that the soul can be delivered from despair through the mercy of God, "Amazing Grace" is one of the most recognizable songs in the English-speaking world. Author Gilbert Chase writes that it is "without a doubt the most famous of all the folk hymns," and Jonathan Aitken, a Newton biographer, estimates that it is performed about 10 million times annually. It has had particular influence in folk music, and has become an emblematic African American spiritual. Its universal message has been a significant factor in its crossover into secular music. "Amazing Grace" saw a resurgence in popularity in the U.S. during the 1960s and has been recorded thousands of times during and since the 20th century, occasionally appearing on popular music charts.
John Newton's conversion.
How industrious is Satan served. I was formerly one of his active undertemptors and had my influence been equal to my wishes I would have carried all the human race with me. A common drunkard or profligate is a petty sinner to what I was.
 John Newton, 1778
According to the "Dictionary of American Hymnology" "Amazing Grace" is John Newton's spiritual autobiography in verse.
In 1725, Newton was born in Wapping, a district in London near the Thames. His father was a shipping merchant who was brought up as a Catholic but had Protestant sympathies, and his mother was a devout Independent unaffiliated with the Anglican Church. She had intended Newton to become a clergyman, but she died of tuberculosis when he was six years old. For the next few years, Newton was raised by his emotionally distant stepmother while his father was at sea, and spent some time at a boarding school where he was mistreated. At the age of eleven, he joined his father on a ship as an apprentice; his seagoing career would be marked by headstrong disobedience.
As a youth, Newton began a pattern of coming very close to death, examining his relationship with God, then relapsing into bad habits. As a sailor, he denounced his faith after being influenced by a shipmate who discussed "Characteristics of Men, Manners, Opinions, Times", a book by the Third Earl of Shaftesbury, with him. In a series of letters he later wrote, "Like an unwary sailor who quits his port just before a rising storm, I renounced the hopes and comforts of the Gospel at the very time when every other comfort was about to fail me." His disobedience caused him to be pressed into the Royal Navy, and he took advantage of opportunities to overstay his leave and finally deserted to visit Mary "Polly" Catlett, a family friend with whom he had fallen in love. After enduring humiliation for deserting, he managed to get himself traded to a slave ship where he began a career in slave trading.
Newton often openly mocked the captain by creating obscene poems and songs about him that became so popular the crew began to join in. He entered into disagreements with several colleagues that resulted in his being starved almost to death, imprisoned while at sea and chained like the slaves they carried, then outright enslaved and forced to work on a plantation in Sierra Leone near the Sherbro River. After several months he came to think of Sierra Leone as his home, but his father intervened after Newton sent him a letter describing his circumstances, and a ship found him by coincidence. Newton claimed the only reason he left was because of Polly.
While aboard the ship "Greyhound", Newton gained notoriety for being one of the most profane men the captain had ever met. In a culture where sailors commonly used oaths and swore, Newton was admonished several times for not only using the worst words the captain had ever heard, but creating new ones to exceed the limits of verbal debauchery. In March 1748, while the "Greyhound" was in the North Atlantic, a violent storm came upon the ship that was so rough it swept overboard a crew member who was standing where Newton had been moments before. After hours of the crew emptying water from the ship and expecting to be capsized, Newton and another mate tied themselves to the ship's pump to keep from being washed overboard, working for several hours. After proposing the measure to the captain, Newton had turned and said, "If this will not do, then Lord have mercy upon us!" Newton rested briefly before returning to the deck to steer for the next eleven hours. During his time at the wheel he pondered his divine challenge.
About two weeks later, the battered ship and starving crew landed in Lough Swilly, Ireland. For several weeks before the storm, Newton had been reading "The Christian's Pattern", a summary of the 15th-century "The Imitation of Christ" by Thomas à Kempis. The memory of the uttered phrase in a moment of desperation did not leave him; he began to ask if he was worthy of God's mercy or in any way redeemable as he had not only neglected his faith but directly opposed it, mocking others who showed theirs, deriding and denouncing God as a myth. He came to believe that God had sent him a profound message and had begun to work through him.
Newton's conversion was not immediate, but he contacted Polly's family and announced his intentions to marry her. Her parents were hesitant as he was known to be unreliable and impetuous. They knew he was profane, but they allowed him to write to Polly, and he set to begin to submit to authority for her sake. He sought a place on a slave ship bound for Africa, and Newton and his crewmates participated in most of the same activities he had written about before; the only immorality from which he was able to free himself was profanity. After a severe illness his resolve was renewed, yet he retained the same attitude towards slavery as was held by his contemporaries. Newton continued in the slave trade through several voyages where he sailed up rivers in Africa – now as a captain – procured slaves being offered for sale in larger ports, and subsequently transported them to North America. In between voyages, he married Polly in 1750 and he found it more difficult to leave her at the beginning of each trip. After three shipping experiences in the slave trade, Newton was promised a position as ship's captain with cargo unrelated to slavery when, at the age of thirty, he collapsed and never sailed again.
Olney curate.
Working as a customs agent in Liverpool starting in 1756, Newton began to teach himself Latin, Greek, and theology. He and Polly immersed themselves in the church community, and Newton's passion was so impressive that his friends suggested he become a priest in the Church of England. He was turned down by the Bishop of York in 1758, ostensibly for having no university degree, although the more likely reasons were his leanings toward evangelism and tendency to socialize with Methodists. Newton continued his devotions, and after being encouraged by a friend, he wrote about his experiences in the slave trade and his conversion. The Earl of Dartmouth, impressed with his story, sponsored Newton for ordination with the Bishop of Lincoln, and offered him the curacy of Olney, Buckinghamshire, in 1764.
"Olney Hymns".
<poem>
Amazing grace! (how sweet the sound)
 That sav'd a wretch like me!
I once was lost, but now am found,
 Was blind, but now I see.
'Twas grace that taught my heart to fear,
 And grace my fears reliev'd;
How precious did that grace appear
 The hour I first believ'd!
Thro' many dangers, toils, and snares,
 I have already come;
'Tis grace hath brought me safe thus far,
 And grace will lead me home.
The Lord has promis'd good to me,
 His word my hope secures;
He will my shield and portion be
 As long as life endures.
Yes, when this flesh and heart shall fail,
 And mortal life shall cease;
I shall possess, within the veil,
 A life of joy and peace.
The earth shall soon dissolve like snow,
 The sun forbear to shine;
But God, who call'd me here below,
 Will be forever mine.
</poem>
 John Newton, "Olney Hymns", 1779
Olney was a village of about 2,500 residents whose main industry was making lace by hand. The people were mostly illiterate and many of them were poor. Newton's preaching was unique in that he shared many of his own experiences from the pulpit; many clergy preached from a distance, not admitting any intimacy with temptation or sin. He was involved in his parishioners' lives and was much loved, although his writing and delivery were sometimes unpolished. But his devotion and conviction were apparent and forceful, and he often said his mission was to "break a hard heart and to heal a broken heart". He struck a friendship with William Cowper, a gifted writer who had failed at a career in law and suffered bouts of insanity, attempting suicide several times. Cowper enjoyed Olney – and Newton's company; he was also new to Olney and had gone through a spiritual conversion similar to Newton's. Together, their effect on the local congregation was impressive. In 1768, they found it necessary to start a weekly prayer meeting to meet the needs of an increasing number of parishioners. They also began writing lessons for children.
Partly from Cowper's literary influence, and partly because learned vicars were expected to write verses, Newton began to try his hand at hymns, which had become popular through the language, made plain for common people to understand. Several prolific hymn writers were at their most productive in the 18th century, including Isaac Watts – whose hymns Newton had grown up hearing – and Charles Wesley, with whom Newton was familiar. Wesley's brother John, the eventual founder of the Methodist Church, had encouraged Newton to go into the clergy. Watts was a pioneer in English hymn writing, basing his work the Psalms. The most prevalent hymns by Watts and others were written in the common meter in 8.6.8.6: the first line is eight syllables and the second is six.
Newton and Cowper attempted to present a poem or hymn for each prayer meeting. The lyrics to "Amazing Grace" were written in late 1772 and probably used in a prayer meeting for the first time on January 1, 1773. A collection of the poems Newton and Cowper had written for use in services at Olney was bound and published anonymously in 1779 under the title "Olney Hymns". Newton contributed 280 of the 348 texts in "Olney Hymns"; "1 Chronicles 17:16–17, Faith's Review and Expectation" was the title of the poem with the first line "Amazing grace! (how sweet the sound)".
Critical analysis.
The general impact of "Olney Hymns" was immediate and it became a widely popular tool for evangelicals in Britain for many years. Scholars appreciated Cowper's poetry somewhat more than Newton's plaintive and plain language driven from his forceful personality. The most prevalent themes in the verses written by Newton in "Olney Hymns" are faith in salvation, wonder at God's grace, his love for Jesus, and his cheerful exclamations of the joy he found in his faith. As a reflection of Newton's connection to his parishioners, he wrote many of the hymns in first person, admitting his own experience with sin. Bruce Hindmarsh in "Sing Them Over Again To Me: Hymns and Hymnbooks in America" considers "Amazing Grace" an excellent example of Newton's testimonial style afforded by the use of this perspective. Several of Newton's hymns were recognized as great work ("Amazing Grace" was not among them) while others seem to have been included to fill in when Cowper was unable to write. Jonathan Aitken calls Newton, specifically referring to "Amazing Grace", an "unashamedly middlebrow lyricist writing for a lowbrow congregation", noting that only twenty-one of the nearly 150 words used in all six verses have more than one syllable.
William Phipps in the "Anglican Theological Review" and author James Basker have interpreted the first stanza of "Amazing Grace" as evidence of Newton's realization that his participation in the slave trade was his wretchedness, perhaps representing a wider common understanding of Newton's motivations. Newton joined forces with a young man named William Wilberforce, the British Member of Parliament who led the Parliamentarian campaign to abolish the slave trade in the British Empire, culminating in the Slave Trade Act 1807. However, Newton became an ardent and outspoken abolitionist after he left Olney in the 1780s; he never connected the construction of the hymn that became "Amazing Grace" to anti-slavery sentiments. The lyrics in "Olney Hymns" were arranged by their association to the Biblical verses that would be used by Newton and Cowper in their prayer meetings and did not address any political objective. For Newton, the beginning of the year was a time to reflect on one's spiritual progress. At the same time he completed a diary – which has since been lost – that he had begun 17 years before, two years after he quit sailing. The last entry of 1772 was a recounting of how much he had changed since then.
And David the king came and sat before the LORD, and said, Who "am" I, O LORD God, and what "is" mine house, that thou hast brought me hitherto? And "yet" this was a small thing in thine eyes, O God; for thou hast "also" spoken of thy servant's house for a great while to come, and hast regarded me according to the estate of a man of high degree, O LORD God.
 1 Chronicles 17:16–17, King James Version
The title ascribed to the hymn, "1 Chronicles 17:16–17", refers to David's reaction to the prophet Nathan telling him that God intends to maintain his family line forever. Some Christians interpret this as a prediction that Jesus Christ, as a descendant of David, was promised by God as the salvation for all people. Newton's sermon on that January day in 1773 focused on the necessity to express one's gratefulness for God's guidance, that God is involved in the daily lives of Christians though they may not be aware of it, and that patience for deliverance from the daily trials of life is warranted when the glories of eternity await. Newton saw himself a sinner like David who had been chosen, perhaps undeservedly, and was humbled by it. According to Newton, unconverted sinners were "blinded by the god of this world" until "mercy came to us not only undeserved but undesired ... our hearts endeavored to shut him out till he overcame us by the power of his grace."
The New Testament served as the basis for many of the lyrics of "Amazing Grace". The first verse, for example, can be traced to the story of the Prodigal Son. In the Gospel of Luke the father says, "For this son of mine was dead and is alive again; he was lost, and is found". The story of Jesus healing a blind man who tells the Pharisees that he can now see is told in the Gospel of John. Newton used the words "I was blind but now I see" and declared "Oh to grace how great a debtor!" in his letters and diary entries as early as 1752. The effect of the lyrical arrangement, according to Bruce Hindmarsh, allows an instant release of energy in the exclamation "Amazing grace!", to be followed by a qualifying reply in "how sweet the sound". In "An Annotated Anthology of Hymns", Newton's use of an exclamation at the beginning of his verse is called "crude but effective" in an overall composition that "suggest(s) a forceful, if simple, statement of faith". Grace is recalled three times in the following verse, culminating in Newton's most personal story of his conversion, underscoring the use of his personal testimony with his parishioners.
The sermon preached by Newton was his last, of those that William Cowper heard in Olney, since Cowper's mental instability returned shortly thereafter. Steve Turner, author of "Amazing Grace: The Story of America's Most Beloved Song", suggests Newton may have had his friend in mind, employing the themes of assurance and deliverance from despair for Cowper's benefit.
Dissemination.
Although it had its roots in England, "Amazing Grace" became an integral part of the Christian tapestry in the United States. More than 60 of Newton and Cowper's hymns were republished in other British hymnals and magazines, but "Amazing Grace" was not, appearing only once in a 1780 hymnal sponsored by the Countess of Huntingdon. Scholar John Julian commented in his 1892 "A Dictionary of Hymnology" that outside of the United States, the song was unknown and it was "far from being a good example of Newton's finest work". Between 1789 and 1799, four variations of Newton's hymn were published in the U.S. in Baptist, Dutch Reformed, and Congregationalist hymnodies; by 1830 Presbyterians and Methodists also included Newton's verses in their hymnals.
The greatest influences in the 19th century that propelled "Amazing Grace" to spread across the U.S. and become a staple of religious services in many denominations and regions were the Second Great Awakening and the development of shape note singing communities. A tremendous religious movement swept the U.S. in the early 19th century, marked by the growth and popularity of churches and religious revivals that got their start in Kentucky and Tennessee. Unprecedented gatherings of thousands of people attended camp meetings where they came to experience salvation; preaching was fiery and focused on saving the sinner from temptation and backsliding. Religion was stripped of ornament and ceremony, and made as plain and simple as possible; sermons and songs often used repetition to get across to a rural population of poor and mostly uneducated people the necessity of turning away from sin. Witnessing and testifying became an integral component to these meetings, where a congregation member or even a stranger would rise and recount his turn from a sinful life to one of piety and peace. "Amazing Grace" was one of many hymns that punctuated fervent sermons, although the contemporary style used a refrain, borrowed from other hymns, that employed simplicity and repetition such as:
<poem>
Amazing grace! How sweet the sound
That saved a wretch like me.
I once was lost, but now am found,
Was blind but now I see.
Shout, shout for glory,
Shout, shout aloud for glory;
Brother, sister, mourner,
All shout glory hallelujah.
</poem>
Simultaneously, an unrelated movement of communal singing was established throughout the South and Western states. A format of teaching music to illiterate people appeared in 1800. It used four sounds to symbolize the basic scale: fa-sol-la-fa-sol-la-mi-fa. Each sound was accompanied by a specifically shaped note and thus became known as shape note singing. The method was simple to learn and teach, so schools were established throughout the South and West. Communities would come together for an entire day of singing in a large building where they sat in four distinct areas surrounding an open space, one member directing the group as a whole. Most of the music was Christian, but the purpose of communal singing was not primarily spiritual. Communities either could not afford music accompaniment or rejected it out of a Calvinistic sense of simplicity, so the songs were sung a cappella.
"New Britain" tune.
When originally used in Olney, it is unknown what music, if any, accompanied the verses written by John Newton. Contemporary hymnbooks did not contain music and were simply small books of religious poetry. The first known instance of Newton's lines joined to music was in "A Companion to the Countess of Huntingdon's Hymns" (London, 1808), where it is set to the tune "Hephzibah" by English composer John Jenkins Husband. Common meter hymns were interchangeable with a variety of tunes; more than twenty musical settings of "Amazing Grace" circulated with varying popularity until 1835 when William Walker assigned Newton's words to a traditional song named "New Britain", which was itself an amalgamation of two melodies ("Gallaher" and "St. Mary") first published in the "Columbian Harmony" by Charles H. Spilman and Benjamin Shaw (Cincinnati, 1829). Spilman and Shaw, both students at Kentucky's Centre College, compiled their tunebook both for public worship and revivals, to satisfy "the wants of the Church in her triumphal march." Most of the tunes had been previously published, but "Gallaher" and "St. Mary" had not. As neither tune is attributed and both show elements of oral transmission, scholars can only speculate that they are possibly of British origin. A manuscript from 1828 by Lucius Chapin, a famous hymn writer of that time, contains a tune very close to "St. Mary", but that does not mean that he wrote it.
"Amazing Grace", with the words written by Newton and joined with "New Britain", the melody most currently associated with it, appeared for the first time in Walker's shape note tunebook "Southern Harmony" in 1847. It was, according to author Steve Turner, a "marriage made in heaven ... The music behind 'amazing' had a sense of awe to it. The music behind 'grace' sounded graceful. There was a rise at the point of confession, as though the author was stepping out into the open and making a bold declaration, but a corresponding fall when admitting his blindness." Walker's collection was enormously popular, selling about 600,000 copies all over the U.S. when the total population was just over 20 million. Another shape note tunebook named "The Sacred Harp" (1844) by Georgia residents Benjamin Franklin White and Elisha J. King became widely influential and continues to be used.
Another verse was first recorded in Harriet Beecher Stowe's immensely influential 1852 anti-slavery novel "Uncle Tom's Cabin". Three verses were emblematically sung by Tom in his hour of deepest crisis. He sings the sixth and fifth verses in that order, and Stowe included another verse not written by Newton that had been passed down orally in African American communities for at least 50 years. It was originally one of between 50 to 70 verses of a song titled "Jerusalem, My Happy Home" that first appeared in a 1790 book called "A Collection of Sacred Ballads":
<poem>
When we've been there ten thousand years,
Bright shining as the sun,
We've no less days to sing God's praise,
Than when we first begun.
</poem>
"Amazing Grace" came to be an emblem of a Christian movement and a symbol of the U.S. itself as the country was involved in a great political experiment, attempting to employ democracy as a means of government. Shape note singing communities, with all the members sitting around an open center, each song employing a different director, illustrated this in practice. Simultaneously, the U.S. began to expand westward into previously unexplored territory that was often wilderness. The "dangers, toils, and snares" of Newton's lyrics had both literal and figurative meanings for Americans. This became poignantly true during the most serious test of American cohesion in the U.S. Civil War (1861–1865). "Amazing Grace" set to "New Britain" was included in two hymnals distributed to soldiers and with death so real and imminent, religious services in the military became commonplace. The hymn was translated into other languages as well: while on the Trail of Tears, the Cherokee sang Christian hymns as a way of coping with the ongoing tragedy, and a version of the song by Samuel Worcester that had been translated into the Cherokee language became very popular.
Urban revival.
Although "Amazing Grace" set to "New Britain" was popular, other versions existed regionally. Primitive Baptists in the Appalachian region often used "New Britain" with other hymns, and sometimes sing the words of "Amazing Grace" to other folk songs, including titles such as "In the Pines", "Pisgah", "Primrose", and "Evan", as all are able to be sung in common meter, of which the majority of their repertoire consists. A tune named "Arlington" accompanied Newton's verses as much as "New Britain" for a time in the late 19th century.
Two musical arrangers named Dwight Moody and Ira Sankey heralded another religious revival in the cities of the U.S. and Europe, giving the song international exposure. Moody's preaching and Sankey's musical gifts were significant; their arrangements were the forerunners of gospel music, and churches all over the U.S. were eager to acquire them. Moody and Sankey began publishing their compositions in 1875, and "Amazing Grace" appeared three times with three different melodies, but they were the first to give it its title; hymns were typically published using the first line of the lyrics, or the name of the tune such as "New Britain". A publisher named Edwin Othello Excell gave the version of "Amazing Grace" set to "New Britain" immense popularity by publishing it in a series of hymnals that were used in urban churches. Excell altered some of Walker's music, making it more contemporary and European, giving "New Britain" some distance from its rural folk-music origins. Excell's version was more palatable for a growing urban middle class and arranged for larger church choirs. Several editions featuring Newton's first three stanzas and the verse previously included by Harriet Beecher Stowe in "Uncle Tom's Cabin" were published by Excell between 1900 and 1910, and his version of "Amazing Grace" became the standard form of the song in American churches.
Recorded versions.
With the advent of recorded music and radio, "Amazing Grace" began to cross over from primarily a gospel standard to secular audiences. The ability to record combined with the marketing of records to specific audiences allowed "Amazing Grace" to take on thousands of different forms in the 20th century. Where Edwin Othello Excell sought to make the singing of "Amazing Grace" uniform throughout thousands of churches, records allowed artists to improvise with the words and music specific to each audience. AllMusic lists more than 7,000 recordings – including re-releases and compilations – as of September 2011. Its first recording is an a cappella version from 1922 by the Sacred Harp Choir. It was included from 1926 to 1930 in Okeh Records' catalogue, which typically concentrated strongly on blues and jazz. Demand was high for black gospel recordings of the song by H. R. Tomlin and J. M. Gates. A poignant sense of nostalgia accompanied the recordings of several gospel and blues singers in the 1940s and 1950s who used the song to remember their grandparents, traditions, and family roots. It was recorded with musical accompaniment for the first time in 1930 by Fiddlin' John Carson, although to another folk hymn named "At the Cross", not to "New Britain". "Amazing Grace" is emblematic of several kinds of folk music styles, often used as the standard example to illustrate such musical techniques as lining out and call and response, that have been practiced in both black and white folk music.
Those songs come out of conviction and suffering. The worst voices can get through singing them 'cause they're telling their experiences.
 Mahalia Jackson
Mahalia Jackson's 1947 version received significant radio airplay, and as her popularity grew throughout the 1950s and 1960s, she often sang it at public events such as concerts at Carnegie Hall. Author James Basker states that the song has been employed by African Americans as the "paradigmatic Negro spiritual" because it expresses the joy felt at being delivered from slavery and worldly miseries. Anthony Heilbut, author of "The Gospel Sound", states that the "dangers, toils, and snares" of Newton's words are a "universal testimony" of the African American experience. In the 1960s with the African American Civil Rights Movement and opposition to the Vietnam War, the song took on a political tone. Mahalia Jackson employed "Amazing Grace" for Civil Rights marchers, writing that she used it "to give magical protection – a charm to ward off danger, an incantation to the angels of heaven to descend ... I was not sure the magic worked outside the church walls ... in the open air of Mississippi. But I wasn't taking any chances." Folk singer Judy Collins, who knew the song before she could remember learning it, witnessed Fannie Lou Hamer leading marchers in Mississippi in 1964, singing "Amazing Grace". Collins also considered it a talisman of sorts, and saw its equal emotional impact on the marchers, witnesses, and law enforcement who opposed the civil rights demonstrators. According to fellow folk singer Joan Baez, it was one of the most requested songs from her audiences, but she never realized its origin as a hymn; by the time she was singing it in the 1960s she said it had "developed a life of its own". It even made an appearance at the Woodstock Music Festival in 1969 during Arlo Guthrie's performance.
Collins decided to record it in the late 1960s amid an atmosphere of counterculture introspection; she was part of an encounter group that ended a contentious meeting by singing "Amazing Grace" as it was the only song to which all the members knew the words. Her producer was present and suggested she include a version of it on her 1970 album "Whales & Nightingales". Collins, who had a history of alcohol abuse, claimed that the song was able to "pull her through" to recovery. It was recorded in St. Paul's, the chapel at Columbia University, chosen for the acoustics. She chose an "a cappella" arrangement that was close to Edwin Othello Excell's, accompanied by a chorus of amateur singers who were friends of hers. Collins connected it to the Vietnam War, to which she objected: "I didn't know what else to do about the war in Vietnam. I had marched, I had voted, I had gone to jail on political actions and worked for the candidates I believed in. The war was still raging. There was nothing left to do, I thought ... but sing 'Amazing Grace'." Gradually and unexpectedly, the song began to be played on the radio, and then be requested. It rose to number 15 on the "Billboard" Hot 100, remaining on the charts for 15 weeks, as if, she wrote, her fans had been "waiting to embrace it". In the UK, it charted 8 times between 1970 and 1972, peaking at number 5 and spending a total of 75 weeks on popular music charts.
Although Collins used it as a catharsis for her opposition to the Vietnam War, two years after her rendition, the Royal Scots Dragoon Guards, senior Scottish regiment of the British Army, recorded an instrumental version featuring a bagpipe soloist accompanied by a pipe and drum band. The tempo of their arrangement was slowed to allow for the bagpipes, but it was based on Collins': it began with a bagpipe solo introduction similar to her lone voice, then it was accompanied by the band of bagpipes and horns, whereas in her version she is backed up by a chorus. It hit number 1 in the UK singles chart in April 1972, spending 24 weeks total on the charts, topped the "RPM" national singles chart in Canada for three weeks, and rose as high as number 11 in the U.S. It is also a controversial instrumental, as it combined pipes with a military band. The Pipe Major of the Royal Scots Dragoon Guards was summoned to Edinburgh Castle and chastised for demeaning the bagpipes. Funeral processions for killed police, fire, and military personnel have often played a bagpipes version ever since.
Aretha Franklin and Rod Stewart also recorded "Amazing Grace" around the same time, and both of their renditions were popular. All four versions were marketed to distinct types of audiences thereby assuring its place as a pop song. Johnny Cash recorded it on his 1975 album "Sings Precious Memories", dedicating it to his older brother Jack, who had been killed in a mill accident when they were boys in Dyess, Arkansas. Cash and his family sang it to themselves while they worked in the cotton fields following Jack's death. Cash often included the song when he toured prisons, saying "For the three minutes that song is going on, everybody is free. It just frees the spirit and frees the person."
The U.S. Library of Congress has a collection of 3,000 versions of and songs inspired by "Amazing Grace", some of which were first-time recordings by folklorists Alan and John Lomax, a father and son team who in 1932 traveled thousands of miles across the South to capture the different regional styles of the song. More contemporary renditions include samples from such popular artists as Sam Cooke and The Soul Stirrers (1963), The Byrds (1970), Elvis Presley (1971), Skeeter Davis (1972), Mighty Clouds of Joy (1972), Andy Williams (1972), Amazing Rhythm Aces (1975), Willie Nelson (1976), The Lemonheads (1992) and Dropkick Murphys (1999).
Somehow, "Amazing Grace" [embraced] core American values without ever sounding triumphant or jingoistic. It was a song that could be sung by young and old, Republican and Democrat, Southern Baptist and Roman Catholic, African American and Native American, high-ranking military officer and anticapitalist campaigner.
Steve Turner, 2002
In popular culture.
Following the appropriation of the hymn in secular music, "Amazing Grace" became such an icon in American culture that it has been used for a variety of secular purposes and marketing campaigns, placing it in danger of becoming a cliché. It has been mass-produced on souvenirs, lent its name to a Superman villain, appeared on "The Simpsons" to demonstrate the redemption of a murderous character named Sideshow Bob, incorporated into Hare Krishna chants and adapted for Wicca ceremonies. The hymn has been employed in several films, including "Alice's Restaurant", "Coal Miner's Daughter", and "Silkwood". It is referenced in the 2006 film "Amazing Grace", which highlights Newton's influence on the leading British abolitionist William Wilberforce, and in the upcoming film biography of Newton, "Newton's Grace". The 1982 science fiction film "" used "Amazing Grace" amid a context of Christian symbolism, to memorialize Mr. Spock following his death, but more practically, because the song has become "instantly recognizable to many in the audience as music that sounds appropriate for a funeral" according to a Star Trek scholar. Since 1954 when an organ instrumental of "New Britain" became a bestseller, "Amazing Grace" has been associated with funerals and memorial services. It has become a song that inspires hope in the wake of tragedy, becoming a sort of "spiritual national anthem" according to authors Mary Rourke and Emily Gwathmey.
Modern interpretations.
In recent years, the words of the hymn have been changed in some religious publications to downplay a sense of imposed self-loathing by its singers. The second line, "That saved a wretch like me!" has been rewritten as "That saved and strengthened me", "save a soul like me", or "that saved and set me free". Kathleen Norris in her book "Amazing Grace: A Vocabulary of Faith" characterizes this transformation of the original words as "wretched English" making the line that replaces the original "laughably bland". Part of the reason for this change has been the altered interpretations of what wretchedness and grace means. Newton's Calvinistic view of redemption and divine grace formed his perspective that he considered himself a sinner so vile that he was unable to change his life or be redeemed without God's help. Yet his lyrical subtlety, in Steve Turner's opinion, leaves the hymn's meaning open to a variety of Christian and non-Christian interpretations. "Wretch" also represents a period in Newton's life when he saw himself outcast and miserable, as he was when he was enslaved in Sierra Leone; his own arrogance was matched by how far he had fallen in his life.
The communal understanding of redemption and human self-worth has changed since Newton's time. Since the 1970s, self-help books, psychology, and some modern expressions of Christianity have viewed this disparity in terms of grace being an innate quality within all people who must be inspired or strong enough to find it: something to achieve. In contrast to Newton's vision of wretchedness as his willful sin and distance from God, wretchedness has instead come to mean an obstacle of physical, social, or spiritual nature to overcome in order to achieve a state of grace, happiness, or contentment. Since its immense popularity and iconic nature, "grace" and the meaning behind the words of "Amazing Grace" have become as individual as the singer or listener. Bruce Hindmarsh suggests that the secular popularity of "Amazing Grace" is due to the absence of any mention of God in the lyrics until the fourth verse (by Excell's version, the fourth verse begins "When we've been there ten thousand years"), and that the song represents the ability of humanity to transform itself instead of a transformation taking place at the hands of God. "Grace", however, to John Newton had a clearer meaning, as he used the word to represent God or the power of God.
The transformative power of the song was investigated by journalist Bill Moyers in a documentary released in 1990. Moyers was inspired to focus on the song's power after watching a performance at Lincoln Center, where the audience consisted of Christians and non-Christians, and he noticed that it had an equal impact on everybody in attendance, unifying them. James Basker also acknowledged this force when he explained why he chose "Amazing Grace" to represent a collection of anti-slavery poetry: "there is a transformative power that is applicable ... : the transformation of sin and sorrow into grace, of suffering into beauty, of alienation into empathy and connection, of the unspeakable into imaginative literature."
Moyers interviewed Collins, Cash, opera singer Jessye Norman, Appalachian folk musician Jean Ritchie and her family, white Sacred Harp singers in Georgia, black Sacred Harp singers in Alabama, and a prison choir at the Texas State Penitentiary at Huntsville. Collins, Cash, and Norman were unable to discern if the power of the song came from the music or the lyrics. Norman, who once notably sang it at the end of a large outdoor rock concert for Nelson Mandela's 70th birthday, stated, "I don't know whether it's the text – I don't know whether we're talking about the lyrics when we say that it touches so many people – or whether it's that tune that everybody knows." A prisoner interviewed by Moyers explained his literal interpretation of the second verse: "'Twas grace that taught my heart to fear, and grace my fears relieved" by saying that the fear became immediately real to him when he realized he may never get his life in order, compounded by the loneliness and restriction in prison. Gospel singer Marion Williams summed up its effect: "That's a song that gets to everybody".
The "Dictionary of American Hymnology" claims it is included in more than a thousand published hymnals, and recommends its use for "occasions of worship when we need to confess with joy that we are saved by God's grace alone; as a hymn of response to forgiveness of sin or as an assurance of pardon; as a confession of faith or after the sermon."
References.
</dl>

</doc>
<doc id="1408" url="http://en.wikipedia.org/wiki?curid=1408" title="Alcuin">
Alcuin

Alcuin of York (Latin: "Alcuinus", c. 735 – 19 May 804), also called Ealhwine, Albinus or Flaccus, was an English scholar, ecclesiastic, poet and teacher from York, Northumbria. He was born around 735 and became the student of Archbishop Ecgbert at York. At the invitation of Charlemagne, he became a leading scholar and teacher at the Carolingian court, where he remained a figure in the 780s and 790s. He wrote many theological and dogmatic treatises, as well as a few grammatical works and a number of poems. He was made Abbot of Tours in 796, where he remained until his death. "The most learned man anywhere to be found", according to Einhard's "Life of Charlemagne", he is considered among the most important architects of the Carolingian Renaissance. Among his pupils were many of the dominant intellectuals of the Carolingian era.
Biography.
Background.
Alcuin was born in Northumbria, presumably sometime in the 730s. Virtually nothing is known of his parents, family background, or origin. In common hagiographical fashion, the "Vita Alcuini" asserts that Alcuin was 'of noble English stock,' and this statement has usually been accepted by scholars. Alcuin's own work only mentions such collateral kinsmen as Wilgils, father of the missionary saint Willibrord; and Beornred, abbot of Echternach and bishop of Sens, who was more distantly related. In his "Life" of St Willibrord, Alcuin writes that Wilgils, called a "paterfamilias", had founded an oratory and church at the mouth of the Humber, which had fallen into Alcuin's possession by inheritance. Because in early Anglo-Latin writing "paterfamilias" ("head of a family, householder") usually referred to a ceorl, Donald A. Bullough suggests that Alcuin's family was of cierlisc status: "i.e.," free but subordinate to a noble lord, and that Alcuin and other members of his family rose to prominence through beneficial connections with the aristocracy. If so, Alcuin's origins may lie in the southern part of what was formerly known as Deira.
York.
The young Alcuin came to the cathedral church of York during the golden age of Archbishop Ecgbert and his brother, the Northumbrian King Eadberht. Ecgbert had been a disciple of the Venerable Bede, who urged him to raise York to an archbishopric. King Eadberht and Archbishop Ecgbert oversaw the re-energising and re-organisation of the English church, with an emphasis on reforming the clergy and on the tradition of learning that Bede had begun. Ecgbert was devoted to Alcuin, who thrived under his tutelage.
The York school was renowned as a centre of learning in the liberal arts, literature, and science, as well as in religious matters. It was from here that Alcuin drew inspiration for the school he would lead at the Frankish court. He revived the school with the trivium and quadrivium disciplines, writing a codex on the trivium, while his student Hraban wrote one on the quadrivium.
Alcuin graduated to become a teacher during the 750s. His ascendancy to the headship of the York school, the ancestor of St Peter's School, began after Aelbert became Archbishop of York in 767. Around the same time Alcuin became a deacon in the church. He was never ordained as a priest and there is no real evidence that he became an actual monk, but he lived his life as one.
In 781, King Elfwald sent Alcuin to Rome to petition the Pope for official confirmation of York's status as an archbishopric and to confirm the election of the new archbishop, Eanbald I. On his way home he met Charlemagne (whom he had met once before), this time in the Italian city of Parma.
Charlemagne.
Alcuin's intellectual curiosity allowed him to be reluctantly persuaded to join Charlemagne's court. He joined an illustrious group of scholars that Charlemagne had gathered around him, the mainsprings of the Carolingian Renaissance: Peter of Pisa, Paulinus of Aquileia, Rado, and Abbot Fulrad. Alcuin would later write that "the Lord was calling me to the service of King Charles."
He was welcomed at the Palace School of Charlemagne in Aachen ("Urbs Regale") in 782. It had been founded by the king's ancestors as a place for the education of the royal children (mostly in manners and the ways of the court). However, Charlemagne wanted to include the liberal arts and, most importantly, the study of the religion. From 782 to 790, Alcuin taught Charlemagne himself, his sons Pepin and Louis, the young men sent to be educated at court, and the young clerics attached to the palace chapel. Bringing with him from York his assistants Pyttel, Sigewulf, and Joseph, Alcuin revolutionised the educational standards of the Palace School, introducing Charlemagne to the liberal arts and creating a personalised atmosphere of scholarship and learning, to the extent that the institution came to be known as the 'school of Master Albinus'.
In this role as adviser, he tackled the emperor over his policy of forcing pagans to be baptised on pain of death, arguing, "Faith is a free act of the will, not a forced act. We must appeal to the conscience, not compel it by violence. You can force people to be baptised, but you cannot force them to believe." His arguments seem to have prevailed – Charlemagne abolished the death penalty for paganism in 797.
Charlemagne was a master at gathering the best men of every land in his court. He himself became far more than just the king at the centre. It seems that he made many of these men his closest friends and counsellors. They referred to him as 'David', a reference to the Biblical king David. Alcuin soon found himself on intimate terms with Charlemagne and the other men at court, where pupils and masters were known by affectionate and jesting nicknames. Alcuin himself was known as 'Albinus' or 'Flaccus'. While at Aachen, Alcuin bestowed pet names upon his pupils – derived mainly from Virgil's "Eclogues".
Return to Northumbria and back to Francia.
In 790 Alcuin returned from the court of Charlemagne to England, to which he had remained attached. He dwelt there for some time, but Charlemagne then invited him back to help in the fight against the Adoptionist heresy which was at that time making great progress in Toledo, the old capital of the Visigoths and still a major city for the Christians under Islamic rule in Spain. He is believed to have had contacts with Beatus of Liébana, from the Kingdom of Asturias, who fought against Adoptionism. At the Council of Frankfurt in 794, Alcuin upheld the orthodox doctrine against the views expressed by Felix of Urgel, an heresiarch according to the Catholic Encyclopaedia. Having failed during his stay in Northumbria to influence King Æthelred in the conduct of his reign, Alcuin never returned home.
He was back at Charlemagne's court by at least mid-792, writing a series of letters to Æthelred, to Hygbald, Bishop of Lindisfarne, and to Æthelhard, Archbishop of Canterbury in the succeeding months, dealing with the Viking attack on Lindisfarne in July 793. These letters and Alcuin's poem on the subject, "De clade Lindisfarnensis monasterii", provide the only significant contemporary account of these events. In his description of the Viking attack, he wrote: ""Never before has such terror appeared in Britain. Behold the church of St Cuthbert, splattered with the blood of God's priests, robbed of its ornaments"."
Tours and death.
In 796 Alcuin was in his sixties. He hoped to be free from court duties and was given the chance upon the death of Abbot Itherius of Saint Martin at Tours, when Charlemagne put Marmoutier Abbey into Alcuin's care, with the understanding that he should be available if the king ever needed his counsel.
Alcuin died on 19 May 804, some ten years before the emperor, and was buried at St. Martin's Church under an epitaph that partly read:
Dust, worms, and ashes now ... Alcuin my name, wisdom I always loved, Pray, reader, for my soul.
He was later canonised as a saint, and remains recognised within the Roman Catholic, Anglican and Eastern Orthodox traditions.
The majority of details on Alcuin's life come from his letters and poems. There are also autobiographical sections in Alcuin's poem on York and in the "Vita Alcuini", a "Life" written for him at Ferrières in the 820s, possibly based in part on the memories of Sigwulf, one of Alcuin's pupils.
Carolingian Renaissance figure and legacy.
Literary influence.
Alcuin made the abbey school into a model of excellence and many students flocked to it. He had many manuscripts copied using outstandingly beautiful calligraphy, the Carolingian minuscule based on round and legible uncial letters. He wrote many letters to his English friends, to Arno, bishop of Salzburg and above all to Charlemagne. These letters (of which 311 are extant) are filled mainly with pious meditations, but they form an important source of information as to the literary and social conditions of the time and are the most reliable authority for the history of humanism during the Carolingian age. Alcuin trained the numerous monks of the abbey in piety, and it was in the midst of these pursuits that he died.
Alcuin is the most prominent figure of the Carolingian Renaissance, in which three main periods have been distinguished: in the first of these, up to the arrival of Alcuin at the court, the Italians occupy a central place; in the second, Alcuin and the Anglo-Saxons are dominant; in the third (from 804), the influence of Theodulf, the Visigoth is preponderant.
Alcuin also developed manuals used in his educational work – a grammar and works on rhetoric and dialectics. These are written in the form of dialogues, and in two of them the interlocutors are Charlemagne and Alcuin. He wrote several theological treatises: a "De fide Trinitatis", and commentaries on the Bible. Alcuin is credited with inventing the first known question mark, though it didn't resemble the modern symbol.
Alcuin transmitted to the Franks the knowledge of Latin culture which had existed in Anglo-Saxon England. A number of his works still exist. Besides some graceful epistles in the style of Venantius Fortunatus, he wrote some long poems, and notably he is the author of a history (in verse) of the church at York, "Versus de patribus, regibus et sanctis Eboracensis ecclesiae".
Mathematician.
The collection of mathematical and logical word problems entitled "Propositiones ad acuendos juvenes" ("Problems to Sharpen Youths") is sometimes attributed to Alcuin. In a 799 letter to Charlemagne the scholar claimed to have sent "certain figures of arithmetic for the joy of cleverness," which some scholars have identified with the "Propositiones."
The text contains about 53 mathematical word problems (with solutions), in no particular pedagogical order. Among the most famous of these problems are: four that involve river crossings, including the problem of three anxious brothers, each of whom has an unmarried sister whom he cannot leave alone with either of the other men lest she be defiled (Problem 17); the problem of the wolf, goat, and cabbage (Problem 18); and the problem of "the two adults and two children where the children weigh half as much as the adults" (Problem 19). Alcuin's sequence is the solution to one of the problems of that book.
Use of eroticised language.
Passages in Alcuin's writings have been seen to exhibit homosocial desire, possibly even homoerotic imagery. David Clark suggests it is not possible to determine whether Alcuin's homosocial desires were the result of an outward expression of erotic feelings. Historian John Boswell cited this as a personal outpouring of Alcuin's internalized homosexual feelings. Others agree that Alcuin at times "comes perilously close to communicating openly his same sex desires", and this reflects the erotic subculture of the Carolingian monastic school, but also perhaps a 'queer space' where "erotic attachment and affections may be safely articulated”. Erotic and religious love are intertwined in Alcuin's writings, and he frequently "eroticizes his personal relationships to his beloved friends”. Alcuin's friendships also extended to the ladies of the court, especially the queen mother and the king's daughters, though his relationships with these women never reached the intense level of those of the men around him.
However, the interpretation of homosexual desire has been disputed by Allen Frantzen who identifies Alcuin's language with that of medieval Christian "amicitia" or friendship. Karl Liersch, in his 1880 inaugural dissertation, cites several passages from poems by Theodulf of Orleans. In these poems Theodulf reports that Alcuin had a female muse named Delia in the king's court (she was probably Charlemagne's daughter). Delia is also the addresse of several poems by Alcuin.
Nevertheless, despite inconclusive of evidence of Alcuin's personal passions, he was clear in his own writings that the men of Sodom had been punished with fire for "sinning against nature with men". Such sins, argued Alcuin, were more serious than lustful acts with women, for which the earth was cleansed and revivified by the water of the Flood, and merit to be "withered by flames unto eternal barrenness."
Legacy.
In several churches of the Anglican Communion, Alcuin is celebrated on 20 May, the first available day after the day of his death (as Dunstan is celebrated on 19 May).
Alcuin College, one of the colleges of the University of York, England, is named after him.
Selected works.
For a complete census of Alcuin's works, see Marie-Hélène Jullien and Françoise Perelman, eds., "Clavis scriptorum latinorum medii aevi: Auctores Galliae 735–987. Tomus II: Alcuinus." Turnhout: Brepols, 1999.
Of Alcuin's letters, just over 310 have survived.
Further reading.
 #if: 
 #if: A Short Biographical Dictionary of English Literature
 |, 
 #if: 
 }}{{
 #if: 
 #if: 
 | ()
 |{{
 #if: 
 #if: 
 | {{#if:||}}{{
 #if: 
 #if: 
 #if: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Alcuin or Ealhwine
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Alcuin or Ealhwine
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 
 #if: A Short Biographical Dictionary of English Literature
 |{{
 #if: 
 }} {{Citation/make link
 | 1={{
 #if: 
 #if: 
 #if: 
 |{{
 #if: 
 | 2=" A Short Biographical Dictionary of English Literature
 #if:| []
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 | ( ed.)
 #if: 
 }}{{
 #if: 
 #if: 
 |,
 #if: 
 |{{
 #if: 1910
 |, 1910{{
 #if:
}}{{
 #if: 
 #ifeq: | 1910
 |{{
 #if: 
 #if: 
 | (published )
 |{{
 #if: 
 | (published )
}}{{
 #if: 
 |{{
 #if: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Alcuin or Ealhwine
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |, {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Alcuin or Ealhwine
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
}}{{
 #if:
 | , {{#ifeq: | no
 | {{#if:
 |{{Citation/make link||{{#ifeq:|.|A|a}}rchived}} from the original
 |{{#ifeq:|.|A|a}}rchived
 | {{#ifeq:|.|A|a}}rchived{{#if:
 }}{{#if:| on }}{{
 |. {{citation error|nocat=
 #if: A Short Biographical Dictionary of English Literature
 |, {{
 #if: 
 |
 |, {{
 #if: 
 |
 }}{{
 #if: 
 | {{#ifeq:|,|, r|. R}}etrieved 
}}{{#if:
}}{{#if:
}}{{#if:
}}<span
 class="Z3988"
 title="ctx_ver=Z39.88-2004&rft_val_fmt={{urlencode:info:ofi/fmt:kev:mtx:}}{{
 #if: 
 |journal&rft.genre=article&rft.atitle={{urlencode: A Short Biographical Dictionary of English Literature
 |book{{
 #if: 
 |&rft.genre=bookitem&rft.btitle={{urlencode:}}&rft.atitle={{urlencode: A Short Biographical Dictionary of English Literature
 |&rft.genre=book&rft.btitle={{urlencode: A Short Biographical Dictionary of English Literature
 #if: |&rft.aulast={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Alcuin or Ealhwine
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |&rft.pages={{urlencode: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Alcuin or Ealhwine
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 }}{{
 }}&rfr_id=info:sid/en.wikipedia.org:{{FULLPAGENAMEE}}"> 
 |IncludedWorkTitle = 
 |IncludedWorkURL = 
 |Other = 
 |Edition = 
 |Place = 
 |PublicationPlace = 
 |Publisher = Wikisource
 |PublicationDate = 
 |EditorSurname1 = 
 |EditorSurname2 = 
 |EditorSurname3 = 
 |EditorSurname4 = 
 |EditorGiven1 = 
 |EditorGiven2=
 |EditorGiven3=
 |EditorGiven4=
 |Editorlink1=
 |Editorlink2=
 |Editorlink3=
 |Editorlink4=
 |language = 
 |format = 
 |ARXIV=
 |ASIN=
 |BIBCODE=
 |DOI=
 |DoiBroken=
 |ISBN=
 |ISSN=
 |JFM=
 |JSTOR=
 |LCCN=
 |MR=
 |OCLC=
 |OL=
 |OSTI=
 |PMC=
 |Embargo=1010-10-10
 |PMID=
 |RFC=
 |SSRN=
 |ZBL=
 |ID=
 |AccessDate=
 |DateFormat=none
 |quote = 
 |laysummary = 
 |laydate = 
 |Ref=
 |Sep = .
 |PS = 
 |AuthorSep = ; 
 |NameSep = , 
 |Trunc = 8
 |amp = 
 #if: 
 #if: A Short Biographical Dictionary of English Literature
 |, 
 #if: 
 }}{{
 #if: 
 #if: 
 | ()
 |{{
 #if: 
 #if: 
 | {{#if:||}}{{
 #if: 
 #if: 
 #if: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Alcuin or Ealhwine
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Alcuin or Ealhwine
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 
 #if: A Short Biographical Dictionary of English Literature
 |{{
 #if: 
 }} {{Citation/make link
 | 1={{
 #if: 
 #if: 
 #if: 
 |{{
 #if: 
 | 2=" A Short Biographical Dictionary of English Literature
 #if:| []
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 | ( ed.)
 #if: 
 }}{{
 #if: 
 #if: 
 |,
 #if: 
 |{{
 #if: 1910
 |, 1910{{
 #if:
}}{{
 #if: 
 #ifeq: | 1910
 |{{
 #if: 
 #if: 
 | (published )
 |{{
 #if: 
 | (published )
}}{{
 #if: 
 |{{
 #if: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Alcuin or Ealhwine
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |, {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Alcuin or Ealhwine
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
}}{{
 #if:
 | , {{#ifeq: | no
 | {{#if:
 |{{Citation/make link||{{#ifeq:|.|A|a}}rchived}} from the original
 |{{#ifeq:|.|A|a}}rchived
 | {{#ifeq:|.|A|a}}rchived{{#if:
 }}{{#if:| on }}{{
 |. {{citation error|nocat=
 #if: A Short Biographical Dictionary of English Literature
 |, {{
 #if: 
 |
 |, {{
 #if: 
 |
 }}{{
 #if: 
 | {{#ifeq:|,|, r|. R}}etrieved 
}}{{#if:
}}{{#if:
}}{{#if:
}}<span
 class="Z3988"
 title="ctx_ver=Z39.88-2004&rft_val_fmt={{urlencode:info:ofi/fmt:kev:mtx:}}{{
 #if: 
 |journal&rft.genre=article&rft.atitle={{urlencode: A Short Biographical Dictionary of English Literature
 |book{{
 #if: 
 |&rft.genre=bookitem&rft.btitle={{urlencode:}}&rft.atitle={{urlencode: A Short Biographical Dictionary of English Literature
 |&rft.genre=book&rft.btitle={{urlencode: A Short Biographical Dictionary of English Literature
 #if: |&rft.aulast={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Alcuin or Ealhwine
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |&rft.pages={{urlencode: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Alcuin or Ealhwine
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 }}{{
 }}&rfr_id=info:sid/en.wikipedia.org:{{FULLPAGENAMEE}}"> 
 |IncludedWorkTitle = 
 |IncludedWorkURL = 
 |Other = 
 |Edition = 
 |Place = 
 |PublicationPlace = 
 |Publisher = Wikisource
 |PublicationDate = 
 |EditorSurname1 = 
 |EditorSurname2 = 
 |EditorSurname3 = 
 |EditorSurname4 = 
 |EditorGiven1 = 
 |EditorGiven2=
 |EditorGiven3=
 |EditorGiven4=
 |Editorlink1=
 |Editorlink2=
 |Editorlink3=
 |Editorlink4=
 |language = 
 |format = 
 |ARXIV=
 |ASIN=
 |BIBCODE=
 |DOI=
 |DoiBroken=
 |ISBN=
 |ISSN=
 |JFM=
 |JSTOR=
 |LCCN=
 |MR=
 |OCLC=
 |OL=
 |OSTI=
 |PMC=
 |Embargo=1010-10-10
 |PMID=
 |RFC=
 |SSRN=
 |ZBL=
 |ID=
 |AccessDate=
 |DateFormat=none
 |quote = 
 |laysummary = 
 |laydate = 
 |Ref=
 |Sep = .
 |PS = 
 |AuthorSep = ; 
 |NameSep = , 
 |Trunc = 8
 |amp = 

</doc>
<doc id="1486" url="http://en.wikipedia.org/wiki?curid=1486" title="Alemanni">
Alemanni

The Alemanni (also "Alamanni"; "Suebi" "Swabians") were a confederation of Germanic tribes on the upper Rhine river. First mentioned by Cassius Dio in the context of the campaign of Caracalla of 213, the Alemanni captured the Agri Decumates in 260, and later expanded into present-day Alsace, and northern Switzerland, leading to the establishment of the Old High German language in those regions. 
In 496, the Alemanni were conquered by Frankish leader Clovis and incorporated into his dominions. Mentioned as still pagan allies of the Christian Franks, the Alemanni were gradually Christianized during the 7th century. The "Pactus Alamannorum" is a record of their customary law during this period. Until the 8th century, Frankish suzerainty over Alemannia was mostly nominal. But after an uprising by Theudebald, Duke of Alamannia, Carloman executed the Alamannic nobility and installed Frankish dukes. 
During the later and weaker years of the Carolingian Empire the Alemannic counts became almost independent, and a struggle for supremacy took place between them and the Bishopric of Constance. The chief family in Alamannia was that of the counts of Raetia Curiensis, who were sometimes called margraves, and one of whom, Burchard II established the Duchy of Swabia, which was recognized by Henry the Fowler in 919 and became a stem duchy of the Holy Roman Empire.
The area settled by the Alemanni corresponds roughly to the area where Alemannic German dialects remain spoken, including German Swabia and Baden, French Alsace, German-speaking Switzerland and Austrian Vorarlberg.
Name.
According to Asinius Quadratus (quoted in the mid-6th century by Byzantine historian Agathias) their name means "all men". It indicates that they were a conglomeration drawn from various Germanic tribes. This was the derivation of "Alemanni" used by Edward Gibbon, in his "Decline and Fall of the Roman Empire" and by the anonymous contributor of notes assembled from the papers of Nicolas Fréret, published in 1753, who noted that it was the name used by outsiders for those who called themselves the "Suebi". This etymology has remained the standard derivation of the term.
Walafrid Strabo, a monk of the Abbey of St. Gall writing in the 9th century, remarked, in discussing the people of Switzerland and the surrounding regions, that only foreigners called them the "Alemanni", but that they gave themselves the name of "Suebi".
The name of Germany and the German language in several languages is derived from the name of this early Germanic tribal alliance. For details, see Names of Germany.
History.
First explicit mention.
The Alemanni were first mentioned by Cassius Dio describing the campaign of Caracalla in 213. At that time they apparently dwelt in the basin of the Main, to the south of the Chatti.
Cassius Dio (78.13.4) portrays the Alemanni as victims of this treacherous emperor. They had asked for his help, says Dio, but instead he colonized their country, changed their place names and executed their warriors under a pretext of coming to their aid. When he became ill, the Alemanni claimed to have put a hex on him (78.15.2). Caracalla, it was claimed, tried to counter this influence by invoking his ancestral spirits.
In retribution Caracalla then led the Legio II "Traiana Fortis" against the Alemanni, who lost and were pacified for a time. The legion was as a result honored with the name "Germanica". The 4th-century fictional Historia Augusta, "Life of Antoninus Caracalla", relates (10.5) that Caracalla then assumed the name "Alemannicus", at which Helvius Pertinax jested that he should really be called "Geticus Maximus", because in the year before he had murdered his brother, Geta. Not on good terms with Caracalla, Geta had been invited to a family reconciliation, at which time he was ambushed by centurions in Caracalla's army and slain in his mother Julia's arms. True or not, Caracalla, pursued by devils of his own, left Rome never to return.
Caracalla left for the frontier, where for the rest of his short reign he was known for his unpredictable and arbitrary operations launched by surprise after a pretext of peace negotiations. If he had any reasons of state for such actions they remained unknown to his contemporaries. Whether or not the Alemanni had been previously neutral, they were certainly further influenced by Caracalla to become thereafter notoriously implacable enemies of Rome.
This mutually antagonistic relationship is perhaps the reason why the Roman writers persisted in calling the Alemanni "barbari", "savages". The archaeology, however, shows that they were largely Romanized, lived in Roman-style houses and used Roman artifacts, the Alemannic women having adopted the Roman fashion of the "tunic" even earlier than the men.
Most of the Alemanni were probably at the time in fact resident in or close to the borders of Germania Superior. Although Dio is the earliest writer to mention them, Ammianus Marcellinus used the name to refer to Germans on the Limes Germanicus in the time of Trajan's governorship of the province shortly after it was formed, c. 98/99. At that time the entire frontier was being fortified for the first time. Trees from the earliest fortifications found in Germania Inferior are dated by dendrochronology to 99/100 AD. Shortly afterwards Trajan was chosen by Nerva to be his successor, adopted with public fanfare in absentia by the old man shortly before his death. By 100 AD. Trajan was back in Rome as Emperor instead of merely being a Consul.
Ammianus relates () that much later the Emperor Julian undertook a punitive expedition against the Alemanni, who by then were in Alsace, and crossed the Main (Latin "Menus"), entering the forest, where the trails were blocked by felled trees. As winter was upon them, they reoccupied a
"fortification which was founded on the soil of the Alemanni that Trajan wished to be called with his own name".
In this context the use of Alemanni is possibly an anachronism but it reveals that Ammianus believed they were the same people, which is consistent with the location of the Alemanni of Caracalla's campaigns.
Alemanni and Hermunduri.
The early detailed source, the "Germania" of Tacitus, has sometimes been interpreted in such a way as to provide yet other historical problems. In we read of the Hermunduri, a tribe certainly located in the region that later became Thuringia. Tacitus stated that they traded with Rhaetia, which in Ptolemy is located across the Danube from Germania Superior. A logical conclusion to draw is that the Hermunduri extended over later Swabia and therefore the Alemanni originally derived from the Hermunduri!
However, no Hermunduri appear in Ptolemy, though after the time of Ptolemy, the Hermunduri joined with the Marcomanni in the wars of 166–180 against the empire. A careful reading of Tacitus provides one solution. He says that the source of the Elbe is among the Hermunduri, somewhat to the east of the upper Main. He places them also between the Naristi (Varisti), whose location at the very edge of the ancient Black Forest is well known, and the Marcomanni and Quadi. Moreover, the Hermunduri were broken in the Marcomannic Wars and made a separate peace with Rome. The Alemanni thus were probably not primarily the Hermunduri, although some elements of them may have been present in the mix of peoples at that time that became Alemannian.
Ptolemy's "Geography".
Before the mention of "Alemanni" in the time of Caracalla, you would search in vain for Alemanni in the moderately detailed geography of southern Germany in Claudius Ptolemy, written in Greek in the mid-2nd century; it is likely that at that time, the people who later used that name were known by other designations.
Nevertheless some conclusions can be drawn from Ptolemy. Germania Superior is easily identified. Following up the Rhine one comes to a town, Mattiacum, which must be at the border of the Roman Germany (vicinity of Wiesbaden). Upstream from it and between the Rhine and Abnoba (in the Black Forest) are the Ingriones, Intuergi, Vangiones, Caritni and Vispi, some of whom were there since the days of the early empire or before. On the other side of the northern Black Forest were the Chatti about where Hesse is today, on the lower Main.
Historic Swabia was eventually replaced by today's Baden-Württemberg, but it had been the most significant territory of mediaeval Alamannia, comprising all Germania Superior and territory east to Bavaria. It did not include the upper Main, but that is where Caracalla campaigned. Moreover, the territory of Germania Superior was not originally included among the Alemanni's possessions.
However, if we look for the peoples in the region from the upper Main in the north, south to the Danube and east to the Czech Republic where the Quadi and Marcomanni were located, Ptolemy does not give any tribes. There are the Tubanti just south of the Chatti and at the other end of what was then the Black Forest, the Varisti, whose location is known. One possible reason for this distribution is that the population preferred not to live in the forest except in troubled times. The region between the forest and the Danube on the other hand included about a dozen settlements, or "cantons".
Ptolemy's view of Germans in the region indicates that the tribal structure had lost its grip in the Black Forest region and was replaced by a canton structure. The tribes stayed in the Roman province, perhaps because the Romans offered stability. Also, Caracalla perhaps felt more comfortable about campaigning in the upper Main because he was not declaring war on any specific historic tribe, such as the Chatti or Cherusci, against whom Rome had suffered grievous losses. By Caracalla's time the name "Alemanni" was being used by cantons themselves banding together for purposes of supporting a citizen army (the "war bands").
Concentration of Germanic peoples under Ariovistus.
The term Suebi has a double meaning in the sources. On the one hand Tacitus' "Germania" tells us ( that they occupy more than half of Germany, use a distinctive hair style, and are spiritually centered on the Semnones. On the other hand the Suebi of the upper Danube are described as though they were a tribe.
The solution to the puzzle as well as explaining the historical circumstances leading to the choice of the Agri Decumates as a defensive point and the concentration of Germans there are probably to be found in the German attack on the Gallic fortified town of Vesontio in 58 BC. The upper Rhine and Danube appear to form a funnel pointing straight at Vesontio.
Julius Caesar in "Gallic Wars" tells us () that Ariovistus had gathered an army from a wide region of Germany, but especially the Harudes, Marcomanni, Triboci, Vangiones, Nemetes and Sedusii. The Suebi were being invited to join. They lived in 100 cantons () from which 1000 young men per year were chosen for military service, a citizen-army by our standards and by comparison with the Roman professional army.
Ariovistus had become involved in an invasion of Gaul, which the German wished to settle. Intending to take the strategic town of Vesontio, he concentrated his forces on the Rhine near Lake Constance, and when the Suebi arrived, he crossed. The Gauls had called to Rome for military aid. Caesar occupied the town first and defeated the Germans before its walls, slaughtering most of the German army as it tried to flee across the river (1.36ff). He did not pursue the retreating remnants, leaving what was left of the German army and their dependents intact on the other side of the Rhine.
The Gauls were ambivalent in their policies toward the Romans. In 53 BC the Treveri broke their alliance and attempted to break free of Rome. Caesar foresaw that they would now attempt to ally themselves with the Germans. He crossed the Rhine to forestall that event, a successful strategy. Remembering their expensive defeat at the Battle of Vesontio, the Germans withdrew to the Black Forest, concentrating there a mixed population dominated by Suebi. As they had left their tribal homes behind, they probably took over all the former Celtic cantons along the Danube.
Conflicts with the Roman Empire.
The Alemanni were continually engaged in conflicts with the Roman Empire in the 3rd and 4th centuries. They launched a major invasion of Gaul and northern Italy in 268, when the Romans were forced to denude much of their German frontier of troops in response to a massive invasion of the Goths from the east. Their raids throughout the three parts of Gaul were traumatic: Gregory of Tours (died ca 594) mentions their destructive force at the time of Valerian and Gallienus (253–260), when the Alemanni assembled under their "king", whom he calls Chrocus, who "by the advice, it is said, of his wicked mother, and overran the whole of the Gauls, and destroyed from their foundations all the temples which had been built in ancient times. And coming to Clermont he set on fire, overthrew and destroyed that shrine which they call "Vasso Galatae" in the Gallic tongue," martyring many Christians (). Thus 6th-century Gallo-Romans of Gregory's class, surrounded by the ruins of Roman temples and public buildings, attributed the destruction they saw to the plundering raids of the Alemanni.
In the early summer of 268, the Emperor Gallienus halted their advance into Italy, but then had to deal with the Goths. When the Gothic campaign ended in Roman victory at the Battle of Naissus in September, Gallienus' successor Claudius II Gothicus turned north to deal with the Alemanni, who were swarming over all Italy north of the Po River.
After efforts to secure a peaceful withdrawal failed, Claudius forced the Alemanni to battle at the Battle of Lake Benacus in November. The Alemanni were routed, forced back into Germany, and did not threaten Roman territory for many years afterwards.
Their most famous battle against Rome took place in Argentoratum (Strasbourg), in 357, where they were defeated by Julian, later Emperor of Rome, and their king Chnodomarius was taken prisoner to Rome.
On January 2, 366, the Alemanni yet again crossed the frozen Rhine in large numbers, to invade the Gallic provinces, this time being defeated by Valentinian (see Battle of Solicinium). In the great mixed invasion of 406, the Alemanni appear to have crossed the Rhine river a final time, conquering and then settling what is today Alsace and a large part of the Swiss Plateau. The crossing is described in Wallace Breem's historical novel "Eagle in the Snow." Fredegar's Chronicle gives the account. At "Alba Augusta" (Alba-la-Romaine) the devastation was so complete, that the Christian bishop retired to Viviers, but in Gregory's account at Mende in Lozère, also deep in the heart of Gaul, bishop Privatus was forced to sacrifice to idols in the very cave where he was later venerated. It is thought this detail may be a generic literary ploy to epitomize the horrors of barbarian violence.
Subjugation by the Franks.
The kingdom of Alamannia between Strasbourg and Augsburg lasted until 496, when the Alemanni were conquered by Clovis I at the Battle of Tolbiac. The war of Clovis with the Alemanni forms the setting for the conversion of Clovis, briefly treated by Gregory of Tours () Subsequently the Alemanni formed part of the Frankish dominions and were governed by a Frankish duke.
In 746, Carloman ended an uprising by summarily executing all Alemannic nobility at the blood court at Cannstatt, and for the following century, Alemannia was ruled by Frankish dukes. Following the treaty of Verdun of 843, Alemannia became a province of the eastern kingdom of Louis the German, the precursor of the Holy Roman Empire. The duchy persisted until 1268.
Culture.
Language.
The German spoken today over the range of the former Alemanni is termed Alemannic German, and is recognised among the subgroups of the High German languages. Alemannic runic inscriptions such as those on the Pforzen buckle are among the earliest testimonies of Old High German.
The High German consonant shift is thought to have originated around the 5th century either in Alemannia or among the Lombards; before that the dialect spoken by Alemannic tribes was little different from that of other West Germanic peoples.
"Alemannia" lost its distinct jurisdictional identity when Charles Martel absorbed it into the Frankish empire, early in the 8th century. Today, "Alemannic" is a linguistic term, referring to Alemannic German, encompassing the dialects of the southern two thirds of Baden-Württemberg (German State), in western Bavaria (German State), in Vorarlberg (Austrian State), Swiss German in Switzerland and the Alsatian language of the Alsace (France).
Political organization.
The Alemanni established a series of territorially defined "pagi" (cantons) on the east bank of the Rhine. The exact number and extent of these "pagi" is unclear and probably changed over time.
"Pagi", usually pairs of "pagi" combined, formed kingdoms ("regna") which, it is generally believed, were permanent and hereditary. Ammianus describes Alemanni rulers with various terms: "reges excelsiores ante alios" ("paramount kings"), "reges proximi" ("neighbouring kings"), "reguli" ("petty kings") and "regales" ("princes"). This may be a formal hierarchy, or they may be vague, overlapping terms, or a combination of both. In 357, there appear to have been two paramount kings (Chnodomar and Westralp) who probably acted as presidents of the confederation and seven other kings ("reges"). Their territories were small and mostly strung along the Rhine (although a few were in the hinterland). It is possible that the "reguli" were the rulers of the two "pagi" in each kingdom. Underneath the royal class were the nobles (called "optimates" by the Romans) and warriors (called "armati" by the Romans). The warriors consisted of professional warbands and levies of free men. Each nobleman could raise an average of c. 50 warriors.
Religion.
The Christianization of the Alemanni took place during Merovingian times (6th to 8th centuries). We know that in the 6th century, the Alemanni were predominantly pagan, and in the 8th century, they were predominantly Christian. The intervening 7th century was a period of genuine syncretism during which Christian symbolism and doctrine gradually grew in influence.
Some scholars have speculated that members of the Alemannic elite such as king Gibuld due to Visigothic influence may have been converted to Arianism even in the later 5th century.
In the mid-6th century, the Byzantine historian Agathias of Myrina records, in the context of the wars of the Goths and Franks against Byzantium, that the Alemanni fighting among the troops of Frankish king Theudebald were like the Franks in all respects except religion, since
they worship certain trees, the waters of rivers, hills and mountain valleys, in whose honour they sacrifice horses, cattle and countless other animals by beheading them, and imagine that they are performing an act of piety thereby.
He also spoke of the particular ruthlessness of the Alemanni in destroying Christian sanctuaries and plundering churches while the genuine Franks were respectful towards those sanctuaries. Agathias expresses his hope that the Alemanni would assume better manners through prolonged contact with the Franks, which is by all appearances, in a manner of speaking, what eventually happened.
Apostles of the Alemanni were Saint Columbanus and his disciple Saint Gall. Jonas of Bobbio records that Columbanus was active in Bregenz, where he disrupted a beer sacrifice to Wodan. Despite these activities, for some time, the Alemanni seem to have continued their pagan cult activities, with only superficial or syncretistic Christian elements. In particular, there is no change in burial practice, and tumulus warrior graves continued to be erected throughout Merovingian times. Syncretism of traditional Germanic animal-style with Christian symbolism is also present in artwork, but Christian symbolism becomes more and more prevalent during the 7th century. Unlike the later Christianization of the Saxon and of the Slavs, the Alemanni seem to have adopted Christianity gradually, and voluntarily, spread in emulation of the Merovingian elite.
From c. the 520s to the 620s, there was a surge of Alemannic Elder Futhark inscriptions. About 70 specimens have survived, roughly half of them on fibulae, others on belt buckles (see Pforzen buckle, Bülach fibula) and other jewelry and weapon parts. Use of runes subsides with the advance of Christianity.
The Nordendorf fibula (early 7th century) clearly records pagan theonyms, "logaþorewodanwigiþonar " read as "Wodan and Donar are magicians/sorcerers", but this may be interpreted as either a pagan invocation of the powers of these deities, or a Christian protective charm against them.
A runic inscription on a fibula found at Bad Ems reflects Christian pious sentiment (and is also explicitly marked with a Christian cross), reading "god fura dih deofile ᛭" ("God for/before you, Theophilus!", or alternatively "God before you, Devil!"). Dated to between AD 660 and 690, it marks the end of the native Alemannic tradition of runic literacy. Bad Ems is in Rhineland-Palatinate, on the northwestern boundary of Alemannic settlement, where Frankish influence would have been strongest.
The establishment of the bishopric of Konstanz cannot be dated exactly and was possibly undertaken by Columbanus himself (before 612). In any case, it existed by 635, when Gunzo appointed John of Grab bishop. Constance was a missionary bishopric in newly converted lands, and did not look back on late Roman church history (unlike the Raetian bishopric of Chur, established 451) and Basel, which was an episcopal seat from 740, and which continued the line of Bishops of Augusta Raurica, see Bishop of Basel. The establishment of the church as an institution recognized by worldly rulers is also visible in legal history. In the early 7th century "Pactus Alamannorum" hardly ever mentions the special privileges of the church, while Lantfrid's "Lex Alamannorum" of 720 has an entire chapter reserved for ecclesial matters alone.
List of Alemannic rulers.
Dukes under Frankish suzerainty.
</dl>

</doc>
<doc id="1504" url="http://en.wikipedia.org/wiki?curid=1504" title="Albert">
Albert

Albert may refer to:

</doc>
<doc id="1514" url="http://en.wikipedia.org/wiki?curid=1514" title="Albert, Duke of Prussia">
Albert, Duke of Prussia

Albert of Prussia (17 May 1490 – 20 March 1568) was the last Grand Master of the Teutonic Knights, who after converting to Lutheranism, became the first monarch of the Duchy of Prussia, the secularized state that emerged from the former Monastic State of the Teutonic Knights. Albert was the first European ruler to establish Protestantism as the official state religion of his lands. He proved instrumental in the political spread of Protestantism in its early stage, ruling the Prussian lands for nearly six decades (1510–1568).
A member of the Brandenburg-Ansbach branch of the House of Hohenzollern, Albert's election as Grand Master had brought about hopes of a reversal of the declining fortune of the Teutonic Knights. He was a skilled political administrator and leader, and did indeed reverse the decline of the Teutonic Order. However, Albert, who was sympathetic to the demands of Martin Luther, rebelled against the Catholic church, and the Holy Roman Empire by converting the Teutonic state into a Protestant and hereditary realm, the Duchy of Prussia, for which he did homage to his uncle, the King of Poland. The arrangement was confirmed by the Treaty of Kraków in 1525. Albert pledged a personal oath to the King and in return was invested with the duchy for himself and his heirs.
Albert's rule in Prussia was fairly prosperous. Although he had some trouble with the peasantry, the confiscation of the lands and treasures of the Catholic church enabled him to propitiate the nobles and provide for the expenses of the newly established Prussian court. He was active in imperial politics, joining the League of Torgau in 1526, and acted in unison with the Protestants in plotting to overthrow Emperor Charles V after the issue of the Augsburg Interim in May 1548. Albert established schools in every town and founded Königsberg University in 1544. He promoted culture and arts, patronising the works of Erasmus Reinhold and Caspar Hennenberger. Albert's final years were clouded by peasant resentment of heavy taxes, a step he felt compelled to take due to there no longer being any church land available to confiscate in Prussia. The intrigues of court favourites, Johann Funck, and Paul Skalić also caused various religious and political disputes. Albert spent his final years virtually deprived of power, and died at Tapiau on 20 March 1568. His son Albert Frederick succeeded him as Duke of Prussia.
Albert's dissolution of the Teutonic State caused the founding of the Duchy of Prussia, paving the way for the rise of the House of Hohenzollern. He is therefore often seen as the father of the Prussian nation, and even as indirectly responsible for the unification of Germany.
Early life.
Albert was born in Ansbach in Franconia as the third son of Frederick I, Margrave of Brandenburg-Ansbach. His mother was Sophia, daughter of Casimir IV Jagiellon, Grand Duke of Lithuania and King of Poland, and his wife Elisabeth of Austria. He was raised for a career in the Church and spent some time at the court of Hermann IV of Hesse, Elector of Cologne, who appointed him canon of the Cologne Cathedral.
Not only was he quite religious, he was also interested in mathematics and science, and sometimes is claimed to have contradicted the teachings of the Church in favour of scientific theories. His career was forwarded by the Church however and institutions of the Catholic clerics supported his early advance.
Turning to a more active life, Albert accompanied Emperor Maximilian I to Italy in 1508, and after his return spent some time in the Kingdom of Hungary.
Grand Master.
Duke Frederick of Saxony, Grand Master of the Teutonic Order, died in December 1510. Albert was chosen as his successor early in 1511 in the hope that his relationship to his maternal uncle, Sigismund I the Old, Grand Duke of Lithuania and King of Poland, would facilitate a settlement of the disputes over eastern Prussia, which had been held by the Order under Polish suzerainty since the Second Peace of Thorn (1466).
The new Grand Master, aware of his duties to the empire and to the papacy, refused to submit to the crown of Poland. As war over the Order's existence appeared inevitable, Albert made strenuous efforts to secure allies and carried on protracted negotiations with Emperor Maximilian I. The ill-feeling, influenced by the ravages of members of the Order in Poland, culminated in a war which began in December 1519 and devastated Prussia. Albert was granted a four-year truce early in 1521.
The dispute was referred to Emperor Charles V and other princes, but as no settlement was reached Albert continued his efforts to obtain help in view of a renewal of the war. For this purpose he visited the Diet of Nuremberg in 1522, where he made the acquaintance of the Reformer Andreas Osiander, by whose influence Albert was won over to Protestantism.
The Grand Master then journeyed to Wittenberg, where he was advised by Martin Luther to abandon the rules of his Order, to marry, and to convert Prussia into a hereditary duchy for himself. This proposal, which was understandably appealing to Albert, had already been discussed by some of his relatives; but it was necessary to proceed cautiously, and he assured Pope Adrian VI that he was anxious to reform the Order and punish the knights who had adopted Lutheran doctrines. Luther for his part did not stop at the suggestion, but in order to facilitate the change made special efforts to spread his teaching among the Prussians, while Albert's brother, Margrave George of Brandenburg-Ansbach, laid the scheme before their uncle, Sigismund I the Old of Poland.
Duke in Prussia.
After some delay Sigismund assented to the offer, with the provision that Prussia should be treated as a Polish fiefdom; and after this arrangement had been confirmed by a treaty concluded at Kraków, Albert pledged a personal oath to Sigismund I and was invested with the duchy for himself and his heirs on 10 February 1525.
The Estates of the land then met at Königsberg (Kaliningrad) and took the oath of allegiance to the new duke, who used his full powers to promote the doctrines of Luther. This transition did not, however, take place without protest. Summoned before the imperial court of justice, Albert refused to appear and was proscribed, while the Order elected a new Grand Master, Walter von Cronberg, who received Prussia as a fief at the imperial Diet of Augsburg. As the German princes were experiencing the tumult of the Reformation, the German Peasants' War, and the wars against the Ottoman Turks, they did not enforce the ban on the duke, and agitation against him soon died away.
In imperial politics Albert was fairly active. Joining the League of Torgau in 1526, he acted in unison with the Protestants, and was among the princes who banded and plotted together to overthrow Charles V after the issue of the Augsburg Interim in May 1548. For various reasons, however, poverty and personal inclination among others, he did not take a prominent part in the military operations of this period.
The early years of Albert's rule in Prussia were fairly prosperous. Although he had some trouble with the peasantry, the lands and treasures of the church enabled him to propitiate the nobles and for a time to provide for the expenses of the court. He did something for the furtherance of learning by establishing schools in every town and by freeing serfs who adopted a scholastic life. In 1544, in spite of some opposition, he founded Königsberg University, where he appointed his friend Andreas Osiander to a professorship in 1549. Albert also paid for the printing of the Astronomical "Prutenic Tables" compiled by Erasmus Reinhold and the first maps of Prussia by Caspar Hennenberger.
This step was the beginning of the troubles which clouded the closing years of Albert's reign. Osiander's divergence from Luther's doctrine of justification by faith involved him in a violent quarrel with Philip Melanchthon, who had adherents in Königsberg, and these theological disputes soon created an uproar in the town. The duke strenuously supported Osiander, and the area of the quarrel soon broadened. There were no longer church lands available with which to conciliate the nobles, the burden of taxation was heavy, and Albert's rule became unpopular.
After Osiander's death in 1552, Albert favoured a preacher named Johann Funck, who, with an adventurer named Paul Skalić, exercised great influence over him and obtained considerable wealth at public expense. The state of turmoil caused by these religious and political disputes was increased by the possibility of Albert's early death and the need, should that happen, to appoint a regent, as his only son, Albert Frederick was still a mere youth. The duke was forced to consent to a condemnation of the teaching of Osiander, and the climax came in 1566 when the Estates appealed to King Sigismund II Augustus of Poland, Albert's cousin, who sent a commission to Königsberg. Skalić saved his life by flight, but Funck was executed. The question of the regency was settled, and a form of Lutheranism was adopted and declared binding on all teachers and preachers.
Virtually deprived of power, the duke lived for two more years, and died at Tapiau on 20 March 1568. Cornelis Floris de Vriendt designed his tomb within Königsberg Cathedral.
Albert was a voluminous letter writer, and corresponded with many of the leading personages of the time.
Legacy.
Although Albert has received relatively little recognition in German history, his dissolution of the Teutonic State caused the founding of the Duchy of Prussia, which would eventually become arguably the most powerful German state and instrumental in uniting the whole of Germany. Albert is therefore often seen as the father of the Prussian nation, and even as indirectly responsible for the unification of Germany. He was a skilled political administrator and leader, and effectively reversed the decline of the Teutonic Order, until he betrayed it by transforming the order's lands into his own duchy, secularizing it in the process.
Albert was the first German noble to support Luther's ideas and in 1544 founded the University of Königsberg, the Albertina, as a rival to the Roman Catholic Cracow Academy. It was the second Lutheran university in the German states, after the University of Marburg.
A relief of Albert over the Renaissance-era portal of Königsberg Castle's southern wing was created by Andreas Hess in 1551 according to plans by Christoph Römer. Another relief by an unknown artist was included in the wall of the Albertina's original campus. This depiction, which showed the duke with his sword over his shoulder, was the popular "Albertus", the symbol of the university. The original was moved to Königsberg Public Library to protect it from the elements, while the sculptor Paul Kimritz created a duplicate for the wall. Another version of the "Albertus" by Lothar Sauer was included at the entrance of the Königsberg State and Royal Library.
In 1880 Friedrich Reusch created a sandstone bust of Albert at the Regierungsgebäude, the administrative building for Regierungsbezirk Königsberg. On 19 May 1891 Reusch premiered a famous statue of Albert at Königsberg Castle with the inscription: "Albert of Brandenburg, Last Grand Master, First Duke in Prussia". Albert Wolff also designed an equestrian statue of Albert located at the new campus of the Albertina. King's Gate contains a statue of Albert.
Albert was oft-honored in the quarter Maraunenhof in northern Königsberg. Its main street was named Herzog-Albrecht-Allee in 1906. Its town square, König-Ottokar-Platz, was renamed Herzog-Albrecht-Platz in 1934 to match its church, the Herzog-Albrecht-Gedächtniskirche.
Spouse and issue.
Albert married first, to Princess Dorothea (1 August 1504 – 11 April 1547), daughter of King Frederick I of Denmark, in 1526. They had six children;
He married secondly to Anna Maria (1532–20 March 1568), daughter of Eric I, Duke of Brunswick-Lüneburg, in 1550. The couple had two children;

</doc>
<doc id="1550" url="http://en.wikipedia.org/wiki?curid=1550" title="Agesilaus II">
Agesilaus II

Agesilaus II (; Greek: Ἀγησίλαος "Agesilaos"; c. 444 – c. 360 BC), was a Eurypontid king of the Ancient Greek city-state of Sparta, ruling from approximately 400 BC to 360 BC, during most of which time he was, in Plutarch's words, "as good as thought commander and king of all Greece," and was for the whole of it greatly identified with his country's deeds and fortunes. Small in stature and lame from birth, Agesilaus became ruler somewhat unexpectedly in his mid-forties. His reign saw successful military incursions into various states in Asia Minor, as well as successes in the Corinthian War; although several diplomatic decisions resulted in Sparta becoming increasingly isolated prior to his death at the age of 84 in Cyrenaica.
He was greatly admired by his friend, the historian Xenophon, who wrote a minor work about him titled "Agesilaus".
History.
Early life.
Agesilaus was the son of Archidamus II and his second wife, Eupoleia, brother to Cynisca (the first woman in ancient history to achieve an Olympic victory), and younger half-brother of Agis II.
There is little surviving detail on the youth of Agesilaus. Born with one leg shorter, he was not expected to succeed to the throne after his brother king Agis II, especially because the latter had a son (Leotychidas). Therefore, Agesilaus was trained in the traditional curriculum of Sparta, the "agoge." However, Leotychidas was ultimately set aside as illegitimate (contemporary rumors representing him as the son of Alcibiades) and Agesilaus became king around 401 BC, at the age of about forty. In addition to questions of his nephew's paternity, Agesilaus' succession was largely due to the intervention of his Spartan general, Lysander, who hoped to find in him a willing tool for the furtherance of his political designs. Lysander and the young Agesilaus came to maintain an intimate relation (see Pederasty in Ancient Greece), as was common of the period. Their unique relationship would serve an important role during Agesilaus' later campaigns in Asia Minor.
Early reign.
Agesilaus is first recorded as king during the suppression of the conspiracy of Cinadon, shortly after 398 BC. Then, in 396 BC, Agesilaus crossed into Asia with a force of 2,000 neodamodes (freed helots) and 6,000 allies (including 30 Spartiates) to liberate Greek cities from Persian dominion. On the eve of sailing from Aulis he attempted to offer a sacrifice, as Agamemnon had done before the Trojan expedition, but the Thebans intervened to prevent it, an insult for which he never forgave them. On his arrival at Ephesus a three months' truce was concluded with Tissaphernes, the satrap of Lydia and Caria, but negotiations conducted during that time proved fruitless, and on its termination Agesilaus raided Phrygia, where he easily won immense booty from the satrap Pharnabazus; Tissaphernes could offer no assistance, as he had concentrated his troops in Caria. In these campaigns Agesilaus also benefited from the aid of the Ten Thousand (a mercenary army), who marched through miles of Persian territory to reach the Black Sea. After spending the winter organizing a cavalry force ("hippeis"), he made a successful incursion into Lydia in the spring of 395 BC. Tithraustes was sent to replace Tissaphernes, who paid with his life for his continued failure. An armistice was concluded between Tithraustes and Agesilaus, who left the southern satrapy and again invaded Phrygia, which he ravaged until the following spring. He then came to an agreement with Pharnabazus and once more turned southward.
During these campaigns, Lysander attempted to manipulate Agesilaus into ceding his authority. Agesilaus, the former passive lover of Lysander, would have nothing of this, and reminded Lysander (who was only a Spartan general) who was king. He had Lysander sent away to assist the naval campaigns in the Aegean. This dominating move by Agesilaus earned the respect of his men-at-arms and of Lysander himself, who remained emotionally close with Agesilaus.
In 394 BC, while encamped on the plain of Thebe, he was planning a campaign in the interior, or even an attack on Artaxerxes II himself, when he was recalled to Greece owing to the war between Sparta and the combined forces of Athens, Thebes, Corinth, Argos and several minor states. A rapid march through Thrace and Macedonia brought him to Thessaly, where he repulsed the Thessalian cavalry who tried to impede him. Reinforced by Phocian and Orchomenian troops and a Spartan army, he met the confederate forces at Coronea in Boeotia and in a hotly contested battle was technically victorious. However, the Spartan baggage train was ransacked and Agesilaus himself was injured during the fighting, resulting in a subsequent retreat by way of Delphi to the Peloponnese. Shortly before this battle the Spartan navy, of which he had received the supreme command, was totally defeated off Cnidus by a powerful Persian fleet under Conon and Pharnabazus.
During these conflicts in mainland Greece, Lysander perished while attacking the walls of Thebes. Pausanias, the second king of Sparta (see Spartan Constitution for more information on Sparta's dual monarchy), was supposed to provide Lysander with reinforcements as they marched into Boeotia, yet failed to arrive in time to assist Lysander, likely because Pausanias disliked him for his brash and arrogant attitude towards the Spartan royalty and government. Pausanias failed to fight for the bodies of the dead, and because he retrieved the bodies under truce (a sign of defeat), he was disgraced and banished from Sparta.
In 393 BC, Agesilaus engaged in a ravaging invasion of Argolis. In 392 BC he took a prominent part in the Corinthian War, making several successful expeditions into Corinthian territory and capturing Lechaeum and Piraeus. The loss, however, of a battalion (mora), destroyed by Iphicrates, neutralized these successes, and Agesilaus returned to Sparta. In 389 BC he conducted a campaign in Acarnania, but two years later the Peace of Antalcidas, warmly supported by Agesilaus, put an end to hostilities. In this interval, Agesilaus declined command over Sparta's aggression on Mantineia, and justified Phoebidas' seizure of the Theban Cadmea so long as the outcome provided glory to Sparta.
Decline.
When war broke out afresh with Thebes, Agesilaus twice invaded Boeotia (in 378 BC and 377 BC), although he spent the next five years largely out of action due to an unspecified but apparently grave illness. In the congress of 371 BC an altercation is recorded between him and the Theban general Epaminondas, and due to his influence, Thebes was peremptorily excluded from the peace, and orders given for Cleombrotus to march against Thebes in 371 BC. Cleombrotus was defeated at Leuctra and the Spartan supremacy overthrown.
In 370 BC Agesilaus was engaged in an embassy to Mantineia, and reassured the Spartans with an invasion of Arcadia. He preserved an un-walled Sparta against the revolts and conspiracies of helots, perioeci and even other Spartans; and against external enemies, with four different armies led by Epaminondas penetrating Laconia that same year. Again, in 362 BC, Epaminondas almost succeeded in seizing the city with a rapid and unexpected march. The Battle of Mantinea, in which Agesilaus took no part, was followed by a general peace: Sparta, however, stood aloof, hoping even yet to recover her supremacy. According to Xenophon, Agesilaus, in order to gain money for prosecuting the war, supported the satrap Ariobarzanes II in his revolt against Artaxerxes II in 364 BC (Revolt of the Satraps), and in 361 BC he went to Egypt at the head of a mercenary force to aid the king Nectanebo I and his regent Teos against Persia. He soon transferred his services to Teos's cousin and rival Nectanebo II, who, in return for his help, gave him a sum of over 200 talents. On his way home Agesilaus died in Cyrenaica, around the age of 84, after a reign of some 41 years. His body was embalmed in wax, and buried at Sparta.
Legacy.
Agesilaus was of small stature and unimpressive appearance, and was lame from birth. These facts were used as an argument against his succession, an oracle having warned Sparta against a "lame reign." Most ancient writers considered him a highly successful leader in guerrilla warfare, alert and quick, yet cautious—a man, moreover, whose personal bravery was rarely questioned in his own time. Of his courage, temperance, and hardiness, many instances are cited: and to these were added the less Spartan qualities of kindliness and tenderness as a father and a friend. As examples: there is the story of his riding across a stick (horse made of stick) with his children and upon being discovered by a friend desiring that he not mention till he himself were the father of children; and because of the affection of his son Archidamus' for Cleonymus, he saved Sphodrias, Cleonymus' father, from execution for his incursion into the Piraeus, and dishonorable retreat, in 378 BC. Modern writers tend to be slightly more critical of Agesilaus' reputation and achievements, reckoning him an excellent soldier, but one who had a poor understanding of sea power and siegecraft.
As a statesman he won himself both enthusiastic adherents and bitter enemies. Agesilaus was most successful in the opening and closing periods of his reign: commencing but then surrendering a glorious career in Asia; and in extreme age, maintaining his prostrate country. Other writers acknowledge his extremely high popularity at home, but suggest his occasionally rigid and arguably irrational political loyalties and convictions contributed greatly to Spartan decline, notably his unremitting hatred of Thebes, which led to Sparta's humiliation at the Battle of Leuctra and thus the end of Spartan hegemony.
Other historical accounts paint Agesilaus as a prototype for the ideal leader. His awareness, thoughtfulness, and wisdom were all traits to be emulated diplomatically, while his bravery and shrewdness in battle epitomized the heroic Greek commander. These historians point towards the unstable oligarchies established by Lysander in the former Athenian Empire and the failures of Spartan leaders (such as Pausanias and Kleombrotos) for the eventually suppression of Spartan power. The ancient historian Xenophon was a huge admirer and served under Agesilaus during the campaigns into Asia Minor.
Plutarch includes among his 78 essays and speeches comprising the apophthegmata Agesilaus' letter to the ephors on his recall:
We have reduced most of Asia, driven back the barbarians, made arms abundant in Ionia. But since you bid me, according to the decree, come home, I shall follow my letter, may perhaps be even before it. For my command is not mine, but my country's and her allies'. And a commander then commands truly according to right when he sees his own commander in the laws and ephors, or others holding office in the state.
And when asked whether he wanted a memorial erected in his honor:
If I have done any noble action, that is a sufficient memorial; if I have done nothing noble, all the statues in the world will not preserve my memory.
He lived in the most frugal style alike at home and in the field, and though his campaigns were undertaken largely to secure booty, he was content to enrich the state and his friends and to return as poor as he had set forth.
He was succeeded by his son Archidamus III.
Selected quotes.
When someone was praising an orator for his ability to magnify small points, he said, "In my opinion it's not a good cobbler who fits large shoes on small feet."
Another time he watched a mouse being pulled from its hole by a small boy. When the mouse turned around, bit the hand of its captor and escaped, he pointed this out to those present and said, "When the tiniest creature defends itself like this against aggressors, what ought men to do, do you reckon?"
Certainly when somebody asked what gain the laws of Lycurgus had brought Sparta, he answered, "Contempt for pleasures."
Asked once how far Sparta's boundaries stretched, he brandished his spear and said, "As far as this can reach."
On noticing a house in Asia roofed with square beams, he asked the owner whether timber grew square in that area. When told no, it grew round, he said, "What then? If it were square, would you make it round?"
As he was dying on the voyage back from Egypt, he gave instructions to those close to him that they should not be responsible for making any image of his person, be it modeled or painted or copied, "For if I have accomplished any glorious feat, that will be my memorial. But if I have not, not even all the statues in the world—the products of vulgar, worthless men—would make any difference."
Invited to hear an actor who could perfectly imitate the nightingale, Agesilaus declined, saying he had heard the nightingale itself.

</doc>
<doc id="1566" url="http://en.wikipedia.org/wiki?curid=1566" title="Akkadian Empire">
Akkadian Empire

The Akkadian Empire was an ancient Semitic empire centered in the city of Akkad and its surrounding region, also called Akkad in ancient Mesopotamia. The empire united all the indigenous Akkadian-speaking Semites and the Sumerian speakers under one rule. The Akkadian Empire controlled Mesopotamia, the Levant, and parts of Iran.
During the 3rd millennium BC, there developed a very intimate cultural symbiosis between the Sumerians and the Semitic Akkadians, which included widespread bilingualism. Akkadian gradually replaced Sumerian as a spoken language somewhere between the 3rd and the 2nd millennia BC (the exact dating being a matter of debate).
The Akkadian Empire reached its political peak between the 24th and 22nd centuries BC, following the conquests by its founder Sargon of Akkad (2334–2279 BC). Under Sargon and his successors, Akkadian language was briefly imposed on neighboring conquered states such as Elam. Akkad is sometimes regarded as the first empire in history, though there are earlier Sumerian claimants.
After the fall of the Akkadian Empire, the Akkadian people of Mesopotamia eventually coalesced into two major Akkadian speaking nations: Assyria in the north, and, a few centuries later, Babylonia in the south.
History of research.
Before Akkad was identified in Mesopotamian cuneiform texts, the city was known only from a single reference in Genesis 10:10 where it is written אַכַּד ("Accad"). Today, some 7,000 texts from the Akkadian period alone are known, written in both Sumerian and Akkadian. Many later texts also deal with the Akkadian Empire.
Good understanding of the Akkadian Empire continues to be hampered by the fact that its capital Akkad has not yet been located, despite numerous attempts. Precise dating of archaeological sites is hindered by the fact that there are no clear distinctions between artifact assemblages thought to stem from the preceding Early Dynastic period, and those thought to be Akkadian. Likewise, material that is thought to be Akkadian continues to be in use into the Ur III period.
Many of the more recent insights on the Akkadian Empire have come from excavations in the Upper Khabur area in northeastern Syria. For example, excavations at Tell Mozan (ancient Urkesh) brought to light a sealing of Tar'am-Agade, a previously unknown daughter of Naram-Sin, who was possibly married to an unidentified "endan" (ruler) of Urkesh. The excavators at nearby Tell Leilan (ancient Shekhna/Shubat-Enlil) have used the results from their investigations to argue that the Akkadian Empire came to an end due to abrupt climate change, the so-called 4.2 kiloyear event. The impact of this climate event on Mesopotamia in general, and on the Akkadian Empire in particular, continues to be hotly debated.
Dating and periodization.
The Akkadian period is generally dated to 2350–2170 BC according to the Middle Chronology, or 2230–2050 BC according to the Short Chronology. It was preceded by the Early Dynastic period (ED) and succeeded by the Ur III period, although both transitions are blurry. For example, it is likely that the rise of Sargon of Akkad coincided with the late ED period and that the final Akkadian kings ruled simultaneously with the Gutian kings and rulers at Uruk and Lagash. The Akkadian period is contemporary with EB IV in Palestine, EB IVA and EJ IV in Syria, and EB IIIB in Turkey.
Timeline of rulers.
The relative order of Akkadian kings is clear. The absolute dates of their reigns are, as with all dates prior to the mid-/late-second millennium BC, approximate.
Development of the empire.
Pre-Sargonic Akkad.
The Akkadian Empire takes its name from the region and city of Akkad, both of which were localized in the general confluence area of the Tigris and Diyala Rivers. Although the city of Akkad has not yet been identified on the ground, it is known from various textual sources. Among these is at least one text pre-dating the reign of Sargon. Together with the fact that the name Akkad is of non-Akkadian origin, this suggests that the city of Akkad may have already been occupied in pre-Sargonic times.
Sargon of Akkad.
Sargon of Akkad ("Sharru-kin" = "legitimate king", possibly a title he took on gaining power) defeated and captured Lugal-Zage-Si in the Battle of Uruk and conquered his empire. The earliest records in the Akkadian language date to the time of Sargon. Sargon was claimed to be the son of La'ibum or Itti-Bel, a humble gardener, and possibly a hierodule, or priestess to Ishtar or Inanna. One legend related of Sargon in Assyrian times says that
"My mother was a changeling, my father I knew not. The brothers of my father loved the hills. My city is Azurpiranu (the wilderness herb fields), which is situated on the banks of the Euphrates. My changeling mother conceived me, in secret she bore me. She set me in a basket of rushes, with bitumen she sealed my lid. She cast me into the river which rose not over me. The river bore me up and carried me to Akki, the drawer of water. Akki, the drawer of water, took me as his son and reared me. Akki the drawer of water, appointed me as his gardener. While I was gardener Ishtar granted me her love, and for four and (fifty?) ... years I exercised kingship."
 Later claims on behalf of Sargon, that his mother was an "entu" priestess (high priestess). The claims might have been made to ensure a descendancy of nobility, considering only a high placed family can be made such a position.
Originally a cupbearer ("Rabshaqe") to a king of Kish with a Semitic name, Ur-Zababa, Sargon thus became a gardener, responsible for the task of clearing out irrigation canals. This gave him access to a disciplined corps of workers, who also may have served as his first soldiers. Displacing Ur-Zababa, Sargon was crowned king, and he entered upon a career of foreign conquest. Four times he invaded Syria and Canaan, and he spent three years thoroughly subduing the countries of "the west" to unite them with Mesopotamia "into a single empire."
However, Sargon took this process further, conquering many of the surrounding regions to create an empire that reached westward as far as the Mediterranean Sea and perhaps Cyprus ("Kaptara"); northward as far as the mountains (a later Hittite text asserts he fought the Hattian king Nurdaggal of Burushanda, well into Anatolia); eastward over Elam; and as far south as Magan (Oman) — a region over which he reigned for purportedly 56 years, though only four "year-names" survive. He consolidated his dominion over his territories by replacing the earlier opposing rulers with noble citizens of Akkad, his native city where loyalty would thus be ensured.
Trade extended from the silver mines of Anatolia to the lapis lazuli mines in Afghanistan, the cedars of Lebanon and the copper of Magan. This consolidation of the city-states of Sumer and Akkad reflected the growing economic and political power of Mesopotamia. The empire's breadbasket was the rain-fed agricultural system of northern Mesopotamia (Assyria) and a chain of fortresses was built to control the imperial wheat production.
Images of Sargon were erected on the shores of the Mediterranean, in token of his victories, and cities and palaces were built at home with the spoils of the conquered lands. Elam and the northern part of Mesopotamia (Assyria/Subartu) were also subjugated, and rebellions in Sumer were put down. Contract tablets have been found dated in the years of the campaigns against Canaan and against Sarlak, king of Gutium. He also boasted of having subjugated the "four quarters" — the lands surrounding Akkad to the north (Assyria), the south (Sumer), the east (Elam) and the west (Martu). Some of the earliest historiographic texts (ABC 19, 20) suggest he rebuilt the city of Babylon ("Bab-ilu") in its new location near Akkad.
Sargon, throughout his long life, showed special deference to the Sumerian deities, particularly Inanna (Ishtar), his patroness, and Zababa, the warrior god of Kish. He called himself "The anointed priest of Anu" and "the great" ensi "of Enlil" and his daughter, Enheduanna, was installed as priestess to Nanna at the temple in Ur.
Troubles multiplied toward the end of his reign. A later Babylonian text states;
"In his old age, all the lands revolted against him, and they besieged him in Akkad (the city)"...but "he went forth to battle and defeated them, he knocked them over and destroyed their vast army".
 It refers to his campaign in "Elam", where he defeated a coalition army led by the King of Awan, where he forced the vanquished to become his vassals.
Also shortly after, another revolt had been made;"the Subartu (mountainous tribes of Assyria) the upper country—in their turn attacked, but they submitted to his arms, and Sargon settled their habitations, and he smote them grievously".
Rimush and Manishtushu.
Sargon had crushed opposition even at old age. These difficulties broke out again in the reign of his sons, where revolts broke out during the 9-year reign, Rimush (2278–2270 BC), who fought hard to retain the empire, and was successful until he was assassinated by some of his own courtiers. Rimush's elder brother, Manishtushu (2269–2255 BC) succeeded and reigned for a period of 15 years. The latter king seems to have fought a sea battle against 32 kings who had gathered against him and took control over their country of what is today the United Arab Emirates and Oman. Despite the success, similarly to his brother, he seems to have been assassinated in a palace conspiracy.
Naram-Sin.
Manishtushu's son and successor, Naram-Sin (2254–2218 BC) (Beloved of Sin), due to vast military conquests, assumed the imperial title "King Naram-Sin, king of the four quarters" ("Lugal Naram-Sîn, Šar kibrat 'arbaim"), "the four quarters as a reference to the entire world. He was also for the first time in Sumerian culture, addressed as "the god (Sumerian = DINGIR, Akkadian = "ilu") of Agade" (Akkad), in opposition to the previous religious belief that kings were only representatives of the people towards the gods.
He also faced revolts at the start of his reign, but quickly crushed them.
Naram-Sin also recorded the Akkadian conquest of Ebla as well as Armanum and its king. Armanum location is debated; it is sometimes identified with a Syrian kingdom mentioned in the tablets of Ebla as Armi, the location of Armi is also debated, historian Adelheid Otto identify it with the Citadel of Bazi – Tall Banat complex on the Euphrates River between Ebla and Tell Brak, others like Wayne Horowitz identify it with Aleppo, and while most scholars place Armanum in Syria, Michael C. Astour believes it to be located north of the Hamrin Mountains in northern Iraq.
To better police Syria, he built a royal residence at Tell Brak, a crossroads at the heart of the Khabur River basin of the Jezirah. Naram-Sin campaigned against Magan which also revolted; Naram-Sin, "marched against Magan and personally caught Mandannu, its king", where he instated garrisons to protect the main roads. The chief threat seemed to be coming from the northern Zagros Mountains, the Lulubis and the Gutians. A campaign against the Lullubi led to the carving of the famous "Victory Stele of Naram-Suen", now in the Louvre. Hittite sources claim Naram-Sin of Akkad even ventured into Anatolia, battling the Hittite and Hurrian kings Pamba of Hatti, Zipani of Kanesh, and 15 others.
This newfound Akkadian wealth may have been based upon benign climatic conditions, huge agricultural surpluses and the confiscation of the wealth of other peoples.
The economy was highly planned. Grain was cleaned, and rations of grain and oil were distributed in standardized vessels made by the city's potters. Taxes were paid in produce and labour on public walls, including city walls, temples, irrigation canals and waterways, producing huge agricultural surpluses.
In later Assyrian and Babylonian texts, the name "Akkad", together with "Sumer", appears as part of the royal title, as in the Sumerian LUGAL KI-EN-GI KI-URI or Akkadian "Šar māt Šumeri u Akkadi", translating to "king of Sumer and Akkad". This title was assumed by the king who seized control of Nippur, the intellectual and religious center of southern Mesopotamia.
During the Akkadian period, the Akkadian language became the lingua franca of the Middle East, and was officially used for administration, although the Sumerian language remained as a spoken and literary language. The spread of Akkadian stretched from Syria to Elam, and even the Elamite language was temporarily written in Mesopotamian cuneiform. Akkadian texts later found their way to far-off places, from Egypt (in the Amarna Period) and Anatolia, to Persia (Behistun).
Collapse of the Akkadian Empire.
The Empire of Akkad collapsed in 2154 BCE, within 180 years of its founding, ushering in a Dark Age period of regional decline that lasted until the rise of the Third Dynasty of Ur in 2112 BC. By the end of the reign of Naram-Sin's son, Shar-kali-sharri (2217–2193 BC), the empire had weakened. There was a period of anarchy between 2192 BC and 2168 BC. Shu-Durul (2168–2154 BC) appears to have restored some centralized authority, however he was unable to prevent the empire eventually collapsing outright from the invasion of barbarian peoples from the Zagros Mountains known as the Gutians.
Little is known about the Gutian period, or how long it endured. Cuneiform sources suggest that the Gutians' administration showed little concern for maintaining agriculture, written records, or public safety; they reputedly released all farm animals to roam about Mesopotamia freely, and soon brought about famine and rocketing grain prices. The decline coincided with severe drought, possibly connected with climatic changes reaching all across the area from Egypt to Greece. The Sumerian king Ur-Nammu (2112–2095 BC) cleared the Gutians from Mesopotamia during his reign.
It has recently been suggested that the regional decline at the end of the Akkadian period (and of the First Intermediary Period that followed the Ancient Egyptian Old Kingdom) was associated with rapidly increasing aridity, and failing rainfall in the region of the Ancient Near East, caused by a global centennial-scale drought. H. Weiss et al. have shown "Archaeological and soil-stratigraphic data define the origin, growth, and collapse of Subir, the third millennium rain-fed agriculture civilization of northern Mesopotamia on the Habur Plains of Syria. At 2200 B. C., a marked increase in aridity and wind circulation, subsequent to a volcanic eruption, induced a considerable degradation of land-use conditions. After four centuries of urban life, this abrupt climatic change evidently caused abandonment of Tell Leilan, regional desertion, and collapse of the Akkadian empire based in southern Mesopotamia. Synchronous collapse in adjacent regions suggests that the impact of the abrupt climatic change was extensive.". Peter B. deMenocal, has shown "there was an influence of the North Atlantic Oscillation on the stream flow of the Tigris and Euphrates at this time, which led to the collapse of the Akkadian Empire".
The Sumerian King List, describing the Akkadian Empire after the death of Shar-kali-shari, states:
"Who was king? Who was not king? Irgigi the king; Nanum, the king; Imi the king; Ilulu, the king—the four of them were kings but reigned only three years. Dudu reigned 21 years; Shu-Turul, the son of Dudu, reigned 15 years. ... Agade was defeated and its kingship carried off to Uruk. In Uruk, Ur-ningin reigned 7 years, Ur-gigir, son of Ur-ningin, reigned 6 years; Kuda reigned 6 years; Puzur-ili reigned 5 years, Ur-Utu reigned 6 years. Uruk was smitten with weapons and its kingship carried off by the Gutian hordes.
However, there are no known year-names or other archaeological evidence verifying any of these later kings of Akkad or Uruk, apart from a single artifact referencing king Dudu of Akkad. The named kings of Uruk may have been contemporaries of the last kings of Akkad, but in any event could not have been very prominent.
In the Gutian hordes, (first reigned) a nameless king; (then) Imta reigned 3 years as king; Shulme reigned 6 years; Elulumesh reigned 6 years; Inimbakesh reigned 5 years; Igeshuash reigned 6 years; Iarlagab reigned 15 years; Ibate reigned 3 years; ... reigned 3 years; Kurum rained 1 year; ... reigned 3 years; ... reigned 2 years; Iararum reigned 2 years; Ibranum reigned 1 year; Hablum reigned 2 years; Puzur-Sin son of Hablum reigned 7 years; Iarlaganda reigned 7 years; ... reigned 7 years; ... reigned 40 days. Total 21 kings reigned 91 years, 40 days.
Evidence from Tell Leilan in Northern Mesopotamia shows what may have happened. The site was abandoned soon after the city's massive walls were constructed, its temple rebuilt and its grain production reorganised. The debris, dust and sand that followed show no trace of human activity. Soil samples show fine wind-blown sand, no trace of earthworm activity, reduced rainfall and indications of a drier and windier climate. Evidence shows that skeleton-thin sheep and cattle died of drought, and up to 28,000 people abandoned the site, seeking wetter areas elsewhere. Tell Brak shrank in size by 75%. Trade collapsed. Nomadic herders such as the Amorites moved herds closer to reliable water suppliers, bringing them into conflict with Akkadian populations. This climate-induced collapse seems to have affected the whole of the Middle East, and to have coincided with the collapse of the Egyptian Old Kingdom.
This collapse of rain-fed agriculture in the Upper Country meant the loss to southern Mesopotamia of the agrarian subsidies which had kept the Akkadian Empire solvent. Water levels within the Tigris and Euphrates fell 1.5 metres beneath the level of 2600 BC, and although they stabilised for a time during the following Ur III period, rivalries between pastoralists and farmers increased. Attempts were undertaken to prevent the former from herding their flocks in agricultural lands, such as the building of a 180 km wall known as the "Repeller of the Amorites" between the Tigris and Euphrates under the Ur III ruler Shu-Sin. Such attempts led to increased political instability; meanwhile, severe depression occurred to re-establish demographic equilibrium with the less favorable climatic conditions.
The period between ca. 2112 BC and 2004 BC is known as the Ur III period. Documents again began to be written in Sumerian, although Sumerian was becoming a purely literary or liturgical language, much as Latin later would be in Medieval Europe.
Government.
The Akkadian government formed a "classical standard" with which all future Mesopotamian states compared themselves. Traditionally, the "ensi" was the highest functionary of the Sumerian city-states. In later traditions, one became an "ensi" by marrying the goddess Inanna, legitimising the rulership through divine consent.
Initially, the monarchical "lugal" ("lu" = man, "gal" = great) was subordinate to the priestly "ensi", and was appointed at times of troubles, but by later dynastic times, it was the "lugal" who had emerged as the preeminent role, having his own "é" (= house) or "palace", independent from the temple establishment. By the time of Mesalim, whichever dynasty controlled the city of Kish was recognised as "šar kiššati" (= king of Kish), and was considered preeminent in Sumer, possibly because this was where the two rivers approached, and whoever controlled Kish ultimately controlled the irrigation systems of the other cities downstream.
As Sargon extended his conquest from the "Lower Sea" (Persian Gulf), to the "Upper Sea" (Mediterranean), it was felt that he ruled "the totality of the lands under heaven", or "from sunrise to sunset", as contemporary texts put it. Under Sargon, the "ensi"s generally retained their positions, but were seen more as provincial governors. The title "šar kiššati" became recognised as meaning "lord of the universe". Sargon is even recorded as having organised naval expeditions to Dilmun (Bahrain) and Magan, amongst the first organised military naval expeditions in history. Whether he also did in the case of the Mediterranean with the kingdom of Kaptara (possibly Cyprus), as claimed in later documents, is more questionable.
With Naram-Sin, Sargon's grandson, this went further than with Sargon, with the king not only being called "Lord of the Four Quarters (of the Earth)", but also elevated to the ranks of the "dingir" (= gods), with his own temple establishment. Previously a ruler could, like Gilgamesh, become divine after death but the Akkadian kings, from Naram-Sin onward, were considered gods on earth in their lifetimes. Their portraits showed them of larger size than mere mortals and at some distance from their retainers.
One strategy adopted by both Sargon and Naram-Sin, to maintain control of the country, was to install their daughters, Enheduanna and Emmenanna respectively, as high priestess to Sin, the Akkadian version of the Sumerian moon deity, Nanna, at Ur, in the extreme south of Sumer; to install sons as provincial "ensi" governors in strategic locations; and to marry their daughters to rulers of peripheral parts of the Empire (Urkesh and Marhashe). A well documented case of the latter is that of Naram-Sin's daughter Tar'am-Agade at Urkesh.
Economy.
The population of Akkad, like nearly all pre-modern states, was entirely dependent upon the agricultural systems of the region, which seem to have had two principal centres: the irrigated farmlands of southern Iraq that traditionally had a yield of 30 grains returned for each grain sown and the rain-fed agriculture of northern Iraq, known as the "Upper Country."
Southern Iraq during Akkadian period seems to have been approaching its modern rainfall level of less than 20 mm per year, with the result that agriculture was totally dependent upon irrigation. Before the Akkadian period the progressive salinisation of the soils, produced by poorly drained irrigation, had been reducing yields of wheat in the southern part of the country, leading to the conversion to more salt-tolerant barley growing. Urban populations there had peaked already by 2,600 BC, and illogical pressures were high, contributing to the rise of militarism apparent immediately before the Akkadian period (as seen in the Stele of the Vultures of Eannatum). Warfare between city states had led to a population decline, from which Akkad provided a temporary respite. It was this high degree of agricultural productivity in the south that enabled the growth of the highest population densities in the world at this time, giving Akkad its military advantage.
The water table in this region was very high and replenished regularly—by winter storms in the headwaters of the Tigris and Euphrates from October to March and from snow-melt from March to July. Flood levels, that had been stable from about 3,000 to 2,600 BC, had started falling, and by the Akkadian period were a half-meter to a meter lower than recorded previously. Even so, the flat country and weather uncertainties made flooding much more unpredictable than in the case of the Nile; serious deluges seem to have been a regular occurrence, requiring constant maintenance of irrigation ditches and drainage systems. Farmers were recruited into regiments for this work from August to October—a period of food shortage—under the control of city temple authorities, thus acting as a form of unemployment relief. Some have suggested that this was Sargon's original employment for the king of Kish, giving him experience in effectively organising large groups of men; a tablet reads, "Sargon, the king, to whom Enlil permitted no rival—5,400 warriors ate bread daily before him".
Harvest was in the late spring and during the dry summer months. Nomadic Amorites from the northwest would pasture their flocks of sheep and goats to graze on the stubble and be watered from the river and irrigation canals. For this privilege, they would have to pay a tax in wool, meat, milk, and cheese to the temples, who would distribute these products to the bureaucracy and priesthood. In good years, all would go well, but in bad years, wild winter pastures would be in short supply, nomads would seek to pasture their flocks in the grain fields, and conflicts with farmers would result. It would appear that the subsidizing of southern populations by the import of wheat from the north of the Empire temporarily overcame this problem, and it seems to have allowed economic recovery and a growing population within this region.
As a result, Sumer and Akkad had a surplus of agricultural products but was short of almost everything else, particularly metal ores, timber and building stone, all of which had to be imported. The spread of the Akkadian state as far as the "silver mountain" (possibly the Taurus Mountains), the "cedars" of Lebanon, and the copper deposits of Magan, was largely motivated by the goal of securing control over these imports. One tablet reads "Sargon, the king of Kish, triumphed in thirty-four battles (over the cities) up to the edge of the sea (and) destroyed their walls. He made the ships from Meluhha, the ships from Magan (and) the ships from Dilmun tie up alongside the quay of Agade. Sargon the king prostrated himself before (the god) Dagan (and) made supplication to him; (and) he (Dagan) gave him the upper land, namely Mari, Yarmuti, (and) Ebla, up to the Cedar Forest (and) up to the Silver Mountain".
Culture.
Language.
During the 3rd millennium BC, there developed a very intimate cultural symbiosis between the Sumerians and the Akkadians, which included widespread bilingualism. The influence of Sumerian on Akkadian (and vice versa) is evident in all areas, from lexical borrowing on a massive scale, to syntactic, morphological, and phonological convergence. This has prompted scholars to refer to Sumerian and Akkadian in the third millennium as a "sprachbund". Akkadian gradually replaced Sumerian as a spoken language somewhere around the turn of the 3rd and the 2nd millennium BC (the exact dating being a matter of debate), but Sumerian continued to be used as a sacred, ceremonial, literary and scientific language in Mesopotamia until the 1st century AD.
Poet–priestess Enheduanna.
Sumerian literature continued in rich development during the Akkadian period (a notable example being Enheduanna). Enheduanna, the "wife (Sumerian "dam" = high priestess) of Nanna [the Sumerian moon god] and daughter of Sargon" of the temple of Sin at Ur, who lived ca. 2285–2250 BC, is the first poet in history whom we know by name. Her known works include hymns to the goddess Inanna, the "Exaltation of Inanna" and "In-nin sa-gur-ra". A third work, the "Temple Hymns", a collection of specific hymns, addresses the sacred temples and their occupants, the deity to whom they were consecrated. The works of this poetess are significant, because although they start out using the third person, they shift to the first person voice of the poet herself, and they mark a significant development in the use of cuneiform. As poetess, princess, and priestess, she was a personality who, according to William W Hallo, "set standards in all three of her roles for many succeeding centuries"
In the "Exultation of Inanna",
The Curse.
Later material described how the fall of Akkad was due to Naram-Sin's attack upon the city of Nippur. When prompted by a pair of inauspicious oracles, the king sacked the E-kur temple, supposedly protected by the god Enlil, head of the pantheon. As a result of this, eight chief deities of the Anunnaki pantheon were supposed to have come together and withdrawn their support from Akkad.
For many years, the events described in "The Curse of Akkad" were thought, like the details of Sargon's birth, to be purely fictional. But now the evidence of Tell Leilan, and recent findings of elevated dust deposits in sea-cores collected off Oman, that date to the period of Akkad's collapse suggest that this climate change may have played a role.
Technology.
One tablet from this period reads, "(From the earliest days) no-one had made a statue of lead, (but) Rimush king of Kish, had a statue of himself made of lead. It stood before Enlil; and it recited his (Rimush's) virtues to the idu of the gods". The copper Bassetki Statue, cast with the lost wax method, testifies to the high level of skill that craftsmen achieved during the Akkadian period.
Achievements.
The empire was bound together by roads, along which there was a regular postal service. Clay seals that took the place of stamps bear the names of Sargon and his son. A cadastral survey seems also to have been instituted, and one of the documents relating to it states that a certain Uru-Malik, whose name appears to indicate his Canaanite origin, was governor of the land of the Amorites, or "Amurru" as the semi-nomadic people of Syria and Canaan were called in Akkadian. It is probable that the first collection of astronomical observations and terrestrial omens was made for a library established by Sargon. The earliest "year names", whereby each year of a king's reign was named after a significant event performed by that king, date from the reign of Sargon the Great. Lists of these "year names" henceforth became a calendrical system used in most independent Mesopotamian city-states. In Assyria, however, years came to be named for the annual presiding "limmu" official appointed by the king, rather than for an event.

</doc>
<doc id="1686" url="http://en.wikipedia.org/wiki?curid=1686" title="Alfonso V of Aragon">
Alfonso V of Aragon

Alfonso the Magnanimous KG (also Alphonso; Catalan: Alfons; 1396 – 27 June 1458) was the King of Aragon (as Alfonso V), Valencia (as Alfonso III), Majorca, Sardinia and Corsica (as Alfonso II), Sicily (as Alfonso I) and Count of Barcelona (as Alfonso IV) from 1416, and King of Naples (as Alfonso I) from 1442 until his death. He was one of the most prominent figures of the early Renaissance and a knight of the Order of the Dragon.
Biography.
Born at Medina del Campo, he was the son of Ferdinand I of Aragon and Eleanor of Alburquerque. He represented the old line of the counts of Barcelona through the female line, and was on his father's side descended from the House of Trastamara, the reigning House of Castile. By hereditary right he was king of Sicily and claimed the island of Sardinia for himself, though it was then in the possession of Genoa. Alfonso was also in possession of much of Corsica by the 1420s.
In 1421 the childless Queen Joan II of Naples adopted and named him as heir to the Kingdom of Naples, and Alfonso went to Naples. Here he hired the famous condottiero Braccio da Montone with the task of reducing the resistance of his rival claimant, Louis III of Anjou, and his forces led by Muzio Attendolo Sforza. Pope Martin V supporting Sforza, Alfonso switched his religious allegiance to the Aragonese antipope Benedict XIII. When Sforza abandoned Louis' cause, Alfonso seemed to have all his problems solved; however, his relationship with Joan suddenly worsened, and in May 1423 he had her lover, Gianni Caracciolo, a powerful figure in the Neapolitan court, arrested.
After an attempt to arrest the queen herself had failed, Joan called on Sforza who defeated the Aragonese militias near Castel Capuano in Naples. Alfonso fled to Castel Nuovo, but the help of a fleet of 22 galleys led by Giovanni da Cardona improved his situation. Sforza and Joan ransomed Caracciolo and retreated to the fortress of Aversa. Here she repudiated her earlier adoption of Alfonso and, with the backing of Martin V, named Louis III as her heir instead.
The Duke of Milan, Filippo Maria Visconti, joined the anti-Aragonese coalition. Alfonso requested support from Braccio da Montone, who was besieging Joan's troops in L'Aquila, but had to set sail for Spain, where a war had broken out between his brothers and the Kingdom of Castile. On his way towards Barcelona, Alfonso destroyed Marseille, a possession of Louis III.
In late 1423 the Genoese fleet of Filippo Maria Visconti moved in the southern Tyrrhenian Sea, rapidly conquering Gaeta, Procida, Castellammare and Sorrento. Naples, which was held by Alfonso's brother, Pedro de Aragon, was besieged in 1424 by the Genoese ships and Joan's troops, now led by Francesco Sforza, the son of Muzio Sforza (who had met his death at L'Aquila). The city fell in April 1424. Pedro, after a short resistance in Castel Nuovo, fled to Sicily in August. Joan II and Louis III again took possession of the realm, although the true power was in the hands of Gianni Caracciolo.
An opportunity for Alfonso to reconquer Naples occurred in 1432, when Caracciolo was killed in a conspiracy. Alfonso tried to regain the favour of the queen, but failed, and had to wait for the death of both Louis (at Cosenza in 1434) and Joan herself (February 1435). In her will, she bequeathed her realm to René of Anjou, Louis III's younger brother. This solution was opposed by the new pope, Eugene IV, who nominally was the feudal lord of the King of Naples. The Neapolitans having called in the French, Alfonso decided to intervene and, with the support of several barons of the kingdom, captured Capua and besieged the important sea fortress of Gaeta. His fleet of 25 galleys was met by the Genoese ships sent by Visconti, led by Biagio Assereto. In the battle that ensued, Alfonso was defeated and taken prisoner.
In Milan, however, he impressed his captor with his cultured demeanor and persuaded him to let him go by making it plain that it was not in Milan's interest to prevent the victory of the Aragonese party in Naples. Helped by a Sicilian fleet, Alfonso recaptured Capua and set his base in Gaeta in February 1436. Meanwhile, papal troops had invaded the Neapolitan kingdom, but Alfonso bribed their commander, Cardinal Giovanni Vitelleschi, and their successes waned.
In the meantime, René had managed to reach Naples on 19 May 1438. Alfonso tried to besiege the city in the following September, but failed. His brother Pedro was killed during the battle. Castel Nuovo, where an Aragonese garrison resisted, fell to the Angevine mercenaries in August 1439. After the death of his condottiero Jacopo Caldora, however, René's fortune started to decline: Alfonso could easily capture Aversa, Salerno, Benevento, Manfredonia and Bitonto. René, whose possession included now only part of the Abruzzi and Naples, obtained 10,000 men from the pope, but the cardinal leading them signed a truce with Alfonso. Giovanni Sforza came with a reduced corps, as troops sent by Eugene IV had halted his father Francesco in the Marche.
Alfonso, provided with the most impressive artillery of the times, again besieged Naples. The siege began on 10 November 1441, ending on 2 June the following year. After the return of René to Provence, Alfonso easily reduced the remaining resistance and made his triumphal entrance in Naples on 26 February 1443, as the monarch of a pacified kingdom. In 1446 he also conquered Sardinia.
Alfonso, by formally submitting his reign to the Papacy, obtained the consent of Pope Eugene IV that the Kingdom of Naples would go to his illegitimate son Ferdinand. He died in Castel dell'Ovo in 1458, while he was planning the conquest of Genoa. At the time, Alfonso was at odds with Callixtus III, who died shortly afterwards.
His Spanish possessions were ruled for him by his brother John, later king John II of Aragon. Sicily and Sardinia were also inherited by his brother.
Alfonso was also a powerful and faithful supporter of Skanderbeg, whom he decided to take under his protection as a vassal in 1451, shortly after the latter had scored his second victory against Murad II. In addition to financial assistance, he supplied the Albanian leader with troops, military equipment, and sanctuary for himself and his family if such a need should arise. This was because in 1448, while Skanderbeg was victoriously fighting off the Turkish invasions, three military columns, commanded by Demetrio Reres along with his sons Giorgio and Basilio, had been dispatched to help Alfonso V defeat the barons of Naples who had rebelled against him.
Family.
Alfonso had been betrothed to Maria of Castile (1401–1458; sister of John II of Castile) in Valladolid in 1408; the marriage was celebrated in Valencia on 12 June 1415. They failed to produce children. Alfonso had been in love with a woman of noble family named Lucrezia d'Alagno, who served as a "de facto" queen at the Neapolitan court as well as an inspiring muse.
Genealogical records in the Old Occitan Chronicle of Montpellier in "Le petit Thalamus de Montpellier" indicate that Alphonso's relationship with his mistress, Giraldona Carlino, produced three children:
Giraldona was the daughter of Enrique Carlino and his wife, Isabel.
Art and administration.
Like many Renaissance rulers, Alfonso V was a patron of the arts. He founded the Academy of Naples under Giovanni Pontano, and for his entrance into the city in 1443 had a magnificent triumphal arch added to the main gate of Castel Nuovo. This edifice, considered the most important civil piece of art of the time, was designed by Francesco Laurana.
Alfonso was particularly attracted to classical literature. He reportedly brought copies of the works of Livy and Julius Caesar on his campaigns; the poet Antonio Beccadelli even claimed that Alfonso was cured of a disease by the reading of a few pages from Quintus Curtius Rufus' history of Alexander the Great. Although this reputed erudition attracted scholars to his court, Alfonso apparently enjoyed pitting them against each other in spectacles of bawdy Latin rhetoric.
After his conquest of Naples in 1442, Alfonso ruled primarily through his mercenaries and political lackeys. In his Italian kingdom, he maintained the former political and administrative institutions. His holdings in Spain were governed by his brother John.
A unified General Chancellorship for the whole Aragonese realm was set up in Naples, although the main functionaries were of Aragonese nationality. Apart from financial, administrative and artistic improvements, his other accomplishments in the Sicilian kingdom include the restoration of the aqueducts, the drainage of marshy areas, and the paving of streets.
Connection with Ethiopia.
Alfonso was the object of diplomatic contacts from the empire of Ethiopia. In 1428, he received a letter from Yeshaq I of Ethiopia, borne by two dignitaries, which proposed an alliance against the Muslims and would be sealed by a dual marriage that would require the Infante Peter to bring a group of artisans to Ethiopia where he would marry Yeshaq's daughter.
In return, Alfonso sent a party of 13 craftsmen, all of whom perished on the way to Ethiopia. He later sent a letter to Yeshaq's successor Zara Yaqob in 1450, in which he wrote that he would be happy to send artisans to Ethiopia if their safe arrival could be guaranteed, but it probably never reached the Emperor.

</doc>
<doc id="1724" url="http://en.wikipedia.org/wiki?curid=1724" title="Ammonius Saccas">
Ammonius Saccas

Ammonius Saccas (; Greek: Ἀμμώνιος Σακκᾶς; fl. 3rd century AD) was a Greek philosopher from Alexandria who was often referred to as one of the founders of Neoplatonism. He is mainly known as the teacher of Plotinus, whom he taught for eleven years from 232 to 243. He was undoubtedly the biggest influence on Plotinus in his development of Neoplatonism, although little is known about his own philosophical views. Later Christian writers stated that Ammonius was a Christian, but it is now generally assumed that there was a different Ammonius of Alexandria who wrote biblical texts.
Life.
Not much is known about the life of Ammonius Saccas. His cognomen "Sakkas" has been interpreted to indicate that he was a porter in his youth. 
However Erich Seeberg argued that the cognomen refers to the "Śākyas" of India, the ruling clan to which Gautama Buddha also belonged. The "Śākyas" were known in antiquity. The cognomen "Sakkas" therefore referred to India as a marker of ethnic identity. This is, according to this interpretation, supported by the fact that Ammianus Marcellinus refers to him as "Saccas Ammonius", thus as the "Sacian Ammonius", which makes any reading as denoting "sakkos" impossible. This interpretation of the name, which has subsequently been contested, would corroborate Porphyry's report that Plotinus, Ammonius' foremost student, acquired his high esteem for Indian philosophy and his eager desire to travel to India from Ammonius. The interpretation that "Saccas" denotes ethnic northern Indian origin, rather than alluding to Gautama Buddha, supports the possibility that Ammonius may have been raised a Christian, who reverted to paganism, as reported by Eusebius, drawing on Porphyry's Contra Christianos. In this case Ammonius may have been a second-generation Indian who remained in contact with the philosophy of his ancestral country. The intensity of commerce of goods and ideas between Alexandria and India makes this a wholly possible option. A misreading of "Sakkas" for "sakkophoros", being grammatically incorrect, is thus nothing but cheap polemics that stuck. The link to India however explains not only Plotinus' passion for India, but also the often noted substantial agreements and shared ideas between Vedanta and Neoplatonism which are increasingly attributed to direct Indian influence.
Most details of his life come from the fragments left from Porphyry's writings. The most famous pupil of Ammonius Saccas was Plotinus who studied under Ammonius for eleven years. According to Porphyry, in 232, at the age of 28, Plotinus went to Alexandria to study philosophy:
In his twenty-eighth year he [Plotinus] felt the impulse to study philosophy and was recommended to the teachers in Alexandria who then had the highest reputation; but he came away from their lectures so depressed and full of sadness that he told his trouble to one of his friends. The friend, understanding the desire of his heart, sent him to Ammonius, whom he had not so far tried. He went and heard him, and said to his friend, "This is the man I was looking for." From that day he stayed continually with Ammonius and acquired so complete a training in philosophy that he became eager to make acquaintance with the Persian philosophical discipline and that prevailing among the Indians.
According to Porphyry, the parents of Ammonius were Christians, but upon learning Greek philosophy, Ammonius rejected his parents' religion for paganism. This conversion is contested by the Christian writers Jerome and Eusebius, who state that Ammonius remained a Christian throughout his lifetime:
[Porphyry] plainly utters a falsehood (for what will not an opposer of Christians do?) when he says that ... Ammonius fell from a life of piety into heathen customs. ... Ammonius held the divine philosophy unshaken and unadulterated to the end of his life. His works yet extant show this, as he is celebrated among many for the writings which he has left.
However we are told by Longinus that Ammonius wrote nothing, and if Ammonius was the principal influence on Plotinus, then it is unlikely that Ammonius would have been a Christian. One way to explain much of the confusion concerning Ammonius is to assume that there were two people called Ammonius: Ammonius Saccas who taught Plotinus, and an Ammonius the Christian who wrote biblical texts. Another explanation might be that there was only one Ammonius but that Origen, who found the Neo-Platonist views of his teacher essential to his own beliefs about the essential nature of Christianity, chose to suppress Ammonius' choice of Paganism over Christianity. The insistence of Eusebius, Origen's pupil, and Jerome, all of whom were recognized Fathers of the Christian Church, that Ammonius Saccas had not rejected his Christian roots would be easier for Christians to accept than the assertion of Prophyry, who was a Pagan, that Ammonius had chosen Paganism over Christianity.
To add to the confusion, it seems that Ammonius had two pupils called Origen: Origen the Christian, and Origen the Pagan. It is quite possible that Ammonius Saccas taught both Origens. And since there were two Origens who were accepted as contemporaries it was easy for later Christians to accept that there were two individuals named Ammonius, one a Christian and one a Pagan. Among Ammonius' other pupils there were Herennius and Cassius Longinus.
Philosophy.
Hierocles, writing in the 5th century, states that Ammonius' fundamental doctrine was that Plato and Aristotle were in full agreement with each other:
He was the first who had a godly zeal for the truth in philosophy and despised the views of the majority, which were a disgrace to philosophy. He apprehended well the views of each of the two philosophers [Plato and Aristotle] and brought them under one and the same "nous" and transmitted philosophy without conflicts to all of his disciples, and especially to the best of those acquainted with him, Plotinus, Origen, and their successors.
According to Nemesius, a bishop and Neoplatonist c. 400, Ammonius held that the soul was immaterial.
Little is known about Ammonius's role in the development of Neoplatonism. Porphyry seems to suggest that Ammonius was instrumental in helping Plotinus think about philosophy in new ways:
But he [Plotinus] did not just speak straight out of these books but took a distinctive personal line in his consideration, and brought the mind of Ammonius' to bear on the investigation in hand.
Two of Ammonius's students - Origen the Pagan, and Longinus - seem to have held philosophical positions which were closer to Middle Platonism than Neoplatonism, which perhaps suggests that Ammonius's doctrines were also closer to those of Middle Platonism than the Neoplatonism developed by Plotinus (see the Enneads), but Plotinus does not seem to have thought that he was departing in any significant way from that of his master.

</doc>
<doc id="1737" url="http://en.wikipedia.org/wiki?curid=1737" title="Anaxagoras">
Anaxagoras

Anaxagoras (; Greek: Ἀναξαγόρας, "Anaxagoras", "lord of the assembly"; c. 510 – 428 BC) was a Pre-Socratic Greek philosopher. Born in Clazomenae in Asia Minor, Anaxagoras was the first philosopher to bring philosophy to Athens. According to Diogenes Laertius and Plutarch, in later life he was charged with impiety and went into exile in Lampsacus; the charges may have been political, owing to his association with Pericles.
Responding to the claims of Parmenides on the impossibility of change, Anaxagoras described the world as a mixture of primary imperishable ingredients, where material variation was never caused by an absolute presence of a particular ingredient, but rather by its relative preponderance over the other ingredients; in his words, "each one is... most manifestly those things of which there are the most in it". He introduced the concept of "Nous" (Mind) as an ordering force, which moved and separated out the original mixture, which was homogeneous, or nearly so.
He also gave a number of novel scientific accounts of natural phenomena. He produced a correct explanation for eclipses and described the sun as a fiery mass larger than the Peloponnese, as well as attempting to explain rainbows and meteors.
Biography.
Anaxagoras appears to have had some amount of property and prospects of political influence in his native town of Clazomenae in Asia Minor. However, he supposedly surrendered both of these out of a fear that they would hinder his search for knowledge. The Roman author Valerius Maximus preserves a different tradition: Anaxagoras, coming home from a long voyage, found his property in ruin, and said: "If this had not perished, I would have." A sentence, denoted by Maximus, as being "possessed of sought-after wisdom!" Although a Greek, he may have been a soldier of the Persian army when Clazomenae was suppressed during the Ionian Revolt.
In early manhood (c. 464–461 BC) he went to Athens, which was rapidly becoming the centre of Greek culture. There he is said to have remained for thirty years. Pericles learned to love and admire him, and the poet Euripides derived from him an enthusiasm for science and humanity.
Anaxagoras brought philosophy and the spirit of scientific inquiry from Ionia to Athens. His observations of the celestial bodies and the fall of meteorites led him to form new theories of the universal order, and to a putative prediction of the impact of a meteorite in 467BC. He attempted to give a scientific account of eclipses, meteors, rainbows, and the sun, which he described as a mass of blazing metal, larger than the Peloponnese. He was the first to explain that the moon shines due to reflected light from the sun. He also said that the moon had mountains and believed that it was inhabited. The heavenly bodies, he asserted, were masses of stone torn from the earth and ignited by rapid rotation. He explained that, though both sun and the stars were fiery stones, we do not feel the heat of the stars because of their enormous distance from earth. He thought that the earth is flat and floats supported by 'strong' air under it and disturbances in this air sometimes causes earthquakes. These speculations made him vulnerable in Athens to a charge of impiety. Diogenes Laertius reports the story that he was prosecuted by Cleon for impiety, but Plutarch says that Pericles sent his former tutor, Anaxagoras, to Lampsacus for his own safety after the Athenians began to blame him for the Peloponnesian war.
About 450 BC, according to Laertius, Pericles spoke in defense of Anaxagoras at his trial. Even so, Anaxagoras was forced to retire from Athens to Lampsacus in Troad (c. 434–433 BC). He died there in around the year 428 BC. Citizens of Lampsacus erected an altar to Mind and Truth in his memory, and observed the anniversary of his death for many years.
Anaxagoras wrote a book of philosophy, but only fragments of the first part of this have survived, through preservation in work of Simplicius of Cilicia in the sixth century AD.
Philosophy.
According to Anaxagoras all things have existed in some way from the beginning, but originally they existed in infinitesimally small fragments of themselves, endless in number and inextricably combined throughout the universe. All things existed in this mass, but in a confused and indistinguishable form. There was an infinite number of homogeneous parts (Greek: ὁμοιομερῆ) as well as heterogeneous ones.
The work of arrangement, the segregation of like from unlike and the summation of the whole into totals of the same name, was the work of Mind or Reason (Greek: νοῦς). Mind is no less unlimited than the chaotic mass, but it stood pure and independent, a thing of finer texture, alike in all its manifestations and everywhere the same. This subtle agent, possessed of all knowledge and power, is especially seen ruling in all the forms of life. Its first appearance, and the only manifestation of it which Anaxagoras describes, is Motion. It gave distinctness and reality to the aggregates of like parts.
Decease and growth only mean a new aggregation (Greek: σὐγκρισις) and disruption (Greek: διάκρισις). However, the original intermixture of things is never wholly overcome. Each thing contains in itself parts of other things or heterogeneous elements, and is what it is, only on account of the preponderance of certain homogeneous parts which constitute its character. Out of this process arises the things we see in this world.
Literary references.
In a quote chosen to begin Nathanael West's first book "The Dream Life of Balso Snell", Marcel Proust's character Bergotte says, "After all, my dear fellow, life, Anaxagoras has said, is a journey."
Anaxagoras appears as a character in "The Ionia Sanction", by Gary Corby.
Anaxagoras is referred to and admired by Cyrus Spitama, the hero and narrator of "Creation", by Gore Vidal. The book contains this passage, explaining how Anaxagoras became influential:
William H. Gass begins his novel, The Tunnel (1995), with a quote from Anaxagoras: "The descent to hell is the same from every place."

</doc>
<doc id="1802" url="http://en.wikipedia.org/wiki?curid=1802" title="Ægir">
Ægir

In Norse mythology, Ægir (Old Norse "sea") is a sea jötunn associated with the ocean. He is also known for hosting elaborate parties for the gods.
Ægir's servants are Fimafeng (killed by Loki) and Eldir.
Description.
The "Nafnaþulur" attached to the "Prose Edda" list Ægir as a giant. Richard Cleasby and Guðbrandur Vigfússon saw his name as pre-Norse, derived from an ancient Indo-European root.
Attestations.
Both "Fundinn Noregr" and Snorri Sturluson in "Skáldskaparmál" state that Ægir is the same as the sea-giant Hlér, who lives on the isle of Hlésey, and this is borne out by kennings. Snorri uses his visiting the Æsir as the frame of that section of the Prose Edda.
In "Lokasenna", Ægir hosts a party for the gods where he provides the ale brewed in an enormous pot or cauldron provided by Thor and Týr. The story of their obtaining the pot from the giant Hymir is told in "Hymiskviða".
The prose introduction to "Lokasenna" and Snorri's list of kennings state that Ægir is also known as Gymir, who is Gerðr's father, but this is evidently an erroneous interpretation of kennings in which different giant-names are used interchangeably.
Family.
According to "Fundinn Noregr", Ægir is a son of the giant Fornjótr, the king of Finland, Kvenland and Gotland, and brother of Logi ("fire") and Kári ("wind").
Ægir's wife is Rán. She is by Ægir mother of nine billow maidens, whose names are:

</doc>
<doc id="1807" url="http://en.wikipedia.org/wiki?curid=1807" title="ASA">
ASA

ASA as an abbreviation or initialism may refer to:

</doc>
<doc id="1834" url="http://en.wikipedia.org/wiki?curid=1834" title="Allophone">
Allophone

In phonology, an allophone (; from the Greek: ἄλλος, "állos", "other" and φωνή, "phōnē", "voice, sound") is one of a set of multiple possible spoken sounds (or "phones") used to pronounce a single phoneme in a particular language. For example, [pʰ] (as in "pin") and [p] (as in "spin") are allophones for the phoneme /p/ in the English language. The specific allophone selected in a given situation is often predictable from the phonetic context (such allophones are called positional variants), but sometimes allophones occur in free variation. Replacing a sound by another allophone of the same phoneme will usually not change the meaning of a word, although sometimes the result may sound non-native or even unintelligible. Native speakers of a given language usually perceive one phoneme in that language as a single distinctive sound, and are "both unaware of and even shocked by" the allophone variations used to pronounce single phonemes.
History of concept.
The term "allophone" was coined by Benjamin Lee Whorf in the 1940s. In doing so, he placed a cornerstone in consolidating early phoneme theory. The term was popularized by G. L. Trager and Bernard Bloch in a 1941 paper on English phonology and went on to become part of standard usage within the American structuralist tradition.
Complementary and free-variant allophones.
Every time a speech sound is produced for a given phoneme, it will be slightly different from other utterances, even for the same speaker. This has led to some debate over how real, and how universal, phonemes really are (see phoneme for details). Only some of the variation is significant (i.e., detectable or perceivable) to speakers. There are two types of allophones, based on whether a phoneme must be pronounced using a specific allophone in a specific situation, or whether the speaker has freedom to (unconsciously) choose which allophone to use.
When a specific allophone (from a set of allophones that correspond to a phoneme) "must" be selected in a given context (i.e., using a different allophone for a phoneme will cause confusion or make the speaker sound non-native), the allophones are said to be complementary (i.e., the allophones complement each other, and one is not used in a situation where the usage of another is standard). In the case of complementary allophones, each allophone is used in a specific phonetic context and may be involved in a phonological process.
In other cases, the speaker is able to select freely from free variant allophones, based on personal habit or preference.
Allotone.
A tonic allophone is sometimes called an allotone, for example in the neutral tone of Mandarin.
Examples.
English.
There are many allophonic processes in English, like lack of plosion, nasal plosion, partial devoicing of sonorants, complete devoicing of sonorants, partial devoicing of obstruents, lengthening and shortening vowels, and retraction.
Because the choice of allophone is seldom under conscious control, people may not realize they exist. English speakers may be unaware of the differences among six allophones of the phoneme /t/, namely unreleased [ t̚] as in "cat", aspirated [tʰ] as in "top", glottalized [ʔ] as in "button", flapped [ɾ] as in American English "water", nasalized flapped as in "winter", and none of the above [t] as in "stop". However, they may become aware of the differences if, for example, they contrast the pronunciations of the following words:
If a flame is held before the lips while these words are spoken, it flickers more during aspirated "nitrate" than during unaspirated "night rate." The difference can also be felt by holding the hand in front of the lips. For a Mandarin speaker, to whom /t/ and /tʰ/ are separate phonemes, the English distinction is much more obvious than it is to the English speaker who has learned since childhood to ignore it.
Allophones of English /l/ may be noticed if the 'light' [l] of "leaf" [ˈliːf] is contrasted with the 'dark' [ɫ] of "feel" [ˈfiːɫ]. Again, this difference is much more obvious to a Turkish speaker, for whom /l/ and /ɫ/ are separate phonemes, than to an English speaker, for whom they are allophones of a single phoneme.
Cross-language comparison.
There are many examples for allophones in languages other than English. Typically, languages with a small phoneme inventory allow for quite a lot of allophonic variation. (See e.g. Hawaiian and Toki Pona.) Examples: (Links of language names go to the specific article or subsection on the phenomenon.)
Representing a phoneme with an allophone.
Since phonemes are abstractions of speech sounds, not the sounds themselves, they have no direct phonetic transcription. When they are realized without much allophonic variation, a simple "broad transcription" is used. However, when there are complementary allophones of a phoneme, so that the allophony is significant, things become more complicated. Often, if only one of the allophones is simple to transcribe, in the sense of not requiring diacritics, then that representation is chosen for the phoneme. 
However, there may be several such allophones, or the linguist may prefer greater precision than this allows. In such cases a common convention is to use the "elsewhere condition" to decide which allophone will stand for the phoneme. The "elsewhere" allophone is the one that remains once the conditions for the others are described by phonological rules. For example, English has both oral and nasal allophones of its vowels. The pattern is that vowels are nasal only when preceding a nasal consonant within the same syllable; elsewhere they're oral. Therefore, by the "elsewhere" convention, the oral allophones are considered basic; nasal vowels in English are considered to be allophones of oral phonemes.
In other cases, an allophone may be chosen to represent its phoneme because it is more common in the world's languages than the other allophones, because it reflects the historical origin of the phoneme, or because it gives a more balanced look to a chart of the phonemic inventory. Another alternative, commonly employed for archiphonemes, is the use of a capital letter, such as /N/ for [m], [n], [ŋ]. 
In rare cases a linguist may represent phonemes with abstract symbols, such as dingbats, so as not to privilege any one allophone.

</doc>
<doc id="1857" url="http://en.wikipedia.org/wiki?curid=1857" title="Approval voting">
Approval voting

Approval voting is a single-winner voting method used for elections. Each voter may 'approve' of (i.e., select) any number of candidates. The winner is the most-approved candidate.
Guy Ottewell first described the system in 1977. and also by Robert J. Weber, who coined the term "Approval Voting." It was more fully published in 1978 by political scientist Steven Brams and mathematician Peter Fishburn.
Description.
Approval voting can be considered a form of score voting, with the range restricted to two values, 0 and 1—or a form of majority judgment, with grades restricted to "good" and "poor". Approval Voting can also be compared to plurality voting, without the rule that discards ballots that vote for more than one candidate.
By treating each candidate as a separate question, "Do you approve of this person for the job?" approval voting lets each voter indicate support for one, some, or all candidates. All votes count equally, and everyone gets the same number of votes: one vote per candidate, either for or against. Final tallies show how many voters support each candidate, and the winner is the candidate whom the most voters support.
Approval voting ballots show, for each office being contested, a list of the candidates running for that seat. Next to each name is a checkbox, or another similar way to mark 'Yes' or 'No' for that candidate. This "check yes or no" approach means approval voting provides one of the simplest ballots for a voter to understand.
Ballots on which the voter marked every candidate the same (whether yes or no) have no effect on the outcome of the election. Each ballot can, therefore, be viewed as a small "delta" that separates two groups of candidates, those supported and those that are not. Each candidate approved is considered preferred to any candidate not approved, while the voter's preferences among approved candidates is unspecified, and likewise the voter's preferences among unapproved candidates is also unspecified.
Uses.
Approval voting has been adopted by the Mathematical Association of America (1986), the Institute of Management Sciences (1987) (now the Institute for Operations Research and the Management Sciences), the American Statistical Association (1987), and the Institute of Electrical and Electronics Engineers (1987). According to Steven J. Brams and Peter C. Fishburn, the IEEE board in 2002 rescinded its decision to use approval voting. IEEE Executive Director Daniel J. Senese stated that approval voting was abandoned because "few of our members were using it and it was felt that it was no longer needed."
Approval voting was used for Dartmouth Alumni Association elections for seats on the College Board of Trustees, but after some controversy it was replaced with traditional runoff elections by an alumni vote of 82% to 18% in 2009. Dartmouth students started to use approval voting to elect their student body president in 2011. In the first election, the winner secured the support of 41% of voters against several write-in candidates. In 2012, Suril Kantaria won with the support of 32% of the voters. In 2013, the winner earned the support of just under 40% of the voters.
Historically, several voting methods that incorporate aspects of approval voting have been used:
Effect on elections.
Approval voting advocates Steven Brams and Dudley R. Herschbach predict that approval voting should increase voter participation, prevent minor-party candidates from being spoilers, and reduce negative campaigning. The effect of this system as an electoral reform measure is not without critics, however. FairVote has a position paper arguing that approval voting has three flaws that undercut it as a method of voting and political vehicle. They argue that it can result in the defeat of a candidate who would win an absolute majority in a plurality system, can allow a candidate to win who might not win any support in a plurality elections, and has incentives for tactical voting.
One study showed that approval voting would not have chosen the same two winners as plurality voting (Chirac and Le Pen) in France's presidential election of 2002 (first round) – it instead would have chosen Chirac and Jospin as the top two to proceed to a runoff. Le Pen lost by a very high margin in the runoff, 82.2% to 17.8%, a sign that the true top two had not been found. Straight approval voting without a runoff, from the study, still would have selected Chirac, but with an approval percentage of only 36.7%, compared to Jospin at 32.9%. Le Pen, in that study, would have received 25.1%. In the real primary election, the top three were Chirac, 19.9%, Le Pen, 16.9%, and Jospin, 16.2%.
A generalized version of the Burr dilemma applies to approval voting when two candidates are appealing to the same subset of voters. Although approval voting differs from the voting system used in the Burr dilemma, approval voting can still leave candidates and voters with the generalized dilemma of whether to compete or cooperate.
While in the modern era there have been relatively few competitive approval voting elections where tactical voting is more likely, Brams argues that approval voting usually elects Condorcet winners in practice. Critics of the use of approval voting in the alumni elections for the Dartmouth Board of Trustees in 2009 placed its ultimately successful repeal before alumni voters, arguing that the system has not been electing the most centrist candidates. "The Dartmouth" editorialized that "When the alumni electorate fails to take advantage of the approval voting process, the three required Alumni Council candidates tend to split the majority vote, giving petition candidates an advantage. By reducing the number of Alumni Council candidates, and instituting a more traditional one-person, one-vote system, trustee elections will become more democratic and will more accurately reflect the desires of our alumni base."
Strategic voting.
Overview.
Approval voting is vulnerable to Bullet Voting and Compromising, while it is immune to Push-Over and Burying.
Bullet Voting occurs when a voter approves "only" candidate 'a' instead of "both" 'a' and 'b' for the reason that voting for 'b' can cause 'a' to lose.
Compromising occurs when a voter approves an "additional" candidate who is otherwise considered unacceptable to the voter to prevent an even worse alternative from winning.
Strategic Approval voting differs from ranked choice voting methods where voters might "reverse" the preference order of two options. Strategic Approval voting, with more than two options, involves the voter changing their approval threshold. The voter decides which options to give the "same" rating, despite having a strict preference order between them.
Sincere voting.
Approval voting experts describe sincere votes as those "... that directly reflect the true preferences of a voter, i.e., that do not report preferences 'falsely.'" They also give a specific definition of a sincere approval vote in terms of the voter's ordinal preferences as being any vote that, if it votes for one candidate, it also votes for any more preferred candidate. This definition allows a sincere vote to treat strictly preferred candidates the same, ensuring that every voter has at least one sincere vote. The definition also allows a sincere vote to treat equally preferred candidates differently. When there are two or more candidates, every voter has at least three sincere approval votes to choose from. Two of those sincere approval votes do not distinguish between any of the candidates: vote for none of the candidates and vote for all of the candidates. When there are three or more candidates, every voter has more than one sincere approval vote that distinguishes between the candidates.
Examples.
Based on the definition above, if there are four candidates, A, B, C, and D, and a voter has a strict preference order, preferring A to B to C to D, then the following are the voter's possible sincere approval votes:
If the voter instead equally prefers B and C, while A is still the most preferred candidate and D is the least preferred candidate, then all of the above votes are sincere and the following combination is also a sincere vote:
The decision between the above ballots is equivalent to deciding an arbitrary "approval cutoff." All candidates preferred to the cutoff are approved, all candidates less preferred are not approved, and any candidates equal to the cutoff may be approved or not arbitrarily.
Sincere strategy with ordinal preferences.
A sincere voter with multiple options for voting sincerely still has to choose which sincere vote to use. Voting strategy is a way to make that choice, in which case strategic approval voting includes sincere voting, rather than being an alternative to it. This differs from other voting systems that typically have a unique sincere vote for a voter.
When there are three or more candidates, the winner of an approval voting election can change, depending on which sincere votes are used. In some cases, approval voting can sincerely elect any one of the candidates, including a Condorcet winner and a Condorcet loser, without the voter preferences changing. To the extent that electing a Condorcet winner and not electing a Condorcet loser is considered desirable outcomes for a voting system, approval voting can be considered vulnerable to sincere, strategic voting. In one sense, conditions where this can happen are robust and are not isolated cases. On the other hand, the variety of possible outcomes has also been portrayed as a virtue of approval voting, representing the flexibility and responsiveness of approval voting, not just to voter ordinal preferences, but cardinal utilities as well.
Dichotomous preferences.
Approval voting avoids the issue of multiple sincere votes in special cases when voters have dichotomous preferences. For a voter with dichotomous preferences, approval voting is strategy-proof (also known as strategy-free). When all voters have dichotomous preferences and vote the sincere, strategy-proof vote, approval voting is guaranteed to elect the Condorcet winner, if one exists. However, having dichotomous preferences when there are three or more candidates is not typical. It is an unlikely situation for all voters to have dichotomous preferences when there are more than a few voters.
Having dichotomous preferences means that a voter has bi-level preferences for the candidates. All of the candidates are divided into two groups such that the voter is indifferent between any two candidates in the same group and any candidate in the top-level group is preferred to any candidate in the bottom-level group. A voter that has strict preferences between three candidates—prefers A to B and B to C—does not have dichotomous preferences.
Being strategy-proof for a voter means that there is a unique way for the voter to vote that is a strategically best way to vote, regardless of how others vote. In approval voting, the strategy-proof vote, if it exists, is a sincere vote.
Approval threshold.
Another way to deal with multiple sincere votes is to augment the ordinal preference model with an approval or acceptance threshold. An approval threshold divides all of the candidates into two sets, those the voter approves of and those the voter does not approve of. A voter can approve of more than one candidate and still prefer one approved candidate to another approved candidate. Acceptance thresholds are similar. With such a threshold, a voter simply votes for every candidate that meets or exceeds the threshold.
With threshold voting, it is still possible to not elect the Condorcet winner and instead elect the Condorcet loser when they both exist. However, according to Steven Brams, this represents a strength rather than a weakness of approval voting. Without providing specifics, he argues that the pragmatic judgements of voters about which candidates are acceptable should take precedence over the Condorcet criterion and other social choice criteria.
Strategy with cardinal utilities.
Voting strategy under approval is guided by two competing features of approval voting. On the one hand, approval voting fails the later-no-harm criterion, so voting for a candidate can cause that candidate to win instead of a more preferred candidate. On the other hand, approval voting satisfies the monotonicity criterion, so not voting for a candidate can never help that candidate win, but can cause that candidate to lose to a less preferred candidate. Either way, the voter can risk getting a less preferred election winner. A voter can balance the risk-benefit trade-offs by considering the voter's cardinal utilities, particularly via the von Neumann–Morgenstern utility theorem, and the probabilities of how others vote.
A rational voter model described by Myerson and Weber specifies an approval voting strategy that votes for those candidates that have a positive prospective rating. This strategy is optimal in the sense that it maximizes the voter's expected utility, subject to the constraints of the model and provided the number of other voters is sufficiently large.
An optimal approval vote always votes for the most preferred candidate and not for the least preferred candidate. However, an optimal vote can require voting for a candidate and not voting for a more preferred candidate if there 4 candidates or more.
Other strategies are also available and coincide with the optimal strategy in special situations. For example:
Another strategy is to vote for the top half of the candidates, the candidates that have an above-median utility. When the voter thinks that others are balancing their votes randomly and evenly, the strategy maximizes the voter's power or efficacy, meaning that it maximizes the probability that the voter will make a difference in deciding which candidate wins.
Optimal strategic approval voting fails to satisfy the Condorcet criterion and can elect a Condorcet loser. Strategic approval voting can guarantee electing the Condorcet winner in some special circumstances. For example, if all voters are rational and cast a strategically optimal vote based on a common knowledge of how all the other voters vote except for small-probability, statistically independent errors in recording the votes, then the winner will be the Condorcet winner, if one exists.
Strategy examples.
In the example election described here, assume that the voters in each faction share the following von Neumann-Morgenstern utilities, fitted to the interval between 0 and 100. The utilities are consistent with the rankings given earlier and reflect a strong preference each faction has for choosing its city, compared to weaker preferences for other factors such as the distance to the other cities.
Using these utilities, voters choose their optimal strategic votes based on what they think the various pivot probabilities are for pairwise ties. In each of the scenarios summarized below, all voters share a common set of pivot probabilities.
In the first scenario, voters all choose their votes based on the assumption that all pairwise ties are equally likely. As a result, they vote for any candidate with an above-average utility. Most voters vote for only their first choice. Only the Knoxville faction also votes for its second choice, Chattanooga. As a result, the winner is Memphis, the Condorcet loser, with Chattanooga coming in second place.
In the second scenario, all of the voters expect that Memphis is the likely winner, that Chattanooga is the likely runner-up, and that the pivot probability for a Memphis-Chattanooga tie is much larger than the pivot probabilities of any other pair-wise ties. As a result, each voter votes for any candidate they prefer more than the leading candidate, and also vote for the leading candidate if they prefer that candidate more than the expected runner-up. Each remaining scenario follows a similar pattern of expectations and voting strategies.
In the second scenario, there is a three-way tie for first place. This happens because the expected winner, Memphis, was the Condorcet loser and was also ranked last by any voter that did not rank it first.
Only in the last scenario does the actual winner and runner-up match the expected winner and runner-up. As a result, this can be considered a stable strategic voting scenario. In the language of game theory, this is an "equilibrium." In this scenario, the winner is also the Condorcet winner.
Compliance with voting system criteria.
Most of the mathematical criteria by which voting systems are compared were formulated for voters with ordinal preferences. In this case, approval voting requires voters to make an additional decision of where to put their approval cutoff (see examples above). Depending on how this decision is made, approval voting satisfies different sets of criteria.
There is no ultimate authority on which criteria should be considered, but the following are criteria that many voting theorists accept and considered desirable:
Multiple winners.
Approval voting can be extended to multiple winner elections. The naive way to do so is as "block approval voting", a simple variant on block voting where each voter can select an unlimited number of candidates and the candidates with the most approval votes win. This does not provide proportional representation and is subject to the Burr dilemma, among other problems.
Other ways of extending Approval voting to multiple winner elections have been devised. Among these are proportional approval voting for determining a proportional assembly, and Minimax Approval for determining a consensus assembly where the least satisfied voter is satisfied the most.
Ballot types.
Approval ballots can be of at least four semi-distinct forms. The simplest form is a blank ballot on which voters hand-write the names of the candidates they support. A more structured ballot lists all candidates, and voters mark each candidate they support. A more explicit structured ballot can list the candidates and provide two choices by each. (Candidate list ballots can include spaces for write-in candidates as well.)
All four ballots are theoretically equivalent. The more structured ballots may aid voters in offering clear votes so they explicitly know all their choices. The Yes/No format can help to detect an "undervote" when a candidate is left unmarked and allow the voter a second chance to confirm the ballot markings are correct. The "single bubble" format is incapable of producing invalid ballots (which might otherwise be rejected in counting).
Unless the second or fourth format is used, fraudulently adding votes to an approval voting ballot does not invalidate the ballot (that is, it does not make it appear inconsistent). Thus, approval voting raises the importance of ensuring that the "chain of custody" of ballots is secure.

</doc>
<doc id="1866" url="http://en.wikipedia.org/wiki?curid=1866" title="Alarums and Excursions">
Alarums and Excursions

Alarums and Excursions (A&E) is an amateur press association started in June 1975 by Lee Gold (at the request of Bruce Pelz, who felt that discussion of "Dungeons & Dragons" was taking up too much space in Apa-L, the APA of the Los Angeles Science Fantasy Society). It was the first publication to focus solely on role-playing games. 
Each issue is a collection of contributions from different authors, often featuring game design discussions, rules variants, write-ups of game sessions, reviews, and comments on others contributions. It was a four-time winner of the Charles Roberts/Origins Award, winning "Best Amateur Adventure Gaming Magazine" in 1984, "Best Amateur Game Magazine" in 1999, and "Best Amateur Game Periodical" in 2000 and 2001.
Although game reports and social reactions are common parts of many "A&E" contributions, it has also, over the years, become a testing ground for new ideas on the development of the RPG as a genre and an art form. The idea that role-playing games "are" an art form took strong root in this zine, and left a lasting impression on many of the RPG professionals who contributed.
The July 2013 collation of "Alarums and Excursions" was #466.
Over the years, contributors have included: 
The role-playing game "Over the Edge" was inspired by discussions in "A&E".
"Alarums and excursions" is a stage direction for the moving of soldiers across a stage, used in Elizabethan drama.

</doc>
<doc id="1869" url="http://en.wikipedia.org/wiki?curid=1869" title="Alfred Jarry">
Alfred Jarry

Alfred Jarry (]; 8 September 1873 – 1 November 1907) was a French symbolist writer who is best known for his play "Ubu Roi" (1896). He also coined the term and philosophical concept of 'pataphysics.
Jarry was born in Laval, Mayenne, France, and his mother was from Brittany. He was associated with the Symbolist movement. His play "Ubu Roi" (1896) is often cited as a forerunner to the Surrealist and Futurist movements of the 1920s and 1930s. Jarry wrote in a variety of hybrid genres and styles, prefiguring the Postmodern. He wrote plays, novels, poetry, essays and speculative journalism. His texts present us with pioneering work in the fields of absurdist literature and postmodern philosophy.
Biography and works.
At the lycée in Rennes when he was 15, he led a group of boys who enjoyed poking fun at their well-meaning, but obese and incompetent physics teacher, a man named Hébert. Jarry and his classmate, Henri Morin, wrote a play they called "Les Polonais" and performed it with marionettes in the home of one of their friends. The main character, "Père Heb", was a blunderer with a huge belly; three teeth (one of stone, one of iron, and one of wood); a single, retractable ear; and a misshapen body. In Jarry's later work "Ubu Roi", Père Heb would develop into Ubu, one of the most monstrous and astonishing characters in French literature.
At 17 Jarry passed his baccalauréat and moved to Paris to prepare for admission to the École Normale Supérieure. Though he was not admitted, he soon gained attention for his original poems and prose-poems. A collection of his work, "Les minutes de sable mémorial", was published in 1893.
That same year, both his parents died, leaving him a small inheritance which he quickly spent.
Jarry had meantime discovered the pleasures of alcohol, which he called "my sacred herb" or, when referring to absinthe, the "green goddess". A story is told that he once painted his face green and rode through town on his bicycle in its honour (and possibly under its influence).
When he was drafted into the army in 1894, his gift for turning notions upside down defeated attempts to instill military discipline. The sight of the small man in a uniform much too large for his less than 5-foot frame—the army did not issue uniforms small enough—was so disruptively funny that he was excused from parades and marching drills. Eventually the army discharged him for medical reasons. His military experience eventually inspired his novel "Days and Nights".
Jarry returned to Paris and applied himself to writing, drinking, and the company of friends who appreciated his witty, sweet-tempered, and unpredictable conversation. This period is marked by his intense involvement with Remy de Gourmont in the publication of "L'Ymagier", a luxuriously produced "art" magazine devoted to the symbolic analysis of medieval and popular prints. Symbolism as an art movement was in full swing at this time, and "L'Ymagier" provided a nexus for many of its key contributors. Jarry's play "Caesar Antichrist" (1895) drew on this movement for material. This is a work that bridges the gap between serious symbolic meaning and the type of critical absurdity with which Jarry would soon become associated. Using the biblical Book of Revelation as a point of departure, "Caesar Antichrist" presents a parallel world of extreme formal symbolism in which Christ is resurrected not as an agent of spirituality but as an agent of the Roman Empire that seeks to dominate spirituality. It is a unique narrative that effectively links the domination of the soul to contemporaneous advances in the field of Egyptology such as the 1894 excavation of the Narmer Palette, an ancient artifact used for situating the rebus within hermeneutics. The character Ubu Roi first appears in this play.
The spring of 1896 saw the publication, in Paul Fort's review "Le Livre d'art", of Jarry's 5-act play "Ubu Roi"—the rewritten and expanded "Les Polonais" of his school days. "Ubu Roi"'s savage humor and monstrous absurdity, unlike anything thus far performed in French theater, seemed unlikely to ever actually be performed on stage. However, impetuous theater director Aurélien-Marie Lugné-Poe took the risk, producing the play at his Théâtre de l'Oeuvre.
On opening night (10 December 1896), with traditionalists and the avant-garde in the audience, King Ubu (played by Firmin Gémier) stepped forward and intoned the opening word, "Merdre!" (often translated as "Pshit" or "Shittr!" in English). A quarter of an hour of pandemonium ensued: outraged cries, booing, and whistling by the offended parties, countered by cheers and applause by the more degenerate contingent. Such interruptions continued through the evening. At the time, only the dress rehearsal and opening night performance were held, and the play was not revived until after Jarry's death.
The play brought fame to the 23-year-old Jarry, and he immersed himself in the fiction he had created. Gémier had modeled his portrayal of Ubu on Jarry's own staccato, nasal vocal delivery, which emphasized each syllable (even the silent ones). From then on, Jarry would always speak in this style. He adopted Ubu's ridiculous and pedantic figures of speech; for example, he referred to himself using the royal "we", and called the wind "that which blows" and the bicycle he rode everywhere "that which rolls".
Jarry moved into a flat which the landlord had created through the unusual expedient of subdividing a larger flat by means of a horizontal rather than a vertical partition. The diminutive Jarry could just manage to stand up in the place, but guests had to bend or crouch. Jarry also took to carrying a loaded revolver. In response to a neighbor's complaint that his target shooting endangered her children, he replied, "If that should ever happen, ma-da-me, we should ourselves be happy to get new ones with you".
With Franc-Nohain and Claude Terrasse, he co-founded the Théatre des Pantins, which in 1898 was the site of marionette performances of "Ubu Roi".
Living in worsening poverty, neglecting his health, and drinking excessively, Jarry went on to write the novel, "Le Surmâle" ("The Supermale"), which is partly a satire on the Symbolist ideal of self-transcendence.
Unpublished until after his death, his fiction "Exploits and Opinions of Dr. Faustroll, Pataphysician" ("Gestes et opinions du docteur Faustroll, pataphysicien") describes the exploits and teachings of a sort of antiphilosopher who, born at age 63, travels through a hallucinatory Paris in a sieve and subscribes to the tenets of "'pataphysics". 'Pataphysics deals with "the laws which govern exceptions and will explain the universe supplementary to this one". In 'pataphysics, every event in the universe is accepted as an extraordinary event.
Jarry once wrote, expressing some of the bizarre logic of 'pataphysics, "If you let a coin fall and it falls, the next time it is just by an infinite coincidence that it will fall again the same way; hundreds of other coins on other hands will follow this pattern in an infinitely unimaginable fashion".
In his final years, he was a legendary and heroic figure to some of the young writers and artists in Paris. Guillaume Apollinaire, André Salmon, and Max Jacob sought him out in his truncated apartment. Pablo Picasso was fascinated with Jarry. After Jarry's death Picasso acquired his revolver and wore it on his nocturnal expeditions in Paris. He later bought many of his manuscripts as well as executing a fine drawing of him.
Jarry died in Paris on 1 November 1907 of tuberculosis, aggravated by drug and alcohol use. It is recorded that his last request was for a toothpick. He was interred in the Cimetière de Bagneux, near Paris.
The complete works of Alfred Jarry are published in three volumes by Gallimard in the collection "Bibliothèque de la Pléiade".

</doc>
<doc id="1878" url="http://en.wikipedia.org/wiki?curid=1878" title="Alphonse, Count of Poitiers">
Alphonse, Count of Poitiers

Alphonse or Alfonso (11 November 1220 – 21 August 1271) was the Count of Poitou from 1225 and Count of Toulouse (as Alphonse II) from 1249.
Life.
Birth and early life.
Born at Poissy, Alphonse was a son of Louis VIII, King of France and Blanche of Castile. He was a younger brother of Louis IX of France and an older brother of Charles I of Sicily. In 1229, his mother, who was regent of France, forced the Treaty of Paris on Raymond VII of Toulouse after his rebellion. It stipulated that a brother of King Louis was to marry Joan of Toulouse, daughter of Raymond VII of Toulouse, and so in 1237 Alphonse married her. Since she was Raymond's only child, they became rulers of Toulouse at Raymond's death in 1249.
By the terms of his father's will he received an "appanage" of Poitou and Auvergne. To enforce this Louis IX won the battle of Taillebourg in the Saintonge War together with Alphonse against a revolt allied with king Henry III of England, who also participated in the battle.
Crusades.
Alphonse took part in two crusades with his brother, St Louis, in 1248 (the Seventh Crusade) and in 1270 (the Eighth Crusade). For the first of these, he raised a large sum and a substantial force, arriving in Damietta on 24 October 1249, after the town had already been captured. He sailed for home on 10 August 1250. His father-in-law had died while he was away, and he went directly to Toulouse to take possession. There was some resistance to his accession as count, which was suppressed with the help of his mother Blanche of Castile who was acting as regent in the absence of Louis IX. The county of Toulouse, since then, was joined to the Alphonse's "appanage".
Later life.
In 1252, on the death of his mother, Blanche of Castile, Alphonse was joint regent with Charles of Anjou until the return of Louis IX. During that time he took a great part in the campaigns and negotiations which led to the Treaty of Paris in 1259, under which King Henry III of England recognized his loss of continental territory to France (including Normandy, Maine, Anjou, and Poitou) in exchange for France withdrawing support from English rebels.
Aside from the crusades, Alphonse stayed primarily in Paris, governing his estates by officials, inspectors who reviewed the officials work, and a constant stream of messages. His main work was on his own estates. There he repaired the evils of the Albigensian war and made a first attempt at administrative centralization, thus preparing the way for union with the crown. The charter known as "Alphonsine," granted to the town of Riom, became the code of public law for Auvergne. Honest and moderate, protecting the middle classes against exactions of the nobles, he exercised a happy influence upon the south, in spite of his naturally despotic character and his continual and pressing need of money. He is noted for ordering the first recorded local expulsion of Jews, when he did so in Poitou in 1249.
When Louis IX again engaged in a crusade (the Eighth Crusade), Alphonse again raised a large sum of money and accompanied his brother. This time, however, he did not return to France, dying while on his way back, probably at Savona in Italy, on 21 August 1271. He had been appointed a Knight of the Order of the Ship by his brother.
Death and Legacy.
Alphonse's death without heirs raised some questions as to the succession to his lands. One possibility was that they should revert to the crown, another that they should be redistributed to his family. The latter was claimed by Charles of Anjou, but in 1283 Parlement decided that the County of Toulouse should revert to the crown, if there were no male heirs. Alphonse's wife Joan (who died four days after Alphonse) had attempted to dispose of some of her inherited lands in her will. Joan was the only surviving child and heiress of Raymond VII, Count of Toulouse, Duke of Narbonne, and Marquis of Provence, so under Provençal and French law, the lands should have gone to her nearest male relative. But, her will was invalidated by Parlement in 1274. One specific bequest in Alphonse's will, giving his wife's lands in the Comtat Venaissin to the Holy See, was allowed, and it became a Papal territory, a status that it retained until 1791.

</doc>
<doc id="1893" url="http://en.wikipedia.org/wiki?curid=1893" title="Albert Spalding">
Albert Spalding

Albert Goodwill Spalding (September 2, 1849 – September 9, 1915) was an American pitcher, manager and executive in the early years of professional baseball, and the co-founder of A.G. Spalding sporting goods company. He played major league baseball between 1871 and 1878. In 1877, he became the first well-known player to use a fielding glove; such gloves were among the items sold at his sporting goods store.
After his retirement as a player, Spalding remained active with the Chicago White Stockings as president and part-owner. In the 1880s, he took players on the first world tour of baseball. With William Hulbert, Spalding organized the National League. He later called for the commission that investigated the origins of baseball and credited Abner Doubleday with creating the game. He also wrote the first set of official baseball rules.
Baseball career.
Player.
Having played baseball throughout his youth, Spalding first played competitively with the Rockford Pioneers, a youth team, which he joined in 1865. After pitching his team to a 26–2 victory over a local men's amateur team (the Mercantiles), he was approached at the age of 15 by another, the Forest Citys, for whom he played for two years. In the autumn of 1867 he accepted a $40 per week contract, nominally as a clerk, but really to play professionally for the Chicago Excelsiors, not an uncommon arrangement used to circumvent the rules of the time, which forbade the hiring of professional players. Following the formation of baseball's first professional organization, the National Association of Professional Base Ball Players (which became known as the National Association, the Association, or NA) in 1871, Spalding joined the Boston Red Stockings (precursor club to the modern Atlanta Braves) and was highly successful; winning 206 games (and losing only 53) as a pitcher and batting .323 as a hitter.
William Hulbert, principal owner of the Chicago White Stockings, did not like the loose organization of the National Association and the gambling element that influenced it, so he decided to create a new organization, which he dubbed the National League of Baseball Clubs. To aid him in this venture, Hulbert enlisted the help of Spalding. Playing to the pitcher's desire to return to his Midwestern roots and challenging Spalding's integrity, Hulbert convinced Spalding to sign a contract to play for the White Stockings (now known as the Chicago Cubs) in 1876. Spalding then coaxed teammates Deacon White, Ross Barnes and Cal McVey, as well as Philadelphia Athletics players Cap Anson and Bob Addy, to sign with Chicago. This was all done under complete secrecy during the playing season because players were all free agents in those days and they did not want their current club and especially the fans to know they were leaving to play elsewhere the next year. News of the signings by the Boston and Philadelphia players leaked to the press before the season ended and all of them faced verbal abuse and physical threats from the fans of those cities.
He was "the premier pitcher of the 1870s", leading the league in victories for each of his six full seasons as a professional. During each of those years he was his team's only pitcher. In 1876, Spalding won 47 games as the prime pitcher for the White Stockings and led them to win the first-ever National League pennant by a wide margin.
In 1877, Spalding began to use a glove to protect his catching hand. People had used gloves previously, but never had a star like Spalding used one. Spalding had an ulterior motive for doing so: he now owned a sporting goods store which sold baseball gloves and wearing one himself was good advertising for his business.
Spalding retired from playing baseball in 1878 at the age of 27, although he continued as president and part owner of the White Stockings and a major influence on the National League. Spalding's .796 career winning percentage (from an era when teams played about once or twice a week) is the highest ever achieved by a baseball pitcher.
Organizer and executive.
In the months after signing for Chicago, Hulbert and Spalding organized the National League by enlisting the four major teams in the East and the three other top teams in what was then considered to be the West. Joining Chicago initially were the leading teams from Cincinnati, Louisville, and St. Louis. The owners of these western clubs accompanied Hulbert and Spalding to New York where they secretly met with owners from New York, Philadelphia, Hartford, and Boston. Each signed the league's constitution, and the National League was officially born. "Spalding was thus involved in the transformation of baseball from a game of gentlemen athletes into a business and a professional sport." Although the National Association held on for a few more seasons, it was no longer recognized as the premier organization for professional baseball. Gradually, it faded out of existence and was replaced by myriad minor leagues and associations around the country.
In 1905, after Henry Chadwick wrote an article saying that baseball grew from the British sports of cricket and rounders, Spalding called for a commission to find out the real source of baseball. The commission called for citizens who knew anything about the founding of baseball to send in letters. After three years of searching, on December 30, 1907, Spalding received a letter that (erroneously) declared baseball to be the invention of Abner Doubleday. The commission, though, was biased, as Spalding would not appoint anyone to the commission if they believed the sport was somewhat related to the English sport of rounders. Just before the commission, in a letter to sportswriter Tim Murnane, Spalding noted, "Our good old American game of baseball must have an American Dad." The project, later called the Mills Commission, concluded that "Base Ball had its origins in the United States" and "the first scheme for playing baseball, according to the best evidence available to date, was devised by Abner Doubleday at Cooperstown, N.Y., in 1839."
Receiving the archives of Henry Chadwick in 1908, Spalding combined these records with his own memories (and biases) to write "America's National Game" (published 1911) which, despite its flaws, was probably the first scholarly account of the history of baseball.
Businessman.
In 1874 while Spalding was playing and organizing the league, Spalding and his brother Walter began a sporting goods store in Chicago, which grew rapidly (14 stores by 1901) and expanded into a manufacturer and distributor of all kinds of sporting equipment. The company became "synonymous with sporting goods" and is still a going concern.
Spalding published the first official rules guide for baseball. In it he stated that only Spalding balls could be used (previously, the quality of the balls used had been subpar). Spalding also founded the "Baseball Guide," which at the time was the most widely read baseball publication.
In 1888–1889, Spalding took a group of major league players around the world to promote baseball and Spalding sporting goods. This was the first-ever world baseball tour. Playing across the western U.S., the tour made stops in Hawaii (although no game was played), New Zealand, Australia, Ceylon, Egypt, Italy, France, and England. The tour returned to grand receptions in New York, Philadelphia, and Chicago. The tour included future Hall of Famers Cap Anson and John Montgomery Ward. While the players were on the tour, the National League instituted new rules regarding player pay that led to a revolt of players, led by Ward, who started the Players League the following season (1890). The league lasted one year, partially due to the anti-competitive tactics of Spalding to limit its success. The tour and formation of the Player's League is depicted in the 2015 movie "Deadball."
In 1900 Spalding was appointed by President McKinley as the USA's Commissioner at that year's Summer Olympic Games.
Other activities.
Spalding had been a prominent member of the Theosophical Society under William Quan Judge. In 1900, Spalding moved to San Diego with his newly acquired second wife, Elizabeth and became a prominent member and supporter of the Theosophical community Lomaland, which was being developed on Point Loma by Katherine Tingley. He built an estate in the Sunset Cliffs area of Point Loma where he lived with Elizabeth for the rest of his life. The Spaldings raised race horses and collected Chinese fine furniture and art.
The Spaldings had an extensive library which included many volumes on Theosophy, art, and literature. In 1907-1909 he was the driving force behind the development of a paved road, known as the "Point Loma boulevard", from downtown San Diego to Point Loma and Ocean Beach; the road also provided good access to Lomaland. It later provided the basis for California State Route 209. He proposed the project, supervised it on behalf of the city, and paid a portion of the cost out of his own pocket. He joined with George Marston and other civic-minded businessmen to purchase the site of the original Presidio of San Diego, which they developed as a historic park and eventually donated to the city of San Diego. He ran unsuccessfully for the United States Senate in 1910. He helped to organize the 1915 Panama-California Exposition, serving as second vice-president.
Death.
He died on September 9, 1915 in San Diego, and his ashes were scattered at his request.
Legacy.
He was elected to the Baseball Hall of Fame by the Veterans Committee in 1939, as one of the first inductees from the 19th century at that summer's opening ceremonies. His plaque in the Hall of Fame reads "Albert Goodwill Spalding. Organizational genius of baseball's pioneer days. Star pitcher of Forest City Club in late 1860s, 4-year champion Bostons 1871-1875 and manager-pitcher of champion Chicagos in National League's first year. Chicago president for 10 years. Organizer of baseball's first round-the-world tour in 1888."
His nephew, also named Albert Spalding, was a renowned violinist.

</doc>
<doc id="1923" url="http://en.wikipedia.org/wiki?curid=1923" title="Alessandro Volta">
Alessandro Volta

Alessandro Giuseppe Antonio Anastasio Volta (18 February 1745 – 5 March 1827) was an Italian physicist and chemist, credited with the invention of the first electrical battery, the Voltaic pile, which he invented in 1799 and the results of which he reported in 1800 in a two part letter to the President of the Royal Society. With this invention Volta proved that electricity could be generated chemically and debased the prevalent theory that electricity was generated solely by living beings. Volta's invention sparked a great amount of scientific excitement and led others to conduct similar experiments which eventually led to the development of the field of electrochemistry.
Alessandro Volta also drew admiration from Napoleon Bonaparte for his invention, and was invited to the Institute of France to demonstrate his invention to the members of the Institute. Volta enjoyed a certain amount of closeness with the Emperor throughout his life and he was conferred numerous honours by him. Alessandro Volta held the chair of experimental physics at the University of Pavia for nearly 40 years and was widely idolised by his students.
Despite his professional success Volta tended to be a person inclined towards domestic life and this was more apparent in his later years. At this time he tended to live secluded from public life and more for the sake of his family until his eventual death in 1827 from a series of illnesses which began in 1823. The SI unit of electric potential is named in his honour as the volt.
Early life and works.
Volta was born in Como, a town in present-day northern Italy (near the Swiss border) on 18 February 1745. In 1794, Volta married an aristocratic lady also from Como, Teresa Peregrini, with whom he raised three sons: Zanino, Flaminio, and Luigi. His own father Filippo Volta was of noble lineage. His mother Donna Maddalena came from the family of the Inzaghis.
In 1774, he became a professor of physics at the Royal School in Como. A year later, he improved and popularised the electrophorus, a device that produced static electricity. His promotion of it was so extensive that he is often credited with its invention, even though a machine operating on the same principle was described in 1762 by the Swedish experimenter Johan Wilcke. In 1777, he travelled through Switzerland. There he befriended H. B. de Saussure.
In the years between 1776 and 1778, Volta studied the chemistry of gases. He researched and discovered methane after reading a paper by Benjamin Franklin of United States on "flammable air". In November 1776, he found methane at Lake Maggiore, and by 1778 he managed to isolate methane. He devised experiments such as the ignition of methane by an electric spark in a closed vessel. Volta also studied what we now call electrical capacitance, developing separate means to study both electrical potential ("V" ) and charge ("Q" ), and discovering that for a given object, they are proportional. This is called Volta's Law of Capacitance, and it was for this work the unit of electrical potential has been named the volt.
In 1779 he became a professor of experimental physics at the University of Pavia, a chair that he occupied for almost 40 years.
Volta and Galvani.
Luigi Galvani, an Italian physicist, discovered something he named "animal electricity" when two different metals were connected in series with a frog's leg and to one another. Volta realised that the frog's leg served as both a conductor of electricity (what we would now call an electrolyte) and as a detector of electricity. He replaced the frog's leg with brine-soaked paper, and detected the flow of electricity by other means familiar to him from his previous studies.
In this way he discovered the electrochemical series, and the law that the electromotive force (emf) of a galvanic cell, consisting of a pair of metal electrodes separated by electrolyte, is the difference between their two electrode potentials (thus, two identical electrodes and a common electrolyte give zero net emf). This may be called Volta's Law of the electrochemical series.
In 1800, as the result of a professional disagreement over the galvanic response advocated by Galvani, Volta invented the voltaic pile, an early electric battery, which produced a steady electric current. Volta had determined that the most effective pair of dissimilar metals to produce electricity was zinc and silver. Initially he experimented with individual cells in series, each cell being a wine goblet filled with brine into which the two dissimilar electrodes were dipped. The voltaic pile replaced the goblets with cardboard soaked in brine.
First battery.
In announcing his discovery of the voltaic pile, Volta paid tribute to the influences of William Nicholson, Tiberius Cavallo, and Abraham Bennet.
The battery made by Volta is credited as the first electrochemical cell. It consists of two electrodes: one made of zinc, the other of copper. The electrolyte is either sulfuric acid mixed with water or a form of saltwater brine. The electrolyte exists in the form 2H+ and SO42−. The zinc, which is higher in the electrochemical series than both copper and hydrogen, reacts with the negatively charged sulfate (SO42−). The positively charged hydrogen ions (protons) capture electrons from the copper, forming bubbles of hydrogen gas, H2. This makes the zinc rod the negative electrode and the copper rod the positive electrode.
Thus, there are two terminals, and an electric current will flow if they are connected. The chemical reactions in this voltaic cell are as follows:
The copper does not react, but rather it functions as an electrode for the electric current.
However, this cell also has some disadvantages. It is unsafe to handle, since sulfuric acid, even if diluted, can be hazardous. Also, the power of the cell diminishes over time because the hydrogen gas is not released. Instead, it accumulates on the surface of the zinc electrode and forms a barrier between the metal and the electrolyte solution.
Last years and retirement.
In honour of his work, Volta was made a count by Napoleon Bonaparte in 1810. His image was depicted on the Italian 10,000 lira note along with a sketch of his voltaic pile.
Volta retired in 1819 to his estate in Camnago, a frazione of Como, Italy, now named "Camnago Volta" in his honour. He died there on 5 March 1827, just after his 82nd birthday. Volta's remains were buried in Camnago Volta.
Volta's legacy is celebrated by the Tempio Voltiano memorial located in the public gardens by the lake. There is also a museum which has been built in his honour, which exhibits some of the equipment that Volta used to conduct experiments. Nearby stands the Villa Olmo, which houses the Voltian Foundation, an organization promoting scientific activities. Volta carried out his experimental studies and produced his first inventions near Como.
Religious beliefs.
Volta was raised as a Catholic and for all of his life continued to maintain his belief. Because he was not ordained a clergyman as his family expected, he was sometimes accused of being irreligious and some people have speculated about his possible unbelief, stressing that "he did not join the Church", or that he virtually "ignored the church's call". Nevertheless, he cast out doubts in a declaration of faith in which he said:
I do not understand how anyone can doubt the sincerity and constancy of my attachment to the religion which I profess, the Roman, Catholic and Apostolic religion in which I was born and brought up, and of which I have always made confession, externally and internally. I have, indeed, and only too often, failed in the performance of those good works which are the mark of a Catholic Christian, and I have been guilty of many sins: but through the special mercy of God I have never, as far as I know, wavered in my faith... In this faith I recognise a pure gift of God, a supernatural grace; but I have not neglected those human means which confirm belief, and overthrow the doubts which at times arise. I studied attentively the grounds and basis of religion, the works of apologists and assailants, the reasons for and against, and I can say that the result of such study is to clothe religion with such a degree of probability, even for the merely natural reason, that every spirit unperverted by sin and passion, every naturally noble spirit must love and accept it. May this confession which has been asked from me and which I willingly give, written and subscribed by my own hand, with authority to show it to whomsoever you will, for I am not ashamed of the Gospel, may it produce some good fruit!

</doc>
<doc id="1939" url="http://en.wikipedia.org/wiki?curid=1939" title="Approximant consonant">
Approximant consonant

Approximants are speech sounds that involve the articulators approaching each other but not narrowly enough nor with enough articulatory precision to create turbulent airflow. Therefore, approximants fall between fricatives, which do produce a turbulent airstream, and vowels, which produce no turbulence. This class of sounds includes lateral approximants like [l] (as in "less"), non-lateral approximants like [ɹ] (as in "rest"), and semivowels like [j] and [w] (as in "yes" and "west", respectively).
Before Peter Ladefoged coined the term "approximant" in the 1960s the term "frictionless continuant" referred to non-lateral approximants.
Semivowels.
Some approximants resemble vowels in acoustic and articulatory properties and the terms "semivowel" and "glide" are often used for these non-syllabic vowel-like segments. The correlation between semivowels and vowels is strong enough that cross-language differences between semivowels correspond with the differences between their related vowels.
Vowels and their corresponding semivowels alternate in many languages depending on the phonological environment, or for grammatical reasons, as is the case with Indo-European ablaut. Similarly, languages often avoid configurations where a semivowel precedes its corresponding vowel. A number of phoneticians distinguish between semivowels and approximants by their location in a syllable. Although he uses the terms interchangeably, remarks that, for example, the final glides of English "par" and "buy" differ from French "par" ('through') and "baille" ('tub') in that, in the latter pair, the approximants appear in the syllable coda, whereas, in the former, they appear in the syllable nucleus. This means that opaque (if not minimal) contrasts can occur in languages like Italian (with the i-like sound of "piede" 'foot', appearing in the nucleus: [ˈpi̯eˑde], and that of "piano" 'slow', appearing in the syllable onset: [ˈpjaˑno]) and Spanish (with a near minimal pair being "abyecto" [aβˈjekto] 'abject' and "abierto" [aˈβi̯erto] 'opened').
In articulation and often diachronically, palatal approximants correspond to front vowels, velar approximants to back vowels, and labialized approximants to rounded vowels. In American English, the rhotic approximant corresponds to the rhotic vowel. This can create alternations (as shown in the above table).
In addition to alternations, glides can be inserted to the left or the right of their corresponding vowels when occurring next to a hiatus. For example, in Ukrainian, medial /i/ triggers the formation of an inserted [j] that acts as a syllable onset so that when the affix /-ist/ is added to футбол ('football') to make футболіст 'football player', it's pronounced [futˈbo̞list] but маоїст ('Maoist'), with the same affix, is pronounced [ˈmao̞jist] with a glide. Dutch has a similar process that extends to mid vowels:
Similarly, vowels can be inserted next to their corresponding glide in certain phonetic environments. Sievers' law describes this behaviour for Germanic.
Non-high semivowels also occur. In colloquial Nepali speech, a process of glide-formation occurs, wherein one of two adjacent vowels becomes non-syllabic; this process includes mid vowels so that [dʱo̯a] ('cause to wish') features a non-syllabic mid vowel. Spanish features a similar process and even nonsyllabic /a/ can occur so that "ahorita" ('right away') is pronounced [a̯o̞ˈɾita]. It is not often clear, however, whether such sequences involve a semivowel (a consonant) or a diphthong (a vowel), and in many cases that may not be a meaningful distinction.
Although many languages have central vowels [ɨ, ʉ], which lie between back/velar [ɯ, u] and front/palatal [i, y], there are few cases of a corresponding approximant [ ȷ̈]. One is in the Korean diphthong [ ȷ̈i] or [ɨ̯i], though this is more frequently analyzed as velar (as in the table above), and Mapudungun may be another: It has three high vowel sounds, /i/, /u/, /ɨ/ and three corresponding consonants, /j/, and /w/, and a third one is often described as a voiced unrounded velar fricative; some texts note a correspondence between this approximant and /ɨ/ that is parallel to /j/–/i/ and /w/–/u/. An example is "liq" /ˈliɣ/ ([ˈliɨ̯]?) ('white').
Approximants versus fricatives.
In addition to less turbulence, approximants also differ from fricatives in the precision required to produce them. 
When emphasized, approximants may be slightly fricated (that is, the airstream may become slightly turbulent), which is reminiscent of fricatives. For example, the Spanish word "ayuda" ('help') features a palatal approximant that is pronounced as a fricative in emphatic speech. However, such frication is generally slight and intermittent, unlike the strong turbulence of fricative consonants. 
Because voicelessness has comparatively reduced resistance to air flow from the lungs, the increased air flow creates more turbulence, making acoustic distinctions between voiceless approximants (which are extremely rare cross-linguistically) and voiceless fricatives difficult. This is why, for example, the voiceless labialized velar approximant [w̥] (also transcribed with the special letter ⟨ʍ⟩) has traditionally been labeled a fricative, and no language is known to contrast it with a voiceless labialized velar fricative [xʷ]. Similarly, Standard Tibetan has a voiceless lateral approximant, [l̥], and Welsh has a voiceless lateral fricative [ɬ], but the distinction is not always clear from descriptions of these languages. Again, no language is known to contrast the two. Iaai is reported to have an unusually large number of voiceless approximants, with /l̥ ɥ̊ w̥/.
For places of articulation further back in the mouth, languages do not contrast voiced fricatives and approximants. Therefore the IPA allows the symbols for the voiced fricatives to double for the approximants, with or without a lowering diacritic. 
Occasionally, the glottal "fricatives" are called approximants, since [h] typically has no more frication than voiceless approximants, but they are often phonations of the glottis without any accompanying manner or place of articulation.
Lateral approximants.
In lateral approximants, the center of tongue makes solid contact with the roof of the mouth. However, the defining location is the side of the tongue, which only approaches the teeth. 
Voiceless approximants.
Voiceless approximants are rarely distinguished from voiceless fricatives. Some of them are:
Nasal approximants.
Examples are:
In Portuguese, the nasal glides [j̃] and [w̃] historically became /ɲ/ and /m/ in some words. In Bini, the nasalized allophones of the approximants /j/ and /w/ are nasal occlusives, [ɲ] and [ŋʷ].
What are transcribed as nasal approximants may include non-syllabic elements of nasal vowels/diphthongs.

</doc>
<doc id="2028" url="http://en.wikipedia.org/wiki?curid=2028" title="Ambient">
Ambient

Ambient or Ambiance may refer to:

</doc>
<doc id="2038" url="http://en.wikipedia.org/wiki?curid=2038" title="August Horch">
August Horch

August Horch (12 October 1868 – 3 February 1951) was a German engineer and automobile pioneer, the founder of the manufacturing giant which would eventually become Audi.
Beginnings.
Horch was born in Winningen, Rhenish Prussia. His initial trade was as a blacksmith, and then was educated at Hochschule Mittweida (Mittweida Technical College). After receiving a degree in engineering, he worked in shipbuilding. Horch worked for Karl Benz from 1896, before founding "A. Horch & Co." in November 1899, in Ehrenfeld, Cologne, Germany.
Manufacturing.
The first Horch automobile was built in 1901. The company moved to Reichenbach in 1902 and Zwickau in 1904. Horch left the company in 1909 after a dispute, and set up in competition in Zwickau. His new firm was initially called "Horch Automobil-Werke GmbH", but following a legal dispute over the "Horch" name, he decided to make another automobile company. (The court decided that "Horch" was a registered trademark on behalf of August Horch's former partners and August Horch was not entitled to use it any more). Consequently, Horch named his new company "Audi Automobilwerke GmbH" in 1910, "Audi" being the Latinization of Horch.
Post Audi.
Horch left Audi in 1920 and went to Berlin and took various jobs. He published his autobiography, "I Built Cars (Ich Baute Autos)" in 1937. He also served on the board of Auto Union, the successor to Audi Automobilwerke GmbH. He was an honorary citizen of Zwickau and had a street named for his Audi cars in both Zwickau and his birthplace Winningen. He was made an honorary professor at Braunschweig University of Technology.

</doc>
<doc id="2070" url="http://en.wikipedia.org/wiki?curid=2070" title="Act of Settlement 1701">
Act of Settlement 1701

The Act of Settlement is an Act of the Parliament of England that was passed in 1701 to settle the succession to the English and Irish crowns and thrones on the Electress Sophia of Hanover (a granddaughter of James VI of Scotland and I of England) and her non-Roman Catholic heirs. Her mother, Princess Elizabeth Stuart, had been born in Scotland but became famous in history as Elizabeth of Bohemia.
The act was prompted by the failure of King William III and Queen Mary II, as well as of Mary's sister Queen Anne, to produce any surviving children, and the Roman Catholic religion of all other members of the House of Stuart. The line of Sophia of Hanover was the most junior among the Stuarts, but consisted of convinced Protestants. Sophia died on 8 June 1714, before the death of Queen Anne on 1 August 1714, at which time Sophia's son duly became King George I and started the Hanoverian dynasty.
The act played a key role in the formation of the Kingdom of Great Britain. England and Scotland had shared a monarch since 1603, but had remained separately governed countries. The Scottish parliament was more reluctant than the English to abandon the House of Stuart, members of which had been Scottish monarchs long before they became English ones. English pressure on Scotland to accept the Act of Settlement was one factor leading to the parliamentary union of the two countries in 1707.
Under the Act of Settlement anyone who becomes a Roman Catholic, or who marries one, becomes disqualified to inherit the throne. The act also placed limits on both the role of foreigners in the British government and the power of the monarch with respect to the Parliament of England, though some of those provisions have been altered by subsequent legislation.
Along with the Bill of Rights 1689, the Act of Settlement remains today one of the main constitutional laws governing the succession not only to the throne of the United Kingdom, but to those of the other Commonwealth realms, whether by assumption or by patriation. The Act of Settlement cannot be altered in any realm except by that realm's own parliament and, by convention, only with the consent of all the other realms, as it touches on the succession to the shared crown.
The original documents are deposited in the Lower Saxon State Archives in Hanover, Germany.
Following the Perth Agreement in 2011, legislation amending the act came into effect across the Commonwealth realms on 26 March 2015.
Original context.
Following the Glorious Revolution, the line of succession to the English throne was governed by the Bill of Rights 1689, which declared that the flight of James II from England to France during the revolution amounted to an abdication of the throne and that James' son-in-law and nephew William of Orange, and his wife, James' daughter, Mary, were James' successors, who ruled jointly as William III and Mary II. The Bill of Rights also provided that the line of succession would go through their descendants, then through Mary's sister Princess Anne, and her descendants, and then to the issue of William III by a later marriage (if he were to marry again after the death of Mary II). During the debate, the House of Lords had attempted to append Sophia and her descendants to the line of succession, but the amendment failed in the Commons.
Mary II died childless in 1694, after which William III did not remarry. In 1700, Prince William, Duke of Gloucester, who was the only child of Princess Anne to survive infancy, died of a fever at the age of 11. Thus, Anne was left as the last remaining legal heir to the throne. The Bill of Rights excluded Catholics from the throne, which ruled out James II and his descendants. However, it also provided for no further succession after Anne. Parliament thus saw the need to settle the succession on Sophia and her descendants, and thereby guarantee the continuity of the Crown in the Protestant line.
Provisions.
The Act of Settlement provided that the throne would pass to the Electress Sophia of Hanover – a granddaughter of James VI of Scotland and I of England, niece of Charles I of Scotland and England – and her Protestant descendants who had not married a Roman Catholic; those who were Roman Catholic, and those who married a Roman Catholic, were barred from ascending the throne "for ever". Eight additional provisions of the act would only come into effect upon the death of both William and Anne:
Effects.
For different reasons, various constitutionalists have praised the Act of Settlement: Henry Hallam called the Act "the seal of our constitutional laws" and David Lindsay Keir placed its importance above the Bill of Rights of 1689. Naamani Tarkow has written: "If one is to make sweeping statements, one may say that, save Magna Carta (more truly, its implications), the Act of Settlement is probably the most significant statute in English history".
Kingdom of Great Britain.
The Act of Settlement was, in many ways, the major cause of the union of Scotland with England and Wales to form the Kingdom of Great Britain. The Parliament of Scotland was not happy with the Act of Settlement and, in response, passed the Act of Security in 1704, through which Scotland reserved the right to choose its own successor to Queen Anne. Stemming from this, the Parliament of England decided that, to ensure the stability and future prosperity of Great Britain, full union of the two parliaments and nations was essential before Anne's death.
It used a combination of exclusionary legislation (the Alien Act of 1705), politics, and bribery to achieve this within three years under the Act of Union 1707. This success was in marked contrast to the four attempts at political union between 1606 and 1689, which all failed owing to a lack of political will in both kingdoms. By virtue of Article II of the Treaty of Union, which defined the succession to the throne of Great Britain, the Act of Settlement became part of Scots Law as well.
Succession to the Crown.
In addition to James II (who died a few months after the Act received the royal assent) and his Roman Catholic children Prince James and the Princess Royal, the Act also excluded the descendants of King James's sister Henrietta, the youngest daughter of Charles I. Henrietta's daughter Anne was then the Queen of Sardinia and a Roman Catholic; subsequent Jacobite pretenders are descended from her.
With the legitimate descendants of Charles I either childless (in the case of William III and Anne) or Roman Catholic, Parliament's choice was limited to the Protestant descendants of Elizabeth of Bohemia, the only other child of King James I not to have died in childhood. Elizabeth had borne nine children who reached adulthood, of whom Sophia was the "youngest". In 1701, in favour of Sophia, Parliament passed over senior living representatives of lines which included Elizabeth Charlotte, Duchess of Orléans, Louis Otto, Prince of Salm and his sisters, Anne Henriette, Princess of Condé, Benedicta Henrietta, Duchess of Brunswick-Lüneburg, and Sophia's sister Louise Hollandine of the Palatinate.
Removal from the succession due to Catholicism.
Since the Act's passing the most senior living member of the Royal Family to have married a Roman Catholic, and thereby to have been removed from the line of succession, is Prince Michael of Kent, who married Baroness Marie-Christine von Reibnitz in 1978; he was fifteenth in the line of succession at the time. He was restored to the line of succession in 2015 when the Succession to the Crown Act 2013 came into force, and became 34th in line.
The next most senior living descendant of the Electress Sophia who had been ineligible to succeed on this ground is George Windsor, Earl of St Andrews, the eldest son of Prince Edward, Duke of Kent, who married the Roman Catholic Sylvana Palma Tomaselli in 1988. His son, Lord Downpatrick, converted to Roman Catholicism in 2003 and is the most senior descendant of Sophia to be barred as a result of his religion. More recently, Peter Phillips, the son of Anne, Princess Royal, and eleventh in line to the throne, married Autumn Kelly; Kelly had been brought up as a Roman Catholic, but she converted to Anglicanism prior to the wedding. Had she not done so, Phillips would have forfeited his place in the succession upon their marriage.
Excluding those princesses who have married into Roman Catholic royal families, such as Marie of Edinburgh, Victoria Eugenie of Battenberg and Princess Beatrice of Edinburgh, one member of the Royal Family (that is, with the style of "Royal Highness") has converted to Roman Catholicism since the passage of the Act: the Duchess of Kent, wife of Prince Edward, Duke of Kent who converted on 14 January 1994, but her husband did not lose his place in the succession because she was an Anglican at the time of their marriage.
The abdication of Edward VIII.
Under the Act of Settlement, male-preference primogeniture succession of an Anglican legitimate descendant of the Electress Sophia is automatic and immediate, neither depending on, nor waiting for, any proclamation. Thus, during the abdication crisis of 1936, caused by Edward VIII's desire to marry Wallis Simpson, the consent of all realms, along with, in some cases, new acts of parliament, was required to allow for Edward's stepping aside and the exclusion of any potential children of his marriage. In the United Kingdom, His Majesty's Declaration of Abdication Act was, with the consent of the Australian, Canadian, New Zealand, and South African governments, passed through Parliament; and the Crown thus passed to the next-in-line descendant of Sophia: Edward's brother, Prince Albert, Duke of York. The Irish Free State legislated independently. To formalise its government's consent to the abdication, the Canadian parliament passed, the following year, the Succession to the Throne Act (1 Geo. VI, c.16) and South Africa took a similar course of action.
Present status.
In the Australian Capital Territory, the Act of Settlement was, on 11 May 1989, converted, from an act of the Parliament of England into an ACT enactment, by section 34(4) of the Australian Capital Territory (Self-Government) Act 1988 (Cwlth), and then renamed "The Act of Settlement 1700" by the Legislation Act 2001.
Amendment proposals.
Challenges have been made against the Act of Settlement, especially its provisions regarding Roman Catholics and preference for males. However, legislating for alterations to the Act is a complex process, since the Act is a common denominator in the shared succession of all the Commonwealth realms. The Statute of Westminster 1931 acknowledges by established convention that any changes to the rules of succession may be made only with the agreement of all of the states involved, with concurrent amendments to be made by each state's parliament or parliaments. Further, as the current monarch's eldest child and, in turn, his eldest child, are Anglican males, any change to the succession laws would have no immediate implications. Consequently, there was little public concern with the issues and debate had been confined largely to academic circles until, in November 2010, the announcement that Prince William was to marry. This raised the question of what would happen if he were to produce first a daughter and then a son.
"The Times" reported on 6 November 1995 that Prince Charles had said on that day to Tony Blair and Paddy Ashdown that "Catholics should be able to ascend to the British throne". Ashdown claimed the Prince said: "I really can't think why we can't have Catholics on the throne". In 1998, during debate on a Succession to the Crown Bill, Junior Home Office Minister Lord Williams of Mostyn informed the House of Lords that the Queen had "no objection to the Government's view that in determining the line of succession to the throne, daughters and sons should be treated in the same way".
Australia.
In October 2011 the Australian federal government was reported to have reached an agreement with all of the states on potential changes to their laws in the wake of amendments to the Act of Settlement. The practice of the Australian states—for example, New South Wales and Victoria—has been, when legislating to repeal some imperial statutes so far as they still applied in Australia, to provide that imperial statutes concerning the royal succession remain in force.
The legal process required at the federal level remains unclear. The Australian constitution, as was noted during the crisis of 1936, contains no power for the federal parliament to legislate with respect to the monarchy. Everything thus turns upon the status and meaning of clause 2 in the Commonwealth of Australia Constitution Act 1900, which provides: "The provisions of this Act referring to the Queen shall extend to Her Majesty's heirs and successors in the sovereignty of the United Kingdom." Anne Twomey reviews three possible interpretations of the clause. First: it "mandates that whoever is the sovereign of the United Kingdom is also, by virtue of this external fact, sovereign of Australia"; accordingly, changes to British succession laws would have no effect on Australian law, but if the British amendment changed the sovereign, then the new sovereign of the United Kingdom would automatically become the new sovereign of Australia. Second, it is "merely an interpretative provision", operating to ensure that references to "the Queen" in the Constitution are references to whoever may at the time be the incumbent of the "sovereignty of the United Kingdom" as determined with regard to Australia, following the Australia Act 1986, by Australian law. Or, third, it incorporates the United Kingdom rules of succession into the Commonwealth of Australia Constitution Act, which itself can now be altered only by Australia, according to the Australia Act 1986; in that way, the British rules of succession have been patriated to Australia and, with regard to Australia, are subject to amendment or repeal solely by Australian law. However, Twomey expresses confidence that, if the High Court of Australia were to be faced with the problems of covering clause 2, it would find some way to conclude that, with regard to Australia, the clause is subject solely to Australian law. Canadian scholar Richard Toporoski theorised in 1998 that "if, let us say, an alteration were to be made in the United Kingdom to the Act of Settlement 1701, providing for the succession of the Crown... [i]t is my opinion that the domestic constitutional law of Australia or Papua New Guinea, for example, would provide for the succession in those countries of the same person who became Sovereign of the United Kingdom."
Canada.
In Canada, where the Act of Settlement is now a part of Canadian constitutional law, Tony O'Donohue, a Canadian civic politician, took issue with the provisions that exclude Roman Catholics from the throne, and which make the monarch of Canada the Supreme Governor of the Church of England, requiring him or her to be an Anglican. This, he claimed, discriminated against non-Anglicans, including Catholics, who are the largest faith group in Canada. In 2002, O'Donohue launched a court action that argued the Act of Settlement violates the Canadian Charter of Rights and Freedoms, but the case was dismissed by the court. It found that, as the Act of Settlement is part of the Canadian constitution, the Charter of Rights and Freedoms, as another part of the same constitution, does not have supremacy over it. Also, the court noted that, while Canada has the power to amend the line of succession to the Canadian throne, the Statute of Westminster stipulates that the agreement of the governments of the fifteen other Commonwealth realms that share the Crown would first have to be sought if Canada wished to continue its relationship with these countries. An appeal of the decision was dismissed on 16 March 2005. Some commentators state that, as a result of this, any single provincial legislature could hinder any attempts to change this Act, and by extension, to the line of succession for the shared crown of all 16 Commonwealth realms. Others contend that that is not the case, and changes to the succession instituted by an Act of the Parliament of Canada "[accord] with the convention of symmetry that preserves the personal unity of the British and Dominion Crowns."
With the announcement in 2007 of the engagement of Peter Phillips to Autumn Kelly, a Roman Catholic and a Canadian, discussion about the Act of Settlement was revived. Norman Spector called in "The Globe and Mail" for Prime Minister Stephen Harper to address the issue of the Act's bar on Catholics, saying that Phillips' marriage to Kelly would be the first time the provisions of the act would bear directly on Canada—Phillips would be barred from acceding to the Canadian throne because he married a Roman Catholic Canadian. (In fact, the Earl of St. Andrews had already lost his place in the line of succession when he married the Roman Catholic Canadian Sylvana Palma Tomaselli in 1988, but St. Andrews' place in the line of succession was significantly lower than Phillips'.) Criticism of the Act of Settlement due to the Phillips-Kelly marriage was muted when Autumn Kelly converted to Anglicanism shortly before her marriage, thus preserving her husband's place in the line of succession.
United Kingdom.
From time to time there has been debate over repealing the clause that prevents Roman Catholics, or those who marry one, from ascending to the British throne. Proponents of repeal argue that the clause is a bigoted anachronism; Cardinal Winning, who was leader of the Roman Catholic Church in Scotland, called the act an "insult" to Catholics. Cardinal Murphy-O'Connor, the leader of the Roman Catholic Church in England, pointed out that Prince William (later the Duke of Cambridge) "can marry by law a Hindu, a Buddhist, anyone, but not a Roman Catholic". Opponents of repeal, such as Enoch Powell and Adrian Hilton, believe that it would lead to the disestablishment of the Church of England as the state religion if a Roman Catholic were to come to the throne. They also note that the monarch must swear to defend the faith and be a member of the Anglican Communion, but that a Roman Catholic monarch would, like all Roman Catholics, owe allegiance to the Pope. This would, according to opponents of repeal, amount to a loss of sovereignty for the Anglican Church.
When in December 1978 there was media speculation that Prince Charles might marry a Roman Catholic, Powell defended the provision that excludes Roman Catholics from ascending the throne, claiming his objection was not rooted in religious bigotry but in political considerations. He stated a Roman Catholic monarch would mean the acceptance of a source of authority external to the realm and "in the literal sense, foreign to the Crown-in-Parliament... Between Roman Catholicism and royal supremacy there is, as St Thomas More concluded, no reconciliation". Powell concluded that a Roman Catholic crown would be the destruction of the Church of England because "it would contradict the essential character of that church".
He continued:
When Thomas Hobbes wrote that "the Papacy is no other than the ghost of the deceased Roman Empire sitting crowned upon the grave thereof", he was promulgating an enormously important truth. Authority in the Roman Church is the exertion of that "imperium" from which England in the 16th century finally and decisively declared its national independence as the "alter imperium", the "other empire", of which Henry VIII declared "This realm of England is an empire" ... It would signal the beginning of the end of the British monarchy. It would portend the eventual surrender of everything that has made us, and keeps us still, a nation.
The Scottish Parliament unanimously passed a motion in 1999 calling for the complete removal of any discrimination linked to the monarchy and the repeal of the Act of Settlement. The following year, "The Guardian" challenged the succession law in court, claiming that it violated the European Convention on Human Rights, which provides,
"The enjoyment of the rights and freedoms set forth in this Convention shall be secured without discrimination on any ground such as sex, race, colour, language, religion, political or other opinion, national or social origin, association with a national minority, property, birth or other status."As the Convention nowhere lists the right to succeed to the Crown as a human right, the challenge was rejected.
Adrian Hilton, writing in "The Spectator" in 2003, defended the Act of Settlement as not "irrational prejudice or blind bigotry," but claimed that it was passed because "the nation had learnt that when a Roman Catholic monarch is upon the throne, religious and civil liberty is lost." He points to the Pope's claiming universal jurisdiction, and Hilton argues that "it would be intolerable to have, as the sovereign of a Protestant and free country, one who owes any allegiance to the head of any other state" and contends that, if such situation came about, "we will have undone centuries of common law." He said that because the Roman Catholic Church does not recognise the Church of England as an apostolic church, a Roman Catholic monarch who abided by their faith's doctrine would be obliged to view Anglican and Church of Scotland archbishops, bishops, and clergy as part of the laity and therefore "lacking the ordained authority to preach and celebrate the sacraments." (Hilton noted that the Church of Scotland's Presbyterian polity does not include bishops or archbishops.) Hilton said a Roman Catholic monarch would be unable to be crowned by the Archbishop of Canterbury and notes that other European states have similar religious provisions for their monarchs: Denmark, Norway, and Sweden, whose constitutions compel their monarchs to be Lutherans; the Netherlands, which has a constitution requiring its monarchs be members of the Protestant House of Orange; and Belgium, which has a constitution that provides for the succession to be through Roman Catholic houses.
In December 2004, a private member's bill—the Succession to the Crown Bill—was introduced in the House of Lords. The government, headed by Tony Blair, blocked all attempts to revise the succession laws, claiming it would raise too many constitutional issues and it was unnecessary at the time. In the British general election the following year, Michael Howard promised to work towards having the prohibition removed if the Conservative Party gained a majority of seats in the House of Commons, but the election was won by Blair's Labour Party. Four years later, plans drawn up by Chris Bryant were revealed that would end the exclusion of Catholics from the throne and end the doctrine of agnatic (male-preference) primogeniture in favour of absolute primogeniture, which governs succession solely on birth order and not on sex. The issue was raised again in January 2009, when a private members bill to amend the Act of Succession was introduced in parliament.
Across the realms.
In early 2011 Keith Vaz, a Labour Member of Parliament, introduced to the House of Commons at Westminster a private member's bill which proposed that the Act of Settlement be amended to remove the provisions relating to Roman Catholicism and change the primogeniture governing the line of succession to the British throne from agnatic to absolute cognatic. Vaz sought support for his project from the Canadian Cabinet and Prime Minister Stephen Harper, but the Office of the Prime Minister of Canada responded that the issue was "not a priority for the government or for Canadians without further elaboration on the merits or drawbacks of the proposed reforms." Stephenson King, Prime Minister of Saint Lucia, said he supported the idea and it was reported that the government of New Zealand did, as well. The Monarchist League of Canada said at the time to the media that it "supports amending the Act of Settlement in order to modernize the succession rules."
Later the same year, the Deputy Prime Minister of the United Kingdom, Nick Clegg, announced that the government was considering a change in the law. At approximately the same time, it was reported that British Prime Minister David Cameron had written to each of the prime ministers of the other fifteen Commonwealth realms, asking for their support in changing the succession to absolute primogeniture and notifying them he would raise his proposals at that year's Commonwealth Heads of Government Meeting (CHOGM) in Perth, Australia. Cameron reportedly also proposed removing the restriction on successors being or marrying Roman Catholics; however, potential Roman Catholic successors would be required to convert to Anglicanism prior to acceding to the throne. In reaction to the letter and media coverage, Harper stated that, this time, he was "supportive" of what he saw as "reasonable modernizations".
At CHOGM on 28 October 2011, the prime ministers of the other Commonwealth realms agreed to support Cameron's proposed changes to the Act. The bill put before the Parliament of the United Kingdom would act as a model for the legislation required to be passed in at least some of the other realms, and any changes would only first take effect if the Duke and Duchess of Cambridge were to have a daughter before a son.
The British group Republic asserted that succession reform would not make the monarchy any less discriminatory. As it welcomed the gender equality reforms, the British newspaper "The Guardian" criticized the lack of a proposal to remove the ban on Catholics sitting on the throne, as did Alex Salmond, First Minister of Scotland, who pointed out that "It is deeply disappointing that the reform [of the Act of Settlement of 1701] has stopped short of removing the unjustifiable barrier on a Catholic becoming monarch". On the subject, Cameron asserted: "Let me be clear, the monarch must be in communion with the Church of England because he or she is the head of that Church."

</doc>
<doc id="2140" url="http://en.wikipedia.org/wiki?curid=2140" title="Atlanta Braves">
Atlanta Braves

The Atlanta Braves are a Major League Baseball (MLB) team in Atlanta, Georgia, playing in the Eastern Division of the National League. The Braves have played home games at Turner Field since 1997 and play spring training games in Lake Buena Vista, Florida. In 2017, the team is to move to SunTrust Park, a new stadium complex in the Cumberland district of Cobb County just north of the I-285 bypass.
The "Braves" name, which was first used in 1912, originates from a term for a Native American warrior. They are nicknamed "the "Bravos"", and often referred to as "America's Team" in reference to the team's games being broadcast on the nationally available TBS from the 1970s until 2007, giving the team a wide fan base.
From 1991 to 2005 the Braves were one of the most successful franchises in baseball, winning division titles an unprecedented 14 consecutive times in that period (omitting the strike-shortened 1994 season in which there were no official division champions). The Braves won the NL West 1991–93 and the NL East 1995–2005, and they returned to the playoffs as the National League Wild Card in 2010. The Braves advanced to the World Series five times in the 1990s, winning the title in 1995. Since their debut in the National League in 1876, the franchise has won 16 divisional titles, 17 National League pennants, and three World Series championships—in 1914 as the Boston Braves, in 1957 as the Milwaukee Braves, and in 1995 in Atlanta. The Braves are the only Major League Baseball franchise to have won the World Series in three different home cities.
The club is one of the National League's two remaining charter franchises (the other being the Chicago Cubs) and was founded in Boston, Massachusetts, in 1871 as the Boston Red Stockings (not to be confused with the American League's Boston Red Sox). They are considered "the oldest continuously playing team in major North American sports." There is an argument as to which team is actually older, because, although the Cubs are a full season "older" (formed as the Chicago White Stockings in 1870), Chicago did not sponsor a White Stockings team for two seasons due to the Great Chicago Fire; therefore, the Braves have played more consecutive seasons.
After various name changes, the team eventually began operating as the Boston Braves, which lasted for most of the first half of the 20th century. Then, in 1953, the team moved to Milwaukee, Wisconsin and became the Milwaukee Braves, followed by the final move to Atlanta in 1966. The team's tenure in Atlanta is noted for Hank Aaron breaking Babe Ruth's career home run record in 1974.
History.
Boston.
1870–1913.
The Cincinnati Red Stockings, established in 1869 as the first openly all-professional baseball team, voted to dissolve after the 1870 season. Player-manager Harry Wright, with brother George and two other Cincinnati players, then went to Boston Massachusetts at the invitation of Boston Red Stockings founder Ivers Whitney Adams to form the nucleus of the "Boston Red Stockings", a charter member of the National Association of Professional Base Ball Players (NAPBBP). The original Boston Red Stockings team and its successors can lay claim to being the oldest continuously playing team in American professional sports. (The only other team that has been organized as long, the Chicago Cubs, did not play for the two years following the Great Chicago Fire of 1871.) Two young players hired away from the Forest City club of Rockford, Illinois, turned out to be the biggest stars during the NAPBBP years: pitcher Al Spalding (founder of Spalding sporting goods) and second baseman Ross Barnes.
Led by the Wright brothers, Barnes, and Spalding, the Red Stockings dominated the National Association, winning four of that league's five championships. The team became one of the National League's charter franchises in 1876, sometimes called the "Red Caps" (as a new Cincinnati Red Stockings club was another charter member). Boston came to be called the "Beaneaters" in 1883, while retaining red as the team color.
The Boston Red Caps played in the first game in the history of the National League, on Saturday, April 22, 1876, defeating the Athletics, 6-5.
Although somewhat stripped of talent in the National League's inaugural year, Boston bounced back to win the 1877 and 1878 pennants. The Red Caps/Beaneaters were one of the league's dominant teams during the 19th century, winning a total of eight pennants. For most of that time, their manager was Frank Selee. The 1898 team finished 102–47, a club record for wins that would stand for almost a century. Stars of those 1890s Beaneater teams included the "Heavenly Twins", Hugh Duffy and Tommy McCarthy, as well as "Slidin'" Billy Hamilton.
The team was decimated when the American League's new Boston entry set up shop in 1901. Many of the Beaneaters' stars jumped to the new team, which offered contracts that the Beaneaters' owners did not even bother to match. They only managed one winning season from 1900 to 1913, and lost 100 games five times. In 1907, the Beaneaters (temporarily) eliminated the last bit of red from their stockings because their manager thought the red dye could cause wounds to become infected (as noted in "The Sporting News Baseball Guide" during the 1940s when each team's entry had a history of its nickname(s). See details in History of baseball team nicknames). The American League club's owner, Charles Taylor, wasted little time in adopting Red Sox as his team's first official nickname (up to that point they had been called by the generic "Americans"). Media-driven nickname changes to the "Doves" in 1907 and the "Rustlers" in 1911 did nothing to change the National League club's luck. The team became the "Braves" for the first time in 1912. Their owner, James Gaffney, was a member of New York City's political machine, Tammany Hall, which used an Indian chief as their symbol.
1914: Miracle.
Two years later, the Braves put together one of the most memorable seasons in baseball history. After a dismal 4–18 start, the Braves seemed to be on pace for a last place finish. On July 4, 1914, the Braves lost both games of a doubleheader to the Brooklyn Dodgers. The consecutive losses put their record at 26–40 and the Braves were in last place, "15 games" behind the league-leading New York Giants, who had won the previous three league pennants. After a day off, the Braves started to put together a hot streak, and from July 6 through September 5, the Braves went 41–12. On September 7 and 8, the Braves took two of three from the New York Giants and moved into first place. The Braves tore through September and early October, closing with 25 wins against six losses, while the Giants went 16–16. They were the only team, under the old eight-team league format, to win a pennant after being in last place on the Fourth of July. They were in last place as late as July 18, but were close to the pack, moving into fourth on July 21 and second place on August 12.
Despite their amazing comeback, the Braves entered the World Series as a heavy underdog to Connie Mack's Philadelphia A's. Nevertheless, the Braves swept the Athletics—the first unqualified sweep in the young history of the modern World Series (the 1907 Series had one tied game) to win the world championship. Meanwhile, Johnny Evers won the Chalmers Award.
The Braves played the World Series (as well as the last few games of the 1914 season) at Fenway Park, since their normal home, the South End Grounds, was too small. However, the Braves' success inspired owner Gaffney to build a modern park, Braves Field, which opened in August 1915. It was the largest park in the majors at the time, with 40,000 seats and a very spacious outfield. The park was novel for its time; public transportation brought fans right to the park.
1915–1953.
After contending for most of 1915 and 1916, the Braves only twice posted winning records from 1917 to 1932. The lone highlight of those years came when Judge Emil Fuchs bought the team in 1923 to bring his longtime friend, pitching great Christy Mathewson, back into the game. However, Mathewson died in 1925, leaving Fuchs in control of the team.
Fuchs was committed to building a winner, but the damage from the years prior to his arrival took some time to overcome. The Braves finally managed to be competitive in 1933 and 1934 under manager Bill McKechnie, but Fuchs' revenue was severely depleted due to the Great Depression.
Looking for a way to get more fans and more money, Fuchs worked out a deal with the New York Yankees to acquire Babe Ruth, who had started his career with the Red Sox. Fuchs made Ruth team vice president, and promised him a share of the profits. He was also granted the title of assistant manager, and was to be consulted on all of the Braves' deals. Fuchs even suggested that Ruth, who had long had his heart set on managing, could take over as manager once McKechnie stepped down—perhaps as early as 1936.
At first, it appeared that Ruth was the final piece the team needed in 1935. On opening day, he had a hand in all of the Braves' runs in a 4–2 win over the Giants. However, that proved to be the only time the Braves were over .500 all year. Events went downhill quickly. While Ruth could still hit, he could do little else. He could not run, and his fielding was so terrible that three of the Braves' pitchers threatened to go on strike if Ruth were in the lineup. It soon became obvious that he was vice president and assistant manager in name only and Fuchs' promise of a share of team profits was hot air. In fact, Ruth discovered that Fuchs expected him to invest some of "his" money in the team.
Seeing a franchise in complete disarray, Ruth retired on June 1—only six days after he clouted what turned out to be the last three home runs of his career. He had wanted to quit as early as May 12, but Fuchs wanted him to hang on so he could play in every National League park. The Braves finished 38–115, the worst season in franchise history. Their .248 winning percentage is the third-worst in baseball history, and the second-worst in National League history (behind only the 1899 Cleveland Spiders).
Fuchs lost control of the team in August 1935, and the new owners tried to change the team's image by renaming it the "Boston Bees". This did little to change the team's fortunes. After five uneven years, a new owner, construction magnate Lou Perini, changed the nickname back to the Braves. He immediately set about rebuilding the team. World War II slowed things down a little, but the team rode the pitching of Warren Spahn to impressive seasons in 1946 and 1947.
In 1948, the team won the pennant, behind the pitching of Spahn and Johnny Sain, who won 39 games between them. The remainder of the rotation was so thin that in September, "Boston Post" writer Gerald Hern wrote this poem about the pair:
The poem received such a wide audience that the sentiment, usually now paraphrased as "Spahn and Sain and pray for rain", entered the baseball vocabulary. However, in the 1948 season, the Braves actually had the same record in games that Spahn and Sain started that the team had overall, in terms of winning percentage.
The 1948 World Series, which the Braves lost in six games to the Indians, turned out to be the Braves' last hurrah in Boston. In 1950, Sam Jethroe became the team's first African American player, making his major league debut on April 18. Amid four mediocre seasons, attendance steadily dwindled until, on March 13, 1953, Perini, who had recently bought out his original partners, announced he was moving the team to Milwaukee, where the Braves had their top farm club, the Brewers. Milwaukee had long been a possible target for relocation. Bill Veeck had tried to return his St. Louis Browns there earlier the same year (Milwaukee was the original home of that franchise), but his proposal had been voted down by the other American League owners.
Milwaukee (1953–1965).
Milwaukee went wild over the Braves, who were welcomed as genuine heroes. The Braves finished 92–62 in their first season in Milwaukee, and drew a then-NL record 1.8 million fans. The success of the team was noted by many owners. Not coincidentally, the Philadelphia Athletics, St. Louis Browns, Brooklyn Dodgers, and New York Giants would leave their original hometowns within the next five years.
As the 1950s progressed, the reinvigorated Braves became increasingly competitive. Sluggers Eddie Mathews and Hank Aaron drove the offense (they would hit a combined 1,226 home runs as Braves, with 850 of those coming while the franchise was in Milwaukee), while Warren Spahn, Lew Burdette, and Bob Buhl anchored the rotation. The 1956 Braves finished second, only one game behind the Brooklyn Dodgers.
In 1957, the Braves celebrated their first pennant in nine years spearheaded by Aaron's MVP season, as he led the National League in home runs and RBI. Perhaps the most memorable of his 44 round-trippers that season came on September 23, a two-run walk-off home run that gave the Braves a 4–2 victory over the St. Louis Cardinals and clinched the League championship. The team then went on to its first World Series win in over 40 years, defeating the New York Yankees of Berra, Mantle, and Ford in seven games. Burdette, the Series MVP, threw three complete game victories, giving up only two earned runs.
In 1958, the Braves again won the National League pennant and jumped out to a three games to one lead in the World Series against the New York Yankees once more, thanks in part to the strength of Spahn's and Burdette's pitching. But the Yankees stormed back to take the last three games, in large part to World Series MVP Bob Turley's pitching.
The 1959 season saw the Braves finish the season in a tie with the Los Angeles Dodgers, both with 86-68 records. Many residents of Chicago and Milwaukee were hoping for a Sox-Braves Series, as the cities are only about 75 mi apart, but it was not to be because Milwaukee fell in a best-of-3 playoff with two straight losses to the Dodgers. The Dodgers would go on to defeat the Chicago White Sox in the World Series.
The next six years were up-and-down for the Braves. The 1960 season featured two no-hitters by Burdette and Spahn, and Milwaukee finished seven games behind the Pittsburgh Pirates, who ultimately were to win the World Series that year, in second place, one year after the Braves were on the winning end of the 13-inning near-perfect game of Pirates pitcher Harvey Haddix. The 1961 season saw a drop in the standings for the Braves down to fourth, despite Spahn recording his 300th victory and pitching another no-hitter that year.
Aaron hit 45 home runs in 1962, a Milwaukee career high for him, but this did not translate into wins for the Braves, as they finished fifth. The next season, Aaron again hit 44 home runs and notched 130 RBI, and Spahn was once again the ace of the staff, going 23–7. However, none of the other Braves produced at that level, and the team finished in the lower half of the league, or "second division", for the first time in its short history in Milwaukee.
The Braves were somewhat mediocre as the 1960s began, but fattened up on the expansion New York Mets and Houston Colt .45s. To this day, the Milwaukee Braves are the only major league team who played more than one season and never had a losing record.
Perini sold the Braves to a Chicago-based group led by William Bartholomay in 1962. Almost immediately Bartholomay started shopping the Braves to a larger television market. Keen to attract them, the fast-growing city of Atlanta, led by Mayor
Ivan Allen, Jr. constructed a new $18 million, 52,000-seat ballpark in less than one year, Atlanta Stadium, which was officially opened in 1965 in hopes of luring an existing major league baseball and/or NFL/AFL team. After the city failed to lure the Kansas City A's to Atlanta (the A's would move to Oakland in 1968), the Braves announced their intention to move to Atlanta for the 1965 season. However, an injunction filed in Wisconsin kept the Braves in Milwaukee for one final year. In 1966, the Braves completed the move to Atlanta.
Eddie Mathews is the only Braves player to have played for the organization in all three cities that they have been based in. Mathews played with the Braves for their last season in Boston, the team's entire tenure in Milwaukee, and their first season in Atlanta.
Atlanta.
1966–1974.
The Braves were a .500 team in their first few years in Atlanta; 85–77 in 1966, 77–85 in 1967, and 81–81 in 1968. The 1967 season was the Braves' first losing season since 1952, their last year in Boston. In 1969, with the onset of divisional play, the Braves won the first-ever National League West Division title, before being swept by the "Miracle Mets" in the National League Championship Series. They would not be a factor during the next decade, posting only two winning seasons between 1970 and 1981 – in some cases, fielding teams as bad as the worst Boston teams.
In the meantime, fans had to be satisfied with the achievements of Hank Aaron. In the relatively hitter-friendly confines and higher-than-average altitude of Atlanta Stadium ("The Launching Pad"), he actually increased his offensive production. Atlanta also produced batting champions in Rico Carty (in 1970) and Ralph Garr (in 1974). In the shadow of Aaron's historical home run pursuit, was the fact that three Atlanta sluggers hit 40 or more home runs in 1973 – Darrell Evans, Davey Johnson and, of course, Aaron.
By the end of the 1973 season, Aaron had hit 713 home runs, one short of Ruth's record. Throughout the winter he received racially motivated death threats, but stood up well under the pressure. The next season, it was only a matter of time before he set a new record. On April 4, opening day, he hit No.714 in Cincinnati, and on April 8, in front of his home fans and a national television audience he finally beat Ruth's mark with a home run to left-center field off left-hander Al Downing of the Los Angeles Dodgers. Aaron spent most of his career as a Milwaukee and Atlanta Brave before asking to be traded to the Milwaukee Brewers, while Ruth finished his career as a Boston Brave. In fact, until Barry Bonds eclipsed the 714 home runs hit by Babe Ruth in 2006, the top two home run hitters in Major League history had at one time been Braves.
1976–77: Ted Turner buys the team.
In 1976, the team was purchased by media magnate Ted Turner, owner of superstation WTBS, as a means to keep the team (and one of his main programming staples) in Atlanta. The financially strapped Turner used money already paid to the team for their broadcast rights as a down-payment. It was then that Atlanta Stadium was renamed Atlanta-Fulton County Stadium. Turner quickly gained a reputation as a quirky, hands-on baseball owner. On May 11, 1977, Turner appointed himself manager, but because MLB passed a rule in the 1950s barring managers from holding a financial stake in their teams, Turner was ordered to relinquish that position after one game (the Braves lost 2–1 to the Pittsburgh Pirates to bring their losing streak to 17 games).
Turner used the Braves as a major programming draw for his fledgling cable network, making the Braves the first franchise to have a nationwide audience and fan base. WTBS marketed the team as "The Atlanta Braves: America's Team", a nickname that still sticks in some areas of the country, especially the South. Among other things, in 1976 Turner suggested the nickname "Channel" for pitcher Andy Messersmith and jersey number 17, in order to promote the television station that aired Braves games. Major League Baseball quickly nixed the idea.
1978–1990.
After three straight losing seasons, Bobby Cox was hired for his first stint as manager for the 1978 season. He promoted 22-year-old slugger Dale Murphy into the starting lineup. Murphy hit 77 home runs over the next three seasons, but he struggled on defense, unable to adeptly play either catcher or first base. In 1980, Murphy was moved to center field and demonstrated excellent range and throwing ability, while the Braves earned their first winning season since 1974. Cox was fired after the 1981 season and replaced with Joe Torre, under whose leadership the Braves attained their first divisional title since 1969. Strong performances from Bob Horner, Chris Chambliss, pitcher Phil Niekro, and short relief pitcher Gene Garber helped the Braves, but no Brave was more acclaimed than Murphy, who won both a Most Valuable Player and a Gold Glove award. Murphy also won an MVP award the following season, but the Braves began a period of decline that defined the team throughout the 1980s. Murphy, excelling in defense, hitting, and running, was consistently recognized as one of the league's best players, but the Braves averaged only 65 wins per season between 1985 and 1990. Their lowest point came in 1988, when they lost 106 games. The 1986 season saw the return of Bobby Cox as general manager. Also in 1986, the team stopped using their Native American-themed mascot, Chief Noc-A-Homa.
1991–2004: Division dominance.
1991–1994.
Cox returned to the dugout as manager in the middle of the 1990 season, replacing Russ Nixon. The Braves finished the year with the worst record in baseball, at 65–97. They traded Dale Murphy to the Philadelphia Phillies after it was clear he was becoming a less dominant player. Pitching coach Leo Mazzone began developing young pitchers Tom Glavine, Steve Avery, and John Smoltz into future stars. That same year, the Braves used the number one overall pick in the 1990 MLB Draft to select Chipper Jones, who became one of the best hitters in team history. Perhaps the Braves' most important move was not on the field, but in the front office. Immediately after the season, John Schuerholz was hired away from the Kansas City Royals as general manager.
The following season, Glavine, Avery, and Smoltz would be recognized as the best young pitchers in the league, winning 52 games among them. Meanwhile, behind position players David Justice, Ron Gant and unexpected league Most Valuable Player and batting champion Terry Pendleton, the Braves overcame a 39–40 start, winning 55 of their final 83 games over the last three months of the season and edging the Los Angeles Dodgers by one game in one of baseball's more memorable playoff races. The "Worst to First" Braves, who had not won a divisional title since 1982, captivated the city of Atlanta (and the entire southeast) during their improbable run to the flag. They defeated the Pittsburgh Pirates in a very tightly contested seven-game NLCS only to lose the World Series, also in seven games, to the Minnesota Twins. The series, considered by many to be one of the greatest ever, was the first time a team that had finished last in its division one year went to the World Series the next; both the Twins and Braves accomplished the feat.
Despite the 1991 World Series loss, the Braves' success would continue. In 1992, the Braves returned to the NLCS and once again defeated the Pirates in seven games, culminating in a dramatic game seven win. Francisco Cabrera's two-out single that scored David Justice and Sid Bream capped a three-run rally in the bottom of the ninth inning that gave the Braves a 3–2 victory. It was the first time in post season history that the tying and winning runs had scored on a single play in the ninth inning. The Braves lost the World Series to the Toronto Blue Jays, however. In 1993, the Braves signed Cy Young Award winning pitcher Greg Maddux from the Chicago Cubs, leading many baseball insiders to declare the team's pitching staff the best in baseball. The 1993 team posted a franchise-best 104 wins after a dramatic pennant race with the San Francisco Giants, who won 103 games. The Braves needed a stunning 55–19 finish to edge out the Giants, who led the Braves by nine games in the standings as late as August 11. However, the Braves fell in the NLCS to the Philadelphia Phillies in six games.
In 1994, in a realignment of the National League's divisions following the 1993 expansion, the Braves moved to the Eastern Division. This realignment was the main cause of the team's heated rivalry with the New York Mets during the mid-to-late 1990s.
The player's strike cut short the 1994 season, prior to the division championships, with the Braves six games behind the Montreal Expos with 48 games left to play.
1995–2004.
The Braves returned strong the following strike-shortened (144 games instead of the customary 162) year and beat the Cleveland Indians in the 1995 World Series. This squelched claims by many Braves critics that they were the "Buffalo Bills of Baseball" (January 1996 issue of "Beckett Baseball Card Monthly"). With this World Series victory, the Braves became the first team in Major League Baseball to win world championships in three different cities. With their strong pitching as a constant, the Braves appeared in the 1996 and 1999 World Series (losing both to the New York Yankees, managed by Joe Torre, a former Braves manager), and had a streak of division titles from 1991 to 2005 (three in the Western Division and eleven in the Eastern) interrupted only in 1994 when the strike ended the season early. Pitching was not the only constant in the Braves organization —Cox was the Braves' manager, while Schuerholz remained the team's GM until after the 2007 season when he was promoted to team president. Terry Pendleton finished his playing career elsewhere, but returned to the Braves system as the hitting coach.
In October 1996, Time Warner acquired Ted Turner's Turner Broadcasting System and all of its assets, including its cable channels and the Atlanta Braves. Over the next few years, Ted Turner's presence as owner of the team would diminish.
A 95–67 record in 2000 produced a ninth consecutive division title. However, a sweep at the hands of the St. Louis Cardinals prevented the Braves from reaching the National League Championship Series for a ninth consecutive time.
In 2001, Atlanta won the National League East division yet again, swept the Houston Astros in the NLDS, then lost to the Arizona Diamondbacks in the National League Championship Series four games to one. One memorable game the Braves played that year came on September 21, when they played rival New York Mets in the first major professional sporting event held in New York City since 9/11.
In 2002, 2003 and 2004, the Braves won the Eastern division again, but lost in the NLDS in all three years in the same fashion: 3 games to 2 to the San Francisco Giants, Chicago Cubs, and Houston Astros.
Cy Young dominance.
Six National League Cy Young Awards in the 1990s were awarded to three Braves pitchers:
2005: A new generation.
In 2005, the Braves won the Division championship for the fourteenth consecutive time from 1991 to 2005. Fourteen consecutive division titles stands as the record for all major league baseball. The 2005 title marked the first time any MLB team made the postseason with more than 4 rookies who each had more than 100 ABs (Wilson Betemit, Brian McCann, Pete Orr, Ryan Langerhans, Jeff Francoeur). Catcher Brian McCann, right fielder Jeff Francoeur, and pitcher Kyle Davies all grew up in the suburbs of Atlanta. The large number of rookies to debut in 2005 were nicknamed the "Baby Braves" by fans and became an Atlanta-area sensation, helping to lead the club to a record of 90–72.
However, the season would end on a sour note as the Braves lost the National League Division Series to the Astros in four games. In Game 4, with the Braves leading by 5 in the eighth inning, the Astros battled back with a Lance Berkman grand slam and a two-out, ninth inning Brad Ausmus home run off of Braves closer Kyle Farnsworth. The game did not end until the 18th inning, becoming the longest game in playoff history at 5 hours 50 minutes. Chris Burke ended the marathon with a home run off of Joey Devine.
After the 2005 season, the Braves lost their long-time pitching coach Leo Mazzone, who left to go to the Baltimore Orioles. Roger McDowell took his place in the Atlanta dugout. Unable to re-sign shortstop Rafael Furcal, the Braves acquired shortstop Edgar Rentería from the Boston Red Sox.
2006: Struggles.
In 2006, the Braves did not perform at the level they had grown accustomed to. Due to an offensive slump, injuries to their starting rotation, and subpar bullpen performances, the Braves compiled a 6–21 record for the month of June, the worst month ever in the city of Atlanta, and just percentage points better than the Boston Braves of May 1935 (4–20).
The Braves made their move in July, going 14–10. However, the team remained in the bottom half of the NL East and trailed the Mets by a double-digit deficit for much of the season (13 games at the All-Star Break). However, despite their struggles, the Braves entered the break down by only six and a half games to the Dodgers for the NL Wild Card slot after winning seven of their last ten games.
After the break, the Braves came out with their bats swinging, setting many franchise records. They won five straight, sweeping the Padres and taking two from the Cardinals, tallying a total of 65 runs in that span. The 65 runs in five games is the best by the franchise since 1897, when the Boston Beaneaters totaled 78, including 25 in one game and 21 in another, from May 31 – June 3; the 2006 Braves also became the first team since the 1930 New York Yankees to score ten runs or more in five straight games. The Braves had a total of 81 hits during their five-game run and 98 hits in their last six games, going back to an 8–3 victory over Cincinnati on July 9, the last game before the All-Star break. Additionally, Chipper Jones was able to maintain a 20-game hitting streak and tie Paul Waner's 69-year-old Major League record with a 14-game extra-base hit streak.
The Braves made their first trade of the season on July 20 to shore up the bullpen, sending Class A Rome catcher Max Ramirez to Cleveland for closer Bob Wickman. He served as the Braves' closer for the remainder of the season, taking over for an embattled Jorge Sosa, who was subsequently traded on the July 31 trade deadline for St. Louis minor league pitcher Rich Scalamandre.
On July 29, the Braves traded reserve third baseman/shortstop Wilson Betemit to the Los Angeles Dodgers for reliever Danys Báez and infielder Willy Aybar. The move came on the night that starting third baseman Chipper Jones went on the 15-day disabled list with a strained oblique muscle. With Betemit gone, Atlanta called up infielder Tony Pena, Jr. from AAA Richmond to supplement Pete Orr.
Before the expansion of rosters on September 1, the Braves acquired Daryle Ward from the Washington Nationals for Class A Myrtle Beach pitcher Luis Atilano, in hopes that he would be a valuable pinch-hitter in the postseason.
However, on September 18, the New York Mets' win over the Florida Marlins mathematically eliminated the Braves from winning the NL East, ending the Atlanta Braves' eleven-year reign over the NL East. On September 24, the Braves' loss to the Colorado Rockies mathematically eliminated the Braves from winning the NL Wild Card, making 2006 the first year that the Braves would not compete in the postseason since 1990, not counting the strike-shortened 1994 season.
Also, a loss to the Mets on September 28 guaranteed the Braves their first losing season since 1990. Although the Braves won two of their last three games against the Astros, including rookie Chuck James besting Roger Clemens, Atlanta finished the season in third place, one game ahead of the Marlins, at 79–83.
After the season, the Atlanta coaching staff underwent a few changes. Brian Snitker became the third base coach after Fredi González left to become the manager for the Florida Marlins. Chino Cadahia replaced Pat Corrales as bench coach and former catcher Eddie Pérez became the new bullpen coach, replacing Bobby Dews.
Sale to Liberty Media.
In December 2005, team owner Time Warner, who inherited the Braves after purchasing TBS in 1996, announced it was placing the team for sale. Liberty Media began negotiations to purchase the team.
In February 2007, after more than a year of negotiations, Time Warner agreed to a deal that would sell the Braves to Liberty Media Group (a company which owned a large amount of stock in Time Warner, Inc.), pending approval by 75 percent of MLB owners and the Commissioner of Baseball, Bud Selig. The deal included the exchange of the Braves, valued in the deal at $450 million, a hobbyist magazine publishing company, and $980 million cash, for 68.5 million shares of Time Warner stock held by Liberty Media, then worth approximately $1.48 billion. Team President Terry McGuirk anticipated no change in the current front office structure, personnel, or day-to-day operations of the Braves. Liberty Media is not expected to take any type of "active" ownership in terms of day-to-day operations.
On May 16, 2007, Major League Baseball's owners approved the sale of the Braves from Time Warner to Liberty Media.
2007: More struggles.
The Braves made their first moves by re-signing Bob Wickman to a one-year deal and picking up John Smoltz's option in September 2006. They traded starting pitcher Horacio Ramírez to the Seattle Mariners for pitcher Rafael Soriano, an American League reliever with a 2.20 ERA in 2006. They also denied arbitration to pitcher Chris Reitsma and second baseman Marcus Giles. The Braves signed utility-man Chris Woodward to fill a spot on the bench. The biggest trade in the offseason involved first baseman Adam LaRoche and a minor league player for Pittsburgh Pirates closer Mike González and a minor league infielder, Brent Lillibridge. Gonzalez, who converted 24 of 24 save opportunities in 2006, joined Soriano as a set-up man for Wickman in the bullpen. The team then signed Craig Wilson to a one-year deal to platoon with Scott Thorman. The Braves also had solid relievers in Macay McBride, Blaine Boyer, and Tyler Yates. In addition, the majority of the Braves' offense, which was second in the NL in runs scored in 2006, returned in 2007. However, Mike Hampton was sidelined for the entire 2007 season with yet another surgery. Mike González was later sidelined for the season while recovering from Tommy John surgery.
The Braves' bullpen and offense came through in the clutch early on, helping the Braves to a 7–1 start, their best start since winning the World Series in 1995. The team finished April with a 16–9 record, but struggled during May, finishing 14–14. The Braves also struggled during interleague play, finishing with an NL-worst 4–11 record. On June 24, the Braves fell to .500 for the first time in the 2007 season, but rebounded by winning the next 5 games.
On July 5, Chipper Jones surpassed Dale Murphy for the Atlanta club record of 372 home runs by belting two against the Los Angeles Dodgers. On July 31, 2007, the Braves finalized the deal to acquire slugger first baseman Mark Teixeira and LHP Ron Mahay from the Texas Rangers for catcher Jarrod Saltalamacchia, SS Elvis Andrus, and three minor-leaguers. The Braves also acquired Octavio Dotel from the Kansas City Royals for Kyle Davies and also traded LHP Wilfredo Ledezma and RHP Will Startup to the San Diego Padres for Royce Ring. On August 19, 2007 John Smoltz passed Phil Niekro for 1st place on the Braves' all-time strikeout list. Braves manager Bobby Cox broke the all-time MLB record for most career ejections by a manager in August 2007.
After struggling during the second half of the 2007 season, Atlanta finished over .500 and missed the post season again. On October 12, 2007, John Schuerholz stepped down as General Manager to take over as team president. Assistant GM Frank Wren took over as General Manager.
2008: Plagued by injuries.
In December 2007, the team announced it would not re-sign center fielder Andruw Jones (who later would sign with the Dodgers). Another major move was acquiring CF Gorkys Hernández and RHP Jair Jurrjens from the Detroit Tigers in exchange for SS Edgar Rentería and cash considerations. Next, LHP Tom Glavine was signed to a one-year contract. They also acquired LHP Will Ohman and INF Omar Infante from the Cubs in exchange for RHP José Ascanio.
The team's first new move for 2008 was acquiring OF Mark Kotsay from the A's (to replace Jones) in exchange for RHP Joey Devine, RHP Jamie Richmond and cash considerations. Days later, Wren traded Willy Aybar, outfielder Tom Lindsey, and infielder Chase Fontaine to the Rays in exchange for left-hand reliever Jeff Ridgway.
Before the trade deadline the Braves traded 1B Mark Teixeira to the Los Angeles Angels for first baseman Casey Kotchman and minor league RHP Stephen Marek. The Braves failed to make the playoffs for the third straight season.
2009: The return of solid pitching.
On December 4, 2008, the Atlanta Braves received Javier Vázquez and Boone Logan, while the Chicago White Sox received prospects catcher Tyler Flowers, shortstop Brent Lillibridge, third baseman Jon Gilmore and pitcher Santos Rodriguez. On January 13, 2009, the Braves signed Japanese pitcher Kenshin Kawakami to a three-year deal, and two days later signed free agent pitcher Derek Lowe to a four-year contract. During the course of the offseason, the Braves signed veteran pitcher and former Brave Tom Glavine, while losing long-time Brave John Smoltz to the Boston Red Sox.
On February 25, 2009, just before the start of spring training, Atlanta agreed to terms on a one-year contract with free-agent outfielder Garret Anderson. The additional outfield depth allowed the Braves to trade Josh Anderson to the Detroit Tigers for minor league pitcher Rudy Darrow on March 30, 2009.
On June 3, 2009, the Braves acquired Nate McLouth from the Pittsburgh Pirates for prospects Jeff Locke, Charlie Morton and Gorkys Hernández. They also released veteran pitcher Tom Glavine. On July 10, 2009, the Braves traded outfielder Jeff Francoeur to the New York Mets for outfielder Ryan Church. On July 31, 2009, hours before the trade deadline, the Braves and Boston Red Sox swapped 1st basemen: Atlanta dealt Casey Kotchman to Boston and reacquired Adam LaRoche, whom the Braves had traded away during the 2006–07 off-season to Pittsburgh.
The Braves made a late-season surge, coming within 2 games of the wild card leading Colorado Rockies in late September. On October 1, 2009 with the Braves four games back, Colorado beat the Milwaukee Brewers 9–2 to clinch the wild card spot and end the Braves' 2009 postseason hopes.
2010: Cox's final season.
The 2010 Atlanta Braves Season features the Braves' attempt to reclaim a postseason berth for the first time since 2005. The Braves were once again skippered by Bobby Cox, now in his 25th and final season managing the team. The Braves started the 2010 season slowly and had a nine-game losing streak in April. Then they had a nine-game winning streak from May 26 through June 3, the Braves longest since 2000 when they won 16 in a row. On May 31, the Atlanta Braves defeated the then-first place Philadelphia Phillies at Turner Field to take sole possession of first place in the National League East standings, a position they had maintained through the middle of August. The last time the Atlanta Braves led the NL East on August 1 was in 2005. On July 13, 2010 at the 2010 MLB All-Star Game in Anaheim, Braves catcher Brian McCann was awarded the All-Star Game MVP Award for his clutch two-out, three-run double in the seventh inning to give the National League its first win in the All-Star Game since 1996. He became the first Brave to win the All-Star Game MVP Award since Fred McGriff did so in 1994. The Braves made two deals before the trade deadline to acquire Álex González, Rick Ankiel and Kyle Farnsworth from the Toronto Blue Jays and Kansas City Royals, giving up shortstop Yunel Escobar, pitchers Jo-Jo Reyes and Jesse Chavez, outfielder Gregor Blanco and three minor leaguers. On August 18, 2010 they traded three pitching prospects for first baseman Derrek Lee from the Chicago Cubs. On August 22, 2010 against the Chicago Cubs, Mike Minor struck out 12 batters across 6 innings; an Atlanta Braves single game rookie strikeout record. The Braves dropped to second in the NL East in early September, but won the NL Wild Card. They lost to the San Francisco Giants in the National League Division Series in four games. Every game of the series was determined by one run. After the series-clinching victory for the Giants in Game 4, Bobby Cox was given a standing ovation by the fans, also by players and coaches of both the Braves and Giants.
2011: Fredi González takes over.
On October 13, 2010, the Atlanta Braves announced that Fredi González would replace long-time Braves manager Bobby Cox as manager of the team in 2011. The announcement came just two days after the 2010 Braves were eliminated from the postseason. It was also announced that pitching coach Roger McDowell, third-base coach Brian Snitker, and bullpen coach Eddie Pérez would retain their current positions, while former hitting coach Terry Pendleton would replace Glenn Hubbard as the first-base coach and newcomer Carlos Tosca would become the new bench coach. Hubbard and former bench coach Chino Cadahia were not offered positions on the new coaching staff. Larry Parrish was hired as hitting coach on October 29, 2010.
On November 16, 2010 in an offseason trade, the Braves acquired Dan Uggla from the Florida Marlins in exchange for left-handed reliever Mike Dunn and infielder Omar Infante. According to Elias Sports Bureau, the Braves had an all-time franchise win-loss record over .500 for the first time since 1923 after their win over the Houston Astros on June 11, 2011. The Braves franchise became the third franchise in MLB history to reach 10,000 wins with their win over the Washington Nationals on July 15, 2011. On July 31, 2011, just sixteen days after registering their 10,000th win, the Florida Marlins defeated the Braves by a score of 3-1, handing the team the 10,000th loss in franchise history. The Braves become only the second team in big league history with 10,000 losses after the Philadelphia Phillies reached the plateau in 2007.
Players from the Braves' farm system, such as Freddie Freeman and Brandon Beachy, played regularly with the big league club, while Julio Teherán, Randall Delgado, and Mike Minor were called up for spot starts. With late season injuries to starters Jair Jurrjens and Tommy Hanson, these three young pitchers made their way into the starting rotation in their absence. Eight players made their major league debuts for the team in 2011.
September collapse.
The Braves led the National League Wild Card standings for much of the 2011 season, with the division-rival Philadelphia Phillies firmly in control of first place in the National League East. The Braves entered the final month of the regular season 25 games above .500 with a record of 80–55 and an 8 1⁄2-game lead in the Wild Card standings. The nearest team trailing them, the St. Louis Cardinals, who also trailed the National League Central-leading Milwaukee Brewers by 8 1⁄2 games at the time, were considered a long-shot to gain a spot in the postseason. Just days prior on August 26, the Cardinals found themselves 10 1⁄2 games behind and in third place.
With 27 games to play, the Braves went 9–18 in September to finish the season with a record of 89–73. The Cardinals, meanwhile, went 18–8 to finish at 90–72. Braves closer Craig Kimbrel, who had not surrendered a single earned run in July or August, carried a 4.76 ERA in September with three blown saves. After being dominant in his role for much of the season, Braves setup man Jonny Venters posted a 5.11 September ERA. These sharp declines in both relievers led many critics to question the handling of the bullpen by Braves manager Fredi González. Veteran starter Derek Lowe posted a win-loss record of 0–5 in September with an ERA of 8.75. Shortly into the offseason, Lowe would be traded to the Cleveland Indians. The Braves starters lasted six or more innings only three times over the last 23 games. Over the last five games, all of which were losses for the Braves, the team managed to score only seven runs. Braves catcher Brian McCann, often regarded as the best offensive catcher in the Majors, hit only .183 with two home runs in September. The offense as a whole hit for only a .235 batting average and a .300 on-base percentage in September, both second-worst in the National League. The .195 RISP average by Braves hitters was second worst in the Majors. Hitting coach Larry Parrish was fired two days following the last game of the season.
2012: Chipper's last season.
In 2012, the Braves began their 138th season after an upsetting end to the 2011 season. On March 22, the Braves announced that third baseman Chipper Jones would retire following the 2012 season after 19 Major League seasons with the team. The Braves also lost many key players through trades or free agency, including pitcher Derek Lowe, shortstop Alex González, and outfielder Nate McLouth. To compensate for this, the team went on to receive many key players such as outfielder Michael Bourn, along with shortstops Tyler Pastornicky and Andrelton Simmons. To fill the void of a quality starting pitcher left by Lowe (as well as a mid-season injury to Brandon Beachy), manager Fredi González elected relief pitcher Kris Medlen to the starting pitching rotation. The Braves went on to win every game Medlen started, setting the MLB record for most consecutive wins when a single pitcher starts (total of 23). Atlanta stayed close to the Washington Nationals in the race to win the National League East title. They also stayed on top of the National League Wild Card race. Washington ended up winning their first division title in franchise history, but the Braves remained in first place of the NL wild card race. Keeping with a new MLB rule for the 2012 season, the top two wild card teams in each league must play each other in a playoff game before entering into the Division Series.
The Braves played the St. Louis Cardinals in the first ever Wild Card Game. The Braves were behind 6–3 in the bottom of the eighth inning when Andrelton Simmons hit a fly ball to left field that dropped in between the Cardinals shortstop and left fielder. Umpire Sam Holbrook called Simmons out, citing the infield fly rule. Had an infield fly not been called, Simmons would have been credited with a single and Atlanta would have had the bases loaded with one out. Fans at Turner Field began to litter the field with debris, prompting the game to be delayed for 19 minutes. The Braves lost the game 6–3, ending their season.
2013: Braves win the East.
During the offseason following a gut wrenching exit against the St. Louis Cardinals in the Wild Card Game, the Braves spent the 2012-2013 offseason revamping and retooling their offense. The Braves turned heads across baseball by acquiring B.J. Upton from the Tampa Bay Rays, signing him to a 5-year $75.25 million contract and making him their starting center fielder, and uniting him with his younger brother Justin Upton from the Arizona Diamondbacks in a seven player trade that sent fan favorite utility man Martin Prado to the Diamondbacks, they also filled a need for a new Third Baseman in Chris Johnson after the retirement of Chipper Jones the previous year. The Braves began the 2013 season with a hot start in April by going 17-9 for the month, which saw the emergence of rookie sensation Evan Gattis, while taking hold of first place in the National League East division, a lead they would never relinquish for the rest of the season. The Braves suffered many injuries to key players throughout the season, including injuries to Jason Heyward, Brian McCann, Freddie Freeman, Eric O'Flaherty, Jonny Venters, Ramiro Pena and others, but found a way to win despite these blows to the team. Leading up to the All Star break, First Baseman Freddie Freeman was voted in to play for the 2013 National League All-Star Team, in the 2013 All Star Game, which he did not play. The Braves also witnessed the emergence of rookie pitcher Julio Teheran after much hype during Spring Training. From July 26 to August 10, the Braves won 14 games in a row. The winning streak was the longest of its kind since April–May 2000.
On June 28, 2013 the Atlanta Braves retired former third baseman Chipper Jones' jersey, number 10, before the game against the Arizona Diamondbacks. He was honored before 51,300 fans at Turner Field in Atlanta, Georgia. He served as a staple of the Braves franchise for 19 years before announcing his retirement at the beginning of the 2012 season. Chipper Jones played his last regular season game for the Braves on September 30, 2012.
The Braves opened up a 15-game lead on the Washington Nationals in the National League East on September 3, 2013, riding that lead en route to its first division title since 2005, the last of 14 straight division titles. This was also Braves manager Fredi Gonzalez's first division title since beginning his managerial career in 1990; including his first since becoming the manager of the Braves after the 2010 season. The Braves clinched the 18th division title in team history on September 22, 2013 after a Nationals loss to the Marlins in the first game of a double header; the Braves also won their game that day, beating the Chicago Cubs 5-2 at Wrigley Field.
2014.
On November 11, 2013, the Braves announced that they would vacate Turner Field for a new stadium in Cobb County, in the northwest suburbs outside of Atlanta in 2017. The move is to follow the expiration of the Braves' 20-year lease on Turner Field in 2016. The new stadium is to be constructed in a public/private partnership. During the offseason the Braves signed few of their young talents to multi year contracts; Craig Kimbrel (4 years/$42M), Freddie Freeman (8 years/$135M), Kris Medlen (1 year/$5.8M), Jason Heyward (2 years/$13.3M), Julio Teherán (6 years/$32.4M) and Andrelton Simmons (7 years/$58M).
The Braves finished the season in a distant second place with a 79-83 record, which was their first losing season since 2008 and only their third since 1990.
2015.
Prior to the 2015 season, the Braves fired their General Manager Frank Wren, and John Hart replaced him as interim GM, choosing to only take the title of President of Baseball Operations. The Braves promptly traded Gold Glove Award winner Jason Heyward to the St. Louis Cardinals along with pitcher Jordan Walden for pitchers Shelby Miller and Tyrell Jenkins. Hart would then trade All Star left fielder Justin Upton to the San Diego Padres for Max Fried, Jace Peterson, Dustin Peterson, and Mallex Smith. Catcher Evan Gattis and minor league prospect James Hoyt were traded to the Houston Astros for minor leaguers Mike Foltynewicz, Rio Ruiz, and Andrew Thurman. A day before the season began, the braves made a final trade involving former All- Star Craig Kimbrel and outfielder Melvin Upton Jr.. They were traded to the San Diego Padres for outfielders Cameron Maybin and Carlos Quentin along with two minor league players. By the beginning of the season, the Braves did 11 trades in all.
World Series Championships.
Over its 138 seasons, the Braves franchise has won a total of three World Series Championships.

</doc>
<doc id="2287" url="http://en.wikipedia.org/wiki?curid=2287" title="Armored car (military)">
Armored car (military)

A military armored (or armoured) car is a wheeled light armored vehicle, lighter than other armored fighting vehicles, primarily being armored and/or armed for self-defense of the occupants. Other multi-axled wheeled military vehicles can be quite large, and actually be superior to some smaller tracked vehicles in terms of armor and armament.
History.
Armed car.
The Motor Scout was designed and built by British inventor F.R. Simms in 1898. It was the first armed petrol engine powered vehicle ever built. The vehicle was a De Dion-Bouton quadricycle with a mounted Maxim machine gun on the front bar. An iron shield in front of the car protected the driver.
Another early armed car was invented by Royal Page Davidson at Northwestern Military and Naval Academy in 1898 with the Davidson-Duryea gun carriage and the later Davidson Automobile Battery armored car.
However, these were not 'armored cars' as the term is understood today, as they provided no real protection for their crews against any kind of opposing fire. They were also, by virtue of their small capacity engines, far less efficient than the cavalry and horse-drawn guns that they were intended to complement.
First armored cars.
At the beginning of the twentieth century, the first military armored vehicles were manufactured, by adding armor and weapons to existing vehicles.
The first armoured car was the Simms' Motor War Car, designed by F.R. Simms and built by Vickers, Sons & Maxim of Barrow on a special Coventry-built Daimler chassis with a German-built Daimler motor in 1899. and a single prototype was ordered in April 1899 The prototype was finished in 1902, too late to be used during the Boer War.
The vehicle had Vickers armour 6 mm thick and was powered by a four-cylinder 3.3-litre 16 hp Cannstatt Daimler engine giving it a maximum speed of around 9 miles per hour (14.5 km/h). The armament, consisting of two Maxim guns, was carried in two turrets with 360° traverse. It had a crew of four. Simms' Motor War Car was presented at the Crystal Palace, London, in April 1902.
Another early armoured car of the period was the French Charron, Girardot et Voigt 1902, presented at the "Salon de l'Automobile et du cycle" in Brussels, on 8 March 1902. The vehicle was equipped with a Hotchkiss machine gun, and with 7 mm armour for the gunner.
The Italians used armored cars during the Italo-Turkish War. A great variety of armored cars appeared on both sides during World War I and these were used in various ways.
World War I.
Generally, the armored cars were used by more or less independent car commanders. However, sometimes they were used in larger units up to squadron size. The cars were primarily armed with light machine guns. But larger units usually employed a few cars with heavier guns. As air power became a factor, armored cars offered a mobile platform for anti-aircraft guns.
The first effective use of an armored vehicle in combat was achieved by the Belgian Army in August–September 1914. They had placed Cockerill armour plating and a Hotchkiss machine gun on Minerva Armored Cars. Their successes in the early days of the war convinced the Belgian GHQ to create a Corps of Armoured Cars, who would be sent to fight on the Eastern front once the western front immobilized after the Battle of the Yser.
The British Royal Naval Air Service dispatched aircraft to Dunkirk to defend the UK from Zeppelins. The officers' cars followed them and these began to be used to rescue downed reconnaissance pilots in the battle areas. They mounted machine guns on them and as these excursions became increasingly dangerous, they improvised boiler plate armoring on the vehicles provided by a local shipbuilder. In London Murray Sueter ordered "fighting cars" based on Rolls-Royce, Talbot and Wolseley chassis. By the time Rolls-Royce Armoured Cars arrived in December 1914, the mobile period on the Western Front was already over. As described below, they had a fascinating birth and long and interesting service.
More tactically important was the development of formed units of armoured cars, such as the Canadian Automobile Machine Gun Brigade, which was the first fully mechanized unit in the history of the British Army. The brigade was established on September 2, 1914 in Ottawa, as Automobile Machine Gun Brigade No. 1 by Brigadier-General Raymond Brutinel. The Brigade was originally equipped with 8 Armoured Autocars mounting 2 machine guns. By 1918 Brutinel's force consisted of two Motor Machine Gun Brigades (each of five gun batteries containing eight weapons apiece). The brigade, and its armoured cars, provided yeoman service in many battles, notably at Amiens.
The Rolls-Royce Armoured Car was famously proposed, developed, and utilised by the 2nd Duke of Westminster. He took a squadron of these cars to France in time to make a noted contribution to the Second Battle of Ypres, and thereafter the cars with their master were sent to the Middle East to play a part in the British campaign in Palestine and elsewhere. These cars appear in the memoirs of numerous officers of the BEF during the earlier stages of the Great War - their ducal master often being described in an almost piratical style.
World War II.
The British Royal Air Force (RAF) in the Middle East was equipped with Rolls-Royce Armoured Cars and Morris tenders. Some of these vehicles were among the last of a consignment of ex-Royal Navy armored cars that had been serving in the Middle East since 1915. In September 1940 a section of the No. 2 Squadron RAF Regiment Company was detached to General Wavell’s ground forces during the first offensive against the Italians in Egypt. It is said that these armored cars became ‘the eyes and ears of Wavell’. During the actions in the October of that year the Company was employed on convoy escort tasks, airfield defense, fighting reconnaissance patrols and screening operations.
During the Anglo-Iraqi War, some of the units located in the British Mandate of Palestine were sent to Iraq and drove Fordson armored cars. "Fordson" armored cars were Rolls-Royce armored cars which received new chassis from a Fordson truck in Egypt.
Since the Treaty of Versailles did not mention armored cars, Germany began developing them early. By the start of the new war, the German army possessed some highly effective reconnaissance vehicles, such as the "Schwerer Panzerspähwagen".
The Soviet BA-64 was influenced by a captured "Leichter Panzerspähwagen" before it was first tested in January 1942.
In the second half of the war, the American M8 Greyhound and the British Daimler Armoured Cars featured turrets with light guns (40 mm or less) mounted in turrets. As with other wartime armored cars, their reconnaissance roles emphasized greater speed and stealth than a tracked vehicle could provide, so their limited armor, armament and off-road capabilities were seen as acceptable compromises.
Military use.
A military armored car is a type of armored fighting vehicle having wheels (from four to ten large, off-road wheels) instead of tracks, and usually light armor. Armored cars are typically less expensive and on roads have better speed and range than tracked military vehicles. They do however have less mobility as they have less off-road capabilities because of the higher ground pressure. They also have less obstacle climbing capabilities than tracked vehicles. Wheels are more vulnerable to enemy fire than tracks, they have a higher signature and in most cases less armor than comparable tracked vehicles. As a result they are not intended for heavy fighting; their normal use is for reconnaissance, command, control, and communications, or for use against lightly armed insurgents or rioters. Only some are intended to enter close combat, often accompanying convoys to protect soft-skinned vehicles.
Light armored cars, such as the British Ferret are armed with just a machine gun. Heavier vehicles are armed with autocannon or a small tank gun. The heaviest armored cars, such as the German, World War II era SdKfz 234 or the modern, US M1128 Mobile Gun System, mount the same guns that arm medium tanks.
Armored cars are popular for peacekeeping or internal security duties. Their appearance is less confrontational and threatening than tanks, and their size and maneuverability is said to be more compatible with tight urban spaces designed for wheeled vehicles. However they do have a larger turning radius compared to tracked vehicles which can turn on the spot and their tires are vulnerable and are less capable in climbing and crushing obstacles. However when there is true combat they are easily outgunned and lightly armored. The threatening appearance of a tank is often enough to keep an opponent from attacking, whereas a less threatening vehicle such as an armored car is more likely to be attacked.
Many modern forces now have their dedicated armored car designs, to exploit the advantages noted above. Examples would be the M1117 Armored Security Vehicle of the USA or Alvis Saladin of the post-World War II era in the United Kingdom.
Alternatively, civilian vehicles may be modified into improvised armored cars in "ad hoc" fashion. Many militias and irregular forces adapt civilian vehicles into AFVs (armored fighting vehicles) and troop carriers, and in some regional conflicts these "technicals" are the only combat vehicles present. On occasion, even the soldiers of national militaries are forced to adapt their civilian-type vehicles for combat use, often using improvised armor and scrounged weapons.

</doc>
<doc id="2289" url="http://en.wikipedia.org/wiki?curid=2289" title="AZ Alkmaar">
AZ Alkmaar

Alkmaar Zaanstreek (]), better known as AZ Alkmaar or simply AZ (]), is a Dutch professional football club from Alkmaar and the Zaanstreek. The club plays in the Eredivisie, the highest professional football league in the Netherlands, and hosts home games at the AFAS Stadion.
AZ has won the Eredivisie twice, in 1980–81 and 2008–09. In the same season as their first league title, they also reached the UEFA Cup Final, which they lost to Ipswich Town. In addition, the team has won the KNVB Cup on four occasions.
History.
1954-1972: Foundation and first years.
AZ was founded on 10 May 1967 as AZ '67, the result of a merger of Alkmaar '54 and FC Zaanstreek. FC Zaanstreek was formed in 1964, continuing the professional adventure of the Kooger Football Club (). KFC had been founded in 1910, had nearly become National Champion in 1934 through a narrow loss to Ajax in the finals, and had been professional since 1955.
In 1964, the brothers Cees and Klaas Molenaar, former players for KFC and owners of a growing appliance store chain, sought to create a powerful football team in Zaanstreek by merging the two local professional teams: KFC and .
After this merger failed, they successfully merged KFC (now "FC Zaanstreek") with Alkmaar '54; the team would be based in Alkmaar. 
Partially through the hiring of expensive foreign players, the new club soon acquired large debts.
1972-1985: The Molenaar years.
Fortunately in 1972 the Molenaar brothers bailed it out and invested heavily in the club, to the point that AZ '67 were successful in the late seventies and early eighties, regularly playing European football from 1977 to 1982 whilst also winning the Dutch Cup three times over that period.
After four close league campaigns AZ finally became Dutch champions in 1981, they were the only team other than the "big three" (Ajax, Feyenoord, and PSV) to do so in a 44-year period spanning from 1965 to 2009, when AZ once again won the league title. They won the 1980–81 season with overwhelming power, winning 27 of 34 matches and only losing once whilst scoring a club record 101 goals and conceding just 30 goals.
The same season AZ reached the final of the UEFA Cup, losing 5–4 on aggregate to Ipswich Town. The next year in the UEFA Champions League they lost in the second round 3–2 on aggregate to Liverpool.
Georg Keßler was AZ's manager over most of these years (1978–82), while star players included: Kees Kist the club's highest ever goalscorer with 212 goals and the first ever Dutchman to win the European Golden Boot in 1979 when he scored 34 goals in a season; Jan Peters who played 120 games for AZ during this period scoring 30 goals from midfield; Hugo Hovenkamp played 239 games in defence for AZ from 1975–83 as well as receiving 31 caps for the Netherlands from 1977–83 and playing each game in Euro '80 while an AZ player. 
John Metgod spent six years at AZ playing 195 games as a defender, scoring 26 goals including a goal against Ipswich Town in the final of the UEFA Cup. Like Hovenkamp, Metgod was included in the Netherlands squad for Euro '80; The Danish forward Kristen Nygaard who spent 10 years at AZ scoring 104 goals in 363 games between 1972 and 1982.
1985-1993: The interim years.
The club deteriorated after Klaas Molenaar left the club in 1985 (Cees died in 1979). AZ were relegated in 1988 from the Eredivisie.
1993-2009: The Scheringa years.
The involvement of businessman Dirk Scheringa in the mid 1990s marked the revival of the club as AZ returned to the Eredivisie in 1998.
After a 22-year hiatus from European football AZ appeared in the 2004–05 UEFA Cup advancing to the semi-finals. The second leg of the semi-final against Sporting CP had a heart-breaking conclusion, when Sporting scored in the 122nd minute (2 minutes into stoppage time) to reach an aggregate score of 4–4, Sporting advanced to the Final thanks to the away goals rule. In the same season AZ finished third in the Eredivisie, qualifying for the UEFA Cup again. These were great achievements for the club which does not have a similar sized fanbase relative to Eredivisie and European rivals; AZ's home ground until the 2006–07 season, the Alkmaarderhout, had a capacity of only 8,390.
In the summer of 2006, the club moved to a new 17,000 capacity stadium AZ Stadion.
AZ had a very good 2006–07 season, despite ending in disaster. Going into the last game of the 2006–07 season, AZ led PSV and Ajax by goal difference in the Eredivisie, but ended up third after losing their last match against the now relegated and bottom of the table Excelsior, playing with 10 men for 80 minutes. 
Furthermore AZ lost the KNVB Cup final to Ajax 8–7 after a penalty shoot-out and also lost to Ajax over two play-off games for the Champions League. After the season, key players like Tim de Cler, Danny Koevermans, and Shota Arveladze left the team.
A remarkable run ended in the 2007–08 season; AZ lost a group game against Everton 3–2 in the UEFA Cup which ended an unbeaten run of 32 home matches in European competitions, a record which ran from 1977 to 2007. 
Also on this season AZ performed so badly (first round loss in the KNVB Cup, elimination from the UEFA Cup group stage and 11th place league finish), that team manager Louis van Gaal felt obliged to hand in his resignation in March 2008. However, after protests from the players and directors, van Gaal withdrew his resignation.
The 2008–09 season had an unpromising start with two defeats against NAC Breda and ADO Den Haag. However, starting with a 1–0 victory over defending league champions PSV, AZ didn't lose a game in the next 28 matches, including a run of 11 matches where AZ did not concede an opposition goal. Three weeks before the end of the season AZ became Eredivisie champions beating nearest rivals Twente and Ajax comfortably. Being league champions, AZ qualified for the UEFA Champions League for the second time, but only took four points from six matches and finished bottom of their group.
2009-Present: Recent years.
Ronald Koeman succeeded Louis van Gaal after the 2008–09 season. Van Gaal had already left for Bayern Munich after becoming league champions with AZ. Koeman became the manager for AZ on 17 May 2009. On 5 December 2009 AZ announced that Koeman no longer was in charge of AZ, after losing 7 of the first 16 games in his reign. Former Rangers and Zenit St. Petersburg manager Dick Advocaat took over for the rest of the season. Under Advocaat, AZ achieved some good results and secured European football for the next season.
For the 2010–11 season AZ appointed Gertjan Verbeek as their new manager. AZ finished the 2010–11 season in 4th place, securing Europa League football for the next season. In the KNVB Cup AZ reached the last eight, where they were beaten by rivals Ajax with 1–0. AZ finished third in their Europa League group, thus not qualifying for the knock out round.
The 2011–12 season AZ finished the Eredivisie in 4th place and performed significantly better in cup competitions, reaching the semi-finals in the KNVB cup (losing to Heracles Almelo after extra time) and the quarter-finals in the Europa League, ultimately losing to Valencia after beating Udinese, Anderlecht, Malmö FF, Austria Wien, Metalist Kharkiv, Aalesund and FK Baumit Jablonec.
On 21 December 2011, during the quarter-finals of the KNVB Cup, a 19 year old Ajax fan entered the Amsterdam ArenA pitch in the 36th minute, with Ajax winning 1–0, attacking AZ goalkeeper Esteban Alvarado. The fan slipped and Alvarado kicked the fan twice, which led to the goalkeeper being sent off. Following this, Gertjan Verbeek ordered his players to leave the pitch for the dressing room in protest. Later, the match was played again on 19 January 2012, with Alvarado's red card rescinded. AZ won the match 3–2.
The 2012–13 season started in the Europa League with a qualifying play-off round against Guus Hiddinks Anzhi Makhachkala, AZ were hammered 6–0 on aggregate. Disappointingly AZ finished the Eredivisie in 10th place, however AZ won silverware by winning the domestic cup after beating PSV 2-1. Winning the KNVB Beker AZ automatically qualified for Europa League football despite finishing the league in tenth position and out of the league's Europa League play-off system.
In September 2013, just a day after emphatically beating PSV, then league leaders, Verbeek was dismissed as first team manager by the club due to 'a lack of chemistry' between the management and players. He was replaced by Dick Advocaat for the rest of the season until a permanent replacement was found. Advocaat took AZ to the semi-finals of the KNVB Beker, the quarter-finals of the Europa League and 8th in the league, ultimately losing to FC Groningen in Europa League play-off final round (their 58th game of the season, a club record).
The 2014–15 season started with a new manager at the helm, former SC Heerenveen manager and Ajax player Marco van Basten.
Current squad.
"As of 1 June 2015"
"For recent transfers, see List of Dutch football transfers summer 2015"
Stadium and sponsor.
AZ play their home games at the AFAS Stadion, located in the southern part of the city of Alkmaar. The stadium, which is owned directly by the club, was opened in 2006 and replaced the old Alkmaarderhout venue as the DSB Stadion. The stadium currently has a capacity of "17,023". During its design stages the name Victorie Stadion was frequently used, referring to the Dutch War of Independence, the phrase ""n Alkmaar begint de victorie" (Victory begins in Alkmaar)" in particular. Until now, this name hasn't been officially in use, the board instead opting for sponsorship deals because of financial motives. However, to this day the name maintains a good share of support among the fans.
In order to further increase revenue, the "AZ board" of directors decided to extend the capacity of the new stadium to a minimum of "30,000" seated spectators somewhere in the future. The extension will be realised by constructing a second tier to three of the four stands. The main stand with all technical areas, "VIP" and sponsor and media facilities will remain in place. However, these plans were put on hold after the DSB bankruptcy and there are no current plans to increase the capacity.
In October 2009 sponsor DSB Bank was declared bankrupt.
The stadium name temporarily changed from DSB Stadion to AZ Stadion, as it was considered undesirable that the stadium was linked with a non-existent bank. In February 2010 a new main sponsor was found: construction works service provider BUKO from Beverwijk.
A year later, in the season 2010–11, "AFAS Erp Software" took over as official shirt sponsor, also taking over duties as stadium sponsor. The current external name of the ground is AFAS Stadion.
AZ in Europe.
Below is a table with AZ's international results in the past seasons.
Domestic results.
Below is a table with AZ's domestic results since the introduction of professional football in 1956.

</doc>
<doc id="2416" url="http://en.wikipedia.org/wiki?curid=2416" title="Athanasian Creed">
Athanasian Creed

The Athanasian Creed, or Quicunque Vult (also "Quicumque Vult"), is a Christian statement of belief focused on Trinitarian doctrine and Christology. The Latin name of the creed, "Quicunque vult", is taken from the opening words, "Whosoever wishes". The creed has been used by Christian churches since the sixth century. It is the first creed in which the equality of the three persons of the Trinity is explicitly stated. It differs from the Nicene-Constantinopolitan and Apostles' Creeds in the inclusion of anathemas, or condemnations of those who disagree with the creed (like the original Nicene Creed).
Widely accepted among Western Christians, including the Roman Catholic Church and some Anglican churches, Lutheran churches (it is considered part of the Lutheran confessions in the Book of Concord), and ancient, liturgical churches generally, the Athanasian Creed has been used in public worship less and less frequently, but part of it can be found as an "Authorized Affirmation of Faith" in the recent (2000) Common Worship liturgy of the Church of England [Main Volume page 145]. The creed has never gained much acceptance in liturgy among Eastern Christians. It was designed to distinguish Nicene Christianity from the heresy of Arianism. Liturgically, this Creed was recited at the Sunday Office of Prime in the Western Church; it is not in common use in the Eastern Church. Today, the Athanasian Creed is rarely used even in the Western Church. When used, one common practice is to use it once a year on Trinity Sunday.
Origin.
A medieval account credited Athanasius of Alexandria, the famous defender of Nicene theology, as the author of the Creed. According to this account, Athanasius composed it during his exile in Rome and presented it to Pope Julius I as a witness to his orthodoxy. This traditional attribution of the Creed to Athanasius was first called into question in 1642 by Dutch Protestant theologian G.J. Voss, and it has since been widely accepted by modern scholars that the creed was not authored by Athanasius, that it was not originally called a creed at all, nor was Athanasius' name originally attached to it. Athanasius' name seems to have become attached to the creed as a sign of its strong declaration of Trinitarian faith. The reasoning for rejecting Athanasius as the author usually relies on a combination of the following:
The use of the creed in a sermon by Caesarius of Arles, as well as a theological resemblance to works by Vincent of Lérins, point to Southern Gaul as its origin. The most likely time frame is in the late fifth or early sixth century AD – at least 100 years after Athanasius. The theology of the creed is firmly rooted in the Augustinian tradition, using exact terminology of Augustine's "On the Trinity" (published 415 AD). In the late 19th century, there was a great deal of speculation about who might have authored the creed, with suggestions including Ambrose of Milan, Venantius Fortunatus, and Hilary of Poitiers, among others. The 1940 discovery of a lost work by Vincent of Lérins, which bears a striking similarity to much of the language of the Athanasian Creed, have led many to conclude that the creed originated either with Vincent or with his students. For example, in the authoritative modern monograph about the creed, J.N.D. Kelly asserts that Vincent of Lérin was not its author, but that it may have come from the same milieu, namely the area of Lérins in southern Gaul. The oldest surviving manuscripts of the Athanasian Creed date from the late 8th century.
Content.
The Athanasian Creed is usually divided into two sections: lines 1–28 addressing the doctrine of the Trinity, and lines 29–44 addressing the doctrine of Christology. Enumerating the three persons of the Trinity (i.e., Father, the Son, and the Holy Spirit), the first section of the creed ascribes the divine attributes to each individually. Thus, each person of the Trinity is described as uncreated ("increatus"), limitless ("Immensus"), eternal ("æternus"), and omnipotent ("omnipotens"). While ascribing the divine attributes and divinity to each person of the Trinity, thus avoiding subordinationism, the first half of the Athanasian Creed also stresses the unity of the three persons in the one Godhead, thus avoiding a theology of tritheism. Furthermore, although one God, the Father, Son, and Holy Spirit are distinct from each other. For the Father is neither made nor begotten; the Son is not made but is begotten from the Father; the Holy Spirit is neither made nor begotten but proceeds from the Father and the Son (filioque).
The text of the Athanasian Creed is as follows:
The Christology of the second section is more detailed than that of the Nicene Creed, and reflects the teaching of the First Council of Ephesus (431) and the definition of the Council of Chalcedon (451). The 'Athanasian' Creed uses the Nicene term "homoousios"' ('one substance', 'one in Being') not only with respect to the relation of the Son to the Father according to his divine nature, but that the Son is "homoousios" with his mother Mary, according to his human nature.
The Creed's wording thus excludes not only Sabellianism and Arianism, but the Christological heresies of Nestorianism and Eutychianism. A need for a clear confession against Arianism arose in western Europe when the Ostrogoths and Visigoths, who had Arian beliefs, invaded at the beginning of the 5th century.
The final section of this Creed also moved beyond the Nicene (and Apostles') Creeds in making negative statements about the people's fate: "They that have done good shall go into life everlasting: and they that have done evil into everlasting fire." This caused considerable debate in England in the mid-nineteenth century, centred on the teaching of Frederick Denison Maurice.
Uses.
Composed of 44 rhythmic lines, the Athanasian Creed appears to have been intended as a liturgical document – that is, the original purpose of the creed was to be spoken or sung as a part of worship. The creed itself uses the language of public worship, speaking of the worship of God rather than the language of belief ("Now this is the catholic faith: We worship one God"). In the Catholic Church in medieval times, this creed was recited following the Sunday sermon or at the Sunday Office of Prime. The creed was often set to music and used in the place of a Psalm.
Early Protestants inherited the late medieval devotion to the Athanasian Creed, and it was considered to be authoritative in many Protestant churches. The statements of Protestant belief (confessional documents) of various Reformers commend the Athanasian Creed to their followers, including the Augsburg Confession, the Formula of Concord, the Second Helvetic Confession, the Belgic Confession, the Bohemian Confession and the Thirty-nine Articles. A metric version titled "Quicumque vult", with a musical setting, was published in "The Whole Booke of Psalmes" printed by John Day in 1562. Among modern Lutheran and Reformed churches adherence to the Athanasian Creed is prescribed by the earlier confessional documents, but the creed does not receive much attention outside of occasional use – especially on Trinity Sunday.
In Reformed circles, it is included (for example) in the Christian Reformed Churches of Australia's Book of Forms (publ. 1991). However, it is rarely recited in public worship.
In the successive Books of Common Prayer of the reformed Church of England, from 1549 to 1662, its recitation was provided for on 19 occasions each year, a practice which continued until the nineteenth century, when vigorous controversy regarding its statement about 'eternal damnation' saw its use gradually decline. It remains one of the three Creeds approved in the Thirty-Nine Articles, and is printed in several current Anglican prayer books (e.g. A Prayer Book for Australia (1995)). As with Roman Catholic practice, its use is now generally only on Trinity Sunday or its octave. The Episcopal Church based in the United States has never provided for its use in worship, but added it to its Book of Common Prayer for the first time in 1979, where it is included in small print in a reference section entitled "Historical Documents of the Church."
In Roman Catholic churches, it was traditionally said at Prime on Sundays after Epiphany and Pentecost, except when a Double feast or day within an octave occurred, and on Trinity Sunday. In the 1960 reforms, it was reduced to once a year on Trinity Sunday. It has been effectively dropped from the Catholic liturgy since the Second Vatican Council. It is however maintained in the "Forma Extraordinaria", per the decree Summorum Pontificum, and also in the rite of exorcism, both in the "Forma Ordinaria" and the "Forma Extraordinaria" of the Roman Rite.
In Lutheranism, the Athanasian Creed is—along with the Apostles' and Nicene Creeds—one of the three ecumenical creeds placed at the beginning of the 1580 Book of Concord, the historic collection of authoritative doctrinal statements (confessions) of the Lutheran Church. It is still used in the liturgy on Trinity Sunday.
A common visualisation of the first half of the Creed is the Shield of the Trinity.

</doc>
<doc id="2473" url="http://en.wikipedia.org/wiki?curid=2473" title="Abacá">
Abacá

Abacá ( ; Spanish: "abacá" ]), binomial name Musa textilis, is a species of banana native to the Philippines, grown as a commercial crop in the Philippines, Ecuador, and Costa Rica. The plant, also known as Manila hemp, has great economic importance, being harvested for its fiber, also called Manila hemp, extracted from the leaf-stems. The plant grows to 13 -, and averages about 12 ft. The fiber was originally used for making twines and ropes; now most is pulped and used in a variety of specialized paper products including tea bags, filter paper and banknotes. It is classified as a hard fiber, along with coir, henequin and sisal.
Description.
The abacá plant is stoloniferous, meaning that the plant produces runners or shoots along the ground that then root at each segment. Cutting and transplanting rooted runners is the primary technique for creating new plants, since seed growth is substantially slower. Abacá has a "false trunk" or pseudostem about 6 - in diameter. The leaf stalks (petioles) are expanded at the base to form sheaths that are tightly wrapped together to form the pseudostem. There are from 12 to 25 leaves, dark green on the top and pale green on the underside, sometimes with large brown patches. They are oblong in shape with a deltoid base. They grow in succession. The petioles grow to at least 1 ft in length. When the plant is mature, the flower stalk grows up inside the pseudostem. The male flower has 5 petals, each about 1.5 in long. The leaf sheaths contain the valuable fiber. After harvesting, the coarse fibers range in length from 6 - long. They are composed primarily of cellulose, lignin, and pectin.
The fruit, which is inedible and is rarely seen as harvesting occurs before the plant fruits, grows to about 2 - in length and 1 in in diameter. It has black turbinate seeds that are 0.167 in in diameter.
Systematics.
The abacá plant belongs to the banana family, Musaceae; it resembles the closely related wild seeded bananas, "Musa acuminata" and "Musa balbisiana". Its scientific name is "Musa textilis". Within the genus "Musa", it is placed in section "Callimusa" (now including the former section "Australimusa"), members of which have a diploid chromosome number of 2n = 20.
History.
Before synthetic textiles came into use, "M. textilis" was a major source of high quality fiber: soft, silky and fine. Europeans first came into contact with it when Magellan made land in the Philippines in 1521, as the natives were cultivating it and utilizing it in bulk for textiles already. By 1897, the Philippines were exporting almost 100,000 tons of abacá, and it was one of the three biggest cash crops, along with tobacco and sugar. In fact, from 1850 through the end of the 19th century, sugar or abacá alternated with each other as the biggest export crop of the Philippines. This 19th century trade was predominantly with the United States and the making of ropes was done mainly in New England, although in time the rope-making was moved back to the Philippines. Excluding the Philippines, abacá was first cultivated on a large scale in Sumatra in 1925 under the Dutch, who had observed its cultivation in the Philippines for cordage since the nineteenth century, followed up by plantings in Central America in 1929 sponsored by the U.S. Department of Agriculture. It also was transplanted into India and Guam. Commercial planting began in 1930 in British North Borneo; with the commencement of World War II, the supply from the Philippines was eliminated by the Japanese. After the war, the U.S. Department of Agriculture started production in Panama, Costa Rica, Honduras, and Guatemala. Today, abacá is produced commercially in only three countries: Philippines, Ecuador, and Costa Rica. The Philippines produces between 85% and 95% of the world's abacá, and the production employs 1.5 million people. Production has declined because of virus diseases.
Ancestors of the modern abaca might have originated from the Eastern Philippines where there are lot of rains (no pronounced dry season), in fact wild type of abaca can still be found in the interior forests of Catanduanes Island which is often not cultivated. Today, Catanduanes has many other modern kinds of abaca which are more competitive. For many years, breeders from various research institutions have made the cultivated varieties of Catanduanes Island even more competitive in local and international markets. This results in the optimum production of the island which had a consistent highest production throughout the archipelago.
Uses.
Due to its strength, it is a sought after product and is the strongest of the natural fibers. It is used by the paper industry for such specialty uses such as teabags, banknotes and decorative papers. It can be used to make handcrafts such as bags, carpets, clothing and furniture. Abacá rope is very durable, flexible and resistant to salt water damage, allowing its use in hawsers, ship's lines and fishing nets. A 1 in rope can require 4 MT to break. Abacá fiber was once used primarily for rope, but this application is now of minor significance. Lupis is the finest quality of abacá. Sinamay is woven chiefly from abacá.
Textiles.
The inner fibers are used in the making of hats, including the "Manila hats," hammocks, matting, cordage, ropes, coarse twines, and types of canvas. Called Manila hemp in the market although it is unlike true hemp. Also known as Cebu hemp and Davao hemp.
Cultivation.
The plant is normally grown in well-drained loamy soil, using rhizomes planted at the start of the rainy season. In addition, new plants can be started by seeds. Growers harvest abacá fields every three to eight months after an initial growth period of 12–25 months. Harvesting is done by removing the leaf-stems after flowering but before fruit appears. The plant loses productivity between 15 and 40 years. The slopes of volcanoes provide a preferred growing environment. Harvesting generally includes several operations involving the leaf sheaths:
When the processing is complete, the bundles of fiber are pale and lustrous with a length of 6 -.
In Costa Rica, more modern harvest and drying techniques are being developed to accommodate the very high yields obtained there.
More than 80% of modern abaca production comes from the Philippines. Bicol is the top producing region while Catanduanes Island remains the top producing province in the Philippines. The Philippine Rural Development Program (PRDP) and the Department of Agriculture reported that in 2009-2013, Bicol Region had 39% share of Philippine abaca producution while overwhelming 92% comes from Catanduanes Island. Eastern Visayas, the second largest producer had 24% and the Davao Region, the third largest producer had 11% of the total production.
Pathogens.
Abacá is vulnerable to a number of pathogens, notably abaca bunchy top virus and abaca bract mosaic virus.

</doc>
<doc id="2551" url="http://en.wikipedia.org/wiki?curid=2551" title="Astronomical year numbering">
Astronomical year numbering

Astronomical year numbering is based on AD/CE year numbering, but follows normal decimal integer numbering more strictly. Thus, it has a year 0, the years before that are designated with negative numbers and the years after that are designated with positive numbers. Astronomers use the Julian calendar for years before 1582, including this year 0, and the Gregorian calendar for years after 1582 as exemplified by Jacques Cassini (1740), Simon Newcomb (1898) and Fred Espenak (2007).
The prefix AD and the suffixes CE, BC or BCE (Common Era, Before Christ or Before Common Era) are dropped. The year 1 BC/BCE is numbered 0, the year 2 BC is numbered −1, and in general the year "n" BC/BCE is numbered "−("n" − 1)" (a negative number equal to 1 − "n"). The numbers of AD/CE years are not changed and are written with either no sign or a positive sign; thus in general "n" AD/CE is simply "n" or +"n". For normal calculation a number zero is often needed, here most notably when calculating the number of years in a period that spans the epoch; the end years need only be subtracted from each other.
The system is so named due to its use in astronomy. Few other disciplines outside history deal with the time before year 1, some exceptions being dendrochronology, archaeology and geology, the latter two of which use 'years before the present'. Although the absolute numerical values of astronomical and historical years only differ by one before year 1, this difference is critical when calculating astronomical events like eclipses or planetary conjunctions to determine when historical events which mention them occurred.
Year zero usage.
In his Rudolphine Tables (1627), Johannes Kepler used a prototype of year zero which he labeled "Christi" (Christ's) between years labeled "Ante Christum" (Before Christ) and "Post Christum" (After Christ) on the mean motion tables for the Sun, Moon, Saturn, Jupiter, Mars, Venus and Mercury. Then in 1702 the French astronomer Philippe de la Hire used a year he labeled "Christum 0" at the end of years labeled "ante Christum" (BC), and immediately before years labeled "post Christum" (AD) on the mean motion pages in his "Tabulæ Astronomicæ", thus adding the designation "0" to Kepler's "Christi". Finally, in 1740 the French astronomer Jacques Cassini (Cassini II), who is traditionally credited with the invention of year zero, completed the transition in his "Tables astronomiques", simply labeling this year "0", which he placed at the end of Julian years labeled "avant Jesus-Christ" (before Jesus Christ or BC), and immediately before Julian years labeled "après Jesus-Christ" (after Jesus Christ or AD).
Cassini gave the following reasons for using a year 0:
The year 0 is that in which one supposes that Jesus Christ was born, which several chronologists mark 1 before the birth of Jesus Christ and which we marked 0, so that the sum of the years before and after Jesus Christ gives the interval which is between these years, and where numbers divisible by 4 mark the leap years as so many before or after Jesus Christ.—Jacques Cassini
Fred Espanak of NASA lists 50 phases of the moon within year 0, showing that it is a full year, not an instant in time. <br> Jean Meeus gives the following explanation:
There is a disagreement between astronomers and historians about how to count the years preceding year 1. In ["Astronomical Algorithms"], the 'B.C.' years are counted astronomically. Thus, the year before the year +1 is the year zero, and the year preceding the latter is the year −1. The year which historians call 585 B.C. is actually the year −584.
The astronomical counting of the negative years is the only one suitable for arithmetical purpose. For example, in the historical practice of counting, the rule of divisibility by 4 revealing Julian leap-years no longer exists; these years are, indeed, 1, 5, 9, 13, ... B.C. In the astronomical sequence, however, these leap-years are called 0, −4, −8, −12, ..., and the rule of divisibility by 4 subsists.—Jean Meeus
Signed years without year 0.
Although he used the usual French terms "avant J.-C." (before Jesus Christ) and "après J.-C." (after Jesus Christ) to label years elsewhere in his book, the Byzantine historian Venance Grumel used negative years (identified by a minus sign, −) to label BC years and unsigned positive years to label AD years in a table, possibly to save space, without a year 0 between them.
Version 1.0 of the XML Schema language, often used to describe data interchanged between computers in XML, includes built-in primitive datatypes date and dateTime. Although these are defined in terms of ISO 8601 which uses the proleptic Gregorian calendar and therefore should include a year 0, the XML Schema specification states that there is no year zero. Version 1.1 of the defining recommendation realigned the specification with ISO 8601 by including a year zero, despite
the problems arising from the lack of backwards compatibility.

</doc>
<doc id="2593" url="http://en.wikipedia.org/wiki?curid=2593" title="Accounting">
Accounting

Accounting, or accountancy, is the measurement, processing and communication of financial information about economic entities. It was developed by the Italian mathematician Luca Pacioli, in the end of the 15th century. Accounting, which has been called the "language of business", measures the results of an organization's economic activities and conveys this information to a variety of users including investors, creditors, management, and regulators. Practitioners of accounting are known as accountants. The terms accounting and financial reporting are often used as synonyms.
Accounting can be divided into several fields including financial accounting, management accounting, auditing, and tax accounting. Financial accounting focuses on the reporting of an organization's financial information, including the preparation of financial statements, to external users of the information, such as investors, regulators and suppliers; and management accounting focuses on the measurement, analysis and reporting of information for internal use by management. The recording of financial transactions, so that summaries of the financials may be presented in financial reports, is known as bookkeeping, of which double-entry bookkeeping is the most common system.
Accounting is facilitated by such as standard-setters, accounting firms and professional bodies. Financial statements are usually audited by accounting firms, and are prepared in accordance with generally accepted accounting principles (GAAP). GAAP is set by various standard-setting organizations such as the Financial Accounting Standards Board (FASB) in the United States and the Financial Reporting Council in the United Kingdom. As of 2012, "all major economies" have plans to converge towards or adopt the International Financial Reporting Standards (IFRS).
History.
The history of accounting is thousands of years old and can be traced to ancient civilizations. The early development of accounting dates back to ancient Mesopotamia, and is closely related to developments in writing, counting and money; there is also evidence for early forms of bookkeeping in ancient Iran, and early auditing systems by the ancient Egyptians and Babylonians. By the time of the Emperor Augustus, the Roman government had access to detailed financial information.
Double-entry bookkeeping developed in medieval Europe, and accounting split into financial accounting and management accounting with the development of joint-stock companies. In Italy, it was published the first work on double-entry bookkeeping system by Luca Pacioli. Accounting began to transition into an organized profession in the nineteenth century, with local professional bodies in England merging to form the Institute of Chartered Accountants in England and Wales in 1880.
Etymology.
Both the words accounting and accountancy were in use in Great Britain by the mid-1800s, and are derived from the words "accompting" and "accountantship" used in the 18th century. In Middle English (used roughly between the 12th and the late 15th century) the verb "to account" had the form "accounten", which was derived from the Old French word "aconter", which is in turn related to the Vulgar Latin word "computare", meaning "to reckon". The base of "computare" is "putare", which "variously meant to prune, to purify, to correct an account, hence, to count or calculate, as well as to think."
The word "accountant" is derived from the French word "compter", which is also derived from the Latin word "computare". The word was formerly written in English as "accomptant", but in process of time the word, which was always pronounced by dropping the "p", became gradually changed both in pronunciation and in orthography to its present form.
Accounting and accountancy.
Accounting has variously been defined as the keeping or preparation of the financial records of an entity, the analysis, verification and reporting of such records and "the principles and procedures of accounting"; it also refers to the job of being an accountant.
Accountancy refers to the occupation or profession of an accountant, particularly in British English.
Topics.
Accounting has several subfields or subject areas, including financial accounting, management accounting, auditing, taxation and accounting information systems.
Financial accounting.
Financial accounting focuses on the reporting of an organization's financial information to external users of the information, such as investors, regulators and suppliers. It measures and records business transactions and prepares financial statements for the external users in accordance with generally accepted accounting principles (GAAP). GAAP, in turn, arises from the wide agreement between accounting theory and practice, and change over time to meet the needs of decision-makers.
Financial accounting produces past-oriented reports—for example the financial statements prepared in 2006 reports on performance in 2005—on an annual or quarterly basis, generally about the organization as a whole.
Management accounting.
Management accounting focuses on the measurement, analysis and reporting of information that can help managers in making decisions to fulfil the goals of an organization. In management accounting, internal measures and reports are based on cost-benefit analysis, and are not required to follow GAAP. In 2014 CIMA created the . The result of research from across 20 countries in five continents, the principles aim to guide best practice in the discipline.
Management accounting produces future-oriented reports—for example the budget for 2006 is prepared in 2005—and the time span of reports varies widely. Such reports may include both financial and nonfinancial information, and may, for example, focus on specific products and departments.
Auditing.
Auditing is the verification of assertions made by others regarding a payoff, and in the context of accounting it is the "unbiased examination and evaluation of the financial statements of an organization".
An audit of financial statements aims to express or disclaim an opinion on the financial statements. The auditor expresses an opinion on the fairness with which the financial statements presents the financial position, results of operations, and cash flows of an entity, in accordance with GAAP and "in all material respects". An auditor is also required to identify circumstances in which GAAP has not been consistently observed.
Accounting information systems.
An accounting information system is a part of an organisation's information system that focuses almost exclusively on processing quantitative data.
Tax accounting.
Tax accounting in the United States concentrates on the preparation, analysis and presentation of tax payments and tax returns. The U.S. tax system requires the use of specialised accounting principles for tax purposes which can differ from the generally accepted accounting principles (GAAP) for financial reporting. U.S. tax law covers four basic forms of business ownership: sole proprietorship, partnership, corporation, and limited liability company. Corporate and personal income are taxed at different rates, both varying according to income levels and including varying marginal rates (taxed on each additional dollar of income) and average rates (set as a percentage of overall income).
Organizations.
See also: .
Professional bodies.
Professional accounting bodies include the American Institute of Certified Public Accountants (AICPA) and the other 179 members of the International Federation of Accountants (IFAC), including CPA Australia, Institute of Chartered Accountants of India (ICAI) and Institute of Chartered Accountants in England and Wales (ICAEW). Professional bodies for subfields of the accounting professions also exist, for example the Chartered Institute of Management Accountants (CIMA). Many of these professional bodies offer education and training including qualification and administration for various accounting designations, such as certified public accountant and chartered accountant.
Accounting firms.
Depending on its size, a company may be legally required to have their financial statements audited by a qualified auditor, and audits are usually carried out by accounting firms.
Accounting firms grew in the United States and Europe in the late nineteenth and early twentieth century, and through several mergers there were large international accounting firms by the mid-twentieth century. Further large mergers in the late twentieth century led to the dominance by the auditing market by the Big Five accounting firms: Arthur Andersen, Deloitte, Ernst & Young, KPMG and PricewaterhouseCoopers. The demise of Arthur Andersen following the Enron scandal reduced the Big Five to the Big Four.
Standard-setters.
Generally accepted accounting principles (GAAP) are accounting standards issued by national regulatory bodies. In addition, the International Accounting Standards Board (IASB) issues the International Financial Reporting Standards (IFRS) implemented by 147 countries. While standards for international audit and assurance, ethics, education, and public sector accounting are all set by independent standard settings boards supported by IFAC. The International Auditing and Assurance Standards Board sets international standards for auditing, assurance, and quality control; the International Ethics Standards Board for Accountants (IESBA) sets the internationally appropriate principles- based "Code of Ethics for Professional Accounts" the International Accounting Education Standards Board (IAESB) sets professional accounting education standards; International Public Sector Accounting Standards Board (IPSASB) sets accrual-based international public sector accounting standards 
Organizations in individual countries may issue accounting standards unique to the countries. For example, in the United States the Financial Accounting Standards Board (FASB) issues the Statements of Financial Accounting Standards, which form the basis of US GAAP, and in the United Kingdom the Financial Reporting Council (FRC) sets accounting standards. However,as of 2012 "all major economies" have plans to converge towards or adopt the IFRS.
Education and qualifications.
Accounting degrees.
At least a bachelor's degree in accounting or a related field is required for most accountant and auditor job positions, and some employers prefer applicants with a master's degree. A degree in accounting may also be required for, or may be used to fulfil the requirements for, membership to professional accounting bodies. For example, the education during an accounting degree can be used to fulfil the American Institute of CPA's (AICPA) 150 semester hour requirement, and associate membership with the Certified Public Accountants Association of the UK is available after gaining a degree in finance or accounting.
A doctorate is required in order to pursue a career in accounting academia, for example to work as a university professor. The Doctor of Philosophy (PhD) and the Doctor of Business Administration (DBA) are the most popular degrees. The PhD is the most common degree for those wishing to pursue a career in academia, while DBA programs generally focus on equipping business executives for business or public careers requiring research skills and qualifications.
Professional qualifications.
Professional accounting qualifications include the Chartered Accountant designations and other qualifications including certificates and diplomas. In the United Kingdom, chartered accountants of the ICAEW undergo annual training, and are bound by the ICAEW's code of ethics and subject to its disciplinary procedures. In the United States, the requirements for joining the AICPA as a Certified Public Accountant are set by the Board of Accountancy of each state, and members agree to abide by the AICPA's Code of Professional Conduct and Bylaws. In India the Apex Accounting body constituted by parliament of India is "Institute of Chartered Accountants of India" (ICAI) was known for its rigorous training and study methodology for granting the Qualification.
Accounting research.
Accounting research is research on the effects of economic events on the process of accounting, and the effects of reported information on economic events. It encompasses a broad range of research areas including financial accounting, management accounting, auditing and taxation.
Accounting research is carried out both by academic researchers and practicing accountants. Academic accounting research "addresses all aspects of the accounting profession" using the scientific method, while research by practicing accountants focuses on solving problems for a client or group of clients. Academic accounting research can make significant contribution to accounting practice, although changes in accounting education and the accounting academia in recent decades has led to a divide between academia and practice in accounting.
Methodologies in academic accounting research can be classified into archival research, which examines "objective data collected from repositories"; experimental research, which examines data "the researcher gathered by administering treatments to subjects"; and analytical research, which is "based on the act of formally modeling theories or substantiating ideas in mathematical terms". This classification is not exhaustive; other possible methodologies include the use of case studies, computer simulations and field research.
Accounting and computer software.
Many laborious practices have been simplified with the help of computer software. Enterprise resource planning (ERP) software provides a comprehensive, centralized, integrated source of information that companies can use to manage all major business processes, from purchasing to manufacturing to human resources. This software can replace up to 200 individual software programs that were previously used. Computer integrated manufacturing allows products to be made and completely untouched by human hands and can increase production by having fewer errors in the manufacturing process.
Computers have reduced the cost of accumulating, storing, and reporting managerial accounting information and have made it possible to produce a more detailed account of all data that is entered into any given system. They have also changed business to business interaction through e-commerce. Rather than dealing with multiple companies to purchase products, a business can purchase a product at a less expensive price and take out the third party and vastly reduces expenses companies once accrued.
Additionally, Inter-organizational information system enable suppliers and businesses to be connected at all times. When a company is low on a product the supplier will be notified and fulfill an order immediately which eliminates the need for someone to do inventory, fill out the proper documents, send them out and wait for their products.
Accounting affects the economy.
Although financial accounting produces past-oriented reports, it is based on generally accepted accounting principles and generally accepted accounting practices compliant with International Financial Reporting Standards/US GAAP. In order to prepare the financial accounts/reports an entity has to comply with these GAAPs and gaaps. Which of these accounting practices and principles the board of directors choose at the start of the financial period and whatever changes in these generally accepted accounting principles and practices are implemented during the accounting period, affect the entity´s economy and affect the financial accounts (financial reports) prepared at the end of the financial period. When all entities implement the same change during the financial year as required by IFRS/US GAAP, then that affects the entire economy.
Accounting scandals.
The year 2001 witnessed a series of financial information frauds involving Enron, auditing firm Arthur Andersen, the telecommunications company WorldCom, Qwest and Sunbeam, among other well-known corporations. These problems highlighted the need to review the effectiveness of accounting standards, auditing regulations and corporate governance principles. In some cases, management manipulated the figures shown in financial reports to indicate a better economic performance. In others, tax and regulatory incentives encouraged over-leveraging of companies and decisions to bear extraordinary and unjustified risk.
The Enron scandal deeply influenced the development of new regulations to improve the reliability of financial reporting, and increased public awareness about the importance of having accounting standards that show the financial reality of companies and the objectivity and independence of auditing firms.
In addition to being the largest bankruptcy reorganization in American history, the Enron scandal undoubtedly is the biggest audit failure. It involved a financial scandal of Enron Corporation and their auditors Arthur Andersen, which was revealed in late 2001. The scandal caused the dissolution of Arthur Andersen, which at the time was one of the five largest accounting firms in the world. After a series of revelations involving irregular accounting procedures conducted throughout the 1990s, Enron filed for Chapter 11 bankruptcy protection in December 2001.
One consequence of these events was the passage of Sarbanes–Oxley Act in the United States 2002, as a result of the first admissions of fraudulent behavior made by Enron. The act significantly raises criminal penalties for securities fraud, for destroying, altering or fabricating records in federal investigations or any scheme or attempt to defraud shareholders.

</doc>
<doc id="2621" url="http://en.wikipedia.org/wiki?curid=2621" title="Aedicula">
Aedicula

In ancient Roman religion, an aedicula (plural aediculae) is a small shrine. The word "aedicula" is the diminutive of the Latin "aedes", a temple building or house.
Many aediculae were household shrines that held small altars or statues of the Lares and Penates. The Lares were Roman deities protecting the house and the family household gods. The Penates were originally patron gods (really genii) of the storeroom, later becoming household gods guarding the entire house.
Other aediculae were small shrines within larger temples, usually set on a base, surmounted by a pediment and surrounded by columns. In Roman architecture the aedicula has this representative function in the society. They are installed in public buildings like the Triumphal arch, City gate, or Thermes. The Celsus Library in Ephesus (2. c. AD) is a good example. From the 4th century Christianization of the Roman Empire onwards such shrines, or the framework enclosing them, are often called by the Biblical term tabernacle, which becomes extended to any elaborated framework for a niche, window or picture.
Gothic aediculae.
As in Classical architecture, in Gothic architecture, too, an aedicule or tabernacle frame is a structural framing device that gives importance to its contents, whether an inscribed plaque, a cult object, a bust or the like, by assuming the tectonic vocabulary of a little building that sets it apart from the wall against which it is placed. A tabernacle frame on a wall serves similar hieratic functions as a free-standing, three-dimensional architectural baldaquin or a ciborium over an altar.
In Late Gothic settings, altarpieces and devotional images were customarily crowned with gables and canopies supported by clustered-column piers, echoing in small the architecture of Gothic churches. Painted ædicules frame figures from sacred history in initial letters of Illuminated manuscripts.
Renaissance aediculae.
Classicizing architectonic structure and decor "all'antica", in the "ancient [Roman] mode", became a fashionable way to frame a painted or bas-relief portrait, or protect an expensive and precious mirror during the High Renaissance; Italian precedents were imitated in France, then in Spain, England and Germany during the later 16th century.
Post-Renaissance classicism.
Aedicular door surrounds that are architecturally treated, with pilasters or columns flanking the doorway and an entablature even with a pediment over it came into use with the 16th century. In the neo-Palladian revival in Britain, architectonic aedicular or tabernacle frames, carved and gilded. are favourite schemes for English Palladian mirror frames of the late 1720s through the 1740s, by such designers as William Kent.
Other aedicula.
Similar small shrines, called "naiskoi", are found in Greek religion, but their use was strictly religious.
Aediculae exist today in Roman cemeteries as a part of funeral architecture.
Presently the most famous Aedicule is situated inside the Church of the Holy Sepulchre in city of Jerusalem.
Contemporary American architect, Charles Moore uses the concept of aediculae in his work to create spaces within spaces and to evoke the spiritual significance of the home.

</doc>
<doc id="2624" url="http://en.wikipedia.org/wiki?curid=2624" title="Aegean civilizations">
Aegean civilizations

Aegean civilization is a general term for the Bronze Age civilizations of Greece around the Aegean Sea. There are three distinct but communicating and interacting geographic regions covered by this term: Crete, the Cyclades and the Greek mainland. Crete is associated with the Minoan civilization from the Early Bronze Age. The Cyclades converge with the mainland during the Early Helladic ("Minyan") period and with Crete in the Middle Minoan period. From ca. 1450 BC (Late Helladic, Late Minoan), the Greek Mycenaean civilization spreads to Crete.
Commerce.
Commerce was practiced to some extent in very early times, as is proved by the distribution of Melian obsidian over all the Aegean area. We find Cretan vessels exported to Melos, Egypt and the Greek mainland. Melian vases came in their turn to Crete. After 1600 B.C. there is very close commerce with Egypt, and Aegean things find their way to all coasts of the Mediterranean. No traces of currency have come to light, unless certain axeheads, too slight for practical use, had that character. Standard weights have been found, as well as representations of ingots. The Aegean written documents have not yet proved (by being found outside the area) to be epistolary (letter writing) correspondence with other countries. Representations of ships are not common, but several have been observed on Aegean gems, gem-sealings, frying pans and vases. They are vessels of low free-board, with masts and oars. Familiarity with the sea is proved by the free use of marine motifs in decoration. The most detailed illustrations are to be found on the 'ship fresco' at Akrotiri on the island of Thera (Santorini) preserved by the ash fall from the volcanic eruption which destroyed the town there.
Discoveries, later in the 20th century, of sunken trading vessels such as those at Uluburun and Cape Gelidonya off the south coast of Turkey have brought forth an enormous amount of new information about that culture.
Evidence.
For details of monumental evidence the articles on Crete, Mycenae, Tiryns, Troad, Cyprus, etc., must be consulted. The most representative site explored up to now is Knossos (see Crete) which has yielded not only the most various but the most continuous evidence from the Neolithic age to the twilight of classical civilization. Next in importance come Hissarlik, Mycenae, Phaestus, Hagia Triada, Tiryns, Phylakope, Palaikastro and Gournia.
External evidence.
Mycenae and Tiryns are the two principal sites on which evidence of a prehistoric civilization was remarked long ago by the classical Greeks.
Discovery.
The curtain-wall and towers of the Mycenaean citadel, its gate with heraldic lions, and the great "Treasury of Atreus" had borne silent witness for ages before Heinrich Schliemann's time; but they were supposed only to speak to the Homeric, or, at farthest, a rude Heroic beginning of purely Hellenic civilization. It was not until Schliemann exposed the contents of the graves which lay just inside the gate, that scholars recognized the advanced stage of art which prehistoric dwellers in the Mycenaean citadel had attained.
There had been, however, a good deal of other evidence available before 1876, which, had it been collated and seriously studied, might have discounted the sensation that the discovery of the citadel graves eventually made. Although it was recognized that certain tributaries, represented for example, in the XVIIIth Dynasty tomb of Rekhmara at Egyptian Thebes as bearing vases of peculiar forms, were of some Mediterranean race, neither their precise habitat nor the degree of their civilization could be determined while so few actual prehistoric remains were known in the Mediterranean lands. Nor did the Aegean objects which were lying obscurely in museums in 1870, or thereabouts, provide a sufficient test of the real basis underlying the Hellenic myths of the Argolid, the Troad and Crete, to cause these to be taken seriously. Aegean vases have been exhibited both at Sèvres and Neuchatel since about 1840, the provenance (i.e. source or origin) being in the one case Phylakope in Melos, in the other Cephalonia.
Ludwig Ross, the German archaeologist appointed Curator of the Antiquities of Athens at the time of the establishment of the Kingdom of Greece, by his explorations in the Greek islands from 1835 onwards, called attention to certain early intaglios, since known as Inselsteine; but it was not until 1878 that C. T. Newton demonstrated these to be no strayed Phoenician products. In 1866 primitive structures were discovered on the island of Therasia by quarrymen extracting pozzolana, a siliceous volcanic ash, for the Suez Canal works. When this discovery was followed up in 1870, on the neighbouring Santorin (Thera), by representatives of the French School at Athens, much pottery of a class now known immediately to precede the typical late Aegean ware, and many stone and metal objects, were found. These were dated by the geologist Ferdinand A. Fouqué, somewhat arbitrarily, to 2000 B.C., by consideration of the superincumbent eruptive stratum.
Meanwhile, in 1868, tombs at Ialysus in Rhodes had yielded to Alfred Biliotti many fine painted vases of styles which were called later the third and fourth "Mycenaean"; but these, bought by John Ruskin, and presented to the British Museum, excited less attention than they deserved, being supposed to be of some local Asiatic fabric of uncertain date. Nor was a connection immediately detected between them and the objects found four years later in a tomb at Menidi in Attica and a rock-cut "bee-hive" grave near the Argive Heraeum.
Even Schliemann's first excavations at Hissarlik in the Troad did not excite surprise. But the "Burnt City" of his second stratum, revealed in 1873, with its fortifications and vases, and a hoard of gold, silver and bronze objects, which the discoverer connected with it, began to arouse a curiosity which was destined presently to spread far outside the narrow circle of scholars. As soon as Schliemann came on the Mycenae graves three years later, light poured from all sides on the prehistoric period of Greece. It was recognized that the character of both the fabric and the decoration of the Mycenaean objects was not that of any well-known art. A wide range in space was proved by the identification of the Inselsteine and the Ialysus vases with the new style, and a wide range in time by collation of the earlier Theraean and Hissarlik discoveries. A relationship between objects of art described by Homer and the Mycenaean treasure was generally allowed, and a correct opinion prevailed that, while certainly posterior, the civilization of the Iliad was reminiscent of the Mycenaean.
Schliemann got to work again at Hissarlik in 1878, and greatly increased our knowledge of the lower strata, but did not recognize the Aegean remains in his "Lydian" city of the sixth stratum. These were not to be fully revealed until Dr. Wilhelm Dorpfeld, who had become Schliemann's assistant in 1879, resumed the work at Hissarlik in 1892 after the first explorer's death. But by laying bare in 1884 the upper stratum of remains on the rock of Tiryns, Schliemann made a contribution to our knowledge of prehistoric domestic life which was amplified two years later by Christos Tsountas's discovery of the palace at Mycenae. Schliemann's work at Tiryns was not resumed till 1905, when it was proved, as had long been suspected, that an earlier palace underlies the one he had exposed.
From 1886 dates the finding of Mycenaean sepulchres outside the Argolid, from which, and from the continuation of Tsountas's exploration of the buildings and lesser graves at Mycenae, a large treasure, independent of Schliemann's princely gift, has been gathered into the National Museum at Athens. In that year tholos-tombs, most already pillaged but retaining some of their furniture, were excavated at Arkina and Eleusis in Attica, at Dimini near Volos in Thessaly, at Kampos on the west of Mount Taygetus, and at Maskarata in Cephalonia. The richest grave of all was explored at Vaphio in Laconia in 1889, and yielded, besides many gems and miscellaneous goldsmiths' work, two golden goblets chased with scenes of bull-hunting, and certain broken vases painted in a large bold style which remained an enigma until the excavation of Knossos.
In 1890 and 1893, Staes cleared out certain less rich tholos-tombs at Thoricus in Attica; and other graves, either rock-cut "bee-hives" or chambers, were found at Spata and Aphidna in Attica, in Aegina and Salamis, at the Argive Heraeum and Nauplia in the Argolid, near Thebes and Delphi, and not far from the Thessalian Larissa. During the Acropolis excavations in Athens, which terminated in 1888, many potsherds of the Mycenaean style were found; but Olympia had yielded either none, or such as had not been recognized before being thrown away, and the temple site at Delphi produced nothing distinctively Aegean (in dating). The American explorations of the Argive Heraeum, concluded in 1895, also failed to prove that site to have been important in the prehistoric time, though, as was to be expected from its neighbourhood to Mycenae itself, there were traces of occupation in the later Aegean periods.
Prehistoric research had now begun to extend beyond the Greek mainland. Certain central Aegean islands, Antiparos, Ios, Amorgos, Syros and Siphnos, were all found to be singularly rich in evidence of the Middle-Aegean period. The series of Syran-built graves, containing crouching corpses, is the best and most representative that is known in the Aegean. Melos, long marked as a source of early objects but not systematically excavated until taken in hand by the British School at Athens in 1896, yielded at Phylakope remains of all the Aegean periods, except the Neolithic.
A map of Cyprus in the later Bronze Age (such as is given by J. L. Myres and M. O. Richter in Catalogue of the Cyprus Museum) shows more than 25 settlements in and about the Mesaorea district alone, of which one, that at Enkomi, near the site of Salamis, has yielded the richest Aegean treasure in precious metal found outside Mycenae. E. Chantre in 1894 picked up lustreless ware, like that of Hissariik, in central Phtygia and at Pteria, and the English archaeological expeditions, sent subsequently into north-western Anatolia, have never failed to bring back ceramic specimens of Aegean appearance from the valleys of the Rhyndncus, Sangarius and Halys.
In Egypt in 1887, W. M. F. Petrie found painted sherds of Cretan style at Kahun in the Fayum, and farther up the Nile, at Tell el-Amarna, chanced on bits of no fewer than 800 Aegean vases in 1889. There have now been recognized in the collections at Cairo, Florence, London, Paris and Bologna several Egyptian imitations of the Aegean style which can be set off against the many debts which the centres of Aegean culture owed to Egypt. Two Aegean vases were found at Sidon in 1885, and many fragments of Aegean and especially Cypriot pottery have been found during recent excavations of sites in Philistia by the Palestine Fund.
Sicily, ever since P. Orsi excavated the Sicel cemetery near Lentini in 1877, has proved a mine of early remains, among which appear in regular succession Aegean fabrics and motives of decoration from the period of the second stratum at Hissarlik. Sardinia has Aegean sites, for example, at Abini near Teti; and Spain has yielded objects recognized as Aegean from tombs near Cadiz and from Saragossa.
One land, however, has eclipsed all others in the Aegean by the wealth of its remains of all the prehistoric ages— Crete; and so much so that, for the present, we must regard it as the fountainhead of Aegean civilization, and probably for long its political and social centre. The island first attracted the notice of archaeologists by the remarkable archaic Greek bronzes found in a cave on Mount Ida in 1885, as well as by epigraphic monuments such as the famous law of Gortyna (also called Gortyn). But the first undoubted Aegean remains reported from it were a few objects extracted from Cnossus by Minos Kalokhairinos of Candia in 1878. These were followed by certain discoveries made in the S. plain Messara by F. Halbherr. Unsuccessful attempts at Cnossus were made by both W. J. Stillman and H. Schliemann, and A. J. Evans, coming on the scene in 1893, travelled in succeeding years about the island picking up trifles of unconsidered evidence, which gradually convinced him that greater things would eventually be found. He obtained enough to enable him to forecast the discovery of written characters, till then not suspected in Aegean civilization. The revolution of 1897-1898 opened the door to wider knowledge, and much exploration has ensued, for which see Crete.
Thus the "Aegean Area" has now come to mean the Archipelago with Crete and Cyprus, the Hellenic peninsula with the Ionian islands, and Western Anatolia. Evidence is still wanting for the Macedonian and Thracian coasts. Offshoots are found in the western Mediterranean area, in Sicily, Italy, Sardinia and Spain, and in the eastern Mediterranean area in Syria and Egypt. Regarding the Cyrenaica, we are still insufficiently informed.

</doc>
<doc id="2677" url="http://en.wikipedia.org/wiki?curid=2677" title="Abd ar-Rahman II">
Abd ar-Rahman II

Abd ar-Rahman II (Arabic: عبد الرحمن الثاني‎) (788–852) was Umayyad Emir of Córdoba in the Al-Andalus (Moorish Iberia) from 822 until his death.
Biography.
Abd ar-Rahman II was born in Toledo, the son of Emir Al-Hakam I. In his youth he took part in the so-called "massacre of the ditch", when from 700 to 5,000 people come to pay homage to the princes who were killed by order of Al-Hakam.
He succeeded his father as Emir of Córdoba in 822 and engaged in nearly continuous warfare against Alfonso II of Asturias, whose southward advance he halted (822–842). 
In 837, he suppressed a revolt of Christians and Jews in Toledo. 
He issued a decree by which the Christians were forbidden to seek martyrdom, and he had a Christian synod held to forbid martyrdom.
In 844, Abd ar-Rahman repulsed an assault by Vikings who had disembarked in Cadiz, conquered Seville (with the exception of its citadel) and attacked Córdoba itself. 
Thereafter he constructed a fleet and naval arsenal at Seville to repel future raids. 
He responded to William of Septimania's requests of assistance in his struggle against Charles the Bald's nominations.
Abd ar-Rahman was famous for his public building program in Córdoba where he died in 852. He made additions to the Mosque–Cathedral of Córdoba. A vigorous and effective frontier warrior, he was also well known as a patron of the arts. 
He was also involved in the execution of the "Martyrs of Córdoba".

</doc>
<doc id="2700" url="http://en.wikipedia.org/wiki?curid=2700" title="Abercarn">
Abercarn

Abercarn is a small town and community in Caerphilly county borough, Wales, 10 miles (16 km) north-west of Newport on the A467 between Cwmcarn and Newbridge, within the historic boundaries of Monmouthshire.
History.
The district was traditionally associated with the coal mining collieries, ironworks and tinplate works of the South Wales coalfield and South Wales Valleys, although all have now closed; the town, which lies in the middle portion of the Ebbw valley, being situated on the south-eastern flank of the once great mining region of Glamorgan and Monmouthshire.
On 11 September 1878, at the Prince of Wales Colliery, an underground explosion killed 268 coal miners.
Local government.
The area was part of the ancient Monmouthshire parish of Mynyddislwyn until the late 19th century. In 1892 a local board of health and local government district of Abercarn was formed. This became Abercarn urban district in 1894, governed by an urban district council of twelve members. Under the Local Government Act 1972 the urban district was abolished in 1974, becoming part of the borough of Islwyn, Gwent. Further local government organisation in 1996 placed the area in the county borough of Caerphilly. The former urban district corresponds to the three communities of Abercarn, Crumlin and Newbridge.
Sport.
Abercarn is home to Abercarn Rugby Club which is a member of the Welsh Rugby Union.

</doc>
<doc id="2754" url="http://en.wikipedia.org/wiki?curid=2754" title="AutoCAD DXF">
AutoCAD DXF

AutoCAD DXF (Drawing Interchange Format, or Drawing Exchange Format) is a CAD data file format developed by Autodesk for enabling data interoperability between AutoCAD and other programs.
DXF was originally introduced in December 1982 as part of AutoCAD 1.0, and was intended to provide an exact representation of the data in the AutoCAD native file format, DWG (Drawing), for which Autodesk for many years did not publish specifications. Because of this, correct imports of DXF files have been difficult. Autodesk now publishes the DXF specifications as a PDF on its website.
Versions of AutoCAD from Release 10 (October 1988) and up support both ASCII and binary forms of DXF. Earlier versions support only ASCII.
As AutoCAD has become more powerful, supporting more complex object types, DXF has become less useful. Certain object types, including ACIS solids and regions, are not documented. Other object types, including AutoCAD 2006's dynamic blocks, and all of the objects specific to the vertical market versions of AutoCAD, are partially documented, but not well enough to allow other developers to support them. For these reasons many CAD applications use the DWG format which can be licensed from AutoDesk or non-natively from the Open Design Alliance.
DXF coordinates are always without dimensions so that the reader or user needs to know the drawing unit or has to extract it from the textual comments in the sheets.
File structure.
ASCII versions of DXF can be read with a text-editor. The basic organization of a DXF file is as follows:
The data format of a DXF is called a "tagged data" format which "means that each data element in the file is preceded by an integer number that is called a group code. A group code's value indicates what type of data element follows. This value also indicates the meaning of a data element for a given object (or record) type. Virtually all user-specified information in a drawing file can be represented in DXF format."

</doc>
<doc id="2779" url="http://en.wikipedia.org/wiki?curid=2779" title="Atari 2600">
Atari 2600

The Atari 2600 is a home video game console released in September 1977 by Atari, Inc. It is credited with popularizing the use of microprocessor-based hardware and ROM cartridges containing game code, a format first used with the Fairchild Channel F game console. This format contrasts with the older model of having non-microprocessor dedicated hardware, which could play only the few games which are physically built in to the unit.
The console was originally sold as the Atari VCS, for Video Computer System. Following the 1982 release of the Atari 5200, the VCS was renamed "Atari 2600", after the unit's Atari part number, CX2600. The 2600 was typically bundled with two joystick controllers, a conjoined pair of paddle controllers, and a cartridge game—initially "Combat" and later "Pac-Man".
History.
Atari Inc. had purchased an engineering think tank in 1973 called Cyan Engineering to research next-generation video game systems, and had been working on a prototype known as "Stella" (named after one of the engineers' bicycles) for some time. Unlike prior generations of machines that used custom logic to play a small number of games, its core was a complete CPU, the famous MOS Technology 6502 in a cost-reduced version, known as the 6507. It was combined with a RAM-and-I/O chip, the MOS Technology 6532, and a display and sound chip known as the Television Interface Adaptor (TIA). The first two versions of the machine contain a fourth chip, a standard CMOS logic buffer IC, making Stella cost-effective. Some later versions of the console eliminated the buffer chip. 
Programs for small computers were generally stored on cassette tape, disk or paper tape. By the early 1970s, Hewlett-Packard manufactured desktop computers costing thousands of dollars such as the HP 9830, which packaged Read Only Memory (ROM) into removable cartridges to add special programming features, and these were being considered for use in games. At first, the design was not going to be cartridge-based, but after seeing a "fake" cartridge system on another machine, they realized they could place the games on cartridges essentially for the price of the connector and packaging.
In August 1976, Fairchild Semiconductor released their own CPU-based system, the Video Entertainment System. Stella was still not ready for production, but it was clear that it needed to be before there were a number of "me too" products filling up the market—which had happened after they released "Pong". Atari Inc. simply did not have the cash flow to complete the system quickly, given that sales of their own Pong systems were cooling. Nolan Bushnell eventually turned to Warner Communications, and sold the company to them in 1976 for US$28 million on the promise that Stella would be produced as soon as possible.
Key to the eventual success of the machine was the hiring of Jay Miner, a chip designer who managed to squeeze an entire wire wrap of equipment making up the TIA into a single chip. Once that was completed and debugged, the system was ready for shipping.
Launch and success.
The unit was originally priced at US$199, and shipped with two joysticks and a "Combat" cartridge (eight additional games were available at launch and sold separately). In a move to compete directly with the Channel F, Atari Inc. named the machine the Video Computer System (or VCS for short), as the Channel F was at that point known as the VES, for "Video Entertainment System". The VCS was also rebadged as the Sears Video Arcade and sold through Sears, Roebuck and Company stores.
When Fairchild learned of Atari Inc.'s naming, they quickly changed the name of their system to become the Channel F. However, both systems were now in the midst of a vicious round of price-cutting: "Pong" clones that had been made obsolete by these newer and more powerful machines were sold off to discounters for ever-lower prices. Soon many of the clone companies were out of business, and both Fairchild and Atari Inc. were selling to a public that was completely burnt out on Pong. In 1977, Atari Inc. sold 250,000 Video Computer Systems.
For the first year of production, the Video Computer System was manufactured in Sunnyvale, California. The consoles manufactured there had thick internal RF shielding, and thick plastic molding around the sides and bottom. These added weight to the console, and because all six switches were on the front, these consoles were nicknamed "Heavy Sixers". After this first year, production moved to Hong Kong, and the consoles manufactured there had thinner plastic molding. In 1978, only 550,000 units from a production run of 800,000 were sold, requiring further financial support from Warner to cover losses. This led directly to the disagreements that caused Atari Inc. founder Nolan Bushnell to leave the company in 1978.
Once the public realized it was possible to play video games other than "Pong", and programmers learned how to push its hardware's capabilities, the VCS gained popularity. By this point, Fairchild had given up, thinking video games were a passing fad, thereby handing the entire quickly growing market to Atari Inc. By 1979, the VCS was the best-selling Christmas gift (and console), mainly because of its exclusive content, and 1 million units were sold that year.
Atari Inc. then licensed the smash arcade hit "Space Invaders" by Taito, which greatly increased the unit's popularity when it was released in January 1980, doubling sales to over 2 million units. The VCS and its cartridges were the main factor behind Atari Inc. grossing more than $2 billion in 1980. Sales then doubled again for the next two years; by 1982, the console had sold 10 million units, while its best-selling game "Pac-Man" sold 7 million copies. The console also sold 450,000 units in West Germany by 1984. By 1982 the 2600 console cost Atari about $40 to make and was sold for an average of $125. The company spent $4.50 to $6 to manufacture each cartridge and $1 to $2 for advertising, and sold it for $18.95 wholesale.
In 1980, the VCS was given a minor revision in which the left and right difficulty switches were moved to the back of the console, leaving four switches on the front. Other than this, these four-switch consoles looked nearly identical to the earlier six-switch models. In 1982, another version of the four-switch console was released without woodgrain. They were nicknamed "Darth Vader" consoles due to their all-black appearance. These were also the first consoles to be officially called "Atari 2600", as the Atari 5200 was released the same year.
During this period, Atari Inc. expanded the 2600 family with two other compatible consoles. They designed the Atari 2700, a wireless version of the console that was never released because of a design flaw. The company also built a sleeker version of the machine dubbed the Atari 2800 to sell directly to the Japanese market in early 1983, but it suffered from competition with the newly released Nintendo Famicom.
In a survey mentioned by Jeff Rovin it is reported that more stores reported breakdowns of the Atari 2600 system than any other, and that Atari repair centers seemed to have the most trouble with consoles manufactured in 1980. In one case it is stated that a system was repaired five times before static electricity from a carpet was discovered as having caused the problem. The controllers were also a source of breakage because of the way they could be gripped by a player holding it with their fist, allowing players to get carried away and over control, which was less likely with other systems released at the time, such as the Magnavox Odyssey², which has controllers that are nearly half its size.
Sears Tele-Games 2600s.
Atari Inc. also continued their OEM relationship with Sears under the latter's Tele-Games brand label, which started in 1975 with the original "Pong". Sears released several versions of the 2600 as the Sears Video Arcade series from 1977 to 1983. These include the Rev. A "Heavy Sixer" model in 1977, the Rev. B "4 switch" model in 1980, and the US version of the Atari 2800 branded as the Sears Video Arcade II in 1983.
Sears also released their own versions of Atari Inc.'s games under the Tele-Games brand — often with different titles — which included the Tele-Games branded variations of text and picture labels. Three games were also produced by Atari Inc. for Sears as exclusive releases under the Tele-Games brand: "Steeplechase", "Stellar Track", and "Submarine Commander".
Sears' Tele-Games brand was unrelated to the company Telegames, which also produced cartridges for the Atari 2600 — mostly re-issues of M Network games.
Decline and remodel.
During this period, Atari Inc. continued to grow until it had one of the largest R&D divisions in Silicon Valley. However, it spent much of its R&D budget on projects that seemed out of place at a video game (or even home computer) company; many of these projects never saw the light of day. Meanwhile, several attempts to bring out newer consoles failed for one reason or another, although Atari Inc.'s home computer systems, the Atari 8-bit family, sold reasonably well, if not spectacularly. Warner was more than happy anyway, as it seemed to have no end to the sales of the 2600, and Atari Inc. was responsible for over half of the company's income.
The programmers of many of Atari Inc.'s biggest hits grew disgruntled with the company for not crediting game developers and many left the company and formed their own independent software companies. The most prominent and longest-lasting of these third-party developers was Activision, founded in 1980, whose titles quickly became more popular than those of Atari Inc. itself. Atari Inc. attempted to block third-party development for the 2600 in court but failed, and soon other publishers, such as Imagic and Coleco, entered the market. Atari Inc. suffered from an image problem when a company named Mystique produced a number of pornographic games for the 2600. The most notorious of these, "Custer's Revenge", was protested by women's and Native American groups because it depicted General George Armstrong Custer raping a bound Native American woman. Atari Inc. sued Mystique in court over the release of the game.
Atari Inc. continued to acquire licenses for the 2600, the most prominent of which included "Pac-Man" and "E.T." Public disappointment with these two titles and the market saturation of poor third-party titles are cited as major contributors to the video game crash of 1983. Suddenly, Atari Inc.'s growth meant it was losing massive amounts of money during the crash, at one point about $10,000 a day. Warner quickly grew tired of supporting Atari Inc., and started looking for buyers in 1984.
By mid-1984 most software development for the 2600 had stopped except by Atari and Activision, with third-party developers emphasizing ColecoVision games. Although not formally discontinued, the 2600 was de-emphasized for two years after Warner's 1984 sale of Atari Inc.'s Consumer Division to Commodore Business Machines founder Jack Tramiel, who wanted to concentrate on home computers. He ended all development of console games, including a 2600 "Garfield" game and an Atari 5200 port of "Super Pac-Man". Due to a large library and a low price point, the 2600 and its smaller cousin, the 2600jr, continued to sell well in the late 1980s and was not discontinued until 1992, outdoing all other hardware that Atari released trying to replicate its success.
Atari 2800.
The Atari 2800 is the Japanese version of the Atari 2600, released in October 1983. It was the first release of a 2600 designed specifically for the Japanese market, despite companies like Epoch distributing the 2600 in Japan previously.
The 2800 never captured a large market in Japan. It was released a short time after the Nintendo Famicom, which became the dominant console in the Japanese video game market of the time.
Codenamed "Cindy", and designed by Atari engineer Joe Tilly, the Atari 2800 had four controller ports instead of the standard two on the Atari 2600's. The controllers are an all-in one design using a combination of an 8-direction digital joystick and a 270-degree paddle, designed by John Amber.
The 2800's case design departed from the standard 2600 format, using a wedge shape with non-protruding switches.
Around 30 specially branded games were released for the 2800. Their boxes are in Japanese and have a silver/red color scheme similar to the packaging of Atari's 2600 branded games of the time. The ROM cartridges themselves had identical labels as their 2600 branded counterparts.
Sears liked the design of the Atari 2800 so much, they opted to sell a version under their Tele-Games label. It was released in the US in 1983 as the Sears Video Arcade II, and was packaged with 2 controllers and "Space Invaders".
The Atari 2800's case style was used as the basis for the Atari 7800's case style by Barney Huang.
Atari 2600 Jr..
In 1984, a new version of the 2600 was released. The new redesigned version of the 2600, unofficially referred to as the 2600 Jr., featured a smaller cost-reduced form factor with a modernized Atari 7800-like appearance. The redesigned 2600 was advertised as a budget gaming system (under US$50) that had the ability to run a large collection of classic games.
The Atari 2600 continued to sell in North America and Europe until 1991, and in Asia until the early 1990s. Its final Atari-licensed release was "KLAX" in 1990. In 2007 the Atari 2600 was inducted into the Toy Hall of Fame, selling 40 million units in its lifetime, and the youngest toy to be inducted. In Brazil, the console became extremely popular in the mid-1980s. The Atari 2600 was officially retired by Atari Corp. on January 1, 1992, making it, at the time, the longest-lived home video game console (14 years, 4 months) in video game history. It was later surpassed by the Sega Master System, a console which never formally ended production in Brazil.
The system was promoted on a United Kingdom TV ad in 1989 in the run-up to Christmas, in which it claimed "The fun is back!". The advertising campaign used its price of under £50 as a selling point. The advert was a re-dubbed version of the early original campaign in the United States. Also, the 2600 Jr. was originally to be packaged with a Pro-Line joystick (the same one used on the Atari 7800), but when released, it instead included the original CX-40 Joystick. Later European versions of the 2600 Jr. included a joypad, which was also featured with the European 7800.
Design.
Hardware.
The CPU was the MOS Technology 6507, a cut-down version of the 6502, running at 1.19 MHz in the 2600. The 6507 included fewer memory address pins—13 instead of 16—and no external interrupts to fit into a smaller 28-pin package. Smaller packaging was, and still is, an important factor in overall system cost, and since memory was very expensive at the time, the 6507's small 8 kB of maximum external memory space was not going to be used up anyway. In fact, memory was so expensive they could not imagine using up even 4 kB, and when Atari got a deal on 24-pin connectors for the cartridge socket, they took it, despite this limiting the games to 4K. Later games get around this limitation with bank switching. The maximum cartridge size supported is 32 kilobytes.
The console has only 128 bytes of RAM for run-time data that includes the call stack and the state of the game world. There is no frame buffer, as the necessary RAM would have been too expensive. Instead the video device has two bitmapped sprites, two one-pixel "missile" sprites, a one-pixel "ball," and a 40-pixel "playfield" that is drawn by writing a bit pattern for each line into a register just before the television scans that line. As each line is scanned, a game must identify the non-sprite objects that overlaps the next line, assemble the appropriate bit patterns to draw for those objects, and write the pattern into the register. In a telling reveal of its Pong heritage, by default, the right side of the screen is a mirrored duplicate of the left; to control it separately, the software may modify the patterns as the scan line is drawn. After the controller scans the last active line, a more leisurely vertical blanking interval begins, during which the game can process input and update the positions and states of objects in the game world. Any mistake in timing produces visual artifacts, a problem programmers call "racing the beam" and which users call "flickering".
The video hardware gives the 2600 a reputation as one of the most complex game consoles in the world to program, but those programmers who understand it realize that such direct control over the video picture is also a source of flexibility. One advantage the 2600 has over more powerful contemporary competitors such as the ColecoVision is that the 2600 has no protection against altering settings in mid-line. For example, although each sprite nominally has only one color, it is possible to color the rows differently by changing the sprite's color as it is drawn. If the two hardware sprites are not enough for a game, a developer may share one sprite among several objects (as with the ghosts in "Pac-Man") or draw software sprites, which is only a little more difficult than drawing a fixed playfield. The "Pitfall!" screenshot below demonstrates some of these tricks: the player is a multi-color sprite, one sprite is multiplexed for the logs and the scorpion, and the swinging vine is drawn by shifting the position of the "ball" on each scan line. Despite the hardware limitations, many Atari 2600 games have a lot of action on the screen, creating an engaging experience.
Additionally, the 2600 supports several types of input devices (joysticks, paddles, keyboards, etc.) and third-party peripherals. Many of these peripherals are interchangeable with the MSX and several other Japanese systems; and in some cases, it is possible to use the Atari joysticks with the Commodore 64, Commodore 128, Amiga, Sega Master System, and Mega Drive/Genesis, though functionality may be limited. Also, Master System and Mega Drive/Genesis controllers work on the Atari 2600, though only the B button can be used in most games. Another adapter is the Starpath Supercharger, an add-on created by Starpath to expand the game capabilities of the Atari 2600. The Supercharger interface adds an extra 6 KB to the Atari 2600's 128 bytes of RAM, allowing for larger games with higher resolution graphics. A cord coming out of the side of the cartridge plugs into the earphone jack of any standard cassette player. Games for the Supercharger are stored on standard audio cassettes.
Color and graphics.
The Atari 2600 uses different color palettes depending on the television signal format used. With the NTSC format, a 128-color palette is available, while in PAL, only 104 colors are available. Additionally, the SECAM palette consists of only 8 colors.
Games.
In 1977 nine games were released on cartridge to accompany the launch of the machine, including Outlaw, Space War and Breakout. During the console's lifetime, Atari Inc and Atari Corp. published many titles. These games include "Adventure" (often credited as starting the action-adventure game genre—its creator, Warren Robinett, also introduced the first widely known Easter egg to the gaming world), "Breakout", and "Yars' Revenge". The console's popularity attracted many third-party developers, which led to popular titles such as Activision's "Pitfall!" and Imagic's "Atlantis". However, two Atari published titles, "E.T. the Extra-Terrestrial" and "Pac-Man", are frequently blamed for contributing to the video game crash of 1983.
Legacy.
The Atari 2600 was wildly successful, and during much of the 1980s, "Atari" was a synonym for this model in mainstream media and, by extension, for video games in general.
The Atari 2600 was inducted into the National Toy Hall of Fame at The Strong in Rochester, New York in 2007. In 2009, the Atari 2600 was named the second greatest video game console of all time by IGN, who cited its remarkable role as the console behind both the first video game boom and the video game crash of 1983, and called it "the console that our entire industry is built upon."
Atari 2000.
The Atari 2000 (model number CX-2000) is a prototype version of the Atari 2600 intended to be released as a cheaper alternative for children in 1982. Although identical in specification to the original 2600, the 2000 includes built-in controllers and a different case design. The 2000 was originally intended to be black, but it was later recolored blue to appeal more to children. While Atari never officially stated the reason for not releasing the 2000, experts have cited the poor quality and durability of its built-in joysticks and the greater in-house popularity of the competing 2600 Jr. design as the most likely reasons.
Atari 3200.
Atari started work on a replacement to the 2600, called the Atari 3200, with codenames including Super Stella, Sylvia, and PAM (a note attached reads "Super Stella: Multipurpose"). The system was to have compatibility with Atari 2600 cartridges, and was rumored based on a 10-bit processor, although design documents shows it was to be based around the 6502 8-bit CPU. It was still unfinished when preliminary game programmers discovered that it was difficult to program. The project was cancelled, and Atari went with the second "System X" also titled PAM, that would later become the Atari 5200. Atari also cloned the Atari 3200 into the Sears Super Arcade II, but this was never released.
Clones and reissues.
The console and its old and new games are very popular with collectors because of its significant impact on video game and consumer electronics history and also due to its nostalgic value for many people, along with a number of games that are still considered highly playable. In addition, modern Atari 2600 clones remain on the market. One example is the Atari Classics 10-in-1 TV Game, manufactured by Jakks Pacific, which emulates the 2600 console, and includes converted versions of 10 games into a single Atari-brand-lookalike joystick with composite video outputs for connecting directly to modern televisions or VCRs. Another is the TV Boy, which includes 127 games in an enlarged joypad.
The Atari Flashback 2 console, released in 2005, contains 40 games (with four more programs unlockable by a cheat code). The console implements the original 2600 architecture and can be modified to play original 2600 cartridges by adding a cartridge port, and is compatible with original 2600 controllers.
In music.
Many games for the Atari 2600 have detailed and easily identifiable music, and its distinctive sound makes it ideal for use in modern lo-fi and industrial music. In 2002, Dallas musician and visual artist Paul Slocum developed a cartridge called Synthcart for the Atari 2600, which allows the user to turn an Atari 2600 into a two-voice synthesizer and drum machine. Adapters have also been developed by amateurs enabling the Atari 2600's use with MIDI devices. A number of bands, such as 8 Bit Weapon, Black Moth Super Rainbow and The Squigs, as well as Slocum's own band Tree Wave, use Synthcart to make modern music on the Atari 2600. Some effects units like the MXR Blue Box are often cited for their ability to produce an Atari-like sound. Phonte from the hip-hop group Little Brother, along with fellow lyricist Eccentric, formed a mock-group named Unheralded Symmetrics, and recorded a tribute to the system, entitled "Atari 2600".
Emulation.
Atari 2600 emulation is available for most major operating systems and is now very accurate. Despite the relative simplicity of the 2600 system, it is not an easy system to emulate. While it does not require a lot of computational power to emulate the 2600, it is hard to accurately do so. For example, because of the lack of a frame buffer, 2600 emulators must not only emulate the console, but the television as well. Due to the longevity of the system, many 2600 games use undocumented features, and even exploit bugs in the hardware to squeeze the most functionality and performance out of the system, doing things even the original designers would deem impossible. A notable example is the starfield of the game "Cosmic Ark". It took some time for the emulator programmers to mature their software to properly emulate the undocumented features, bugs and quirks of the system.
The MESS emulator supports recording and playing back of Atari 2600 emulation sessions. The Home Action Replay Page (a.k.a. HARP) allows Atari 2600 users to archive their favorite play sessions of the Atari 2600 system and its games. The javatari emulator has a multiplayer mode that allows two users to play as Player1 and Player2 respectively, as if they were playing on the same console.
Well known Atari 2600 emulators today include the following:
Homebrews.
After 30 years since the launch of the Atari 2600, new homebrew games for the system are still made and sold by hobbyists with several new titles available each year. Most of the development on the platform is still done in 6502 assembly language but a BASIC-like language compiler named batari Basic (or "bB") and visual environment called Visual batari Basic are also available.
Games created for the Atari can be executed using either an emulator or copied directly to a blank cartridge making use of either a PROM or EPROM chip. This allows the construction of homebrew cartridges that will run on an original Atari 2600.
Programmers.
This is a partial list of Atari 2600 programmers:
References.
</dl>

</doc>
